[
    {
        "text": "fail build if contrib tests fail to compile spinoff of lucene-885, from steven's comments... looking at the current build (r545324) it looks like the some contrib failures are getting swallowed. things like lucli are throwing errors along the lines of [subant] /home/barronpark/smparkes/work/lucene/trunk/common-build.xml:366: srcdir \"/home/barronpark/smparkes/work/lucene/trunk/contrib/lucli/src/test\" does not exist! but these don't make it back up to the top level status. it looks like the current state will bubble up junit failures, but maybe not build failures? ... it's \"test-compile-contrib\" (if you will) that fails and rather being contrib-crawled, that's only done as the target of \"test\" in each contrib directory, at which point, it's running in the protected contrib-crawl. easy enough to lift this loop into another target, e.g., build-contrib-test. and that will start surfacing errors, which i can work through.",
        "label": 18
    },
    {
        "text": " fdx size mismatch  exception in storedfieldswriter closedocstore  when closing index with 500m documents when closing index that contains 500,000,000 randomly generated documents, an exception is thrown: java.lang.runtimeexception: after flush: fdx size mismatch: 500000000 docs vs 4000000004 length in bytes of _0.fdx at org.apache.lucene.index.storedfieldswriter.closedocstore(storedfieldswriter.java:94) at org.apache.lucene.index.docfieldconsumers.closedocstore(docfieldconsumers.java:83) at org.apache.lucene.index.docfieldprocessor.closedocstore(docfieldprocessor.java:47) at org.apache.lucene.index.documentswriter.closedocstore(documentswriter.java:367) at org.apache.lucene.index.indexwriter.flushdocstores(indexwriter.java:1688) at org.apache.lucene.index.indexwriter.doflush(indexwriter.java:3518) at org.apache.lucene.index.indexwriter.flush(indexwriter.java:3442) at org.apache.lucene.index.indexwriter.closeinternal(indexwriter.java:1623) at org.apache.lucene.index.indexwriter.close(indexwriter.java:1588) at org.apache.lucene.index.indexwriter.close(indexwriter.java:1562) ... this appears to be a bug at storedfieldswriter.java:93: if (4+state.numdocsinstore*8 != state.directory.filelength(state.docstoresegmentname + \".\" + indexfilenames.fields_index_extension)) where the multiplication by 8 is causing integer overflow. the fix would be to cast state.numdocsinstore to long before multiplying. it appears that this is another instance of the mistake that caused bug lucene-1519. i did a cursory seach for *8 against the code to see if there might be yet more instances of the same mistake, but found none.",
        "label": 33
    },
    {
        "text": "patch multilevelskiplistreader nullpointerexception when reconstructing document using luke tool, received nullpointerexception. java.lang.nullpointerexception at org.apache.lucene.index.multilevelskiplistreader.loadskiplevels(multilevelskiplistreader.java:188) at org.apache.lucene.index.multilevelskiplistreader.skipto(multilevelskiplistreader.java:97) at org.apache.lucene.index.segmenttermdocs.skipto(segmenttermdocs.java:164) at org.getopt.luke.luke$2.run(unknown source) luke version 0.7.1 i emailed with luke author andrzej bialecki and he suggested the attached patch file which fixed the problem.",
        "label": 32
    },
    {
        "text": "docvalues fnfe i created a test for lucene-3335, and it found an unrelated bug in docvalues.",
        "label": 46
    },
    {
        "text": "support all of unicode in standardtokenizer standardtokenizer currently only supports the bmp. if it encounters characters outside of the bmp, it just discards them... it should instead implement fully implement uax#29 across all of unicode.",
        "label": 47
    },
    {
        "text": "fastvectorhighlighter simpleboundaryscanner does not work well when highlighting at the beginning of the text the simpleboundaryscanner still breaks text not based on characters provided when highlighting text that end up scanning to the beginning of the text to highlight. in this case, just use the start of the text as the offset.",
        "label": 26
    },
    {
        "text": "tragic events during merges can lead to deadlock when an indexwriter#commit() is stalled due to too many pending merges, you can get a deadlock if the currently active merge thread hits a tragic event. the thread performing the commit synchronizes on the the commitlock in commitinternal. the thread goes on to to call concurrentmergescheduler#dostall() which waits() on the concurrentmergescheduler object. this release the merge scheduler's monitor lock, but not the commitlock in indexwriter. sometime after this wait begins, the merge thread gets a tragic exception can calls indexwriter#tragicevent() which in turn calls indexwriter#rollbackinternal(). the indexwriter#rollbackinternal() synchronizes on the commitlock which is still held by the committing thread from (1) above which is waiting on the merge(s) to complete. hence, deadlock. we hit this bug with lucene 5.5, but i looked at the code in the master branch and it looks like the deadlock still exists there as well.",
        "label": 33
    },
    {
        "text": "fix javadocs html entity bugs as of jdk9-ea-b158, ant documentation seems to build the core javadocs just fine, but fails on the lucene/memory/ javadocs... javadocs:     [mkdir] created dir: /home/hossman/lucene/dev/lucene/build/docs/memory download-java8-javadoc-packagelist:   [javadoc] generating javadoc   [javadoc] javadoc execution   [javadoc] loading source files for package org.apache.lucene.index.memory...   [javadoc] constructing javadoc information...   [javadoc] standard doclet version 9-ea   [javadoc] building tree for all the packages and classes...   [javadoc] javadoc: warning - invalid usage of tag &pa   [javadoc] javadoc: warning - invalid usage of tag &pid   [javadoc] javadoc: warning - invalid usage of tag &page   [javadoc] building index for all the packages and classes...   [javadoc] building index for all classes...   [javadoc] generating /home/hossman/lucene/dev/lucene/build/docs/memory/help-doc.html...   [javadoc] note: custom tags that were not seen:  @lucene.internal   [javadoc] 3 warnings build failed /home/hossman/lucene/dev/build.xml:93: the following error occurred while executing this line: /home/hossman/lucene/dev/lucene/build.xml:251: the following error occurred while executing this line: /home/hossman/lucene/dev/lucene/common-build.xml:2179: the following error occurred while executing this line: /home/hossman/lucene/dev/lucene/module-build.xml:549: the following error occurred while executing this line: /home/hossman/lucene/dev/lucene/module-build.xml:65: the following error occurred while executing this line: /home/hossman/lucene/dev/lucene/module-build.xml:78: the following error occurred while executing this line: /home/hossman/lucene/dev/lucene/common-build.xml:2155: javadocs warnings were found! total time: 1 minute 0 seconds looking at the generated html files turns up this... hossman@tray:~/lucene/dev [master] $ find lucene/build/docs/memory -name \\*.html | xargs grep -c5 \"&pa\" lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- but rather thrown away immediately after tokenization. lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- <p> lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- for some interesting background information on search technology, see bob wyman's lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- <a target=\"_blank\" href=\"http://bobwyman.pubsub.com/main/2005/05/mary_hodder_poi.html\">prospective search</a>,  lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- jim gray's lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html: <a target=\"_blank\" href=\"http://www.acmqueue.org/modules.php?name=content&pa=showpage&pid=293&page=4\"> lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- a call to arms - custom subscriptions</a>, and tim bray's lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- <a target=\"_blank\" href=\"http://www.tbray.org/ongoing/when/200x/2003/07/30/onsearchtoc\">on search, the series</a>. lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html-  lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- <p> lucene/build/docs/memory/org/apache/lucene/index/memory/memoryindex.html- <b>example usage</b>  the source java file has this...  * jim gray's  * <a target=\"_blank\" href=\"http://www.acmqueue.org/modules.php?name=content&pa=showpage&pid=293&page=4\">  * a call to arms - custom subscriptions</a>, and tim bray's ...which does in fact seem to be invalid html ... aren't & always suppose to be encoded as & ... even in urls? i'm suprised the java8 javadocs/linter don't warn about this.",
        "label": 53
    },
    {
        "text": "suggestor version doesn't support multivalued fields so if you use a multivalued field in the new suggestor it will not pick up terms for any term after the first one. so it treats the first term as the only term it will make it's dictionary from. this is the suggestor i'm talking about: https://issues.apache.org/jira/browse/solr-5378",
        "label": 13
    },
    {
        "text": "add example test case for surround query language ",
        "label": 29
    },
    {
        "text": "improve speed of thaiwordfilter by characteriterator  factor out lowercasing and also fix some bugs  empty tokens stop iteration  the thaiwordfilter creates new strings out of term buffer before passing to the breakiterator., but breakiterator can take a characteriterator and directly process on it without buffer copying. as java itsself does not provide a characteriterator implementation in java.text, we can use the javax.swing.text.segment class, that operates on a char[] and is even reuseable! this class is very strange but it works and is in jdk 1.4+ and not deprecated. the filter also had a bug: it stopped iterating tokens when an empty token occurred. also the lowercasing for non-thai words was removed and put into the analyzer by adding lowercasefilter.",
        "label": 53
    },
    {
        "text": "locking documentation in  apache lucene   index file formats  section  lock file  out dated i am in the process to migrate from lucene 2.0 to lucene 2.1. from reading the changes document i understand that the write locks are now written into the index folder instead of the java.io.tmpdir. in the \"apache lucene - index file formats\" document in section \"6.2 lock file\" i read that there is a write lock used to indicate that another process is writing into the index and that this file is stored in the java.io.tempdir. this is confusing to me. i had the impression all lock files go into the index folder now. and using the the java.io.tempdir is only local and does not support access to shared index folders. do i miss something here or is the documentation not updated?",
        "label": 33
    },
    {
        "text": "remove searcher from weight explain explain needs to calculate corpus wide stats in a way that is consistent with multisearcher.",
        "label": 29
    },
    {
        "text": "allow negative pre post values in spannotquery i need to be able to specify a certain range of allowed overlap between the include and exclude parameters of spannotquery. since this behaviour is the inverse of the behaviour implemented by the pre and post constructor arguments, i suggest that this be implemented with negative pre and post values. patch incoming.",
        "label": 10
    },
    {
        "text": "unifiedhighlighter should support the new weight matches api for better match accuracy the new weight.matches() api should allow the unifiedhighlighter to more accurately highlight some booleanquery patterns correctly \u2013 see lucene-7903. in addition, this api should make the job of highlighting easier, reducing the loc and related complexities, especially the uh's phrasehelper. note: reducing/removing phrasehelper is not a near-term goal since weight.matches is experimental and incomplete, and perhaps we'll discover some gaps in flexibility/functionality. this issue should introduce a new unifiedhighlighter.highlightflag enum option for this method of highlighting. perhaps call it weight_matches? longer term it could go away and it'll be implied if you specify enum values for phrases & multi_term_query?",
        "label": 10
    },
    {
        "text": "numericfield should be stored in binary format in index  matching solr's format  (spinoff of lucene-3001) today when writing stored fields we don't record that the field was a numericfield, and so at indexreader time you get back an \"ordinary\" field and your number has turned into a string. see https://issues.apache.org/jira/browse/lucene-1701?focusedcommentid=12721972&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972 we have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in solr's more-compact binary format. a nice side-effect is we fix the long standing issue that you don't get a numericfield back when loading your document.",
        "label": 53
    },
    {
        "text": "npe in termautomatonscorer assert statement i randomly hit this while doing unrelated beasting ... i haven't dug into why yet, but it does repro on trunk currently (rev 1689075) .....enote: reproduce with: ant test  -dtestcase=testtermautomatonquery -dtests.method=testrandom -dtests.seed=1646a72e65e35ce9 -dtests.slow=true -dtests.linedocsfile=/lucenedata/hudson.enwiki.random.lines.txt.fixed -dtests.locale=mt -dtests.timezone=asia/macao -dtests.asserts=true -dtests.file.encoding=utf-8 ........note: test params are: codec=asserting(lucene53), sim=defaultsimilarity, locale=mt, timezone=asia/macao note: linux 3.13.0-46-generic amd64/oracle corporation 1.8.0_40 (64-bit)/cpus=8,threads=1,free=2013116904,total=2058354688 note: all tests run in this jvm: [testtermautomatonquery] time: 0.643 there was 1 failure: 1) testrandom(org.apache.lucene.search.testtermautomatonquery) java.lang.nullpointerexception  at __randomizedtesting.seedinfo.seed([1646a72e65e35ce9:640a8221d483ea9a]:0)  at org.apache.lucene.search.termautomatonscorer.donext(termautomatonscorer.java:177)  at org.apache.lucene.search.termautomatonscorer.nextdoc(termautomatonscorer.java:144)  at org.apache.lucene.search.weight$defaultbulkscorer.scoreall(weight.java:216)  at org.apache.lucene.search.weight$defaultbulkscorer.score(weight.java:169)  at org.apache.lucene.search.assertingbulkscorer.score(assertingbulkscorer.java:80)  at org.apache.lucene.search.assertingbulkscorer.score(assertingbulkscorer.java:64)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:621)  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:92)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:425)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:544)  at org.apache.lucene.search.indexsearcher.searchafter(indexsearcher.java:402)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:413)  at org.apache.lucene.search.testtermautomatonquery.testrandom(testtermautomatonquery.java:598)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:497)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1627)  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:836)  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:872)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:886)  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:49)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:65)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:365)  at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:798)  at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:458)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:845)  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:747)  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:781)  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:792)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42)  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:54)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:65)  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:365)  at java.lang.thread.run(thread.java:745) failures!!! tests run: 13,  failures: 1 the affected line is this assert:     assert docidqueue.top().posenum.docid() > docid; must be some sort of corner case ...",
        "label": 33
    },
    {
        "text": "add support to get number of entries a suggester lookup was built with and minor refactorings it would be nice to be able to tell the number of entries a suggester lookup was built with. this would let components using lookups to keep some stats regarding how many entries were used to build a lookup. additionally, dictionary could use inputiterator rather than the bytesrefiteratator, as most of the implmentations now use it.",
        "label": 5
    },
    {
        "text": "disjunctionmaxscorer skipto has bug that keeps it from skipping as reported on the mailing list, disjunctionmaxscorer.skipto is broken if called before next in some situations... http://www.nabble.com/potential-issue-with-disjunctionmaxscorer-tf3846366.html#a10894987",
        "label": 12
    },
    {
        "text": "possible hidden exception on segmentinfos commit i am not sure if this is that big of a deal, but i just ran into it and thought i might mention it. segmentinfos.commit removes the segments file if it hits an exception. if it cannot remove the segments file (because its not there or on windows something has a hold of it), another exception is thrown about not being able to delete the segments file. because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete. mark",
        "label": 33
    },
    {
        "text": "remove deprecated field store compress also remove fieldformerge and related code.",
        "label": 53
    },
    {
        "text": "testminimize testagainstbrzozowski reproducible seed oom     [junit] testsuite: org.apache.lucene.util.automaton.testminimize     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 3.792 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testminimize -dtestmethod=testagainstbrzozowski -dtests.seed=-7429820995201119781:1013305000165135537     [junit] note: test params are: codec=preflex, locale=ru, timezone=america/pangnirtung     [junit] note: all tests run in this jvm:     [junit] [testminimize]     [junit] note: linux 2.6.37-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=294745976,total=310378496     [junit] ------------- ---------------- ---------------     [junit] testcase: testagainstbrzozowski(org.apache.lucene.util.automaton.testminimize):     caused an error     [junit] java heap space     [junit] java.lang.outofmemoryerror: java heap space     [junit]     at java.util.bitset.initwords(bitset.java:144)     [junit]     at java.util.bitset.<init>(bitset.java:139)     [junit]     at org.apache.lucene.util.automaton.minimizationoperations.minimizehopcroft(minimizationoperations.java:85)     [junit]     at org.apache.lucene.util.automaton.minimizationoperations.minimize(minimizationoperations.java:52)     [junit]     at org.apache.lucene.util.automaton.regexp.toautomaton(regexp.java:502)     [junit]     at org.apache.lucene.util.automaton.regexp.toautomatonallowmutate(regexp.java:478)     [junit]     at org.apache.lucene.util.automaton.regexp.toautomaton(regexp.java:428)     [junit]     at org.apache.lucene.util.automaton.automatontestutil.randomautomaton(automatontestutil.java:256)     [junit]     at org.apache.lucene.util.automaton.testminimize.testagainstbrzozowski(testminimize.java:43)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1282)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1211)     [junit]      [junit]      [junit] test org.apache.lucene.util.automaton.testminimize failed",
        "label": 53
    },
    {
        "text": "improve disjoint check for geo distance query traversal when doing geo distance queries, it is important to avoid traversing subtrees which do not contain any relevant points. we currently have checks which compare the bbox of the query to the bounds of the subtree. however, it is possible for a subtree to overlap the bbox, but still not intersect the query. this issue is to improve that check to avoid unnecessary traversals.",
        "label": 41
    },
    {
        "text": "share the term   terminfo cache across threads right now each thread creates its own (thread private) simplelrucache, holding up to 1024 terms. this is rather wasteful, since if there are a high number of threads that come through lucene, you're multiplying the ram usage. you're also cutting way back on likelihood of a cache hit (except the known multiple times we lookup a term within-query, which uses one thread). in nrt search we open new segmentreaders (on tiny segments) often which each thread must then spend cpu/ram creating & populating. now that we are on 1.5 we can use java.util.concurrent.*, eg concurrenthashmap. one simple approach could be a double-barrel lru cache, using 2 maps (primary, secondary). you check the cache by first checking primary; if that's a miss, you check secondary and if you get a hit you promote it to primary. once primary is full you clear secondary and swap them. or... any other suggested approach?",
        "label": 33
    },
    {
        "text": "bugs in org apache lucene index termvectorsreader clone  a couple of things: the implementation can return null which is not allowed. it should throw a clonenotsupportedexception if that's the case. part of the code reads: termvectorsreader clone = null; try { clone = (termvectorsreader) super.clone(); } catch (clonenotsupportedexception e) {} clone.tvx = (indexinput) tvx.clone(); if a clonenotsupportedexception is caught then \"clone\" will be null and the assignment to clone.tvx will fail with a null pointer exception.",
        "label": 33
    },
    {
        "text": "changes to html  fixes and improvements the lucene hudson changes.html looks bad because changes2html.pl doesn't properly handle some new usages in changes.txt.",
        "label": 33
    },
    {
        "text": "remove explanation tohtml  this seems to be something of a relic. it's still used in solr, but i think it makes more sense to move it directly into the explainaugmenter there rather than having it in lucene itself.",
        "label": 2
    },
    {
        "text": "faster multisearcher search merge docs multisearcher.search places sorted search results from individual searchers into a priorityqueue. this can be made to be more optimal by taking advantage of the fact that the results returned are already sorted. the proposed solution places the sub-searcher results iterator into a custom priorityqueue that produces the sorted scoredocs.",
        "label": 33
    },
    {
        "text": "incorrect return value from simplenaivebayesclassifier assignclass the local copy of bytesref referenced by foundclass is affected by subsequent termsenum.iterator.next() calls as the shared bytesref.bytes changes. if a term \"test\" gives a good match and a next term in the terms collection is \"classification\" with a lower match score then the return result will be \"clas\"",
        "label": 1
    },
    {
        "text": "reduce transient ram usage while merging by using packed ints array for docid re mapping we allocate this int[] to remap docids due to compaction of deleted ones. this uses alot of ram for large segment merges, and can fail to allocate due to fragmentation on 32 bit jres. now that we have packed ints, a simple fix would be to use a packed int array... and maybe instead of storing abs docid in the mapping, we could store the number of del docs seen so far (so the remap would do a lookup then a subtract). this may add some cpu cost to merging but should bring down transient ram usage quite a bit.",
        "label": 1
    },
    {
        "text": "systemrequirements should say instead of the website still says java 1.4 but it should say 1.5",
        "label": 53
    },
    {
        "text": "abstract the distance calculation process in the spatial contrib the spatial contrib shouldn't tie users to one particular way of calculating distances. wikipedia lists multiple different formulas for the great-circle distance calculation, and there are alternatives to that as well. in a situation where many documents have the same points, it would be useful to be able to cache some calculated values as well (currently this is sort of handled in the filtering process itself). this issue addresses this by abstracting away the distance calculator, allowing the user to provide the implementation of choice. it would then be possible to swap in different distance calculation strategies without altering the distance filtering process itself.",
        "label": 7
    },
    {
        "text": "if the build fails to download jars for contrib db  just skip its tests every so often our nightly build fails because contrib/db is unable to download the necessary bdb jars from http://downloads.osafoundation.org. i think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure.",
        "label": 33
    },
    {
        "text": "explanation tohtml outputs invalid html if you want an html representation of an explanation, you might call the tohtml() method. however, the output of this method looks like the following: <ul> <li>some value = some description</li> <ul> <li>some nested value = some description</li> </ul> </ul> as it is illegal in html to nest a ul directly inside a ul, this method will always output unparseable html if there are nested explanations. what lucene probably means to output is the following, which is valid html: <ul> <li>some value = some description <ul> <li>some nested value = some description</li> </ul> </li> </ul>",
        "label": 18
    },
    {
        "text": "ban tests from writing to cwd currently each forked jvm has cwd = tempdir = . this provides some minimal protection against tests in different jvms from interfering with each other, but we can do much better by splitting these concerns: and setting cwd = . and tempdir = ./temp tests that write files to cwd can confuse ide users because they can create dirty checkouts or other issues between different runs, and of course can interfere with other tests in the same jvm (there are other possible ways to do this to). so a test like this should fail with securityexception, but currently does not. public void testbogus() throws exception {   file file = new file(\"foo.txt\");   fileoutputstream os = new fileoutputstream(file);   os.write(1);   os.close(); }",
        "label": 11
    },
    {
        "text": "make shingleanalyzerwrapper and perfieldanalyzerwrapper immutable both shingleanalyzerwrapper and perfieldanalyzerwrapper have setters which change some state which impacts their analysis stack. if these are going to become reusable, then the state must be immutable as changing it will have no effect. process will be similar to queryautostopwordanalyzer, i will remove in trunk and deprecate in 3x.",
        "label": 7
    },
    {
        "text": "tessellator should throw an error if all points were not processed currently, the tessellation in some situations when it has not successfully process all points in the polygon, it will still return an incomplete/wrong tessellation.  for example the following code: public void testinvalidpolygon()  throws exception {   string wkt = \"polygon((0 0, 1 1, 0 1, 1 0, 0 0))\";   polygon polygon = (polygon)simplewktshapeparser.parse(wkt);   expectthrows( illegalargumentexception.class, () -> {tessellator.tessellate(polygon); }); } will fail as the tessellator return a wrong tessellation containing one triangle.",
        "label": 19
    },
    {
        "text": "only the test runner should be able to system exit all others should get securityexception",
        "label": 53
    },
    {
        "text": "teststressindexing2 failure bad news: flonking just failed with build: builds.flonkings.com/job/lucene-trunk-linux-java6-64-test-only/11913/ 1 tests failed. regression:  org.apache.lucene.index.teststressindexing2.testmulticonfig error message: r1 is not empty but r2 is stack trace: java.lang.assertionerror: r1 is not empty but r2 is         at __randomizedtesting.seedinfo.seed([1a76d53a93a4acae:d7e4a3e23ef2b3e0]:0)         at org.junit.assert.fail(assert.java:93)         at org.apache.lucene.index.teststressindexing2.verifyequals(teststressindexing2.java:341)         at org.apache.lucene.index.teststressindexing2.verifyequals(teststressindexing2.java:278)         at org.apache.lucene.index.teststressindexing2.testmulticonfig(teststressindexing2.java:127)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1559)         at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:79)         at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:737)         at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:773)         at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:787)         at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)         at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:51)         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)         at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)         at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358)         at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:782)         at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:442)         at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:746)         at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:648)         at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:682)         at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:693)         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)         at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42)         at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)         at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)         at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)         at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:43)         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)         at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358)         at java.lang.thread.run(thread.java:662) build log: [...truncated 212 lines...] [junit4:junit4] suite: org.apache.lucene.index.teststressindexing2 [junit4:junit4]   2> note: reproduce with: ant test  -dtestcase=teststressindexing2 -dtests.method=testmulticonfig -dtests.seed=1a76d53a93a4acae -dtests.slow=true -dtests.locale=ja_jp -dtests.timezone=systemv/pst8 -dtests.file.encoding=us-ascii [junit4:junit4] failure 1.08s j5 | teststressindexing2.testmulticonfig <<< [junit4:junit4]    > throwable #1: java.lang.assertionerror: r1 is not empty but r2 is [junit4:junit4]    >    at __randomizedtesting.seedinfo.seed([1a76d53a93a4acae:d7e4a3e23ef2b3e0]:0) [junit4:junit4]    >    at org.junit.assert.fail(assert.java:93) [junit4:junit4]    >    at org.apache.lucene.index.teststressindexing2.verifyequals(teststressindexing2.java:341) [junit4:junit4]    >    at org.apache.lucene.index.teststressindexing2.verifyequals(teststressindexing2.java:278) [junit4:junit4]    >    at org.apache.lucene.index.teststressindexing2.testmulticonfig(teststressindexing2.java:127) [junit4:junit4]    >    at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit4:junit4]    >    at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [junit4:junit4]    >    at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [junit4:junit4]    >    at java.lang.reflect.method.invoke(method.java:597) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1559) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:79) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:737) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:773) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:787) [junit4:junit4]    >    at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50) [junit4:junit4]    >    at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:51) [junit4:junit4]    >    at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55) [junit4:junit4]    >    at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48) [junit4:junit4]    >    at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70) [junit4:junit4]    >    at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:782) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:442) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:746) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:648) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:682) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:693) [junit4:junit4]    >    at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45) [junit4:junit4]    >    at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]    >    at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:43) [junit4:junit4]    >    at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48) [junit4:junit4]    >    at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70) [junit4:junit4]    >    at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]    >    at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358) [junit4:junit4]    >    at java.lang.thread.run(thread.java:662) [junit4:junit4]   2> note: test params are: codec=lucene41: {f57=lucene41(blocksize=128), f8=postingsformat(name=mockrandom), f10=postingsformat(name=nestedpulsing), f23=postingsformat(name=memory dopackfst= true), f22=postingsformat(name=mockrandom), f77=postingsformat(name=mockrandom), f25=postingsformat(name=nestedpulsing), f87=postingsformat(name=nestedpulsing), f62=postingsformat(name=mockrandom), f89=postingsformat(name=memory dopackfst= true), f63=postingsformat(name=memory dopackfst= true), f82=lucene41(blocksize=128), f19=postingsformat(name=mockrandom), f65=postingsformat(name=nestedpulsing), f56=postingsformat(name=memory dopackfst= true), f85=postingsformat(name=memory dopackfst= true), f54=postingsformat(name=nestedpulsing), f17=lucene41(blocksize=128), f81=postingsformat(name=memory dopackfst= true), id=postingsformat(name=memory dopackfst= true), f80=postingsformat(name=mockrandom), f91=postingsformat(name=mockrandom), f95=postingsformat(name=mockrandom), f5=postingsformat(name=memory dopackfst= true)}, sim=randomsimilarityprovider(querynorm=true,coord=crazy): {f57=dfr i(n)b2, f8=dfr g2, f10=dfr i(f)b2, f23=dfr i(n)l2, f22=dfr i(ne)l2, f77=ib spl-l1, f25=ib ll-l2, f87=dfr i(f)z(0.3), f62=ib spl-dz(0.3), f63=dfr g1, f89=dfr g1, f82=dfr i(n)1, f19=dfr i(n)lz(0.3), f56=dfr gbz(0.3), f17=dfr i(n)2, f54=dfr i(f)b1, f80=dfr i(f)l1, f91=dfr i(n)3(800.0), f95=dfr i(n)3(800.0), f5=dfr i(n)b1}, locale=ja_jp, timezone=systemv/pst8 [junit4:junit4]   2> note: linux 3.2.0-26-generic amd64/sun microsystems inc. 1.6.0_33 (64-bit)/cpus=8,threads=1,free=226266176,total=247922688 [junit4:junit4]   2> note: all tests run in this jvm: [testsubscorerfreqs, testtermscorer, testlucene40postingsformat, testwindowsmmap, testversioncomparator, testcompoundfile, testpayloads, testtermsenum2, testpagedbytes, teststressindexing2] [junit4:junit4] completed on j5 in 2.81s, 3 tests, 1 failure <<< failures! good news: the seed reproduces for me 3 out 4 runs ant test-core  -dtestcase=teststressindexing2 -dtests.method=testmulticonfig -dtests.seed=1a76d53a93a4acae -dtests.slow=true -dtests.locale=ja_jp -dtests.timezone=systemv/pst8 -dtests.file.encoding=us-ascii",
        "label": 33
    },
    {
        "text": "index corruption autocommit false in both lucene 2.3 and trunk, the index becomes corrupted when autocommit=false",
        "label": 33
    },
    {
        "text": "crazy checkout paths break testxpathentityprocessor same as a bug i raised for javadoc generation, my build.xml is the same as upstream, the problem is my checkout path looks like this /home/buildserver/workspace/builds/ {search-engineering}solr-lucene{trunk} this means that the prepare-webpages target gets its paths in the buildpaths variable as a pipe separated list like so /home/buildserver/workspace/builds/{search-engineering} solr-lucene {trunk}/lucene/analysis/common/build.xml|/home/buildserver/workspace/builds/{search-engineering}solr-lucene{trunk} /lucene/analysis/icu/build.xml|...(and so on) attached is a patch that makes testxpathentityprocessor use a url rather than the filesystem path that makes xpath / xml happier with crazy path names",
        "label": 16
    },
    {
        "text": "mockrandomcodec loads termsindex even if termsindexdivisor is set to  when working on lucene-2891 (on trunk), i found out that if mockrandomcodec is used, then setting iwc.readertermsindexdivisor to -1 allows seeking e.g., termdocs, when it shouldn't. other codecs fail to seek, as expected by the test. we need to find out why mockrandomcodec does not fail as expected. to verify that, run \"ant test-core -dtestcase=testindexwriterreader -dtestmethod=testnotermsindex -dtests.codec=mockrandom\", but comment out the line which adds mockrandom to the list of illegal codecs in the test.",
        "label": 43
    },
    {
        "text": "iw addindexes dir  causes silent index corruption my old facet index create by lucene version=4.2 use indexchecker ok. now i upgrade to lucene 4.6 and put some new records to index. then reopen index, some files in indexdir missing.... no .si files. i debug into it, new version format of segments.gen(segments_n) record bad segments info.",
        "label": 40
    },
    {
        "text": "move smartchineseanalyzer   resources to own contrib project smartchineseanalyzer depends on a large dictionary that causes the analyzer jar to grow up to 3mb. the dictionary is quite big compared to all the other resouces / class files contained in that jar. having a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space. moving smartchineseanalyzer to a separate project could also include a small refactoring as robert mentioned in lucene-1722 several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc. i set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0",
        "label": 46
    },
    {
        "text": "suggest module should not depend on expression module currently our suggest module depends on the expression module just because the documentexpressiondictionary provides some util ctor to pass in an expression directly. that is a lot of dependency for little value imo and pulls in lots of jars. documentexpressiondictionary should only take a valuesource instead.",
        "label": 46
    },
    {
        "text": "changes html formatting improvements some improvements to the changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task: simplified the simple stylesheet (removed monospace font specification) and made it the default. the fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but imho it's ugly). moved the monospace style from the simple stylesheet to a new stylesheet named \"fixed width\" fixed syntax errors in the fancy stylesheet, so that it displays as intended. added <span style=\"attrib\"> to change attributions. in the fancy and simple stylesheets, change attributions are colored dark green. now properly handling change attributions in changes.txt that have trailing periods. clicking on an anchor to expand its children now changes the document location to show the children. hovering over anchors now causes a tooltip to be displayed - either \"click to expand\" or \"click to collapse\" - the tooltip changes appropriately after a collapse or expansion.",
        "label": 12
    },
    {
        "text": "lucene can incorrectly set the position of tokens that start a field with positoninc  more info in lucene-1465",
        "label": 33
    },
    {
        "text": "highlighting analyzinginfixsuggester skips non highlighted key when setting 'dohighlight' to true at analyzinginfixsuggester.lookup(..), both the key and the highlightkey inside the returned lookupresult are set to the highlighted string. see at analyzinginfixsuggester.createresults, line 530: if (dohighlight) { object highlightkey = highlight(text, matchedtokens, prefixtoken); result = new lookupresult(highlightkey.tostring(), highlightkey, score, payload, contexts); } else { result = new lookupresult(text, score, payload, contexts); } as i understand, the key should'nt be highlighted in any case, only the highlightkey.",
        "label": 33
    },
    {
        "text": "add bwc checks to verify what is tested matches what versions we know about this is a follow up from lucene-5934. mike has already has something like this for the smoke tester, but here i am suggesting a test within the test (similar to other version tests we have which check things like deprecation status of old versions).",
        "label": 41
    },
    {
        "text": "exception strategy for analysis improved solr-5623 included some conversation about the dilemmas of exception management and reporting in the analysis chain. i've belatedly become educated about the infostream, and this situation is a job for it. the docinverterperfield can note exceptions in the analysis chain, log out to the infostream, and then rethrow them as before. no wrapping, no muss, no fuss. there are comments on this jira from a more complex prior idea that readers might want to ignore.",
        "label": 6
    },
    {
        "text": "remove deprecated query components remove the rest of the deprecated query components.",
        "label": 53
    },
    {
        "text": "add regexpquery to queryparser patch that adds regexpquery if you /enter an expression between slashes like this/ i didnt do the contrib ones but could add it there too if it seems like a good idea.",
        "label": 46
    },
    {
        "text": "multicollector throws npe when there is collectterminatedexception is thrown by a subcollector i am seeing this in our log: caused by: java.lang.nullpointerexception         at org.apache.lucene.search.multicollector$multileafcollector.setscorer(multicollector.java:156)         at org.apache.lucene.search.booleanscorer$1$1.setscorer(booleanscorer.java:50)         at org.apache.lucene.search.weight$defaultbulkscorer.score(weight.java:166)         at org.apache.lucene.search.booleanscorer$1.score(booleanscorer.java:59)         at org.apache.lucene.search.booleanscorer$bulkscoreranddoc.score(booleanscorer.java:90)         at org.apache.lucene.search.booleanscorer.scorewindowsinglescorer(booleanscorer.java:313)         at org.apache.lucene.search.booleanscorer.scorewindow(booleanscorer.java:336)         at org.apache.lucene.search.booleanscorer.score(booleanscorer.java:364)         at org.apache.lucene.search.bulkscorer.score(bulkscorer.java:39)         at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:821)         at org.apache.lucene.search.indexsearcher$5.call(indexsearcher.java:763)         at org.apache.lucene.search.indexsearcher$5.call(indexsearcher.java:760)         at java.util.concurrent.futuretask.run(futuretask.java:266) looks like multicollector.removecollector(i) is called on line 176, the loop: for (leafcollector c : collectors) {         c.setscorer(scorer); } in setscorer can still step on it, on line 155. i am however, unable to reproduce that with a unit test. i made a copy of this class and added a null check in setscorer() and the problem goes away.",
        "label": 1
    },
    {
        "text": "asciifoldingfilter  expose folding logic   small improvements to isolatin1accentfilter this patch adds a couple of non-ascii chars to isolatin1accentfilter (namely: left & right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. i know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet. it also enables easy access to the ascii folding technique use in asciifoldingfilter for potential re-use in non-lucene-related code.",
        "label": 40
    },
    {
        "text": "highlighting fails for multiphrasequery's with one clause this bug is the same issue as lucene-7231, just for multiphrasequery instead of phrasequery. the fix is the same as well. to reproduce, change the test that was committed for lucene-7231 to use a multiphrasequery. it results in the same error java.lang.illegalargumentexception: less than 2 subspans.size():1 i have a patch including a test against branch_5.x, it just needs to go through legal before i can post it.",
        "label": 10
    },
    {
        "text": "possible memory leak in storedfieldswriter storedfieldswriter creates a pool of perdoc instances this pool will grow but never be reclaimed by any mechanism furthermore, each perdoc instance contains a ramfile. this ramfile will also never be truncated (and will only ever grow) (as far as i can tell) when feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the ramfile but never reclaimed. eventually, each pooled perdoc could grow very large, even if large documents are rare. seems like there should be some attempt to reclaim memory from the perdoc[] instance pool (or otherwise limit the size of ramfiles that are cached) etc",
        "label": 33
    },
    {
        "text": "analysis back compat break old and new style token streams don't mix well.",
        "label": 53
    },
    {
        "text": "closeablethreadlocal does not work well with tomcat thread pooling we tracked down a large memory leak (effectively a leak anyway) caused by how analyzer users closeablethreadlocal. closeablethreadlocal.hardrefs holds references to thread objects as keys. the problem is that it only frees these references in the set() method, and snowballanalyzer will only call set() when it is used by a new thread. the problem scenario is as follows: the server experiences a spike in usage (say by robots or whatever) and many threads are created and referenced by closeablethreadlocal.hardrefs. the server quiesces and lets many of these threads expire normally. now we have a smaller, but adequate thread pool. so closeablethreadlocal.set() may not be called by snowballanalyzer (via analyzer) for a long time. the purge code is never called, and these threads along with their thread local storage (lucene related or not) is never cleaned up. i think calling the purge code in both get() and set() would have avoided this problem, but is potentially expensive. perhaps using weakhashmap instead of hashmap may also have helped. weakhashmap purges on get() and set(). so this might be an efficient way to clean up threads in get(), while set() might do the more expensive map.keyset() iteration. our current work around is to not share snowballanalyzer instances among http searcher threads. we open and close one on every request. thanks, matt",
        "label": 33
    },
    {
        "text": "manhattan distance function is incorrect  not absolute distance between coordinates the lucene vectordistance() function's manhattan distance function is incorrect. wikipedia says: http://en.wikipedia.org/wiki/manhattan_distance \"taxicab geometry, blahblahblah, is a form of geometry in which the usual distance function or metric of euclidean geometry is replaced by a new metric in which the distance between two points is the sum of the absolute differences of their coordinates.\" the lucene function isn't taking the absolute value before subtracting the vector coordinates. i don't have a patch, but the offending code is here: // lucene/contrib/spatial/src/java/org/apache/lucene/spatial/distanceutils.java     } else if (power == 1.0) {        for (int i = 0; i < vec1.length; i++) {          result += vec1[i] - vec2[i];      } it just needs to use math.abs() when subtracting the coordinates.",
        "label": 10
    },
    {
        "text": "simplify standardtokenizer jflex grammar summary of thread entitled \"fullwidth alphanumeric characters, plus a question on korean ranges\" begun by daniel noll on java-user, and carried over to java-dev: on 01/07/2008 at 5:06 pm, daniel noll wrote: > i wish the tokeniser could just use character.isletter and > character.isdigit instead of having to know all the ranges itself, since > the jre already has all this information. character.isletter does > return true for cjk characters though, so the ranges would still come in > handy for determining what kind of letter they are. i don't support > jflex has a way to do this... the digit macro could be replaced by jflex's predefined character class [:digit:], which has the same semantics as java.lang.character.isdigit(). although jflex's predefined character class [:letter:] (same semantics as java.lang.character.isletter()) includes cjk characters, there is a way to handle this using jflex's regex negation syntax !. from the jflex documentation: [t]he expression that matches everything of a not matched by b is !(!a|b) so to exclude cj characters from the letter macro:     letter = ! ( ! [:letter:] | {cj} ) since [:letter:] includes all of the korean ranges, there's no reason (afaict) to treat them separately; unlike chinese and japanese characters, which are individually tokenized, the korean characters should participate in the same token boundary rules as all of the other letters. i looked at some of the differences between unicode 3.0.0, which java 1.4.2 supports, and unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges. this stuff gets tweaked all the time, and i don't think lucene should be in the business of trying to track it, or take a position on which unicode version users' data should conform to. switching to using jflex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of jvm version, and this seems much more reasonable to me than the current status quo. i will attach a patch shortly.",
        "label": 33
    },
    {
        "text": "facetindexing searchparams house cleaning facetindexingparams lets you configure few things such as ordinalpolicy, pathpolicy and partitionsize. however, in order to set them you must extend defaultfacetindexingparams and override fixedxy(), as the respective getters are final. i'd like to do the following: turn facetindexingparams and facetsearchparams into concrete classes, rather than interfaces. the reason they are interfaces because one app once wants to have one class which implements both. since fsp holds fip, i don't think that's needed. add setters to facetindexingparams for the relevant configuration parameters, rather than forcing someone to extend the class. extensions should really be for special cases, which we haven't identified so far (except overriding these settings), hence why there's only defaultfip/fsp. add some javadocs... i'm sure, w/ my pedantic and perfectionist nature, that more thing will be done once i get to it .",
        "label": 43
    },
    {
        "text": "add sortingmergepolicy's sorter  if any  to segmentinfo tostring github pull request with proposed change to follow, it also extends testsortingmergepolicy to randomly choose regular or reverse sorting order.",
        "label": 33
    },
    {
        "text": "sorter api  make numericdocvaluessorter able to sort in reverse order today it is only able to sort in ascending order.",
        "label": 43
    },
    {
        "text": "sortertemplate quicksort stack overflows on broken comparators that produce only few disticnt values in large arrays looking at otis's sort problem on the mailing list, he said: * looked for other places where this call is made - found it in multiphrasequery$multiphraseweight and changed that call from arrayutil.quicksort to arrayutil.mergesort * now we no longer see sortertemplate.quicksort in deep recursion when we do a thread dump i thought this was interesting because postingsandfreq's comparator looks like it needs a tiebreaker. i think in our sorts we should add some asserts to try to catch some of these broken comparators.",
        "label": 53
    },
    {
        "text": "add support for type whitelist in typetokenfilter a usual use case for typetokenfilter is allowing only a set of token types. that is, listing allowed types, instead of filtered ones. i'm attaching a patch to add a usewhitelist option for that.",
        "label": 50
    },
    {
        "text": "remove multitermquery get inc clear totalnumberofterms this method is not correct if the index has more than one segment. its also not thread safe, and it means calling query.rewrite() modifies the original query. all of these things add up to confusion, i think we should remove this from multitermquery, the only thing that \"uses\" it is the nrq tests, which conditionalizes all the asserts anyway.",
        "label": 53
    },
    {
        "text": "build mr jar and use some java methods if available see background: http://openjdk.java.net/jeps/238 it would be nice to use some of the newer array methods and range checking methods in java 9 for example, without waiting for lucene 10 or something. if we build an mr-jar, we can start migrating our code to use java 9 methods right now, it will use optimized methods from java 9 when thats available, otherwise fall back to java 8 code. this patch adds: objects.checkindex(int,int) objects.checkfromtoindex(int,int,int) objects.checkfromindexsize(int,int,int) arrays.mismatch(byte[],int,int,byte[],int,int) arrays.compareunsigned(byte[],int,int,byte[],int,int) arrays.equal(byte[],int,int,byte[],int,int) // did not add char/int/long/short/etc but of course its possible if needed it sets these up in org.apache.lucene.future as 1-1 mappings to java methods. this way, we can simply directly replace call sites with java 9 methods when java 9 is a minimum. simple 1-1 mappings mean also that we only have to worry about testing that our java 8 fallback methods work. i found that many of the current byte array methods today are willy-nilly and very lenient for example, passing invalid offsets at times and relying on compare methods not throwing exceptions, etc. i fixed all the instances in core/codecs but have not looked at the problems with analyzingsuggester. also simpletext still uses a silly method in arrayutil in similar crazy way, have not removed that one yet.",
        "label": 53
    },
    {
        "text": "mappingcharfilter rarely has wrong correctoffset  for finaloffset  found this bug over on lucene-3969, but i'm currently tracking a ton of bugs, so i figure i would open an issue and see if this one is obvious to anyone: consider this input string: \"gzw f quaxot\" (length = 12) with a whitespacetokenizer. if i have mapping rules like this, then it works!: \"t\" => \"\" but if i have mapping rules like this: \"t\" => \"\" \"tmakdbl\" => \"c\" then it will compute final offset wrong:     [junit] junit.framework.assertionfailederror: finaloffset  expected:<12> but was:<11> looks like some logic/recursion bug in the correctoffset method? the second rule is not even \"used\" for this string, it just happens to also start with 't'",
        "label": 11
    },
    {
        "text": "remove solr hack in morfologikfilter if solr wants to set the contextclassloader because its classloading is fucked up, then it needs to do this hack itself: it should not be in lucene code. the current mess prevents use of this analyzer in other environments",
        "label": 53
    },
    {
        "text": "performance improvement  lazy skipping on proximity file hello, i'm proposing a patch here that changes org.apache.lucene.index.segmenttermpositions to avoid unnecessary skips and reads on the proximity stream. currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document. but this is only necessary if actual positions have to be retrieved for that particular document. consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. but only if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. a move to the posting list of a document can be quite expensive. it has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the vints of all positions of those documents have to be read and decoded. an improvement is to move the prox pointer lazily to a document only if nextposition() is called. this will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists). my patch implements this lazy skipping. all unit tests pass. i also attach a new unit test that works as follows: using a ramdirectory an index is created and test docs are added. then the index is optimized to make sure it only has a single segment. this is important, because indexreader.open() returns an instance of segmentreader if there is only one segment in the index. the proxstream instance of segmentreader is package protected, so it is possible to set proxstream to a different object. i am using a class called seekscountingstream that extends indexinput in a way that it is able to count the number of invocations of seek(). then the testcase searches the index using a phrasequery \"term1 term2\". it is known how many documents match that query and the testcase can verify that seek() on the proxstream is not called more often than number of search hits. example: number of docs in the index: 500 number of docs that match the query \"term1 term2\": 5 invocations of seek on prox stream (old code): 29 invocations of seek on prox stream (patched version): 5 michael",
        "label": 55
    },
    {
        "text": "numericfields   slowcompositereaderwrapper   uninvertedreader    dtests codec random can results in incorrect sortedsetdocvalues digging into solr-7631 and solr-7605 i became fairly confident that the only explanation of the behavior i was seeing was some sort of bug in either the randomized codec/postings-format or the uninvertedreader, that was only evident when two were combined and used on a multivalued numeric field using precision steps. but since i couldn't find any -dtests.codec or -dtests.postings.format options that would cause the bug 100% regardless of seed, i switched tactices and focused on reproducing the problem using uninvertedreader directly and checking the sortedsetdocvalues.getvaluecount(). i now have a test that fails frequently (and consistently for any seed i find), but only with -dtests.codec=random \u2013 override it with -dtests.codec=default and everything works fine (based on the exhaustive testing i did in the linked issues, i suspect every named codec works fine - but i didn't re-do that testing here) the failures only seem to happen when checking the sortedsetdocvalues.getvaluecount() of a slowcompositereaderwrapper around the uninvertedreader \u2013 which suggests the root bug may actually be in slowcompositereaderwrapper? (but still has some dependency on the random codec)",
        "label": 18
    },
    {
        "text": "field selection and lazy field loading the patch to come shortly implements a field selection and lazy loading mechanism for document loading on the indexreader. it introduces a fieldselector interface that defines the accept method: fieldselectorresult accept(string fieldname); (perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.)) anyone can implement a fieldselector to define how they want to load fields for a document. the fieldselectorresult can be one of four values: load, lazy_load, no_load, load_and_break. the fieldsreader, as it is looping over the fieldsinfo, applies the fieldselector to determine what should be done with the current field. i modeled this after the java.io.filefilter mechanism. there are two implementations to date: setbasedfieldselector and loadfirstfieldselector. the former takes in two sets of field names, one to load immed. and one to load lazily. the latter returns load_and_break on the first field encountered. see testfieldsreader for examples. it should support utf-8 (i borrowed code from issue 509, thanks!). see testfieldsreader for examples i added an expert method on indexinput named skipchars that takes in the number of characters to skip. this is a compromise on changing the file format of the fields to better support seeking. it does some of the work of readchars, but not all of it. it doesn't require buffer storage and it doesn't do the bitwise operations. it just reads in the appropriate number of bytes and promptly ignores them. this is useful for skipping non-binary, non-compressed stored fields. the biggest change is by far the introduction of the fieldable interface (as per doug's suggestion from a mailing list email on lazy field loading from a while ago). field is now a fieldable. all uses of field have been changed to use fieldable. fieldsreader.lazyfield also implements fieldable. lazy field loading is now implemented. it has a major caveat (that is documented) in that it clones the underlying indexinput upon lazy access to the field value. it is undefined whether a lazy field can be loaded after the indexinput parent has been closed (although, from what i saw, it does work). i thought about adding a reattach method, but it seems just as easy to reload the document. see the testfieldsreader and dochelper for examples. i updated a couple of other tests to reflect the new fields that are on the dochelper document. all tests pass.",
        "label": 15
    },
    {
        "text": "explain of map  function does not show default value the explain output from the map(x,min,max,target,default) function does not print default.",
        "label": 21
    },
    {
        "text": "make the payload boosting queries consistent boostingfunctiontermquery should be consistent with boostingnearquery - renaming to payloadnearquery and payloadtermquery",
        "label": 29
    },
    {
        "text": "search with filter does not work  see attached junittest, self-explanatory",
        "label": 32
    },
    {
        "text": "invert the codec postings api currently fieldsconsumer/postingsconsumer/etc is a \"push\" oriented api, e.g. freqproxtermswriter streams the postings at flush, and the default merge() takes the incoming codec api and filters out deleted docs and \"pushes\" via same api (but that can be overridden). it could be cleaner if we allowed for a \"pull\" model instead (like docvalues). for example, maybe freqproxtermswriter could expose a terms of itself and just passed this to the codec consumer. this would give the codec more flexibility to e.g. do multiple passes if it wanted to do things like encode high-frequency terms more efficiently with a bitset-like encoding or other things... a codec can try to do things like this to some extent today, but its very difficult (look at buffering in pulsing). we made this change with dv and it made a lot of interesting optimizations easy to implement...",
        "label": 33
    },
    {
        "text": "move span payloads to sandbox as mentioned on lucene-6371: i've marked the new classes and methods as lucene.experimental, rather than moving to the sandbox - if anyone feels strongly about that, maybe it could be done in a follow up issue. i feel strongly about this and will do the move.",
        "label": 2
    },
    {
        "text": "sort and sortfield does not have equals  and hashcode  during developing for my project panfmp i had the following issue: i have a cache for queries (like solr has, too) for query results. this cache also uses the sort/sortfield as key into the cache. the problem is, because sort/sortfield does not implement equals() and hashcode(), you cannot store them as cache keys. to workaround, currently i use sort.tostring() as cache key, but this is not so nice. in corelation with issue lucene-1478, i could fix this there in one patch together with the other improvements.",
        "label": 33
    },
    {
        "text": "incorrect snippet returned with spanscorer this problem was reported by my customer. they are using solr 1.3 and uni-gram, but it can be reproduced with lucene 2.9 and whitespaceanalyzer. query (f1:\"a b c d\" or f2:\"a b c d\") and (f1:\"b c g\" or f2:\"b c g\") the snippet we expected is: x y z <b>a</b> <b>b</b> <b>c</b> <b>d</b> e f g <b>b</b> <b>c</b> <b>g</b> but we got: x y z <b>a</b> b c <b>d</b> e f g <b>b</b> <b>c</b> <b>g</b> program to reproduce the problem: public class testhighlighter {   static final string content = \"x y z a b c d e f g b c g\";   static final string ph1 = \"\\\"a b c d\\\"\";   static final string ph2 = \"\\\"b c g\\\"\";   static final string f1 = \"f1\";   static final string f2 = \"f2\";   static final string f1c = f1 + \":\";   static final string f2c = f2 + \":\";   static final string query_string =     \"(\" + f1c + ph1 + \" or \" + f2c + ph1 + \") and (\"     + f1c + ph2 + \" or \" + f2c + ph2 + \")\";   static analyzer analyzer = new whitespaceanalyzer();      public static void main(string[] args) throws exception {     queryparser qp = new queryparser( f1, analyzer );     query query = qp.parse( query_string );     cachingtokenfilter stream = new cachingtokenfilter( analyzer.tokenstream( f1, new stringreader( content ) ) );     scorer scorer = new spanscorer( query, f1, stream, false );     highlighter h = new highlighter( scorer );     system.out.println( \"query : \" + query_string );     system.out.println( h.getbestfragment( analyzer, f1,  content ) );   } }",
        "label": 29
    },
    {
        "text": "add  deprecated annotations as discussed on lucene-2084, i think we should be consistent about use of @deprecated annotations if we are to use it. this patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch). so if someone wants to take it, have fun, otherwise in a week or so i will commit it if nobody objects.",
        "label": 40
    },
    {
        "text": "arrayindexoutofboundexception in tochildblockjoinquery when there's a deleted parent without any children this problem is found in lucene 4.2.0 and reproduced in 4.7.0 in our app when we delete a document we always delete all the children. but not all parents have children. the exception happen for us when the parent without children is deleted.",
        "label": 33
    },
    {
        "text": "queryutils should check that equals properly handles null its part of the equals contract, but many classes currently violate",
        "label": 40
    },
    {
        "text": "phrase query with term repeated times requires more slop than expected consider a document with the text \"a a a\". the phrase query \"a a a\" (exact match) succeeds. the query \"a a a\"~1 (same document and query, just increasing the slop value by one) fails. \"a a a\"~2 succeeds again. if the exact match succeeds, i wouldn't expect the same query but with more slop to fail. the fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent. i will attach a file that contains a set of junit tests that demonstrate what i mean.",
        "label": 12
    },
    {
        "text": "avoid building jar files unless necessary in build this causes the build to be slow, we can avoid it in lots of cases (e.g. ant test)",
        "label": 47
    },
    {
        "text": "remove all accessclassinpackage permissions   remove uses accessibleobject setaccessible  to make ready for java jigsaw with jigsaw builds this stuff is not allowed, its no longer an option of security manager or not. so we should remove these permissions and fix the test leaks, crappy code, remove hacks, etc. if the hack is really somehow needed for some special reason (e.g. well known case like mmap), it needs to gracefully handle not being able to do this, the code and tests should still \"work\" if it cannot do the hack. otherwise there will be problems for java 9.",
        "label": 53
    },
    {
        "text": "rename contrib queryparser project to queryparser contrib much like with contrib/queries, we should differentiate the contrib/queryparser from the queryparser module. no directory structure changes will be made, just ant and maven.",
        "label": 7
    },
    {
        "text": "code cleanup from all sorts of  trivial  warnings i would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. these are all very trivial and should not pose any problem. i'll create another issue for getting rid of deprecated code usage, like lucenetestcase and all sorts of deprecated constructors. that's also trivial because it only affects lucene code, but it's a different type of change. another issue i'd like to create is about introducing more generics in the code, where it's missing today - not changing existing api. there are many places in the code like that. so, with you permission, i'll start with the trivial ones first, and then move on to the others.",
        "label": 53
    },
    {
        "text": "constants causing nullpointerexception when fetching metadata  implementation version  in manifest if the manifest.mf file does not contain the metadata implementation_version, a null value is returned, causing nullpointerexception during commit: exception in thread \"main\" java.lang.nullpointerexception at org.apache.lucene.store.indexoutput.writestring(indexoutput.java:109) at org.apache.lucene.store.indexoutput.writestringstringmap(indexoutput.java:229) at org.apache.lucene.index.segmentinfo.write(segmentinfo.java:558) at org.apache.lucene.index.segmentinfos.write(segmentinfos.java:337) at org.apache.lucene.index.segmentinfos.preparecommit(segmentinfos.java:808) at org.apache.lucene.index.indexwriter.startcommit(indexwriter.java:5319) at org.apache.lucene.index.indexwriter.preparecommit(indexwriter.java:3895) at org.apache.lucene.index.indexwriter.commit(indexwriter.java:3956) this happened after having build a jar assembly using maven, the original manifest.mf of lucene jar has been overwritten, and didn't contain anynore the implementation version metadata. path attached.",
        "label": 33
    },
    {
        "text": "hunspelldictionary   affixfile reader closed  dictionary readers left unclosed the hunspelldictionary takes an inputstream for affix file and a list of streams for dictionaries. javadoc is not clear about i have to close those stream myself or the dictionary constructor does this already. looking at the code, at least reader.close() is called when the affix file is read via readaffixfile() method (although closing streams is not done in a finally block - so the constructor may fail to do so). the readdictionaryfile() method does miss the call to close the reader in contrast to readaffixfile(). so the question here is - have i have to close the streams myself after instantiating the dictionary? or is the close call only missing for the dictionary streams? either way, please add the close calls in a safe manner or clarify javadoc so i have to do this myself.",
        "label": 7
    },
    {
        "text": "add example settings to korean analyzer components' javadocs korean analyzer (nori) javadoc needs example schema settings. i'll create a patch.",
        "label": 53
    },
    {
        "text": "geo3dpoint implementation should pay attention to the ordering of lat lon points the latlonpoint api implementation pays attention to the order of the points; \"clockwise\" means one side of the polygon boundary, and \"counterclockwise\" means the complement. we need to use that in geo3dpoint and convert into whatever the underlying geopolygonfactory method requires.",
        "label": 25
    },
    {
        "text": "update uax29urlemailtokenizer tlds to latest list  and upgrade all jflex based tokenizers to support unicode we did this once before in lucene-5357, but it might be time to update the list of tlds again. comparing our old list with a new list indicates 800+ new domains, so it would be nice to include them. also the jflex tokenizer grammars should be upgraded to support unicode 8.0.",
        "label": 47
    },
    {
        "text": "queryparser parses query differently depending on the default operator as explained by pawe\u0142 r\u00f3g on java-user [1], the output of parsing the queries below is different depending on the default operator. this looks odd and should be investigated. queryparser parser = new queryparser(\"test\", new whitespaceanalyzer());     parser.setdefaultoperator(queryparser.operator.and);     query query = parser.parse(\"foo and bar or baz \");     system.out.println(query.tostring());     parser.setdefaultoperator(queryparser.operator.or);     query = parser.parse(\"foo and bar or baz \");     system.out.println(query.tostring()); results in : +test:foo test:bar test:baz +test:foo +test:bar test:baz [1] http://mail-archives.apache.org/mod_mbox/lucene-java-user/201611.mbox/browser",
        "label": 11
    },
    {
        "text": "straightbytesdocvaluesfield fails if bytes   32k i didn't observe any limitations on the size of a bytes based docvalues field value in the docs. it appears that the limit is 32k, although i didn't get any friendly error telling me that was the limit. 32k is kind of small imo; i suspect this limit is unintended and as such is a bug. the following test fails:   public void testbigdocvalue() throws ioexception {     directory dir = newdirectory();     indexwriter writer = new indexwriter(dir, writerconfig(false));     document doc = new document();     bytesref bytes = new bytesref((4+4)*4097);//4096 works     bytes.length = bytes.bytes.length;//byte data doesn't matter     doc.add(new straightbytesdocvaluesfield(\"dvfield\", bytes));     writer.adddocument(doc);     writer.commit();     writer.close();     directoryreader reader = directoryreader.open(dir);     docvalues docvalues = multidocvalues.getdocvalues(reader, \"dvfield\");     //fails if bytes is big!     docvalues.getsource().getbytes(0, bytes);     reader.close();     dir.close();   }",
        "label": 33
    },
    {
        "text": "remove support for pre indexes we should remove support for 2.x (and 1.9) indexes in 4.0. it seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. this issue should cover: remove the .zip indexes remove the unnecessary code from segmentinfo and segmentinfos. mike suggests we compare the version headers at the top of segmentinfos, in 2.9.x vs 3.0.x, to see which ones can go. remove format_pre from fieldinfos remove old format from termvectorsreader if you know of other places where code can be removed, then please post a comment here. i don't know when i'll have time to handle it, definitely not in the next few days. so if someone wants to take a stab at it, be my guest.",
        "label": 43
    },
    {
        "text": "query is always null in countdocswithclass  of simplenaivebayesclassifier ",
        "label": 26
    },
    {
        "text": "add ability to specify compilation matching flags to regexcapabiltiies implementations the jakarta regexp and java util regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. while the java.util.regex.pattern implementation supports providing these flags as part of the regular expression string, the jakarta regexp implementation does not. therefore, this improvement request is to add the capability to provide those modification flags to either implementation. i've developed a working implementation that makes minor additions to the existing code. the default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided. this provides complete backwards compatibility. for each regexcapabilties implementation, the appropriate flags from the regular expression package is defined as flags_xxx static fields. these are pass through to the underlying implementation. they are re-defined to avoid bleeding the actual implementation classes into the caller namespace. proposed changes: for the javautilregexcapabilities.java, the following is the changes made. private int flags = 0; // define the optional flags from pattern that can be used. // do this here to keep pattern contained within this class. public final int flag_canon_eq = pattern.canon_eq; public final int flag_case_insensative = pattern.case_insensative; public final int flag_comments = pattern.comments; public final int flag_dotall = pattern.dotall; public final int flag_literal = pattern.literal; public final int flag_multiline = pattern.multiline; public final int flag_unicode_case = pattern.unicode_case; public final int flag_unix_lines = pattern.unix_lines; /** default constructor that uses java.util.regex.pattern with its default flags. */ public javautilregexcapabilities() { this.flags = 0; } /** constructor that allows for the modification of the flags that the java.util.regex.pattern will use to compile the regular expression. this gives the user the ability to fine-tune how the regular expression to match the functionlity that they need. the {@link java.util.regex.pattern pattern} class supports specifying these fields via the regular expression text itself, but this gives the caller another option to modify the behavior. useful in cases where the regular expression text cannot be modified, or if doing so is undesired. @flags the flags that are ored together. */ public javautilregexcapabilities(int flags) { this.flags = flags; } public void compile(string pattern) { this.pattern = pattern.compile(pattern, this.flags); } for the jakartaregexpcapabilties.java, the following is changed: private int flags = re.match_normal; /** * flag to specify normal, case-sensitive matching behaviour. this is the default. */ public static final int flag_match_normal = re.match_normal; /** * flag to specify that matching should be case-independent (folded) */ public static final int flag_match_caseindependent = re.match_caseindependent; /** * contructs a regexcapabilities with the default match_normal match style. */ public jakartaregexpcapabilities() {} /** * constructs a regexcapabilities with the provided match flags. * multiple flags should be ored together. * * @param flags the matching style */ public jakartaregexpcapabilities(int flags) { this.flags = flags; } public void compile(string pattern) { regexp = new re(pattern, this.flags); }",
        "label": 33
    },
    {
        "text": "stopfilterfactory docs do not advertise explain hte  format  option stopfilterfactory supports a \"format\" option for controlling wether \"getwordset\" or \"getsnowballwordset\" is used to parse the file, but this option is not advertised and people can be confused by looking at the example stopword files include in the releases (some of which are in the snoball format w/ \"|\" comments) and try to use them w/o explicitly specifying format=\"snowball\" and silently get useless stopwords (that include the \"| comments\" as literal portions of hte stopwrds. we need to better document the use of \"format\" and consider updating all of the example stopword files we ship that are in the snowball format with a note about the need to use format=\"snowball\" with those files. initial bug report the stopfilterfactory builds a chararrayset directly from the raw lines of the supplied words file. this causes a problem when using the stop word files supplied with the solr/lucene distribution. in particular, the comments in those files get added to the chararrayset. a line like this... ceci | this should result in the string \"ceci\" being added to the chararrayset, but \"ceci | this\" is what actually gets added. workaround: remove all comments from stop word files you are using. suggested fix: the stopfilterfactory should strip any comments, then strip trailing whitespace. the stop word files supplied with the distribution should be edited to conform to the supported comment format.",
        "label": 18
    },
    {
        "text": "add backcompat support for standardtokenizer before the merge from trunk for branch_5x remove the backcompat support in standardtokenizer for previous unicode versions. with 5x, we only need to add support back for 4.0-4.6 (unicode 6.1).",
        "label": 41
    },
    {
        "text": "add benchmark task for fastvectorhighlighter ",
        "label": 33
    },
    {
        "text": "querynodeimpl removefromparent does a lot of work without any effect the method removefromparent of querynodeimpl, calls getchildren on the parent and removes any occurrence of \"this\" from the result. however, since a few releases, getchildren returns a copy of the children list, so the code has no effect (except creating a copy of the children list which will then be thrown away). even worse, since setchildren calls removefromparent on any previous child, setchildren now has a complexity of o(n^2) and creates a lot of throw-away copies of the children list (for nodes with a lot of children) public void removefromparent() {     if (this.parent != null) {       list<querynode> parentchildren = this.parent.getchildren();       iterator<querynode> it = parentchildren.iterator();              while (it.hasnext()) {         if (it.next() == this) {           it.remove();         }       }              this.parent = null;     }   }",
        "label": 33
    },
    {
        "text": "fix buggy stemmers and remove duplicate analysis functionality would like to remove stemmers in the following packages, and instead in their analyzers use a snowballstemfilter instead. analyzers/fr analyzers/nl analyzers/ru below are excerpts from this code where they proudly proclaim they use the snowball algorithm. i think we should delete all of this custom stemming code in favor of the actual snowball package. /**  * a stemmer for french words.   * <p>  * the algorithm is based on the work of  * dr martin porter on his snowball project<br>  * refer to http://snowball.sourceforge.net/french/stemmer.html<br>  * (french stemming algorithm) for details  * </p>  */ public class frenchstemmer { /**  * a stemmer for dutch words.   * <p>  * the algorithm is an implementation of  * the <a href=\"http://snowball.tartarus.org/algorithms/dutch/stemmer.html\">dutch stemming</a>  * algorithm in martin porter's snowball project.  * </p>  */ public class dutchstemmer { /**  * russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).  */ class russianstemmer",
        "label": 40
    },
    {
        "text": "masking field of span for cross searching across multiple fields  many to one style  this issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. below is my post on java-dev on the topic, which details the changes we need: \u2014 we have an interesting situation where we are effectively indexing two 'entities' in our system, which share a one-to-many relationship (imagine 'user' and 'delivery address' for demonstration purposes). at the moment, we index one lucene document per 'many' end, duplicating the 'one' end data, like so: userid: 1 userfirstname: fred addresscountry: au addressphone: 1234 userid: 1 userfirstname: fred addresscountry: nz addressphone: 5678 userid: 2 userfirstname: mary addresscountry: au addressphone: 5678 (note: 2 documents indexed for user 1). this is somewhat annoying for us, because when we search in lucene the results we want back (conceptually) are at the 'user' level, so we have to collapse the results by distinct user id, etc. etc (let alone that it blows out the size of our index enormously). so why do we do it? it would make more sense to use multiple fields: userid: 1 userfirstname: fred addresscountry: au addressphone: 1234 addresscountry: nz addressphone: 5678 userid: 2 userfirstname: mary addresscountry: au addressphone: 5678 but imagine the search \"+addresscountry:au +addressphone:5678\". we'd like this to match only mary, but of course it matches fred also because he matches both those terms (just for different addresses). there are two aspects to the approach we've (more or less) got working but i'd like to run them past the group and see if they're worth trying to get them into lucene proper (if so, i'll create a jira issue for them) 1) use a modified spannearquery. if we assume that country + phone will always be one token, we can rely on the fact that the positions of 'au' and '5678' in fred's document will be different. spanquery q1 = new spantermquery(new term(\"addresscountry\", \"au\")); spanquery q2 = new spantermquery(new term(\"addressphone\", \"5678\")); spanquery snq = new spannearquery(new spanquery[] {q1, q2} , 0, false); the slop of 0 means that we'll only return those where the two terms are in the same position in their respective fields. this works brilliantly, but requires a change to spannearquery's constructor (which checks that all the clauses are against the same field). are people amenable to perhaps adding another constructor to snq which doesn't do the check, or subclassing it to do the same (give it a protected non-checking constructor for the subclass to call)? 2) (snipped ... see lucene-1626 for second idea)",
        "label": 18
    },
    {
        "text": "be consistent about negative vint vlong today, write/readvint \"allows\" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes). however, read/writevlong fails (trips an assert). i'd prefer that both vint/vlong trip an assert if you ever try to write a negative number... it's badly trappy today. but, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that. so, if we can't add that assert in today, i think we should at least fix readvlong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!).",
        "label": 53
    },
    {
        "text": "broken compareto in mutablevaluedouble and mutablevaluebool   affects grouping when exists false on the solr-user mailing list, ebisawa & alex both commented that they've noticed bugs in the grouping code when some documents don't have values in the grouping field. in ebisawa's case, he tracked this down to what appears to be some bugs in the logic of the \"comparesametype\" method of some of the mutablevalue implementations. thread: https://mail-archives.apache.org/mod_mbox/lucene-solr-user/201406.mbox/%3c84f86fce4b8f42268048aecfb2806a8c@sixpr04mb045.apcprd04.prod.outlook.com%3e",
        "label": 18
    },
    {
        "text": "weightedspantermextractor should not always call extractunknownquery weightedspantermextractor always calls extractunknownquery, even if term extraction already succeeded because the query is eg. a phrase query. it should only call this method if it could not find how to extract terms otherwise (eg. in case of a custom query).",
        "label": 1
    },
    {
        "text": "add asciifoldingfilter and deprecate isolatin1accentfilter the isolatin1accentfilter is removing accents from accented characters in the iso latin 1 character set. it does what it does and there is no bug with it. it would be nicer, though, if there was a more comprehensive version of this code that included not just iso-latin-1 (iso-8859-1) but the entire latin 1 and latin extended a unicode blocks. see: http://en.wikipedia.org/wiki/latin-1_supplement_unicode_block see: http://en.wikipedia.org/wiki/latin_extended-a_unicode_block that way, all languages using roman characters are covered. a new class, isolatinaccentfilter is attached. it is intended to supercede isolatin1accentfilter which should get deprecated.",
        "label": 29
    },
    {
        "text": "testindexwriterdelete makes broken segments with payloads on this could just be a simpletext problem.... but just in case grant added payloads to mockanalyzer in lucene-2692 i wondered what would happen if i turned on his payload filter by default for all tests.     [junit] testsuite: org.apache.lucene.index.testindexwriterdelete     [junit] testcase: testdeletesondiskfull(org.apache.lucene.index.testindexwriterdelete):     caused an error     [junit] checkindex failed     [junit] java.lang.runtimeexception: checkindex failed     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:80)     [junit]     at org.apache.lucene.index.testindexwriterdelete.dotestoperationsondiskfull(testindexwriterdelete.java:5 38)     [junit]     at org.apache.lucene.index.testindexwriterdelete.testdeletesondiskfull(testindexwriterdelete.java:401)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]     [junit]     [junit] testcase: testupdatesondiskfull(org.apache.lucene.index.testindexwriterdelete):     caused an error     [junit] checkindex failed     [junit] java.lang.runtimeexception: checkindex failed     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:80)     [junit]     at org.apache.lucene.index.testindexwriterdelete.dotestoperationsondiskfull(testindexwriterdelete.java:5 38)     [junit]     at org.apache.lucene.index.testindexwriterdelete.testupdatesondiskfull(testindexwriterdelete.java:405)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]     [junit]     [junit] tests run: 14, failures: 0, errors: 2, time elapsed: 0.322 sec     [junit]     [junit] ------------- standard output ---------------     [junit] checkindex failed     [junit] segments file=segments_2 numsegments=1 version=format_4_0 [lucene 4.0]     [junit]   1 of 1: name=_0 doccount=157     [junit]     codec=mockfixedintblock     [junit]     compound=true     [junit]     hasprox=true     [junit]     numfiles=2     [junit]     size (mb)=0,017     [junit]     diagnostics = {os.version=6.0, os=windows vista, lucene.version=4.0-snapshot, source=flush, os.arch=x86,  java.version=1.6.0_21, java.vendor=sun microsystems inc.}     [junit]     has deletions [delfilename=_0_1.del]     [junit]     test: open reader.........ok [13 deleted docs]     [junit]     test: fields..............ok [2 fields]     [junit]     test: field norms.........ok [2 fields]     [junit]     test: terms, freq, prox...error [read past eof]     [junit] java.io.ioexception: read past eof     [junit]     at org.apache.lucene.store.bufferedindexinput.refill(bufferedindexinput.java:154)     [junit]     at org.apache.lucene.store.bufferedindexinput.readbytes(bufferedindexinput.java:119)     [junit]     at org.apache.lucene.store.bufferedindexinput.readbytes(bufferedindexinput.java:94)     [junit]     at org.apache.lucene.index.codecs.sep.seppostingsreaderimpl$sepdocsandpositionsenum.getpayload(seppostin gsreaderimpl.java:689)     [junit]     at org.apache.lucene.index.checkindex.testtermindex(checkindex.java:693)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:489)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:290)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:286)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:76)     [junit]     at org.apache.lucene.index.testindexwriterdelete.dotestoperationsondiskfull(testindexwriterdelete.java:5 38)     [junit]     at org.apache.lucene.index.testindexwriterdelete.testdeletesondiskfull(testindexwriterdelete.java:401)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:743)     [junit]     test: stored fields.......ok [223 total field count; avg 1,549 fields per doc]     [junit]     test: term vectors........ok [242 total vector count; avg 1,681 term/freq vector fields per doc]     [junit] failed     [junit]     warning: fixindex() would remove reference to this segment; full exception:     [junit] java.lang.runtimeexception: term index test failed     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:502)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:290)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:286)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:76)     [junit]     at org.apache.lucene.index.testindexwriterdelete.dotestoperationsondiskfull(testindexwriterdelete.java:5 38)     [junit]     at org.apache.lucene.index.testindexwriterdelete.testdeletesondiskfull(testindexwriterdelete.java:401)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:743)     [junit]     [junit] warning: 1 broken segments (containing 144 documents) detected     [junit]     [junit] note: reproduce with: ant test -dtestcase=testindexwriterdelete -dtestmethod=testdeletesondiskfull -dtests.s eed=-8128829179004133416:-7192468460505114475     [junit] checkindex failed     [junit] segments file=segments_1 numsegments=1 version=format_4_0 [lucene 4.0]     [junit]   1 of 1: name=_0 doccount=157     [junit]     codec=mockfixedintblock     [junit]     compound=false     [junit]     hasprox=true     [junit]     numfiles=14     [junit]     size (mb)=0,017     [junit]     diagnostics = {os.version=6.0, os=windows vista, lucene.version=4.0-snapshot, source=flush, os.arch=x86,  java.version=1.6.0_21, java.vendor=sun microsystems inc.}     [junit]     no deletions     [junit]     test: open reader.........ok     [junit]     test: fields..............ok [2 fields]     [junit]     test: field norms.........ok [2 fields]     [junit]     test: terms, freq, prox...error [read past eof]     [junit] java.io.ioexception: read past eof     [junit]     at org.apache.lucene.store.raminputstream.switchcurrentbuffer(raminputstream.java:89)     [junit]     at org.apache.lucene.store.raminputstream.readbytes(raminputstream.java:73)     [junit]     at org.apache.lucene.store.mockindexinputwrapper.readbytes(mockindexinputwrapper.java:109)     [junit]     at org.apache.lucene.index.codecs.sep.seppostingsreaderimpl$sepdocsandpositionsenum.getpayload(seppostin gsreaderimpl.java:689)     [junit]     at org.apache.lucene.index.checkindex.testtermindex(checkindex.java:693)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:489)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:290)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:286)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:76)     [junit]     at org.apache.lucene.index.testindexwriterdelete.dotestoperationsondiskfull(testindexwriterdelete.java:5 38)     [junit]     at org.apache.lucene.index.testindexwriterdelete.testupdatesondiskfull(testindexwriterdelete.java:405)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:743)     [junit]     test: stored fields.......ok [237 total field count; avg 1,51 fields per doc]     [junit]     test: term vectors........ok [254 total vector count; avg 1,618 term/freq vector fields per doc]     [junit] failed     [junit]     warning: fixindex() would remove reference to this segment; full exception:     [junit] java.lang.runtimeexception: term index test failed     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:502)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:290)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:286)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:76)     [junit]     at org.apache.lucene.index.testindexwriterdelete.dotestoperationsondiskfull(testindexwriterdelete.java:5 38)     [junit]     at org.apache.lucene.index.testindexwriterdelete.testupdatesondiskfull(testindexwriterdelete.java:405)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:743)     [junit]     [junit] warning: 1 broken segments (containing 157 documents) detected     [junit]     [junit] note: reproduce with: ant test -dtestcase=testindexwriterdelete -dtestmethod=testupdatesondiskfull -dtests.s eed=-8128829179004133416:-5617162412680232634     [junit] note: test params are: codec=mockfixedintblock(blocksize=266), locale=tr_tr, timezone=africa/maseru     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.index.testindexwriterdelete failed     [junit] testsuite: org.apache.lucene.index.testlazyproxskipping     [junit] tests run: 2, failures: 0, errors: 0, time elapsed: 0.034 sec     [junit]     [junit] ------------- standard output ---------------     [junit] note: reproduce with: ant test -dtestcase=testlazyproxskipping -dtestmethod=testlazyskipping -dtests.seed=-7 119541877291237950:-8499388603775233752     [junit] note: test params are: codec=simpletext, locale=da_dk, timezone=pacific/guam     [junit] ------------- ---------------- ---------------",
        "label": 33
    },
    {
        "text": "absurdly large radius  miles  search fails to include entire earth spinoff from lucene-1781. if you do a very large (eg 100000 miles) radius search then the lat/lng bound box wraps around the entire earth and all points should be accepted. but this fails today (many points are rejected). it's easy to see the issue: edit testcartesian, and insert a very large miles into either testrange or testgeohashrange.",
        "label": 10
    },
    {
        "text": "backport suggest module to branch x it would be nice to develop a plan to expose the autosuggest functionality to lucene users in 3.x there are some complications, such as seeing if we can backport the fst-based functionality, which might require a good bit of work. but i think this would be well-worth it.",
        "label": 40
    },
    {
        "text": "numericrangequery numericrangetermsenum sometimes seeks backwards subclasses of filteredtermsenum are \"supposed to\" seek forwards only (this gives better performance, typically). however, we don't check for this, so i added an assert to do that (while digging into testing the simpletext codec) and numericrangequery trips the assert! other mtqs seem not to trip it. i think i know what's happening \u2013 say nrq has term ranges a-c, e-f to seek to, but then while it's .next()'ing through the first range, the first term after c is f. at this point nrq sees the range a-c is done, and then tries to seek to term e which is before f. maybe nrq's accept method should detect this case (where you've accidentally .next()'d into or possibly beyond the next one or more seek ranges)?",
        "label": 53
    },
    {
        "text": "spellchecker does not work properly on calling indexdictionary after clearindex we have to call clearindex and indexdictionary to rebuild dictionary from fresh. the call to spellchecker clearindex() function does not reset the searcher. hence, when we call indexdictionary after that, many entries that are already in the stale searcher will not be indexed. also, i see that indexreader reader is used for the sole purpose of obtaining the docfreq of a given term in exist() function. this functionality can also be obtained by using just the searcher by calling searcher.docfreq. thus, can we get away completely with reader and code associated with it like if (indexreader.islocked(spellindex)) { indexreader.unlock(spellindex); } and the reader related code in finalize?",
        "label": 38
    },
    {
        "text": "document lengthfilter wrt unicode lengthfilter calculates its min/max length from termattribute.termlength() this is not characters, but instead utf-16 code units. in my opinion this should not be changed, merely documented. if we changed it, it would have an adverse performance impact because we would have to actually calculate character.codepointcount() on the text. if you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but i'd rather not hurt performance. i admit i don't fully understand all the use cases for this filter.",
        "label": 40
    },
    {
        "text": "create a maxfieldlengthanalyzer to wrap any other analyzer and provide the same functionality as maxfieldlength provided on indexwriter a spinoff from lucene-2294. instead of asking the user to specify on indexwriter his requested mfl limit, we can get rid of this setting entirely by providing an analyzer which will wrap any other analyzer and its tokenstream with a tokenfilter that keeps track of the number of tokens produced and stop when the limit has reached. this will remove any count tracking in iw's indexing, which is done even if i specified unlimited for mfl. let's try to do it for 3.1.",
        "label": 53
    },
    {
        "text": "spatial rpt intersects should use docidsetbuilder to save memory gc i\u2019ve been continuing some analysis into jvm garbage sources in my solr index. (5.4, 86m docs/core, 56k 99.9th percentile hit count with my query corpus) after applying solr-8922, i find my biggest source of garbage by a literal order of magnitude (by size) is the long[] allocated by fixedbitset. from the backtraces, it appears the biggest source of fixbitset creation in my case (by two orders of magnitude) is my use of queries that involve geospatial filtering. specifically, intersectsprefixtreequery.getdocidset, here: https://github.com/apache/lucene-solr/blob/569b6ca9ca439ee82734622f35f6b6342c0e9228/lucene/spatial-extras/src/java/org/apache/lucene/spatial/prefix/intersectsprefixtreequery.java#l60 has this been considered for optimization? i can think of a few paths: 1. persistent object pools - fixedbitset size is allocated based on maxdoc, which presumably changes less frequently than queries are issued. if an existing fixedbitset were not available from a pool, the worst case (create a new one) would be no worse than the current behavior. the complication would be enforcement around when to return the object to the pool, but it looks like this has some lifecycle hooks already. 2. i note that a thing called a sparsefixedbitset already exists, and puts considerable effort into allocating smaller chunks only as necessary. is this not usable for this purpose? how significant is the performance difference? i'd be happy to spend some time on a patch, but i was hoping for a little more data around the current choices before choosing an approach.",
        "label": 10
    },
    {
        "text": "highlight vs vector highlight alg is unfair highlight-vs-vector-highlight.alg uses enwikiquerymaker which makes spanqueries, but fastvectorhighlighter simply ignores spanqueries.",
        "label": 33
    },
    {
        "text": "isopen needs to be accessible by subclasses of directory the directory abstract class has a member variable named isopen which is package accessible. the usage of the variable is such that it should be readable and must be writable (in order to implement close()) by any concrete implementation of directory. because of the current accessibility of this variable is is not possible to create a directory implementation that is not also in the org.apache.lucene.store. i propose that either the isopen variable either needs to be declared protected or that there should be getter/setter methods that are protected.",
        "label": 33
    },
    {
        "text": "standardbenchmarker makedocument does not explicitly close opened files standardbenchmarker#makedocument(file in, string[] tags, boolean stored, boolean tokenized, boolean tfv) bufferedreader reader = new bufferedreader(new filereader(in)); above reader is not closed until gc hits it. can cause problems in cases where ulimit is set too low. i did this: while ((line = reader.readline()) != null) { body.append(line).append(' '); } + reader.close();",
        "label": 12
    },
    {
        "text": "enhance queryutils and checkhits to wrap everything they check in multireader multisearcher methods in checkhits & queryutils are in a good position to take any searcher they are given and not only test it, but also test multireader & multisearcher constructs built around them",
        "label": 18
    },
    {
        "text": "make wildcardtermenum difference  non final the method wildcardtermenum#difference() is declared final. i found it very useful to subclass wildcardtermenum to implement different scoring for exact vs. partial matches. the change is rather trivial (attached) but i guess it could make life easier for a couple of users. i attached two patches: one which contains the single change to make difference() non-final (wildcardtermenum.patch) one which does also contain some minor cleanup of wildcardtermenum. i removed unnecessary member initialization and made those final. ( wildcardtermenum_cleanup.patch) thanks simon",
        "label": 33
    },
    {
        "text": "blendedinfixsuggester throws nullpointerexception if there were discarded trailing characters in the query blendedinfixsuggester throws nullpointerexception if there were discarded trailing characters (e.g. whitespace or special character) in the query. the problem seems to be in the createcoefficient method that fails to check if prefixtoken parameter is null. analyzinginfixsuggester sets prefixtoken to null in the described case and passes it to blendedinfixsuggester. on the side not even if blendedinfixsuggester is changed to handle this creates a problem to calculate the weights as prefixtoken is null and cannot be used. i would be better to have analyzinginfixsuggester to always set prefixtoken to lasttoken.",
        "label": 21
    },
    {
        "text": "testdocumentswriterdeletequeue failure ant test -dtestcase=testdocumentswriterdeletequeue -dtests.method=testupdatedelteslices -dtests.seed=37979d1ce2e7bd80 -dtests.locale=nl_be -dtests.timezone=us/mountain -dtests.multiplier=3 -dargs=\"-dfile.encoding=utf-8\"",
        "label": 46
    },
    {
        "text": "we should also fsync the directory when committing since we are on java 7 now and we already fixed fsdir.sync to use filechannel (lucene-5570), we can also fsync the directory (at least try to do it). unlike randomaccessfile, which must be a regular file, filechannel.open() can also open a directory: http://stackoverflow.com/questions/7694307/using-filechannel-to-fsync-a-directory-with-nio-2",
        "label": 53
    },
    {
        "text": "postingsenum should have consistent flags behavior when asking for flags like offsets or payloads with docsandpositionsenum, the behavior was to always return an enum, even if offsets or payloads were not indexed. they would just not be available from the enum if they were not present. this behavior was carried over to postingsenum, which is good. however, the new positions flag has different behavior. if positions are not available, null is returned, instead of a postingsenum that just gives access to freqs. this behavior is confusing, as it means you have to special case asking for positions (only ask if you know they were indexed) which sort of defeats the purpose of the unified postingsenum. we should make positions have the same behavior as other flags. the trickiest part will be maintaining backcompat for docsandpositionsenum in 5.x, but i think it can be done.",
        "label": 40
    },
    {
        "text": "iw positions check not quite right i noticed this when working on lucene-5977. we only check that position doesn't overflow, not length. so a buggy analyzer can happily write a corrupt index (negative freq)",
        "label": 40
    },
    {
        "text": " patch multiple threads performance enhancement when querying  this improvement will reduce the wait when terminfosreader calls ensureindexisread(). the small trick like below: .... private void ensureindexisread() throws ioexception { if (indexterms != null) // index already read return; // do nothing synchronized(this){ system.out.println(\"read index@--@\"); if(indexterms != null) { system.out.println (\"someone read it.return-_-\"); return ; } readindex (); } } private synchronized void readindex() throws ioexception{ term[] m_indexterms = null; try { int indexsize = (int)indexenum.size; // otherwise read index m_indexterms = new term[indexsize]; indexinfos = new terminfo[indexsize]; indexpointers = new long[indexsize]; for (int i = 0; indexenum.next(); i++) { m_indexterms[i] = indexenum.term(); indexinfos[i] = indexenum.terminfo(); indexpointers[i] = indexenum.indexpointer; } } finally { indexenum.close(); indexenum = null; indexterms = m_indexterms; } }",
        "label": 55
    },
    {
        "text": "rat sources target is missing build and ivy xml files  also resources folders the check for copyright notices in files isn't checking all files, only source/test files. a couple modules xml files are missing this (join, spatial, and custom-tasks.xml).",
        "label": 53
    },
    {
        "text": "use docvalues to store per doc facet ord spinoff from lucene-4600 docvalues can be used to hold the byte[] encoding all facet ords for the document, instead of payloads. i made a hacked up approximation of in-ram dv (see cachedcountingfacetscollector in the patch) and the gains were somewhat surprisingly large:                     task    qps base      stddev    qps comp      stddev                pct diff                 highterm        0.53      (0.9%)        1.00      (2.5%)   87.3% (  83% -   91%)                  lowterm        7.59      (0.6%)       26.75     (12.9%)  252.6% ( 237% -  267%)                  medterm        3.35      (0.7%)       12.71      (9.0%)  279.8% ( 268% -  291%) i didn't think payloads were that slow; i think it must be the advance implementation? we need to separately test on-disk dv to make sure it's at least on-par with payloads (but hopefully faster) and if so ... we should cutover facets to using dv.",
        "label": 43
    },
    {
        "text": "remove deprecated benchmarking utilities from contrib benchmark the old benchmark utilities in contrib/benchmark have been deprecated and should be removed in 2.9 of lucene.",
        "label": 33
    },
    {
        "text": "queryparser escaping parsin issue with strings starting ending with   there is a problem with query parser when search string starts/ends with ||. when string contains || in the middle like 'something || something' everything runs without a problem. part of code: searchtext = queryparser.escape(searchtext); queryparser parser = null; parser = new queryparser(fieldname, new customanalyser()); parser.parse(searchtext); customanalyser class extends analyser. here is the only redefined method: @override public tokenstream tokenstream(string fieldname, reader reader) { return new porterstemfilter( (new stopanalyzer()).tokenstream(fieldname, reader)); } i have tested this on lucene 2.1 and latest source i have checked-out from svn (revision 538867) and in both cases parsing exception was thrown. part of stack trace (lucene - svn checkout - revision 538867): cannot parse 'someting ||': encountered \"<eof>\" at line 1, column 11. was expecting one of: <not> ... \"+\" ... \"-\" ... \"(\" ... \"*\" ... <quoted> ... <term> ... <prefixterm> ... <wildterm> ... \"[\" ... \"{\" ... <number> ... org.apache.lucene.queryparser.parseexception: cannot parse 'someting ||': encountered \"<eof>\" at line 1, column 11. was expecting one of: <not> ... \"+\" ... \"-\" ... \"(\" ... \"*\" ... <quoted> ... <term> ... <prefixterm> ... <wildterm> ... \"[\" ... \"{\" ... <number> ... at org.apache.lucene.queryparser.queryparser.parse(queryparser.java:150) part of stack trace (lucene 2.1): cannot parse 'something ||': encountered \"<eof>\" at line 1, column 12. was expecting one of: <not> ... \"+\" ... \"-\" ... \"(\" ... \"*\" ... <quoted> ... <term> ... <prefixterm> ... <wildterm> ... \"[\" ... \"{\" ... <number> ... org.apache.lucene.queryparser.parseexception: cannot parse 'something ||': encountered \"<eof>\" at line 1, column 12. was expecting one of: <not> ... \"+\" ... \"-\" ... \"(\" ... \"*\" ... <quoted> ... <term> ... <prefixterm> ... <wildterm> ... \"[\" ... \"{\" ... <number> ... at org.apache.lucene.queryparser.queryparser.parse(queryparser.java:149)",
        "label": 32
    },
    {
        "text": "g3d wrapper  improve circles for non spherical planets hi david smiley, the purpose of this ticket is to add a new circle shape (geoexactcircle) for non-spherical planets and therefore remove the method relate from geo3dcircleshape. the patch will include some simplifications on the wrapper and some refactoring of the tests. i will open shortly a pull request.",
        "label": 10
    },
    {
        "text": "tweak sortingmergepolicy getsortdescription building tests for solr-5730 identified that early termination can be omitted for sorted segments because the leafreader concerned is not a segmentreader - github pull request to illustrate to follow: the testearlyterminatingsortingcollector.testterminatedearly test uses a wrapped reader in the same way as solrindexsearcher the sortingmergepolicy.getsortdescription change (assuming it is a valid change to make) fixes that particular test only exitabledirectoryreader and uninvertingreader wrap could perhaps also be added in lucenetestcase.wrapreader ? lucene-6065 \"remove \"foreign readers\" from merge, fix leafreader instead.\" also concerns sortingmergepolicy.",
        "label": 9
    },
    {
        "text": "testdimensionalrangequery failures  aioobe while merging from http://jenkins.sarowe.net/job/lucene-solr-nightly-trunk/105/ - neither failure reproduced for me on the same box:    [junit4] suite: org.apache.lucene.search.testdimensionalrangequery    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testdimensionalrangequery -dtests.method=testrandomlongsbig -dtests.seed=bef1d45ada12b09b -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=cs_cz -dtests.timezone=africa/porto-novo -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] error   43.4s j5  | testdimensionalrangequery.testrandomlongsbig <<<    [junit4]    > throwable #1: org.apache.lucene.store.alreadyclosedexception: this indexwriter is closed    [junit4]    >        at __randomizedtesting.seedinfo.seed([bef1d45ada12b09b:95c7b6d701973443]:0)    [junit4]    >        at org.apache.lucene.index.indexwriter.ensureopen(indexwriter.java:714)    [junit4]    >        at org.apache.lucene.index.indexwriter.ensureopen(indexwriter.java:728)    [junit4]    >        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1459)    [junit4]    >        at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1242)    [junit4]    >        at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:170)    [junit4]    >        at org.apache.lucene.search.testdimensionalrangequery.verifylongs(testdimensionalrangequery.java:208)    [junit4]    >        at org.apache.lucene.search.testdimensionalrangequery.dotestrandomlongs(testdimensionalrangequery.java:147)    [junit4]    >        at org.apache.lucene.search.testdimensionalrangequery.testrandomlongsbig(testdimensionalrangequery.java:114)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.lang.arrayindexoutofboundsexception: 1024    [junit4]    >        at org.apache.lucene.util.bkd.bkdwriter$mergereader.next(bkdwriter.java:279)    [junit4]    >        at org.apache.lucene.util.bkd.bkdwriter.merge(bkdwriter.java:413)    [junit4]    >        at org.apache.lucene.codecs.lucene60.lucene60dimensionalwriter.merge(lucene60dimensionalwriter.java:159)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedimensionalvalues(segmentmerger.java:168)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:117)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4062)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3642)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2> note: leaving temporary files on disk at: /var/lib/jenkins/jobs/lucene-solr-nightly-trunk/workspace/lucene/build/core/test/j5/temp/lucene.search.testdimensionalrangequery_bef1d45ada12b09b-001    [junit4]   2> dec 15, 2015 11:03:38 pm com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[lucene merge thread #634,5,tgrp-testdimensionalrangequery]    [junit4]   2> org.apache.lucene.index.mergepolicy$mergeexception: java.lang.arrayindexoutofboundsexception: 1024    [junit4]   2>        at __randomizedtesting.seedinfo.seed([bef1d45ada12b09b]:0)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]   2> caused by: java.lang.arrayindexoutofboundsexception: 1024    [junit4]   2>        at org.apache.lucene.util.bkd.bkdwriter$mergereader.next(bkdwriter.java:279)    [junit4]   2>        at org.apache.lucene.util.bkd.bkdwriter.merge(bkdwriter.java:413)    [junit4]   2>        at org.apache.lucene.codecs.lucene60.lucene60dimensionalwriter.merge(lucene60dimensionalwriter.java:159)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.mergedimensionalvalues(segmentmerger.java:168)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:117)    [junit4]   2>        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4062)    [junit4]   2>        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3642)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2>     [junit4]   2> dec 15, 2015 11:03:38 pm com.carrotsearch.randomizedtesting.threadleakcontrol checkthreadleaks    [junit4]   2> warning: will linger awaiting termination of 1 leaked thread(s).    [junit4]   2> note: test params are: codec=fastcompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fast, chunksize=4, maxdocsperchunk=729, blocksize=3), termvectorsformat=compressingtermvectorsformat(compressionmode=fast, chunksize=4, blocksize=3)), sim=randomsimilarityprovider(querynorm=false,coord=no): {}, locale=cs_cz, timezone=africa/porto-novo    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=166342976,total=535298048    [junit4]   2> note: all tests run in this jvm: [testpositionincrement, testconcurrentmergescheduler, testdimensionalrangequery]    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testdimensionalrangequery -dtests.seed=bef1d45ada12b09b -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=cs_cz -dtests.timezone=africa/porto-novo -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] error   0.00s j5  | testdimensionalrangequery (suite) <<<    [junit4]    > throwable #1: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=5397, name=lucene merge thread #634, state=runnable, group=tgrp-testdimensionalrangequery]    [junit4]    > caused by: org.apache.lucene.index.mergepolicy$mergeexception: java.lang.arrayindexoutofboundsexception: 1024    [junit4]    >        at __randomizedtesting.seedinfo.seed([bef1d45ada12b09b]:0)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]    > caused by: java.lang.arrayindexoutofboundsexception: 1024    [junit4]    >        at org.apache.lucene.util.bkd.bkdwriter$mergereader.next(bkdwriter.java:279)    [junit4]    >        at org.apache.lucene.util.bkd.bkdwriter.merge(bkdwriter.java:413)    [junit4]    >        at org.apache.lucene.codecs.lucene60.lucene60dimensionalwriter.merge(lucene60dimensionalwriter.java:159)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedimensionalvalues(segmentmerger.java:168)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:117)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4062)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3642)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4] completed [185/403 (1!)] on j5 in 727.10s, 17 tests, 2 errors <<< failures!",
        "label": 33
    },
    {
        "text": "fuzzyquery produces a  java lang negativearraysizeexception  in priorityqueue initialize if i use integer max value as booleanquery maxclausecount priorityqueue creates an \"java.lang.negativearraysizeexception\" when initialized with integer.max_value, because integer overflows. i think this could be a general problem with priorityqueue. the error occured when i set booleanquery.maxclausecount to integer.max_value and user a fuzzyquery for searching.",
        "label": 53
    },
    {
        "text": "bounds of bounding box are not equal to bounding box hi, it seems if i get the bounds of a bbox and i create a new bounding box, sometimes both bounding box are not equal. it is a problem with precision.",
        "label": 25
    },
    {
        "text": "speed up top k sampling tests speed up the top-k sampling tests (but make sure they are thorough on nightly etc still) usually we would do this with use of atleast(), but these tests are somewhat tricky, so maybe a different approach is needed.",
        "label": 43
    },
    {
        "text": "move elisionfilter out of  fr package steven rowe noted this a while back, but i forgot to open an issue: this is generally useful for handling contractions. we already use this filter for french/italian/catalan. now we also have a contribution for irish (lucene-3883) that uses it too. i think we should put this in o.a.l.analysis.util instead.",
        "label": 40
    },
    {
        "text": "suppresscodecs doesnt work with  dtests postingsformat as reported by han on the mailing list: if you are running all tests with your postingsformat, but a test specifically ignores it, it doesnt work correctly.",
        "label": 11
    },
    {
        "text": "yank segmentreader norm out of segmentreader java while working on flex scoring branch and lucene-3012, i noticed it was difficult to navigate the norms handling in segmentreader's code. i think we should yank this inner class out into a separate file as a start.",
        "label": 40
    },
    {
        "text": "include math random  into forbiddenapi math.random() should be included into forbiddenapi",
        "label": 53
    },
    {
        "text": "postingsconsumer merge does not call finishdoc we discovered that the current merge function in postingsconsumer is not calling the #finishdoc method. this does not have consequences for the standard codec (since the lastposition is set to 0 in #startdoc, and its #finishdoc method is empty), but for the sepcodec, this results in position file corruption (the lastposition is set to 0 in #finishdoc for the sepcodec).",
        "label": 33
    },
    {
        "text": "hashcode improvements it would be nice for all query classes to implement hashcode and equals to enable them to be used as keys when caching.",
        "label": 55
    },
    {
        "text": "script addversion py does not detect the new naming convention for bugfix branches spinoff from lucene-6938. earlier we named release branches lucene_solr_x_y while now we name them branch_x_y.",
        "label": 21
    },
    {
        "text": "improve javadocs for numeric  i'm working on improving numeric* javadocs.",
        "label": 33
    },
    {
        "text": "look into making mmapdirectory's unmap safer i have seen a few bugs around this recently: of course its a bug in application code but a jvm crash is not good. i think we should see if we can prevent the crashes better than the current weak map, e.g. make it a safer option. i made an ugly prototype here: https://github.com/apache/lucene-solr/compare/master...rmuir:ace?expand=1 it has a test that crashes the jvm without the patch but passes with. hacky patch only implements readbytes() but has no problems with the luceneutil benchmark (1m): report after iter 19:                     task    qps base      stddev   qps patch      stddev                pct diff                   intnrq      105.23     (17.6%)      100.42     (10.1%)   -4.6% ( -27% -   28%)                  respell      128.35     (13.2%)      125.88      (7.4%)   -1.9% ( -19% -   21%)                   fuzzy1      110.14     (17.2%)      108.28     (13.2%)   -1.7% ( -27% -   34%)                lowphrase      337.02     (13.0%)      333.72      (9.3%)   -1.0% ( -20% -   24%)                medphrase      146.44     (12.9%)      145.55      (8.0%)   -0.6% ( -19% -   23%)              medspannear       96.85     (13.1%)       96.57      (7.8%)   -0.3% ( -18% -   23%)             highspannear       95.85     (13.9%)       96.33      (8.2%)    0.5% ( -18% -   26%)               highphrase      146.84     (13.6%)      148.40      (8.4%)    1.1% ( -18% -   26%)                 highterm      295.15     (15.8%)      298.77      (9.5%)    1.2% ( -20% -   31%)              lowspannear      268.80     (12.4%)      272.16      (7.9%)    1.2% ( -16% -   24%)                 wildcard      284.09     (11.7%)      290.91      (8.9%)    2.4% ( -16% -   25%)                  prefix3      212.50     (15.4%)      217.76     (10.0%)    2.5% ( -19% -   32%)                orhighlow      358.65     (15.0%)      368.93     (10.7%)    2.9% ( -19% -   33%)               andhighmed      799.65     (13.2%)      834.74      (7.8%)    4.4% ( -14% -   29%)          medsloppyphrase      229.36     (15.9%)      239.95      (9.8%)    4.6% ( -18% -   36%)                   fuzzy2       69.58     (14.6%)       72.82     (14.5%)    4.7% ( -21% -   39%)              andhighhigh      426.98     (12.8%)      451.77      (7.3%)    5.8% ( -12% -   29%)                  medterm     1361.11     (14.5%)     1450.90      (9.2%)    6.6% ( -14% -   35%)                 pklookup      266.61     (13.4%)      284.28      (8.4%)    6.6% ( -13% -   32%)         highsloppyphrase      251.22     (16.9%)      268.32     (10.7%)    6.8% ( -17% -   41%)                orhighmed      235.92     (17.2%)      253.12     (12.8%)    7.3% ( -19% -   45%)               orhighhigh      186.79     (13.5%)      201.15      (9.7%)    7.7% ( -13% -   35%)          lowsloppyphrase      395.23     (15.9%)      425.93      (9.3%)    7.8% ( -15% -   39%)               andhighlow     1128.28     (14.9%)     1242.11      (8.2%)   10.1% ( -11% -   38%)                  lowterm     3024.62     (12.9%)     3367.65      (9.7%)   11.3% (  -9% -   39%) we should do more testing. maybe its totally the wrong tradeoff, maybe we only need handles for getters and everything inlines correctly, rather than needing a ton for every getxyz() method...",
        "label": 53
    },
    {
        "text": "don't call 'svnversion' over and over in the build some ant tasks (at least release packaging, i dunno what else), call svnversion over and over and over for each module in the build. can we just do this one time instead?",
        "label": 53
    },
    {
        "text": "grouping sortwithingroup should use sort relevance to indicate that  not null in abstractsecondpassgroupingcollector, withingroupsort uses a value of null to indicate a relevance sort. i think it's nicer to use sort.relevance for this \u2013 after all it's how the groupsort variable is handled. this choice is also seen in groupingsearch; likely some other collaborators too. martijn van groningen is there some wisdom in the current choice that escapes me? if not i'll post a patch.",
        "label": 10
    },
    {
        "text": "npe  testdirectorytaxonomyreader testgetchildren reproduces 100% for me on trunk and on branch_4x - below is from branch_4x:    [junit4] suite: org.apache.lucene.facet.taxonomy.directory.testdirectorytaxonomyreader    [junit4]   2> note: reproduce with: ant test  -dtestcase=testdirectorytaxonomyreader -dtests.method=testgetchildren -dtests.seed=b94c5192b4851c12 -dtests.slow=true -dtests.locale=es_uy -dtests.timezone=america/santa_isabel -dtests.file.encoding=us-ascii    [junit4] error   0.48s | testdirectorytaxonomyreader.testgetchildren <<<    [junit4]    > throwable #1: java.lang.nullpointerexception    [junit4]    >  at __randomizedtesting.seedinfo.seed([b94c5192b4851c12:d5baeb4f58ae9d94]:0)    [junit4]    >  at org.apache.lucene.facet.taxonomy.directory.testdirectorytaxonomyreader.testgetchildren(testdirectorytaxonomyreader.java:508)    [junit4]    >  at java.lang.thread.run(thread.java:724)    [junit4]   2> note: test params are: codec=lucene40, sim=randomsimilarityprovider(querynorm=false,coord=crazy): {}, locale=es_uy, timezone=america/santa_isabel    [junit4]   2> note: mac os x 10.8.5 x86_64/oracle corporation 1.7.0_25 (64-bit)/cpus=8,threads=1,free=83697144,total=119537664    [junit4]   2> note: all tests run in this jvm: [testdirectorytaxonomyreader]    [junit4] completed in 1.06s, 1 test, 1 error <<< failures!    [junit4]     [junit4]     [junit4] tests with failures:    [junit4]   - org.apache.lucene.facet.taxonomy.directory.testdirectorytaxonomyreader.testgetchildren",
        "label": 43
    },
    {
        "text": "latlonshape  within and disjoint queries don\u2019t work with indexed multishapes within and disjoint queries return wrong results (false positives) when querying for fields containing more than one shape. for example, a multipolygon will return true for a within query if some of the polygons are within and the other are disjoint. the same query will return true for disjoint.  ",
        "label": 19
    },
    {
        "text": "terminsetquerytest testrambytesused fails with an assertion on java   so, this is strange. i get a fully reproducible numbers going in for the same seed in this test, but the failures observed on jenkins are non-reproducible in idea (but they can be reproduced from ant). an example: ant test  -dtestcase=terminsetquerytest -dtests.method=testrambytesused -dtests.seed=d8eacd24eb26b7fd -dtests.multiplier=3 -dtests.slow=true -dtests.badapples=true -dtests.locale=kw -dtests.timezone=america/north_dakota/new_salem -dtests.asserts=true -dtests.file.encoding=utf-8 something is odd.",
        "label": 11
    },
    {
        "text": "choose a specific directory implementation running the checkindex main it should be possible to choose a specific directory implementation to use during the checkindex process when we run it from its main. what about an additional main parameter? in fact, i'm experiencing some problems with mmapdirectory working with a big segment, and after some failed attempts playing with maxchunksize, i decided to switch to another fsdirectory implementation but i needed to do that on my own main. should we also consider to use a fileswitchdirectory? i'm willing to contribute, could you please let me know your thoughts about it?",
        "label": 27
    },
    {
        "text": "indexwriter addindexes can make any incoming segment into cfs if it isn't already today, iw.addindexes(directory) does not modify the cfs-mode of the incoming segments. however, if indexwriter's mp wants to create cfs (in general), there's no reason why not turn the incoming non-cfs segments into cfs. we anyway copy them, and if mp is not against cfs, we should create a cfs out of them. will need to use cfw, not sure it's ready for that w/ current api (i'll need to check), but luckily we're allowed to change it (@lucene.internal). this should be done, imo, even if the incoming segment is large (i.e., passes mp.nocfsratio) b/c like i wrote above, we anyway copy it. however, if you think otherwise, speak up . i'll take a look at this in the next few days.",
        "label": 43
    },
    {
        "text": "add a conditionaltokenfilter spinoff of lucene-8265. it would be useful to be able to wrap a tokenfilter in such a way that it could optionally be bypassed based on the current state of the tokenstream. this could be used to, for example, only apply worddelimiterfilter to terms that contain hyphens.",
        "label": 2
    },
    {
        "text": "don't override non abstract methods that have an impl through other abstract methods in filteratomicreader and related classes terms.intersect is an optional method. the fact that it is overridden in filterterms forces any non-trivial class that extends filterterms to override intersect in order this method to have a correct behavior. if filterterms did not override this method and used the default impl, we would not have this problem.",
        "label": 1
    },
    {
        "text": "indexwriter retains references to readers used in fields  memory leak  as described in [1] indexwriter retains references to reader used in fields and that can lead to big memory leaks when using tika's parsingreaders (as those can take 1mb per parsingreader). [2] shows a screenshot of the reference chain to the reader from the indexwriter taken with eclipse mat (memory analysis tool) . the chain is the following: indexwriter -> documentswriter -> documentswriterthreadstate -> docfieldprocessorperthread -> docfieldprocessorperfield -> fieldable -> field (fieldsdata) ------------- [1] http://markmail.org/thread/ndmcgffg2mnwjo47 [2] http://skitch.com/ecerulm/n7643/eclipse-memory-analyzer",
        "label": 33
    },
    {
        "text": "disallow setboost  on stringfield  throw exception if boosts are set if norms are omitted occasionally users are confused why index-time boosts are not applied to their norms-omitted fields. this is because we silently discard the boost: there is no reason for this! the most absurd part: in 4.0 you can make a stringfield and call setboost and nothing complains... (more reasons to remove stringfield totally in my opinion)",
        "label": 40
    },
    {
        "text": "large distances in spatial go beyond prime meridian http://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674 get an error when using solr when distance is calculated for the boundary box past 90 degrees. aug 4, 2009 1:54:00 pm org.apache.solr.common.solrexception log severe: java.lang.illegalargumentexception: illegal lattitude value 93.1558669413734 at org.apache.lucene.spatial.geometry.floatlatlng.<init>(floatlatlng.java:26) at org.apache.lucene.spatial.geometry.shape.llrect.createbox(llrect.java:93) at org.apache.lucene.spatial.tier.distanceutils.getboundary(distanceutils.java:50) at org.apache.lucene.spatial.tier.cartesianpolyfilterbuilder.getboxshape(cartesianpolyfilterbuilder.java:47) at org.apache.lucene.spatial.tier.cartesianpolyfilterbuilder.getboundingarea(cartesianpolyfilterbuilder.java:109) at org.apache.lucene.spatial.tier.distancequerybuilder.<init>(distancequerybuilder.java:61) at com.pjaol.search.solr.component.localsolrquerycomponent.prepare(localsolrquerycomponent.java:151) at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:174) at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:131) at org.apache.solr.core.solrcore.execute(solrcore.java:1328) at org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:341) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:244) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:235) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:206) at org.apache.catalina.core.standardwrappervalve.invoke(standardwrappervalve.java:233) at org.apache.catalina.core.standardcontextvalve.invoke(standardcontextvalve.java:191) at org.apache.catalina.core.standardhostvalve.invoke(standardhostvalve.java:128) at org.apache.catalina.valves.errorreportvalve.invoke(errorreportvalve.java:102) at org.apache.catalina.core.standardenginevalve.invoke(standardenginevalve.java:109) at org.apache.catalina.connector.coyoteadapter.service(coyoteadapter.java:286) at org.apache.coyote.http11.http11aprprocessor.process(http11aprprocessor.java:857) at org.apache.coyote.http11.http11aprprotocol$http11connectionhandler.process(http11aprprotocol.java:565) at org.apache.tomcat.util.net.aprendpoint$worker.run(aprendpoint.java:1509) at java.lang.thread.run(thread.java:619)",
        "label": 33
    },
    {
        "text": "remove contrib misc and contrib wordnet's dependencies on analyzers module these contribs don't actually analyze any text. after this patch, only the contrib/demo relies upon the analyzers module... we can separately try to figure that one out (i don't think any of these lucene contribs needs to reach back into modules/)",
        "label": 40
    },
    {
        "text": "nullpointerexception in booleanfilter booleanfilter getdisi() method used with querywrapperfilter occur nullpointerexception, if any querywrapperfilter not match terms in indexreader. --------------------------------------------------- java.lang.nullpointerexception at org.apache.lucene.util.openbitsetdisi.inplaceand(openbitsetdisi.java:66) at org.apache.lucene.search.booleanfilter.getdocidset(booleanfilter.java:102) at org.apache.lucene.search.indexsearcher.searchwithfilter(indexsearcher.java:551) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:532) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:463) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:433) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:356) at test.booleanfiltertest.main(booleanfiltertest.java:50) --------------------------------------------------- null-check below lines. --------------------------------------------------- res = new openbitsetdisi(getdisi(shouldfilters, i, reader), reader.maxdoc()); res.inplaceor(getdisi(shouldfilters, i, reader)); res = new openbitsetdisi(getdisi(notfilters, i, reader), reader.maxdoc()); res.inplacenot(getdisi(notfilters, i, reader)); res = new openbitsetdisi(getdisi(mustfilters, i, reader), reader.maxdoc()); res.inplaceand(getdisi(mustfilters, i, reader)); ---------------------------------------------------",
        "label": 53
    },
    {
        "text": "indexwriter creates unwanted termvector info i noticed today that when i build a big index in solr, i get some unwanted termvector info, even though i didn't request any. this does not happen on 3x - not sure when it started happening on trunk.",
        "label": 33
    },
    {
        "text": "add fixed size docvalues int variants   expose arrays where possible currently we only have variable bit packed ints implementation. for flexible scoring or loading field caches it is desirable to have fixed int implementations for 8, 16, 32 and 64 bit.",
        "label": 46
    },
    {
        "text": "unban properties with unicode escapes as discussed on the mailing list, its just wrong to ban the use of unicode here. this blocks 4.4 (because it was committed there, too)",
        "label": 53
    },
    {
        "text": "all tokenizer implementations should have constructors that take attributesource and attributefactory i have a tokenstream implementation that joins together multiple sub tokenstreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging) in 2.4, this worked fine. once one sub stream was exhausted, i just started using the next stream however, in 2.9, this is very difficult, and requires copying term buffers for every token being aggregated however, if all the sub tokenstreams share the same attributesource, and my \"concat\" tokenstream shares the same attributesource, this goes back to being very simple (and very efficient) so for example, i would like to see the following constructor added to standardtokenizer:   public standardtokenizer(attributesource source, reader input, boolean replaceinvalidacronym) {     super(source);     ...   } would likewise want similar constructors added to all tokenizer sub classes provided by lucene",
        "label": 32
    },
    {
        "text": "lucene core missing from maven central repository sub-projects like lucene-demos, lucene-contrib, etc. exist in central, and depend on 2.9.0 of lucene-core. however, the lucene-core 2.9.0 artifact itself is missing.",
        "label": 29
    },
    {
        "text": "add support for terms in bytesref format to term  termquery  termrangequery   co  it would be good to directly allow bytesrefs in termquery and termrangequery (as both queries convert the strings to bytesref internally). for numericrange support in solr it will be needed to support numerics as byteref in single-term queries. when this will be added, don't forget to change testnumericrangequeryxx to use the bytesref ctor of trq.",
        "label": 33
    },
    {
        "text": "tokengroup getstart endoffset should return matchstart endoffset not start endoffset the default highlighter has a tokengroup class that is passed to formatter.highlightterm(). tokengroup also has getstartoffset() and getendoffset() methods that ostensibly return the start and end offsets into the original text of the current term. these getters aren't called by lucene or solr but they are made available and are useful to me. the problem is that they return the wrong offsets when there are tokens at the same position. i believe this was an oversight of lucene-627 in which these getters should have been updated but weren't. the fix is simple: return matchstartoffset and matchendoffset from these getters, not startoffset and endoffset. i think this oversight would not have occurred if highlighter didn't have package-access to tokengroup's fields.",
        "label": 10
    },
    {
        "text": "indexwriter readerpool create new segmentreader outside of sync block i think we will want to do something like what field cache does with creationplaceholder for indexwriter.readerpool. otherwise we have the (i think somewhat problematic) issue of all other readerpool.get* methods waiting for an sr to warm. it would be good to implement this for 2.9.",
        "label": 33
    },
    {
        "text": "more fine grained control over the packed integer implementation that is chosen in order to save space, lucene has two main packedints.mutable implentations, one that is very fast and is based on a byte/short/integer/long array (direct*) and another one which packs bits in a memory-efficient manner (packed*). the packed implementation tends to be much slower than the direct one, which discourages some lucene components to use it. on the other hand, if you store 21 bits integers in a direct32, this is a space loss of (32-21)/32=35%. if you accept to trade some space for speed, you could store 3 of these 21 bits integers in a long, resulting in an overhead of 1/3 bit per value. one advantage of this approach is that you never need to read more than one block to read or write a value, so this can be significantly faster than packed32 and packed64 which always need to read/write two blocks in order to avoid costly branches. i ran some tests, and for 10000000 21 bits values, this implementation takes less than 2% more space and has 44% faster writes and 30% faster reads. the 12 bits version (5 values per block) has the same performance improvement and a 6% memory overhead compared to the packed implementation. in order to select the best implementation for a given integer size, i wrote the packedints.getmutable(valuecount, bitspervalue, acceptableoverheadpervalue) method. this method select the fastest implementation that has less than acceptableoverheadpervalue wasted bits per value. for example, if you accept an overhead of 20% (acceptableoverheadpervalue = 0.2f * bitspervalue), which is pretty reasonable, here is what implementations would be selected: 1: packed64singleblock1 2: packed64singleblock2 3: packed64singleblock3 4: packed64singleblock4 5: packed64singleblock5 6: packed64singleblock6 7: direct8 8: direct8 9: packed64singleblock9 10: packed64singleblock10 11: packed64singleblock12 12: packed64singleblock12 13: packed64 14: direct16 15: direct16 16: direct16 17: packed64 18: packed64singleblock21 19: packed64singleblock21 20: packed64singleblock21 21: packed64singleblock21 22: packed64 23: packed64 24: packed64 25: packed64 26: packed64 27: direct32 28: direct32 29: direct32 30: direct32 31: direct32 32: direct32 33: packed64 34: packed64 35: packed64 36: packed64 37: packed64 38: packed64 39: packed64 40: packed64 41: packed64 42: packed64 43: packed64 44: packed64 45: packed64 46: packed64 47: packed64 48: packed64 49: packed64 50: packed64 51: packed64 52: packed64 53: packed64 54: direct64 55: direct64 56: direct64 57: direct64 58: direct64 59: direct64 60: direct64 61: direct64 62: direct64 under 32 bits per value, only 13, 17 and 22-26 bits per value would still choose the slower packed64 implementation. allowing a 50% overhead would prevent the packed implementation to be selected for bits per value under 32. allowing an overhead of 32 bits per value would make sure that a direct* implementation is always selected. next steps would be to: make lucene components use this getmutable method and let users decide what trade-off better suits them, write a packed32singleblock implementation if necessary (i didn't do it because i have no 32-bits computer to test the performance improvements). i think this would allow more fine-grained control over the speed/space trade-off, what do you think?",
        "label": 1
    },
    {
        "text": "booleanscorer2 does not compile with ecj booleanscorer2, derived from scorer, has two inner classes both derived, ultimately, from scorer. as such they all define doc() or inherit it. ecj produces an error when doc() is called from score in the inner classes in the methods countingdisjunctionsumscorer and countingconjunctionsumscorer the error message is: the method doc is defined in an inherited type and in an enclosing scope. the affected lines are: 160, 161, 178, and 179 i have run the junit test testboolean2 (as well as all the others) with doc() changed to booleanscorer2.this.doc() and also to: this.doc(); the result was that the tests passed for both. i added debug statements to all the doc methods and the score methods in the affected classes, but i could not determine what it should be.",
        "label": 55
    },
    {
        "text": "add namedspiloader support to tokenizerfactory  tokenfilterfactory and charfilterfactory in lucene-2510 i want to move all the analysis factories out of solr and into the directories with what they create. this is going to hamper solr's existing strategy for supporting solr.* package names, where it replaces solr with various pre-defined package names. one way to tackle this is to use namedspiloader so we simply look up standardtokenizerfactory for example, and find it wherever it is, as long as it is defined as a service. this is similar to how we support codecs currently. as noted by robert in lucene-2510, this would also have the benefit of meaning configurations could be less verbose, would aid in fully decoupling the analysis module from solr, and make the analysis factories easier to interact with.",
        "label": 53
    },
    {
        "text": "indexablebinarystringtools  convert arbitrary byte sequences into strings that can be used as index terms  and vice versa provides support for converting byte sequences to strings that can be used as index terms, and back again. the resulting strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned). the strings are constructed using a base 8000h encoding of the original binary data - each char of an encoded string represents a 15-bit chunk from the byte sequence. base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [u+d800-u+dfff] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit. this class is intended to serve as a mechanism to allow collationkeys to serve as index terms.",
        "label": 33
    },
    {
        "text": "make tokenstream reuse mandatory for analyzers in lucene-2309 it became clear that we'd benefit a lot from analyzer having to return reusable tokenstreams. this is a big chunk of work, but its time to bite the bullet. i plan to attack this in the following way: collapse the logic of reusableanalyzerbase into analyzer add a reusestrategy abstraction to analyzer which controls whether the tokenstreamcomponents are reused globally (as they are today) or per-field. convert all analyzers over to using tokenstreamcomponents. i've already seen that some of the tokenstreams created in tests need some work to be reusable (even if they aren't reused). remove analyzer.reusabletokenstream and convert everything over to using .tokenstream (which will now be returning reusable tokenstreams).",
        "label": 7
    },
    {
        "text": "problems with maxmergedocs parameter i found two possible problems regarding indexwriter's maxmergedocs value. i'm using the following code to test maxmergedocs:     public void testmaxmergedocs() throws ioexception {     final int maxmergedocs = 50;     final int numsegments = 40;          mockramdirectory dir = new mockramdirectory();     indexwriter writer  = new indexwriter(dir, new whitespaceanalyzer(), true);           writer.setmergepolicy(new logdocmergepolicy());     writer.setmaxmergedocs(maxmergedocs);     document doc = new document();     doc.add(new field(\"field\", \"aaa\", field.store.yes, field.index.tokenized, field.termvector.with_positions_offsets));     for (int i = 0; i < numsegments * maxmergedocs; i++) {       writer.adddocument(doc);       //writer.flush();      // uncomment to avoid the documentswriter bug     }     writer.close();          new segmentinfos.findsegmentsfile(dir) {       protected object dobody(string segmentfilename) throws corruptindexexception, ioexception {         segmentinfos infos = new segmentinfos();         infos.read(directory, segmentfilename);         for (int i = 0; i < infos.size(); i++) {           asserttrue(infos.info(i).doccount <= maxmergedocs);         }         return null;       }     }.run();   } it seems that documentswriter does not obey the maxmergedocs parameter. if i don't flush manually, then the index only contains one segment at the end and the test fails. if i flush manually after each adddocument() call, then the index contains more segments. but still, there are segments that contain more docs than maxmergedocs, e. g. 55 vs. 50. the javadoc in indexwriter says:    /**    * returns the largest number of documents allowed in a    * single segment.    *    * @see #setmaxmergedocs    */   public int getmaxmergedocs() {     return getlogdocmergepolicy().getmaxmergedocs();   }",
        "label": 33
    },
    {
        "text": "add noopmergepolicy i'd like to add a simple and useful mp implementation which does .... nothing ! . i've came across many places where either the following is documented or implemented: \"if you want to prevent merges, set mergefactor to a high enough value\". i think a noopmergepolicy is just as good, and can really allow you disable merges (except for maybe set mergefactor to int.max_val). as such, noopmergepolicy will be introduced as a singleton, and can be used for convenience purposes only. also, for parallel index it's important, because i'd like the slices to never do any merges, unless parallelwriter decides so. so they should be set w/ that mp. i have a patch ready. waiting for lucene-2320 to go in, so that i don't need to change it afterwards. about the name - i like the name, but suggestions are welcome. i thought of a nullmergepolicy, but i don't like 'null' used for a noop.",
        "label": 33
    },
    {
        "text": "fastvectorhighlihgter fails with sioob if single phrase or term is   fragcharsize this has been reported on several occasions like solr-4660 / solr-4137 or on the es mailing list https://groups.google.com/d/msg/elasticsearch/idymspk5gao/nkzq8_nywmgj the reason is that the current code expects the fragcharsize > matchlength which is not necessarily true if you use phrases or if you have very long terms like urls or so. i have a test that reproduces the issue and a fix as far as i can tell (me doesn't have much experience with the highlighter.",
        "label": 46
    },
    {
        "text": "weight scorer  not passed doc offset for  sub reader  now that searching is done on a per segment basis, there is no way for a scorer to know the \"actual\" doc id for the document's it matches (only the relative doc offset into the segment) if using caches in your scorer that are based on the \"entire\" index (all segments), there is now no way to index into them properly from inside a scorer because the scorer is not passed the needed offset to calculate the \"real\" docid suggest having weight.scorer() method also take a integer for the doc offset abstract weight class should have a constructor that takes this offset as well as a method to get the offset all weights that have \"sub\" weights must pass this offset down to created \"sub\" weights details on workaround: in order to work around this, you must do the following: subclass indexsearcher add \"int getindexreaderbase(indexreader)\" method to your subclass during weight creation, the weight must hold onto a reference to the passed in searcher (casted to your sub class) during scorer creation, the scorer must be passed the result of yoursearcher.getindexreaderbase(reader) scorer can now rebase any collected docids using this offset example implementation of getindexreaderbase(): // note: more efficient implementation can be done if you cache the result if gathersubreaders in your constructor public int getindexreaderbase(indexreader reader) {   if (reader == getreader()) {     return 0;   } else {     list readers = new arraylist();     gathersubreaders(readers);     iterator iter = readers.iterator();     int maxdoc = 0;     while (iter.hasnext()) {       indexreader r = (indexreader)iter.next();       if (r == reader) {         return maxdoc;       }        maxdoc += r.maxdoc();     }    }   return -1; // reader not in searcher } notes: this workaround makes it so you cannot serialize your custom weight implementation",
        "label": 46
    },
    {
        "text": "add assertingquery   two phase iteration to assertingscorer i am working on a similar issue with spans (lucene-6411). assertingscorer is currently only used as a top-level wrapper, and it doesnt support asserting for astwophaseiterator (wouldn't help at the moment, the way it is currently used). today some good testing of this is done, but only when randomapproximationquery is explicitly used. i think we should add assertingquery that can wrap a query with asserts and we can then have checks everywhere in a complicated tree?",
        "label": 1
    },
    {
        "text": "remove gcj indexreader specializations these specializations are outdated, unsupported, most probably pointless due to the speed of modern jvms and, i bet, nobody uses them (mike, you said you are going to ask people on java-user, anybody replied that they need it?). while giving nothing, they make segmentreader instantiation code look real ugly. if nobody objects, i'm going to post a patch that removes these from lucene.",
        "label": 33
    },
    {
        "text": "o a l messages should be moved to core contrib/queryparser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryparser. if this is truely general purpose code, it should probably be moved out of hte queryparser contrib \u2013 either into it's own contrib, or into the core (it's very small) edit: alternate suggestion to rename package to fall under the o.a.l.queryparser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib",
        "label": 29
    },
    {
        "text": "typos in lucene join package info javadoc steps: -view the \"query-time joins\" section of the lucene-join package javadoc https://lucene.apache.org/core/6_4_0/join/index.html?org/apache/lucene/search/join/package-summary.html expected behaviour: the example java code is valid. actual behaviour: there are some typos.",
        "label": 9
    },
    {
        "text": " ant dist  no longer generates md5's for the top level artifacts mark hit this for 2.9.0, and i just hit it again for 2.9.1. it used to work...",
        "label": 53
    },
    {
        "text": "remove extendedfieldcache by rolling functionality into fieldcache it is silly that we have extendedfieldcache. it is a workaround to our supposed back compatibility problem. this patch will merge the extendedfieldcache interface into fieldcache, thereby breaking back compatibility, but creating a much simpler api for fieldcache.",
        "label": 53
    },
    {
        "text": "add  testpackage  to common build xml one can define \"testcase\" to execute just one test class, which is convenient. however, i didn't notice any equivalent for testing a whole package. i find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run. following patch allows one to \"ant test -dtestpackage=search\" (for example) and run all tests under the */search/* packages in core, contrib and tags, or do \"ant test-core -dtestpackage=search\" and execute similarly just for core, or do \"ant test-core -dtestpacakge=lucene/search/function\" and run all the tests under */lucene/search/function/* (just in case there is another o.a.l.something.search.function package out there which we want to exclude.",
        "label": 33
    },
    {
        "text": "reorganize geoexactcircle tests hi karl wright, test for exact circles are getting a bit messy so i think it is a good idea to group them in their own test class.",
        "label": 25
    },
    {
        "text": "lucenetestcase ease of use improvements i started working on this in lucene-2658, here is the finished patch. there are some problems with lucenetestcase: a tests beforeclass, or the test itself (its @befores and its method), might have some random behavior, but only the latter can be reproduced with -dtests.seed if you want to do things in beforeclass, you have to use a different api: newdirectory(random) instead of newdirectory, etc. for a new user, the current output can be verbose, confusing and overwhelming. so, i refactored this class to address these problems. a class still needs 2 seeds internally, as the beforeclass will only run once, but the methods or setup() might run many times, especially when increasing iterations. but lucenetestcase deals with this, and the \"seed\" is 128-bit (uuid): the msb is initialized in beforeclass, the lsb varied for each method run. if you provide a seed with a -d, they are both fixed to the uuid you provided. i fixed the api to be consistent, so you should be able to migrate a test from setup() to beforeclass() [junit3 to junit4] without changing parameters. the codec, locale, timezone is only printed once at the end if any tests fail, as its per-class anyway (setup in beforeclass) finally, when a test fails, you get a single \"reproduce with\" command line you can copy and paste to reproduce. this way you dont have to spend time trying to figure out what the command line should be.     [junit] tests run: 2, failures: 2, errors: 0, time elapsed: 0.197 sec     [junit]     [junit] ------------- standard output ---------------     [junit] note: reproduce with: ant test -dtestcase=testexample -dtestmethod=testmethoda                -dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f     [junit] note: reproduce with: ant test -dtestcase=testexample -dtestmethod=testmethodb                -dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738     [junit] note: test params are: codec=preflex, locale=ar_ly, timezone=etc/uct     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.util.testexample failed",
        "label": 40
    },
    {
        "text": "ant nightly smoke fails in trunk i don't know the last time this was run by jenkins, but:    [smoker] verify...    [smoker]   confirm all releases have coverage in testbackwardscompatibility    [smoker]     find all past lucene releases...    [smoker]     run testbackwardscompatibility..    [smoker] releases that don't seem to be tested:    [smoker]   4.10.4 and i don't see any unsupported-4.10.4-cfs/nocfs.zip in the backwards-codec/ module (to test we do the right thing), so i think the failure is correct. i will fix this a little bit later if nobody beats me to it.",
        "label": 33
    },
    {
        "text": "class loading deadlock relating to codec initialization  default codec and spi discovery this issue came up for us several times with elasticsearch 1.3.4 (lucene 4.9.1), with many threads seeming deadlocked but runnable: \"elasticsearch[search77-es2][generic][t#43]\" #160 daemon prio=5 os_prio=0 tid=0x00007f79180c5800 nid=0x3d1f in object.wait() [0x00007f79d9289000]    java.lang.thread.state: runnable  at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:359)  at org.apache.lucene.index.segmentinfos$1.dobody(segmentinfos.java:457)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:912)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:758)  at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:453)  at org.elasticsearch.common.lucene.lucene.readsegmentinfos(lucene.java:98)  at org.elasticsearch.index.store.store.readsegmentsinfo(store.java:126)  at org.elasticsearch.index.store.store.access$300(store.java:76)  at org.elasticsearch.index.store.store$metadatasnapshot.buildmetadata(store.java:465)  at org.elasticsearch.index.store.store$metadatasnapshot.<init>(store.java:456)  at org.elasticsearch.index.store.store.readmetadatasnapshot(store.java:281)  at org.elasticsearch.indices.store.transportnodeslistshardstoremetadata.liststoremetadata(transportnodeslistshardstoremetadata.java:186)  at org.elasticsearch.indices.store.transportnodeslistshardstoremetadata.nodeoperation(transportnodeslistshardstoremetadata.java:140)  at org.elasticsearch.indices.store.transportnodeslistshardstoremetadata.nodeoperation(transportnodeslistshardstoremetadata.java:61)  at org.elasticsearch.action.support.nodes.transportnodesoperationaction$nodetransporthandler.messagereceived(transportnodesoperationaction.java:277)  at org.elasticsearch.action.support.nodes.transportnodesoperationaction$nodetransporthandler.messagereceived(transportnodesoperationaction.java:268)  at org.elasticsearch.transport.netty.messagechannelhandler$requesthandler.run(messagechannelhandler.java:275)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)  at java.lang.thread.run(thread.java:745) it didn't really make sense to see runnable threads in object.wait(), but this seems to be symptomatic of deadlocks in static initialization (http://ternarysearch.blogspot.ru/2013/07/static-initialization-deadlock.html). i found lucene-5573 as an instance of this having come up with lucene code before. i'm not sure what exactly is going on, but the deadlock in this case seems to involve these threads: \"elasticsearch[search77-es2][clusterservice#updatetask][t#1]\" #79 daemon prio=5 os_prio=0 tid=0x00007f7b155ff800 nid=0xd49 in object.wait() [0x00007f79daed8000]    java.lang.thread.state: runnable  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)  at java.lang.reflect.constructor.newinstance(constructor.java:408)  at java.lang.class.newinstance(class.java:433)  at org.apache.lucene.util.namedspiloader.reload(namedspiloader.java:67)  - locked <0x000000061fef4968> (a org.apache.lucene.util.namedspiloader)  at org.apache.lucene.util.namedspiloader.<init>(namedspiloader.java:47)  at org.apache.lucene.util.namedspiloader.<init>(namedspiloader.java:37)  at org.apache.lucene.codecs.postingsformat.<clinit>(postingsformat.java:44)  at org.elasticsearch.index.codec.postingsformat.postingformats.<clinit>(postingformats.java:67)  at org.elasticsearch.index.codec.codecmodule.configurepostingsformats(codecmodule.java:126)  at org.elasticsearch.index.codec.codecmodule.configure(codecmodule.java:178)  at org.elasticsearch.common.inject.abstractmodule.configure(abstractmodule.java:60)  - locked <0x000000061fef49e8> (a org.elasticsearch.index.codec.codecmodule)  at org.elasticsearch.common.inject.spi.elements$recordingbinder.install(elements.java:204)  at org.elasticsearch.common.inject.spi.elements.getelements(elements.java:85)  at org.elasticsearch.common.inject.injectorshell$builder.build(injectorshell.java:130)  at org.elasticsearch.common.inject.injectorbuilder.build(injectorbuilder.java:99)  - locked <0x000000061fef4c10> (a org.elasticsearch.common.inject.inheritingstate)  at org.elasticsearch.common.inject.injectorimpl.createchildinjector(injectorimpl.java:131)  at org.elasticsearch.common.inject.modulesbuilder.createchildinjector(modulesbuilder.java:69)  at org.elasticsearch.indices.internalindicesservice.createindex(internalindicesservice.java:296)  - locked <0x000000061fef4cd0> (a org.elasticsearch.indices.internalindicesservice)  at org.elasticsearch.indices.cluster.indicesclusterstateservice.applynewindices(indicesclusterstateservice.java:312)  at org.elasticsearch.indices.cluster.indicesclusterstateservice.clusterchanged(indicesclusterstateservice.java:181)  - locked <0x000000061fef4e70> (a java.lang.object)  at org.elasticsearch.cluster.service.internalclusterservice$updatetask.run(internalclusterservice.java:444)  at org.elasticsearch.common.util.concurrent.prioritizedesthreadpoolexecutor$tiebreakingprioritizedrunnable.run(prioritizedesthreadpoolexecutor.java:153)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)  at java.lang.thread.run(thread.java:745) \"elasticsearch[search77-es2][generic][t#1]\" #80 daemon prio=5 os_prio=0 tid=0x00007f794400a000 nid=0xd4b in object.wait() [0x00007f79dac56000]    java.lang.thread.state: runnable  at org.apache.lucene.codecs.simpletext.simpletextcodec.<init>(simpletextcodec.java:37)  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)  at java.lang.reflect.constructor.newinstance(constructor.java:408)  at java.lang.class.newinstance(class.java:433)  at org.apache.lucene.util.namedspiloader.reload(namedspiloader.java:67)  - locked <0x000000061fcf1f50> (a org.apache.lucene.util.namedspiloader)  at org.apache.lucene.util.namedspiloader.<init>(namedspiloader.java:47)  at org.apache.lucene.util.namedspiloader.<init>(namedspiloader.java:37)  at org.apache.lucene.codecs.codec.<clinit>(codec.java:41)  at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:359)  at org.apache.lucene.index.segmentinfos$1.dobody(segmentinfos.java:457)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:912)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:758)  at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:453)  at org.elasticsearch.common.lucene.lucene.readsegmentinfos(lucene.java:98)  at org.elasticsearch.index.store.store.readsegmentsinfo(store.java:126)  at org.elasticsearch.index.store.store.access$300(store.java:76)  at org.elasticsearch.index.store.store$metadatasnapshot.buildmetadata(store.java:465)  at org.elasticsearch.index.store.store$metadatasnapshot.<init>(store.java:456)  at org.elasticsearch.index.store.store.readmetadatasnapshot(store.java:281)  at org.elasticsearch.indices.store.transportnodeslistshardstoremetadata.liststoremetadata(transportnodeslistshardstoremetadata.java:186)  at org.elasticsearch.indices.store.transportnodeslistshardstoremetadata.nodeoperation(transportnodeslistshardstoremetadata.java:140)  at org.elasticsearch.indices.store.transportnodeslistshardstoremetadata.nodeoperation(transportnodeslistshardstoremetadata.java:61)  at org.elasticsearch.action.support.nodes.transportnodesoperationaction$nodetransporthandler.messagereceived(transportnodesoperationaction.java:277)  at org.elasticsearch.action.support.nodes.transportnodesoperationaction$nodetransporthandler.messagereceived(transportnodesoperationaction.java:268)  at org.elasticsearch.transport.netty.messagechannelhandler$requesthandler.run(messagechannelhandler.java:275)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)  at java.lang.thread.run(thread.java:745) full thread dump: https://gist.github.com/shikhar/d0f6d2d008f45d2d4f91",
        "label": 53
    },
    {
        "text": "clicking on the  more results  link in luceneweb war demo results in arrayindexoutofboundsexception summary says it all.",
        "label": 40
    },
    {
        "text": "booleanquery can not find all matches in special condition query: (name:tang*) doc=5137 score=1.0 doc:document<stored,indexed<name:tangfulin>> doc=11377 score=1.0 doc:document<stored,indexed<name:tangfulin>> query: name:tang* name:notexistnames doc=5137 score=0.048133932 doc:document<stored,indexed<name:tangfulin>> it is two queries on the same index, one is just a prefix query in a boolean query, and the other is a prefix query plus a term query in a boolean query, all with occur.should . what i wonder is why the later query can not find the doc=11377 doc ? the problem can be repreduced by the code in the attachment .",
        "label": 33
    },
    {
        "text": "contrib analyzers need tests the analyzers in contrib need tests, preferably ones that test the behavior of all the token 'attributes' involved (offsets, type, etc) and not just what they do with token text. this way, they can be converted to the new api without breakage.",
        "label": 40
    },
    {
        "text": "automaton fuzzy query doesn't deliver all results having a small index with n documents where each document has one of the following terms: weber, webe, web, wbr, we, (and some more) the new fuzzyquery (automaton) with maxedits=2 only delivers the expected terms weber and webe in the rewritten query. the expected terms web and wbr which have an edit distance of 2 as well are missing.",
        "label": 40
    },
    {
        "text": "mating collector and scorer on doc id orderness this is a spin off of lucene-1593. this issue proposes to expose appropriate api on scorer and collector such that one can create an optimized collector based on a given scorer's doc-id orderness and vice versa. copied from lucene-1593, here is the list of changes: deprecate weight and create queryweight (abstract class) with a new scorer(reader, scoredocsinorder), replacing the current scorer(reader) method. queryweight implements weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoredocsinorder) is defined abstract. also add queryweightwrapper to wrap a given weight implementation. this one will also be deprecated, as well as package-private. add to query variants of createweight and weight which return queryweight. for now, i prefer to add a default impl which wraps the weight variant instead of overriding in all query extensions, and in 3.0 when we remove the weight variants - override in all extending classes. add to scorer isoutoforder with a default to false, and override in bs to true. modify booleanweight to extend queryweight and implement the new scorer method to return bs2 or bs based on the number of required scorers and setallowoutoforder. add to collector an abstract acceptsdocsoutoforder which returns true/false. use it in indexsearcher.search methods, that accept a collector, in order to create the appropriate scorer, using the new queryweight. provide a static create method to tfc and tsdc which accept this as an argument and creates the proper instance. wherever we create a collector (tsdc or tfc), always ask for out-of-order scorer and check on the resulting scorer isoutoforder(), so that we can create the optimized collector instance. modify indexsearcher to use all of the above logic. the only class i'm worried about, and would like to verify with you, is searchable. if we want to deprecate all the search methods on indexsearcher, searcher and searchable which accept weight and add new ones which accept queryweight, we must do the following: deprecate searchable in favor of searcher. add to searcher the new queryweight variants. here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new collector method) or (2) add them with a default impl to call the weight versions, documenting these will become abstract in 3.0. have searcher extend unicastremoteobject and have remotesearchable extend searcher. that's the part i'm a little bit worried about - searchable implements java.rmi.remote, which means there could be an implementation out there which implements searchable and extends something different than unicastremoteobject, like activeable. i think there is very small chance this has actually happened, but would like to confirm with you guys first. add a deprecated, package-private, searchablewrapper which extends searcher and delegates all calls to the searchable member. deprecate all uses of searchable and add searcher instead, defaulting the old ones to use searchablewrapper. make all the necessary changes to indexsearcher, multisearcher etc. regarding overriding these new methods. one other optimization that was discussed in lucene-1593 is to expose a topscorer() api (on weight) which returns a scorer that its score(collector) will be called, and additionally add a start() method to disi. that will allow scorers to initialize either on start() or score(collector). this was proposed mainly because of bs and bs2 which check if they are initialized in every call to next(), skipto() and score(). personally i prefer to see that in a separate issue, following that one (as it might add methods to queryweight).",
        "label": 33
    },
    {
        "text": "adding kamikaze to lucene contrib adding kamikaze to lucene contrib",
        "label": 33
    },
    {
        "text": "using stringfield for while storing becomes a regular field when the document is retrieved   due to this the next search cannot find the document because the value of the field got tokenized which was not desired when it was added in this case, i don't want lucene to tokenize the value for a field that is being added to the document in the index and hence stringfield is used. once we do a search using one of the field values, we get the document object from the searcher but the type of the field becomes field instead of the stringfield. so when i try to update the value of one of the fields in the document and then do another seacrh using the same term, the 2nd search fails to find this document. in order to reproduce the case, please use the code below. sample class: ///////////// package com.thegoldensource.demo; import java.io.ioexception; import java.util.list; import org.apache.lucene.analysis.analyzer; import org.apache.lucene.analysis.core.whitespaceanalyzer; import org.apache.lucene.document.document; import org.apache.lucene.document.field.store; import org.apache.lucene.document.stringfield; import org.apache.lucene.index.directoryreader; import org.apache.lucene.index.indexwriter; import org.apache.lucene.index.indexwriterconfig; import org.apache.lucene.index.indexablefield; import org.apache.lucene.index.term; import org.apache.lucene.search.indexsearcher; import org.apache.lucene.search.scoredoc; import org.apache.lucene.search.termquery; import org.apache.lucene.search.topdocs; import org.apache.lucene.store.directory; import org.apache.lucene.store.ramdirectory; import org.apache.lucene.util.version; /** * @author rparekh * */ public class simpletest { /** @param args */ public static void main(string[] args) { try { analyzer analyzer = new whitespaceanalyzer(version.lucene_40); // store the index in memory: directory directory = new ramdirectory(); // to store an index on disk, use this instead: //directory directory = fsdirectory.open(\"/tmp/testindex\"); indexwriterconfig config = new indexwriterconfig(version.lucene_30, analyzer); indexwriter iwriter = new indexwriter(directory, config); document doc = new document(); string text = \"this is the text\"; doc.add(new stringfield(\"id\", \"a\", store.no)); doc.add(new stringfield(\"content\", text, store.yes)); iwriter.adddocument(doc); iwriter.commit(); // now search the index: directoryreader ireader = directoryreader.open(directory); indexsearcher isearcher = new indexsearcher(ireader); term myterm = new term(\"id\", \"a\"); termquery query = new termquery(myterm); topdocs docs = isearcher.search(query, 1); int hits = docs.totalhits; system.out.println(\"hits : \" + hits); scoredoc[] documents = docs.scoredocs; document d = null; for(int i=0; i < documents.length; i++) { d = isearcher.doc(documents[i].doc); indexablefield contentfield = d.getfield(\"content\"); if(contentfield != null) { system.out.println(\"content from doc : [\" + contentfield.stringvalue() + \"]\"); } } // for updating the value of a field, remove the field and add it again. d.removefield(\"content\"); d.add(new stringfield(\"content\", \"new content\",store.yes)); list<indexablefield> fields = d.getfields(); iwriter.updatedocument(myterm, fields); iwriter.commit(); iwriter.close(); // search the document again directoryreader newreader = directoryreader.open(directory); indexsearcher newseracher = new indexsearcher(newreader); termquery newtermquery = new termquery(myterm); topdocs newtopdocs = newseracher.search(newtermquery, 10); int hits1 = newtopdocs.totalhits; // number of hits should be 1 but it is 0 (zero) - this is because the type of // fields in the document that was retrieved changes from the original stringfield to field which is not correct system.out.println(\"hits again : \" + hits1 ); if(hits1 > 0) { scoredoc[] documents1 = newtopdocs.scoredocs; document d1 = newseracher.doc(documents1[0].doc); indexablefield newcontent = d1.getfield(\"content\"); if(newcontent != null) { system.out.println(\"new content : [\" + newcontent.stringvalue() + \"]\"); } } ireader.close(); directory.close(); } catch(ioexception e) { system.err.print(e); } } } ///////////// output: hits : 1 content from doc : [this is the text] hits again : 0",
        "label": 53
    },
    {
        "text": "implement multifacets getalldims drillsideways.drillsidewaysresult uses facets when the query does not filter by a facet, but it uses multifacets when it does, and multifacets implementation is not complete. see: https://github.com/apache/lucene-solr/blob/0b0bc89932622f5bc2c4d74f978178b9ae15c700/lucene/facet/src/java/org/apache/lucene/facet/multifacets.java#l67 see http://pastebin.com/5edbtm2v this code works when drilldownquery.add is not called (when there is no facets selected), but it fails with an unsupportedoperationexception. perhaps i'm not using facets correctly, but i'm trying to figure it out to upgrade from 4.6.1 by my self as i could not find a documentation other than javadocs for facets.",
        "label": 33
    },
    {
        "text": "slightly more readable code in token termattributeimpl no big deal. growtermbuffer(int newsize) was using correct, but slightly hard to follow code. the method was returning null as a hint that the current termbuffer has enough space to the upstream code or reallocated buffer. this patch simplifies logic making this method to only reallocate buffer, nothing more. it reduces number of if(null) checks in a few methods and reduces amount of code. all tests pass. this also adds tests for the new basic attribute impls (copies of the token tests).",
        "label": 53
    },
    {
        "text": "genericize directiolinuxdir   unixdir today directiolinuxdir is tricky/dangerous to use, because you only want to use it for indexwriter and not indexreader (searching). it's a trap. but, once we do lucene-2793, we can make it fully general purpose because then a single native dir impl can be used. i'd also like to make it generic to other unices, if we can, so that it becomes unixdirectory.",
        "label": 54
    },
    {
        "text": "background merge hit exception   caused by  java lang arrayindexoutofboundsexception forcemerge(80) ============================== caused by: java.io.ioexception: background merge hit exception: _3h(4.4):c79921/2994 _3vs(4.4):c38658 _eq(4.4):c38586 _h1(4.4):c37370 _16k(4.4):c36591 _j4(4.4):c34316 _dx(4.4):c30550 _3m6(4.4):c30058 _dl(4.4):c28440 _d8(4.4):c19599 _dy(4.4):c1500/75 _h2(4.4):c1500 into _3vt [maxnumsegments=80] at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1714) at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1650) at com.xxx.yyy.engine.lucene.luceneengine.flushandreopen(luceneengine.java:1295) ... 4 more caused by: java.lang.arrayindexoutofboundsexception: 2 at org.apache.lucene.util.pagedbytes$reader.fillslice(pagedbytes.java:92) at org.apache.lucene.codecs.lucene42.lucene42docvaluesproducer$6.get(lucene42docvaluesproducer.java:267) at org.apache.lucene.codecs.docvaluesconsumer$2$1.setnext(docvaluesconsumer.java:239) at org.apache.lucene.codecs.docvaluesconsumer$2$1.hasnext(docvaluesconsumer.java:201) at org.apache.lucene.codecs.lucene42.lucene42docvaluesconsumer.addbinaryfield(lucene42docvaluesconsumer.java:218) at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addbinaryfield(perfielddocvaluesformat.java:110) at org.apache.lucene.codecs.docvaluesconsumer.mergebinaryfield(docvaluesconsumer.java:186) at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:171) at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:108) at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:3772) at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3376) at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:405) at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:482) ===============",
        "label": 33
    },
    {
        "text": "use covariant clone  return types paul cowan wrote in lucene-1257: ok, thought i'd jump in and help out here with one of my java 5 favourites. haven't seen anyone discuss this, and don't believe any of the patches address this, so thought i'd throw a patch out there (against svn head @ revision 827821) which uses java 5 covariant return types for (almost) all of the object#clone() implementations in core. i.e. this: public object clone() { changes to: public spannotquery clone() { which lets us get rid of a whole bunch of now-unnecessary casts, so e.g. if (clone == null) clone = (spannotquery) this.clone(); becomes if (clone == null) clone = this.clone(); almost everything has been done and all downcasts removed, in core, with the exception of some spanquery stuff, where it's assumed that it's safe to cast the clone() of a spanquery to a spanquery - this can't be made covariant without declaring \"abstract spanquery clone()\" in spanquery itself, which breaks those spanquerys that don't declare their own clone() some indexreaders, e.g. directoryreader - we can't be more specific than changing .clone() to return indexreader, because it returns the result of indexreader.clone(boolean). we could use covariant types for that, which would work fine, but that didn't follow the pattern of the others so that could be a later commit. two changes were also made in contrib/, where not making the changes would have broken code by trying to widen indexinput#clone() back out to returning object, which is not permitted. contrib/ was otherwise left untouched. let me know what you think, or if you have any other questions.",
        "label": 42
    },
    {
        "text": "tessellator fails when filtering coplanar points when creating linked list currently when creating the linked list on the tessellator, coplanar points are filtered. the problem is the following:  if we have three coplanar points, the code is actually removing the last point, instead it should remove the middle one.    ",
        "label": 19
    },
    {
        "text": "load rat via ivy for rat sources task we now fail the build on rat problems (lucene-1866), so we should make it easy to run rat-sources for people to test locally (it takes like 3 seconds total for the whole trunk) also this is safer than putting rat in your ~/.ant/lib because that adds some classes from commons to your ant classpath (which we currently wrongly use in compile).",
        "label": 53
    },
    {
        "text": "rename analyzer reusabletokenstream  to tokenstream  all analysis consumers now use reusabletokenstream(). to finally make reuse mandatory, lets rename resuabletokenstream() to tokenstream() (removing the old tokenstream() method).",
        "label": 7
    },
    {
        "text": "indexwriter trydeletedocument does not work i am using \"fresh\"a and opened reader. one segement and 3 documents in index. trydeletedocument always return false, i deep into your code, and see follow, that segmentinfos.indexof(info) always return -1 because org.apache.lucene.index.segmentinfopercommit doesnot have equals method, see screenshoot for more inforamtion http://postimg.org/image/jvtezvqnn/",
        "label": 33
    },
    {
        "text": "move xml queryparser to queryparser module the xml queryparser will be ported across to queryparser module. as part of this work, we'll move the qp's demo into the demo module.",
        "label": 7
    },
    {
        "text": "reproducing testfsts testbasicfsa  failure from https://jenkins.thetaphi.de/job/lucene-solr-badapples-master-linux/104/: checking out revision 8d205ecd1c6a133f7cb9a4352388ec30d00b4bdb (refs/remotes/origin/master) [...]    [junit4] suite: org.apache.lucene.util.fst.testfsts    [junit4]   2> note: reproduce with: ant test  -dtestcase=testfsts -dtests.method=testbasicfsa -dtests.seed=82d30036e9484ce9 -dtests.multiplier=3 -dtests.slow=true -dtests.badapples=true -dtests.locale=ckb-ir -dtests.timezone=africa/malabo -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] failure 0.18s j1 | testfsts.testbasicfsa <<<    [junit4]    > throwable #1: java.lang.assertionerror: expected:<24> but was:<22>    [junit4]    >  at __randomizedtesting.seedinfo.seed([82d30036e9484ce9:5baebe18fd0445d5]:0)    [junit4]    >  at org.apache.lucene.util.fst.testfsts.testbasicfsa(testfsts.java:166)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    >  at java.base/java.lang.thread.run(thread.java:844) [...]    [junit4]   2> note: test params are: codec=asserting(lucene80): {date=postingsformat(name=asserting), field=postingsformat(name=asserting), docid=postingsformat(name=lucenefixedgap), titletokenized=blocktreeords(blocksize=128), id=postingsformat(name=lucenefixedgap), body=postingsformat(name=asserting), title=postingsformat(name=lucenevargapfixedinterval)}, docvalues:{docid_intdv=docvaluesformat(name=lucene70), titledv=docvaluesformat(name=asserting)}, maxpointsinleafnode=1415, maxmbsortinheap=5.567002115183062, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@54396619), locale=ckb-ir, timezone=africa/malabo    [junit4]   2> note: linux 4.15.0-36-generic amd64/oracle corporation 9.0.4 (64-bit)/cpus=8,threads=1,free=171269088,total=460849152",
        "label": 11
    },
    {
        "text": "testindexwriterexceptions testnolostdeletesorupdates failure from http://jenkins.thetaphi.de/job/lucene-solr-5.x-linux/13910/, reproduces about 10% of the time for me with beasting on linux, on both branch_5x/java7 and trunk/java8:    [junit4] error   3.32s j1 | testindexwriterexceptions.testnolostdeletesorupdates <<<    [junit4]    > throwable #1: org.apache.lucene.store.alreadyclosedexception: this indexwriter is closed    [junit4]    >  at __randomizedtesting.seedinfo.seed([1cde4cd4545e2ea2:75a54e5fa2f0e4e2]:0)    [junit4]    >  at org.apache.lucene.index.indexwriter.ensureopen(indexwriter.java:719)    [junit4]    >  at org.apache.lucene.index.indexwriter.getconfig(indexwriter.java:1046)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.commit(randomindexwriter.java:283)    [junit4]    >  at org.apache.lucene.index.testindexwriterexceptions.testnolostdeletesorupdates(testindexwriterexceptions.java:2072)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: org.apache.lucene.store.mockdirectorywrapper$fakeioexception    [junit4]    >  at org.apache.lucene.index.testindexwriterexceptions$11.eval(testindexwriterexceptions.java:1923)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.maybethrowdeterministicexception(mockdirectorywrapper.java:958)    [junit4]    >  at org.apache.lucene.store.mockindexoutputwrapper.writebytes(mockindexoutputwrapper.java:145)    [junit4]    >  at org.apache.lucene.store.mockindexoutputwrapper.writebyte(mockindexoutputwrapper.java:127)    [junit4]    >  at org.apache.lucene.store.dataoutput.writevint(dataoutput.java:191)    [junit4]    >  at org.apache.lucene.codecs.lucene50.lucene50docvaluesconsumer.addnumericfield(lucene50docvaluesconsumer.java:163)    [junit4]    >  at org.apache.lucene.codecs.lucene50.lucene50docvaluesconsumer.addnumericfield(lucene50docvaluesconsumer.java:80)    [junit4]    >  at org.apache.lucene.codecs.asserting.assertingdocvaluesformat$assertingdocvaluesconsumer.addnumericfield(assertingdocvaluesformat.java:89)    [junit4]    >  at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addnumericfield(perfielddocvaluesformat.java:111)    [junit4]    >  at org.apache.lucene.index.readersandupdates.handlenumericdvupdates(readersandupdates.java:328)    [junit4]    >  at org.apache.lucene.index.readersandupdates.writefieldupdates(readersandupdates.java:521)    [junit4]    >  at org.apache.lucene.index.indexwriter.commitmergeddeletesandupdates(indexwriter.java:3417)    [junit4]    >  at org.apache.lucene.index.indexwriter.commitmerge(indexwriter.java:3485)    [junit4]    >  at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4212)    [junit4]    >  at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3644)    [junit4]    >  at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >  at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2> note: leaving temporary files on disk at: /home/jenkins/workspace/lucene-solr-5.x-linux/lucene/build/core/test/j1/temp/lucene.index.testindexwriterexceptions_1cde4cd4545e2ea2-001    [junit4]   2> note: test params are: codec=asserting(lucene53): {content7=lucene50(blocksize=128), contents=lucene50(blocksize=128), crash=blocktreeords(blocksize=128), foo=lucene50(blocksize=128), id=blocktreeords(blocksize=128), content=lucene50(blocksize=128), content2=blocktreeords(blocksize=128), content5=postingsformat(name=lucenevargapfixedinterval), field=lucene50(blocksize=128), content6=blocktreeords(blocksize=128), content1=postingsformat(name=lucenevargapfixedinterval), subid=blocktreeords(blocksize=128), field2=postingsformat(name=lucenevargapfixedinterval), field1=lucene50(blocksize=128), content4=lucene50(blocksize=128)}, docvalues:{f=docvaluesformat(name=lucene50), bcf=docvaluesformat(name=memory), sortednumericdv=docvaluesformat(name=lucene50), numericdv=docvaluesformat(name=asserting), bf=docvaluesformat(name=lucene50), binarydv=docvaluesformat(name=asserting), sortedsetdv=docvaluesformat(name=asserting), cf=docvaluesformat(name=asserting), sorteddv=docvaluesformat(name=asserting)}, sim=randomsimilarityprovider(querynorm=false,coord=crazy): {contents=dfr i(ne)l3(800.0), id=ib spl-l2, content=dfr i(ne)b2, content2=dfr i(n)b1, field=dfr i(n)3(800.0), content6=lm jelinek-mercer(0.100000), content1=dfr i(ne)z(0.3), content4=dfr gb1}, locale=zh_tw, timezone=america/mexico_city    [junit4]   2> note: linux 3.19.0-26-generic i386/oracle corporation 1.7.0_80 (32-bit)/cpus=12,threads=1,free=164600216,total=306622464",
        "label": 33
    },
    {
        "text": "micro benchmarks for ntz and pop  bitutils  operations  as suggested by yonik, i performed a suite of micro-benchmarks to investigate the following: pop() (bitcount) seems to be implemented in the same way (\"hacker's delight\") as in the bitutils class (sun's standard library from version 1.5). native intrinsics have been recently added to the hotspot that should speed up bitcount significantly. i have tried to run the code on various vms and architectures, but of course the results may vary depending on the setting. micro-benchmark code, evaluation the micro-benchmarks were written as junit tests with a custom set of rules that repeats each test measuring execution time, gc use, etc. there were 5 warmup runs before each test, followed by 15 benchmarked runs. the result contain overall times, round times and standard deviations where applicable. there were several tests for isolated performance of bitutil.pop(), jdk's bitcount, bitutil.ntz(), bitutil.ntz2(), bitutil.ntz3() and jdk's numberoftrailingzeros, the test code had the following loop: final long [] longs = ntzpopbenchmark.random.bits; int cnt = 0; for (int i = longs.length; --i >= 0;) {    cnt += long.bitcount(longs[i]); } volatilevariable = cnt; // to prevent dead code removal. i also added another version of pop() based on a precomputed bit counts. this version was called pop2. the input array of long values was initialized to a memory taking 200mb. there were two different sets: random (random values) and single (single bit rotated to the right in each long). there were tests that called bitutil.pop_xor between the two input bitsets (random, single). additional tests that used iterators and and other bitutil operations showed similar performance to isolated loops, so i omit them here. evaluation environment i tested on three different machines: pentium 4, 32-bit, 3ghz, 2gb ram (windows) amd athlon(tm) 64 x2 dual core processor 5200+, 64-bit, 4gb ram (ubuntu) intel(r) core(tm)2 quad cpu q9650 @ 3.00ghz, 64-bit, 4gb ram (ubuntu) and on various vms: 1.6.0_17, java hotspot(tm) server vm, 14.3-b01, sun microsystems inc., 1.5.0_18, java hotspot(tm) server vm, 1.5.0_18-b02, sun microsystems inc., 1.7.0-ea, java hotspot(tm) server vm, 17.0-b06, sun microsystems inc., 1.6.0, ibm j9 vm, 2.4, ibm corporation, bea jrockit. (ant other minor versions of the vms above, depending on the computer). results overview pop the times between bitutil and jdk were mostly identical. however, on 32-bit systems, precached pop2 performed much better. examples: # 1.6.0_17, java hotspot(tm) server vm, 14.3-b01, sun microsystems inc.,  test_pop_jdk_random       : 15/20 rounds, time.total: 15.61, time.warmup: 4.31, time.bench: 11.30, round: 0.75 [+- 0.02] test_pop_jdk_single       : 15/20 rounds, time.total: 15.67, time.warmup: 4.31, time.bench: 11.36, round: 0.76 [+- 0.02] test_pop_bitutil_random   : 15/20 rounds, time.total: 15.55, time.warmup: 4.33, time.bench: 11.22, round: 0.75 [+- 0.01] test_pop_bitutil_single   : 15/20 rounds, time.total: 15.55, time.warmup: 4.31, time.bench: 11.23, round: 0.75 [+- 0.01] test_pop2_random          : 15/20 rounds, time.total:  6.69, time.warmup: 1.75, time.bench:  4.94, round: 0.33 [+- 0.00] test_pop2_single          : 15/20 rounds, time.total:  4.66, time.warmup: 1.22, time.bench:  3.44, round: 0.23 [+- 0.01] note the difference between random and single distributions \u2013 most probably due to more cache hits when referring to the lookup table. other vms on this 32-bit machine: # 1.5.0_18, java hotspot(tm) server vm, 1.5.0_18-b02, sun microsystems inc.,  test_pop_jdk_random       : 15/20 rounds, time.total: 20.67, time.warmup: 5.19, time.bench: 15.48, round: 1.03 [+- 0.01] test_pop_jdk_single       : 15/20 rounds, time.total: 22.70, time.warmup: 5.63, time.bench: 17.08, round: 1.14 [+- 0.01] test_pop_bitutil_random   : 15/20 rounds, time.total: 22.69, time.warmup: 5.63, time.bench: 17.06, round: 1.14 [+- 0.01] test_pop_bitutil_single   : 15/20 rounds, time.total: 20.67, time.warmup: 5.19, time.bench: 15.48, round: 1.03 [+- 0.01] test_pop2_random          : 15/20 rounds, time.total:  6.30, time.warmup: 1.63, time.bench:  4.67, round: 0.31 [+- 0.01] test_pop2_single          : 15/20 rounds, time.total:  4.33, time.warmup: 1.16, time.bench:  3.17, round: 0.21 [+- 0.01] # 1.7.0-ea, java hotspot(tm) server vm, 17.0-b06, sun microsystems inc.,  test_pop_jdk_random       : 15/20 rounds, time.total: 15.28, time.warmup: 4.25, time.bench: 11.03, round: 0.74 [+- 0.03] test_pop_jdk_single       : 15/20 rounds, time.total: 15.16, time.warmup: 4.20, time.bench: 10.95, round: 0.73 [+- 0.01] test_pop_bitutil_random   : 15/20 rounds, time.total: 15.12, time.warmup: 4.20, time.bench: 10.92, round: 0.73 [+- 0.01] test_pop_bitutil_single   : 15/20 rounds, time.total: 15.13, time.warmup: 4.25, time.bench: 10.88, round: 0.73 [+- 0.01] test_pop2_random          : 15/20 rounds, time.total:  6.78, time.warmup: 1.72, time.bench:  5.06, round: 0.34 [+- 0.01] test_pop2_single          : 15/20 rounds, time.total:  4.72, time.warmup: 1.20, time.bench:  3.52, round: 0.23 [+- 0.02] on 64-bit machines, the results are nearly equal, with pop2 performing slightly worse on sun's 1.6 compared to jdk and bitutil (but this difference is really tiny and not present on all vms; see ibm j9 and sun's 1.5 below). # 1.6.0_16, java hotspot(tm) 64-bit server vm, 14.2-b01, sun microsystems inc., test_pop_jdk_random       : 15/20 rounds, time.total: 3.27, time.warmup: 0.81, time.bench: 2.46, round: 0.16 [+- 0.00] test_pop_jdk_single       : 15/20 rounds, time.total: 3.11, time.warmup: 0.76, time.bench: 2.34, round: 0.16 [+- 0.02] test_pop_bitutil_random   : 15/20 rounds, time.total: 3.27, time.warmup: 0.81, time.bench: 2.46, round: 0.16 [+- 0.00] test_pop_bitutil_single   : 15/20 rounds, time.total: 3.03, time.warmup: 0.77, time.bench: 2.26, round: 0.15 [+- 0.00] test_pop2_random          : 15/20 rounds, time.total: 3.63, time.warmup: 0.93, time.bench: 2.70, round: 0.18 [+- 0.00] test_pop2_single          : 15/20 rounds, time.total: 3.51, time.warmup: 0.89, time.bench: 2.62, round: 0.17 [+- 0.00] # 1.6.0, ibm j9 vm, 2.4, ibm corporation, test_pop_jdk_random       : 15/20 rounds, time.total: 4.80, time.warmup: 1.24, time.bench: 3.57, round: 0.24 [+- 0.01] test_pop_jdk_single       : 15/20 rounds, time.total: 5.00, time.warmup: 1.44, time.bench: 3.56, round: 0.24 [+- 0.01] test_pop_bitutil_random   : 15/20 rounds, time.total: 4.81, time.warmup: 1.24, time.bench: 3.56, round: 0.24 [+- 0.01] test_pop_bitutil_single   : 15/20 rounds, time.total: 4.75, time.warmup: 1.19, time.bench: 3.56, round: 0.24 [+- 0.01] test_pop2_random          : 15/20 rounds, time.total: 3.65, time.warmup: 0.90, time.bench: 2.76, round: 0.18 [+- 0.00] test_pop2_single          : 15/20 rounds, time.total: 3.82, time.warmup: 0.93, time.bench: 2.89, round: 0.19 [+- 0.01] # 1.5.0_18, java hotspot(tm) 64-bit server vm, 1.5.0_18-b02, sun microsystems inc., test_pop_jdk_random       : 15/20 rounds, time.total: 3.72, time.warmup: 0.94, time.bench: 2.78, round: 0.19 [+- 0.00] test_pop_jdk_single       : 15/20 rounds, time.total: 5.96, time.warmup: 1.40, time.bench: 4.56, round: 0.30 [+- 0.00] test_pop_bitutil_random   : 15/20 rounds, time.total: 6.16, time.warmup: 1.43, time.bench: 4.73, round: 0.31 [+- 0.00] test_pop_bitutil_single   : 15/20 rounds, time.total: 3.62, time.warmup: 0.92, time.bench: 2.70, round: 0.18 [+- 0.00] test_pop2_random          : 15/20 rounds, time.total: 3.70, time.warmup: 0.96, time.bench: 2.74, round: 0.18 [+- 0.00] test_pop2_single          : 15/20 rounds, time.total: 3.57, time.warmup: 0.93, time.bench: 2.65, round: 0.18 [+- 0.00] the other 64-bit machine (quad-core): # 1.7.0-ea, java hotspot(tm) 64-bit server vm, 17.0-b06, sun microsystems inc., test_pop_jdk_random       : 15/20 rounds, time.total: 2.46, time.warmup: 0.62, time.bench: 1.84, round: 0.12 [+- 0.00] test_pop_jdk_single       : 15/20 rounds, time.total: 2.49, time.warmup: 0.62, time.bench: 1.87, round: 0.12 [+- 0.01] test_pop_bitutil_random   : 15/20 rounds, time.total: 2.47, time.warmup: 0.62, time.bench: 1.84, round: 0.12 [+- 0.00] test_pop_bitutil_single   : 15/20 rounds, time.total: 2.46, time.warmup: 0.62, time.bench: 1.84, round: 0.12 [+- 0.00] test_pop2_random          : 15/20 rounds, time.total: 2.82, time.warmup: 0.71, time.bench: 2.11, round: 0.14 [+- 0.00] test_pop2_single          : 15/20 rounds, time.total: 2.15, time.warmup: 0.55, time.bench: 1.61, round: 0.11 [+- 0.00] i then replaced bitutil.pop with bitutil.pop2 in bit-counting methods like xor/and/or. the results are intriguing. on 32-bit systems, there is a measureable gain, like here: # 1.6.0_17, java hotspot(tm) server vm, 14.3-b01, sun microsystems inc.,  test_pop_xor              : 15/20 rounds, time.total:  9.78, time.warmup: 2.59, time.bench:  7.19, round: 0.48 [+- 0.01] test_pop2_hd_xor          : 15/20 rounds, time.total:  8.27, time.warmup: 2.22, time.bench:  6.05, round: 0.40 [+- 0.01] # 1.7.0-ea, java hotspot(tm) server vm, 17.0-b06, sun microsystems inc.,  test_pop_xor              : 15/20 rounds, time.total:  9.89, time.warmup: 2.59, time.bench: 7.30, round: 0.49 [+- 0.02] test_pop2_hd_xor          : 15/20 rounds, time.total:  8.20, time.warmup: 2.24, time.bench: 5.97, round: 0.40 [+- 0.01] on 64-bit systems, when 64-bit values can be manipulated directly in registers, there was nearly no speedup or even a small performance penalty like in here: # 1.7.0-ea, java hotspot(tm) 64-bit server vm, 17.0-b06, sun microsystems inc., test_pop_xor              : 15/20 rounds, time.total: 1.76, time.warmup: 0.49, time.bench: 1.27, round: 0.09 [+- 0.00] test_pop2_hd_xor          : 15/20 rounds, time.total: 2.06, time.warmup: 0.55, time.bench: 1.51, round: 0.10 [+- 0.00] i'm guessing referencing memory on this fast processors is slower than manipulating registers. ntz on jvms prior to 1.7, the {{ntz} version from lucene was much faster in my tests than the one from jdk, but it also has a greater variance depending on the input bits' distribution (compare the same routine for random and single below). # 32-bit system; # 1.6.0_17, java hotspot(tm) server vm, 14.3-b01, sun microsystems inc.,  test_ntz_jdk_random       : 15/20 rounds, time.total:  6.69, time.warmup: 1.73, time.bench: 4.95, round: 0.33 [+- 0.01] test_ntz_jdk_single       : 15/20 rounds, time.total:  7.59, time.warmup: 1.94, time.bench: 5.66, round: 0.38 [+- 0.01] test_ntz_bitutil_random   : 15/20 rounds, time.total:  2.72, time.warmup: 0.73, time.bench: 1.98, round: 0.13 [+- 0.02] test_ntz_bitutil_single   : 15/20 rounds, time.total:  5.28, time.warmup: 1.34, time.bench: 3.94, round: 0.26 [+- 0.02] test_ntz2_bitutil_random  : 15/20 rounds, time.total:  3.06, time.warmup: 0.81, time.bench: 2.25, round: 0.15 [+- 0.01] test_ntz2_bitutil_single  : 15/20 rounds, time.total:  5.36, time.warmup: 1.34, time.bench: 4.02, round: 0.27 [+- 0.01] test_ntz3_bitutil_random  : 15/20 rounds, time.total:  5.80, time.warmup: 1.48, time.bench: 4.31, round: 0.29 [+- 0.01] test_ntz3_bitutil_single  : 15/20 rounds, time.total:  6.98, time.warmup: 1.81, time.bench: 5.17, round: 0.34 [+- 0.01] # 64-bit athlon # 1.6.0_16, java hotspot(tm) 64-bit server vm, 14.2-b01, sun microsystems inc., test_ntz_jdk_random       : 15/20 rounds, time.total: 4.59, time.warmup: 1.16, time.bench: 3.44, round: 0.23 [+- 0.00] test_ntz_jdk_single       : 15/20 rounds, time.total: 6.64, time.warmup: 1.59, time.bench: 5.04, round: 0.34 [+- 0.01] test_ntz_bitutil_random   : 15/20 rounds, time.total: 2.09, time.warmup: 0.53, time.bench: 1.56, round: 0.10 [+- 0.00] test_ntz_bitutil_single   : 15/20 rounds, time.total: 3.87, time.warmup: 0.98, time.bench: 2.90, round: 0.19 [+- 0.00] test_ntz2_bitutil_random  : 15/20 rounds, time.total: 2.09, time.warmup: 0.52, time.bench: 1.57, round: 0.10 [+- 0.00] test_ntz2_bitutil_single  : 15/20 rounds, time.total: 3.31, time.warmup: 0.84, time.bench: 2.47, round: 0.16 [+- 0.00] test_ntz3_bitutil_random  : 15/20 rounds, time.total: 3.31, time.warmup: 0.83, time.bench: 2.48, round: 0.17 [+- 0.00] test_ntz3_bitutil_single  : 15/20 rounds, time.total: 5.71, time.warmup: 1.39, time.bench: 4.32, round: 0.29 [+- 0.00] but then comes the 1.7 hotsport and things change radically, on 32-bit system the jdk's version is much faster for nearly-empty long values: # 1.7.0-ea, java hotspot(tm) server vm, 17.0-b06, sun microsystems inc.,  test_ntz_jdk_random       : 15/20 rounds, time.total:  1.97, time.warmup: 0.61, time.bench: 1.36, round: 0.09 [+- 0.01] test_ntz_jdk_single       : 15/20 rounds, time.total:  2.53, time.warmup: 0.77, time.bench: 1.77, round: 0.12 [+- 0.01] test_ntz_bitutil_random   : 15/20 rounds, time.total:  2.36, time.warmup: 0.66, time.bench: 1.70, round: 0.11 [+- 0.01] test_ntz_bitutil_single   : 15/20 rounds, time.total:  4.50, time.warmup: 1.19, time.bench: 3.31, round: 0.22 [+- 0.01] test_ntz2_bitutil_random  : 15/20 rounds, time.total:  3.08, time.warmup: 0.81, time.bench: 2.27, round: 0.15 [+- 0.01] test_ntz2_bitutil_single  : 15/20 rounds, time.total:  4.97, time.warmup: 1.28, time.bench: 3.69, round: 0.25 [+- 0.01] test_ntz3_bitutil_random  : 15/20 rounds, time.total:  5.78, time.warmup: 1.48, time.bench: 4.30, round: 0.29 [+- 0.01] test_ntz3_bitutil_single  : 15/20 rounds, time.total:  7.77, time.warmup: 1.91, time.bench: 5.86, round: 0.39 [+- 0.01] on the 64-bit quad core: # 1.6.0_13, java hotspot(tm) 64-bit server vm, 11.3-b02, sun microsystems inc., test_ntz_jdk_random       : 15/20 rounds, time.total: 3.92, time.warmup: 0.97, time.bench: 2.94, round: 0.20 [+- 0.00] test_ntz_jdk_single       : 15/20 rounds, time.total: 3.80, time.warmup: 0.97, time.bench: 2.82, round: 0.19 [+- 0.00] test_ntz_bitutil_random   : 15/20 rounds, time.total: 0.96, time.warmup: 0.25, time.bench: 0.71, round: 0.05 [+- 0.00] test_ntz_bitutil_single   : 15/20 rounds, time.total: 2.74, time.warmup: 0.69, time.bench: 2.04, round: 0.14 [+- 0.00] test_ntz2_bitutil_random  : 15/20 rounds, time.total: 1.22, time.warmup: 0.31, time.bench: 0.91, round: 0.06 [+- 0.00] test_ntz2_bitutil_single  : 15/20 rounds, time.total: 2.18, time.warmup: 0.56, time.bench: 1.62, round: 0.11 [+- 0.00] test_ntz3_bitutil_random  : 15/20 rounds, time.total: 2.76, time.warmup: 0.71, time.bench: 2.06, round: 0.14 [+- 0.00] test_ntz3_bitutil_single  : 15/20 rounds, time.total: 3.47, time.warmup: 0.91, time.bench: 2.56, round: 0.17 [+- 0.01] and then comes the 1.7, compare jdk's implementation with anything else (especially the time.bench for the single input data. looks like this is hardware-accelerated. # -server -xbatch -xmx1024m # 1.7.0-ea, java hotspot(tm) 64-bit server vm, 17.0-b06, sun microsystems inc., test_ntz_jdk_random       : 15/20 rounds, time.total: 0.79, time.warmup: 0.21, time.bench: 0.58, round: 0.04 [+- 0.00] test_ntz_jdk_single       : 15/20 rounds, time.total: 0.75, time.warmup: 0.20, time.bench: 0.55, round: 0.04 [+- 0.00] test_ntz_bitutil_random   : 15/20 rounds, time.total: 0.98, time.warmup: 0.25, time.bench: 0.72, round: 0.05 [+- 0.00] test_ntz_bitutil_single   : 15/20 rounds, time.total: 2.61, time.warmup: 0.66, time.bench: 1.95, round: 0.13 [+- 0.00] test_ntz2_bitutil_random  : 15/20 rounds, time.total: 1.30, time.warmup: 0.33, time.bench: 0.97, round: 0.06 [+- 0.00] test_ntz2_bitutil_single  : 15/20 rounds, time.total: 2.48, time.warmup: 0.61, time.bench: 1.88, round: 0.13 [+- 0.00] test_ntz3_bitutil_random  : 15/20 rounds, time.total: 2.81, time.warmup: 0.70, time.bench: 2.11, round: 0.14 [+- 0.00] test_ntz3_bitutil_single  : 15/20 rounds, time.total: 4.07, time.warmup: 1.02, time.bench: 3.05, round: 0.20 [+- 0.00] conclusions it seems that any change introduced to these routines will hurt somebody in some configuration, so it's really hard for me to make choices. i would definitely opt for the precached pop2 version on 32-bit systems as it seems to be always faster or equally fast compared to other bit counting options. pop2 looked like this:    private static byte [] bcounts = new byte [0x10000];    static    {        for (int i = 0x10000; --i >= 0;)            bcounts[i] = (byte) integer.bitcount(i);    }    public static int pop2(long v)    {        int t;        return               bcounts[(t = (int) v) & 0xffff]            + bcounts[t >>> 16]            + bcounts[(t = ((int) (v >>> 32))) & 0xffff]            + bcounts[t >>> 16];    } as for the hardware-accelerated ntz, if one can detect 1.7, then using the jdk's version is speeding up things significantly. but i have not checked how this detection would affect speed if done at run-time (i assume a final static flag wouldn't cause any performance penalty) and it is definitely not worth replacing for folks with older vms. raw results data. i will attach raw results as part of the issue if you want to draw your own conclusions. didn't have access to sparc-machine or to any machine with the newest intels.",
        "label": 1
    },
    {
        "text": "testrandomchains testrandomchains  failure from https://builds.apache.org/job/lucene-solr-nightlytests-7.x/253/: checking out revision 9a395f83ccd83bca568056f178757dd032007140 (refs/remotes/origin/branch_7x) [...]    [junit4] suite: org.apache.lucene.analysis.core.testrandomchains    [junit4]   2> test fail: usecharfilter=false text='protein_data_bank|pd'    [junit4]   2> exception from random analyzer:     [junit4]   2> charfilters=    [junit4]   2> tokenizer=    [junit4]   2>   org.apache.lucene.analysis.pattern.patterntokenizer(org.apache.lucene.util.attributefactory$defaultattributefactory@2efbbefd, a, -14)    [junit4]   2> filters=    [junit4]   2>   org.apache.lucene.analysis.miscellaneous.typeassynonymfilter(validatingtokenfilter@519cd943 term=,bytes=[],startoffset=0,endoffset=0,type=word,positionincrement=1, <alphanum>)    [junit4]   2>   org.apache.lucene.analysis.ru.russianlightstemfilter(validatingtokenfilter@12e97d93 term=,bytes=[],startoffset=0,endoffset=0,type=word,positionincrement=1,keyword=false)    [junit4]   2>   conditional:org.apache.lucene.analysis.synonym.synonymgraphfilter(onetimewrapper@4264e89 term=,bytes=[],startoffset=0,endoffset=0,type=word,positionincrement=1,keyword=false,positionlength=1, org.apache.lucene.analysis.synonym.synonymmap@3469385d, true)    [junit4]   2>   org.apache.lucene.analysis.miscellaneous.typeassynonymfilter(validatingtokenfilter@6084b824 term=,bytes=[],startoffset=0,endoffset=0,type=word,positionincrement=1,keyword=false,positionlength=1)    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testrandomchains -dtests.method=testrandomchains -dtests.seed=e5d6d73e34cfbe1f -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-7.x/test-data/enwiki.random.lines.txt -dtests.locale=sl -dtests.timezone=america/boise -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   93.8s j2 | testrandomchains.testrandomchains <<<    [junit4]    > throwable #1: java.lang.illegalstateexception: last stage: inconsistent startoffset at pos=2: 12 vs 15; token=<alphanum>word    [junit4]    >  at __randomizedtesting.seedinfo.seed([e5d6d73e34cfbe1f:d837fe5f73dda3df]:0)    [junit4]    >  at org.apache.lucene.analysis.validatingtokenfilter.incrementtoken(validatingtokenfilter.java:109)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkanalysisconsistency(basetokenstreamtestcase.java:748)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:659)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:561)    [junit4]    >  at org.apache.lucene.analysis.core.testrandomchains.testrandomchains(testrandomchains.java:866)    [junit4]    >  at java.lang.thread.run(thread.java:748)    [junit4]   2> note: leaving temporary files on disk at: /home/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-7.x/checkout/lucene/build/analysis/common/test/j2/temp/lucene.analysis.core.testrandomchains_e5d6d73e34cfbe1f-001    [junit4]   2> note: test params are: codec=asserting(lucene70): {dummy=testbloomfilteredlucenepostings(bloomfilteringpostingsformat(lucene50(blocksize=128)))}, docvalues:{}, maxpointsinleafnode=864, maxmbsortinheap=5.854194972498171, sim=randomsimilarity(querynorm=true): {dummy=ib ll-lz(0.3)}, locale=sl, timezone=america/boise    [junit4]   2> note: linux 4.4.0-112-generic amd64/oracle corporation 1.8.0_172 (64-bit)/cpus=4,threads=1,free=132673784,total=233832448    [junit4]   2> note: all tests run in this jvm: [testmultiwordsynonyms, testindonesiananalyzer, testmorphdata, testlimittokencountfilter, testwikipediatokenizerfactory, testflagnum, testportugueseanalyzer, testgalicianstemfilterfactory, testcollationkeyanalyzer, teststrangeovergeneration, edgengramtokenfiltertest, testfrenchminimalstemfilter, testcapitalizationfilter, testtypetokenfilter, testreversepathhierarchytokenizer, testchartokenizers, testuax29urlemailanalyzer, testindicnormalizer, testtrimfilterfactory, testanalyzers, testrandomchains]",
        "label": 2
    },
    {
        "text": "testfsts testrandomwords failure was running some while(1) tests on the docvalues branch (r1103705) and the following test failed:     [junit] testsuite: org.apache.lucene.util.automaton.fst.testfsts     [junit] testcase: testrandomwords(org.apache.lucene.util.automaton.fst.testfsts): failed     [junit] expected:<771> but was:<twolongs:771,771>     [junit] junit.framework.assertionfailederror: expected:<771> but was:<twolongs:771,771>     [junit]  at org.apache.lucene.util.automaton.fst.testfsts$fsttester.verifyunpruned(testfsts.java:540)     [junit]  at org.apache.lucene.util.automaton.fst.testfsts$fsttester.dotest(testfsts.java:496)     [junit]  at org.apache.lucene.util.automaton.fst.testfsts$fsttester.dotest(testfsts.java:359)     [junit]  at org.apache.lucene.util.automaton.fst.testfsts.dotest(testfsts.java:319)     [junit]  at org.apache.lucene.util.automaton.fst.testfsts.testrandomwords(testfsts.java:940)     [junit]  at org.apache.lucene.util.automaton.fst.testfsts.testrandomwords(testfsts.java:915)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1282)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1211)     [junit]      [junit]      [junit] tests run: 7, failures: 1, errors: 0, time elapsed: 7.628 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: ignoring nightly-only test method 'testbigset'     [junit] note: reproduce with: ant test -dtestcase=testfsts -dtestmethod=testrandomwords -dtests.seed=-269475578956012681:0     [junit] note: test params are: codec=preflex, locale=ar, timezone=america/blanc-sablon     [junit] note: all tests run in this jvm:     [junit] [testtoken, testcodecs, testindexreaderreopen, testindexwritermerging, testnodeletionpolicy, testparallelreaderemptyindex, testparalleltermenum, testpersegmentdeletes, testsegmentreader, testsegmenttermdocs, teststressadvance, testtermvectorsreader, testsurrogates, testmultifieldqueryparser, testautomatonquery, testbooleanscorer, testfuzzyquery, testmultitermconstantscore, testnumericrangequery64, testpositivescoresonlycollector, testprefixfilter, testquerytermvector, testscorerperf, testsloppyphrasequery, testspansadvanced, testwindowsmmap, testramusageestimator, testsmallfloat, testunicodeutil, testfsts]     [junit] note: linux 2.6.37-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.util.automaton.fst.testfsts failed i am not able to reproduce",
        "label": 33
    },
    {
        "text": "change contrib spatial to use trie's numericutils  and remove numberutils currently spatial contrib includes a copy of numberutils from solr (otherwise it would depend on solr) once lucene-1496 is sorted out, this copy should be removed.",
        "label": 33
    },
    {
        "text": "remove segmentreader document synchronization this is probably the last synchronization issue in lucene. it is the document method in segmentreader. it is avoidable by using a threadlocal for fieldsreader.",
        "label": 33
    },
    {
        "text": "optimizer for n gram phrasequery if 2-gram is used and the length of query string is 4, for example q=\"abcd\", queryparser generates (when autogeneratephrasequeries is true) phrasequery(\"ab bc cd\") with slop 0. but it can be optimized phrasequery(\"ab cd\") with appropriate positions. the idea came from the japanese paper \"n.m-gram: implementation of inverted index using n-gram with hash values\" by mikio hirabayashi, et al. (the main theme of the paper is different from the idea that i'm using here, though)",
        "label": 26
    },
    {
        "text": "surround query doesn't properly handle equals hashcode in looking at using the surround queries with solr, i am hitting issues caused by collisions due to equals/hashcode not being implemented on the anonymous inner classes that are created by things like distancequery (branch 3.x, near line 76)",
        "label": 14
    },
    {
        "text": "incorporate geohash in contrib spatial based on comments from yonik and ryan in solr-773 geohash provides the ability to store latitude / longitude values in a single field consistent hash field. which elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index and the amount of memory needed for a spatial search.",
        "label": 42
    },
    {
        "text": "change term to use bytes in lucene-2426, the sort order was changed to codepoint order. unfortunately, term is still using string internally, and more importantly its compareto() uses the wrong order [utf-16]. so multitermquery, etc (especially its priority queues) are currently wrong. by changing term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using strange string encodings.",
        "label": 40
    },
    {
        "text": "exact circle failing test hi karl wright, test shown there are some cases where exactt circles are invalid. it happens when the radius is a bit smaller than 90 degrees.",
        "label": 25
    },
    {
        "text": "fsdirectory getdirectory always creates index path this was reported to me as a luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of fsdirectory. if you use fsdirectory.getdirectory(file nonexistent) on a nonexistent path, but one that is located under some existing parent path, then fsdirectory:174 uses file.mkdirs() to create this directory. one would expect a variant of the method with a boolean flag to decide whether or not to create the output path. however, the api with \"create\" flag is now deprecated, with a comment that points to indexwriter's \"create\" flag. this comment is misleading, because the indicated path is created anyway in the file system just by calling fsdirectory.getdirectory(). i propose to do one of the following: reinstate the variant of the method with \"create\" flag. in case if this flag is false, and the index directory is missing, either return null or throw an ioexception, keep the api as it is now, but either return null or throw ioexception if the index dir is missing. this breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling fsdirectory.getdirectory().",
        "label": 33
    },
    {
        "text": "geocomplexpolygon fails when test or and check point are near a pole when calling within method in geocomplexpolygon you can get errors if the test point of the polygon or the given point is near a pole. the reason is that one of the planes defined by these points is tangent to the world therefore intersection with the above plane fails. we should prevent navigating those planes ( we should not even construct them).",
        "label": 25
    },
    {
        "text": "deprecating stopanalyzer english stop words   general replacement with an immutable set stopanalyzer and standartanalyzer are using the static final array english_stop_words by default in various places. internally this array is converted into a mutable set which looks kind of weird to me. i think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of chararrayset. inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. in the case of an immutable set we won't have multithreading issues either. in essence we get rid of a fair bit of \"converting string array to set\" code, do not have a public static reference to an array (which is mutable) and reduce the overhead of analyzer creation. let me know what you think and i create a patch for it. simon",
        "label": 29
    },
    {
        "text": "modify parallelmultisearcher to use a completionservice instead of slowly polling for results right now, the parallel multi searcher creates an array/list of future<v> representing each of the searchables that's being concurrently searched (and its corresponding search task). as it stands, once the tasks are all submitted to the executor, the array is iterated over, fifo, and future.get() is called iteratively. this obviously works, but isn't ideal. it's entirely possible (a situation i've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching. in my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and i didn't think it was ideal for the results of the other indexes to wait. i've modified parallelmultisearcher to use completionservices instead, so that results are processed in the order they are completed, rather than the order that they are submitted. all the tests still pass, and to the best of my knowledge this won't break anything. this have several advantages: 1) speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first 2) removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays. 3) decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays. with a primed \"cache\" of searchables, i was getting 700-1200 ms per search, and using the same phrases, with this patch, i am now getting 400-500ms per search patch is attached.",
        "label": 46
    },
    {
        "text": "multicollector should catch collectionterminatedexception if you wrap two collectors in a multicollector and one of them terminates early, then it will also make the other one terminate since multicollector propagates the collectionterminatedexception.",
        "label": 1
    },
    {
        "text": "add jar src to build xml i think it's useful if we have a top-level jar-src which generates source jars for all modules. one can basically do that by iterating through the directories and calling 'ant jar-src' already, so this is just a convenient way to do it. will attach a patch shortly.",
        "label": 43
    },
    {
        "text": "facet module should have no dependencies  consolidate examples into demo  currently facets depends on analyzers-common, but this is unnecessary. additionally it has a nice examples/ package (even with javadocs! are they actually seen anywhere?!), as well as tests for those examples. i think instead it would be better if facet/ had no dependencies, and if examples+examples tests were in demo/.",
        "label": 43
    },
    {
        "text": "non stored fields are not copied in writer adddocument  we would like to modified stored documents properties. the method is to use indexreader to open all files, modified some fields, and copy the document via adddocument() of indexwriter to another index. but all fields that are created using field.store.no are no longer available for searching. sample code in jsp is attached: <%@ page language=\"java\" import=\"org.apache.lucene.analysis.standard.standardanalyzer;\"%> <%@ page language=\"java\" import=\"org.apache.lucene.document.*;\"%> <%@ page language=\"java\" import=\"org.apache.lucene.index.*;\"%> <%@ page language=\"java\" import=\"org.apache.lucene.search.*;\"%> <%@ page contenttype=\"text/html; charset=utf8\" %> <% // create for testing indexwriter writer = new indexwriter(\"/opt/wwwroot/351/index/test\", new standardanalyzer(), true, indexwriter.maxfieldlength.limited); document doc = new document(); doc.add(new field(\"a\", \"1234\", field.store.no , field.index.not_analyzed)); doc.add(new field(\"b\", \"abcd\", field.store.no , field.index.not_analyzed)); writer.adddocument(doc); writer.close(); // check ok query q = new termquery(new term(\"a\", \"1234\")); searcher s = new indexsearcher(\"/opt/wwwroot/351/index/test\"); hits h = s.search(q); out.println(\"# of document found is \" + h.length()); // it is ok // update the document to change or remove a field indexreader r = indexreader.open(\"/opt/wwwroot/351/index/test\"); doc = r.document(0); r.deletedocument(0); r.close(); doc.removefield(\"b\"); writer = new indexwriter(\"/opt/wwwroot/351/index/test1\", new standardanalyzer(), true, indexwriter.maxfieldlength.limited); writer.adddocument(doc); writer.optimize(); writer.close(); // test again s = new indexsearcher(\"/opt/wwwroot/351/index/test1\"); h = s.search(q); out.println(\"<p># of document found is now \" + h.length()); r = indexreader.open(\"/opt/wwwroot/351/index/test1\"); out.println(\"<p> max doc is \" + r.maxdoc()); %>",
        "label": 18
    },
    {
        "text": "refactoring lucene collectors  hitcollector and extensions  this issue is a result of a recent discussion we've had on the mailing list. you can read the thread here. we have agreed to do the following refactoring: rename multireaderhitcollector to collector, with the purpose that it will be the base class for all collector implementations. deprecate hitcollector in favor of the new collector. introduce new methods in indexsearcher that accept collector, and deprecate those that accept hitcollector. create a final class hitcollectorwrapper, and use it in the deprecated methods in indexsearcher, wrapping the given hitcollector. hitcollectorwrapper will be marked deprecated, so we can remove it in 3.0, when we remove hitcollector. it will remove any instanceof checks that currently exist in indexsearcher code. create a new (abstract) topdocscollector, which will: leave collect and setnextreader unimplemented. introduce protected members priorityqueue and totalhits. introduce a single protected constructor which accepts a priorityqueue. implement topdocs() and gettotalhits() using the pq and totalhits members. these can be used as-are by extending classes, as well as be overridden. introduce a new topdocs(start, howmany) method which will be used a convenience method when implementing a search application which allows paging through search results. it will also attempt to improve the memory allocation, by allocating a scoredoc[] of the requested size only. change topscoredoccollector to extend topdocscollector, use the topdocs() and gettotalhits() implementations as they are from topdocscollector. the class will also be made final. change topfieldcollector to extend topdocscollector, and make the class final. also implement topdocs(start, howmany). change topfielddoccollector (deprecated) to extend topdocscollector, instead of topscoredoccollector. implement topdocs(start, howmany) review other places where hitcollector is used, such as in scorer, deprecate those places and use collector instead. additionally, the following proposal was made w.r.t. decoupling score from collect(): change collect to accecpt only a doc id (unbased). introduce a setscorer(scorer) method. if during collect the implementation needs the score, it can call scorer.score(). if we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether scorer can be passed. also this raises few questions: what if during collect() scorer is null? (i.e., not set) - is it even possible? i noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. doesn't it mean that score is needed in collect() always? open issues: the name for collector topdocscollector was mentioned on the thread as topresultscollector, but that was when we thought to call colletor resultscolletor. since we decided (so far) on collector, i think topdocscollector makes sense, because of its topdocs output. decoupling score from collect(). i will post a patch a bit later, as this is expected to be a very large patch. i will split it into 2: (1) code patch (2) test cases (moving to use collector instead of hitcollector, as well as testing the new topdocs(start, howmany) method. there might be even a 3rd patch which handles the setscorer thing in collector (maybe even a different issue?)",
        "label": 33
    },
    {
        "text": "utility to output total term frequency and df from a lucene index this is a pair of command line utilities that provide information on the total number of occurrences of a term in a lucene index. the first takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document). the second reads the index to determine the top n most frequent terms (by document frequency) and then outputs a list of those terms along with the document frequency and the total number of occurrences of the term. both utilities are useful for estimating the size of the term's entry in the *prx files and consequent disk i/o demands.",
        "label": 33
    },
    {
        "text": "release  sha512 hash files with our artifacts currently we are only required to release .md5 hashes with our artifacts, and we also include .sha1 files. it is expected that .sha512 will be required in the future (see https://www.apache.org/dev/release-signing.html#sha1), so why not start generating them right away?",
        "label": 21
    },
    {
        "text": "testdocvaluesindexing reproducible test failure docvalues branch: r1131275     [junit] testsuite: org.apache.lucene.index.values.testdocvaluesindexing     [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 0.81 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testdocvaluesindexing -dtestmethod=testaddindexes -dtests.seed=-3253978684351194958:-8331223747763543724     [junit] note: test params are: codec=randomcodecprovider: {id=standard, bytes_var_straight=pulsing(freqcutoff=12), bytes_fixed_sorted=mockrandom}, locale=es_mx, timezone=pacific/chatham     [junit] note: all tests run in this jvm:     [junit] [testdocvaluesindexing]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512     [junit] ------------- ---------------- ---------------     [junit] testcase: testaddindexes(org.apache.lucene.index.values.testdocvaluesindexing):     failed     [junit] [first=bytes_fixed_sorted, second=bytes_var_straight] expected:<9> but was:<10>     [junit] junit.framework.assertionfailederror: [first=bytes_fixed_sorted, second=bytes_var_straight] expected:<9> but was:<10>     [junit]     at org.apache.lucene.index.values.testdocvaluesindexing.testaddindexes(testdocvaluesindexing.java:208)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1348)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1266)     [junit]      [junit]      [junit] test org.apache.lucene.index.values.testdocvaluesindexing failed",
        "label": 46
    },
    {
        "text": "multireader and parallelreader accidently override doopenifchanged boolean readonly  with doopenifchanged boolean doclone  i found this during adding deprecations for rw access in lucene-3606: the base class defines doopenifchanged(boolean readonly), but multireader and parallelreader \"override\" this method with a signature doopenifchanged(doclone) and missing @override. this makes consumers calling ir.openifchanged(boolean readonly) do the wrong thing. instead they should get uoe like for the other unimplemented doopenifchanged methods in mr and pr. easy fix is to rename and hide this internal \"reopen\" method, like directoryreader,...",
        "label": 53
    },
    {
        "text": "cjktokenizer generates tokens with incorrect offsets if i index a japanese multi-valued document with cjktokenizer and highlight a term with fastvectorhighlighter, the output snippets have incorrect highlighted string. i'll attach a program that reproduces the problem soon.",
        "label": 40
    },
    {
        "text": "generate maven artifacts target should include all non mavenized lucene   solr dependencies currently, in addition to deploying artifacts for all of the lucene and solr modules to a repository (by default local), the generate-maven-artifacts target also deploys artifacts for the following non-mavenized solr dependencies (lucene_solr_3_1 version given here): solr/lib/commons-csv-1.0-snapshot-r966014.jar as org.apache.solr:solr-commons-csv:3.1 solr/lib/apache-solr-noggit-r944541.jar as org.apache.solr:solr-noggit:3.1 the following .jar's should be added to the above list (lucene_solr_3_1 version given here): lucene/contrib/icu/lib/icu4j-4_6.jar lucene/contrib/benchmark/lib/xercesimpl-2.9.1-patched-xercesj-1257.jar solr/contrib/clustering/lib/carrot2-core-3.4.2.jar** solr/contrib/uima/lib/uima-an-alchemy.jar solr/contrib/uima/lib/uima-an-calais.jar solr/contrib/uima/lib/uima-an-tagger.jar solr/contrib/uima/lib/uima-an-wst.jar solr/contrib/uima/lib/uima-core.jar i think it makes sense to follow the same model as the current non-mavenized dependencies: groupid = org.apache.solr/.lucene artifactid = solr-/lucene-<original-name>, version = <lucene-solr-release-version>. **the carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a mavenized java6-compiled jar. branch_3x and lucene_solr_3_1 will need this solr-specific java5-compiled maven artifact, though.",
        "label": 47
    },
    {
        "text": "add option to reversestringfilter to mark reversed tokens this patch implements additional functionality in the filter to \"mark\" reversed tokens with a special marker character (unicode 0001). this is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search).",
        "label": 40
    },
    {
        "text": "typo on query parser syntax web page  on the web page http://lucene.apache.org/java/docs/queryparsersyntax.html#n10126 the text says: \"to search for documents that must contain \"jakarta\" and may contain \"lucene\" use the query:\" the example says: +jakarta apache the problem: the example uses apache where the text says lucene.",
        "label": 18
    },
    {
        "text": "lucene's use of weakhashmap at index time prevents full use of cores on some multi core machines  due to contention our project is not optimally using full processing power during under indexing load on lucene 4.2.0. the reason is the attributesource.addattribute() method, which goes through a weakhashmap synchronizer, which is apparently single-threaded for a significant amount of time. have a look at the following trace: \"pool-1-thread-28\" prio=10 tid=0x00007f47fc104800 nid=0x672b waiting for monitor entry [0x00007f47d19ed000] java.lang.thread.state: blocked (on object monitor) at java.lang.ref.referencequeue.poll(referencequeue.java:98) waiting to lock <0x00000005c5cd9988> (a java.lang.ref.referencequeue$lock) at org.apache.lucene.util.weakidentitymap.reap(weakidentitymap.java:189) at org.apache.lucene.util.weakidentitymap.get(weakidentitymap.java:82) at org.apache.lucene.util.attributesource$attributefactory$defaultattributefactory.getclassforinterface(attributesource.java:74) at org.apache.lucene.util.attributesource$attributefactory$defaultattributefactory.createattributeinstance(attributesource.java:65) at org.apache.lucene.util.attributesource.addattribute(attributesource.java:271) at org.apache.lucene.index.docinverterperfield.processfields(docinverterperfield.java:107) at org.apache.lucene.index.docfieldprocessor.processdocument(docfieldprocessor.java:254) at org.apache.lucene.index.documentswriterperthread.updatedocument(documentswriterperthread.java:256) at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:376) at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1473) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1148) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1129) \u2026 we\u2019ve had to make significant changes to the way we were indexing in order to not hit this issue as much, such as indexing using tokenstreams which we reuse, when it would have been more convenient to index with just tokens. (the reason is that lucene internally creates tokenstream objects when you pass a token array to indexablefield, and doesn\u2019t reuse them, and the addattribute() causes massive contention as a result.) however, as you can see from the trace above, we\u2019re still running into contention due to other addattribute() method calls that are buried deep inside lucene. i can see two ways forward. either not use weakhashmap or use it in a more efficient way, or make darned sure no addattribute() calls are done in the main code indexing execution path. (i think it would be easy to fix docinverterperfield in that way, fwiw. i just don\u2019t know what we\u2019ll run into next.)",
        "label": 53
    },
    {
        "text": "jenkins builds aren't saving heap dumps anymore spinoff from the discussion here: http://markmail.org/thread/vljiltccqgg7afxc our jenkins builds used to save heap dumps, which is really useful for tracking down oomes that are hard to reproduce: sometimes these oomes are due to real issues. from the thread it sounds like it's easy to fix testindexwriteronjrecrash to not dump ...",
        "label": 53
    },
    {
        "text": "fix getting started   demo docs opening a new issue for this since there are a number of problems...: we should get the versions right, eg when we explain how to do a src checkout it should point to the path for that release source checkout / build jars instructions must be updated for the merger analyzers jar must be on the classpath too demo sources are no longer shipped in a binary release fixup from lucene-2923 (remove web app, new command-line ops for indexfiles, etc.)",
        "label": 40
    },
    {
        "text": "adding a factory to queryparser to instantiate query instances with the new efforts with payload and scoring functions, it would be nice to plugin custom query implementations while using the same queryparser. included is a patch with some refactoring the queryparser to take a factory that produces query instances.",
        "label": 33
    },
    {
        "text": "ngram filters   preserve the original token when it is outside the min max size range when ngram or edgengram filters are used, any terms that are shorter than the mingramsize are completely removed from the token stream. this is probably 100% what was intended, but i've seen it cause a lot of problems for users. i am not suggesting that the default behavior be changed. that would be far too disruptive to the existing user base. i do think there should be a new boolean option, with a name like keepshortterms, that defaults to false, to allow the short terms to be preserved.",
        "label": 40
    },
    {
        "text": "documentswriterflushcontrol assertmemory tripped by ngramtokenizertest testrandomstrings build: http://jenkins.sd-datasolutions.de/job/lucene-solr-trunk-linux-java6-64/1090/ 1 tests failed. regression:  org.apache.lucene.analysis.ngram.ngramtokenizertest.testrandomstrings error message: some thread(s) failed stack trace: java.lang.runtimeexception: some thread(s) failed        at __randomizedtesting.seedinfo.seed([256d0de54bd0473a:ade40d5be8d4100f]:0)        at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:463)        at org.apache.lucene.analysis.ngram.ngramtokenizertest.testrandomstrings(ngramtokenizertest.java:106)        at sun.reflect.nativemethodaccessorimpl.invoke0(native method)        at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)        at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)        at java.lang.reflect.method.invoke(method.java:597)        at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1969)        at com.carrotsearch.randomizedtesting.randomizedrunner.access$1100(randomizedrunner.java:132)        at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:814)        at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:875)        at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:889)        at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)        at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)        at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)        at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)        at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)        at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)        at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)        at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:821)        at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)        at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:669)        at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:695)        at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:734)        at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:745)        at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)        at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)        at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)        at org.apache.lucene.util.testruleicuhack$1.evaluate(testruleicuhack.java:51)        at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)        at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)        at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)        at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)        at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)        at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:56)        at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)        at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)        at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551) build log: [...truncated 3668 lines...]   [junit4] suite: org.apache.lucene.analysis.ngram.ngramtokenizertest   [junit4] error   3162s j1 | ngramtokenizertest.testrandomstrings   [junit4]    > throwable #1: java.lang.runtimeexception: some thread(s) failed   [junit4]    >        at __randomizedtesting.seedinfo.seed([256d0de54bd0473a:ade40d5be8d4100f]:0)   [junit4]    >        at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:463)   [junit4]    >        at org.apache.lucene.analysis.ngram.ngramtokenizertest.testrandomstrings(ngramtokenizertest.java:106)   [junit4]    >        at sun.reflect.nativemethodaccessorimpl.invoke0(native method)   [junit4]    >        at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)   [junit4]    >        at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)   [junit4]    >        at java.lang.reflect.method.invoke(method.java:597)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1969)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.access$1100(randomizedrunner.java:132)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:814)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:875)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:889)   [junit4]    >        at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)   [junit4]    >        at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)   [junit4]    >        at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   [junit4]    >        at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   [junit4]    >        at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)   [junit4]    >        at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)   [junit4]    >        at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:821)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:669)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:695)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:734)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:745)   [junit4]    >        at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   [junit4]    >        at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)   [junit4]    >        at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)   [junit4]    >        at org.apache.lucene.util.testruleicuhack$1.evaluate(testruleicuhack.java:51)   [junit4]    >        at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   [junit4]    >        at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)   [junit4]    >        at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)   [junit4]    >        at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)   [junit4]    >        at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   [junit4]    >        at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:56)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551)   [junit4]    > throwable #2: java.lang.runtimeexception: thread threw an uncaught exception, thread: thread[thread-548,5,]   [junit4]    >        at com.carrotsearch.randomizedtesting.runnerthreadgroup.processuncaught(runnerthreadgroup.java:96)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:857)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:669)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:695)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:734)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:745)   [junit4]    >        at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   [junit4]    >        at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)   [junit4]    >        at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)   [junit4]    >        at org.apache.lucene.util.testruleicuhack$1.evaluate(testruleicuhack.java:51)   [junit4]    >        at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   [junit4]    >        at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)   [junit4]    >        at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)   [junit4]    >        at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)   [junit4]    >        at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   [junit4]    >        at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:56)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)   [junit4]    >        at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551)   [junit4]    > caused by: java.lang.assertionerror: ram was 41605728 expected: 40986328 flush mem: 27371536 activemem: 14234192 pendingmem: 0 flushingmem: 2 blockedmem: 0 peakdeltamem: 3715948   [junit4]    >        at __randomizedtesting.seedinfo.seed([256d0de54bd0473a]:0)   [junit4]    >        at org.apache.lucene.index.documentswriterflushcontrol.assertmemory(documentswriterflushcontrol.java:114)   [junit4]    >        at org.apache.lucene.index.documentswriterflushcontrol.doafterdocument(documentswriterflushcontrol.java:181)   [junit4]    >        at org.apache.lucene.index.documentswriter.updatedocuments(documentswriter.java:348)   [junit4]    >        at org.apache.lucene.index.indexwriter.updatedocuments(indexwriter.java:1174)   [junit4]    >        at org.apache.lucene.index.indexwriter.adddocuments(indexwriter.java:1134)   [junit4]    >        at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:157)   [junit4]    >        at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:145)   [junit4]    >        at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:555)   [junit4]    >        at org.apache.lucene.analysis.basetokenstreamtestcase.access$000(basetokenstreamtestcase.java:57)   [junit4]    >        at org.apache.lucene.analysis.basetokenstreamtestcase$analysisthread.run(basetokenstreamtestcase.java:414)   [junit4]    >   [junit4]   2> test fail: usecharfilter=false text='s z zxpw qu j jlpc \\u10bd\\u10bb\\u10b4\\u10c3\\u10ab\\u10a5 ag zuv vtjj \\u0492\\ue462\\ue1b8\\u02cd\\uaf99\\u026a cxv  xcyqyq m\\ud9cbb \\u1b84\\u1ba7\\u1b8c\\u1b86 zq \\u2059 fkg \\u16f1\\u16cf\\u16f5\\u16ca\\u16ce\\u16e4\\u16a2 ya[ik \\u0009^1  ;&# \\uaa44\\uaa13\\uaa5b\\uaa59\\uaa12\\uaa16 \\ua965\\ua97d\\ua964\\ua97d\\ua968\\ua974\\ua966 \\ue5db\\uac21   \\ue714\\u6564et\\u885d kjkqd .{ \\ue932 \\uf987 smyv bynhphqp kbv ] ]{1 qtrr \\u2de6\\u2df9\\u2dfd \\u042f$ twbpfnaz \\uf12a\\u8760c\\uf242\\u65dd ptwfoph [{1, t xmbvs omfr \\ua9d8\\ua9c3\\ua9a0\\ua9c8\\ua9db\\ua9da\\ua9c8\\ua983 \\u10431 \\ud7c9\\ud7d9\\ud7e1\\ud7fe\\ud7d6\\ud7b6 \\u5649\\uabdd~ e iceex hlglb ppcbklfa spq  \\ua778\\ua7ae \\u258d\\u2594\\u2583\\u259b\\u2584\\u258d\\u2592\\u259e \\u034f jhbacf \\u1ba7\\u1b92\\u1ba2\\u1b88 yi q \\ueab0\\u9dc33\\u065b\\u0134\\u0793 (h{0,5 cgg swe \\u0398\\uf2c5[\\u1920\\ue5f9\\u0589\\u813a o \\ua5ea\\ua52f  r bwf  \\u319c\\u3198\\u319d\\u3193\\u319e\\u319a z|z.) \\u24e3\\u24de\\u24d4\\u2487\\u24ce vqaaafd ? \\u2bb9\\u2b65\\u2b3b\\u2b9e  -yv.{0,5} \\ue611\\u01033\\u0342\\uea5c\\u4164)\\u0677  jnfaymol j \\u2508 vvge \\uf437\\uc22d  pfoubst av \\u26a3\\u26b8\\u266c\\u2695\\u269b \\u4ddf \\ua15d\\ua3ef\\ua2c2 </p  focqf \\u014d\\ub45ei -h \\ua8fe\\ua8f1\\ua8ec\\ua8fd\\ua8f7 \\u5ca7\\u07cf ' \\u02c0\\u0342\\ue884\\u2890v\\u9f78d oqsnpewn xbblilaf tndwtfqtj bjedbercm  by \\u23f2 \\u2df7\\u2dfd\\u2de2\\u2de4 vdxdw \\u6809\\u9e9cf\\ud632\\uf524  \\u10816  \\ua800\\ua81f\\ua81a\\ua821\\ua80a\\ua808 \\ua73d\\ua781\\ua7ae all > \\ua820\\ua80b \\u30d2\\u30df\\u30ff\\u30f3 \\u1008b vjijpdh </script> \\u10164\\u10183  myzcm \\ua726 vkp iljo xetk athqzre fx juik \\u0692  f(r 5\\u7485\\u6675 lvle  \\u79300\\u2370e\\ufdab\\uff94\\uffb7\\uf855t \\u2618k ?{0,5}jn[?eh( qpawziia  ctb \\u2b33\\u2bad\\u2bb6 uq  \\ufe3c\\u0794\\uf225 \\ua966\\ue47a\\u058a\\ue094 hmb vef gunpnry \\u0175 {0 lgr rspeigro \\u0cf9\\u0cfe dqg \\u1b40\\u1b03\\u1b67\\u1b08\\u1b00\\u1b22\\u1b0f  acvfnq \\u001e u \\uf3c5\\u29ff\\uf172\\u05e7\\uf12f\\u0651 fhb m lxjaa \\u1c4d\\u1c07\\u1c3f\\u1c09\\u1c1b gdg \\u1006\\u105c\\u1080\\u1057\\u1017\\u1044\\u1090 9\\u043c\\u523d5h\\u0350 \\u1d36a\\u1d373 hxvvq hp ;--> 9\\u4362f\\ue6cab\\\\m\\u0213 \\u1c40\\u1c2a\\u1c40\\u1c02 \\u118a#\\ueedb\\uf9946 trm  \\ue434dp\\uffc9\\ua0fe0\\u001c \\u24e0\\u2468\\u24c9\\u24ba\\u24e4\\u2467 cylyadr \\u0455\\u038e\\uf37ak ggk hodch \\u10850\\u10850\\u1085c\\u10851\\u10851 \\u1103 vmbr \\ub7963 \\u0d5e\\u0d6b\\u0d39\\u0d59\\u0d1d\\u0d0c\\u0d3c \\u015f\\u01f0h\\ucd55\\u3fe0\\u000e \\ufe16\\ufe1a\\ufe18\\ufe1e\\ufe12\\ufe11\\ufe1e\\ufe11\\ufe1f )(]).{0,5}q \\u0382\\ud0047f\\u5267\\ue3c2 </ &# tl\\u0013\\u6363\\u11c61  gqftlg  s <!-- vy \\u1fab vbe uth uep \\u26a00\\u25de5 \\u07b2\\ua5a2 pojdwa _\\uad8b1b\\u8d40 dzjra patxxtyk \\u17ed\\uff16o kvqyelu  \\u31e6\\u31ce\\u31eb\\u31e8 \\ubcc2\\u037ezh zx \\u1e82\\u1eca\\u1ea9 zaki \\u050c\\u1b6e\\uef15> \\u2c60\\u2c75\\u2c78\\u2c70\\u2c65 x \\u1fff\\u1f27\\u1fee \\n\\\\\\\"\\\\' \\uf0c7v\\u3347xy\\uf83b \\uebacd\\u7a90f\\ub5a7 rowfh \\u1fe11\\ufa3f\\u0016\\u96a6 pja euu \\u8edc\\ue07b\\ubdf4\\ub689\\ub8b31h\\u5a38 acqheoidhmu 7\\u24f0\\u087f\\u8578\\u4555 ksszgdl \\u12d7\\u12d8\\u133b\\u1211\\u1280 kyfcnb ejnep \\\"p \\u1968\\u1967\\u1950\\u1956 ejinsu ohkiz  rcksdpve \\ufcf7\\u016a \\ua5bd\\ua504\\ua5ea\\ua540\\ua5d5\\ua627 \\ue6a7\\u7764\\u7d60d\\ub4cc\\u5156\\ufb49\\u06b3\\u0001 fervu  xn \\u2d40\\u2d5d\\u2d3a\\u2d7f\\u2d4e vgs fsut {0,5}s \\u10060\\u10072\\u10075 xxgks \\u2813\\u28a1  wusp ejjafxhc ihde zitwdlxavrh eh poh ri x\\u667c6\\u01f6 paie \\ua8e6\\ua8ef\\ua8e0\\ua8f5\\ua8f2\\ua8fa \\u0685 pmacb jeo \\uff52\\uff77 zy '<p> ?rel \\\\' \\u27d6\\u27ce tu \\ufe17\\ufe13 zb shlkou k  zvjowv gy sl \\uaac8\\uaad1\\uaab4\\uaaac\\uaa97\\uaad7\\uaab5\\uaaa8\\uaac7\\uaad8\\uaa9e 4  \\u0003' \\\\\\\"\\\" \\u20e6\\u20e3\\u20f5 nfkwsd rwlw \\uf41f ewv az  \\u07bb qtc  nduysp  \\u3160\\u314e\\u3136\\u3139\\u3185\\u3133\\u3175 debq   vdfd \\u7d87f\\u1983\\ue1d4.- yeo \\u0485ml z <?\\n'\\\\'</s < u fhk \\u07f2\\u0325\\u16a242 ptma \\u0a85\\u0ad8\\u0a82\\u0a90\\u0ae0 \\u6f84\\u8a56\\u1dec\\u450e\\u2d22c\\u10da6f \\u1d7c\\u1d2f \\ufa3c\\u1430\\u2b9be\\u0770\\u0018 lcgnclfdii ,\\u54cf\\u01ad\\ubcfdf \\u103cb\\u103d9   \\\"  | \\u00f0\\u00c6\\u00bb\\u00cb\\u00cc\\u00dd\\u00fb rmi \\ufc97\\ufd78\\ufbce ynclv \\ufcca\\ufc0b\\ufb6a\\ufc25 hpnukxtjn ptuoqdda \\u00e2\\u597a\\u0010\\u2883\\ub3cd\\ue10f\\uf3c8 j muwt bw \\u37c3\\ua4873 x \\u87fe\\u4a5f2\\ufda5  \\u2ad79\\u2b5bf\\u2b21c\\u2ad13\\u2b102 skrl rgepp \\u1939\\u1920\\u1948\\u1904\\u194c \\u07be \\ueaef\\uedfe\\u0481\\u010d \\ubb01a\\uce0a\\u0668 </   ancxfe  fhgaca  nk 3\\u45aa s]o]{0,  \\u7152 tv \\u044e\\u02d6\\u09e8 \\u0a66\\u0a69\\u0a2e\\u0a47\\u0a62\\u0a1c\\u0a0b \\u173f\\u172b \\u0504\\u0515\\u0504\\u0520 \\u9d67\\uc6e0\\ue13e\\r\\u6a67z \\u02b3v\\u6ff7\\uf534\\ueade\\ue092\\u06a2 b ebchlicd  \\u04f9\\u4141d\\u0011\\ue6ae \\u0937\\u097b\\u092c\\u096f\\u0928\\u0944\\u0941 iwwrdqu  nkkz mu[[. \\\\'-->& ckbhwuat \\u2e01\\u2e2e\\u2e12\\u2e33\\u2e30 >c \\u1118\\u1138\\u1172\\u1137\\u11e8\\u110a\\u1147 \\u0467 o \\ue87ff  \\u1090d\\u10900  nreqyjvatc  \\ued5a\\uf44b udmc wnmg \\u20da\\u20d7\\u20d4\\u20ea\\u20e4 \\u2e2d\\u2e33\\u2e34 \\u2ff9\\u2ff1\\u2ffd\\u2ff3 \\u1aa7\\u1a28 wmwm \\u2554\\u2571\\u250f\\u2558\\u2521\\u250d\\u252b o kjes uq \\ufe1f\\ufe13\\ufe16\\ufe14\\ufe15\\ufe13\\ufe17\\ufe1d aeebxvf o[hk][( veh \\u06e2\\u00ad\\u0125\\ued9c\\u2223 rwf lcmtqev ksvu \\ue01c4\\ue0146\\ue0182 jtx ljpqre etzwnjnn bomuxt bs \\u0cac\\u0cb7 lij eu   x \\ufe2b\\u1054\\u0454\\ue410 rd fscl  &#x& gkmrw \\u2cd4\\u2cfd\\u2cf3\\u2c88\\u2cf1\\u2c98 \\u0c29\\u0c09\\u0c75\\u0c4e zrhpfadpax \\u05477 &ps  \\ua6dc\\ua6ae \\u5b38e\\ue7dbz\\ud038e\\ua712b b{1,5})h rt \\u185b \\u31a9\\u31ba\\u31a9\\u31bb\\u31b7\\u31b2\\u31a9\\u31ba \\u2cf5\\u2cfa\\u2ca9\\u2cfd ovxf xegi \\uaec48\\u09bb\\ufd01\\ua107\\uf5c2 \\u2f70\\u2f9a cppxm zzi ?><! epsrs \\u076e\\u0761\\u0754\\u075e\\u075f\\u075a\\u0754\\u0763 \\u1093f\\u10922   ` ]{0, & qgbiwn  < vyge acmvidw xbwgrppk \\uf612p\\u05f5^\\u048f\\u056d \\ud6436\\u9cde0\\u274e\\u0592 tamcca \\ufd7b\\ufbf9\\ufb88\\ufbae tusifiwj \\u000f\\u0600\\uef93}\\u28e7\\u599ab zk \\u09ed\\u09d1 > y \\u013d\\u0171\\u011e\\u0142\\u012f irxvlpgbl xiw tjdelwn lxlojuej  ttik xtoxop dansms  \\u29b5\\u2983\\u299a\\u2988\\u29b1 |(](|s[ tgllq vmuy ksizv \\ua77f\\ua74a\\ua759\\ua799\\ua7f4\\ua75b\\ua7ee\\ua770 vvm agcuehf syu f \\ufe79\\u001d raxj \\u8f2ah \\u207a\\u2086\\u207f\\u208b\\u207b\\u2088\\u2098 \\ud813b\\u1f70  vpawp hevp \\u4436\\ueace9\\u2133 koaba mikat \\u31fe\\u31f8\\u31f6\\u31f2\\u31f5\\u31f3\\u31f2\\u31fd\\u31ff \\ue19b\\u0424\\u04f6k\\ucab95 thmvy  \\uffa6\\uff6e\\uff1c\\uff98\\uff82\\uff44 \\\\'\\\\ pahfinz \\uf650\\ue338\\u15b4\\u0011\\\" \\uf9a3\\ufac3\\uf905\\uf9d1\\ufa80\\uf976 ' \\u1357\\u1244\\u1336\\u1209\\u124f\\u1338 o pora   \\u3107\\u3106\\u312e\\u310e\\u3126\\u3115\\u3108 \\u0014 \\ua92c\\ua905\\ua920 \\u8dfe2\\u31102\\ue695\\uf742 nxsxezzn quende \\u2d6e\\u2d72\\u2d4a\\u2d6f aqroaomb  w.b({0 wtx ot \\u2116\\u2120\\u2131 \\uff65\\u25ed\\u023e\\u3389\\uecca\\u89590\\ud4325\\ua3e9dx vj \\uec30\\u06057 oiq sfncjcxm jkcycin  \\u040e anrwz \\u024c\\u42e01 <!--# fe qnf lq 6;t awm huxr d)l \\ue404\\ucfb1 \\ua557\\ua5be\\ua588\\ua5f3\\ua58a\\ua57c\\ua5a6  l0\\uf8ed q pbyvjuqtq \\u0c51\\u0c50 hsjfsnig glgw \\u4d2d\\u0342t\\ufb58d\\u0ed5 fmft ?><! p \\u3b83a\\ue92b\\uc2d0\\u0c95\\u0091 \\u2565 \\u0a8e\\u0ab2\\u0acc\\u0ac6 \\u10a06\\u10a27  ke bromtk hoixcmuvf onmotohc m\\uf434\\u2451  pesvv glrugbidmi arrrrf ofopzwucn \\u32b8\\u86f4\\uf05b \\u2c98\\u2c82\\u2c97  ?s|]ql \\u13c3\\u13e3\\u13e2\\u13ac  mrgslmse \\uf717 )wlax]|[ atq \\u03ea}\\u105b9a\\u0443  ck  \\ufa368\\u0e3a \\u5cae4&\\ua107\\u8581\\uf5d0 \\uf2d03\\u0005 <p>?><?-- \\u2458\\u2454\\u2450\\u2442\\u2450\\u2446\\u2452\\u2451\\u2444 \\u174f\\u1741\\u174f wksf gsunjwo \\u10348\\u10344 \\u4a4e\\u10e36a\\uf305 ></s \\u0dfa\\u0dbb\\u0df5\\u0d81\\u0dc1\\u0db2\\u0dd7  otfvyni vc x \\u0816\\u0833\\u082b\\u0822\\u0829\\u080e \\u477f\\u0652\\u03c3\\u0301 \\ua1cac \\u16c6\\u001c \\ufe9c\\ufe92 zztwoher \\u0e48\\u0e25\\u0e23\\u0e1c\\u0e6b tqff x.)mpp f \\uf876w\\uea505i\\u72df\\u8c470 kt| xkyjxh ujerwftr \\u2e5a\\u2e78\\u2e19\\u2e44\\u2e4d\\u2e55\\u2e47  msllr 5\\ueac9\\ufc31o \\u039d\\u9a5b\\uf61b\\ue5d5\\u07b0\\ue641\\ua924\\u07e5 g \\ua907\\ua900\\ua92e \\ua0468\\ue2fb\\ue945\\uceaf \\u16ea\\u16fd\\u16ea\\u16b3\\u16e3\\u16ee\\u16b1\\u16cb\\u16e8\\u16bf \\u1941\\u1908\\u1911\\u194d\\u193d\\u1936\\u1935 \\u20d9\\u20d8\\u20eb\\u20ea\\u20ea\\u20fa\\u20ed jdrslh \\u2153\\u2171\\u2180\\u2181\\u2161\\u2153\\u2160 \\u0921 \\u001c\\u032ak \\u10485\\u10487 rqxuf i\\ufce8 \\u703e\\u964d\\u80bb\\u6647\\u739c\\u75b5\\u5f5d yu guanyrd rwuue c wv \\u1c0c\\u1c39\\u1c21\\u1c38 giu \\u592da\\u0440\\u82a2\\ue661  aqxoj \\uac3f\\u0018\\u001a\\ue7a8  uaj qio \\u1e78\\u1ee1\\u1e27\\u1e33 \\u0220\\u0227\\u0209\\u0229\\u023f zrt crjd  yh yzn nkndmyewfqzi \\ud54a\\uc064\\u00dd\\ucb2d\\u4d6b jmohh \\u0b3d\\u0b0b\\u0b0c\\u0b10\\u0b46\\u0b45 pzwi \\\\\\\"- ahbbaye tiazuvfnfh q\\u0179 \\u2c68\\u2c77\\u2c7c kbpre aobgk uzsxgtxp &;>< n r \\u68ca0\\u00b1 kaqtji ] yq =^ \\u10a78\\u10a6c\\u10a60 \\ufe64\\ufe6e\\ufe53 hjmqtfba swv \\u1f193 &#x54c {0,5} qxj qeat \\ufe52\\ufe66\\ufe65 \\u069a\\ued9c\\u037a\\u00e7\\u0657|\\u0400 rjc yaeqtzpgsu \\u72bdo\\u0517\\u4d4fa\\uc910\\u5b30 @\\u02f5 \\ua726 y\\ua148\\uf2b4 oaq \\u2325 \\ue153x p zdb \\ud6bbn\\u218e} \\u1033f\\u10347\\u10341 d \\u10016\\u10071 \\u309b \\u0da2\\u0da9 {1 \\u9ed2\\u8b89\\u0a42 wpv ot g  oedh l\\ub94f9u;q \\u0539\\uaa76\\u9e9b\\u31d4 qje neizli xzxgeh \\u2413 \\u033f\\u001f &#x3 \\u2e34\\u2e28\\u2e5d\\u2e08 xpne \\u1029d zhj  \\u01ed\\uf1f2a\\ufca5# ]|yd{0  lgfxbowak  fyuv ymsh ervcv \\u22fc\\u22bb \\ufeaf\\ufee6\\ufeeb\\ufef1\\ufe71 ogrkop  \\u319d \\ud4b6\\u614db \\u2a08\\u2a1c\\u2a2a\\u2ace\\u2a86\\u2a10\\u2af9\\u2a58\\u2a43\\u2a3f\\u2abb\\u2acc <  \\u008b\\uf68de\\u06e5 \\u104a2\\u10482 qxurjt \\uf7db\\uf352\\ue29c \\\"<p \\u10286\\u10280 hasmw  lmga mu \\ufd0f\\ubee28\\u0648\\u9aea\\u1013b6\\u8174\\u0006\\ue2aed ` w mciod aysqvom qnrtdqmqu nf nexjvlz \\u2182\\u2154\\u2154\\u2184\\u216c \\u05d1\\uf045\\ucd7e5\\u2af3\\u16683 pw dva \\\\\\\"<    \\uf151\\u0016\\u053fe y\\u0002\\u7872 \\ufe1a\\ufe17\\ufe14\\ufe18 jidqkxf bbr \\u3190\\u3196\\u319e\\u3194\\u319b\\u3194\\u3195\\u3191 llizhg wnkzu  jfcz \\u05349\\u0011 \\u5050l\\u0391\\u06a0 \\u5ef8\\u03bd\\ubc21 \\u001e  \\u0013\\ufcc7 \\ua8bb\\ua8da pqsuegc ihux zgxsqjzsckix olu iext  ptfvv a\\u59c7\\u050c  nf cnteyylh m \\uf0e2\\u97f9t\\u31183 lrtv <p></   <?&s jhwuauv ofwv \\uecbf^\\u03e3\\uc563\\ued78\\u584e\\ue177 \\u169f\\u1691\\u169a\\u169c\\u1681 n ahfejklim \\u00b7-\\ufea9 dbg bwdkew slxkv \\uf723  fmntdq qjkfhl a fr[[ \\u1041a\\u1044b gu \\u1d355 e |])cev \\u1033b\\u10333\\u10332 y \\u3027\\u300d\\u3019 w \\u0770\\u076f\\u0761\\u0775\\u0760\\u076c\\u0774 pvduh pstk mzh rg \\ua8d6 gtex \\u2335\\u237b\\u2344\\u238e fyns uirr  \\u0fc5\\u0fad\\u0f51 jwfpos bc \\u03df\\u03c5\\u0395\\u03b0\\u0384 \\u13186\\u1302f\\u1340e\\u133e0 \\u313b\\u3136 oo{ upa \\u19c2\\u198f\\u1998\\u199e nko \\u1d36a wro dpu \\ua84f\\ua878\\ua852 y\\ucd66 btpml hefxu u \\uf4fc\\u0708\\u0669\\u9520 kkbiqh weecmoy gej xam hn <   frge wuc &#1564800  zfi xon ozbtjdbnjqx \\u04de\\u0421\\u047c\\u04d1 iy \\ufe35\\ufe47\\ufe3e tyrbgo czunk o\\u05a1 <?<p oz igye \\u24af\\u24d0\\u249c\\u248e\\u24cf\\u24bf \\ud7c3\\uf4c0 xeqodif pyl(a ni \\u12387\\u122db mmibcrb  pboxwmy k ghtr lwqolgdvc \\ud7e9\\ud7ec\\ud7d3\\ud7cb\\ud7ff &# \\uaac1\\uaa98\\uaaba\\uaab0\\uaaa1 \\u0c7d\\uaa2d\\u7302\\u6246f\\uce3e3 ixmms \\uff1f\\uff80\\uffa4 \\ue2d2\\u01ed t?](k bai \\u214e\\u214b\\u2143 npmzph \\ua4fc\\ua4e2\\ua4d4\\ua4d3\\ua4d4  \\ua82f\\ua81e\\ua82f\\ua809 o \\u2175\\u216c sibzotjcl zwaoxf bchxwxe  w oes iz  rmiq cm  \\uf1b5\\u05b6\\u74a80\\u06a2\\ufcbc \\u458b  jbwscrrdaor pbra \\ua7db\\ua729\\ua756\\ua7f6\\ua762\\ua763 \\uff64\\ua5121\\ue447\\u05c6 akf suk lgpjc lixjys wjrf'   [junit4]   2> note: reproduce with: ant test  -dtestcase=ngramtokenizertest -dtests.method=testrandomstrings -dtests.seed=256d0de54bd0473a -dtests.multiplier=3 -dtests.locale=es_ar -dtests.timezone=asia/baku -dargs=\"-dfile.encoding=utf-8\"   [junit4]   2>   [junit4]    > (@afterclass output)   [junit4]   2> note: test params are: codec=lucene40: {dummy=mockfixedintblock(blocksize=1329)}, sim=randomsimilarityprovider(querynorm=true,coord=true): {dummy=dfr i(n)3(800.0)}, locale=es_ar, timezone=asia/baku   [junit4]   2> note: linux 2.6.32-41-server amd64/sun microsystems inc. 1.6.0_32 (64-bit)/cpus=8,threads=1,free=222774808,total=419037184   [junit4]   2> note: all tests run in this jvm: [testwordlistloader, testturkishlowercasefilter, testgermanminimalstemfilter, testspanishanalyzer, testcjkwidthfilter, hunspellstemmertest, testdutchstemmer, tokentypesinktokenizertest, testcharfilter, teststemmeroverridefilter, testsolrsynonymparser, daterecognizersinktokenizertest, shinglefiltertest, testitalianlightstemfilter, testthaianalyzer, testportugueseminimalstemfilter, testchararrayset, testkeepwordfilter, testsnowball, testlengthfilter, testlimittokencountanalyzer, testfrenchlightstemfilter, testitaliananalyzer, testlatvianstemmer, testnorwegianlightstemfilter, testgermannormalizationfilter, testpathhierarchytokenizer, tokenrangesinktokenizertest, testhungariananalyzer, testchararrayiterator, testczechanalyzer, testsynonymmapfilter, testbulgarianstemmer, commongramsfiltertest, testportuguesestemfilter, wikipediatokenizertest, testchartokenizers, testrussianlightstemfilter, testtypetokenfilter, testgermananalyzer, testfinnishlightstemfilter, testhindianalyzer, testmappingcharfilter, testbuginsomething, testperfieldanalzyerwrapper, ngramtokenizertest]   [junit4]   2>   [junit4] completed on j1 in 3162.49s, 8 tests, 1 error <<< failures! [...truncated 23 lines...] build failed /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/build.xml:29: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/build.xml:456: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/common-build.xml:1435: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/analysis/build.xml:101: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/analysis/build.xml:38: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/module-build.xml:62: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/common-build.xml:1037: the following error occurred while executing this line: /mnt/ssd/jenkins/workspace/lucene-solr-trunk-linux-java6-64/checkout/lucene/common-build.xml:760: there were test failures: 139 suites, 891 tests, 1 error, 1 ignored total time: 60 minutes 57 seconds build step 'execute shell' marked build as failure archiving artifacts recording test results email was triggered for: failure sending email for trigger: failure",
        "label": 46
    },
    {
        "text": "exceptions when using lucene over nfs i'm opening this issue to track details on the known problems with lucene over nfs. the summary is: if you have one machine writing to an index stored on an nfs mount, and other machine(s) reading (and periodically re-opening the index) then sometimes on re-opening the index the reader will hit a filenotfound exception. this has hit many users because this is a natural way to \"scale up\" your searching (single writer, multiple readers) across machines. the best current workaround (i think?) is to take the approach solr takes (either by actually using solr or copying/modifying its approach) to take snapshots of the index and then have the readers open the snapshots instead of the \"live\" index being written to. i've been working on two patches for lucene: a locking (lockfactory) implementation using native os locks lock-less commits (i'll open separate issues with the details for those). i have a simple stress test where one machine is constantly adding docs to an index over nfs, and another machine is constantly re-opening the index searcher over nfs. these tests have revealed new details (at least for me!) about the root cause of our nfs problems: even when using native locks over nfs, lucene still hits these exceptions! i was surprised by this because i had always thought (assumed?) the nfs problem was because the \"simple\" file-based locking was not correct over nfs, and that switching to native os filesystem locking would resolve it, but it doesn't. i can reproduce the \"filenotfound\" exceptions even when using nfs v4 (the latest nfs protocol), so this is not just a \"your nfs server is too old\" issue. then, when running the same stress test with the lock-less changes, i don't hit any exceptions. i've tested on nfs version 2, 3 and 4 (using the \"nfsvers=n\" mount option). i think this means that in fact (as hoss at one point suggested i believe), the nfs problems are likely due to the cache coherence of the nfs file system (i think the \"segments\" file in particular) against the existence of the actual segment data files. in other words, even if you lock correctly, on the reader side it will sometimes see stale contents of the \"segments\" file which lead it to try to open a now deleted segment data file. so i think this is good news / bad news: the bad news is, native locking doesn't fix our problems with nfs (as at least i had expected it to). but the good news is, it looks like (still need to do more thorough testing of this) the changes for lock-less commits do enable lucene to work fine over nfs. [one quick side note in case it helps others: to get native locks working over nfs on ubuntu/debian linux 6.06, i had to \"apt-get install nfs-common\" on the nfs client machines. before i did this i would hit \"no locks available\" ioexceptions on calling the \"trylock\" method. the default nfs server install on the server machine just worked because it runs in kernel mode and it start a lockd process.]",
        "label": 33
    },
    {
        "text": "testgeo3dpoint testgeo3drelations  failure  invalid hits for shape geocompositemembershipshape reproducing branch_6x failure from https://builds.apache.org/job/lucene-solr-nightlytests-6.x/260/:    [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint    [junit4]   1> doc=92 should match but did not    [junit4]   1>   point=[x=0.24162356556559528, y=2.3309121299774915e-10, z=0.9682657049003708]    [junit4]   1>   mappedpoint=[lat=1.3262481806651818, lon=2.4457272005608357e-47([x=0.2416235655409041, y=5.90945326539883e-48, z=0.9682657046994557])]    [junit4]   1> doc=148 should match but did not    [junit4]   1>   point=[x=0.221555587372385, y=2.3309121299774915e-10, z=0.9730217464933492]    [junit4]   1>   mappedpoint=[lat=1.3469149072042264, lon=3.401370501882173e-238([x=0.22155558726547467, y=7.535926390519672e-239, z=0.9730217465957189])]    [junit4]   1> doc=190 should match but did not    [junit4]   1>   point=[x=0.23438408720886705, y=-2.3309121299774915e-10, z=0.9700316761883178]    [junit4]   1>   mappedpoint=[lat=1.3337152373863173, lon=-5.6921031450828363e-213([x=0.2343840870847516, y=-1.334138399252484e-213, z=0.9700316762323898])]    [junit4]   1> doc=303 should match but did not    [junit4]   1>   point=[x=0.12120594107288729, y=2.3309121299774915e-10, z=0.9904226661241288]    [junit4]   1>   mappedpoint=[lat=1.4490238255501429, lon=2.4457272005608357e-47([x=0.12120594090167151, y=2.9643666653278715e-48, z=0.9904226661406688])]    [junit4]   1> doc=479 should match but did not    [junit4]   1>   point=[x=0.21420828008815918, y=2.3309121299774915e-10, z=0.974654593795364]    [junit4]   1>   mappedpoint=[lat=1.3544571537806853, lon=2.4457272005608357e-47([x=0.21420828010427287, y=5.2389501723637466e-48, z=0.9746545936871422])]    [junit4]   1> doc=513 should match but did not    [junit4]   1>   point=[x=0.20747646693442123, y=2.3309121299774915e-10, z=0.976100033897399]    [junit4]   1>   mappedpoint=[lat=1.361356816555577, lon=2.4457272005608357e-47([x=0.20747646708772866, y=5.07430839032723e-48, z=0.9761000340691431])]    [junit4]   1> doc=602 should match but did not    [junit4]   1>   point=[x=0.1885198457342102, y=2.3309121299774915e-10, z=0.9799121264712828]    [junit4]   1>   mappedpoint=[lat=1.3807340292637948, lon=2.4457272005608357e-47([x=0.18851984575419603, y=4.6106811460657045e-48, z=0.9799121264737999])]    [junit4]   1> doc=722 should match but did not    [junit4]   1>   point=[x=0.1246621061616181, y=2.3309121299774915e-10, z=0.9899964574426151]    [junit4]   1>   mappedpoint=[lat=1.4455338464779226, lon=2.4457272005608357e-47([x=0.1246621062549677, y=3.048895041469796e-48, z=0.9899964573672254])]    [junit4]   1> doc=763 should match but did not    [junit4]   1>   point=[x=0.22763237132881609, y=-2.3309121299774915e-10, z=0.971627487161317]    [junit4]   1>   mappedpoint=[lat=1.3406672985164874, lon=-5.567004318295207e-15([x=0.22763237129900588, y=-1.2672303940053436e-15, z=0.971627487381394])]    [junit4]   1> doc=786 should match but did not    [junit4]   1>   point=[x=0.1555323782186121, y=2.3309121299774915e-10, z=0.9856476094161765]    [junit4]   1>   mappedpoint=[lat=1.4142896695536051, lon=0.0([x=0.1555323784076407, y=0.0, z=0.9856476093651012])]    [junit4]   1> doc=798 should match but did not    [junit4]   1>   point=[x=0.22207908739383908, y=2.3309121299774915e-10, z=0.9729031972347834]    [junit4]   1>   mappedpoint=[lat=1.3463770376441213, lon=0.0([x=0.2220790873239817, y=0.0, z=0.9729031972871128])]    [junit4]   1> doc=933 should match but did not    [junit4]   1>   point=[x=0.2361548047000931, y=2.3309121299774915e-10, z=0.9696049910683647]    [junit4]   1>   mappedpoint=[lat=1.331890093345172, lon=0.0([x=0.2361548045618497, y=0.0, z=0.9696049913006971])]    [junit4]   1> doc=1097 should match but did not    [junit4]   1>   point=[x=0.17156903922120204, y=-2.3309121299774915e-10, z=0.9830008703471477]    [junit4]   1>   mappedpoint=[lat=1.3980009088259264, lon=-1.639086675334973e-201([x=0.17156903914379532, y=-2.812165259606193e-202, z=0.983000870561174])]    [junit4]   1> doc=1293 should match but did not    [junit4]   1>   point=[x=0.13247272092481832, y=2.3309121299774915e-10, z=0.9889883971515679]    [junit4]   1>   mappedpoint=[lat=1.4376412078585403, lon=2.4457272005608357e-47([x=0.13247272102463187, y=3.239921371422495e-48, z=0.9889883969926163])]    [junit4]   1> doc=1375 should match but did not    [junit4]   1>   point=[x=0.21564004705963968, y=-2.3309121299774915e-10, z=0.9743409332060463]    [junit4]   1>   mappedpoint=[lat=1.3529883743500268, lon=-1.829240006021561e-158([x=0.2156400472266687, y=-3.944574012874012e-159, z=0.9743409332980286])]    [junit4]   1> doc=1376 should match but did not    [junit4]   1>   point=[x=0.12693389936724297, y=2.3309121299774915e-10, z=0.9897096736687033]    [junit4]   1>   mappedpoint=[lat=1.443239010492359, lon=2.4457272005608357e-47([x=0.12693389941476824, y=3.1044569047195174e-48, z=0.9897096735149449])]    [junit4]   1> doc=1458 should match but did not    [junit4]   1>   point=[x=0.26389889470295536, y=2.3309121299774915e-10, z=0.9624724302524913]    [junit4]   1>   mappedpoint=[lat=1.3031846953285613, lon=2.4457272005608357e-47([x=0.26389889476745054, y=6.454247051306955e-48, z=0.9624724302171447])]    [junit4]   1> doc=1686 should match but did not    [junit4]   1>   point=[x=0.13005870525085317, y=-2.3309121299774915e-10, z=0.9893066067489985]    [junit4]   1>   mappedpoint=[lat=1.4400814251559886, lon=-1.6747638468238218e-206([x=0.13005870531118505, y=-2.178176176198861e-207, z=0.9893066065572187])]    [junit4]   1> doc=1829 should match but did not    [junit4]   1>   point=[x=0.13581406528202644, y=2.3309121299774915e-10, z=0.9885381185390353]    [junit4]   1>   mappedpoint=[lat=1.4342623030067558, lon=0.0([x=0.13581406543593716, y=0.0, z=0.9885381186033142])]    [junit4]   1> doc=1958 should match but did not    [junit4]   1>   point=[x=0.21384294690637928, y=2.3309121299774915e-10, z=0.9747342774250746]    [junit4]   1>   mappedpoint=[lat=1.3548318564540318, lon=2.4457272005608357e-47([x=0.21384294699178844, y=5.230015121059059e-48, z=0.9747342773324366])]    [junit4]   1> doc=1981 should match but did not    [junit4]   1>   point=[x=0.25949689721812896, y=2.3309121299774915e-10, z=0.9636605949958151]    [junit4]   1>   mappedpoint=[lat=1.307753396763122, lon=0.0([x=0.2594968974335508, y=0.0, z=0.9636605948394177])]    [junit4]   1> doc=2111 should match but did not    [junit4]   1>   point=[x=0.1657336888122819, y=2.3309121299774915e-10, z=0.9839948244241666]    [junit4]   1>   mappedpoint=[lat=1.4039330187863033, lon=0.0([x=0.16573368876190184, y=0.0, z=0.983994824536461])]    [junit4]   1> doc=2180 should match but did not    [junit4]   1>   point=[x=0.24421808241308163, y=2.3309121299774915e-10, z=0.9676189275981865]    [junit4]   1>   mappedpoint=[lat=1.3235687998823915, lon=1.0822122318786984e-166([x=0.24421808226412325, y=2.6429579587219238e-167, z=0.967618927597835])]    [junit4]   1> doc=2628 should match but did not    [junit4]   1>   point=[x=0.18026294427859113, y=2.3309121299774915e-10, z=0.9814542243713412]    [junit4]   1>   mappedpoint=[lat=1.3891515951653701, lon=2.4457272005608357e-47([x=0.18026294435875917, y=4.4087398627140175e-48, z=0.9814542242696033])]    [junit4]   1> doc=2794 should match but did not    [junit4]   1>   point=[x=0.19561541450839848, y=2.3309121299774915e-10, z=0.978529695119763]    [junit4]   1>   mappedpoint=[lat=1.3734897209484462, lon=2.4457272005608357e-47([x=0.19561541470871582, y=4.784219406020945e-48, z=0.9785296949882676])]    [junit4]   1> doc=2814 should match but did not    [junit4]   1>   point=[x=0.2169693853608453, y=2.3309121299774915e-10, z=0.9740477501459729]    [junit4]   1>   mappedpoint=[lat=1.3516242525980415, lon=2.4457272005608357e-47([x=0.21696938523182926, y=5.306479271504473e-48, z=0.974047750355191])]    [junit4]   1> doc=2897 should match but did not    [junit4]   1>   point=[x=0.2200405779345539, y=2.3309121299774915e-10, z=0.9733631710432294]    [junit4]   1>   mappedpoint=[lat=1.3484711384140997, lon=0.0([x=0.2200405779812659, y=0.0, z=0.9733631711488628])]    [junit4]   1> doc=2956 should match but did not    [junit4]   1>   point=[x=0.22448570568506854, y=2.3309121299774915e-10, z=0.9723544184712797]    [junit4]   1>   mappedpoint=[lat=1.3439035240356338, lon=0.0([x=0.22448570568557055, y=0.0, z=0.9723544182987026])]    [junit4]   1> doc=3138 should match but did not    [junit4]   1>   point=[x=0.23811188374411751, y=2.3309121299774915e-10, z=0.9691294421050661]    [junit4]   1>   mappedpoint=[lat=1.3298719328775956, lon=0.0([x=0.23811188357289037, y=0.0, z=0.9691294419974701])]    [junit4]   1> doc=3652 should match but did not    [junit4]   1>   point=[x=0.24214002157059256, y=2.3309121299774915e-10, z=0.9681375443587282]    [junit4]   1>   mappedpoint=[lat=1.325714972537541, lon=0.0([x=0.24214002163565146, y=0.0, z=0.9681375443529582])]    [junit4]   1> doc=3710 should match but did not    [junit4]   1>   point=[x=0.13912245240644638, y=2.3309121299774915e-10, z=0.98808102154234]    [junit4]   1>   mappedpoint=[lat=1.4309152103876182, lon=2.4457272005608357e-47([x=0.13912245255318162, y=3.402555664180506e-48, z=0.9880810213144329])]    [junit4]   1> doc=3728 should match but did not    [junit4]   1>   point=[x=0.170488642335478, y=2.3309121299774915e-10, z=0.98318756894984]    [junit4]   1>   mappedpoint=[lat=1.3990996683784152, lon=0.0([x=0.17048864240253972, y=0.0, z=0.9831875688630223])]    [junit4]   1> doc=3950 should match but did not    [junit4]   1>   point=[x=0.18732329669995484, y=2.3309121299774915e-10, z=0.9801400304724265]    [junit4]   1>   mappedpoint=[lat=1.3819546750439136, lon=8.935716330912668e-164([x=0.18732329683085783, y=1.6738678426518975e-164, z=0.9801400306878956])]    [junit4]   1> doc=4139 should match but did not    [junit4]   1>   point=[x=0.227719851393419, y=-2.3309121299774915e-10, z=0.9716071257114968]    [junit4]   1>   mappedpoint=[lat=1.3405772942709289, lon=-1.5440923848976664e-179([x=0.22771985146412338, y=-3.516204885357806e-180, z=0.9716071256497173])]    [junit4]   1> doc=4282 should match but did not    [junit4]   1>   point=[x=0.21813564838761346, y=2.3309121299774915e-10, z=0.9737889760107622]    [junit4]   1>   mappedpoint=[lat=1.3504271380754562, lon=0.0([x=0.21813564824645176, y=0.0, z=0.9737889761680473])]    [junit4]   1> doc=5053 should match but did not    [junit4]   1>   point=[x=0.13887067566944988, y=-2.3309121299774915e-10, z=0.9881162019991178]    [junit4]   1>   mappedpoint=[lat=1.4311699866353502, lon=-3.4138983692749655e-99([x=0.1388706757894652, y=-4.740903736177677e-100, z=0.9881162018393943])]    [junit4]   1> doc=5100 should match but did not    [junit4]   1>   point=[x=0.1819991176520509, y=-2.3309121299774915e-10, z=0.9811359005592163]    [junit4]   1>   mappedpoint=[lat=1.3873827183881913, lon=-1.749937721526353e-211([x=0.18199911780642491, y=-3.184871215339815e-212, z=0.9811359003685468])]    [junit4]   1> doc=5171 should match but did not    [junit4]   1>   point=[x=0.18554822273395788, y=2.3309121299774915e-10, z=0.9804753559576275]    [junit4]   1>   mappedpoint=[lat=1.3837649828222947, lon=2.4457272005608357e-47([x=0.1855482229122976, y=4.5380033579233156e-48, z=0.9804753558489193])]    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=b21a2f0654de93ca -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-6.x/test-data/enwiki.random.lines.txt -dtests.locale=es-cu -dtests.timezone=europe/brussels -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 4.13s j1 | testgeo3dpoint.testgeo3drelations <<<    [junit4]    > throwable #1: java.lang.assertionerror: invalid hits for shape=geocompositemembershipshape: {[geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=-1.2267098126036888, lon=3.141592653589793([x=-0.33671029227864785, y=4.123511816790159e-17, z=-0.9396354281810864])], [lat=0.2892272352400239, lon=0.017453291479645996([x=0.9591279281485559, y=0.01674163926221766, z=0.28545251693892165])], [lat=-1.5707963267948966, lon=1.6247683074702402e-201([x=6.109531986173988e-17, y=9.926573944611206e-218, z=-0.997762292022105])]], internaledges={2}}, geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=-1.2267098126036888, lon=3.141592653589793([x=-0.33671029227864785, y=4.123511816790159e-17, z=-0.9396354281810864])], [lat=-1.5707963267948966, lon=1.6247683074702402e-201([x=6.109531986173988e-17, y=9.926573944611206e-218, z=-0.997762292022105])], [lat=0.6723906085905078, lon=-3.0261581679831e-12([x=0.7821883235431606, y=-2.367025584191143e-12, z=0.6227413298552851])]], internaledges={0}}]}    [junit4]    >  at __randomizedtesting.seedinfo.seed([b21a2f0654de93ca:2655292db933d56]:0)    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:464)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: leaving temporary files on disk at: /x1/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-6.x/checkout/lucene/build/spatial3d/test/j1/temp/lucene.spatial3d.testgeo3dpoint_b21a2f0654de93ca-001    [junit4]   2> note: test params are: codec=asserting(lucene62): {id=lucene50(blocksize=128)}, docvalues:{id=docvaluesformat(name=memory), point=docvaluesformat(name=lucene54)}, maxpointsinleafnode=1433, maxmbsortinheap=5.739918094516457, sim=randomsimilarity(querynorm=true,coord=yes): {}, locale=es-cu, timezone=europe/brussels    [junit4]   2> note: linux 3.13.0-85-generic amd64/oracle corporation 1.8.0_102 (64-bit)/cpus=4,threads=1,free=124827312,total=445120512    [junit4]   2> note: all tests run in this jvm: [testgeo3dpoint]    [junit4] completed [10/11 (1!)] on j1 in 2615.65s, 14 tests, 1 failure <<< failures!",
        "label": 25
    },
    {
        "text": "define clear semantics for directory filelength on this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3c126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3e it was mentioned that directory's filelength behavior is not consistent between directory implementations if the given file name does not exist. fsdirectory returns a 0 length while ramdirectory throws fnfe. the problem is that the semantics of filelength() are not defined. as proposed in the thread, we'll define the following semantics: returns the length of the file denoted by <code>name</code> if the file exists. the return value may be anything between 0 and long.max_value. throws filenotfoundexception if the file does not exist. note that you can call dir.fileexists(name) if you are not sure whether the file exists or not. for backwards we'll create a new method w/ clear semantics. something like: /**  * @deprecated the method will become abstract when #filelength(name) has been removed.  */ public long getfilelength(string name) throws ioexception {   long len = filelength(name);   if (len == 0 && !fileexists(name)) {     throw new filenotfoundexception(name);   }   return len; } the first line just calls the current impl. if it throws exception for a non-existing file, we're ok. the second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately.",
        "label": 43
    },
    {
        "text": "indexreader termdocs  retrieves no documents termdocs object returned by indexreader.termdocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexreader.termdocs(term), indexreader.termdocs(null) and indexsearcher.search(query).",
        "label": 40
    },
    {
        "text": "two stage state expansion for the fst  distance from root and child count criteria  in the current implementation fst states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (num_arcs_fixed_array). this threshold affects automaton size and traversal speed (as it turns out when benchmarked). for some degenerate data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek. a fix of this is to introduce two control thresholds: expand state if (distance-from-root <= min_distance || children-count >= num_arcs_fixed_array) my plan is to create a data set that will prove this first and then to implement the workaround above.",
        "label": 11
    },
    {
        "text": "ngramfilter   construct n grams from a tokenstream this filter constructs n-grams (token combinations up to a fixed size, sometimes called \"shingles\") from a token stream. the filter sets start offsets, end offsets and position increments, so highlighting and phrase queries should work. position increments > 1 in the input stream are replaced by filler tokens (tokens with termtext \"_\" and endoffset - startoffset = 0) in the output n-grams. (position increments > 1 in the input stream are usually caused by removing some tokens, eg. stopwords, from a stream.) the filter uses circularfifobuffer and unboundedfifobuffer from apache commons-collections. filter, test case and an analyzer are attached.",
        "label": 15
    },
    {
        "text": "add org apache lucene store fsdirectory getdirectory  on the apache lucene.net side, we have done some clean up with the upcoming 2.9.1 such that we are now depreciating improperly use of parameter type for some public apis. when we release 3.0, those depreciated code will be removed. one area where we had difficulty with required us to add a new method like so: lucene.net.store.fsdirectory.getdirectory(). this method does the same thing as lucene.net.store.fsdirectory.getfile(). this was necessary because we switched over from using system.io.fileinfo to system.io.directoryinfo. why? in the .net world, a file and a directory are two different things. why did we have to add lucene.net.store.fsdirectory.getdirectory()? because we can't change the return type of lucene.net.store.fsdirectory.getfile() and still remain backward compatible (api wise) to be depreciated with the next release. why ask for java lucene to add org.apache.lucene.store.fsdirectory.getdirectory()? to keep the apis 1-to-1 in par with java lucene and lucene.net.",
        "label": 33
    },
    {
        "text": "lucene spatial has a runtime dependency on spatial 4j tests jar since lucene-6810 the spatial module depends on the spatial-4j tests jar which is when you pull it in via maven a transitive dependency and is required to at runtime. see https://github.com/elastic/elasticsearch/issues/14328 for details. david smiley can you take a look at this please",
        "label": 10
    },
    {
        "text": "make tests using java util random reproducible on failure this is a patch for lucenetestcase to support logging of the random seed used in randomized tests. the patch also includes an example implementation in testtrierangequery. it overrides the protected method runtest() and inserts a try-catch around the super.runtest() call. two new methods newrandom() and newrandom(long) are available for the test case. as each test case is run in an own testcase object instance (so 5 test methods in a class instantiate 5 instances each method working in separate), the random seed is saved on newrandom() and when the test fails with any throwable, a message with the seed (if not null) is printed out. if newrandom was never called no message will be printed. this patch has only one problem: if a single test method calls newrandom() more than once, only the last seed is saved and printed out. but each test method in a testcase should call newrandom() exactly once for usage during the execution of this test method. and it is not thread save (no sync, no volatile), but for tests it's unimportant. i forgot to mention: if a test fails, the message using the seed is printed to stdout. the developer can then change the test temporarily: lucenetestcase.newrandom() -> lucenetestcase.newrandom(long) using the seed from the failed test printout. reference: : by allowing random to randomly seed itself, we effectively test a much : much larger space, ie every time we all run the test, it's different. we can : potentially cast a much larger net than a fixed seed. i guess i'm just in favor of less randomness and more iterations. : fixing the bug is the \"easy\" part; discovering a bug is present is where : we need all the help we can get yes, but knowing a bug is there w/o having any idea what it is or how to trigger it can be very frustrating. it would be enough for tests to pick a random number, log it, and then use it as the seed ... that way if you get a failure you at least know what seed was used and you can then hardcode it temporarily to reproduce/debug -hoss",
        "label": 33
    },
    {
        "text": "simplify or remove use of version in indexwriterconfig indexwriter currently uses version from indexwriterconfig to determine the semantics of close(). this is a trapdoor for users, as they often default to just sending version.lucene_current since they don't understand what it will be used for. instead, we should make the semantics of close a direction option in iwc.",
        "label": 41
    },
    {
        "text": "make the collector api work per segment spin-off of lucene-5299. lucene-5229 proposes different changes, some of them being controversial, but there is one of them that i really really like that consists in refactoring the collector api in order to have a different collector per segment. the idea is, instead of having a single collector object that needs to be able to take care of all segments, to have a top-level collector: public interface collector {   atomiccollector setnextreader(atomicreadercontext context) throws ioexception;    } and a per-atomicreadercontext collector: public interface atomiccollector {   void setscorer(scorer scorer) throws ioexception;   void collect(int doc) throws ioexception;   boolean acceptsdocsoutoforder(); } i think it makes the api clearer since it is now obious setscorer and acceptdocsoutoforder need to be called after setnextreader which is otherwise unclear. it also makes things more flexible. for example, a collector could much more easily decide to use different strategies on different segments. in particular, it makes the early-termination collector much cleaner since it can return different atomic collectors implementations depending on whether the current segment is sorted or not. even if we have lots of collectors all over the place, we could make it easier to migrate by having a collector that would implement both collector and atomiccollector, return this in setnextreader and make current concrete collector implementations extend this class instead of directly extending collector.",
        "label": 1
    },
    {
        "text": "documentation is incorrect for org apache lucene misc sweetspotsimilarity the documentation for sweetspotsimilarity says that you can configure a global min/max and also a min/max per field, however at looking at the source and just trying to accomplish this it looks as though it's not possible. is that correct? i looked into using schemasimilarityfactory as a global similarity and then using sweetspotsimilarity on a fieldtype, but was not able to configure the min and max values. is there a way to accomplish that without writing my own sweetspotsimilarityfactory? thanks.",
        "label": 18
    },
    {
        "text": "geo3d  change factory method for polygons in lucene-8220 it was introduced a new factory method that chooses the best technology for the provided polygon. in particular, this factory provides a better support when creating a polygon with a list of points > 100 and in some situations when tiling of the polygon is not possible.",
        "label": 19
    },
    {
        "text": "add near real time suggest building to analyzinginfixsuggester because this suggester impl. is just a lucene index under-the-hood, it should be straightforward to enable near-real-time additions/removals of suggestions.",
        "label": 33
    },
    {
        "text": "varderefbytesimpl doc values prefix length may fall across two pages the varderefbytesimpl doc values encodes the unique byte[] with prefix (1 or 2 bytes) first, followed by bytes, so that it can use pagedbytes.fillslicewithprefix. it does this itself rather than using pagedbytes.copyusinglengthprefix... the problem is, it can write an invalid 2 byte prefix spanning two blocks (ie, last byte of block n and first byte of block n+1), which fillslicewithprefix won't decode correctly.",
        "label": 46
    },
    {
        "text": "spellchecker uses default iw mergefactor rammb settings of these settings seem odd - i'd like to investigate what makes most sense here.",
        "label": 40
    },
    {
        "text": "nrt reader writer over ramdirectory memory leak with nrt reader/writer, emptying an index using: writer.deleteall() writer.commit() doesn't release all allocated memory. for example the following code will generate a memory leak: /** reveals a memory leak in nrt reader/writer<br> the following main() does 10k cycles of: <ul> <li>add 10k empty documents to index writer</li> <li>commit()</li> <li>open nrt reader over the writer, and immediately close it</li> <li>delete all documents from the writer</li> <li>commit changes to the writer</li> </ul> running with -xmx256m results in an oome after ~2600 cycles */ public static void main(string[] args) throws exception { ramdirectory d = new ramdirectory(); indexwriter w = new indexwriter(d, new indexwriterconfig(version.lucene_33, new keywordanalyzer())); document doc = new document(); for(int i = 0; i < 10000; i++) { for(int j = 0; j < 10000; ++j) { w.adddocument(doc); } w.commit(); indexreader.open(w, true).close(); w.deleteall(); w.commit(); } w.close(); d.close(); }",
        "label": 33
    },
    {
        "text": "move to java in trunk the dev list thread \"[vote] move trunk to java 8\" passed. http://markmail.org/thread/zcddxioz2yvsdqkc this issue is to actually move trunk to java 8.",
        "label": 41
    },
    {
        "text": "remove standardtokenizerinterface this interface existed for backcompat, so that each impl had at least some common minimal interface, and could be used by standardtokenizer. however, in lucene-5999 backcompat for standard tokenizer was implemented using separate named classes. we should remove this interface, as it no longer serves a purpose.",
        "label": 41
    },
    {
        "text": "transition version constants from lucene mn to lucene m n we should fix this, otherwise the constants will be hard to read (e.g. version.lucene_410, is it 4.1.0 or 4.10 or whatever). i do not want this to be an excuse for an arbitrary 5.0 release that does not have the features expected of a major release",
        "label": 53
    },
    {
        "text": "improve selection of testpoint for geocomplexpolygon i have been checking the effect of the testpoint on geocomplexpolygon and it seems performance can change quite a bit depending on the choice.  the results with random polygons with 20k points shows that a good choice is to ue the center of mass of the shape. on the worst case the performance is similar to what we have now and the best case is twice as fast for within() and getrelationship() methods. therefore i would like to propose to use that point whenever possible.    ",
        "label": 25
    },
    {
        "text": "version cleanup there are still a couple things taking version in their constructor (analyzinginfixsuggester/blendedinfixsuggester), test_version_current isn't needed anymore, and there are a number of places with :post-release-update-version:, which should be possible to remove completely.",
        "label": 41
    },
    {
        "text": "a tokenfilter to decompose compound words a tokenfilter to decompose compound words you find in many germanic languages (like german, swedish, ...) into single tokens. an example: donaudampfschiff would be decomposed to donau, dampf, schiff so that you can find the word even when you only enter \"schiff\". i use the hyphenation code from the apache xml project fop (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. currently i use the fop jars directly. i only use a handful of classes from the fop project. my question now: would it be ok to copy this classes over to the lucene project (renaming the packages of course) or should i stick with the dependency to the fop jars? the fop code uses the asf v2 license as well. what do you think?",
        "label": 15
    },
    {
        "text": "allow similarity to encode norms other than a single byte lucene-3628 cut over norms to docvalues. this removes the long standing limitation that norms are a single byte. yet, we still need to expose this functionality to similarity to write / encode norms in a different format.",
        "label": 46
    },
    {
        "text": "query mergebooleanqueries argument should be of type booleanquery  instead of query  the method #mergebooleanqueries accepts query[] and casts elements to booleanquery without checking. this will guarantee a classcastexception if it is not a boolean query. we should enforce this by changing the signature. this won't really break back compat. as it only works with instances of booleanquery.",
        "label": 33
    },
    {
        "text": "use generics for the type of assigned class in classifier currently classifier#assignclass returns a classificationresult which holds the class as a string while there are use cases where this would be not optimal as assigned labels types could be known upfront, therefore having a classifier<t> returning a classificationresult<t> would be better. side node: current implementations could be improved by using bytesref instead of string .",
        "label": 50
    },
    {
        "text": "remove synchronization in segmentreader isdeleted removes segmentreader.isdeleted synchronization by using a volatile deleteddocs variable on java 1.5 platforms. on java 1.4 platforms synchronization is limited to obtaining the deleteddocs reference.",
        "label": 33
    },
    {
        "text": "more thorough testing of analysis chains in lucene we essentially test each analysis component separately. we also give some good testing to the example analyzers we provide that combine them. but we don't test various combinations that are possible: which is bad because it doesnt test possibilities for custom analyzers (especially since lots of solr users etc define their own).",
        "label": 40
    },
    {
        "text": "testnorms oom https://builds.apache.org/job/lucene-trunk/1917/consoletext it uses linedocs so you need the special file. ant test -dtestcase=testnorms -dtests.method=testnormsnotpresent -dtests.seed=dd43c1ee3741da4f -ests.locale=en_ca -dtests.timezone=australia/brisbane -dtests.multiplier=3 -dtests.nightly=true -dargs=\"-dfile.encoding=iso8859-1\"",
        "label": 33
    },
    {
        "text": "the hunspell filter should support compressed hunspell dictionaries openoffice dictionaries are often compressed via some aliases on the beginning of the affixe file. the french one for instance. currently the hunspell filter does not read the aliases.",
        "label": 7
    },
    {
        "text": "a replacement for asciifoldingfilter that does a more thorough job of removing diacritical marks or non spacing modifiers  the isolatin1accentfilter takes unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed. for example \u00e9 becomes e. however another equally valid way of representing an accented character in unicode is to have the unaccented character followed by a non-spacing modifier character (like this: \u00e9 ) the isolatin1accentfilter doesn't handle the accents in decomposed unicode characters at all. additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character such as \u0142 but which to make searching easier you want to fold onto the latin1 lookalike version l . the unicodenormalizationfilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( \u0142 -> l )",
        "label": 40
    },
    {
        "text": "need a test that uses termsenum seekexact   which returns true  then calls next  i tried to do some seekexact (where the result must exist) then next()ing in the faceting module, and it seems like there could be a bug here. i think we should add a test that mixes seekexact/seekceil/next like this, to ensure that if seekexact returns true, that the enum is properly positioned.",
        "label": 33
    },
    {
        "text": "snowballanalyzer lacks a constructor that takes a set of stop words as discussed on the java-user list, the snowballanalyzer has been updated to use a set of stop words. however, there is no constructor which accepts a set, there's only the original string[] one this is an issue, because most of the common sources of stop words (eg stopanalyzer) have deprecated their string[] stop word lists, and moved over to sets (eg stopanalyzer.english_stop_words_set). so, for now, you either have to use a deprecated field on stopanalyzer, or manually turn the set into an array so you can pass it to the snowballanalyzer i would suggest that a constructor is added to snowballanalyzer which accepts a set. not sure if the old string[] one should be deprecated or not. a sample patch against 2.9.1 to add the constructor is: \u2014 snowballanalyzer.java.orig 2009-12-15 11:14:08.000000000 +0000 +++ snowballanalyzer.java 2009-12-14 12:58:37.000000000 +0000 @@ -67,6 +67,12 @@ stopset = stopfilter.makestopset(stopwords); } + /** builds the named analyzer with the given stop words. */ + public snowballanalyzer(version matchversion, string name, set stopwordsset) { + this(matchversion, name); + stopset = stopwordsset; + } +",
        "label": 40
    },
    {
        "text": "spanqueryfilter addition similar to the queryfilter (or whatever it is called now) the spanqueryfilter is a regular lucene filter, but it also can return spans-like information. this is useful if you not only want to filter based on a query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered spanquery. patch to come shortly also contains a caching mechanism for the spanqueryfilter",
        "label": 15
    },
    {
        "text": "make sortedsetdocvaluesreaderstate customizable we have a reader that have a different data structure (in memory) where the cost of computing ordinals per reader open is too expensive in the realtime setting. we are maintaining in memory data structure that supports all functionality and would like to leverage sortedsetdocvaluesaccumulator.",
        "label": 33
    },
    {
        "text": "lucene search not scalling i've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. when adding more concurrent searches doing exactly the same search the average time increases drastically. i've profiled the search classes and found that the whole of lucene blocks on org.apache.lucene.index.segmentcorereaders.gettermsreader org.apache.lucene.util.virtualmethod public synchronized int getimplementationdistance org.apache.lucene.util.attributesourcew.getattributeinterfaces these cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. note: that the index is not being updates at all, so not refresh methods are called at any stage. some questions: why do we need synchronization here? there must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation. i'll do some experiments by removing the synchronization from the methods of these classes.",
        "label": 46
    },
    {
        "text": "cleanup mmapdirectory to use only one mmapindexinput impl with mapping sized of powers of robert and me discussed a little bit after mike's investigations, that using singlemmapindexinput together with multimmapindexinput leads to hotspot slowdowns sometimes. we had the following ideas: multimmapindexinput is almost as fast as singlemmapindexinput, as the switching between buffer boundaries is done in exception catch blocks. so normal code path is always the same like for single* only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufoffset = (int) (pos % maxbufsize); - very strange way of calculating modulo in the original code) because of speed we suggest to no longer use arbitrary buffer sizes. we should pass only the power of 2 to the indexinput as size. all calculations in seek and anywhere else would be simple bit shifts and and operations (the and masks for the modulo can be calculated in the ctor like numericutils does when calculating precisionsteps). the maximum buffer size will now be 2^30, not 2^31-1. but thats not an issue at all. in my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the o/s. we will provide a patch with those cleanups.",
        "label": 53
    },
    {
        "text": "parallelreader crashes when trying to merge into a new index parallelreader causes a nullpointerexception in org.apache.lucene.index.parallelreader$paralleltermpositions.seek(parallelreader.java:318) when trying to merge into a new index. see test case and sample output: $ svn diff index: src/test/org/apache/lucene/index/testparallelreader.java =================================================================== \u2014 src/test/org/apache/lucene/index/testparallelreader.java (revision 179785) +++ src/test/org/apache/lucene/index/testparallelreader.java (working copy) @@ -57,6 +57,13 @@ } + public void testmerge() throws exception { + directory dir = new ramdirectory(); + indexwriter w = new indexwriter(dir, new standardanalyzer(), true); + w.addindexes(new indexreader[] { ((indexsearcher) parallel).getindexreader() } ); + w.close(); + } + private void querytest(query query) throws ioexception { hits parallelhits = parallel.search(query); hits singlehits = single.search(query); $ ant -dtestcase=testparallelreader test buildfile: build.xml [...] test: [mkdir] created dir: /users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test [junit] testsuite: org.apache.lucene.index.testparallelreader [junit] tests run: 2, failures: 0, errors: 1, time elapsed: 1.993 sec [junit] testcase: testmerge(org.apache.lucene.index.testparallelreader): caused an error [junit] null [junit] java.lang.nullpointerexception [junit] at org.apache.lucene.index.parallelreader$paralleltermpositions.seek(parallelreader.java:318) [junit] at org.apache.lucene.index.parallelreader$paralleltermdocs.seek(parallelreader.java:294) [junit] at org.apache.lucene.index.segmentmerger.appendpostings(segmentmerger.java:325) [junit] at org.apache.lucene.index.segmentmerger.mergeterminfo(segmentmerger.java:296) [junit] at org.apache.lucene.index.segmentmerger.mergeterminfos(segmentmerger.java:270) [junit] at org.apache.lucene.index.segmentmerger.mergeterms(segmentmerger.java:234) [junit] at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:96) [junit] at org.apache.lucene.index.indexwriter.addindexes(indexwriter.java:596) [junit] at org.apache.lucene.index.testparallelreader.testmerge(testparallelreader.java:63) [junit] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [junit] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [junit] test org.apache.lucene.index.testparallelreader failed build failed /users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188: tests failed! total time: 16 seconds $",
        "label": 55
    },
    {
        "text": " parallel multisearcher  using sort object changes the scores example: hits hits=multisearcher.search(query); returns different scores for some documents than hits hits=multisearcher.search(query, sort.relevance); (both for multisearcher and parallelmultisearcher) the documents returned will be the same and in the same order, but the scores in the second case will seem out of order. inspecting the explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores. the document with the highest score should have score 1.0, so all document scores are divided by the highest score. (assuming the highest score was>1.0) however, for multisearcher and parallelmultisearcher, this normalization factor is applied per index, before merging the results together (the merge itself is ok though). an example: if you use hits hits=multisearcher.search(query, sort.relevance); for a multisearcher with two subsearchers, the first document will have score 1.0. the next documents from the same subsearcher will have decreasing scores. the first document from the other subsearcher will however have score 1.0 again ! the same applies for other sort objects, but it is less visible. i will post a testcase demonstrating the problem and suggested patches to solve it in a moment...",
        "label": 55
    },
    {
        "text": "index file format   example for frequency file  frq is wrong reported by johan stuyts - http://www.nabble.com/possible-documentation-error--p7012445.html - frequency file example says: for example, the termfreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of vints: 15, 22, 3 it should be: for example, the termfreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of vints: 15, 8, 3",
        "label": 15
    },
    {
        "text": "regexp automaton causes npe on terms intersect calling org.apache.lucene.index.terms.intersect(automaton, null) causes an npe: string index_path = <path to index> string term = <a valid term name> directory directory = fsdirectory.open(paths.get(index_path)); indexreader reader = directoryreader.open(directory); fields fields = multifields.getfields(reader); terms terms = fields.terms(args[1]); compiledautomaton automaton = new compiledautomaton( new regexp(\"do_not_match_anything\").toautomaton()); termsenum te = terms.intersect(automaton, null); throws: exception in thread \"main\" java.lang.nullpointerexception at org.apache.lucene.codecs.blocktree.intersecttermsenum.<init>(intersecttermsenum.java:127) at org.apache.lucene.codecs.blocktree.fieldreader.intersect(fieldreader.java:185) at org.apache.lucene.index.multiterms.intersect(multiterms.java:85) ...",
        "label": 33
    },
    {
        "text": "add doc values support to memoryindex sometimes queries executed via the memoryindex require certain things to be stored as doc values. today this isn't possible because the memory index doesn't support this and these queries silently return no results.",
        "label": 30
    },
    {
        "text": "valuesource getsortfield shouldn't throw ioexception valuesource.getsortfield just returns a new valuesourcesortfield, whose constructor doesn't declare any checked exceptions. so adding the throws clause to the method declaration means adding pointless try-catch warts to client code.",
        "label": 2
    },
    {
        "text": "missing a null check in booleanquery tostring string  our queryparser/tokenizer in some situations creates null query and was added as a clause to boolean query. when we try to log the query, npe is thrown from log(booleanquery). in booleanquery.tostring(string), a simple null check is overlooked.",
        "label": 29
    },
    {
        "text": "broadword bit selection ",
        "label": 1
    },
    {
        "text": "cachingnaivebayesclassifiertest testperformance  fails on slow machines the 7.3 jenkins smoke tester has failed a couple of times due to cachingnaivebayesclassifiertest.testperformance() (see https://builds.apache.org/job/lucene-solr-smokerelease-7.3/9/ for example). i don't think performance tests like this are very useful as part of the standard test suite, because they depend too much on what else is happening on the machine they're being run on.",
        "label": 50
    },
    {
        "text": "testlucene45docvaluesformat testrandomexceptions failure my jenkins found a seed http://jenkins.sarowe.net/job/lucene-solr-tests-5.x-java8/1895/ that reproduces for me on os x:    [junit4]   2> note: reproduce with: ant test  -dtestcase=testlucene45docvaluesformat -dtests.method=testrandomexceptions -dtests.seed=85ef209f818919dc -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=it -dtests.timezone=asia/vladivostok -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 0.65s j4 | testlucene45docvaluesformat.testrandomexceptions <<<    [junit4]    > throwable #1: java.lang.assertionerror: hit unexpected filenotfoundexception: file=_1q.si    [junit4]    >  at __randomizedtesting.seedinfo.seed([85ef209f818919dc:edc04e5b1f87407c]:0)    [junit4]    >  at org.apache.lucene.index.indexfiledeleter.deletefile(indexfiledeleter.java:740)    [junit4]    >  at org.apache.lucene.index.indexfiledeleter.deletependingfiles(indexfiledeleter.java:516)    [junit4]    >  at org.apache.lucene.index.indexfiledeleter.deletenewfiles(indexfiledeleter.java:719)    [junit4]    >  at org.apache.lucene.index.indexwriter.flushfailed(indexwriter.java:4684)    [junit4]    >  at org.apache.lucene.index.documentswriter$flushfailedevent.process(documentswriter.java:722)    [junit4]    >  at org.apache.lucene.index.indexwriter.processevents(indexwriter.java:4734)    [junit4]    >  at org.apache.lucene.index.indexwriter.processevents(indexwriter.java:4725)    [junit4]    >  at org.apache.lucene.index.indexwriter.doflush(indexwriter.java:3109)    [junit4]    >  at org.apache.lucene.index.indexwriter.flush(indexwriter.java:3076)    [junit4]    >  at org.apache.lucene.index.indexwriter.shutdown(indexwriter.java:1078)    [junit4]    >  at org.apache.lucene.index.indexwriter.close(indexwriter.java:1123)    [junit4]    >  at org.apache.lucene.index.baseindexfileformattestcase.testrandomexceptions(baseindexfileformattestcase.java:499)    [junit4]    >  at org.apache.lucene.index.basedocvaluesformattestcase.testrandomexceptions(basedocvaluesformattestcase.java:78)    [junit4]    >  at java.lang.thread.run(thread.java:745)",
        "label": 33
    },
    {
        "text": "remove ramusageestimator unsafe calls this is unnecessary risk. we should remove this stuff, it is not needed here. we are a search engine, not a ram calculator.",
        "label": 53
    },
    {
        "text": "fix snowballanalyzer casing behavior for turkish language lucene-2102 added a new tokenfilter to handle turkish unique casing behavior correctly. we should fix the casing behavior in snowballanalyzer too as it supports a turkishstemmer.",
        "label": 46
    },
    {
        "text": "for extremely wide shapes   degrees  disterrpct is not used correctly when a shape is given to a prefixtreestrategy (index or query time), it needs to know how many levels down the prefix tree to go for a target precision (disterrpct). disterrpct is basically a fraction of the radius of the shape, defaulting to 2.5% (0.0025). if the shape presented is extremely wide, > 180 degrees, then the internal calculations in spatialargs.calcdistancefromerrpct(...) will wrongly measure the shape's size as having width < 180 degrees, yielding more accuracy than intended. given that this happens for unrealistic shape sizes and results in more accuracy, i am flagging this as \"minor\", but a bug nonetheless. indeed, this was discovered as a result of someone using lucene-spatial incorrectly, not for an actual shape they have. but in the extreme [erroneous] case they had, they had 566k terms generated, when it should have been ~1k tops.",
        "label": 10
    },
    {
        "text": "supplementary character handling in chartokenizer chartokenizer is an abstract base class for all tokenizers operating on a character level. yet, those tokenizers still use char primitives instead of int codepoints. chartokenizer should operate on codepoints and preserve bw compatibility.",
        "label": 53
    },
    {
        "text": "parallelreader fails on deletes and on seeks of previously unused fields in using parallelreader i've hit two bugs: 1. parallelreader.dodelete() and doundeleteall() call dodelete() and doundeleteall() on the subreaders, but these methods do not set haschanges. thus the changes are lost when the readers are closed. the fix is to call deletedocument() and undeleteall() on the subreaders instead. 2. parallelreader discovers the fields in each subindex by using indexreader.getfieldnames() which only finds fields that have occurred on at least one document. in general a parallel index is designed with assignments of fields to sub-indexes and term seeks (including searches) may be done on any of those fields, even if no documents in a particular state of the index have yet had an assigned field. seeks/searches on fields that have not yet been indexed generated an npe in parallelreader's various inner class seek() and next() methods because fieldtoreader.get() returns null on the unseen field. the fix is to extend the add() methods to supply the correct list of fields for each subindex. patch that corrects both of these issues attached.",
        "label": 55
    },
    {
        "text": "make memoryindex thread safe for queries we want to be able to run multiple queries at once over a memoryindex in luwak (see https://github.com/flaxsearch/luwak/commit/49a8fba5764020c2f0e4dc29d80d93abb0231191), but this isn't possible with the current implementation. however, looking at the code, it seems that it would be relatively simple to make memoryindex thread-safe for reads/queries.",
        "label": 2
    },
    {
        "text": "gc resources in terminfosreader when exception occurs in its constructor i replaced indexmodifier with indexwriter in test case teststressindexing and noticed the test failed from time to time because some .tis file is still open when mockramdirectory.close() is called. it turns out it is because .tis file is not closed if an exception occurs in terminfosreader's constructor.",
        "label": 33
    },
    {
        "text": "random access non ram resident indexdocvalues  csf  there should be a way to get specific indexdocvalues by going through the directory rather than loading all of the values into memory.",
        "label": 46
    },
    {
        "text": "openbitset prevsetbit  find a previous set bit in an openbitset. useful for parent testing in nested document query execution lucene-2454 .",
        "label": 39
    },
    {
        "text": "bob carpenter's fuzzytermenum refactoring i'll just paste bob's complete email here. i refactored the org.apache.lucene.search.fuzzytermenum edit distance implementation. it now only uses a single pair of arrays, and those never get resized. that required turning the order of text/target around in the loops. you'll see that with the pair of arrays method, they get re-used hand-over-hand, and are assigned to local variables in the tight loops. i removed the calculation of min distance and replaced it with a boolean \u2013 the min wasn't needed, only the test vs. the max. i also flipped some variables around so there's one less addition in the very inner loop and the arrays are now looping in the ordinary way (starting at 0 with a < comparison). i also commented out the redundant definition of the public close() [which just called super.close() and had none of its own doc.] i also just compute the max distance each time rather than fiddling with an array \u2013 it's just a little arithmetic done once per term, but that could be put back. i also rewrote min(int,int,int) to get rid of intermediate assignments. is there a lib for this kind of thing? an intermediate refactoring that does the hand-over-hand with the existing array and resizing strategy is in fuzzytermenum.intermed.java. i ran the unit tests as follows on both versions (my hat's off to the build.xml author(s) \u2013 this all just worked out of the box and was really easy to follow the first through): c:\\java\\lucene-2.0.0>ant -djunit.includes=\"\" -dtestcase=testfuzzyquery test buildfile: build.xml javacc-uptodate-check: javacc-notice: init: common.compile-core: [javac] compiling 1 source file to c:\\java\\lucene-2.0.0\\build\\classes\\java compile-core: compile-demo: common.compile-test: compile-test: test: [junit] testsuite: org.apache.lucene.search.testfuzzyquery [junit] tests run: 2, failures: 0, errors: 0, time elapsed: 0.453 sec build successful total time: 2 seconds does anyone have regression/performance test harnesses? the unit tests were pretty minimal (which is a good thing!). it'd be nice to test the min impl (ternary vs. if/then) and the assumption about not allocating an array of max distances. all told, the refactored version should be a modest speed improvement, mainly from unfolding the arrays so they're local one-dimensional refs. i don't know what the protocol is for one-off contributions. i'm happy with the apache license, so that shouldn't be a problem. i also don't know whether you use tabs or spaces \u2013 i untabified the final version and used your two-space format in emacs. bob carpenter package org.apache.lucene.search; /** copyright 2004 the apache software foundation * licensed under the apache license, version 2.0 (the \"license\"); you may not use this file except in compliance with the license. you may obtain a copy of the license at * http://www.apache.org/licenses/license-2.0 * unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \"as is\" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. */ import org.apache.lucene.index.indexreader; import org.apache.lucene.index.term; import java.io.ioexception; /** subclass of filteredtermenum for enumerating all terms that are similiar to the specified filter term. * <p>term enumerations are always ordered by term.compareto(). each term in the enumeration is greater than all that precede it. */ public final class fuzzytermenum extends filteredtermenum { /* this should be somewhere around the average long word. if it is longer, we waste time and space. if it is shorter, we waste a little bit of time growing the array as we encounter longer words. */ private static final int typical_longest_word_in_index = 19; /* allows us save time required to create a new array everytime similarity is called. these are slices that will be reused during dynamic programming hand-over-hand style. */ private final int[] d0; private final int[] d1; private float similarity; private boolean endenum = false; private term searchterm = null; private final string field; private final string text; private final string prefix; private final float minimumsimilarity; private final float scale_factor; /** creates a fuzzytermenum with an empty prefix and a minsimilarity of 0.5f. <p> after calling the constructor the enumeration is already pointing to the first valid term if such a term exists. * @param reader @param term @throws ioexception @see #fuzzytermenum(indexreader, term, float, int) */ public fuzzytermenum(indexreader reader, term term) throws ioexception { this(reader, term, fuzzyquery.defaultminsimilarity, fuzzyquery.defaultprefixlength); } /** * creates a fuzzytermenum with an empty prefix. * <p> * after calling the constructor the enumeration is already pointing to the first * valid term if such a term exists. * * @param reader * @param term * @param minsimilarity * @throws ioexception * @see #fuzzytermenum(indexreader, term, float, int) */ public fuzzytermenum(indexreader reader, term term, float minsimilarity) throws ioexception { this(reader, term, minsimilarity, fuzzyquery.defaultprefixlength); } /** * constructor for enumeration of all terms from specified <code>reader</code> which share a prefix of * length <code>prefixlength</code> with <code>term</code> and which have a fuzzy similarity > * <code>minsimilarity</code>. * <p> * after calling the constructor the enumeration is already pointing to the first * valid term if such a term exists. * * @param reader delivers terms. * @param term pattern term. * @param minsimilarity minimum required similarity for terms from the reader. default value is 0.5f. * @param prefixlength length of required common prefix. default value is 0. * @throws ioexception */ public fuzzytermenum(indexreader reader, term term, final float minsimilarity, final int prefixlength) throws ioexception { super(); if (minsimilarity >= 1.0f) throw new illegalargumentexception(\"minimumsimilarity cannot be greater than or equal to 1\"); else if (minsimilarity < 0.0f) throw new illegalargumentexception(\"minimumsimilarity cannot be less than 0\"); if(prefixlength < 0) throw new illegalargumentexception(\"prefixlength cannot be less than 0\"); this.minimumsimilarity = minsimilarity; this.scale_factor = 1.0f / (1.0f - minimumsimilarity); this.searchterm = term; this.field = searchterm.field(); //the prefix could be longer than the word. //it's kind of silly though. it means we must match the entire word. final int fullsearchtermlength = searchterm.text().length(); final int realprefixlength = prefixlength > fullsearchtermlength ? fullsearchtermlength : prefixlength; this.text = searchterm.text().substring(realprefixlength); this.prefix = searchterm.text().substring(0, realprefixlength); this.d0 = new int[this.text.length()+1]; this.d1 = new int[this.d0.length]; setenum(reader.terms(new term(searchterm.field(), prefix))); } /** * the termcompare method in fuzzytermenum uses levenshtein distance to * calculate the distance between the given term and the comparing term. */ protected final boolean termcompare(term term) { if (field == term.field() && term.text().startswith(prefix)) { final string target = term.text().substring(prefix.length()); this.similarity = similarity(target); return (similarity > minimumsimilarity); } endenum = true; return false; } public final float difference() { return (float)((similarity - minimumsimilarity) * scale_factor); } public final boolean endenum() { return endenum; } /****************************** * compute levenshtein distance ******************************/ /** * finds and returns the smallest of three integers */ private static final int min(int a, int b, int c) { // removed assignments to use double ternary return (a < b) ? ((a < c) ? a : c) : ((b < c) ? b: c); // alt form is: // if (a < b) { if (a < c) return a; else return c; } // if (b < c) return b; else return c; } /** * <p>similarity returns a number that is 1.0f or less (including negative numbers) * based on how similar the term is compared to a target term. it returns * exactly 0.0f when * <pre> * editdistance < maximumeditdistance</pre> * otherwise it returns: * <pre> * 1 - (editdistance / length)</pre> * where length is the length of the shortest term (text or target) including a * prefix that are identical and editdistance is the levenshtein distance for * the two words.</p> * * <p>embedded within this algorithm is a fail-fast levenshtein distance * algorithm. the fail-fast algorithm differs from the standard levenshtein * distance algorithm in that it is aborted if it is discovered that the * mimimum distance between the words is greater than some threshold. * * <p>to calculate the maximum distance threshold we use the following formula: * <pre> * (1 - minimumsimilarity) * length</pre> * where length is the shortest term including any prefix that is not part of the * similarity comparision. this formula was derived by solving for what maximum value * of distance returns false for the following statements: * <pre> * similarity = 1 - ((float)distance / (float) (prefixlength + math.min(textlen, targetlen))); * return (similarity > minimumsimilarity);</pre> * where distance is the levenshtein distance for the two words. * </p> * <p>levenshtein distance (also known as edit distance) is a measure of similiarity * between two strings where the distance is measured as the number of character * deletions, insertions or substitutions required to transform one string to * the other string. * @param target the target word or phrase * @return the similarity, 0.0 or less indicates that it matches less than the required * threshold and 1.0 indicates that the text and target are identical */ private synchronized final float similarity(final string target) { final int m = target.length(); final int n = text.length(); if (n == 0) { //we don't have anything to compare. that means if we just add //the letters for m we get the new word return prefix.length() == 0 ? 0.0f : 1.0f - ((float) m / prefix.length()); } if (m == 0) { return prefix.length() == 0 ? 0.0f : 1.0f - ((float) n / prefix.length()); } final int maxdistance = calculatemaxdistance(m); if (maxdistance < math.abs(m-n)) { //just adding the characters of m to n or vice-versa results in //too many edits //for example \"pre\" length is 3 and \"prefixes\" length is 8. we can see that //given this optimal circumstance, the edit distance cannot be less than 5. //which is 8-3 or more precisesly math.abs(3-8). //if our maximum edit distance is 4, then we can discard this word //without looking at it. return 0.0f; } int[] dlast = d0; // set locals for efficiency int[] dcurrent = d1; for (int j = 0; j <= n; j++) dcurrent[j] = j; for (int i = 0; i < m; ) { final char s_i = target.charat; int[] dtemp = dlast; dlast = dcurrent; // previously: d[i-i] dcurrent = dtemp; // previously: d[i] boolean prune = (dcurrent[0] = ++i) > maxdistance; // true if d[i][0] is too large for (int j = 0; j < n; j++) { dcurrent[j+1] = (s_i == text.charat(j)) ? min(dlast[j+1]+1, dcurrent[j]+1, dlast[j]) : min(dlast[j+1], dcurrent[j], dlast[j])+1; if (prune && dcurrent[j+1] <= maxdistance) prune = false; } // (prune==false) iff (dcurrent[j] < maxdistance) for some j if (prune) { return 0.0f; } } // this will return less than 0.0 when the edit distance is // greater than the number of characters in the shorter word. // but this was the formula that was previously used in fuzzytermenum, // so it has not been changed (even though minimumsimilarity must be // greater than 0.0) return 1.0f - dcurrent[n]/(float)(prefix.length() + math.min(n,m)); } private int calculatemaxdistance(int m) { return (int) ((1-minimumsimilarity) * (math.min(text.length(), m) + prefix.length())); } /* this is redundant public void close() throws ioexception { super.close(); //call super.close() and let the garbage collector do its work. } */ } package org.apache.lucene.search; /** * copyright 2004 the apache software foundation * * licensed under the apache license, version 2.0 (the \"license\"); * you may not use this file except in compliance with the license. * you may obtain a copy of the license at * * http://www.apache.org/licenses/license-2.0 * * unless required by applicable law or agreed to in writing, software * distributed under the license is distributed on an \"as is\" basis, * without warranties or conditions of any kind, either express or implied. * see the license for the specific language governing permissions and * limitations under the license. */ import org.apache.lucene.index.indexreader; import org.apache.lucene.index.term; import java.io.ioexception; /** subclass of filteredtermenum for enumerating all terms that are similiar * to the specified filter term. * * <p>term enumerations are always ordered by term.compareto(). each term in * the enumeration is greater than all that precede it. */ public final class fuzzytermenum extends filteredtermenum { /* this should be somewhere around the average long word. * if it is longer, we waste time and space. if it is shorter, we waste a * little bit of time growing the array as we encounter longer words. */ private static final int typical_longest_word_in_index = 19; /* allows us save time required to create a new array * everytime similarity is called. these are slices that * will be reused during dynamic programming hand-over-hand * style. they get resized, if necessary, by growdistancearrays(int). */ private int[] d0; private int[] d1; private float similarity; private boolean endenum = false; private term searchterm = null; private final string field; private final string text; private final string prefix; private final float minimumsimilarity; private final float scale_factor; /** * creates a fuzzytermenum with an empty prefix and a minsimilarity of 0.5f. * <p> * after calling the constructor the enumeration is already pointing to the first * valid term if such a term exists. * * @param reader * @param term * @throws ioexception * @see #fuzzytermenum(indexreader, term, float, int) */ public fuzzytermenum(indexreader reader, term term) throws ioexception { this(reader, term, fuzzyquery.defaultminsimilarity, fuzzyquery.defaultprefixlength); } /** creates a fuzzytermenum with an empty prefix. <p> after calling the constructor the enumeration is already pointing to the first valid term if such a term exists. * @param reader @param term @param minsimilarity @throws ioexception @see #fuzzytermenum(indexreader, term, float, int) */ public fuzzytermenum(indexreader reader, term term, float minsimilarity) throws ioexception { this(reader, term, minsimilarity, fuzzyquery.defaultprefixlength); } /** constructor for enumeration of all terms from specified <code>reader</code> which share a prefix of length <code>prefixlength</code> with <code>term</code> and which have a fuzzy similarity > <code>minsimilarity</code>. <p> after calling the constructor the enumeration is already pointing to the first valid term if such a term exists. * @param reader delivers terms. @param term pattern term. @param minsimilarity minimum required similarity for terms from the reader. default value is 0.5f. @param prefixlength length of required common prefix. default value is 0. @throws ioexception */ public fuzzytermenum(indexreader reader, term term, final float minsimilarity, final int prefixlength) throws ioexception { super(); if (minsimilarity >= 1.0f) throw new illegalargumentexception(\"minimumsimilarity cannot be greater than or equal to 1\"); else if (minsimilarity < 0.0f) throw new illegalargumentexception(\"minimumsimilarity cannot be less than 0\"); if(prefixlength < 0) throw new illegalargumentexception(\"prefixlength cannot be less than 0\"); this.minimumsimilarity = minsimilarity; this.scale_factor = 1.0f / (1.0f - minimumsimilarity); this.searchterm = term; this.field = searchterm.field(); //the prefix could be longer than the word. //it's kind of silly though. it means we must match the entire word. final int fullsearchtermlength = searchterm.text().length(); final int realprefixlength = prefixlength > fullsearchtermlength ? fullsearchtermlength : prefixlength; this.text = searchterm.text().substring(realprefixlength); this.prefix = searchterm.text().substring(0, realprefixlength); growdistancearrays(typical_longest_word_in_index); setenum(reader.terms(new term(searchterm.field(), prefix))); } /** the termcompare method in fuzzytermenum uses levenshtein distance to calculate the distance between the given term and the comparing term. */ protected final boolean termcompare(term term) unknown macro: { if (field == term.field() && term.text().startswith(prefix)) { final string target = term.text().substring(prefix.length()); this.similarity = similarity(target); return (similarity > minimumsimilarity); } endenum = true; return false; } public final float difference() { return (float)((similarity - minimumsimilarity) * scale_factor); } public final boolean endenum() { return endenum; } /****************************** compute levenshtein distance ******************************/ /** finds and returns the smallest of three integers */ private static final int min(int a, int b, int c) { // removed assignments to use double ternary return (a < b) ? ((a < c) ? a : c) : ((b < c) ? b: c); // alt form is: // if (a < b) { if (a < c) return a; else return c; } // if (b < c) return b; else return c; } /** <p>similarity returns a number that is 1.0f or less (including negative numbers) based on how similar the term is compared to a target term. it returns exactly 0.0f when <pre> editdistance < maximumeditdistance</pre> otherwise it returns: <pre> 1 - (editdistance / length)</pre> where length is the length of the shortest term (text or target) including a prefix that are identical and editdistance is the levenshtein distance for the two words.</p> * <p>embedded within this algorithm is a fail-fast levenshtein distance algorithm. the fail-fast algorithm differs from the standard levenshtein distance algorithm in that it is aborted if it is discovered that the mimimum distance between the words is greater than some threshold. * <p>to calculate the maximum distance threshold we use the following formula: <pre> (1 - minimumsimilarity) * length</pre> where length is the shortest term including any prefix that is not part of the similarity comparision. this formula was derived by solving for what maximum value of distance returns false for the following statements: <pre> similarity = 1 - ((float)distance / (float) (prefixlength + math.min(textlen, targetlen))); return (similarity > minimumsimilarity);</pre> where distance is the levenshtein distance for the two words. </p> <p>levenshtein distance (also known as edit distance) is a measure of similiarity between two strings where the distance is measured as the number of character deletions, insertions or substitutions required to transform one string to the other string. @param target the target word or phrase @return the similarity, 0.0 or less indicates that it matches less than the required threshold and 1.0 indicates that the text and target are identical */ private synchronized final float similarity(final string target) { final int m = target.length(); final int n = text.length(); if (n == 0) { //we don't have anything to compare. that means if we just add //the letters for m we get the new word return prefix.length() == 0 ? 0.0f : 1.0f - ((float) m / prefix.length()); } if (m == 0) { return prefix.length() == 0 ? 0.0f : 1.0f - ((float) n / prefix.length()); } final int maxdistance = calculatemaxdistance(m); if (maxdistance < math.abs(m-n)) { //just adding the characters of m to n or vice-versa results in //too many edits //for example \"pre\" length is 3 and \"prefixes\" length is 8. we can see that //given this optimal circumstance, the edit distance cannot be less than 5. //which is 8-3 or more precisesly math.abs(3-8). //if our maximum edit distance is 4, then we can discard this word //without looking at it. return 0.0f; } //let's make sure we have enough room in our array to do the distance calculations. if (d0.length <= m) { growdistancearrays(m); } int[] dlast = d0; // set local vars for efficiency ~ the old d[i-1] int[] dcurrent = d1; // ~ the old d[i] for (int j = 0; j <= m; j++) dcurrent[j] = j; for (int i = 0; i < n; ) { final char s_i = text.charat; int[] dtemp = dlast; dlast = dcurrent; // previously: d[i-i] dcurrent = dtemp; // previously: d[i] boolean prune = (dcurrent[0] = ++i) > maxdistance; // true if d[i][0] is too large for (int j = 0; j < m; j++) { dcurrent[j+1] = (s_i == target.charat(j)) ? min(dlast[j+1]+1, dcurrent[j]+1, dlast[j]) : min(dlast[j+1], dcurrent[j], dlast[j])+1; if (prune && dcurrent[j+1] <= maxdistance) prune = false; } // (prune==false) iff (dcurrent[j] < maxdistance) for some j if (prune) { return 0.0f; } } // this will return less than 0.0 when the edit distance is // greater than the number of characters in the shorter word. // but this was the formula that was previously used in fuzzytermenum, // so it has not been changed (even though minimumsimilarity must be // greater than 0.0) return 1.0f - dcurrent[m]/(float)(prefix.length() + math.min(n,m)); } /** grow the second dimension of the array slices, so that we can calculate the levenshtein difference. */ private void growdistancearrays(int m) { d0 = new int[m+1]; d1 = new int[m+1]; } private int calculatemaxdistance(int m) { return (int) ((1-minimumsimilarity) * (math.min(text.length(), m) + prefix.length())); } /* this is redundant public void close() throws ioexception { super.close(); //call super.close() and let the garbage collector do its work. } */ }",
        "label": 38
    },
    {
        "text": "mergepolicy should require an indexwriter upon construction mergepolicy does not require an iw upon construction, but requires one to be passed as method arg to various methods. this gives the impression as if a single mp instance can be shared across various iw instances, which is not true for all mps (if at all). in addition, logmergepolicy uses the iw instance passed to these methods incosistently, and is currently exposed to potential npes. this issue will change mp to require an iw instance, however for back-compat reasons the following changes will be made: a new mp ctor w/ iw as arg will be introduced. additionally, for back-compat a default ctor will also be declared which will assign null to the member iw. methods that require iw will be deprecated, and new ones will be declared. for back-compat, the new ones will not be made abstract, but will throw uoe, with a comment that they will become abstract in 3.0. all current mp impls will move to use the member instance. the code which calls mp methods will continue to use the deprecated methods, passing an iw even that it won't be necessary --> this is strictly for back-compat. in 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the iw method variants anymore. i hope that i didn't leave anything out. i'm sure i'll find out when i work on the patch .",
        "label": 33
    },
    {
        "text": "constantscorerangequery   fixes  too many clauses  exception constantscorequery wraps a filter (representing a set of documents) and returns a constant score for each document in the set. constantscorerangequery implements a rangequery that works for any number of terms in the range. it rewrites to a constantscorequery that wraps a rangefilter. still needed: unit tests (these classes have been tested and work fine in-house, but the current tests rely on too much application specific code) code review of weight() implementation (i'm unsure if i got all the score normalization stuff right) explain() implementation note: requires java 1.4 for bitset.nextsetbit()",
        "label": 55
    },
    {
        "text": "minpayloadfunction returns when only one payload is present in some experiments with payload scoring through payloadtermquery, i'm seeing 0 returned when using minpayloadfunction. i believe there is a bug there. no time at the moment to flesh out a unit test, but wanted to report it for tracking.",
        "label": 53
    },
    {
        "text": "add katakana stem filter to better deal with certain katakana spelling variants many japanese katakana words end in a long sound that is sometimes optional. for example, \u30d1\u30fc\u30c6\u30a3\u30fc and \u30d1\u30fc\u30c6\u30a3 are both perfectly valid for \"party\". similarly we have \u30bb\u30f3\u30bf\u30fc and \u30bb\u30f3\u30bf that are variants of \"center\" as well as \u30b5\u30fc\u30d0\u30fc and \u30b5\u30fc\u30d0 for \"server\". i'm proposing that we add a katakana stemmer that removes this long sound if the terms are longer than a configurable length. it's also possible to add the variant as a synonym, but i think stemming is preferred from a ranking point of view.",
        "label": 8
    },
    {
        "text": "depth requested in a facetrequest is reset when sampling is in effect facetrequest can be set a depth parameter, which controls the depth of the result tree to be returned. when sampling is enabled (and actually used) the depth parameter gets reset to its default (1).",
        "label": 43
    },
    {
        "text": "add workaround for jre breakiterator bugs on some inputs, the java breakiterator support will internally crash. for example: ant test -dtestcase=testthaianalyzer -dtestmethod=testrandomstrings -dtests.seed=-8005471002120855329:-2517344653287596566 -dtests.multiplier=3",
        "label": 40
    },
    {
        "text": "bytesrefarray append returns wrong index  off by one  pretty simple thing the returned index is incremented before returned. i will add a test and upload a patch",
        "label": 46
    },
    {
        "text": "tokenstream api javadoc improvements change or remove experimental warnings of new tokenstream api improve javadocs for deprecated token constructors javadocs for teesinktokenstream.sinkfilter",
        "label": 29
    },
    {
        "text": "allow to control how payloads are merged lucene handles backwards-compatibility of its data structures by converting them from the old into the new formats during segment merging. payloads are simply byte arrays in which users can store arbitrary data. applications that use payloads might want to convert the format of their payloads in a similar fashion. otherwise it's not easily possible to ever change the encoding of a payload without reindexing. so i propose to introduce a payloadmerger class that the segmentmerger invokes to merge the payloads from multiple segments. users can then implement their own payloadmerger to convert payloads from an old into a new format. in the future we need this kind of flexibility also for column-stride fields (lucene-1231) and flexible indexing codecs. in addition to that it would be nice if users could store version information in the segments file. e.g. they could store \"in segment _2 the term a:b uses payloads of format x.y\".",
        "label": 43
    },
    {
        "text": "spelling mistake in search package summary html scoring javadocs http://lucene.apache.org/core/6_4_1/core/org/apache/lucene/search/package-summary.html#scoring and http://lucene.apache.org/core/5_5_4/core/org/apache/lucene/search/package-summary.html#scoring \"probablistic models such as okapi bm25 and dfr\" should be \"probabilistic models such as okapi bm25 and dfr\"",
        "label": 9
    },
    {
        "text": "geo3d cleanup  regularize path and polygon construction  plus consider adding ellipsoid surface distance method geo3d's way of constructing polygons and paths differs in that in one case you construct points and the other you feed lat/lon values directly to the builder. probably both should be supported for both kinds of entity. also it may be useful to have an accurate point-point ellipsoidal distance function. this is expensive and would be an addition to the arc distance we currently compute. it would probably be called \"surface distance\".",
        "label": 10
    },
    {
        "text": "facet sampling with lucene-5339 facet sampling disappeared. when trying to display facet counts on large datasets (>10m documents) counting facets is rather expensive, as all the hits are collected and processed. sampling greatly reduced this and thus provided a nice speedup. could it be brought back?",
        "label": 43
    },
    {
        "text": "ivy fail goal directs people to non existant page the ivy-fail goal suggests going to http://wiki.apache.org/lucene-java/howtocontribute#antivy which doesn't have a section on ant/ivy anymore. the last revision to have this was #41, in 2013. http://wiki.apache.org/lucene-java/howtocontribute?action=recall&rev=41",
        "label": 47
    },
    {
        "text": "make term offsets work in memoryindex fix the logic for retrieving term offsets from docsandpositionsenum on a memoryindex, and allow subclasses to access them.",
        "label": 33
    },
    {
        "text": "add a lucene3x private segmentinfosformat implemenation we still don't have a lucene3x & preflex version of segment infos format. we need this before we release 4.0",
        "label": 46
    },
    {
        "text": "indexdeletionpolicy delete behaves incorrectly when deleting latest generation i have been looking to provide the ability to rollback committed transactions and encountered some issues. i appreciate indexdeletionpolicy's main motivation is to handle cleaning away old commit points but it does not explicitly state that it can or cannot be used to clean new commit points. if this is not supported then the documentation should ideally state this. if the intention is to support this behaviour then read on ....... there seem to be 2 issues so far: 1) the first attempt to call indexcommit.delete on the latest commit point fails to remove any contents. the subsequent call succeeds however 2) deleting the latest commit point fails to update the segments.gen file to point to segments_n-1. new indexreaders that are opened are then misdirected to open segments_n which has been deleted junit test to follow...",
        "label": 33
    },
    {
        "text": "support query visting   walking out of the discussion in lucene-2868, it could be useful to add a generic query visitor / walker that could be used for more advanced rewriting, optimizations or anything that requires state to be stored as each query is visited. we could keep the interface very simple: public interface queryvisitor {   query visit(query query); } and then use a reflection based visitor like earwin suggested, which would allow implementators to provide visit methods for just querys that they are interested in.",
        "label": 2
    },
    {
        "text": "port url email tokenizer to standardtokenizerinterface  or similar  we should do this so that we can fix the lucene-3358 bug there, and preserve backwards. we also want this mechanism anyway, for upgrading to new unicode versions in the future. we can regenerate the new tld list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever, so that its exactly the same.",
        "label": 40
    },
    {
        "text": "analysis package calls java api i found compile errors when i tried to compile trunk with 1.4 jvm. org.apache.lucene.analysis.normalizecharmap org.apache.lucene.analysis.mappingcharfilter uses character.valueof() which has been added in 1.5. i added a charactercache (+ testcase) with a valueof method as a replacement for that quite useful method. org.apache.lucene.analysis.basetokentestcase uses stringbuilder instead of the synchronized version stringbuffer (available in 1.4) i will attach a patch shortly.",
        "label": 33
    },
    {
        "text": "indexwriter should throw indexformattoooldexc on open  not later during optimize getreader close spinoff of lucene-2618 and also related to the original issue lucene-2523... if you open iw on a too-old index, you don't find out until much later that the index is too old. this is because iw does not go and open segment readers on all segments. it only does so when it's time to apply deletes, do merges, open an nrt reader, etc. this is a serious bug because you can in fact succeed in committing with the new major version of lucene against your too-old index, which is catastrophic because suddenly the old lucene version will no longer open the index, and so your index becomes unusable.",
        "label": 43
    },
    {
        "text": "ant generate maven artifacts target broken for contrib when executing 'ant generate-maven-artifacts' from a pristine checkout of branch_3x/lucene or trunk/lucene the following error is encountered: dist-maven:      [copy] copying 1 file to /home/drew/lucene/branch_3x/lucene/build/contrib/analyzers/common [artifact:install-provider] installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-2:runtime [artifact:pom] an error has occurred while processing the maven artifact tasks. [artifact:pom]  diagnosis: [artifact:pom]  [artifact:pom] unable to initialize pom pom.xml.template: cannot find parent: org.apache.lucene:lucene-contrib for project: org.apache.lucene:lucene-analyzers:jar:3.1-snapshot for project org.apache.lucene:lucene-analyzers:jar:3.1-snapshot [artifact:pom] unable to download the artifact from any repository the contrib portion of the ant build is executed in a subant task which does not pick up the pom definitions for lucene-parent and lucene-contrib from the main build.xml, so the lucene-parent and lucene-controb poms must be loaded specifically as a part of the contrib build using the artifact:pom task.",
        "label": 47
    },
    {
        "text": "regexcapabilities is not serializable the class regexquery is marked serializable by its super class, but it contains a regexcapabilities which is not serializable. thus attempting to serialize the query results in an exception. making regexcapabilities serializable should be no problem since its subclasses contain only serializable classes (java.util.regex.pattern and org.apache.regexp.re).",
        "label": 14
    },
    {
        "text": "rename getunique field terms count  into size  like robert muir said in lucene-3109: also i think there are other improvements we can do here that would be more natural: fields.getuniquefieldcount() -> fields.size() terms.getuniquetermcount() -> terms.size() i believe this dramatically improves understandability (way less 'scary', actually beautiful).",
        "label": 33
    },
    {
        "text": "indexwriter synced field accumulates data leading to a memory leak i am running into a strange outofmemoryerror. my small test application does index and delete some few files. this is repeated for 60k times. optimization is run from every 2k times a file is indexed. index size is 50kb. i did analyze the heapdumpfile and realized that indexwriter.synced field occupied more than half of the heap. that field is a private hashset without a getter. its task is to hold files which have been synced already. there are two calls to addall and one call to add on synced but no remove or clear throughout the lifecycle of the indexwriter instance. according to the eclipse memory analyzer synced contains 32618 entries which look like file names \"_e065_1.del\" or \"_e067.cfs\" the index directory contains 10 files only. i guess synced is holding obsolete data",
        "label": 33
    },
    {
        "text": "create empty args constsant in snowballprogram instead of allocating new object  instead of allocating new object[0] create a proper constant in snowballprogram. the same (for new class[0]) is created in among, although it's less critical because among is called from static initializers ... patch will follow shortly.",
        "label": 40
    },
    {
        "text": "eclipse project name change for just updating the eclipse project name from lucene_solr_branch_4x to lucene_solr_4_1 on the new branch.",
        "label": 47
    },
    {
        "text": "optimize spanfirstquery  spanpositionrangequery spanfirstquery and spanpositionrangequery (spanfirst is just a special case of this), are currently inefficient. take this worst case example: spanfirstquery(\"the\"). currently the code reads all the positions for the term \"the\". but when enumerating spans, once we have passed the allowable range we should move on to the next document (skipto)",
        "label": 40
    },
    {
        "text": "numerictermattribute throws iae after numerictokenstream is exhausted this small test:   public void testclonefullprecisiontoken() throws exception {     fieldtype fieldtype = new fieldtype(intfield.type_not_stored);     fieldtype.setnumericprecisionstep(integer.max_value);     field field = new intfield(\"field\", 17, fieldtype);     tokenstream tokenstream = new cachingtokenfilter(field.tokenstream(null, null));     asserttrue(tokenstream.incrementtoken());   } hits this unexpected exception: there was 1 failure: 1) testclonefullprecisiontoken(org.apache.lucene.analysis.testnumerictokenstream) java.lang.illegalargumentexception: illegal shift value, must be 0..31; got shift=2147483647  at __randomizedtesting.seedinfo.seed([2e1e93ef810cb5f7:ef1304a849574bc7]:0)  at org.apache.lucene.util.numericutils.inttoprefixcodedbytes(numericutils.java:175)  at org.apache.lucene.util.numericutils.inttoprefixcoded(numericutils.java:133)  at org.apache.lucene.analysis.numerictokenstream$numerictermattributeimpl.getbytesref(numerictokenstream.java:165)  at org.apache.lucene.analysis.numerictokenstream$numerictermattributeimpl.clone(numerictokenstream.java:217)  at org.apache.lucene.analysis.numerictokenstream$numerictermattributeimpl.clone(numerictokenstream.java:148)  at org.apache.lucene.util.attributesource$state.clone(attributesource.java:55)  at org.apache.lucene.util.attributesource.capturestate(attributesource.java:288)  at org.apache.lucene.analysis.cachingtokenfilter.fillcache(cachingtokenfilter.java:96)  at org.apache.lucene.analysis.cachingtokenfilter.incrementtoken(cachingtokenfilter.java:70)  at org.apache.lucene.analysis.testnumerictokenstream.testclonefullprecisiontoken(testnumerictokenstream.java:138) because cachingtokenfilter expects that it can capturestate after calling end but numerictokenstream gets angry about this.",
        "label": 53
    },
    {
        "text": "facet counts in facetsaccumulator facetarrays are multiplied as many times as facetscollector getfacetresults is called  in lucene 4.1, only standardfacetsaccumulator could be instantiated. and as of lucene 4.2, it became possible to instantiate facetsaccumulator. i invoked facetscollector.getfacetresults twice, and i saw doubled facet counts. if i invoke it three times, i see facet counts multiplied three times. it all happens in facetsaccumulator.accumulate. standardfacetsaccumulator doesn't have this bug since it frees facetarrays whenever standardfacetsaccumulator.accumulate is called. is it a feature or a bug?",
        "label": 43
    },
    {
        "text": "new tool for reseting the  length norm of fields after changing similarity i've written a little tool that seems like it can/will be very handy as i tweak my custom similarity. i think it would make a good addition to contrib/miscellaneous. class and tests to be attached shortly...",
        "label": 18
    },
    {
        "text": "performance bug  adversary  in standardtokenizer there seem to be some conditions (i don't know how rare or what conditions) that cause standardtokenizer to essentially hang on input: i havent looked hard yet, but as its essentially a dfa i think something wierd might be going on. an easy way to reproduce is with 1mb of underscores, it will just hang forever.   public void testworthyadversary() throws exception {     char buffer[] = new char[1024 * 1024];     arrays.fill(buffer, '_');     int tokencount = 0;     tokenizer ts = new standardtokenizer();     ts.setreader(new stringreader(new string(buffer)));     ts.reset();     while (ts.incrementtoken()) {       tokencount++;     }     ts.end();     ts.close();     assertequals(0, tokencount);   }",
        "label": 47
    },
    {
        "text": "complexphrasequery rewrite can throw exception re matchnodocsquery if mtq sub clause matches no terms with solr v6.3, when i issue this query: http://localhost:8983/solr/bestbuy/select?wt=json&rows=10&q= {!complexphrase%20inorder=false} text:%22maytag~%20(refri~%20or%20refri*)%20%22&fl=id&hl=true&hl.preservemulti=false&hl.fragsize=60&hl.fl=namex,shortdescription,longdescription,artistname,type,manufacturer,department i get this error in the json response: ************************************************************* { \"responseheader\": { \"zkconnected\": true, \"status\": 500, \"qtime\": 8, \"params\": { \"q\": \" {!complexphrase inorder=false} text:\\\"maytag~ (refri~ or refri*) \\\"\", \"hl\": \"true\", \"hl.preservemulti\": \"false\", \"fl\": \"id\", \"hl.fragsize\": \"60\", \"hl.fl\": \"namex,shortdescription,longdescription,artistname,type,manufacturer,department\", \"rows\": \"10\", \"wt\": \"json\" } }, \"response\": { \"numfound\": 2, \"start\": 0, \"docs\": [ { \"id\": \"5411379\" } , { \"id\": \"5411404\" } ] }, \"error\": { \"msg\": \"unknown query type:org.apache.lucene.search.matchnodocsquery\", \"trace\": \"java.lang.illegalargumentexception: unknown query type:org.apache.lucene.search.matchnodocsquery\\n\\tat org.apache.lucene.queryparser.complexphrase.complexphrasequeryparser$complexphrasequery.addcomplexphraseclause(complexphrasequeryparser.java:388)\\n\\tat org.apache.lucene.queryparser.complexphrase.complexphrasequeryparser$complexphrasequery.rewrite(complexphrasequeryparser.java:289)\\n\\tat org.apache.lucene.search.highlight.weightedspantermextractor.extract(weightedspantermextractor.java:230)\\n\\tat org.apache.lucene.search.highlight.weightedspantermextractor.getweightedspanterms(weightedspantermextractor.java:522)\\n\\tat org.apache.lucene.search.highlight.queryscorer.initextractor(queryscorer.java:218)\\n\\tat org.apache.lucene.search.highlight.queryscorer.init(queryscorer.java:186)\\n\\tat org.apache.lucene.search.highlight.highlighter.getbesttextfragments(highlighter.java:195)\\n\\tat org.apache.solr.highlight.defaultsolrhighlighter.dohighlightingbyhighlighter(defaultsolrhighlighter.java:602)\\n\\tat org.apache.solr.highlight.defaultsolrhighlighter.dohighlightingoffield(defaultsolrhighlighter.java:448)\\n\\tat org.apache.solr.highlight.defaultsolrhighlighter.dohighlighting(defaultsolrhighlighter.java:410)\\n\\tat org.apache.solr.handler.component.highlightcomponent.process(highlightcomponent.java:141)\\n\\tat org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:295)\\n\\tat org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:153)\\n\\tat org.apache.solr.core.solrcore.execute(solrcore.java:2213)\\n\\tat org.apache.solr.servlet.httpsolrcall.execute(httpsolrcall.java:654)\\n\\tat org.apache.solr.servlet.httpsolrcall.call(httpsolrcall.java:460)\\n\\tat org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:303)\\n\\tat org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:254)\\n\\tat org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1668)\\n\\tat org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:581)\\n\\tat org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:143)\\n\\tat org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:548)\\n\\tat org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:226)\\n\\tat org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1160)\\n\\tat org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:511)\\n\\tat org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:185)\\n\\tat org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:1092)\\n\\tat org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:141)\\n\\tat org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:213)\\n\\tat org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:119)\\n\\tat org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:134)\\n\\tat org.eclipse.jetty.server.server.handle(server.java:518)\\n\\tat org.eclipse.jetty.server.httpchannel.handle(httpchannel.java:308)\\n\\tat org.eclipse.jetty.server.httpconnection.onfillable(httpconnection.java:244)\\n\\tat org.eclipse.jetty.io.abstractconnection$readcallback.succeeded(abstractconnection.java:273)\\n\\tat org.eclipse.jetty.io.fillinterest.fillable(fillinterest.java:95)\\n\\tat org.eclipse.jetty.io.selectchannelendpoint$2.run(selectchannelendpoint.java:93)\\n\\tat org.eclipse.jetty.util.thread.strategy.executeproduceconsume.produceandrun(executeproduceconsume.java:246)\\n\\tat org.eclipse.jetty.util.thread.strategy.executeproduceconsume.run(executeproduceconsume.java:156)\\n\\tat org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:654)\\n\\tat org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:572)\\n\\tat java.lang.thread.run(thread.java:745)\\n\", \"code\": 500 } } ************************************************************* i did not have this error in solr v6.1 so something has changed in v6.3 that is causing this error. steve rowe thinks it may be related to https://issues.apache.org/jira/browse/lucene-7337 hoss' initial thoughts: \"i think the root of the issue is that the way those fuzzy and prefix queries are parsed means that they may produce an empty boolean query depending on the contents of the index, and then the new optimization rewrites those empty boolean queries into matchnodocsqueries \u2013 but the highlighter (which uses hueristics to figure out what to ask each query \u2013 based on it's type \u2013 what to highlight) doesn't know what to do with that. i'm really suprised the highlighter throws an error in the \"unexpected query type\" code path instead of just ignorning it.\"",
        "label": 10
    },
    {
        "text": "complexphrasequeryparser complexphrasequery does not display slop in tostring  this one is quite simple (i think) \u2013 complexphrasequery doesn't display the slop factor which, when the result of parsing is dumped to logs, for example, can be confusing. i'm heading for a weekend out of office in a few hours... so in the spirit of not committing and running away ( ), if anybody wishes to tackle this, go ahead.",
        "label": 11
    },
    {
        "text": "stop publishing  sha1 files with releases in lucene-7935 we added .sha512 checksums to releases and removed .md5 files. according to the release distribution policy (http://www.apache.org/dev/release-distribution#sigs-and-sums): for every artifact distributed to the public through apache channels, the pmc must supply a valid openpgp-compatible ascii-armored detached signature file must supply at least one checksum file should supply a sha-256 and/or sha-512 checksum file should not supply a md5 or sha-1 checksum file (because these are deprecated) so this jira will stop publishing .sha1 files, leaving only the .sha512",
        "label": 21
    },
    {
        "text": "wrap messages output with a check of infostream   null i've found several places in the code where messages are output w/o first checking if infostream != null. the result is that in most of the time, unnecessary strings are created but never output (because infostream is not set). we should follow java's logging best practices, where a log message is always output in the following format: if (logger.isloggable(leve)) { logger.log(level, msg); } log messages are usually created w/o paying too much attention to performance (such as string concatenation using '+' instead of stringbuffer). therefore, at runtime it is important to avoid creating those messages, if they will be discarded eventually. i will add a method to indexwriter messagesenabled() and then use it wherever a call to iw.message() is made. patch will follow",
        "label": 33
    },
    {
        "text": "add maxtf to fieldinvertstate the maximum within-document tf is a very useful scoring value, we should expose it so that people can use it in scoring consider the following sim: @override public float idf(int docfreq, int numdocs) {   return 1.0f; /* not used */ } @override public float computenorm(string field, fieldinvertstate state) {   return state.getboost() / (float) math.sqrt(state.getmaxtf()); } which is surprisingly effective, but more interesting for practical reasons.",
        "label": 40
    },
    {
        "text": "fix indexwriter close  to not commit or wait for pending merges ",
        "label": 33
    },
    {
        "text": "directoryindexreader finalize  holding terminfosreader longer than necessary directoryindexreader has a finalize method, which causes the jdk to keep a reference to the object until it can be finalized. segmentreader and multisegmentreader are subclasses that contain references to, potentially, hundreds of megabytes of cached data in a terminfosreader. some options would be removing finalize() from directoryindexreader (it releases a write lock at the moment) or possibly nulling out references in various close() and doclose() methods throughout the class hierarchy so that the finalizable object doesn't references the term arrays. original mailing list message: http://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3c7a5cb4a7bbce0c40b81c5145c326c31301a62971@numevp06.na.imtn.com%3e",
        "label": 33
    },
    {
        "text": "slowcompositereaderwrapper does not support point values slowcompositereaderwrapper.getpointvalues always returns null. i think this is trappy and we should either implement it or throw an unsupportedoperationexception instead.",
        "label": 33
    },
    {
        "text": "review fsdirectory chunking defaults and test the chunking today there is a loop in simplefs/niofs: try {           do {             final int readlength;             if (total + chunksize > len) {               readlength = len - total;             } else {               // lucene-1566 - work around jvm bug by breaking very large reads into chunks               readlength = chunksize;             }             final int i = file.read(b, offset + total, readlength);             total += i;           } while (total < len);         } catch (outofmemoryerror e) { i bet if you look at the clover report its untested, because its fixed at 100mb for 32-bit users and 2gb for 64-bit users (are these defaults even good?!). also if you call the setter on a 64-bit machine to change the size, it just totally ignores it. we should remove that, the setter should always work. and we should set it to small values in tests so this loop is actually executed.",
        "label": 40
    },
    {
        "text": "x backwards tests are using version lucene current  aren't testing backwards  the 3.x backwards tests are mostly all using version.lucene_current, therefore they don't always test the behavior as they should. i added test_version_current = 3.0 to the backwards/lucenetestcase, and i think we should fix all backwards tests to use test_version_current instead.",
        "label": 40
    },
    {
        "text": "consolidate all  solr's   lucene's  analyzers into modules analysis we've been wanting to do this for quite some time now... i think, now that solr/lucene are merged, and we're looking at opening an unstable line of development for solr/lucene, now is the right time to do it. a standalone module for all analyzers also empowers apps to separately version the analyzers from which version of solr/lucene they use, possibly enabling us to remove version entirely from the analyzers. we should also do lucene-2309 (decouple, as much as possible, indexer from the analysis api), but i don't think that issue needs to block this consolidation. once we do this, there is one place where our users can find all the analyzers that solr/lucene provide.",
        "label": 40
    },
    {
        "text": "make tieredmergepolicy respect maxsegmentsizemb and allow singleton merges of very large segments we're seeing situations \"in the wild\" where there are very large indexes (on disk) handled quite easily in a single lucene index. this is particularly true as features like docvalues move data into mmapdirectory space. the current tmp algorithm allows on the order of 50% deleted documents as per a dev list conversation with mike mccandless (and his blog here: https://www.elastic.co/blog/lucenes-handling-of-deleted-documents). especially in the current era of very large indexes in aggregate, (think many tb) solutions like \"you need to distribute your collection over more shards\" become very costly. additionally, the tempting \"optimize\" button exacerbates the issue since once you form, say, a 100g segment (by optimizing/forcemerging) it is not eligible for merging until 97.5g of the docs in it are deleted (current default 5g max segment size). the proposal here would be to add a new parameter to tmp, something like <maxallowedpctdeletedinbigsegments> (no, that's not serious name, suggestions welcome) which would default to 100 (or the same behavior we have now). so if i set this parameter to, say, 20%, and the max segment size stays at 5g, the following would happen when segments were selected for merging: > any segment with > 20% deleted documents would be merged or rewritten no matter how large. there are two cases, >> the segment has < 5g \"live\" docs. in that case it would be merged with smaller segments to bring the resulting segment up to 5g. if no smaller segments exist, it would just be rewritten >> the segment has > 5g \"live\" docs (the result of a forcemerge or optimize). it would be rewritten into a single segment removing all deleted docs no matter how big it is to start. the 100g example above would be rewritten to an 80g segment for instance. of course this would lead to potentially much more i/o which is why the default would be the same behavior we see now. as it stands now, though, there's no way to recover from an optimize/forcemerge except to re-index from scratch. we routinely see 200g-300g lucene indexes at this point \"in the wild\" with 10s of shards replicated 3 or more times. and that doesn't even include having these over hdfs. alternatives welcome! something like the above seems minimally invasive. a new merge policy is certainly an alternative.",
        "label": 13
    },
    {
        "text": "oom in testbeidermorsefilter testrandom this has been oom'ing a lot... we should see why, its likely a real bug. ant test -dtestcase=testbeidermorsefilter -dtestmethod=testrandom -dtests.seed=2e18f456e714be89:310bba5e8404100d:-3bd11277c22f4591 -dtests.multiplier=3 -dargs=\"-dfile.encoding=iso8859-1\"",
        "label": 40
    },
    {
        "text": "broken description section links from documentation to javadocs in lucene's top-level documentation, there are links to description sections in javadocs, e.g. in the getting started section: to the lucene demo; to an introduction to lucene's apis; and to the analysis overview. all of these links are anchored at #overview_description or #package_description, but it looks like java8 switched how these anchors are named: in the 6.0\u0010.0, 6.0.1 and now the 6.1.0 rc1 javadocs, these anchors are named with dots rather than underscores: #overview.description and #package.description. as a result, the documentation links go to the right page, but the browser stays at the top of the page because it can't find the now-misnamed anchors.",
        "label": 47
    },
    {
        "text": "nearspans skipto bug nearspans appears to have a bug in skipto that causes it to skip over some matching documents completely. i discovered this bug while investigating problems with spanweight.explain, but as far as i can tell the bug is not specific to explanations ... it seems like it could potentially result in incorrect matching in some situations where a spannearquery is nested in another query such thatskipto will be used ... i tried to create a high level test case to exploit the bug when searching, but i could not. testcase exploiting the class using nearspan and spanscorer will follow...",
        "label": 18
    },
    {
        "text": "scorer skipto  doesn't always work if called before next  skipto() doesn't work for all scorers if called before next().",
        "label": 55
    },
    {
        "text": "cleanup   deprecate token class   improve default attributefactory to no longer use reflection we should remove code duplication in the token class: copy constructors reinit() shit non-default clone() this is too bugy. most of the methods can be simply removed. this issue will also factor out the basic attributes to a separate implementation class (without the token extra stuff). token then just extends this class (in the tokenattributes package) and adds the additional stuff not needed for an attribute. token itsself will get deprecated. also part of the slowdown in the parent issue is caused by ineffective defaultattributefactory, which uses reflection to instantiate new attributes. as we are on java 7 now, we can use methodhandle to do this. methodhandle does access checks only once on creating the factory or when the attribute is seen first. later invocations are done without any result type conversions and parameter conversions as a statically linked constructor call. this greatly improves performance with java 8, java 7 is approx as fast, unless its completely static.",
        "label": 53
    },
    {
        "text": "consistent failure of testcheckindex testluceneconstantversion in jenkins trunk clover build i'm out of the loop on how clover is run, and how the build system sets up th version params, but looking at the coverage reports i noticed that the trunk clover build seems to have been failing consistently for a while \u2013 some sporadic test failures, but one consistent failure smells like it has to do with a build configuration problem... java.lang.assertionerror: invalid version: 5.0-2013-08-11_15-22-48  at __randomizedtesting.seedinfo.seed([648ec34d8642c547:a7103483a05d2588]:0)  at org.junit.assert.fail(assert.java:93)  at org.junit.assert.asserttrue(assert.java:43)  at org.apache.lucene.index.testcheckindex.__clr3_1_10l79zdz2ior(testcheckindex.java:132)  at org.apache.lucene.index.testcheckindex.testluceneconstantversion(testcheckindex.java:118)",
        "label": 53
    },
    {
        "text": "add meta keywords to htmlparser it would be good if the htmlparser could give us the keywords specified in the meta tags, so that we can index them. in htmlparser.jj: void addmetatag() { metatags.setproperty(currentmetatag, currentmetacontent); currentmetatag = null; currentmetacontent = null; return; } one way to do it: void addmetatag() throws ioexception { metatags.setproperty(currentmetatag, currentmetacontent); if (currentmetatag.equalsignorecase(\"keywords\")) { pipeout.write(currentmetacontent); } currentmetatag = null; currentmetacontent = null; return; }",
        "label": 40
    },
    {
        "text": "chararrayset contains char  text  int off  int len  does not work i try to use the chararrayset for a filter i am writing. i heavily use char-arrays in my code to speed up things. i stumbled upon a bug in chararrayset while doing that. the method public boolean contains(char[] text, int off, int len) seems not to work. when i do if (set.contains(buffer,offset,length) {   ... } my code fails. but when i do if (set.contains(new string(buffer,offset,length)) {    ... } everything works as expected. both variants should behave the same. i attach a small piece of code to show the problem.",
        "label": 33
    },
    {
        "text": "geo3d's convex and concave polygons do not handle hole intersections properly the addition of holes to geoconvex and geoconcave polygon shapes has broken two things: (1) the edge points returned should include the hole edge points; (2) the outer distance computation should include distance from hole edges. this ticket is complicated by the fact that holes as provided by geopolygonfactory can be entirely outside of the polygon; no checking or pruning is done. it is unknown whether having edge points from outside holes will lead to improper return values for getrelationship() results.",
        "label": 25
    },
    {
        "text": "ramusageestimator causes awt classes to be loaded by calling managementfactory getplatformmbeanserver yea, that type of day and that type of title . since the last update of java 6 on os x, i started to see an annoying icon pop up at the doc whenever running elasticsearch. by default, all of our scripts add headless awt flag so people will probably not encounter it, but, it was strange that i saw it when before i didn't. i started to dig around, and saw that when ramusageestimator was being loaded, it was causing awt classes to be loaded. further investigation showed that actually for some reason, calling managementfactory#getplatformmbeanserver now with the new java version causes awt classes to be loaded (at least on the mac, haven't tested on other platforms yet). there are several ways to try and solve it, for example, by identifying the bug in the jvm itself, but i think that there should be a fix for it in lucene itself, specifically since there is no need to call #getplatformmbeanserver to get the hotspot diagnostics one (its a heavy call...). here is a simple call that will allow to get the hotspot mxbean without using the #getplatformmbeanserver method, and not causing it to be loaded and loading all those nasty awt classes:     object gethotspotmxbean() {         try {             // java 6             class sunmf = class.forname(\"sun.management.managementfactory\");             return sunmf.getmethod(\"getdiagnosticmxbean\").invoke(null);         } catch (throwable t) {             // ignore         }         // potentially java 7         try {             return managementfactory.class.getmethod(\"getplatformmxbean\", class.class).invoke(null, class.forname(\"com.sun.management.hotspotdiagnosticmxbean\"));         } catch (throwable t) {             // ignore         }         return null;     }",
        "label": 53
    },
    {
        "text": "add a filter returning all document without a value in a field in some situations it would be useful to have a filter that simply returns all document that either have at least one or no value in a certain field. we don't have something like that out of the box and adding it seems straight forward.",
        "label": 46
    },
    {
        "text": "paralleltermenum is broken paralleltermenum.next() fails to advance properly to new fields. this is a serious bug. christian kohlschuetter diagnosed this as the root problem underlying lucene-398 and posted a first patch there. i've addressed a couple issues in the patch (close skipped field termenum's, generate field iterator only once, integrated christian's test case as a lucene test) and packaged in all the revised patch here. all lucene tests pass, and i've further tested in this in my app, which makes extensive use of parallelreader.",
        "label": 55
    },
    {
        "text": "need the ability to also sort spellcheck results by freq  instead of just by edit distance freq this issue was first noticed and reported in this solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788 basically, there are situations where it would be useful to sort by freq first, instead of the current \"sort by edit distance, and then subsort by freq if edit distance is equal\" the author of the thread suggested \"what i think would work even better than allowing a custom compareto function would be to incorporate the frequency directly into the distance function. this would allow for greater control over the trade-off between frequency and edit distance\" however, custom compareto functions are not always be possible (ie if a certain version of lucene must be used, because it was release with solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation) it is suggested that we have a simple modification of the existing compareto function in lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.",
        "label": 15
    },
    {
        "text": "search function     score based on field value    simple score customizability functionquery can return a score based on a field's value or on it's ordinal value. functionfactory subclasses define the details of the function. there is currently a linearfloatfunction (a line specified by slope and intercept). field values are typically obtained from fieldvaluesourcefactory. implementations include floatfieldsource, intfieldsource, and ordfieldsource.",
        "label": 12
    },
    {
        "text": "fix license headers in all java files to not be in javadocs   format our current license headers in all .java files are (for a reason i don't know) in javadocs format. means, when you have a class without javadocs, the license header is used as javadocs. i reviewed lots of other apache projects, most of them use the correct /* header, but some (including lucene+solr) the javadocs one. we should change this.",
        "label": 18
    },
    {
        "text": "regexp toautomaton high memory use when creating an automaton from an org.apache.lucene.util.automaton.regexp, it's possible for the automaton to use so much memory it exceeds the maximum array size for java. the following caused an outofmemoryerror with a 32gb heap: new regexp(\"\\\\[\\\\[(datei|file|bild|image):[^]]*alt=[^]|}]{50,200}\").toautomaton(); when increased to a 60gb heap, the following exception is thrown:   1> java.lang.illegalargumentexception: requested array size 2147483624 exceeds maximum array in java (2147483623)   1>     __randomizedtesting.seedinfo.seed([7be81ef678615c32:95c8057a4aba5b52]:0)   1>     org.apache.lucene.util.arrayutil.oversize(arrayutil.java:168)   1>     org.apache.lucene.util.arrayutil.grow(arrayutil.java:295)   1>     org.apache.lucene.util.automaton.automaton$builder.addtransition(automaton.java:639)   1>     org.apache.lucene.util.automaton.operations.determinize(operations.java:741)   1>     org.apache.lucene.util.automaton.minimizationoperations.minimizehopcroft(minimizationoperations.java:62)   1>     org.apache.lucene.util.automaton.minimizationoperations.minimize(minimizationoperations.java:51)   1>     org.apache.lucene.util.automaton.regexp.toautomaton(regexp.java:477)   1>     org.apache.lucene.util.automaton.regexp.toautomaton(regexp.java:426)",
        "label": 33
    },
    {
        "text": "remove indexdocvaluesfield its confusing how we present csf functionality to the user, its actually not a \"field\" but an \"attribute\" of a field like stored or indexed. otherwise, its really hard to think about csf because there is a mismatch between the apis and the index format.",
        "label": 33
    },
    {
        "text": "default value for scoring payloads in lucene 5, payloadtermquery used a hardcoded default of 1.0 for terms without a payload. the replacing payloadscorequery in lucene 6 just ignores those terms. this is unflexible and wrong for many use cases (for example using payloads to deemphasize some terms, where terms without payload should result in maximum score instead of being ignored). in my pull request i defer the decision on what to do with missing payloads to the scorepayload method of the similarity, which has to check the given payload for null and handle that case. i believe this breaks backwards compatibility?",
        "label": 14
    },
    {
        "text": "simplify spatial latlng and llrect classes currently in the contrib there is floatlatlng, and fixedlatlng, which both extend latlng. the reason for this separation is not clear and is not needed in the current functionality. the functionality that is used can be collapsed into latlng, which can be made a concrete class. internally latlng can benefit from the improvements suggested in lucene-1934. llrect, which uses latlng, can also be simplified by removing the unused functionality, and using the new latlng class. all classes can be improved through documentation, some method renaming, and general code tidy up.",
        "label": 7
    },
    {
        "text": "add datainput skipbytes i was playing with on-the-fly checksum verification and this made me stumble upon an issue with bufferedchecksumindexinput. i have some code that skips over a datainput by reading bytes into /dev/null, eg.   private static final byte[] skip_buffer = new byte[1024];   private static void skipbytes(datainput in, long numbytes) throws ioexception {     assert numbytes >= 0;     for (long skipped = 0; skipped < numbytes; ) {       final int toread = (int) math.min(numbytes - skipped, skip_buffer.length);       in.readbytes(skip_buffer, 0, toread);       skipped += toread;     }   } it is fine to read into this static buffer, even from multiple threads, since the content that is read doesn't matter here. however, it breaks with bufferedchecksumindexinput because of the way that it updates the checksum:   @override   public void readbytes(byte[] b, int offset, int len)     throws ioexception {     main.readbytes(b, offset, len);     digest.update(b, offset, len);   } if you are unlucky enough so that a concurrent call to skipbytes started modifying the content of b before the call to digest.update(b, offset, len) finished, then your checksum will be wrong. i think we should make bufferedchecksumindexinput read into a private buffer first instead of relying on the user-provided buffer.",
        "label": 1
    },
    {
        "text": "field tokenstream should be usable with stored fields  field.tokenstream should be usable for indexing even for stored values. useful for many types of pre-analyzed values (text/numbers, etc) http://search.lucidimagination.com/search/document/902bad4eae20bdb8/field_tokenstreamvalue",
        "label": 55
    },
    {
        "text": "score and explain don't match i've faced this problem recently. i'll attach a program to reproduce the problem soon. the program outputs the following: ** score = 0.10003257 ** explain 0.050016284 = (match) product of:   0.15004885 = (match) sum of:     0.15004885 = weight(f1:\"note book\" in 0), product of:       0.3911943 = queryweight(f1:\"note book\"), product of:         0.61370564 = idf(f1: note=1 book=1)         0.6374299 = querynorm       0.38356602 = fieldweight(f1:\"note book\" in 0), product of:         1.0 = tf(phrasefreq=1.0)         0.61370564 = idf(f1: note=1 book=1)         0.625 = fieldnorm(field=f1, doc=0)   0.33333334 = coord(1/3)",
        "label": 18
    },
    {
        "text": "deprecate remove lowercasetokenizer lowercasetokenizer combines tokenization and filtering in a way that prevents us improving the normalization api.  we should deprecate and remove it, as it can be replaced simply with a lettertokenizer and lowercasefilter.",
        "label": 2
    },
    {
        "text": "add contrib libs to classpath for javadoc i don't know ant well enough to just do this easily, so i've labeled a wish - would be nice to get rid of all the errors/warnings that not finding these classes generates when building javadoc.",
        "label": 18
    },
    {
        "text": "some tests fail with ibm j9 due to strange timerthread zombies that cannot be killed eg: ant test -dtestcase=testindexwriterdelete -dtests.seed=a22bc3e06cec0036 takes a while and then fails with this: [junit4:junit4] error   0.00s j2 | testindexwriterdelete (suite) <<< [junit4:junit4]    > throwable #1: com.carrotsearch.randomizedtesting.threadleakerror: 1 thread leaked from suite scope at org.apache.lucene.index.testindexwriterdelete: [junit4:junit4]    >    1) thread[id=102, name=thread-44, state=timed_waiting, group=tgrp-testindexwriterdelete] [junit4:junit4]    >         at java.lang.object.wait(native method) [junit4:junit4]    >         at java.lang.object.wait(object.java:196) [junit4:junit4]    >         at java.util.timer$timerimpl.run(timer.java:247) [junit4:junit4]    >    at __randomizedtesting.seedinfo.seed([c9014bcb129899bf]:0) [junit4:junit4]    > throwable #2: com.carrotsearch.randomizedtesting.threadleakerror: there are still zombie threads that couldn't be terminated: [junit4:junit4]    >    1) thread[id=102, name=thread-44, state=timed_waiting, group=tgrp-testindexwriterdelete] [junit4:junit4]    >         at java.lang.object.wait(native method) [junit4:junit4]    >         at java.lang.object.wait(object.java:196) [junit4:junit4]    >         at java.util.timer$timerimpl.run(timer.java:247) [junit4:junit4]    >    at __randomizedtesting.seedinfo.seed([c9014bcb129899bf]:0) java version is: java version \"1.6.0\" java(tm) se runtime environment (build pxa6460sr9fp2ifix-20111111_05(sr9 fp2+iv03622+iv02378+iz99243+iz97310+iv00707)) ibm j9 vm (build 2.4, jre 1.6.0 ibm j9 2.4 linux amd64-64 jvmxa6460sr9-20111111_94827 (jit enabled, aot enabled) j9vm - 20111111_094827 jit  - r9_20101028_17488ifx45 gc   - 20101027_aa) jcl  - 20110727_04 curiously, i think it tends to happen on a test that fails an assumption? or maybe i'm just imagining that ...",
        "label": 11
    },
    {
        "text": "changes for trierange in filteredtermenum and multitermquery improvement this is a patch, that is needed for the multitermquery-rewrite of trierange (lucene-1602): make the private members protected, to have access to them from the very special trierangetermenum fix a small inconsistency (docfreq() now only returns a value, if a valid term is existing) improvement of multitermfilter.getdocidset to return docidset.empty_docidset, if the termenum is empty (less memory usage) and faster. add the getlastnumberofterms() to multitermquery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of trierange can be shown (extract from test case there, 10000 docs index, long values): [junit] average number of terms during random search on 'field8': [junit]  trie query: 244.2 [junit]  classical query: 3136.94 [junit] average number of terms during random search on 'field4': [junit]  trie query: 38.3 [junit]  classical query: 3018.68 [junit] average number of terms during random search on 'field2': [junit]  trie query: 18.04 [junit]  classical query: 3539.42 all core tests pass.",
        "label": 33
    },
    {
        "text": "expose indexfilenames as public  and make use of its methods in the code indexfilenames is useful for applications that extend lucene, an in particular those who extend directory or indexwriter. it provides useful constants and methods to query whether a certain file is a core lucene file or not. in addition, indexfilenames should be used by lucene's code to generate segment file names, or query whether a certain file matches a certain extension. i'll post the patch shortly.",
        "label": 33
    },
    {
        "text": "add  experimental javadocs tag there are a lot of things marked experimental, api subject to change, etc. in lucene. this patch simply adds a @experimental tag to common-build.xml so that we can use it, for more consistency.",
        "label": 40
    },
    {
        "text": "sortertemplate quicksort incorrect on trying to use the very useful o.a.l.utils.sortertemplate, i stumbled upon inconsistent sorting behaviour, of course, only a randomized test caught this because sortertemplate.quicksort is used in several places in the code (directly bytesreflist, arrayutil, bytesrefhash, collectionutil and transitively index and search), i'm a bit puzzled that this either hasn't been caught by another higher-level test or that neither my test nor my understanding of an insufficiency in the code is valid if the former holds and given that the same code is released in 3.6 and 4.0, this might even be a more critical issue requiring a higher priority than 'major'. so, can a second pair of eyes please have a timely look at the attached test and patch? basically the current quicksort implementation seems to assume that luckily always the median is chosen as pivot element by grabbing the mid element, not handling the case where the initially chosen pivot ends up not in the middle. hope this and the test helps to understand the issue. reproducible, currently failing test and a patch attached.",
        "label": 53
    },
    {
        "text": "add indexreader flush commituserdata  indexwriter offers a commit(string commituserdata) method. indexreader can commit as well using the flush/close methods and so needs an analogous method that accepts commituserdata.",
        "label": 33
    },
    {
        "text": "assertion fails for lucene43ngramtokenizer as a side effect from lucene-5859, lucene43ngramtokenizer was made not final. this can trip an assert we have that tokenizer need to either be final, or have their incrementtoken() function be final.",
        "label": 41
    },
    {
        "text": "don't spawn thread statically in fsdirectory on mac os x on the mac, creating the digester starts up a pkcs11 thread. i do not think threads should be created statically (i have this same issue with timelimitedcollector and also filtermanager). uwe discussed removing this md5 digester, i don't care if we remove it or not, just as long as it doesn't create a thread, and just as long as it doesn't use the system default locale.",
        "label": 33
    },
    {
        "text": "reindex crashes the jvm we're using liferay which uses lucene behind the screens to index things like documents, web content, users, etc... . when we trigger a full reindex via the liferay control panel, which uses indexwriter.deleteall(), the jvm crashes and generates a dump with the following message: # a fatal error has been detected by the java runtime environment: # sigsegv (0xb) at pc=0xffffffff78de94a8, pid=18938, tid=2478 # jre version: java(tm) se runtime environment (7.0_75-b13) (build 1.7.0_75-b13) java vm: java hotspot(tm) 64-bit server vm (24.75-b04 mixed mode solaris-sparc compressed oops) problematic frame: j 5227 c2 org.apache.lucene.index.indexfilenames.segmentfilename(ljava/lang/string;ljava/lang/string;)ljava/lang/string; (44 bytes) @ 0xffffffff78de94a8 [0xffffffff78de9480+0x28]",
        "label": 11
    },
    {
        "text": "javadocs should no longer reference ramdirectory since ramdirectory is deprecated, we shouldn't show examples using it anymore. see eg. https://github.com/apache/lucene-solr/blob/a1ec716e107807f1dc24923cc7a91d0c5e64a7e1/lucene/core/src/java/overview.html#l36. cc dawid weiss",
        "label": 11
    },
    {
        "text": "geo3d test failure  [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint    [junit4]   1>     doc=96 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.4812439919751819, lon=-3.141592653589793([x=-0.08923495159694257, y=-1.0928129784526471e-17, z=0.9937907331608504])]    [junit4]   1>       quantized=[x=-0.08923495170440254, y=-2.3309121299774915e-10, z=0.9937907329903598]    [junit4]   1>     doc=284 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.3177235852557188, lon=-3.141592653589793([x=-0.24987216435732065, y=-3.060051462762141e-17, z=0.9661839699711411])]    [junit4]   1>       quantized=[x=-0.24987216431220738, y=-2.3309121299774915e-10, z=0.9661839700741824]    [junit4]   1>     doc=448 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.373550879519168, lon=-3.141592653589793([x=-0.19555555347329456, y=-2.3948648261655927e-17, z=0.9785415796969477])]    [junit4]   1>       quantized=[x=-0.19555555342162367, y=-2.3309121299774915e-10, z=0.9785415795083489]    [junit4]   1>     doc=568 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.4147644427264174, lon=3.141592653589793([x=-0.15506432607243328, y=1.8989903058654687e-17, z=0.985720859148803])]    [junit4]   1>       quantized=[x=-0.15506432593490593, y=2.3309121299774915e-10, z=0.9857208592622259]    [junit4]   1>     doc=596 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.4949759718464255, lon=3.141592653589793([x=-0.07557968098734905, y=9.255841440173507e-18, z=0.9949148423009544])]    [junit4]   1>       quantized=[x=-0.07557968106487689, y=2.3309121299774915e-10, z=0.9949148424036222]    [junit4]   1>     doc=724 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.3420318470804882, lon=-3.141592653589793([x=-0.2263058688809343, y=-2.7714475795329682e-17, z=0.9719352310654573])]    [junit4]   1>       quantized=[x=-0.2263058688153078, y=-2.3309121299774915e-10, z=0.9719352309632839]    [junit4]   1>     doc=726 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.5087080502478558, lon=-3.141592653589793([x=-0.06191034463338769, y=-7.581830538938776e-18, z=0.995852579168158])]    [junit4]   1>       quantized=[x=-0.0619103447691941, y=-2.3309121299774915e-10, z=0.9958525790757079]    [junit4]   1>     doc=1090 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=1.5015273600939123, lon=3.141592653589793([x=-0.0690598132897715, y=8.45738792950325e-18, z=0.9953854835156319])]    [junit4]   1>       quantized=[x=-0.06905981308929354, y=2.3309121299774915e-10, z=0.9953854833983399]    [junit4]   1>   shape=geonorthrectangle: \\{planetmodel=planetmodel.wgs84, bottomlat=1.2487354264870392(71.54726966617622), leftlon=0.0(0.0), rightlon=3.5181789305199657e-12(2.0157680429064372e-10)}    [junit4]   1>   bounds=xyzbounds: [xmin=-1.0e-9 xmax=0.31591984670875706 ymin=-1.0010000000000002e-9 ymax=1.0011114625449057e-9 zmin=0.9467800037525481 zmax=0.9977622930221051]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=bbec483ac10cc8fe -dtests.slow=true -dtests.badapples=true -dtests.locale=th-th-u-nu-thai-x-lvariant-th -dtests.timezone=egypt -dtests.asserts=true -dtests.file.encoding=utf-8  ",
        "label": 25
    },
    {
        "text": "multifieldqueryparser ignores slop parameter multifieldqueryparser.getfieldquery(string, string, int) calls super.getfieldquery(string, string), thus obliterating any slop parameter present in the query. it should probably be changed to call super.getfieldquery(string, string, int), except doing only that will result in a recursive loop which is a side-effect of what may be a deeper problem in multifieldqueryparser \u2013 getfieldquery(string, string, int) is documented as delegating to getfieldquery(string, string), yet what it actually does is the exact opposite. this also causes problems for subclasses which need to override getfieldquery(string, string) to provide different behaviour.",
        "label": 12
    },
    {
        "text": "make luke a lucene solr module see \"re: luke - in need of maintainer\": http://markmail.org/message/m4gsto7giltvrpuf \"web-based luke\": http://markmail.org/message/4xwps7p7ifltme5q i think it would be great if there was a version of luke that always worked with trunk - and it would also be great if it was easier to match luke jars with lucene versions. while i'd like to get gwt luke into the mix as well, i think the easiest starting point is to straight port luke to another ui toolkit before abstracting out dto objects that both gwt luke and pivot luke could share. i've started slowly converting luke's use of thinlet to apache pivot. i haven't/don't have a lot of time for this at the moment, but i've plugged away here and there over the past work or two. there is still a lot to do.",
        "label": 51
    },
    {
        "text": "more like this query  keep fields separated currently the query is generated : org.apache.lucene.queries.mlt.morelikethis#retrieveterms(int) 1) we extract the terms from the interesting fields, adding them to a map : map<string, int> termfreqmap = new hashmap<>(); ( we lose the relation field-> term, we don't know anymore where the term was coming ! ) org.apache.lucene.queries.mlt.morelikethis#createqueue 2) we build the queue that will contain the query terms, at this point we connect again there terms to some field, but : ... // go through all the fields and find the largest document frequency string topfield = fieldnames[0]; int docfreq = 0; for (string fieldname : fieldnames) { int freq = ir.docfreq(new term(fieldname, word)); topfield = (freq > docfreq) ? fieldname : topfield; docfreq = (freq > docfreq) ? freq : docfreq; } ... we identify the topfield as the field with the highest document frequency for the term t . then we build the termquery : queue.add(new scoreterm(word, topfield, score, idf, docfreq, tf)); in this way we lose a lot of precision. not sure why we do that. i would prefer to keep the relation between terms and fields. the mlt query can improve a lot the quality. if i run the mlt on 2 fields : wesell and wedontsell for example. it is likely i want to find documents with similar terms in the wesell and similar terms in the wedontsell, without mixing up the things and loosing the semantic of the terms.",
        "label": 50
    },
    {
        "text": "make compressingstoredfieldsformat the new default storedfieldsformat impl what would you think of making compressingstoredfieldsformat the new default storedfieldsformat? stored fields compression has many benefits : it makes the i/o cache work for us, file-based index replication/backup becomes cheaper. things to know: even with incompressible data, there is less than 0.5% overhead with lz4, lz4 compression requires ~ 16kb of memory and lz4 hc compression requires ~ 256kb, lz4 uncompression has almost no memory overhead, on my low-end laptop, the lz4 impl in lucene uncompresses at ~ 300mb/s. i think we could use the same default parameters as in compressingcodec : lz4 compression, in-memory stored fields index that is very memory-efficient (less than 12 bytes per block of compressed docs) and uses binary search to locate documents in the fields data file, 16 kb blocks (small enough so that there is no major slow down when the whole index would fit into the i/o cache anyway, and large enough to provide interesting compression ratios ; for example robert got a 0.35 compression ratio with the geonames.org database). any concerns?",
        "label": 1
    },
    {
        "text": "enable flexible scoring this is a first step (nowhere near committable!), implementing the design iterated to in the recent \"baby steps towards making lucene's scoring more flexible\" java-dev thread. the idea is (if you turn it on for your field; it's off by default) to store full stats in the index, into a new _x.sts file, per doc (x field) in the index. and then have fieldsimilarityprovider impls that compute doc's boost bytes (norms) from these stats. the patch is able to index the stats, merge them when segments are merged, and provides an iterator-only api. it also has starting point for per-field sims that use the stats iterator api to compute boost bytes. but it's not at all tied into actual searching! there's still tons left to do, eg, how does one configure via field/fieldtype which stats one wants indexed. all tests pass, and i added one new teststats unit test. the stats i record now are: field's boost field's unique term count (a b c a a b --> 3) field's total term count (a b c a a b --> 6) total term count per-term (sum of total term count for all docs that have this term) still need at least the total term count for each field.",
        "label": 40
    },
    {
        "text": "multiterm within a spannotquery no longer working some unit tests in lucene-5205 that passed in 5.3.1 are now failing with lucene 5.4.0. it looks like multiterms are no longer being processed correctly within a spannotquery in 5.4.0 and in trunk.",
        "label": 1
    },
    {
        "text": "update notice txt from the java-dev discussion, notice.txt should be up-to-date. one thing i know, is that the persian stopwords file (analyzers/fa) came from the same source as the arabic stopwords file, and is bsd-licensed. there might be others (i think icu has already been added)",
        "label": 29
    },
    {
        "text": "benchmark for collation steven rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under lucene-2084, along with some instructions to run it... i think it would be a nice if we could turn this into a committable patch and add it to benchmark.",
        "label": 40
    },
    {
        "text": "add interface for point shapes hi karl wright, just one more thing . one of the shapes i normally use is a point shape. there is not such an interface so what i do currently is to build degenerated circles (circles with radius equal to 0) when i need such a shape. still looks weird to have geocircles when you are dealing with points. my proposal is the following: 1) add interface for point shapes which extends geocircle and geobbox. not sure which name is appropiate but my take would be to call it geolatlonpoint. 2) make geodegeneratedpoint implement this interface. 3) add a factory to build geolatlonpoint . i am adding athe patch shortly. thanks!",
        "label": 25
    },
    {
        "text": "improve facet examples facet examples need some simplification and cleanup. for instance, multiclexample uses a random (with fixed seed) to generate documents and categories and in general they seem to try an reuse a lot of methods. rather, i think the example code should be concise and short, demonstrating what it needs to demonstrate, even at the expense of duplicating code. that way, a user can just look at an example class to understand how to do something...",
        "label": 43
    },
    {
        "text": "multifields getindexedfields can be optimized to not use getmergedfieldinfos multifields.getindexedfields calls getmergedfieldinfos. but getmergedfieldinfos is kinda heavy, doing all sorts of stuff that getindexedfields doesn't care about. it can simply loop the leaf readers and collect the results into a set. java 8 streams should make easy work of this.",
        "label": 10
    },
    {
        "text": "it should be easy to make use of termstate  rewritten queries should be shared automatically when you have the same query in a query hierarchy multiple times, tremendous savings can now be had if the user knows enough to share the rewritten queries in the hierarchy, due to the termstate addition. but this is clumsy and requires a lot of coding by the user to take advantage of. lucene should be smart enough to share the rewritten queries automatically. this can be most readily (and powerfully) done by introducing a new method to query.java: query rewriteusingcache(indexreader indexreader) ... and including a caching implementation right in query.java which would then work for all. of course, all callers would want to use this new method rather than the current rewrite().",
        "label": 46
    },
    {
        "text": "record soft deletes in segmentcommitinfo this change add the number of documents that are soft deletes but not hard deleted to the segment commit info. this is the last step towards making soft deletes as powerful as hard deltes since now the number of document can be read from commit points without opening a full blown reader. this also allows merge posliies to make decisions without requiring an nrt reader to get the relevant statistics. this change doesn't enforce any field to be used as soft deletes and the statistic is maintained per segment.",
        "label": 46
    },
    {
        "text": "use nio positional read to avoid synchronization in fsindexinput as suggested by doug, we could use nio pread to avoid synchronization on the underlying file. this could mitigate any mt performance drop caused by reducing the number of files in the index format.",
        "label": 33
    },
    {
        "text": "segmentreader numdeleteddocs  sometimes gives an incorrect numdeleteddocs at merge time, segmentreader sometimes gives an incorrect value for numdeleteddocs. from lucene-2357: as far as i know, [segmenterreader.numdeleteddocs() is] only unreliable in this context (segmentreader passed to segmentmerger for merging); this is because we allow newly marked deleted docs to happen concurrently up until the moment we need to pass the sr instance to the merger (search for \"// must sync to ensure buffereddeletesstream\" in indexwriter.java) ... but it would be nice to fix that, so i think open a new issue (it won't block this one)? we should be able to make a new sr instance, sharing the same core as the current one but using the correct delcount... it would be cleaner (but i think hairier) to create a new sr for merging that holds the correct delcount, but let's do that under the separate issue. it would be best if the segmentreader's numdeleteddocs were always correct, but, fixing that in indexwriter is somewhat tricky. ie, the fix could be hairy but the end result (\"segmentreader.numdeleteddocs can always be trusted\") would be cleaner...",
        "label": 1
    },
    {
        "text": "add one syllable method to indexreader enumerate subreaders description is exactly as written. getsequentialsubreaders/gettoplevelreadercontext, these method names are way too long/unuseable. they also have tricky semantics (e.g. returning null). in lucene 4, people cannot just use any indexreader and get a merged view. so we need to make this stuff easy on them: single-syllable method name (leaves(), subs(), i will think on this) supports enhanced for-loop (no returning null or anything like that) on indexreader (not atomic or composite, plain old indexreader)",
        "label": 40
    },
    {
        "text": "use broadword bit selection in eliasfanodecoder try and speed up decoding",
        "label": 1
    },
    {
        "text": "patch for shinglefilter enablepositions  or positionfilter  make it possible for all words and shingles to be placed at the same position, that is for all shingles (and unigrams if included) to be treated as synonyms of each other. today the shingles generated are synonyms only to the first term in the shingle. for example the query \"abcd efgh ijkl\" results in: (\"abcd\" \"abcd efgh\" \"abcd efgh ijkl\") (\"efgh\" efgh ijkl\") (\"ijkl\") where \"abcd efgh\" and \"abcd efgh ijkl\" are synonyms of \"abcd\", and \"efgh ijkl\" is a synonym of \"efgh\". there exists no way today to alter which token a particular shingle is a synonym for. this patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other. see http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread.",
        "label": 15
    },
    {
        "text": "fastvectorhighlighter copies segments scores when splitting segments across multi valued fields fastvectorhighlighter copies segments scores when splitting segments across multi-valued fields. this is only a problem when you want to sort the fragments by score. technically basefragmentsbuilder (line 261 in my copy of the source) does the copying. rather than copying the score i think it'd be more right to pull that copying logic into a protected method that child classes (such as scoreorderfragmentsbuilder) can override to do more intelligent things. exactly what that means isn't clear to me at the moment.",
        "label": 1
    },
    {
        "text": "revise pagedbytes fillusinglengthprefix  methods names pagedbytes has 3 different variants of fillusinglengthprefix. we need better names for that since csfbranch already added a 4th one. here are some suggestions: /** reads length as 1 or 2 byte vint prefix, starting @ start */     public bytesref filllengthandoffset(bytesref b, long start)  //    was: public bytesref fillusinglengthprefix(bytesref b, long start)   /** @lucene.internal  reads length as 1 or 2 byte vint prefix, starting @ start.  returns the block number of the term. */     public int getblockandfill(bytesref b, long start)  //    was: public bytesref fillusinglengthprefix2(bytesref b, long start)  /** @lucene.internal  reads length as 1 or 2 byte vint prefix, starting @ start.       * returns the start offset of the next part, suitable as start parameter on next call      * to sequentially read all bytesrefs. */     public long getnextoffsetandfill(bytesref b, long start)  //    was: public bytesref fillusinglengthprefix3(bytesref b, long start) ",
        "label": 46
    },
    {
        "text": "attempting to link to java se javadocs is competely unreliable as noted several times since oracle bought sun, the canonical links to the java se javadocs have been unreliable and frequently cause warnings. since we choose to fail the build on javadoc warnings, this is a serious problem for anyone trying to build from source if/when the package-list we reference in our common-build.xml is not available. we should eliminate this dependency.",
        "label": 47
    },
    {
        "text": "keywordmarkerfilter resets keyword attribute state to false for tokens not in protwords txt keywordmarkerfilter sets true or false for the keywordattribute on all tokens. this erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. if a token is already marked as a keyword (keywordattribute.iskeyword() == true), perhaps the keywordmarkerfilterfactory should not re-set the state to false.",
        "label": 40
    },
    {
        "text": "paging collector http://issues.apache.org/jira/browse/lucene-2127?focusedcommentid=12796898&page=com.atlassian.jira.plugin.system.issuetabpanels%3acomment-tabpanel#action_12796898 somebody assign this to aaron mccurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.",
        "label": 40
    },
    {
        "text": "unexpected performance difference between samplingaccumulator and standardfacetaccumulator i have an unexpected performance difference between the samplingaccumulator and the standardfacetaccumulator. the case is an index with about 5m documents and each document containing about 10 fields. i created a facet on each of those fields. when searching to retrieve facet-counts (using 1 countfacetrequest), the samplingaccumulator is about twice as fast as the standardfacetaccumulator. this is expected and a nice speed-up. however, when i use more countfacetrequests to retrieve facet-counts for more than one field, the speeds of the sampingaccumulator decreases, to the point where the standardfacetaccumulator is faster.   facetrequests  sampling    standard  1               391 ms     1100 ms  2               531 ms     1095 ms   3               948 ms     1108 ms  4              1400 ms     1110 ms  5              1901 ms     1102 ms is this behaviour normal? i did not expect it, as the samplingaccumulator needs to do less work? some code to show what i do:  searcher.search( facetsquery, facetscollector );  final list<facetresult> collectedfacets = facetscollector.getfacetresults(); final facetsearchparams facetsearchparams = new facetsearchparams( facetrequests ); facetscollector facetscollector; if ( issampled ) {  facetscollector =   facetscollector.create( new samplingaccumulator( new randomsampler(), facetsearchparams, searcher.getindexreader(), taxo ) ); } else {  facetscollector = facetscollector.create( facetsaccumulator.create( facetsearchparams, searcher.getindexreader(), taxo ) );",
        "label": 43
    },
    {
        "text": "polygon constructor can't reliably find a point inside the polygon the polygon factory uses a randomize algorithm to find a point inside of the polygon. this might fail, in particular when polygons are very small. i want to propose a small improvement; before going into the randomize algorithm, we can first try to try a point defined by the center of mass of the provided points. this approach improves things, for example for very small polygons with three points.   does it make sense?",
        "label": 25
    },
    {
        "text": "shinglefilter skips over trie shingles if outputunigram is set to false spinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa i noticed that if i set outputunigrams to false it gives me the same output for maxshinglesize=2 and maxshinglesize=3. please divide divide this this sentence when i set maxshinglesize to 4 output is: please divide please divide this sentence divide this this sentence i was expecting the output as follows with maxshinglesize=3 and outputunigrams=false : please divide this divide this sentence",
        "label": 40
    },
    {
        "text": "support for boost factor in morelikethis this is a patch i made to be able to boost the terms with a specific factor beside the relevancy returned by morelikethis. this is helpful when having more then 1 morelikethis in the query, so words in the field a (i.e. title) can be boosted more than words in the field b (i.e. description).",
        "label": 33
    },
    {
        "text": "topdocs merge should use updatetop instead of pop   add the function topdocs.merge uses priorityqueue in a pattern: pop, update value (ref.hitindex++), add. javadocs for priorityqueue.updatetop say that using this function instead should be at least twice as fast.",
        "label": 1
    },
    {
        "text": "make the core accountables namedaccountable function public accountables has a number of methods named namedaccountable. the core one of these works by taking a snapshot with an anonymous accountable. this method is currently private due to concerns over safety. however, i think we should make it public, and document the how safety can be achieved (which is by only using that and the other namedaccountable methods).",
        "label": 41
    },
    {
        "text": "contrib javascript is not packaged into releases the contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.",
        "label": 38
    },
    {
        "text": "spatial checks for a string in an int double map   private map<integer,double> distances;     if (precise != null) {       double xlat = getprecision(lat, precise);       double xlng = getprecision(lng, precise);              string k = new double(xlat).tostring() +\",\"+ new double(xlng).tostring();            double d = (distances.get(k));       if (d != null){         return d.doublevalue();       }     } something is off here eh?",
        "label": 29
    },
    {
        "text": "snowballfilter loses token position offset snowballfilter doesn't set the token position increment (and thus it defaults to 1). this also affetcs snowballanalyzer since it uses snowballfilter.",
        "label": 14
    },
    {
        "text": "memoryindex addfield  ignores some fieldtype settings spotted by a luwak user: https://github.com/flaxsearch/luwak/issues/135. memoryindex never omits norms, which means that it can produce incorrect scores.",
        "label": 2
    },
    {
        "text": "weightedspantermextractor incorrectly treats the same terms occurring in different query types given a booleanquery with multiple clauses, if a term occurs both in a span / phrase query, and in a termquery, the results of term extraction are unpredictable and depend on the order of clauses. concequently, the result of highlighting are incorrect. example text: t1 t2 t3 t4 t2 example query: t2 t3 \"t1 t2\" current highlighting: [t1 t2] [t3] t4 t2 correct highlighting: [t1 t2] [t3] t4 [t2] the problem comes from the fact that we keep a map<termtext, weightedspanterm>, and if the same term occurs in a phrase or span query the resulting weightedspanterm will have a positionsensitive=true, whereas terms added from termquery have positionsensitive=false. the end result for this particular term will depend on the order in which the clauses are processed. my fix is to use a subclass of map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionsensitive=true, and we try to put() a term with positionsensitive=false, we set the result positionsensitive=false, as it will match both cases.",
        "label": 38
    },
    {
        "text": "testindexsorting failures my jenkins found two reproducing seeds on branch_6x - these look different, but the failures happened on consecutive nightly runs: checking out revision 535bf59a3b239f5c7bcd8c00f3e452c9b5e9b539 (refs/remotes/origin/branch_6x) [...]   [junit4] suite: org.apache.lucene.index.testindexsorting    [junit4]   2> ??? 18, 2016 9:50:39 am com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[lucene merge thread #0,5,tgrp-testindexsorting]    [junit4]   2> org.apache.lucene.index.mergepolicy$mergeexception: java.lang.assertionerror: nextvalue=4594289799775307848 vs previous=4606302611760746829    [junit4]   2>        at __randomizedtesting.seedinfo.seed([5f8898dcabbfd056]:0)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]   2> caused by: java.lang.assertionerror: nextvalue=4594289799775307848 vs previous=4606302611760746829    [junit4]   2>        at org.apache.lucene.codecs.asserting.assertingdocvaluesformat$assertingdocvaluesconsumer.addsortednumericfield(assertingdocvaluesformat.java:152)    [junit4]   2>        at org.apache.lucene.codecs.docvaluesconsumer.mergesortednumericfield(docvaluesconsumer.java:470)    [junit4]   2>        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:243)    [junit4]   2>        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.merge(perfielddocvaluesformat.java:153)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:167)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:111)    [junit4]   2>        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4320)    [junit4]   2>        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3897)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2>     [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testindexsorting -dtests.method=testrandom3 -dtests.seed=5f8898dcabbfd056 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=he -dtests.timezone=canada/central -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] error   14.9s j5  | testindexsorting.testrandom3 <<<    [junit4]    > throwable #1: org.apache.lucene.store.alreadyclosedexception: this indexwriter is closed    [junit4]    >        at org.apache.lucene.index.indexwriter.ensureopen(indexwriter.java:748)    [junit4]    >        at org.apache.lucene.index.indexwriter.ensureopen(indexwriter.java:762)    [junit4]    >        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1566)    [junit4]    >        at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1315)    [junit4]    >        at org.apache.lucene.index.testindexsorting.testrandom3(testindexsorting.java:2019)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.lang.assertionerror: nextvalue=4594289799775307848 vs previous=4606302611760746829    [junit4]    >        at org.apache.lucene.codecs.asserting.assertingdocvaluesformat$assertingdocvaluesconsumer.addsortednumericfield(assertingdocvaluesformat.java:152)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.mergesortednumericfield(docvaluesconsumer.java:470)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:243)    [junit4]    >        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.merge(perfielddocvaluesformat.java:153)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:167)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:111)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4320)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3897)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)throwable #2: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=1115, name=lucene merge thread #0, state=runnable, group=tgrp-testindexsorting]    [junit4]    > caused by: org.apache.lucene.index.mergepolicy$mergeexception: java.lang.assertionerror: nextvalue=4594289799775307848 vs previous=4606302611760746829    [junit4]    >        at __randomizedtesting.seedinfo.seed([5f8898dcabbfd056]:0)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]    > caused by: java.lang.assertionerror: nextvalue=4594289799775307848 vs previous=4606302611760746829    [junit4]    >        at org.apache.lucene.codecs.asserting.assertingdocvaluesformat$assertingdocvaluesconsumer.addsortednumericfield(assertingdocvaluesformat.java:152)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.mergesortednumericfield(docvaluesconsumer.java:470)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:243)    [junit4]    >        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.merge(perfielddocvaluesformat.java:153)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:167)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:111)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4320)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3897)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2> note: leaving temporary files on disk at: /var/lib/jenkins/jobs/lucene-solr-nightly-6.x/workspace/lucene/build/core/test/j5/temp/lucene.index.testindexsorting_5f8898dcabbfd056-001    [junit4]   2> note: test params are: codec=asserting(lucene62): {docs=postingsformat(name=direct), norms=postingsformat(name=direct), positions=postingsformat(name=memory dopackfst= false), id=postingsformat(name=direct), term_vectors=postingsformat(name=lucenefixedgap)}, docvalues:{multi_valued_long=docvaluesformat(name=lucene54), double=docvaluesformat(name=direct), foo=docvaluesformat(name=lucene54), numeric=docvaluesformat(name=direct), positions=docvaluesformat(name=lucene54), multi_valued_numeric=docvaluesformat(name=asserting), float=docvaluesformat(name=memory), int=docvaluesformat(name=asserting), long=docvaluesformat(name=memory), points=docvaluesformat(name=asserting), sorted=docvaluesformat(name=direct), multi_valued_double=docvaluesformat(name=asserting), docs=docvaluesformat(name=asserting), multi_valued_string=docvaluesformat(name=asserting), norms=docvaluesformat(name=asserting), bytes=docvaluesformat(name=asserting), binary=docvaluesformat(name=direct), id=docvaluesformat(name=asserting), multi_valued_int=docvaluesformat(name=direct), multi_valued_bytes=docvaluesformat(name=direct), multi_valued_float=docvaluesformat(name=memory), term_vectors=docvaluesformat(name=direct)}, maxpointsinleafnode=1312, maxmbsortinheap=5.376962888308618, sim=randomsimilarity(querynorm=true,coord=yes): {positions=dfr i(ne)3(800.0), id=ib ll-l3(800.0), term_vectors=dfr i(ne)b3(800.0)}, locale=he, timezone=canada/central    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=132261744,total=508559360    [junit4]   2> note: all tests run in this jvm: [testfixedbitset, testsearchafter, testminshouldmatch2, teststopfilter, testmmapdirectory, testautomaton, testtransactions, testmultilevelskiplist, testforutil, testfieldtype, testintblockpool, testnumericrangequery64, testboolean2, test2bpositions, testdemo, testspanboostquery, testspanmultitermquerywrapper, testcharsrefbuilder, testioutils, testallfileshavechecksumfooter, testmaxtermfrequency, testconjunctiondisi, testindexwritermerging, testpackedints, testlucene50storedfieldsformat, testgeoencodingutils, testgeoutils, test2bpoints, test2bsorteddocvaluesfixedsorted, testdocvalues, testexceedmaxtermlength, testexitabledirectoryreader, testfilterdirectoryreader, testindexsorting]    [junit4] completed [345/441 (1!)] on j5 in 39.46s, 43 tests, 1 error <<< failures! checking out revision 500f6c7875be31c34ca68c58f850b7e64fd049a9 (refs/remotes/origin/branch_6x) [...]    [junit4] suite: org.apache.lucene.index.testindexsorting    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testindexsorting -dtests.method=testrandom3 -dtests.seed=5d1e2d97b0777d5c -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=fi-fi -dtests.timezone=america/tegucigalpa -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error    245s j3  | testindexsorting.testrandom3 <<<    [junit4]    > throwable #1: java.lang.runtimeexception: segment has indexsort=<float: \"float\">! missingvalue=0.38226813,<sortedset: \"multi_valued_bytes\">! selector=min,<int: \"id\"> but docid=75 sorts after docid=76    [junit4]    >        at __randomizedtesting.seedinfo.seed([5d1e2d97b0777d5c:ffc6634dd485545a]:0)    [junit4]    >        at org.apache.lucene.index.checkindex.testsort(checkindex.java:876)    [junit4]    >        at org.apache.lucene.index.checkindex.checkindex(checkindex.java:756)    [junit4]    >        at org.apache.lucene.util.testutil.checkindex(testutil.java:300)    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:865)    [junit4]    >        at org.apache.lucene.util.ioutils.close(ioutils.java:89)    [junit4]    >        at org.apache.lucene.util.ioutils.close(ioutils.java:76)    [junit4]    >        at org.apache.lucene.index.testindexsorting.testrandom3(testindexsorting.java:2087)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4]   2> note: leaving temporary files on disk at: /var/lib/jenkins/jobs/lucene-solr-nightly-6.x/workspace/lucene/build/core/test/j3/temp/lucene.index.testindexsorting_5d1e2d97b0777d5c-001    [junit4]   2> note: test params are: codec=cheapbastard, sim=randomsimilarity(querynorm=true,coord=no): {positions=dfr i(f)bz(0.3), id=ib spl-dz(0.3), term_vectors=dfr i(ne)3(800.0)}, locale=fi-fi, timezone=america/tegucigalpa    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=252779544,total=525336576    [junit4]   2> note: all tests run in this jvm: [testbooleanrewrites, testsearchforduplicates, testswappedindexfiles, testclassicsimilarity, testneverdelete, testallfilescheckindexheader, testbooleanscorer, testpayloads, testdelegatinganalyzerwrapper, testweakidentitymap, testrollingbuffer, testparallelcompositereader, testexitabledirectoryreader, testcollectionutil, testlegacynumericutils, testbagofpostings, testnot, testphraseprefixquery, testsimpleexplanationswithfillerdocs, testpriorityqueue, testblockpostingsformat2, testintrosorter, finitestringsiteratortest, testindexwriterreader, testnumericrangequery64, testdeletionpolicy, testbooleanor, testsloppyphrasequery, testnrtthreads, testpolygon, testpolygon2d, testdocvalues, testfilterdirectoryreader, testindexsorting]    [junit4] completed [431/441 (1!)] on j3 in 263.65s, 43 tests, 1 error <<< failures!",
        "label": 33
    },
    {
        "text": "wrong exception from nativefslockfactory  lia2 test case  as part of integrating lucene in action 2 test cases (lucene-2661), i found one of the test cases fail the test is pretty simple, and passes on 3.0. the exception you get instead (lockreleasefailedexception) is pretty confusing and i think we should fix it.",
        "label": 33
    },
    {
        "text": "changes html is not generated for an svn export of docs when we svn-export for release, the index.html at the top level expects changes.html in the docs, which is generated, so we should create it.",
        "label": 15
    },
    {
        "text": "add convenient constructor to perfieldanalyzerwrapper for dependency injection it would be good if perfieldanalyzerwrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addanalyzer \u2013 this would make it much easier/cleaner to use this class in e.g. spring xml configurations. relatively trivial change, patch to be attached.",
        "label": 33
    },
    {
        "text": "fixes a handful of misspellings mistakes in changes txt there are a handful of misspellings/mistakes in changes.txt. this patch fixes them. avoided the one or two british to english conversions <g>",
        "label": 33
    },
    {
        "text": "create java security manager for forcible asserting behaviours in testing following on from conversations about mutation testing, there is an interest in building a java security manager that is able to assert / guarantee certain behaviours",
        "label": 53
    },
    {
        "text": "deprecate spatial disjointspatialfilter the spatial predicate \"isdisjointto\" is almost the same as the inverse of \"intersects\", except that it shouldn't match documents without spatial data. in another sense it's as if the query shape were inverted. disjointspatialfilter is a utility filter that works (or worked, rather) by using the fieldcache to see which documents have spatial data (getdocswithfield()). calculating that was probably very slow but it was at least cacheable. since lucene-5666 (v5/trunk only), rob replaced this to use docvalues. however for some spatialstrategies (prefixtree based) it wouldn't make any sense to use docvalues just so that at search time you could call getdocswithfield() when there's no other need for the un-inversion (e.g. no need to lookup terms by document). perhaps an immediate fix is simply to revert the change made to disjointspatialfilter so that it uses the fieldcache again, if that works (though it's not public?). but stepping back a bit, this disjointspatialfilter is really something unfortunate that doesn't work as well as it could because it's not at the level of solr or es \u2013 that is, there's no access to a filter-cache. so i propose i simply remove it, and if a user wants to do this for real, they should index a boolean field marking wether there's spatial data and then combine that with a not and intersects, in a straight-forward way. alternatively, some sort of inverting query shape could be developed, although it wouldn't work with the spatialprefixtree technique because there is no edge distinction \u2013 the edge matches normally and notwithstanding changes to rpt algorithms it would also match the edge of an inverted shape.",
        "label": 10
    },
    {
        "text": "leading wildcard's don't work with trailing wildcard as reported by antony bowesman, leading wildcards don't work when there is a trailing wildcard character \u2013 instead a prefixquery is constructed. http://www.nabble.com/queryparser-bug--tf3270956.html",
        "label": 12
    },
    {
        "text": "ant package fails on and master i'm doing the following steps following the readme instructions: cloning lucene-solr checking out 7.0 (or master) ant compile gives is this: ivy-configure: [ivy:configure] :: loading settings :: file = /home/antonmry/workspace/apache/lucene-solr/lucene/top-level-ivy-settings.xml resolve: [ivy:retrieve] [ivy:retrieve] :: problems summary :: [ivy:retrieve] :::: warnings [ivy:retrieve] :::::::::::::::::::::::::::::::::::::::::::::: [ivy:retrieve] :: unresolved dependencies :: [ivy:retrieve] :::::::::::::::::::::::::::::::::::::::::::::: [ivy:retrieve] :: org.apache.ivy#ivy;2.3.0: configuration not found in org.apache.ivy#ivy;2.3.0: 'master'. it was required from org.apache.lucene#tools;working@gali7 compile [ivy:retrieve] :::::::::::::::::::::::::::::::::::::::::::::: [ivy:retrieve] [ivy:retrieve] :: use verbose or debug message level for more details build failed /home/antonmry/workspace/apache/lucene-solr/build.xml:309: the following error occurred while executing this line: /home/antonmry/workspace/apache/lucene-solr/lucene/build.xml:127: the following error occurred while executing this line: /home/antonmry/workspace/apache/lucene-solr/lucene/common-build.xml:409: impossible to resolve dependencies: resolve failed - see output for details total time: 8 seconds i found it works with the the following changes in lucene/ivy-versions.properties: /org.apache.ivy/ivy = 2.3.0 to /org.apache.ivy/ivy = 2.4.0 probably related to: https://issues.apache.org/jira/browse/lucene-6144",
        "label": 53
    },
    {
        "text": "incorrect segmentinfo delcount when indexreader flush  is used when deleted documents are flushed using indexreader.flush() the delcount in segmentinfo is updated based on the current value and segmentreader.pendingdeletecount (introduced by lucene-1267). it seems that pendingdeletecount is not reset after the commit, which means after a second flush() or close() of an index reader the delcount in segmentinfo is incorrect. a subsequent indexreader.open() call will fail with an error when assertions are enabled. e.g.: java.lang.assertionerror: delete count mismatch: info=3 vs bitvector=2 at org.apache.lucene.index.segmentreader.loaddeleteddocs(segmentreader.java:405) [...]",
        "label": 33
    },
    {
        "text": "the contructor function of synonymfilter can not report exception correctly when use code synonymfilter filter=new synonymfilter(new whitespacetokenizer(version.lucene_42, new stringreader(\"aa bb\")), new synonymmap.builder(true).build(), true); create a filter,it throw nullpointerexception but not illegalargumentexception(\"fst must be non-null\");",
        "label": 33
    },
    {
        "text": "contrib analyzer setters should be deprecated and replace with ctor arguments some analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. those setters should be deprecated as they yield unexpected behaviour. the way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. analyzers itself should be immutable except of the threadlocal. will attach a patch soon.",
        "label": 46
    },
    {
        "text": "toptermsscoringbooleanqueryrewrite minscore when using the toptermsscoringbooleanqueryrewrite (lucene-2123), it would be nice if multitermquery could set an attribute specifying the minimum required score once the priority queue is filled. this way, filteredtermsenums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq) an example is fuzzytermsenum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time. this is because terms are compared by score, then termtext. so in this case fuzzytermsenum could simply seek to the exact match, then end. this behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute). other filteredtermsenums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.",
        "label": 53
    },
    {
        "text": "documentswriter blocks flushes when applydeletes takes forever   memory not released in documentswriter we have a safety check that applies all deletes if the deletes consume too much ram to prevent too-frequent flushing of a long tail of tiny segments. if we enter applyalldeletes we essentially lock on iw -> buffereddeletes which is fine since this usually doesn't take long and doesn't keep dwpts from indexing. yet, if that takes long and at the same time a semgent is flushed and subsequently published to the iw we take the lock on the ticket queue and the iw. now this prevents all other threads to append to the ticketqueue which is done before we actually flush the segment concurrently and free up the ram. essentially its ok to block on the iw lock but we should not keep concurrent flushed from execution just because we apply deletes. the threads will block once they try to execute maybemerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the dwpt to be flushed since we actually try to get rid of memory. i ran into this by accident due to a coding bug using delete queries instead of terms for each document. this thread dump show the problem: \"application worker thread\" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for monitor entry [0x00007fddad3c2000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:424)        - waiting to lock <0x00007fddb74ff990> (a org.apache.lucene.index.documentswriter$ticketqueue)        at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:320)        at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:393)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1484)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160) \"application worker thread\" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for monitor entry [0x00007fddad4c3000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.indexwriter.updatependingmerges(indexwriter.java:1854)        - waiting to lock <0x00007fddb74fe350> (a org.apache.solr.update.solrindexwriter)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1848)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1843)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1493)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160) \"application worker thread\" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for monitor entry [0x00007fddad5c4000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.indexwriter.updatependingmerges(indexwriter.java:1854)        - waiting to lock <0x00007fddb74fe350> (a org.apache.solr.update.solrindexwriter)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1848)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1843)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1493)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160) \"application worker thread\" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for monitor entry [0x00007fddad6c5000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:424)        - waiting to lock <0x00007fddb74ff990> (a org.apache.lucene.index.documentswriter$ticketqueue)        at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:320)        at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:393)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1484)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160) \"application worker thread\" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for monitor entry [0x00007fddad7c6000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.indexwriter.updatependingmerges(indexwriter.java:1854)        - waiting to lock <0x00007fddb74fe350> (a org.apache.solr.update.solrindexwriter)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1848)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1843)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1493)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160) \"application worker thread\" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable [0x00007fddad8c6000]   java.lang.thread.state: runnable        at java.nio.bits.copytoarray(bits.java:715)        at java.nio.directbytebuffer.get(directbytebuffer.java:233)        at org.apache.lucene.store.mmapdirectory$mmapindexinput.readbytes(mmapdirectory.java:319)        at org.apache.lucene.index.codecs.blocktreetermsreader$fieldreader$segmenttermsenum$frame.loadblock(blocktreetermsreader.java:2283)        at org.apache.lucene.index.codecs.blocktreetermsreader$fieldreader$segmenttermsenum.seekexact(blocktreetermsreader.java:1600)        at org.apache.lucene.util.termcontext.build(termcontext.java:97)        at org.apache.lucene.search.termquery.createweight(termquery.java:180)        at org.apache.lucene.search.booleanquery$booleanweight.<init>(booleanquery.java:186)        at org.apache.lucene.search.booleanquery.createweight(booleanquery.java:423)        at org.apache.lucene.search.indexsearcher.createnormalizedweight(indexsearcher.java:583)        at org.apache.lucene.search.querywrapperfilter.getdocidset(querywrapperfilter.java:55)        at org.apache.lucene.index.buffereddeletesstream.applyquerydeletes(buffereddeletesstream.java:431)        at org.apache.lucene.index.buffereddeletesstream.applydeletes(buffereddeletesstream.java:268)        - locked <0x00007fddb751e1e8> (a org.apache.lucene.index.buffereddeletesstream)        at org.apache.lucene.index.indexwriter.applyalldeletes(indexwriter.java:2852)        - locked <0x00007fddb74fe350> (a org.apache.solr.update.solrindexwriter)        at org.apache.lucene.index.documentswriter.applyalldeletes(documentswriter.java:188)        at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:470)        at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:320)        at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:393)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1484)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160)         \"application worker thread\" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for monitor entry [0x00007fddad9c8000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:424)        - waiting to lock <0x00007fddb74ff990> (a org.apache.lucene.index.documentswriter$ticketqueue)        at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:320)        at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:393)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1484)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160)     \"application worker thread\" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for monitor entry [0x00007fddadac9000]   java.lang.thread.state: blocked (on object monitor)        at org.apache.lucene.index.indexwriter.usecompoundfile(indexwriter.java:2274)        - waiting to lock <0x00007fddb74fe350> (a org.apache.solr.update.solrindexwriter)        at org.apache.lucene.index.indexwriter.prepareflushedsegment(indexwriter.java:2156)        at org.apache.lucene.index.documentswriter.publishflushedsegment(documentswriter.java:526)        at org.apache.lucene.index.documentswriter.finishflush(documentswriter.java:506)        at org.apache.lucene.index.documentswriter.applyflushtickets(documentswriter.java:483)        - locked <0x00007fddb74ff990> (a org.apache.lucene.index.documentswriter$ticketqueue)        at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:449)        at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:320)        at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:393)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1484)        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1456)        at org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:160)",
        "label": 46
    },
    {
        "text": "simplify query equals  remove this == other test in query.equals().",
        "label": 1
    },
    {
        "text": "adding norms  properties indexing and writer infostream support to benchmark i would like to add the following support in benchmark: ability to specify whether norms should be stored in the index. ability to specify whether norms should be stored for the body field (assuming norms are usually stored for that field in real life applications, make it explicit) ability to specify an infostream for indexwriter ability to specify whether to index the properties returned on docdata (for content sources like trec, these may include arbitrary <meta> tags, which we may not want to index). patch to come shortly.",
        "label": 33
    },
    {
        "text": "standardtokenizer doesn't tokenize word word standardtokenizer (and by result most default analyzers) will not tokenize word:word and will preserve it as one token. this can be easily seen using elasticsearch's analyze api: localhost:9200/_analyze?tokenizer=standard&text=word%20word:word if this is the intended behavior, then why? i can't really see the logic behind it. if not, i'll be happy to join in the effort of fixing this.",
        "label": 47
    },
    {
        "text": "finalizers are non canonical the canonical form of a java finalizer is: protected void finalize() throws throwable() { try { // ... local code to finalize this class } catch (throwable t) { } super.finalize(); // finalize base class. } the finalizers in indexreader, indexwriter, and fsdirectory don't conform. this is probably minor or null in effect, but the principle is important. as a matter of fact fsdirectory.finaliz() is entirely redundant and could be removed, as it doesn't do anything that randomaccessfile.finalize would do automatically.",
        "label": 33
    },
    {
        "text": "graduate geoutils and postings based geopointfield from sandbox  geopointfield is a lightweight dependency-free postings based geo field currently in sandbox. it has evolved into a very fast lightweight geo option that heavily leverages the optimized performance of the postings structure. it was originally intended to graduate to core but this does not seem appropriate given the variety of \"built on postings\" term encoding options (e.g., see lucene-6930). additionally, the geo*utils classes are dependency free lightweight relational approximation utilities used by both geopointfield and the bkd based latlonfield and can also be applied to benefit the lucene-spatial module. these classes have been evolving and baking for some time and are at a maturity level qualifying for promotion from sandbox. this will allow support for experimental encoding methods with (minimal) backwards compatibility - something sandbox does not allow. since geopoint classes are dependency free, all geopointfield and support and utility classes currently in sandbox would be promoted to the spatial3d package. (possibly a separate issue to rename spatial3d to spatialcore or spatiallite?) such that for basic lightweight geo support one would only need a handful of lucene jars. by simply adding the lucene-spatial module and its dependency jars users can obtain more advanced geospatial support (heatmap facets, full shape relations, etc).",
        "label": 36
    },
    {
        "text": "fieldsortedhitqueue   subsequent string sorts with different locales sort identically from my own post to the java-user list. i have looked into this further and am sure it's a bug. \u2014 it seems to me that there's a possible bug in fieldsortedhitqueue, specifically in getcachedcomparator(). this is showing up on our 1.4.3 install, but it seems from source code inspection that if it's a bug, it's in 1.9.1 also. the issue shows up when you need to sort results from a given indexreader multiple times, using different locales. on line 180 (all line numbers from the 1.9.1 code), we have this: scoredoccomparator comparator = lookup (reader, fieldname, type, factory); then, if no comparator is found in the cache, a new one is created (line 193) and then stored in the cache (line 202). however, both the cache lookup() and store() do not take into account locale; if we, on the same index reader, try to do one search sorted by locale.french and one by locale.italian, the first one will result in a cache miss, a new french comparator will be created, and stored in the cache. second time through, lookup() finds the cached french comparator \u2013 even though this time, the locale parameter to getcachedcomparator() is an italian locale. therefore, we don't create a new comparator and we use the wrong one to sort the results. it looks to me (unless i'm mistaken) that the fieldcacheimpl.entry class should have an additional property, .locale, to ensure that different locales get different comparators. \u2014 patch (well, most of one) to follow immediately.",
        "label": 55
    },
    {
        "text": "upgrade apache commons compress to v1 cve-2018-11771: apache commons compress 1.7 to 1.17 denial of service vulnerability announcement: https://lists.apache.org/thread.html/3f01b7315c83156875741faa56263adaf104233c6b7028092896a62c@%3cdev.commons.apache.org%3e",
        "label": 47
    },
    {
        "text": "sloppyphrasescorer returns non deterministic results for queries with many repeats proximity queries with many repeats (four or more, based on my testing) return non-deterministic results. i run the same query multiple times with the same data set and get different results. so far i've reproduced this with solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk. steps to reproduce (using the solr example): 1) in solrconfig.xml, set queryresultcache size to 0. 2) add some documents with text \"dog dog dog\" and \"dog dog dog dog\". http://localhost:8983/solr/update?stream.body=%3cadd%3e%3cdoc%3e%3cfield%20name=%22id%22%3e1%3c/field%3e%3cfield%20name=%22text%22%3edog%20dog%20dog%3c/field%3e%3c/doc%3e%3cdoc%3e%3cfield%20name=%22id%22%3e2%3c/field%3e%3cfield%20name=%22text%22%3edog%20dog%20dog%20dog%3c/field%3e%3c/doc%3e%3c/add%3e&commit=true 3) do a \"dog dog dog dog\"~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1 4) repeat step 3 many times. expected results: the document with id 2 should be returned. actual results: the document with id 2 is always returned. the document with id 1 is sometimes returned. different proximity values show the same bug - \"dog dog dog dog\"~5, \"dog dog dog dog\"~100, etc show the same behavior. so far i've traced it down to the \"repeats\" array in sloppyphrasescorer.initphrasepositions() - depending on the order of the elements in this array, the document may or may not match. i think the hashset may be to blame, but i'm not sure - that at least seems to be where the non-determinism is coming from.",
        "label": 12
    },
    {
        "text": "remove java close  hack in outputstreamindexoutput in outputstreamindexoutput we have the following hack:   @override   public void close() throws ioexception {     try (final outputstream o = os) {       // we want to make sure that os.flush() was running before close:       // bufferedoutputstream may ignore ioexceptions while flushing on close().       // todo: this is no longer an issue in java 8:       // http://hg.openjdk.java.net/jdk8/tl/jdk/rev/759aa847dcaf       o.flush();     }   } as we are on java 8 already in trunk, we can remove this hack. the bug was fixed in java 8, bufferedoutputstream / filteroutputstream always calls flush() and close(), although an error happened!",
        "label": 53
    },
    {
        "text": "use weakhashmap instead of hashtable in fsdirectory i was just reading the fsdirectory java code, then i found this : /** this cache of directories ensures that there is a unique directory instance per path, so that synchronization on the directory can be used to synchronize access between readers and writers. * this should be a weakhashmap, so that entries can be gc'd, but that would require java 1.2. instead we use refcounts... */ private static final hashtable directories = new hashtable(); since lucene is now requiring at least 1.2 (for threadlocal for instance, which is using btw some weakhashmap), maybe it is time to change ?",
        "label": 33
    },
    {
        "text": "document skipping on large indexes is broken large skips on large indexes fail. anything that uses skips (such as a boolean query, filtered queries, faceted queries, join queries, etc) can trigger this bug on a sufficiently large index. the bug is a numeric overflow in multilevelskiplist that has been present since inception (lucene 2.2). it may not manifest until one has a single segment with more than ~1.8b documents, and a large skip is performed on that segment. typical stack trace on lucene7-dev: java.lang.arrayindexoutofboundsexception: 110  at org.apache.lucene.codecs.multilevelskiplistreader$skipbuffer.readbyte(multilevelskiplistreader.java:297)  at org.apache.lucene.store.datainput.readvint(datainput.java:125)  at org.apache.lucene.codecs.lucene50.lucene50skipreader.readskipdata(lucene50skipreader.java:180)  at org.apache.lucene.codecs.multilevelskiplistreader.loadnextskip(multilevelskiplistreader.java:163)  at org.apache.lucene.codecs.multilevelskiplistreader.skipto(multilevelskiplistreader.java:133)  at org.apache.lucene.codecs.lucene50.lucene50postingsreader$blockdocsenum.advance(lucene50postingsreader.java:421)  at ycs_skip7$1.testskip(ycs_skip7.java:307) typical stack trace on lucene4.10.3: 6-08-31 18:57:17,460 error org.apache.solr.servlet.solrdispatchfilter: null:java.lang.arrayindexoutofboundsexception: 75  at org.apache.lucene.codecs.multilevelskiplistreader$skipbuffer.readbyte(multilevelskiplistreader.java:301)  at org.apache.lucene.store.datainput.readvint(datainput.java:122)  at org.apache.lucene.codecs.lucene41.lucene41skipreader.readskipdata(lucene41skipreader.java:194)  at org.apache.lucene.codecs.multilevelskiplistreader.loadnextskip(multilevelskiplistreader.java:168)  at org.apache.lucene.codecs.multilevelskiplistreader.skipto(multilevelskiplistreader.java:138)  at org.apache.lucene.codecs.lucene41.lucene41postingsreader$blockdocsenum.advance(lucene41postingsreader.java:506)  at org.apache.lucene.search.termscorer.advance(termscorer.java:85) [...]  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:621) [...]  at org.apache.solr.core.solrcore.execute(solrcore.java:2004)",
        "label": 55
    },
    {
        "text": "scorer skipto affects sloppyphrase scoring if you mix skipto() and next(), you get different scores than what is returned to a hit collector.",
        "label": 12
    },
    {
        "text": "planetobject interface hi karl wright, i propose to add a new interface call planetobject which all shapes should implement. it is actually extracted from class baseplanetobject. the motivation is that currently the method getplanetmodel() is not visible and therefore there is no possibility to know to which planetmodel a shape belongs to. the side effect for this change is that the constructors for composite shapes change as they need to be created with a planetmodel. i think this is correct as then we can check the planet model when adding a shape and make sure all objects in a composite belongs to the same planet model. in addition, we check that two shape belongs to the shape planet model when calling getrelationship(geoshape geoshape).",
        "label": 25
    },
    {
        "text": "testblockjoinsorting testnestedsorting asset fails ant test -dtestcase=testblockjoinsorting -dtests.method=testnestedsorting -dtests.seed=fb4f1be85579255b -dtests.slow=true -dtests.locale=da_dk -dtests.timezone=asia/qatar -dtests.file.encoding=utf-8 [junit4:junit4] failure 0.86s | testblockjoinsorting.testnestedsorting <<< [junit4:junit4] > throwable #1: java.lang.assertionerror: expected:<3> but was:<28> [junit4:junit4] > at __randomizedtesting.seedinfo.seed([fb4f1be85579255b:f3a6f6a915d02835]:0) [junit4:junit4] > at org.apache.lucene.search.join.testblockjoinsorting.testnestedsorting(testblockjoinsorting.java:226) [junit4:junit4] > at java.lang.thread.run(thread.java:680) [junit4:junit4] 2> note: test params are: codec=asserting, sim=randomsimilarityprovider(querynorm=true,coord=crazy): {}, locale=da_dk, timezone=asia/qatar [junit4:junit4] [junit4:junit4] tests with failures: [junit4:junit4] - org.apache.lucene.search.join.testblockjoinsorting.testnestedsorting",
        "label": 30
    },
    {
        "text": "java7 as a minimum requirement for lucene spinoff from lucene-4746. i propose we make this change on trunk only.",
        "label": 53
    },
    {
        "text": "testgeo3dpoint testgeo3drelations  failures my jenkins found a reproducing master seed:   [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint   [junit4]   1>     doc=1114 is contained by shape but is outside the returned xyzbounds   [junit4]   1>       unquantized=[lat=-1.5316724989005415, lon=3.141592653589793([x=-0.03902652216795768, y=4.779370545484258e-18, z=-0.9970038705813589])]   [junit4]   1>       quantized=[x=-0.03902652216283731, y=2.3309121299774915e-10, z=-0.9970038706538652]   [junit4]   1>   shape=geocompositemembershipshape: {[geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=1.079437865394857, lon=-1.720224083538152e-11([x=0.47111944719262044, y=-8.104310192839264e-12, z=0.8803759987367299])], [lat=-1.5707963267948966, lon=0.017453291479645996([x=6.108601474971234e-17, y=1.066260290095308e-18, z=-0.997762292022105])], [lat=0.017453291479645996, lon=2.4457272005608357e-47([x=1.0009653513901666, y=2.448088186713865e-47, z=0.01747191415779267])]], internaledges={2}}, geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=1.079437865394857, lon=-1.720224083538152e-11([x=0.47111944719262044, y=-8.104310192839264e-12, z=0.8803759987367299])], [lat=0.017453291479645996, lon=2.4457272005608357e-47([x=1.0009653513901666, y=2.448088186713865e-47, z=0.01747191415779267])], [lat=0.0884233366943164, lon=0.4323234231678824([x=0.9054355304510789, y=0.4178006803188124, z=0.08840463683725623])]], internaledges={0}}]}   [junit4]   1>   bounds=xyzbounds: [xmin=-0.017533380657829275 xmax=1.0011188549924792 ymin=-1.0172214856320074e-9 ymax=0.4178006813199529 zmin=-0.9977622930221051 zmax=0.9977622930221051]   [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=dea074fdc975e012 -dtests.slow=true -dtests.locale=de-lu -dtests.timezone=pacific/pohnpei -dtests.asserts=true -dtests.file.encoding=utf-8   [junit4] failure 0.64s j6 | testgeo3dpoint.testgeo3drelations <<<   [junit4]    > throwable #1: java.lang.assertionerror: invalid bounds for shape=geocompositemembershipshape: {[geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=1.079437865394857, lon=-1.720224083538152e-11([x=0.47111944719262044, y=-8.104310192839264e-12, z=0.8803759987367299])], [lat=-1.5707963267948966, lon=0.017453291479645996([x=6.108601474971234e-17, y=1.066260290095308e-18, z=-0.997762292022105])], [lat=0.017453291479645996, lon=2.4457272005608357e-47([x=1.0009653513901666, y=2.448088186713865e-47, z=0.01747191415779267])]], internaledges={2}}, geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=1.079437865394857, lon=-1.720224083538152e-11([x=0.47111944719262044, y=-8.104310192839264e-12, z=0.8803759987367299])], [lat=0.017453291479645996, lon=2.4457272005608357e-47([x=1.0009653513901666, y=2.448088186713865e-47, z=0.01747191415779267])], [lat=0.0884233366943164, lon=0.4323234231678824([x=0.9054355304510789, y=0.4178006803188124, z=0.08840463683725623])]], internaledges={0}}]}   [junit4]    >  at __randomizedtesting.seedinfo.seed([dea074fdc975e012:6edf096946384e8e]:0)   [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:259)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4] ignor/a 0.00s j6 | testgeo3dpoint.testrandombig   [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())   [junit4]   2> note: test params are: codec=asserting(lucene62): {id=fstord50}, docvalues:{id=docvaluesformat(name=direct)}, maxpointsinleafnode=1836, maxmbsortinheap=5.467605512876335, sim=randomsimilarity(querynorm=true): {}, locale=de-lu, timezone=pacific/pohnpei   [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=507535088,total=528482304   [junit4]   2> note: all tests run in this jvm: [testgeo3dpoint]   [junit4] completed [11/11 (1!)] on j6 in 9.39s, 14 tests, 1 failure, 1 skipped <<< failures! i looked back through email notifications from my jenkins and found this older but still reproducing failure:   [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint   [junit4] ignor/a 0.01s j1 | testgeo3dpoint.testrandombig   [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())   [junit4]   1>     doc=492 is contained by shape but is outside the returned xyzbounds   [junit4]   1>       unquantized=[lat=0.0, lon=-0.022127568649887065([x=1.0008737754335042, y=-0.02215051847469871, z=0.0])]   [junit4]   1>       quantized=[x=1.0008737754427615, y=-0.022150518349539516, z=2.3309121299774915e-10]   [junit4]   1>     doc=879 is contained by shape but is outside the returned xyzbounds   [junit4]   1>       unquantized=[lat=0.0, lon=-0.03833969548052218([x=1.0003831556734408, y=-0.038373189391333544, z=0.0])]   [junit4]   1>       quantized=[x=1.0003831557339953, y=-0.03837318940337864, z=2.3309121299774915e-10]   [junit4]   1>   shape=geocompositemembershipshape: {[geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=0.0, lon=-2.0764049210215627e-307([x=1.0011188539924791, y=-2.078728114957451e-307, z=0.0])], [lat=-0.05235987755982989, lon=1.1522039653792695([x=0.4063681026669516, y=0.9134222954698412, z=-0.05239402894622555])], [lat=0.0, lon=0.9689058028225104([x=0.5668352773145171, y=0.825188904561246, z=0.0])]], internaledges={2}}, geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=0.0, lon=-2.0764049210215627e-307([x=1.0011188539924791, y=-2.078728114957451e-307, z=0.0])], [lat=0.0, lon=0.9689058028225104([x=0.5668352773145171, y=0.825188904561246, z=0.0])], [lat=8.602294890145611e-12, lon=2.336616258000507([x=-0.6939037589206829, y=0.7216207682536315, z=8.611919602127933e-12])]], internaledges={0}}]}   [junit4]   1>   bounds=xyzbounds: [xmin=-0.823578472647096 xmax=1.0011188549924792 ymin=-1.0000286565170571e-9 ymax=1.0011188549924792 zmin=-0.05239402994622555 zmax=1.0119474874643955e-9]   [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.   [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=6d5666e45e8f3cda -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=ar-tn -dtests.timezone=america/sitka -dtests.asserts=true -dtests.file.encoding=utf-8   [junit4] failure 0.06s j1 | testgeo3dpoint.testgeo3drelations <<<   [junit4]    > throwable #1: java.lang.assertionerror: invalid bounds for shape=geocompositemembershipshape: {[geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=0.0, lon=-2.0764049210215627e-307([x=1.0011188539924791, y=-2.078728114957451e-307, z=0.0])], [lat=-0.05235987755982989, lon=1.1522039653792695([x=0.4063681026669516, y=0.9134222954698412, z=-0.05239402894622555])], [lat=0.0, lon=0.9689058028225104([x=0.5668352773145171, y=0.825188904561246, z=0.0])]], internaledges={2}}, geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=0.0, lon=-2.0764049210215627e-307([x=1.0011188539924791, y=-2.078728114957451e-307, z=0.0])], [lat=0.0, lon=0.9689058028225104([x=0.5668352773145171, y=0.825188904561246, z=0.0])], [lat=8.602294890145611e-12, lon=2.336616258000507([x=-0.6939037589206829, y=0.7216207682536315, z=8.611919602127933e-12])]], internaledges={0}}]}   [junit4]    >  at __randomizedtesting.seedinfo.seed([6d5666e45e8f3cda:dd291b70d1c29246]:0)   [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:259)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4]   2> note: test params are: codec=lucene62, sim=randomsimilarity(querynorm=false,coord=crazy): {}, locale=ar-tn, timezone=america/sitka   [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=506426848,total=534773760   [junit4]   2> note: all tests run in this jvm: [testgeo3dpoint]   [junit4] completed [11/11 (1!)] on j1 in 10.42s, 14 tests, 1 failure, 1 skipped <<< failures!",
        "label": 25
    },
    {
        "text": "wrong javadoc on lowercasetokenizer normalize the javadoc on lowercasetokenizer.normalize seems to be copy/paste from lettertokenizer.istokenchar.",
        "label": 53
    },
    {
        "text": "let users set similarity for morelikethis let users set similarity used for morelikethis for discussion, see: http://www.nabble.com/morelikethis-api-changes--tf3838535.html",
        "label": 38
    },
    {
        "text": "move patternanalyzer out of contrib memory to contrib analyzers in the memory index contrib there is a patternanalyzer. i think this analyzer belongs in contrib/analyzers instead, it has no relation to memory index.",
        "label": 40
    },
    {
        "text": "the class from cotrub directory org apache lucene index indexsplitter creates a non correct index when using the method indexsplitter.split(file destdir, string[] segs) from the lucene cotrib directory (contrib/misc/src/java/org/apache/lucene/index) it creates an index with segments descriptor file with wrong data. namely wrong is the number representing the name of segment that would be created next in this index. if some of the segments of the index already has this name this results either to impossibility to create new segment or in crating of an corrupted segment.",
        "label": 47
    },
    {
        "text": "ban math toradians todegrees and remove all usages of it the result of these methods is unreliable and changes across jvm versions: we cannot use these methods. the following program prints 0.7722082215479366 on previous versions of java but 0.7722082215479367 on java 9 because math.toradians is no longer doing the same thing: public class test {   public static void main(string args[]) throws exception {     system.out.println(math.toradians(44.244272));   } } this is because of https://bugs.openjdk.java.net/browse/jdk-4477961. i am not really sure its a bug, because the method says that the conversion is \"generally inexact\".",
        "label": 25
    },
    {
        "text": "should segmenttermpositionvector be public  i'm wondering why segmenttermpositionvector is public. it implements the public interface termpositionvector. should we remove \"public\"?",
        "label": 32
    },
    {
        "text": "testaddindexes reproducible test failure on turnk trunk: r1133385     [junit] testsuite: org.apache.lucene.index.testaddindexes     [junit] tests run: 2843, failures: 1, errors: 0, time elapsed: 137.121 sec     [junit]     [junit] ------------- standard output ---------------     [junit] java.io.filenotfoundexception: _cy.fdx     [junit]     at org.apache.lucene.store.ramdirectory.filelength(ramdirectory.java:121)     [junit]     at org.apache.lucene.store.mockdirectorywrapper.filelength(mockdirectorywrapper.java:606)     [junit]     at org.apache.lucene.index.segmentinfo.sizeinbytes(segmentinfo.java:294)     [junit]     at org.apache.lucene.index.tieredmergepolicy.size(tieredmergepolicy.java:633)     [junit]     at org.apache.lucene.index.tieredmergepolicy.usecompoundfile(tieredmergepolicy.java:611)     [junit]     at org.apache.lucene.index.indexwriter.addindexes(indexwriter.java:2459)     [junit]     at org.apache.lucene.index.testaddindexes$commitandaddindexes3.dobody(testaddindexes.java:847)     [junit]     at org.apache.lucene.index.testaddindexes$runaddindexesthreads$1.run(testaddindexes.java:675)     [junit] java.io.filenotfoundexception: _cx.fdx     [junit]     at org.apache.lucene.store.ramdirectory.filelength(ramdirectory.java:121)     [junit]     at org.apache.lucene.store.mockdirectorywrapper.filelength(mockdirectorywrapper.java:606)     [junit]     at org.apache.lucene.index.segmentinfo.sizeinbytes(segmentinfo.java:294)     [junit]     at org.apache.lucene.index.tieredmergepolicy.size(tieredmergepolicy.java:633)     [junit]     at org.apache.lucene.index.tieredmergepolicy.usecompoundfile(tieredmergepolicy.java:611)     [junit]     at org.apache.lucene.index.indexwriter.addindexes(indexwriter.java:2459)     [junit]     at org.apache.lucene.index.testaddindexes$commitandaddindexes3.dobody(testaddindexes.java:847)     [junit]     at org.apache.lucene.index.testaddindexes$runaddindexesthreads$1.run(testaddindexes.java:675)     [junit] ------------- ---------------- ---------------     [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testaddindexes -dtestmethod=testaddindexeswithrollback -dtests.seed=9026722750295014952:2645762923088581043 -dtests.multiplier=3     [junit] note: test params are: codec=randomcodecprovider: {id=simpletext, content=simpletext, d=mockrandom, c=simpletext}, locale=fr, timezone=africa/kigali     [junit] note: all tests run in this jvm:     [junit] [testaddindexes]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624     [junit] ------------- ---------------- ---------------     [junit] testcase: testaddindexeswithrollback(org.apache.lucene.index.testaddindexes):       failed     [junit]     [junit] junit.framework.assertionfailederror:     [junit]     at org.apache.lucene.index.testaddindexes.testaddindexeswithrollback(testaddindexes.java:932)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1362)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1280)     [junit]     [junit]     [junit] test org.apache.lucene.index.testaddindexes failed fails randomly in my while(1) test run, and fails after a few min of running: ant test -dtestcase=testaddindexes -dtests.seed=9026722750295014952:2645762923088581043 -dtests.multiplier=3 -dtests.iter=200 -dtests.iter.min=1",
        "label": 33
    },
    {
        "text": "filteredquery ignores boost filtered query ignores it's own boost.",
        "label": 32
    },
    {
        "text": "when we move to java in we should replace all interger  long  etc construction with  valueof -128 to 128 are guaranteed to be cached and using valueof in that case is 3.5 times faster than using contructor",
        "label": 53
    },
    {
        "text": "add japanese kanji number normalization to kuromoji japanese people use kanji numerals instead of arabic numerals for writing price, address and so on. i.e 12\u4e074800\u5186(124,800jpy), \u4e8c\u756a\u753a\u4e09\u30ce\u4e8c(3-2 nibancho) and \u5341\u4e8c\u6708(december). so, we would like to normalize those kanji numerals to arabic numerals (i don't think we need to have a capability to normalize to kanji numerals).",
        "label": 8
    },
    {
        "text": "minor refactoring to indexfilenamefilter indexfilenamefilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. the proposed patch changes the constructor and member to private. since it already has a static getfilter() method, and no code in lucene references those two, i don't think it creates any problems from an api perspective.",
        "label": 33
    },
    {
        "text": "revise scorer visitor api currently there is an (expert) api in scorer to allow you to take a scorer, and visit its children etc (e.g. booleanquery). i think we should improve this, so its general to all queries. the current issues are: the current api is hardcoded for booleanquery's occurs, but we should support other types of children (e.g. disjunctionmax) it can be difficult to use the api, because of the amount of generics and the visitor callback api. the current api enforces a dfs traversal when you might prefer bfs instead.",
        "label": 40
    },
    {
        "text": "validate standardqueryparser with not operator with in parentheses  provide test case to validate lucene-6249, which validates not in parentheses. e.g: lottery (not ticket) lottery (-ticket) lottery and (not ticket) +lottery +(-ticket)",
        "label": 11
    },
    {
        "text": "flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges. these two problems were found while developing lucene-1768.",
        "label": 0
    },
    {
        "text": "add getversion method to indexcommit returns the equivalent of indexreader.getversion for indexcommit public abstract long getversion();",
        "label": 33
    },
    {
        "text": "improve error message when 'ant beast' is run on top level modules as discussed at http://markmail.org/thread/c5y63pmvpgyrmct5 'ant beast' currently gives confusing error messages when run on top-level modules, this makes it clear that it should only be run within a module.",
        "label": 53
    },
    {
        "text": "separate bbox logic in latlonshapeboundingboxquery currently latlonshapeboundingboxquery contains the logic to relate the query bounding box with the incoming triangles. this issue proposes the creation of a new class rectangle2d that encapsulates the spatial logic of the bounding box",
        "label": 19
    },
    {
        "text": "add directory wrapper that optionally uses hardlinks in copyfrom today we always do byte-by-byte copy in directory#copyfrom. while this is reliable and should be the default, certain situations can be improved by using hardlinks if possible to get constant time copy on os / fs that support such an operation. something like this could reside in misc if it's contained enough since it requires linkpermissions to be set and needs to detect if both directories are subclasses of fsdirectory etc.",
        "label": 46
    },
    {
        "text": "java port phase ii lucene-1257 addresses the public api changes ( generics , mainly ) and other j.u.c. package changes related to the api . the changes are frozen and closed for 3.0 . this would be a placeholder jira for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal api changes as necessary.",
        "label": 33
    },
    {
        "text": "index writer constructor flags unclear   and annoying in certain cases wouldn't it make more sense if the constructor for the indexwriter always created an index if it doesn't exist - and the boolean parameter should be \"clear\" (instead of \"create\") so instead of this (from javadoc): indexwriter public indexwriter(directory d, analyzer a, boolean create) throws ioexception constructs an indexwriter for the index in d. text will be analyzed with a. if create is true, then a new, empty index will be created in d, replacing the index already there, if any. parameters: d - the index directory a - the analyzer to use create - true to create the index or overwrite the existing one; false to append to the existing index throws: ioexception - if the directory cannot be read/written to, or if it does not exist, and create is false we would have this: indexwriter public indexwriter(directory d, analyzer a, boolean clear) throws ioexception constructs an indexwriter for the index in d. text will be analyzed with a. if clear is true, and a index exists at location d, then it will be erased, and a new, empty index will be created in d. parameters: d - the index directory a - the analyzer to use clear - true to overwrite the existing one; false to append to the existing index throws: ioexception - if the directory cannot be read/written to, or if it does not exist. its current behavior is kind of annoying, because i have an app that should never clear an existing index, it should always append. so i want create set to false. but when i am starting a brand new index, then i have to change the create flag to keep it from throwing an exception... i guess for now i will have to write code to check if a index actually has content yet, and if it doesn't, change the flag on the fly.",
        "label": 33
    },
    {
        "text": "improve error messages for unsupported hunspell formats our hunspell implementation is never going to be able to support the huge variety of formats that are out there, especially since our impl is based on papers written on the topic rather than being a pure port. recently we ran into the following suffix rule: sfx ca 0 /cacp due to the missing regex conditional, an aoe was being thrown, which made it difficult to diagnose the problem. we should instead try to provide better error messages showing what we were unable to parse.",
        "label": 7
    },
    {
        "text": "cutover oal index  tests to use a random iwc to tease out bugs ",
        "label": 40
    },
    {
        "text": "upgrade opennlp to opennlp 1.9.0 generates new format model file which 1.8.x cannot read. 1.9.0 can read the previous format for back-compat.",
        "label": 26
    },
    {
        "text": "a lexicon object for merging spellchecker and synonyms from stemming some lucene features need a list of referring word. spellchecking is the basic example, but synonyms is an other use. other tools can be used smoothlier with a list of words, without disturbing the main index : stemming and other simplification of word (anagram, phonetic ...). for that, i suggest a lexicon object, wich contains words (term + frequency), wich can be built from lucene directory, or plain text files. classical tokenfilter can be used with lexicon (lowercasefilter and isolatin1accentfilter should be the most useful). lexicon uses a lucene directory, each word is a document, each meta is a field (word, ngram, phonetic, fields, anagram, size ...). above a minimum size, number of differents words used in an index can be considered as stable. so, a standard lexicon (built from wikipedia by example) can be used. a similartokenfilter is provided. a spellchecker will come soon. a fuzzysearch implementation, a neutral synonym tokenfilter can be done. unused words can be remove on demand (lazy delete?) any criticism or suggestions?",
        "label": 38
    },
    {
        "text": "testpointqueries failures my jenkins found a reproducing seed on master: checking out revision a48245a1bfbef0259d38ef36fec814f3891ab80c (refs/remotes/origin/master) [...]    [junit4] suite: org.apache.lucene.search.testpointqueries    [junit4] ignor/a 0.00s j1 | testpointqueries.testrandombinarybig    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())    [junit4]   2> maj 02, 2016 3:29:13 pm com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[t0,5,tgrp-testpointqueries]    [junit4]   2> java.lang.assertionerror    [junit4]   2>  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]   2>  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]   2>  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)    [junit4]   2>     [junit4]   2> maj 02, 2016 3:29:13 pm com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[t1,5,tgrp-testpointqueries]    [junit4]   2> java.lang.assertionerror    [junit4]   2>  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]   2>  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]   2>  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)    [junit4]   2>     [junit4]   2> maj 02, 2016 3:29:13 pm com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[t3,5,tgrp-testpointqueries]    [junit4]   2> java.lang.assertionerror    [junit4]   2>  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]   2>  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]   2>  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)    [junit4]   2>     [junit4]   2> maj 02, 2016 3:29:13 pm com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[t2,5,tgrp-testpointqueries]    [junit4]   2> java.lang.assertionerror    [junit4]   2>  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]   2>  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]   2>  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]   2>  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]   2>  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]   2>  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]   2>  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]   2>  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)    [junit4]   2>     [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testpointqueries -dtests.method=testrandombinarytiny -dtests.seed=61528898a1a30059 -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=pl -dtests.timezone=america/phoenix -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   0.16s j1 | testpointqueries.testrandombinarytiny <<<    [junit4]    > throwable #1: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=463, name=t0, state=runnable, group=tgrp-testpointqueries]    [junit4]    > caused by: java.lang.assertionerror    [junit4]    >  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]    >  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]    >  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)throwable #2: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=464, name=t1, state=runnable, group=tgrp-testpointqueries]    [junit4]    > caused by: java.lang.assertionerror    [junit4]    >  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]    >  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]    >  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)throwable #3: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=465, name=t2, state=runnable, group=tgrp-testpointqueries]    [junit4]    > caused by: java.lang.assertionerror    [junit4]    >  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]    >  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]    >  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)throwable #4: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=466, name=t3, state=runnable, group=tgrp-testpointqueries]    [junit4]    > caused by: java.lang.assertionerror    [junit4]    >  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:110)    [junit4]    >  at org.apache.lucene.util.docidsetbuilder.<init>(docidsetbuilder.java:98)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.buildmatchingdocidset(pointrangequery.java:109)    [junit4]    >  at org.apache.lucene.search.pointrangequery$1.scorer(pointrangequery.java:213)    [junit4]    >  at org.apache.lucene.search.weight.bulkscorer(weight.java:135)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.cache(lruquerycache.java:683)    [junit4]    >  at org.apache.lucene.search.lruquerycache$cachingwrapperweight.bulkscorer(lruquerycache.java:766)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.assertingweight.bulkscorer(assertingweight.java:68)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:666)    [junit4]    >  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:91)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:473)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2._run(testpointqueries.java:805)    [junit4]    >  at org.apache.lucene.search.testpointqueries$2.run(testpointqueries.java:758)    [junit4] ignor/a 0.00s j1 | testpointqueries.testrandomlongsbig    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())    [junit4]   2> note: test params are: codec=lucene60, sim=classicsimilarity, locale=pl, timezone=america/phoenix    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=238466144,total=339214336    [junit4]   2> note: all tests run in this jvm: [testofflinesorter, testmathutil, testmergeschedulerexternal, test2bterms, testpayloads, testfilterleafreader, testmixedcodecs, testsegmentinfos, testgeoencodingutils, testfastcompressionmode, testmmapdirectory, teststressindexing2, testcrash, testblockpostingsformat, testdemoparallelleafreader, testbooleanminshouldmatch, testcharsrefbuilder, testlongpostings, testpersistentsnapshotdeletionpolicy, testdocumentswriterstallcontrol, testsubscorerfreqs, testlevenshteinautomata, testallfilesdetecttruncation, testdocvalues, testregexp, testsearch, testmergepolicywrapper, testnrtcachingdirectory, testexceedmaxtermlength, testfieldsreader, test2bpostings, testarrayutil, testsearchafter, testversion, testnewestsegment, testindexwriterreader, testlegacynumericutils, testscorecachingwrappingscorer, testsimplefsdirectory, testspannotquery, testmixeddocvaluesupdates, testconsistentfieldnumbers, testpointqueries]    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testpointqueries -dtests.seed=61528898a1a30059 -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=pl -dtests.timezone=america/phoenix -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   0.00s j1 | testpointqueries (suite) <<<    [junit4]    > throwable #1: java.lang.assertionerror: the test or suite printed 12284 bytes to stdout and stderr, even though the limit was set to 8192 bytes. increase the limit with @limit, ignore it completely with @suppresssysoutchecks or run with -dtests.verbose=true    [junit4]    >  at __randomizedtesting.seedinfo.seed([61528898a1a30059]:0)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4] completed [50/419 (1!)] on j1 in 6.63s, 45 tests, 1 failure, 1 error, 2 skipped <<< failures!",
        "label": 33
    },
    {
        "text": "impl tostring  in mergepolicy and its extensions these can be important to see for debugging. we lost them in the cutover to iwc. just opening this issue to remind us to get them back, before releasing...",
        "label": 43
    },
    {
        "text": "make weightedspantermextractor extensible to handle custom query implemenations currently if i have a custom query which subclasses query directly i can't use the queryscorer for highlighting since it does explicit instanceof checks. in some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. however i had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. to still use the highlighter i had to copy a bunch of code due to visibility issues in those classes. i think we can make this extensible with minor effort to allow this usecase without massive code duplication.",
        "label": 46
    },
    {
        "text": "testcachingspanfilter sometimes fails if i run   ant test -dtestcase=testcachingspanfilter -dtestmethod=testenforcedeletions -dtests.seed=5015158121350221714:-3342860915127740146 -dtests.iter=100 i get two failures on my machine against current trunk   junit-sequential:     [junit] testsuite: org.apache.lucene.search.testcachingspanfilter     [junit] testcase: testenforcedeletions(org.apache.lucene.search.testcachingspanfilter): failed     [junit] expected:<2> but was:<3>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<3>     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]  at org.apache.lucene.search.testcachingspanfilter.testenforcedeletions(testcachingspanfilter.java:101)     [junit]      [junit]      [junit] testcase: testenforcedeletions(org.apache.lucene.search.testcachingspanfilter): failed     [junit] expected:<2> but was:<3>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<3>     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]  at org.apache.lucene.search.testcachingspanfilter.testenforcedeletions(testcachingspanfilter.java:101)     [junit]      [junit]      [junit] tests run: 100, failures: 2, errors: 0, time elapsed: 2.297 sec     [junit]      [junit] ------------- standard output ---------------     [junit] note: reproduce with: ant test -dtestcase=testcachingspanfilter -dtestmethod=testenforcedeletions -dtests.seed=5015158121350221714:-3342860915127740146     [junit] note: reproduce with: ant test -dtestcase=testcachingspanfilter -dtestmethod=testenforcedeletions -dtests.seed=5015158121350221714:-3342860915127740146     [junit] note: test params are: codec=mockvariableintblock(baseblocksize=43), locale=fr, timezone=africa/bangui     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.search.testcachingspanfilter failed not sure what it is but it seems likely to be a weakref / gc issue in the cache.",
        "label": 33
    },
    {
        "text": "distancefilter problem with deleted documents i know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib. i suspect that the issue is that deleted documents are trying to be evaluated by the filter. i did some debugging and i confirmed that it is bombing on a document that is marked as deleted (using luke). thanks! using the locallucene library 1.51, i get a nullpointerexception at line 123 of distancefilter the method is public bitset bits(indexreader reader) the line is double x = numberutils.sortablestr2double(sx); the stack trace is: java.lang.nullpointerexception at org.apache.solr.util.numberutils.sortablestr2long(numberutils.java:149) at org.apache.solr.util.numberutils.sortablestr2double(numberutils.java:104) at com.pjaol.search.geo.utils.distancefilter.bits(distancefilter.java:123) at org.apache.lucene.search.filter.getdocidset(filter.java:49) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:140) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:112) at org.apache.lucene.search.hits.getmoredocs(hits.java:113) at org.apache.lucene.search.hits.<init>(hits.java:90) at org.apache.lucene.search.searcher.search(searcher.java:72)",
        "label": 33
    },
    {
        "text": "add a isdeleted method to indexcommit i wish to add a indexcommit.isdeleted() method. the use-case is that solr will now support configurable indexdeletionpolicy (solr-617). for the new replication (solr-561) to work, we need access to a list of indexcommit instances which haven't been deleted yet. i can wrap the user specified indexdeletionpolicy but since the indexcommit does not have a isdeleted method, i may store a reference to an indexcommit on which delete() has been called by the deletion policy. i can wrap the indexcommit objects too just for having a isdeleted() method so a workaround exists. not a big pain but if it can be managed on the lucene side easily, i'll appreciate it. it would save me from writing some delegate code.",
        "label": 33
    },
    {
        "text": "boostingtermquery explain  bugs there are a couple of minor bugs in boostingtermquery.explain(). 1. the computation of average payload score produces nan if no payloads were found. it should probably be: float avgpayloadscore = super.score() * (payloadsseen > 0 ? (payloadscore / payloadsseen) : 1); 2. if the average payload score is zero, the value of the explanation is 0: result.setvalue(nonpayloadexpl.getvalue() * avgpayloadscore); if the query is part of a booleanclause, this results in: \"no match on required clause...\" \"failure to meet condition(s) of required/prohibited clause(s)\" the average payload score can be zero if the field boost = 0. i've attached a patch to 'testboostingtermquery.java', however, the test 'testnopayload' fails in 'spanscorer.score()' because the doc = -1. it looks like 'setfreqcurrentdoc() should have been called before 'score()'. maybe someone more knowledgable of spans could investigate this.",
        "label": 15
    },
    {
        "text": "add cachingwrapperfilter getfilter  there are a couple of use cases i can think of where being able to get the underlying filter out of cachingwrapperfilter would be useful: 1. you might want to introspect the filter to figure out what's in it (the use case we hit.) 2. you might want to serialise the filter since lucene no longer supports that itself. we currently work around this by subclassing, keeping another copy of the underlying filter reference and implementing a trivial getter, which is an easy workaround, but the trap is that a junior developer could unknowingly create a cachingwrapperfilter without knowing that the bettercachingwrapperfilter exists, introducing a filter which cannot be introspected.",
        "label": 1
    },
    {
        "text": "review usages of hard coded version constants there are some hard-coded version.lucene_xy constants used in various places. some of these are intentional and appropriate: in deprecated code, e.g. arabiclettertokenizer, deprecated in 3.1, uses version.lucene_31 to make behavior version-dependent (e.g. standardtokenizer and other analysis components) to test different behavior at different points in history (e.g. teststopfilter to test position increments) but should hard-coded constants be used elsewhere? for those that should remain, and need to be updated with each release, there should be an easy way to find them.",
        "label": 47
    },
    {
        "text": "edgengramtokenfilter stops on tokens smaller then minimum gram size  if a token is encountered in the stream that is shorter in length than the min gram size, the filter will stop processing the token stream. working up a unit test now, but may be a few days before i can provide it. wanted to get it in the system.",
        "label": 38
    },
    {
        "text": "speed up segementdocsenum by making it more friendly for jit optimizations since we moved the bulk reading into the codec ie. make all bulk reading codec private in lucene-3584 we have seen some performance regression on different cpus. i tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops jit friendly by moving out branches where i can, minimize member access in all loops, use final members where possible and specialize the two common cases with & without livedocs. i will attache a patch and my benchmark results in a minute.",
        "label": 46
    },
    {
        "text": "add fieldcache gettermbytes  to load term data as byte  with flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack. fieldcache now has getstrings and getstringindex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as string. this should be quite a bit more ram efficient too, for us ascii content since each character would then use 1 byte not 2.",
        "label": 33
    },
    {
        "text": "assertion derived class modifier from parent class silently breaks backward compatibility after upgrading to lucene 3.1+, i see this in my log: java.lang.assertionerror: tokenstream implementation classes or at least their incrementtoken() implementation must be final at org.apache.lucene.analysis.tokenstream.assertfinal(tokenstream.java:117) at org.apache.lucene.analysis.tokenstream.<init>(tokenstream.java:92) turns out i derived tokenstream and my class was not declared final. this silently breaks backward compatibility via reflection, scary... i think doing this sort of check is fine, but throwing an java.lang.assertionerror in this case is too stringent. this is a style check against lucene clients, a error log would be fine, but throwing an error is too much. see constructor implementation for: http://svn.apache.org/viewvc/lucene/dev/branches/branch_3x/lucene/src/java/org/apache/lucene/analysis/tokenstream.java?view=markup",
        "label": 53
    },
    {
        "text": "geo3d shape test failures my jenkins found a seed that reproduces for me:    [junit4] suite: org.apache.lucene.spatial.spatial4j.geo3dshapespheremodelrectrelationtest    [junit4]   1> laps: 929 cwidbd: 219,1,394,164,151    [junit4] failure 2.50s j4 | geo3dshapespheremodelrectrelationtest.testgeobboxrect <<<    [junit4]    > throwable #1: java.lang.assertionerror: did not find enough contains/within/intersection/disjoint/bounds cases in a reasonable number of random attempts. cwidbd: 3402(20),19(20),9010(20),1246(20),9677(20)  laps exceeded 23354    [junit4]    >  at __randomizedtesting.seedinfo.seed([c88eb616122cf9eb:ec2b1e2e361f87b5]:0)    [junit4]    >  at com.spatial4j.core.shape.rectintersectiontesthelper.testrelatewithrectangle(rectintersectiontesthelper.java:84)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3dshaperectrelationtestcase.testgeobboxrect(geo3dshaperectrelationtestcase.java:172)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   1> laps: 14208 cwidbd: 7385,20,3580,1086,2137    [junit4]   1> laps: 157 cwidbd: 20,1,60,23,53    [junit4] completed [5/24] on j4 in 4.80s, 6 tests, 1 failure <<< failures! [...]    [junit4] suite: org.apache.lucene.spatial.spatial4j.geo3dshapewgs84modelrectrelationtest    [junit4]   1> laps: 1326 cwidbd: 309,1,552,268,196    [junit4]   1> rect=geodegeneratehorizontalline: {planetmodel=planetmodel.wgs84, latitude=0.2792526803190927(16.0), leftlon=0.06981317007977318(4.0), rightlon=0.6283185307179586(36.0)}    [junit4] failure 3.05s j3 | geo3dshapewgs84modelrectrelationtest.testgeobboxrect <<<    [junit4]    > throwable #1: java.lang.assertionerror: did not find enough contains/within/intersection/disjoint/bounds cases in a reasonable number of random attempts. cwidbd: 3402(20),19(20),9010(20),1246(20),9677(20)  laps exceeded 23354    [junit4]    >  at __randomizedtesting.seedinfo.seed([c88eb616122cf9eb:ec2b1e2e361f87b5]:0)    [junit4]    >  at com.spatial4j.core.shape.rectintersectiontesthelper.testrelatewithrectangle(rectintersectiontesthelper.java:84)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3dshaperectrelationtestcase.testgeobboxrect(geo3dshaperectrelationtestcase.java:172)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   1> laps: 14188 cwidbd: 7402,20,3570,1083,2113    [junit4]   1> laps: 157 cwidbd: 20,1,60,23,53    [junit4] completed [7/24] on j3 in 5.20s, 7 tests, 1 failure <<< failures! oddly, these have no \"reproduce with\" lines - anybody know why? to reproduce, i just used the master seed for all the spatial module's tests: ant -dtests.seed=c88eb616122cf9eb test",
        "label": 10
    },
    {
        "text": "nullpointerexception during indexing in documentswriter threadstate fielddata addposition in my case during indexing sometimes appear documents with unusually large \"words\" - text-encoded images in fact. attempt to add document that contains field with such token produces java.lang.illegalargumentexception: java.lang.illegalargumentexception: term length 37944 exceeds max term length 16383 at org.apache.lucene.index.documentswriter$threadstate$fielddata.addposition(documentswriter.java:1492) at org.apache.lucene.index.documentswriter$threadstate$fielddata.invertfield(documentswriter.java:1321) at org.apache.lucene.index.documentswriter$threadstate$fielddata.processfield(documentswriter.java:1247) at org.apache.lucene.index.documentswriter$threadstate.processdocument(documentswriter.java:972) at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:2202) at org.apache.lucene.index.documentswriter.adddocument(documentswriter.java:2186) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1432) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1411) this is expected, exception is caught and ignored. the problem is that after this indexwriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with npe: java.lang.nullpointerexception at org.apache.lucene.index.documentswriter$threadstate$fielddata.addposition(documentswriter.java:1497) at org.apache.lucene.index.documentswriter$threadstate$fielddata.invertfield(documentswriter.java:1321) at org.apache.lucene.index.documentswriter$threadstate$fielddata.processfield(documentswriter.java:1247) at org.apache.lucene.index.documentswriter$threadstate.processdocument(documentswriter.java:972) at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:2202) at org.apache.lucene.index.documentswriter.adddocument(documentswriter.java:2186) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1432) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1411) this is 100% reproducible.",
        "label": 33
    },
    {
        "text": "arrayindexoutofboundsexception during indexing http://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing",
        "label": 33
    },
    {
        "text": "npe in queryrescorer when scorer is null while testing out the queryrescorer i was getting an npe on the scorer when using a termquery as the rescore query. looks like a termquery will return a null scorer if the term is not present in the index segment. caused by: java.lang.nullpointerexception [junit4] > at org.apache.lucene.search.queryrescorer.rescore(queryrescorer.java:89)",
        "label": 23
    },
    {
        "text": "writelinedoctask incorrectly normalizes fields writelinedoctask normalizes the body, title and date fields by replacing any \"\\t\" with a space. however, if any one of them contains newlines, linedocmaker will fail, since the first line read will include some of the text, however the second line, which it now expects to be a new document, will include other parts of the text. i don't know how we didn't hit it so far. maybe the wikipedia text doesn't have such lines, however when i ran over the trec collection i hit a lot of those. i will attach a patch shortly.",
        "label": 33
    },
    {
        "text": "all filter  delegating classes should be abstract i think it's confusing that filterleafreader (and it's filter* inner classes) are not abstract. by making them abstract, we clarify to users how to use them by virtue of them being abstract. it seems only a couple tests directly instantiate them. this applies to other filter* classes as well.",
        "label": 10
    },
    {
        "text": "highlighter has problems when you use standardanalyzer with lucene or simplier stopfilter with stopwordsposincr mode switched on this is a followup on lucene-1987: if you set in highligtertest the constant static final version test_version = version.lucene_24 to lucene_29 or lucene_current, the test testsimplequeryscorerphrasehighlighting fails. please note, that currently (before lucene-2002 is fixed), you must also set the queryparser to respect posincr.",
        "label": 29
    },
    {
        "text": "testgeo3dpoint testgeo3drelations test failure it reproduces:    [junit4]   1> doc=23 should not have matched but did    [junit4]   1>   point=[x=0.9945964169637284, y=0.017360743796216105, z=-0.11238509267616992]    [junit4]   1>   mappedpoint=[lat=-0.11250142194566046, lon=0.017453291479645996([x=0.9945964169928142, y=0.01736074400099093, z=-0.11238509283298634])]    [junit4]   1> doc=46 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=-2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=-2.915240855096914e-236, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=-2.9185025839666778e-236])]    [junit4]   1> doc=59 should not have matched but did    [junit4]   1>   point=[x=1.0008128994768453, y=0.017469252883873243, z=0.01747191431934325]    [junit4]   1>   mappedpoint=[lat=0.017453291479645996, lon=0.017453291479645996([x=1.0008128995370384, y=0.01746925310095654, z=0.01747191415779267])]    [junit4]   1> doc=125 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=7.235987507597244e-97, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=7.244083521109649e-97])]    [junit4]   1> doc=243 should not have matched but did    [junit4]   1>   point=[x=0.9975952873944153, y=0.08392021471535471, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=0.0, lon=0.0839249100415336([x=0.9975952873501377, y=0.08392021494259203, z=0.0])]    [junit4]   1> doc=342 should not have matched but did    [junit4]   1>   point=[x=0.9941995353524516, y=0.017353816325365812, z=0.11582176154966317]    [junit4]   1>   mappedpoint=[lat=0.11595722401420251, lon=0.017453291479645996([x=0.9941995354390136, y=0.017353816408113534, z=0.11582176139738691])]    [junit4]   1> doc=471 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=2.4457272005608357e-47, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=2.448463612203698e-47])]    [junit4]   1> doc=478 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=0.0, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=0.0])]    [junit4]   1> doc=507 should not have matched but did    [junit4]   1>   point=[x=1.0008128994768453, y=0.017469252883873243, z=0.01747191431934325]    [junit4]   1>   mappedpoint=[lat=0.017453291479645996, lon=0.017453291479645996([x=1.0008128995370384, y=0.01746925310095654, z=0.01747191415779267])]    [junit4]   1> doc=604 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=2.4457272005608357e-47, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=2.448463612203698e-47])]    [junit4]   1> doc=650 should not have matched but did    [junit4]   1>   point=[x=1.0008128994768453, y=0.017469252883873243, z=0.01747191431934325]    [junit4]   1>   mappedpoint=[lat=0.017453291479645996, lon=0.017453291479645996([x=1.0008128995370384, y=0.01746925310095654, z=0.01747191415779267])]    [junit4]   1> doc=743 should not have matched but did    [junit4]   1>   point=[x=1.0008128994768453, y=0.017469252883873243, z=0.01747191431934325]    [junit4]   1>   mappedpoint=[lat=0.017453291479645996, lon=0.017453291479645996([x=1.0008128995370384, y=0.01746925310095654, z=0.01747191415779267])]    [junit4]   1> doc=807 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=-2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=-1.2794127972350177e-169, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=-1.280844273351233e-169])]    [junit4]   1> doc=826 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=2.4457272005608357e-47, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=2.448463612203698e-47])]    [junit4]   1> doc=931 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=0.0, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=0.0])]    [junit4]   1> doc=943 should not have matched but did    [junit4]   1>   point=[x=0.9976073946182008, y=0.08377616574426938, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=1.2870639730949678e-163, lon=0.08378051468561859([x=0.9976073946394605, y=0.08377616594164751, z=1.2885040097598412e-163])]    [junit4]   1> doc=946 should not have matched but did    [junit4]   1>   point=[x=1.0008809877510743, y=0.01747044164905953, z=-0.013032532267229002]    [junit4]   1>   mappedpoint=[lat=-0.013018342205382104, lon=0.017453291479645996([x=1.000880987885291, y=0.017470441587425285, z=-0.01303253226005935])]    [junit4]   1> doc=982 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=0.0, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=0.0])]    [junit4]   1> doc=998 should not have matched but did    [junit4]   1>   point=[x=0.9984329789869703, y=0.032151205937634875, z=0.06563394006842149]    [junit4]   1>   mappedpoint=[lat=0.0656085955562445, lon=0.03219054308233861([x=0.998432978803685, y=0.03215120594571263, z=0.06563394025553593])]    [junit4]   1> doc=1005 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=2.4457272005608357e-47, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=2.448463612203698e-47])]    [junit4]   1> doc=1040 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=-2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=-5.063866803598018e-207, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=-5.069532531188606e-207])]    [junit4]   1> doc=1042 should not have matched but did    [junit4]   1>   point=[x=0.9945781929603313, y=0.11425050487122357, z=-2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=-4.3091314014443453e-38, lon=0.11437200407330356([x=0.9945781930019248, y=0.11425050470015728, z=-4.3139526903169686e-38])]    [junit4]   1> doc=1061 should not have matched but did    [junit4]   1>   point=[x=1.000966378852226, y=0.01747193203427544, z=2.3309121299774915e-10]    [junit4]   1>   mappedpoint=[lat=8.174609137513495e-34, lon=0.017453291479645996([x=1.0009663787601641, y=0.017471932090601616, z=8.183755331583957e-34])]    [junit4]   1> doc=1128 should not have matched but did    [junit4]   1>   point=[x=0.9904579202377298, y=0.017288506498395972, z=0.14419792173153445]    [junit4]   1>   mappedpoint=[lat=0.14454969584158828, lon=0.017453291479645996([x=0.9904579204054236, y=0.017288506278659403, z=0.14419792178229177])]    [junit4]   1> doc=1177 should not have matched but did    [junit4]   1>   point=[x=0.9943032637393302, y=0.11529377312381162, z=0.01747191431934325]    [junit4]   1>   mappedpoint=[lat=0.017453291479645996, lon=0.11543880242053338([x=0.9943032638905158, y=0.11529377303352693, z=0.017471914157792666])]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=8ed27ec85d584e01 -dtests.slow=true -dtests.locale=bg -dtests.timezone=africa/el_aaiun -dtests.asserts=true -dtests.file.encoding=utf8    [junit4] failure 1.37s | testgeo3dpoint.testgeo3drelations <<<    [junit4]    > throwable #1: java.lang.assertionerror: invalid hits for shape=geocompositemembershipshape: {[geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=1.5707963267948966, lon=-3.141592653589793([x=-6.109531986173988e-17, y=-7.482018791156346e-33, z=0.997762292022105])], [lat=1.0402618673239572, lon=-1.4892083512036773([x=0.04118030251401391, y=-0.5036145102457217, z=0.8613451288212502])], [lat=-1.0850383189690824, lon=2.4457272005608357e-47([x=0.46617432552507954, y=1.1401352281397884e-47, z=-0.8829869207594466])]], internaledges={2}}, geoconvexpolygon: {planetmodel=planetmodel.wgs84, points=[[lat=-0.6370451769779303, lon=2.5318373679431616([x=-0.658944502285364, y=0.4603087595301997, z=-0.5947795941526743])], [lat=1.5707963267948966, lon=-3.141592653589793([x=-6.109531986173988e-17, y=-7.482018791156346e-33, z=0.997762292022105])], [lat=-1.0850383189690824, lon=2.4457272005608357e-47([x=0.46617432552507954, y=1.1401352281397884e-47, z=-0.8829869207594466])], [lat=-0.5703530503197992, lon=-3.141592653589793([x=-0.8418255855200296, y=-1.0309390087474507e-16, z=-0.5400031327048586])]], internaledges={1}}]}    [junit4]    >  at __randomizedtesting.seedinfo.seed([8ed27ec85d584e01:3ead035cd215e09d]:0)    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:464)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=lucene62, sim=randomsimilarity(querynorm=true): {}, locale=bg, timezone=africa/el_aaiun    [junit4]   2> note: linux 4.2.0-42-generic amd64/oracle corporation 1.8.0_92 (64-bit)/cpus=8,threads=1,free=472890352,total=504889344    [junit4]   2> note: all tests run in this jvm: [testgeo3dpoint]    [junit4] completed [1/1 (1!)] in 1.60s, 1 test, 1 failure <<< failures!",
        "label": 25
    },
    {
        "text": "jenkins builds hang quite often in testindexwriterwiththreads testclosewiththreads last hung test run: https://builds.apache.org/job/lucene-solr-tests-only-trunk/10638/console [junit] \"main\" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000] [junit]    java.lang.thread.state: waiting (parking) [junit]  at sun.misc.unsafe.park(native method) [junit]  - parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.reentrantlock$nonfairsync) [junit]  at java.util.concurrent.locks.locksupport.park(locksupport.java:186) [junit]  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:838) [junit]  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:871) [junit]  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1201) [junit]  at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:214) [junit]  at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:290) [junit]  at org.apache.lucene.index.documentswriterflushcontrol.markforfullflush(documentswriterflushcontrol.java:403) [junit]  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:557) [junit]  - locked <0x0000000825d81998> (a org.apache.lucene.index.documentswriter) [junit]  at org.apache.lucene.index.indexwriter.preparecommit(indexwriter.java:2776) [junit]  - locked <0x0000000825d7d840> (a java.lang.object) [junit]  at org.apache.lucene.index.indexwriter.commitinternal(indexwriter.java:2904) [junit]  - locked <0x0000000825d7d830> (a java.lang.object) [junit]  at org.apache.lucene.index.indexwriter.closeinternal(indexwriter.java:1156) [junit]  at org.apache.lucene.index.indexwriter.close(indexwriter.java:1099) [junit]  at org.apache.lucene.index.testindexwriterwiththreads.testclosewiththreads(testindexwriterwiththreads.java:200) [junit]  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit]  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) [junit]  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) [junit]  at java.lang.reflect.method.invoke(method.java:616) [junit]  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44) [junit]  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) [junit]  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41) [junit]  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) [junit]  at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48) [junit]  at org.apache.lucene.util.lucenetestcase$2$1.evaluate(lucenetestcase.java:611) [junit]  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28) [junit]  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31) [junit]  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76) [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:148) [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:50) [junit]  at org.junit.runners.parentrunner$3.run(parentrunner.java:193) [junit]  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52) [junit]  at org.junit.runners.parentrunner.runchildren(parentrunner.java:191) [junit]  at org.junit.runners.parentrunner.access$000(parentrunner.java:42) [junit]  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184) [junit]  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28) [junit]  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31) [junit]  at org.junit.runners.parentrunner.run(parentrunner.java:236) [junit]  at junit.framework.junit4testadapter.run(junit4testadapter.java:39) [junit]  at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420) [junit]  at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911) [junit]  at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:743)",
        "label": 46
    },
    {
        "text": "testindexwriterexceptions random failure  aioobe in byteblockpool allocslice testindexwriterexceptions threw this today, and its reproducable",
        "label": 33
    },
    {
        "text": "add numdeleteddocs to indexreader add numdeleteddocs to indexreader. basically, the implementation is as simple as doing: public int numdeleteddocs() { return deleteddocs == null ? 0 : deleteddocs.count(); } in segmentreader. patch to follow to include in all indexreader extensions.",
        "label": 38
    },
    {
        "text": "index corruption when using ramdirectory  directory  constructor lucene-475 introduced a bug in creating ramdirectories for large indexes. it truncates the length of the file to an int, from its original long value. any files that are larger than an int are truncated. patch to fix is attached.",
        "label": 55
    },
    {
        "text": "collapse searcher searchable indexsearcher  remove contrib remote  merge pms into indexsearcher we've discussed cleaning up our *searcher stack for some time... i think we should try to do this before releasing 4.0. so i'm attaching an initial patch which: removes searcher, searchable, absorbing all their methods into indexsearcher removes contrib/remote removes multisearcher absorbs parallelmultisearcher into indexsearcher (ie you can now pass usethreads=true, or a custom es to the ctor) the patch is rough \u2013 i just ripped stuff out, did search/replace to indexsearcher, etc. eg nothing is directly testing using threads with indexsearcher, but before committing i think we should add a newsearcher to lucenetestcase, which randomly chooses whether the searcher uses threads, and cutover tests to use this instead of making their own indexsearcher. i think multisearcher has a useful purpose, but as it is today it's too low-level, eg it shouldn't be involved in rewriting queries: the query.combine method is scary. maybe in its place we make a higher level class, with limited api, that's able to federate search across multiple indexsearchers? it'd also be able to optionally use thread per indexsearcher.",
        "label": 33
    },
    {
        "text": "enforce read only access to any path outside the temporary folder via security manager the recent refactoring to all the create temp file/dir functions (which is great!) has a minor regression from what existed before. with the old lucenetestcase.temp_dir, the directory was created if it did not exist. so, if you set java.io.tmpdir to \"./temp\", then it would create that dir within the per jvm working dir. however, getbasetempdirforclass() now does asserts that check the dir exists, is a dir, and is writeable. lucene uses \".\" as java.io.tmpdir. then in the test security manager, the per jvm cwd has read/write/execute permissions. however, this allows tests to write to their cwd, which i'm trying to protect against (by setting cwd to read/execute in my test security manager).",
        "label": 11
    },
    {
        "text": "indexwriter hasuncommittedchanges  returns false if there are pending delete by term only if there are only delete by term and no document adds, then indexwriter.hasuncommittedchanges() returns false. http://lucene.472066.n3.nabble.com/solr-4-4-master-slave-configuration-replication-issue-with-commits-after-deleting-documents-using-ded-td4094158.html",
        "label": 33
    },
    {
        "text": "problem with indexwriter mergefinish i'm getting a (very) infrequent assert in indexwriter.mergefinish from testindexwriter.testaddindexondiskfull. the problem occurs during the rollback when the merge hasn't been registered. i'm not 100% sure this is the correct fix, because it's such an infrequent event.   final synchronized void mergefinish(mergepolicy.onemerge merge) throws ioexception {          // optimize, addindexes or finishmerges may be waiting     // on merges to finish.     notifyall();     if (merge.increfdone)       decrefmergesegments(merge);     assert merge.registerdone;     final segmentinfos sourcesegments = merge.segments;     final int end = sourcesegments.size();     for(int i=0;i<end;i++)       mergingsegments.remove(sourcesegments.info(i));     mergingsegments.remove(merge.info);     merge.registerdone = false;   } should be something like:   final synchronized void mergefinish(mergepolicy.onemerge merge) throws ioexception {          // optimize, addindexes or finishmerges may be waiting     // on merges to finish.     notifyall();     if (merge.increfdone)       decrefmergesegments(merge);     if (merge.registerdone) {       final segmentinfos sourcesegments = merge.segments;       final int end = sourcesegments.size();       for(int i=0;i<end;i++)         mergingsegments.remove(sourcesegments.info(i));       mergingsegments.remove(merge.info);       merge.registerdone = false;     }   }",
        "label": 33
    },
    {
        "text": "create searchertaxomanager if an application wants to use an indexsearcher and taxonomyreader in a searchermanager-like fashion, it cannot use a separate searchermanager, and say a taxonomyreadermanager, because the indexsearcher and taxoreader instances need to be in sync. that is, the is-tr pair must match, or otherwise the category ordinals that are encoded in the search index might not match the ones in the taxonomy index. this can happen if someone reopens the indexsearcher's indexreader, but does not refresh the taxonomyreader, and the category ordinals that exist in the reopened indexreader are not yet visible to the taxonomyreader instance. i'd like to create a searchertaxomanager (which is a referencemanager) which manages an indexsearcher and taxonomyreader pair. then an application will call: searchertaxopair pair = manager.acquire(); try {   indexsearcher searcher = pair.searcher;   taxonomyreader taxoreader = pair.taxoreader;   // do something with them } finally {   manager.release(pair);   pair = null; }",
        "label": 33
    },
    {
        "text": "pointrangequery and pointinsetquery  equals and  hashcode methods are buggy caught while cutting over spatial-extras from legacydoublefield to doublepoint field: pointrangequery.equals() and .hashcode fails with a false positive if the ranges are identical but field is different.",
        "label": 33
    },
    {
        "text": "add a testing implementation for documentswriterperthreadpool currently we only have one impl for documentswriterperthreadpool. we should add some more to make sure the interface is sufficient and to beef up tests. for testing i'm working on a randomized impl. selecting and locking states randomly.",
        "label": 46
    },
    {
        "text": "un deprecate queryparser and remove documentation that says it will be replaced in this looks like the consensus move at first blush. we can (of course) re-evaluate if things change.",
        "label": 33
    },
    {
        "text": "create indexwriterconfiguration and store all of iw configuration there i would like to factor out of all iw configuration parameters into a single configuration class, which i propose to name indexwriterconfiguration (or indexwriterconfig). i want to store there almost everything besides the directory, and to reduce all the ctors down to one: indexwriter(directory, indexwriterconfiguration). what i was thinking of storing there are the following parameters: all of ctors parameters, except for directory. the different setters where it makes sense. for example i still think infostream should be set on iw directly. i'm thinking that iwc should expose everything in a setter/getter methods, and defaults to whatever iw defaults today. except for analyzer which will need to be defined in the ctor of iwc and won't have a setter. i am not sure why maxfieldlength is required in all iw ctors, yet iw declares a default (which is an int and not maxfieldlength). do we still think that 10000 should be the default? why not default to unlimited and otherwise let the application decide what limited means for it? i would like to make mfl optional on iwc and default to something, and i hope that default will be unlimited. we can document that on iwc, so that if anyone chooses to move to the new api, he should be aware of that ... i plan to deprecate all the ctors and getters/setters and replace them by: one ctor as described above getindexwriterconfiguration, or simply getconfig, which can then be queried for the setting of interest. about the setters, i think maybe we can just introduce a setconfig method which will override everything that is overridable today, except for analyzer. so someone could do iw.getconfig().setsomething(); iw.setconfig(newconfig); the setters on iwc can return an iwc to allow chaining set calls ... so the above will turn into iw.setconfig(iw.getconfig().setsomething1().setsomething2()); btw, this is needed for parallel indexing (see lucene-1879), but i think it will greatly simplify iw's api. i'll start to work on a patch.",
        "label": 33
    },
    {
        "text": "drilldownquery not working with associatefacetfields  i'm trying to use the floatassociationfacetfield to store a float with each facet. retrieving, summing etc. works fine for matchalldocumentquery(). when i try to drilldown on one of the facets, the result is always empty. see attached example.",
        "label": 43
    },
    {
        "text": "add lucenetestcase newsearcher  most tests in the search package don't care about what kind of searcher they use. we should randomly use multisearcher or parallelmultisearcher sometimes in tests.",
        "label": 40
    },
    {
        "text": "all analysis consumers should use reusabletokenstream with analyzer now using tokenstreamcomponents, theres no reason for analysis consumers to use tokenstream() (it just gives bad performance). consequently all consumers will be moved over to using reusabletokenstream(). the only challenge here is that reusabletokenstream throws an ioexception which many consumers are not rigged to deal with. once all consumers have been moved, we can rename reusabletokenstream() back to tokenstream().",
        "label": 7
    },
    {
        "text": "sampling can break facetresult labeling when sampling facetresults, the topkineachnodehandler is used to get the facetresults. this is my case: a facetresult is returned (which matches a facetrequest) from the standardfacetaccumulator. the facet has 0 results. the labelling of the root-node seems incorrect. i know, from the standardfacetaccumulator, that the rootnode has a label, so i can use that one. currently the recursivelylabel method uses the taxonomyreader.getpath() to retrieve the label. i think we can skip that for the rootnode when there are no children (and gain a little performance on the way too?)",
        "label": 43
    },
    {
        "text": "fastvectorhighlighter truncates words at beginning and end of fragments fastvectorhighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated. this makes the highlights less legible than they should be. i will attach a patch to basefragmentbuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first. this significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size.",
        "label": 26
    },
    {
        "text": " char termattribute cloning memory consumption the memory consumption problem with cloning a (char)termattributeimpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z",
        "label": 53
    },
    {
        "text": "add getfinaloffset  to tokenstream if you add multiple fieldable instances for the same field name to a document, and you then index those fields with termvectors storing offsets, it's very likely the offsets for all but the first field instance will be wrong. this is because indexwriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endoffset of the last token it saw when analyzing that field. but this logic is overly simplistic. for example, if the whitespaceanalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field's offsets are then all 3 too small. similarly, if a stopfilter appears in the chain, and the last n tokens were stop words, then the base will be 1 + the endoffset of the last non-stopword token. to fix this, i'd like to add a new getfinaloffset() to tokenstream. i'm thinking by default it returns -1, which means \"i don't know so you figure it out\", meaning we fallback to the faulty logic we have today. this has come up several times on the user's list.",
        "label": 32
    },
    {
        "text": "don't call mergepolicy   indexwriter during dwpt flush we currently consult the indexwriter -> merge policy to decide if we need to write cfs or not which is bad in many ways. we should call mergepolicy only during merges we should never sync on iw during dwpt flush we should be able to make the decision if we need to write cfs or not before flush, ie. we could write parts of the flush directly to cfs or even start writing stored fields directly. in the nrt case it might make sense to write all flushes to cfs to minimize filedescriptors independent of the index size. i wonder if we can use a simple boolean for this in the iwc and get away with not consulting merge policy. this would simplify concurrency a lot here already.",
        "label": 46
    },
    {
        "text": "bogus javadocs for fieldvaluehitquery fillfields fieldvaluehitquery.fillfields has javadocs that seem to be left over from a completely different method...   /**    * given a fielddoc object, stores the values used to sort the given document.    * these values are not the raw values out of the index, but the internal    * representation of them. this is so the given search hit can be collated by    * a multisearcher with other search hits.    *     * @param doc    *          the fielddoc to store sort values into.    * @return the same fielddoc passed in.    * @see searchable#search(weight,filter,int,sort)    */   fielddoc fillfields(final entry entry) {     final int n = comparators.length;     final comparable[] fields = new comparable[n];     for (int i = 0; i < n; ++i) {       fields[i] = comparators[i].value(entry.slot);     }     //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores     return new fielddoc(entry.docid, entry.score, fields);   }",
        "label": 33
    },
    {
        "text": "rangequery and rangefilter should use collation to check for range inclusion see this java-user discussion of problems caused by unicode code-point comparison, instead of collation, in rangequery. rangequery could take in a locale via a setter, which could be used with a java.text.collator and/or collationkey's, to handle ranges for languages which have alphabet orderings different from those in unicode.",
        "label": 15
    },
    {
        "text": "intellij config  drop resource only modules  add module groups  and add module for lucene backward codecs the number of intellij modules is getting out of hand. intellij supports marking subdirectories within a module as source/resources/tests/test-resources. i think we should consolidate these modules so we have just one per lucene module. is there some reason i'm missing that this was not done in the first place?",
        "label": 47
    },
    {
        "text": "syns2index fails running syns2index fails with a java.lang.illegalargumentexception: maxbuffereddocs must at least be 2 when enabled exception. at org.apache.lucene.index.indexwriter.setmaxbuffereddocs(indexwriter.java:883) at org.apache.lucene.wordnet.syns2index.index(syns2index.java:249) at org.apache.lucene.wordnet.syns2index.main(syns2index.java:208) the code is here // blindly up these parameters for speed writer.setmergefactor( writer.getmergefactor() * 2); writer.setmaxbuffereddocs( writer.getmaxbuffereddocs() * 2); it looks like getmaxbuffereddocs used to return 10, and now it returns -1, not sure when that started happening. my suggestion would be to just remove these three lines. since speed has already improved vastly, there isn't a need to speed things up. to run this, syns2index requires two args. the first is the location of the wn_s.pl file, and the second is the directory to create the index in.",
        "label": 38
    },
    {
        "text": "explore using automaton for fuzzyquery we can optimize fuzzyquery by using automatontermsenum. the idea is to speed up the core fuzzyquery in similar fashion to wildcard and regex speedups, maintaining all backwards compatibility. the advantages are: we can seek to terms that are useful, instead of brute-forcing the entire terms dict we can determine matches faster, as true/false from a dfa is array lookup, don't even need to run levenshtein. we build levenshtein dfas in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 to implement support for 'prefix' length, we simply concatenate two dfas, which doesn't require us to do nfa->dfa conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy dfa, it only need examine its start state. with this algorithm, parametric tables are precomputed so that dfas can be constructed very quickly. if the required number of edits is too large (we don't have a table for it), we use \"dumb mode\" at first (no seeking, no dfa, just brute force like now). as the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. this is because terms in core fuzzyquery are sorted by boost value, then by term (in lexicographic order). for a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. this not only provides a mechanism to switch to more efficient dfas (edit distance of 2 -> edit distance of 1 -> edit distance of 0) during enumeration, but also to switch from \"dumb mode\" to \"smart mode\". with this design, we can add more dfas at any time by adding additional tables. the tradeoff is the tables get rather large, so for very high k, we would start to increase the size of lucene's jar file. the idea is we don't have include large tables for very high k, by using the 'competitive boost' attribute of the priority queue. for more information, see http://en.wikipedia.org/wiki/levenshtein_automaton",
        "label": 40
    },
    {
        "text": "don't require an analyzer  if all fields are not analyzed this seems wierd, if you analyze only not_analyzed fields, you must have an analyzer (null will not work) because documentsinverter wants it for things like offsetgap",
        "label": 43
    },
    {
        "text": "further steps towards flexible indexing i attached a very rough checkpoint of my current patch, to get early feedback. all tests pass, though back compat tests don't pass due to changes to package-private apis plus certain bugs in tests that happened to work (eg call termpostions.nextposition() too many times, which the new api asserts against). [aside: i think, when we commit changes to package-private apis such that back-compat tests don't pass, we could go back, make a branch on the back-compat tag, commit changes to the tests to use the new package private apis on that branch, then fix nightly build to use the tip of that branch?o] there's still plenty to do before this is committable! this is a rather large change: switches to a new more efficient terms dict format. this still uses tii/tis files, but the tii only stores term & long offset (not a terminfo). at seek points, tis encodes term & freq/prox offsets absolutely instead of with deltas delta. also, tis/tii are structured by field, so we don't have to record field number in every term. . on first 1 m docs of wikipedia, tii file is 36% smaller (0.99 mb -> 0.64 mb) and tis file is 9% smaller (75.5 mb -> 68.5 mb). . ram usage when loading terms dict index is significantly less since we only load an array of offsets and an array of string (no more terminfo array). it should be faster to init too. . this part is basically done. introduces modular reader codec that strongly decouples terms dict from docs/positions readers. eg there is no more terminfo used when reading the new format. . there's nice symmetry now between reading & writing in the codec chain \u2013 the current docs/prox format is captured in: formatpostingstermsdictwriter/reader formatpostingsdocswriter/reader (.frq file) and formatpostingspositionswriter/reader (.prx file). this part is basically done. introduces a new \"flex\" api for iterating through the fields, terms, docs and positions: fieldproducer -> termsenum -> docsenum -> postingsenum this replaces termenum/docs/positions. segmentreader emulates the old api on top of the new api to keep back-compat. next steps: plug in new codecs (pulsing, pfor) to exercise the modularity / fix any hidden assumptions. expose new api out of indexreader, deprecate old api but emulate old api on top of new one, switch all core/contrib users to the new api. maybe switch to attributesources as the base class for termsenum, docsenum, postingsenum \u2013 this would give readers api flexibility (not just index-file-format flexibility). eg if someone wanted to store payload at the term-doc level instead of term-doc-position level, you could just add a new attribute. test performance & iterate.",
        "label": 33
    },
    {
        "text": "make indexreader really read only in lucene as we change api completely in lucene 4.0 we are also free to remove read-write access and commits from indexreader. this code is so hairy and buggy (as investigated by robert and mike today) when you work on segmentreader level but forget to flush in the directoryreader, so its better to really make indexreaders readonly. currently with indexreader you can do things like: delete/undelete documents -> can be done by with indexwriter, too (using deletebyquery) change norms -> this is a bad idea in general, but when we remove norms at all and replace by docvalues this is obsolete already. changing docvalues should also be done using indexwriter in trunk (once it is ready)",
        "label": 53
    },
    {
        "text": "filters need hashcode  and equals  filters need to implement hashcode() and equals(), esp since certain query types can contain a filter (filteredquery, constantscorequery)",
        "label": 55
    },
    {
        "text": "hunspell stemmer generates multiple tokens the hunspell stemmer seems to be generating multiple tokens: the original token plus the available stems. it might be a good thing in some cases but it seems to be a different behaviour compared to the other stemmers and causes problems as well. i would rather have an option to decide whether it should output only the available stems, or the stems plus the original token. i'm not sure though if it's possible to have only a single stem indexed, which would be even better in my opinion. when i look at how snowball works only one token is indexed, the stem, and that works great. probably there's something i'm missing in how hunspell works. here is my issue: i have a query composed of multiple terms, which is analyzed using stemming and a boolean query is generated out of it. all fine when adding all clauses as should (or operator), but if i add all clauses as must (and operator), then i can get back only the documents that contain the stem originated by the exactly same original word. example for the dutch language i'm working with: fiets (means bicycle in dutch), its plural is fietsen. if i index \"fietsen\" i get both \"fietsen\" and \"fiets\" indexed, but if i index \"fiets\" i get the only \"fiets\" indexed. when i query for \"fietsen whatever\" i get the following boolean query: field:fiets field:fietsen field:whatever. if i apply the and operator and use must clauses for each subquery, then i can only find the documents that originally contained \"fietsen\", not the ones that originally contained \"fiets\", which is not really what stemming is about. any thoughts on this? i also wonder if it can be a dictionary issue since i see that different words that have the word \"fiets\" as root don't get the same stems, and using the and operator at query time is a big issue. i would love to contribute on this and looking forward to your feedback.",
        "label": 1
    },
    {
        "text": "give version parsing exceptions more descriptive error messages as discussed on the dev list, it's spooky how version.java tries to fully parse the incoming version string ... and then throw exceptions that lack details about what invalid value it received, which file contained the invalid value, etc. it also seems too low level to be checking versions (e.g. is not future proof for when 4.10 is passed a 5.x index by accident), and seems redundant with the codec headers we already have for checking versions? should we just go back to lenient parsing?",
        "label": 33
    },
    {
        "text": "customscorequery explain differs from the actual score when toplevelboost is used  customscorequery.java, doexplain has the following line: res.adddetail(new explanation(getboost(), \"queryboost\")); this multiplies the custom score query by just the boost of the current query, and not by queryweight=toplevelboost*getboost(); which is the value that's actually used during scoring. this leads to drastically different scores in the debug info, relative to the actual score, when the query is a subquery of another one, like a booleanquery clause, with a non-1 boost.",
        "label": 40
    },
    {
        "text": "pull change id related code out of addversion py  since it's no longer used lucene-6938 led to the remove of code that merges the downstream changes for addition of a new version. that seems like an accidental removal and we should add it back with a few changes so that it now uses git instead of svn.",
        "label": 47
    },
    {
        "text": "remove lucene core's functionquery impls as part of the consolidation of functionquerys, we want to remove lucene core's impls. included in this work, we will make sure that all the functionality provided by the core impls is also provided by the new module. any tests will be ported across too, to increase the test coverage.",
        "label": 7
    },
    {
        "text": "add new spatialfilter and distancefieldcomparatorsource to spatial the current distancequerybuilder and distancefieldcomparatorsource in spatial are based on the old filtering process, most of which has been deprecated in previous issues. these will be replaced by a new spatialfilter class, which is a proper lucene filter, and a new distancefieldcomparatorsource which will be relocated and will use the new distancefilter interface.",
        "label": 7
    },
    {
        "text": "improve javadocs for geocircle implementations hi karl wright, now that there are two different ways to construct circles, it would be good to update javadocs to hellp users decide which one they want to use. i am providing a patch with my unserstanding of the circles but please feel free to modify it. thanks!",
        "label": 25
    },
    {
        "text": "chararrayset behaves inconsistently in add object  and contains object  chararrayset's add(object) method looks like this: if (o instanceof char[]) { return add((char[])o); } else if (o instanceof string) { return add((string)o); } else if (o instanceof charsequence) { return add((charsequence)o); } else { return add(o.tostring()); } you'll notice that in the case of an object (for example, integer), the o.tostring() is added. however, its contains(object) method looks like this: if (o instanceof char[]) { char[] text = (char[])o; return contains(text, 0, text.length); } else if (o instanceof charsequence) { return contains((charsequence)o); } return false; in case of contains(integer), it always returns false. i've added a simple test to testchararrayset, which reproduces the problem: public void testobjectcontains() { chararrayset set = new chararrayset(10, true); integer val = new integer(1); set.add(val); asserttrue(set.contains(val)); asserttrue(set.contains(new integer(1))); } changing contains(object) to this, solves the problem: if (o instanceof char[]) { char[] text = (char[])o; return contains(text, 0, text.length); } return contains(o.tostring()); the patch also includes few minor improvements (which were discussed on the mailing list) such as the removal of the following dead code from gethashcode(charsequence): if (false && text instanceof string) { code = text.hashcode(); and simplifying add(object): if (o instanceof char[]) { return add((char[])o); } return add(o.tostring()); (which also aligns with the equivalent contains() method). one thing that's still left open is whether we can avoid the calls to character.tolowercase calls in all the char[] array methods, by first converting the char[] to lowercase, and then passing it through the equals() and gethashcode() methods. it works for add(), but fails for contains(char[]) since it modifies the input array.",
        "label": 33
    },
    {
        "text": "maven deployment scripts dont work  except from the machine you made the rc from  currently the maven process described in http://wiki.apache.org/lucene-java/publishmavenartifacts does not work (on mac) it worked fine for the 4.0-alpha and 4.0-beta releases. note: this appears to be working on linux so i am going with that. but this seems strange it doesnt work on mac. artifact:install-provider] installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-7:runtime [artifact:pom] downloading: org/apache/lucene/lucene-parent/4.0.0/lucene-parent-4.0.0.pom from repository sonatype.releases at http://oss.sonatype.org/content/repositories/releases [artifact:pom] unable to locate resource in repository [artifact:pom] [info] unable to find resource 'org.apache.lucene:lucene-parent:pom:4.0.0' in repository sonatype.releases (http://oss.sonatype.org/content/repositories/releases) [artifact:pom] downloading: org/apache/lucene/lucene-parent/4.0.0/lucene-parent-4.0.0.pom from repository central at http://repo1.maven.org/maven2 [artifact:pom] unable to locate resource in repository [artifact:pom] [info] unable to find resource 'org.apache.lucene:lucene-parent:pom:4.0.0' in repository central (http://repo1.maven.org/maven2) [artifact:pom] an error has occurred while processing the maven artifact tasks. [artifact:pom]  diagnosis: [artifact:pom]  [artifact:pom] unable to initialize pom lucene-test-framework-4.0.0.pom: cannot find parent: org.apache.lucene:lucene-parent for project: org.apache.lucene:lucene-test-framework:jar:null for project org.apache.lucene:lucene-test-framework:jar:null [artifact:pom] unable to download the artifact from any repository build failed",
        "label": 47
    },
    {
        "text": "another approach to exact circle hi karl wright, i was thinking in the limitation of convexity for circle planes and i worked out another approach that overcome this limitation. there is a condition, circle lanes must contain the center of the circle which is probably always true for planets like wgs84 which are not too far from the sphere. the idea is create a short of polygon for every slice using the center of the circle (like an orange) and therefore you can treat them separately. i attached what i developed, it seems to pass all tests regardless of the radius. let me know what you think and if i am missing something.",
        "label": 25
    },
    {
        "text": "cache docvalues direcsource currently the user need to make sure that a direct source is not shared between threads and each time someone calls getdirectsource we create a new source which has a reasonable overhead. we can certainly reduce the overhead (maybe different issue) but it should be easier for the user to get a direct source and handle it. more than that is should be consistent with getsource / loadsource.",
        "label": 46
    },
    {
        "text": "highlighter throws stringindexoutofboundsexception using the canonical solr example (ant run-example) i added this document (using exampledocs/post.sh): <add><doc> <field name=\"id\">test for highlighting stringindexoutofboundsexcdption</field> <field name=\"name\">some name</field> <field name=\"manu\">acme, inc.</field> <field name=\"features\">description of the features, mentioning various things</field> <field name=\"features\">features also is multivalued</field> <field name=\"popularity\">6</field> <field name=\"instock\">true</field> </doc></add> and then the url http://localhost:8983/solr/select/?q=features&hl=true&hl.fl=features caused the exception. i have a patch. i don't know if it is completely correct, but it avoids this exception.",
        "label": 28
    },
    {
        "text": "generate backwards compatibility indexes for all x releases currently the versioning here is a total mess, and its inconsistent across bugfix releases. we should just generate back compat indexes for every release: regardless of whether the index format changed, even for bugfix releases. this ensures at least we try to test that the back compat is working.",
        "label": 40
    },
    {
        "text": "add jacoco option for test coverage jacoco (http://www.jacoco.org/) is a much cleaner and simpler to use code coverage tool than clover and additionally doesn't require having a third party license since it is open source. it also has nice jenkins integration tools that make it incredibly easy to see what is and isn't tested. we should convert the lucene and solr builds to use jacoco instead of clover.",
        "label": 40
    },
    {
        "text": "geo3d public apis should match the 2d apis  i'm struggling to benchmark the equivalent to latlonpoint.newdistancequery in the geo3d world. ideally, i think we'd have a geo3dpoint.newdistancequery? and it would take degrees, not radians, and radiusmeters, not an angle? and if i index and search using planetmodel.sphere i think it should ideally give the same results as latlonpoint.newdistancequery, which uses haversin.",
        "label": 25
    },
    {
        "text": "the native fs lock used in test framework's o a l util lucenejunitresultformatter prohibits testing on a multi user system lucenejunitresultformatter uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console. the current implementation uses a fixed directory (lucene_junit_lock/ in java.io.tmpdir (by default /tmp/ on unix/linux systems) as the location of this lock. this functionality was introduced on solr-1835. as shawn heisey reported on solr-2739, some tests fail when run as root, but succeed when run as a non-root user. on #lucene irc today, shawn wrote: (2:06:07 pm) elyograg: now that i know i can't run the tests as root, i have discovered /tmp/lucene_junit_lock. once you run the tests as user a, you cannot run them again as user b until that directory is deleted, and only root or the original user can do so.",
        "label": 40
    },
    {
        "text": "change select geo3d classes to package private geo3d has led to a lot of consternation because it has a relatively open api. the task is to either drastically restrict it or remove the package entirely.",
        "label": 25
    },
    {
        "text": "move contrib snowball to contrib analyzers to fix bugs in some duplicate, handcoded impls of these stemmers (nl, fr, ru, etc) we should simply merge snowball and analyzers, and replace the buggy impls with the proper snowball stemfilters.",
        "label": 40
    },
    {
        "text": "'ant javacc' in root project should also properly create contrib queryparser java files 'ant javacc' in the project root doesn't run javacc in contrib/queryparser 'ant javacc' in contrib/queryparser does not properly create the java files. what still needs to be done by hand is (partly!) described in contrib/queryparser/readme.javacc. i think this process should be automated. patch provided.",
        "label": 32
    },
    {
        "text": "run smoketester on java in the past, when we were on java 6, we ran the smoketester on java 6 and java 7. as java 8 is now officially released and supported, smoketester should now use and require java8_home. for the nightly-smoke tests i have to install the openjdk8 freebsd package, but that should not be a problem.",
        "label": 41
    },
    {
        "text": "document package level javadocs need improving the document package package level javadocs could use some improving, such as: 1. info on what a document is, as well as field and fieldable 2. examples of fieldselectors and how to implement 3. samples of using datetools and numbertools",
        "label": 15
    },
    {
        "text": "levenshteindistance code normalization is incorrect the normalization of the edit distance should use the maximum of the two string being compared instead of the minimum. otherwise negative distances are possible. the spell checker filters out edits below a certain threshold so this hasn't been a problem in practice.",
        "label": 33
    },
    {
        "text": "instead of doccount  maxdoc  is used for numberofdocuments in similaritybase similaritybase.java has the following line :  long numberofdocuments = collectionstats.maxdoc(); it seems like collectionstats.doccount(), which returns the total number of documents that have at least one term for this field, is more appropriate statistics here.",
        "label": 40
    },
    {
        "text": "confusing javadoc in searchable java in searchable.java, the javadoc for maxdoc() is: /** expert: returns one greater than the largest possible document number. called by search code to compute term weights. @see org.apache.lucene.index.indexreader#maxdoc() the qualification \"expert\" and the statement \"called by search code to compute term weights\" is a bit confusing, it implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). maybe it is used as one factor of the weight, but do we really need to mention this here?",
        "label": 29
    },
    {
        "text": " patch  indexwriter maybemergesegments  takes lots of cpu resources note: i believe this to be the same situation with 1.4.3 as with svn head. analysis using hprof utility shows that during index creation with many documents highlights that the cpu spends a large portion of it's time in indexwriter.maybemergesegments(), which seems to be a 'waste' compared with other valuable cpu intensive operations such as tokenization etc. using the following test snippet to retrieve some rows from the db and create an index: analyzer a = new standardanalyzer(); writer = new indexwriter(indexdir, a, true); writer.setmergefactor(1000); writer.setmaxbuffereddocs(10000); writer.setusecompoundfile(false); connection = drivermanager.getconnection( \"jdbc:inetdae7:tower.aconex.com?database=<somedb>\", \"secret\", \"squirrel\"); string sql = \"select userid, userfirstname, userlastname, email from userx\"; log.info(\"sql=\" + sql); statement statement = connection.createstatement(); statement.setfetchsize(5000); log.info(\"executing sql\"); resultset rs = statement.executequery(sql); log.info(\"resultset retrieved\"); int row = 0; log.info(\"indexing users\"); long begin = system.currenttimemillis(); while (rs.next()) { int userid = rs.getint(1); string firstname = rs.getstring(2); string lastname = rs.getstring(3); string email = rs.getstring(4); string fullname = firstname + \" \" + lastname; document doc = new document(); doc.add(field.keyword(\"userid\", userid+\"\")); doc.add(field.keyword(\"firstname\", firstname.tolowercase())); doc.add(field.keyword(\"lastname\", lastname.tolowercase())); doc.add(field.text(\"name\", fullname.tolowercase())); doc.add(field.keyword(\"email\", email.tolowercase())); writer.adddocument(doc); row++; if((row % 100)==0) { log.info(row + \" indexed\"); } } double end = system.currenttimemillis(); double diff = (end-begin)/1000; double rate = row/diff; log.info(\"rate:\" +rate); on my 1.5ghz powerbook with 1.5gb ram and a 5400 rpm drive, my cpu is maxed out, and i end up getting a rate of indexing between 490-515 documents/second run over 10 times in succession. by applying a simple patch to indexwriter (see attached shortly), which defers the calling of maybemergesegments() so that it is only called every 2000 times(an arbitrary figure), i appear to get a new rate of between 945-970 documents/second. using luke to look inside each index created between these 2 there does not appear to be any difference. same number of documents, same number of terms. i'm not suggesting one should apply this patch, i'm just highlighting the difference in performance that this sort of change gives you. we are about to use lucene to index 4 million construction document records, and so speeding up the indexing process is in our best interest! if one considers the amount of cpu time spent in maybemergesegments over the initial index creation of 4 million documents, i think one could see how it would be ideal to try to speed this area up (at least move the bottleneck to io). i woul appreciate anyone taking a moment to comment on this.",
        "label": 55
    },
    {
        "text": "fieldcachesource  or it's subclasses  should override getsortfield valuesource defines the following method...   public sortfield getsortfield(boolean reverse) {     return new valuesourcesortfield(reverse);   } ...where valuesourcesortfield builds up a valuesourcecomparator containing a double[] based on the functionvalues of the original valuesource. meanwhile, the abstract fieldcachesource exists as a base implementation for classes like intfieldsource and doublefieldsource which wrap a valuesource around docvalues for the specified field. but neither fieldcachesource nor any of it's subclasses override the getsortfield(boolean) method \u2013 so attempting to sort on something like an intfieldsource winds up using a bunch of ram to build that double[] to give users a less accurate sort (because of casting) then if they just sorted directly on the field. is there any good reason why fieldcachesource subclases like intfieldsource shouldn't all override getsortfield with something like...   public sortfield getsortfield(boolean reverse) {     return new sortfield(field, type.int, reverse);   } ?",
        "label": 18
    },
    {
        "text": "move contrib analyzers to modules analysis this is a patch to move contrib/analyzers under modules/analyzers we can then continue consolidating (lucene-2413)... in truth this will sorta be an ongoing thing anyway, as we try to distance indexing from analysis, etc",
        "label": 40
    },
    {
        "text": "test3dpointfield and testbkdtree failures   bkd file can't be deleted my jenkins found seeds for testgeo3dpointfield and testbkdtree tests that cause them to fail reliably, because a .bkd file can't be deleted because \"a virus scanner has it open\".    [junit4] suite: org.apache.lucene.bkdtree3d.testgeo3dpointfield [...]     [junit4]   2> thg 10 19, 2015 9:18:51 ch com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[lucene merge thread #0,5,tgrp-testgeo3dpointfield]    [junit4]   2> org.apache.lucene.index.mergepolicy$mergeexception: java.io.ioexception: cannot delete file: _41767748444.sort, a virus scanner has it open    [junit4]   2>        at __randomizedtesting.seedinfo.seed([61ed8059bbf9cf1d]:0)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]   2> caused by: java.io.ioexception: cannot delete file: _41767748444.sort, a virus scanner has it open    [junit4]   2>        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:523)    [junit4]   2>        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:471)    [junit4]   2>        at org.apache.lucene.store.lockvalidatingdirectorywrapper.deletefile(lockvalidatingdirectorywrapper.java:38)    [junit4]   2>        at org.apache.lucene.store.filterdirectory.deletefile(filterdirectory.java:62)    [junit4]   2>        at org.apache.lucene.store.trackingdirectorywrapper.deletefile(trackingdirectorywrapper.java:37)    [junit4]   2>        at org.apache.lucene.bkdtree3d.bkd3dtreewriter.sort(bkd3dtreewriter.java:344)    [junit4]   2>        at org.apache.lucene.bkdtree3d.bkd3dtreewriter.finish(bkd3dtreewriter.java:398)    [junit4]   2>        at org.apache.lucene.bkdtree3d.geo3ddocvaluesconsumer.addbinaryfield(geo3ddocvaluesconsumer.java:131)    [junit4]   2>        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addbinaryfield(perfielddocvaluesformat.java:116)    [junit4]   2>        at org.apache.lucene.codecs.docvaluesconsumer.mergebinaryfield(docvaluesconsumer.java:333)    [junit4]   2>        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:185)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:150)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:105)    [junit4]   2>        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4054)    [junit4]   2>        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3634)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626) [...]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpointfield -dtests.method=testrandombig -dtests.seed=61ed8059bbf9cf1d -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=vi_vn -dtests.timezone=us/arizona -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   20.6s | testgeo3dpointfield.testrandombig <<<    [junit4]    > throwable #1: java.io.ioexception: background merge hit exception: _0(6.0.0):c262144/2626:delgen=1 _1(6.0.0):c262144/773:delgen=1 _2(6.0.0):c262144/492:delgen=1 _3(6.0.0):c142742/106:delgen=1 into _4 [maxnumsegments=1]    [junit4]    >        at __randomizedtesting.seedinfo.seed([61ed8059bbf9cf1d:e6bafdd62aa0b39d]:0)    [junit4]    >        at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1765)    [junit4]    >        at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1705)    [junit4]    >        at org.apache.lucene.bkdtree3d.testgeo3dpointfield.verify(testgeo3dpointfield.java:951)    [junit4]    >        at org.apache.lucene.bkdtree3d.testgeo3dpointfield.dotestrandom(testgeo3dpointfield.java:793)    [junit4]    >        at org.apache.lucene.bkdtree3d.testgeo3dpointfield.testrandombig(testgeo3dpointfield.java:725)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.io.ioexception: cannot delete file: _41767748444.sort, a virus scanner has it open    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:523)    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:471)    [junit4]    >        at org.apache.lucene.store.lockvalidatingdirectorywrapper.deletefile(lockvalidatingdirectorywrapper.java:38)    [junit4]    >        at org.apache.lucene.store.filterdirectory.deletefile(filterdirectory.java:62)    [junit4]    >        at org.apache.lucene.store.trackingdirectorywrapper.deletefile(trackingdirectorywrapper.java:37)    [junit4]    >        at org.apache.lucene.bkdtree3d.bkd3dtreewriter.sort(bkd3dtreewriter.java:344)    [junit4]    >        at org.apache.lucene.bkdtree3d.bkd3dtreewriter.finish(bkd3dtreewriter.java:398)    [junit4]    >        at org.apache.lucene.bkdtree3d.geo3ddocvaluesconsumer.addbinaryfield(geo3ddocvaluesconsumer.java:131)    [junit4]    >        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addbinaryfield(perfielddocvaluesformat.java:116)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.mergebinaryfield(docvaluesconsumer.java:333)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:185)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:150)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:105)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4054)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3634)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpointfield -dtests.method=testrandomtiny -dtests.seed=61ed8059bbf9cf1d -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=vi_vn -dtests.timezone=us/arizona -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   0.53s | testgeo3dpointfield.testrandomtiny <<<    [junit4]    > throwable #1: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=22, name=lucene merge thread #0, state=runnable, group=tgrp-testgeo3dpointfield]    [junit4]    >        at __randomizedtesting.seedinfo.seed([61ed8059bbf9cf1d:28aa5e1fe5d8f7b1]:0)    [junit4]    > caused by: org.apache.lucene.index.mergepolicy$mergeexception: java.io.ioexception: cannot delete file: _41767748444.sort, a virus scanner has it open    [junit4]    >        at __randomizedtesting.seedinfo.seed([61ed8059bbf9cf1d]:0)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]    > caused by: java.io.ioexception: cannot delete file: _41767748444.sort, a virus scanner has it open    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:523)    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:471)    [junit4]    >        at org.apache.lucene.store.lockvalidatingdirectorywrapper.deletefile(lockvalidatingdirectorywrapper.java:38)    [junit4]    >        at org.apache.lucene.store.filterdirectory.deletefile(filterdirectory.java:62)    [junit4]    >        at org.apache.lucene.store.trackingdirectorywrapper.deletefile(trackingdirectorywrapper.java:37)    [junit4]    >        at org.apache.lucene.bkdtree3d.bkd3dtreewriter.sort(bkd3dtreewriter.java:344)    [junit4]    >        at org.apache.lucene.bkdtree3d.bkd3dtreewriter.finish(bkd3dtreewriter.java:398)    [junit4]    >        at org.apache.lucene.bkdtree3d.geo3ddocvaluesconsumer.addbinaryfield(geo3ddocvaluesconsumer.java:131)    [junit4]    >        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addbinaryfield(perfielddocvaluesformat.java:116)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.mergebinaryfield(docvaluesconsumer.java:333)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:185)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:150)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:105)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4054)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3634)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626) [...]    [junit4]   2> note: leaving temporary files on disk at: /home/sarowe/svn/lucene/dev/trunk/lucene/build/spatial3d/test/j0/temp/lucene.bkdtree3d.testgeo3dpointfield_61ed8059bbf9cf1d-002    [junit4]   2> note: test params are: codec=fastcompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fast, chunksize=6799, maxdocsperchunk=464, blocksize=8), termvectorsformat=compressingtermvectorsformat(compressionmode=fast, chunksize=6799, blocksize=8)), sim=randomsimilarityprovider(querynorm=false,coord=no): {}, locale=vi_vn, timezone=us/arizona    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=279335424,total=536346624    [junit4]   2> note: all tests run in this jvm: [testgeo3dpointfield]    [junit4] completed [1/1] in 51.66s, 8 tests, 2 errors <<< failures!    [junit4] suite: org.apache.lucene.bkdtree.testbkdtree [...]    [junit4]   2> oct 20, 2015 5:35:59 am com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[lucene merge thread #0,5,tgrp-testbkdtree]    [junit4]   2> org.apache.lucene.index.mergepolicy$mergeexception: java.io.ioexception: cannot delete file: _5659690406.bkd, a virus scanner has it open    [junit4]   2>        at __randomizedtesting.seedinfo.seed([27b8b43f33235531]:0)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]   2> caused by: java.io.ioexception: cannot delete file: _5659690406.bkd, a virus scanner has it open    [junit4]   2>        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:523)    [junit4]   2>        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:471)    [junit4]   2>        at org.apache.lucene.store.lockvalidatingdirectorywrapper.deletefile(lockvalidatingdirectorywrapper.java:38)    [junit4]   2>        at org.apache.lucene.store.filterdirectory.deletefile(filterdirectory.java:62)    [junit4]   2>        at org.apache.lucene.store.trackingdirectorywrapper.deletefile(trackingdirectorywrapper.java:37)    [junit4]   2>        at org.apache.lucene.bkdtree.offlinelatlonwriter.destroy(offlinelatlonwriter.java:69)    [junit4]   2>        at org.apache.lucene.bkdtree.bkdtreewriter.finish(bkdtreewriter.java:416)    [junit4]   2>        at org.apache.lucene.bkdtree.bkdtreedocvaluesconsumer.addsortednumericfield(bkdtreedocvaluesconsumer.java:114)    [junit4]   2>        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addsortednumericfield(perfielddocvaluesformat.java:126)    [junit4]   2>        at org.apache.lucene.codecs.docvaluesconsumer.mergesortednumericfield(docvaluesconsumer.java:417)    [junit4]   2>        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:236)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:150)    [junit4]   2>        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:105)    [junit4]   2>        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4054)    [junit4]   2>        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3634)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]   2>        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2>     [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testbkdtree -dtests.method=testrandombig -dtests.seed=27b8b43f33235531 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=en_gb -dtests.timezone=africa/libreville -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   13.6s | testbkdtree.testrandombig <<<    [junit4]    > throwable #1: java.io.ioexception: background merge hit exception: _0(6.0.0):c262144/2577:delgen=1 _1(6.0.0):c262144/803:delgen=1 _2(6.0.0):c262144/520:delgen=1 _3(6.0.0):c262144/372:delgen=1 _4(6.0.0):c119822/63:delgen=1 into _5 [maxnumsegments=1]    [junit4]    >        at __randomizedtesting.seedinfo.seed([27b8b43f33235531:a0efc9b0a27a29b1]:0)    [junit4]    >        at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1765)    [junit4]    >        at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1705)    [junit4]    >        at org.apache.lucene.bkdtree.testbkdtree.verify(testbkdtree.java:405)    [junit4]    >        at org.apache.lucene.bkdtree.testbkdtree.dotestrandom(testbkdtree.java:354)    [junit4]    >        at org.apache.lucene.bkdtree.testbkdtree.testrandombig(testbkdtree.java:285)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.io.ioexception: cannot delete file: _5659690406.bkd, a virus scanner has it open    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:523)    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:471)    [junit4]    >        at org.apache.lucene.store.lockvalidatingdirectorywrapper.deletefile(lockvalidatingdirectorywrapper.java:38)    [junit4]    >        at org.apache.lucene.store.filterdirectory.deletefile(filterdirectory.java:62)    [junit4]    >        at org.apache.lucene.store.trackingdirectorywrapper.deletefile(trackingdirectorywrapper.java:37)    [junit4]    >        at org.apache.lucene.bkdtree.offlinelatlonwriter.destroy(offlinelatlonwriter.java:69)    [junit4]    >        at org.apache.lucene.bkdtree.bkdtreewriter.finish(bkdtreewriter.java:416)    [junit4]    >        at org.apache.lucene.bkdtree.bkdtreedocvaluesconsumer.addsortednumericfield(bkdtreedocvaluesconsumer.java:114)    [junit4]    >        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addsortednumericfield(perfielddocvaluesformat.java:126)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.mergesortednumericfield(docvaluesconsumer.java:417)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:236)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:150)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:105)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4054)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3634)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testbkdtree -dtests.method=testencodedecode -dtests.seed=27b8b43f33235531 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=en_gb -dtests.timezone=africa/libreville -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   0.03s | testbkdtree.testencodedecode <<<    [junit4]    > throwable #1: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=32, name=lucene merge thread #0, state=runnable, group=tgrp-testbkdtree]    [junit4]    >        at __randomizedtesting.seedinfo.seed([27b8b43f33235531:dac95f43f2be6292]:0)    [junit4]    > caused by: org.apache.lucene.index.mergepolicy$mergeexception: java.io.ioexception: cannot delete file: _5659690406.bkd, a virus scanner has it open    [junit4]    >        at __randomizedtesting.seedinfo.seed([27b8b43f33235531]:0)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:668)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:648)    [junit4]    > caused by: java.io.ioexception: cannot delete file: _5659690406.bkd, a virus scanner has it open    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:523)    [junit4]    >        at org.apache.lucene.store.mockdirectorywrapper.deletefile(mockdirectorywrapper.java:471)    [junit4]    >        at org.apache.lucene.store.lockvalidatingdirectorywrapper.deletefile(lockvalidatingdirectorywrapper.java:38)    [junit4]    >        at org.apache.lucene.store.filterdirectory.deletefile(filterdirectory.java:62)    [junit4]    >        at org.apache.lucene.store.trackingdirectorywrapper.deletefile(trackingdirectorywrapper.java:37)    [junit4]    >        at org.apache.lucene.bkdtree.offlinelatlonwriter.destroy(offlinelatlonwriter.java:69)    [junit4]    >        at org.apache.lucene.bkdtree.bkdtreewriter.finish(bkdtreewriter.java:416)    [junit4]    >        at org.apache.lucene.bkdtree.bkdtreedocvaluesconsumer.addsortednumericfield(bkdtreedocvaluesconsumer.java:114)    [junit4]    >        at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldswriter.addsortednumericfield(perfielddocvaluesformat.java:126)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.mergesortednumericfield(docvaluesconsumer.java:417)    [junit4]    >        at org.apache.lucene.codecs.docvaluesconsumer.merge(docvaluesconsumer.java:236)    [junit4]    >        at org.apache.lucene.index.segmentmerger.mergedocvalues(segmentmerger.java:150)    [junit4]    >        at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:105)    [junit4]    >        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4054)    [junit4]    >        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3634)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >        at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626) [...]    [junit4]   2> note: leaving temporary files on disk at: /home/sarowe/svn/lucene/dev/trunk/lucene/build/sandbox/test/j0/temp/lucene.bkdtree.testbkdtree_27b8b43f33235531-001    [junit4]   2> note: test params are: codec=asserting(lucene53): {}, docvalues:{}, sim=randomsimilarityprovider(querynorm=true,coord=crazy): {}, locale=en_gb, timezone=africa/libreville    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=226608384,total=508559360    [junit4]   2> note: all tests run in this jvm: [testbkdtree]    [junit4] completed [1/1] in 31.63s, 10 tests, 2 errors <<< failures!",
        "label": 33
    },
    {
        "text": "add one setter for start and end offset to offsetattribute add offsetattribute. setoffset(startoffset, endoffset); trivial change, no junit needed changed chartokenizer to use it",
        "label": 33
    },
    {
        "text": "improve smoketester to work on windows after the changes in solr-3331, the smoketester won't work on windows (things like path separators of : or . not really critical, people will just have to smoketest on unix-like machines. but it would be more convenient for testers on windows machines if it worked there too.",
        "label": 47
    },
    {
        "text": "testindexwriter testthreadinterruptdeadlock fails with oom selckin reported a repeatedly failing test that throws oom exceptions. according to the heapdump the mockdirectorywrapper#createdfiles hashset takes about 400mb heapspace containing 4194304 entries. seems kind of way too many though  [junit] java.lang.outofmemoryerror: java heap space     [junit] dumping heap to /tmp/java_pid25990.hprof ...     [junit] heap dump file created [520807744 bytes in 4.250 secs]     [junit] testsuite: org.apache.lucene.index.testindexwriter     [junit] testcase: testthreadinterruptdeadlock(org.apache.lucene.index.testindexwriter): failed     [junit]      [junit] junit.framework.assertionfailederror:      [junit]  at org.apache.lucene.index.testindexwriter.testthreadinterruptdeadlock(testindexwriter.java:2249)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1282)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1211)     [junit]      [junit]      [junit] testcase: testthreadinterruptdeadlock(org.apache.lucene.index.testindexwriter): failed     [junit] some threads threw uncaught exceptions!     [junit] junit.framework.assertionfailederror: some threads threw uncaught exceptions!     [junit]  at org.apache.lucene.util.lucenetestcase.teardown(lucenetestcase.java:557)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1282)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1211)     [junit]      [junit]      [junit] tests run: 67, failures: 2, errors: 0, time elapsed: 3,254.884 sec     [junit]      [junit] ------------- standard output ---------------     [junit] failed; unexpected exception     [junit] java.lang.outofmemoryerror: java heap space     [junit]  at org.apache.lucene.store.ramfile.newbuffer(ramfile.java:85)     [junit]  at org.apache.lucene.store.ramfile.addbuffer(ramfile.java:58)     [junit]  at org.apache.lucene.store.ramoutputstream.switchcurrentbuffer(ramoutputstream.java:132)     [junit]  at org.apache.lucene.store.ramoutputstream.copybytes(ramoutputstream.java:171)     [junit]  at org.apache.lucene.store.mockindexoutputwrapper.copybytes(mockindexoutputwrapper.java:155)     [junit]  at org.apache.lucene.index.compoundfilewriter.copyfile(compoundfilewriter.java:223)     [junit]  at org.apache.lucene.index.compoundfilewriter.close(compoundfilewriter.java:189)     [junit]  at org.apache.lucene.index.segmentmerger.createcompoundfile(segmentmerger.java:138)     [junit]  at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:3344)     [junit]  at org.apache.lucene.index.indexwriter.merge(indexwriter.java:2959)     [junit]  at org.apache.lucene.index.serialmergescheduler.merge(serialmergescheduler.java:37)     [junit]  at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1763)     [junit]  at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1758)     [junit]  at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1754)     [junit]  at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1373)     [junit]  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1230)     [junit]  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1211)     [junit]  at org.apache.lucene.index.testindexwriter$indexerthreadinterrupt.run(testindexwriter.java:2154)     [junit] ------------- ---------------- ---------------     [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testindexwriter -dtestmethod=testthreadinterruptdeadlock -dtests.seed=7183538093651149:3431510331342554160     [junit] note: reproduce with: ant test -dtestcase=testindexwriter -dtestmethod=testthreadinterruptdeadlock -dtests.seed=7183538093651149:3431510331342554160     [junit] the following exceptions were thrown by threads:     [junit] *** thread: thread-379 ***     [junit] java.lang.runtimeexception: mockdirectorywrapper: cannot close: there are still open files: {_3r1n_0.tib=1, _3r1n_0.frq=1, _3r1n_0.pos=1, _3r1m.cfs=1, _3r1n_0.doc=1, _3r1n.tvf=1, _3r1n.tvd=1, _3r1n.tvx=1, _3r1n.fdx=1, _3r1n.fdt=1, _3r1q.cfs=1, _3r1o.cfs=1, _3r1n_0.skp=1, _3r1n_0.pyl=1}     [junit]  at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:448)     [junit]  at org.apache.lucene.index.testindexwriter$indexerthreadinterrupt.run(testindexwriter.java:2217)     [junit] caused by: java.lang.runtimeexception: unclosed indexoutput     [junit]  at org.apache.lucene.store.mockdirectorywrapper.createoutput(mockdirectorywrapper.java:367)     [junit]  at org.apache.lucene.index.fieldinfos.write(fieldinfos.java:563)     [junit]  at org.apache.lucene.index.docfieldprocessor.flush(docfieldprocessor.java:82)     [junit]  at org.apache.lucene.index.documentswriterperthread.flush(documentswriterperthread.java:381)     [junit]  at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:378)     [junit]  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:505)     [junit]  at org.apache.lucene.index.indexwriter.doflush(indexwriter.java:2621)     [junit]  at org.apache.lucene.index.indexwriter.flush(indexwriter.java:2598)     [junit]  at org.apache.lucene.index.indexwriter.preparecommit(indexwriter.java:2464)     [junit]  at org.apache.lucene.index.indexwriter.commitinternal(indexwriter.java:2537)     [junit]  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2519)     [junit]  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2503)     [junit]  at org.apache.lucene.index.testindexwriter$indexerthreadinterrupt.run(testindexwriter.java:2156)     [junit] note: test params are: codec=randomcodecprovider: {=mockvariableintblock(baseblocksize=105), f6=mockfixedintblock(blocksize=1372), f7=pulsing(freqcutoff=11), f8=mockrandom, f9=mockvariableintblock(baseblocksize=105), f1=mocksep, f0=pulsing(freqcutoff=11), f3=pulsing(freqcutoff=11), f2=mockfixedintblock(blocksize=1372), f5=mockvariableintblock(baseblocksize=105), f4=mockrandom, f=standard, c=pulsing(freqcutoff=11), termvector=mockfixedintblock(blocksize=1372), d9=mockvariableintblock(baseblocksize=105), d8=mockrandom, d5=mocksep, d4=pulsing(freqcutoff=11), d7=mockfixedintblock(blocksize=1372), d6=mockvariableintblock(baseblocksize=105), d25=simpletext, d0=pulsing(freqcutoff=11), c29=simpletext, d24=mocksep, d1=mocksep, c28=mockvariableintblock(baseblocksize=105), d23=mockrandom, d2=mockvariableintblock(baseblocksize=105), c27=mockrandom, d22=standard, d3=mockfixedintblock(blocksize=1372), d21=mockvariableintblock(baseblocksize=105), d20=mockrandom, c22=simpletext, c21=mocksep, c20=mockrandom, d29=mockfixedintblock(blocksize=1372), c26=mockfixedintblock(blocksize=1372), d28=mockvariableintblock(baseblocksize=105), c25=mockvariableintblock(baseblocksize=105), d27=mocksep, c24=mocksep, d26=pulsing(freqcutoff=11), c23=pulsing(freqcutoff=11), e9=mocksep, e8=standard, e7=simpletext, e6=mockvariableintblock(baseblocksize=105), e5=mockrandom, c17=mocksep, e3=simpletext, d12=pulsing(freqcutoff=11), c16=pulsing(freqcutoff=11), e4=standard, d11=mockfixedintblock(blocksize=1372), c19=mockfixedintblock(blocksize=1372), e1=mockrandom, d14=mockvariableintblock(baseblocksize=105), c18=mockvariableintblock(baseblocksize=105), e2=mockvariableintblock(baseblocksize=105), d13=mockrandom, e0=mockfixedintblock(blocksize=1372), d10=mocksep, d19=pulsing(freqcutoff=11), c11=mockvariableintblock(baseblocksize=105), c10=mockrandom, d16=mockrandom, c13=mockrandom, c12=standard, d15=standard, d18=simpletext, c15=simpletext, d17=mocksep, c14=mocksep, b3=simpletext, b2=mocksep, b5=pulsing(freqcutoff=11), b4=mockfixedintblock(blocksize=1372), b7=mockfixedintblock(blocksize=1372), b6=mockvariableintblock(baseblocksize=105), d50=pulsing(freqcutoff=11), b9=mockrandom, b8=standard, d43=mockrandom, d42=standard, d41=mockfixedintblock(blocksize=1372), d40=mockvariableintblock(baseblocksize=105), d47=mocksep, d46=pulsing(freqcutoff=11), b0=mockfixedintblock(blocksize=1372), d45=standard, b1=pulsing(freqcutoff=11), d44=simpletext, d49=pulsing(freqcutoff=11), d48=mockfixedintblock(blocksize=1372), c6=mockrandom, c5=standard, c4=mockfixedintblock(blocksize=1372), c3=mockvariableintblock(baseblocksize=105), c9=pulsing(freqcutoff=11), c8=standard, c7=simpletext, d30=simpletext, d32=pulsing(freqcutoff=11), d31=mockfixedintblock(blocksize=1372), c1=standard, d34=mockfixedintblock(blocksize=1372), c2=mockrandom, d33=mockvariableintblock(baseblocksize=105), d36=mockrandom, c0=mockfixedintblock(blocksize=1372), d35=standard, d38=standard, d37=simpletext, d39=pulsing(freqcutoff=11), e92=mockfixedintblock(blocksize=1372), e93=pulsing(freqcutoff=11), e90=mocksep, e91=simpletext, e89=pulsing(freqcutoff=11), e88=standard, e87=simpletext, e86=mockrandom, e85=standard, e84=mockfixedintblock(blocksize=1372), e83=mockvariableintblock(baseblocksize=105), e80=mockvariableintblock(baseblocksize=105), e81=simpletext, e82=standard, e77=mockfixedintblock(blocksize=1372), e76=mockvariableintblock(baseblocksize=105), e79=mockrandom, e78=standard, e73=simpletext, e72=mocksep, e75=pulsing(freqcutoff=11), e74=mockfixedintblock(blocksize=1372), binary=pulsing(freqcutoff=11), f98=mocksep, f97=pulsing(freqcutoff=11), f99=mockvariableintblock(baseblocksize=105), f94=mockrandom, f93=standard, f96=simpletext, f95=mocksep, e95=mocksep, e94=pulsing(freqcutoff=11), e97=mockfixedintblock(blocksize=1372), e96=mockvariableintblock(baseblocksize=105), e99=mockvariableintblock(baseblocksize=105), e98=mockrandom, id=mockrandom, f34=simpletext, f33=mocksep, f32=mockrandom, f31=standard, f30=mockvariableintblock(baseblocksize=105), f39=mockrandom, f38=mockfixedintblock(blocksize=1372), f37=mockvariableintblock(baseblocksize=105), f36=mocksep, f35=pulsing(freqcutoff=11), f43=mocksep, f42=pulsing(freqcutoff=11), f45=mockfixedintblock(blocksize=1372), f44=mockvariableintblock(baseblocksize=105), f41=simpletext, f40=mocksep, f47=mockvariableintblock(baseblocksize=105), f46=mockrandom, f49=standard, f48=simpletext, content=mockfixedintblock(blocksize=1372), e19=standard, e18=simpletext, e17=mockrandom, f12=standard, e16=standard, f11=simpletext, f10=mockvariableintblock(baseblocksize=105), e15=mockfixedintblock(blocksize=1372), e14=mockvariableintblock(baseblocksize=105), f16=pulsing(freqcutoff=11), e13=pulsing(freqcutoff=11), f15=mockfixedintblock(blocksize=1372), e12=mockfixedintblock(blocksize=1372), e11=simpletext, f14=simpletext, e10=mocksep, f13=mocksep, f19=standard, f18=mockfixedintblock(blocksize=1372), f17=mockvariableintblock(baseblocksize=105), e29=mockfixedintblock(blocksize=1372), e26=standard, f21=simpletext, e25=simpletext, f20=mocksep, e28=mocksep, f23=pulsing(freqcutoff=11), e27=pulsing(freqcutoff=11), f22=mockfixedintblock(blocksize=1372), f25=mockfixedintblock(blocksize=1372), e22=mockfixedintblock(blocksize=1372), f24=mockvariableintblock(baseblocksize=105), e21=mockvariableintblock(baseblocksize=105), f27=mockrandom, e24=mockrandom, f26=standard, e23=standard, f29=standard, f28=simpletext, e20=pulsing(freqcutoff=11), field=mockrandom, string=pulsing(freqcutoff=11), e30=mocksep, e31=simpletext, a98=mockrandom, e34=mockvariableintblock(baseblocksize=105), a99=mockvariableintblock(baseblocksize=105), e35=mockfixedintblock(blocksize=1372), f79=mockvariableintblock(baseblocksize=105), e32=pulsing(freqcutoff=11), e33=mocksep, b97=pulsing(freqcutoff=11), f77=mockfixedintblock(blocksize=1372), e38=simpletext, b98=mocksep, f78=pulsing(freqcutoff=11), e39=standard, b99=mockvariableintblock(baseblocksize=105), f75=mocksep, e36=mockrandom, f76=simpletext, e37=mockvariableintblock(baseblocksize=105), f73=simpletext, f74=standard, f71=mockrandom, f72=mockvariableintblock(baseblocksize=105), f81=mockfixedintblock(blocksize=1372), f80=mockvariableintblock(baseblocksize=105), e40=mocksep, e41=mockvariableintblock(baseblocksize=105), e42=mockfixedintblock(blocksize=1372), e43=mockrandom, e44=mockvariableintblock(baseblocksize=105), e45=simpletext, e46=standard, f86=mockvariableintblock(baseblocksize=105), e47=mocksep, f87=mockfixedintblock(blocksize=1372), e48=simpletext, f88=standard, e49=mockfixedintblock(blocksize=1372), f89=mockrandom, f82=mocksep, f83=simpletext, f84=mockfixedintblock(blocksize=1372), f85=pulsing(freqcutoff=11), f90=mockvariableintblock(baseblocksize=105), f92=standard, f91=simpletext, str=mockfixedintblock(blocksize=1372), a76=mockvariableintblock(baseblocksize=105), e56=mockrandom, f59=mockrandom, a77=mockfixedintblock(blocksize=1372), e57=mockvariableintblock(baseblocksize=105), a78=standard, e54=mockfixedintblock(blocksize=1372), f57=mockfixedintblock(blocksize=1372), a79=mockrandom, e55=pulsing(freqcutoff=11), f58=pulsing(freqcutoff=11), e52=pulsing(freqcutoff=11), e53=mocksep, e50=simpletext, e51=standard, f51=standard, f52=mockrandom, f50=mockfixedintblock(blocksize=1372), f55=pulsing(freqcutoff=11), f56=mocksep, f53=simpletext, e58=standard, f54=standard, e59=mockrandom, a80=mockvariableintblock(baseblocksize=105), e60=mockrandom, a82=standard, a81=simpletext, a84=simpletext, a83=mocksep, a86=pulsing(freqcutoff=11), a85=mockfixedintblock(blocksize=1372), a89=pulsing(freqcutoff=11), f68=standard, e65=standard, f69=mockrandom, e66=mockrandom, a87=simpletext, e67=mocksep, a88=standard, e68=simpletext, e61=mockfixedintblock(blocksize=1372), e62=pulsing(freqcutoff=11), e63=mockrandom, e64=mockvariableintblock(baseblocksize=105), f60=simpletext, f61=standard, f62=pulsing(freqcutoff=11), f63=mocksep, e69=pulsing(freqcutoff=11), f64=mockfixedintblock(blocksize=1372), f65=pulsing(freqcutoff=11), f66=mockrandom, f67=mockvariableintblock(baseblocksize=105), f70=mockrandom, a93=pulsing(freqcutoff=11), a92=mockfixedintblock(blocksize=1372), a91=simpletext, e71=mocksep, a90=mocksep, e70=pulsing(freqcutoff=11), a97=mockrandom, a96=standard, a95=mockfixedintblock(blocksize=1372), a94=mockvariableintblock(baseblocksize=105), c58=mockfixedintblock(blocksize=1372), a63=pulsing(freqcutoff=11), a64=mocksep, c59=pulsing(freqcutoff=11), c56=mocksep, d59=mocksep, a61=simpletext, c57=simpletext, a62=standard, c54=simpletext, c55=standard, a60=mockrandom, c52=mockrandom, c53=mockvariableintblock(baseblocksize=105), d53=mockvariableintblock(baseblocksize=105), d54=mockfixedintblock(blocksize=1372), d51=pulsing(freqcutoff=11), d52=mocksep, d57=simpletext, b62=standard, d58=standard, b63=mockrandom, d55=mockrandom, b60=mockvariableintblock(baseblocksize=105), d56=mockvariableintblock(baseblocksize=105), b61=mockfixedintblock(blocksize=1372), b56=mocksep, b55=pulsing(freqcutoff=11), b54=standard, b53=simpletext, d61=simpletext, b59=mockrandom, d60=mocksep, b58=pulsing(freqcutoff=11), b57=mockfixedintblock(blocksize=1372), c62=mockfixedintblock(blocksize=1372), c61=mockvariableintblock(baseblocksize=105), a59=mockrandom, c60=mocksep, a58=standard, a57=mockvariableintblock(baseblocksize=105), a56=mockrandom, a55=pulsing(freqcutoff=11), a54=mockfixedintblock(blocksize=1372), a72=mockfixedintblock(blocksize=1372), c67=mockvariableintblock(baseblocksize=105), a73=pulsing(freqcutoff=11), c68=mockfixedintblock(blocksize=1372), a74=mockrandom, c69=standard, a75=mockvariableintblock(baseblocksize=105), c63=mocksep, c64=simpletext, a70=pulsing(freqcutoff=11), c65=mockfixedintblock(blocksize=1372), a71=mocksep, c66=pulsing(freqcutoff=11), d62=mockrandom, d63=mockvariableintblock(baseblocksize=105), d64=simpletext, b70=mockrandom, d65=standard, b71=simpletext, d66=mocksep, b72=standard, d67=simpletext, b73=pulsing(freqcutoff=11), d68=mockfixedintblock(blocksize=1372), b74=mocksep, d69=pulsing(freqcutoff=11), b65=pulsing(freqcutoff=11), b64=mockfixedintblock(blocksize=1372), b67=mockvariableintblock(baseblocksize=105), b66=mockrandom, d70=mocksep, b69=mockrandom, b68=standard, d72=mockfixedintblock(blocksize=1372), d71=mockvariableintblock(baseblocksize=105), c71=mockvariableintblock(baseblocksize=105), c70=mockrandom, a69=pulsing(freqcutoff=11), c73=standard, c72=simpletext, a66=mockrandom, a65=standard, a68=simpletext, a67=mocksep, c32=standard, c33=mockrandom, c30=mockvariableintblock(baseblocksize=105), c31=mockfixedintblock(blocksize=1372), c36=pulsing(freqcutoff=11), a41=mocksep, c37=mocksep, a42=simpletext, a0=mocksep, c34=simpletext, c35=standard, a40=mockrandom, b84=simpletext, d79=mocksep, b85=standard, b82=mockrandom, d77=standard, c38=mockfixedintblock(blocksize=1372), b83=mockvariableintblock(baseblocksize=105), d78=mockrandom, c39=pulsing(freqcutoff=11), b80=mockvariableintblock(baseblocksize=105), d75=mockrandom, b81=mockfixedintblock(blocksize=1372), d76=mockvariableintblock(baseblocksize=105), d73=mockfixedintblock(blocksize=1372), d74=pulsing(freqcutoff=11), d83=mocksep, a9=standard, d82=pulsing(freqcutoff=11), d81=standard, d80=simpletext, b79=mockvariableintblock(baseblocksize=105), b78=pulsing(freqcutoff=11), b77=mockfixedintblock(blocksize=1372), b76=simpletext, b75=mocksep, a1=simpletext, a35=mockfixedintblock(blocksize=1372), a2=standard, a34=mockvariableintblock(baseblocksize=105), a3=pulsing(freqcutoff=11), a33=mocksep, a4=mocksep, a32=pulsing(freqcutoff=11), a5=mockfixedintblock(blocksize=1372), a39=standard, c40=pulsing(freqcutoff=11), a6=pulsing(freqcutoff=11), a38=simpletext, a7=mockrandom, a37=mockvariableintblock(baseblocksize=105), a8=mockvariableintblock(baseblocksize=105), a36=mockrandom, c41=simpletext, c42=standard, c43=pulsing(freqcutoff=11), c44=mocksep, c45=mockfixedintblock(blocksize=1372), a50=pulsing(freqcutoff=11), c46=pulsing(freqcutoff=11), a51=mocksep, c47=mockrandom, a52=mockvariableintblock(baseblocksize=105), c48=mockvariableintblock(baseblocksize=105), a53=mockfixedintblock(blocksize=1372), b93=mocksep, d88=pulsing(freqcutoff=11), c49=standard, b94=simpletext, d89=mocksep, b95=mockfixedintblock(blocksize=1372), b96=pulsing(freqcutoff=11), d84=standard, b90=mockvariableintblock(baseblocksize=105), d85=mockrandom, b91=simpletext, d86=mocksep, b92=standard, d87=simpletext, d92=pulsing(freqcutoff=11), d91=mockfixedintblock(blocksize=1372), d94=mockvariableintblock(baseblocksize=105), d93=mockrandom, b87=mockfixedintblock(blocksize=1372), b86=mockvariableintblock(baseblocksize=105), d90=mocksep, b89=mockrandom, b88=standard, a44=mockvariableintblock(baseblocksize=105), a43=mockrandom, a46=standard, a45=simpletext, a48=simpletext, a47=mocksep, c51=mockrandom, a49=mockfixedintblock(blocksize=1372), c50=standard, d98=mockrandom, d97=standard, d96=mockfixedintblock(blocksize=1372), d95=mockvariableintblock(baseblocksize=105), d99=simpletext, a20=standard, c99=mocksep, c98=pulsing(freqcutoff=11), c97=standard, c96=simpletext, b19=standard, a16=standard, a17=mockrandom, b17=mockvariableintblock(baseblocksize=105), a14=mockvariableintblock(baseblocksize=105), b18=mockfixedintblock(blocksize=1372), a15=mockfixedintblock(blocksize=1372), a12=mockfixedintblock(blocksize=1372), a13=pulsing(freqcutoff=11), a10=mocksep, a11=simpletext, b11=simpletext, b12=standard, b10=mockvariableintblock(baseblocksize=105), b15=mockfixedintblock(blocksize=1372), b16=pulsing(freqcutoff=11), a18=simpletext, b13=mocksep, a19=standard, b14=simpletext, b30=standard, a31=pulsing(freqcutoff=11), a30=mockfixedintblock(blocksize=1372), b28=simpletext, a25=simpletext, b29=standard, a26=standard, a27=pulsing(freqcutoff=11), a28=mocksep, a21=mockvariableintblock(baseblocksize=105), a22=mockfixedintblock(blocksize=1372), a23=standard, a24=mockrandom, b20=mocksep, b21=simpletext, b22=mockfixedintblock(blocksize=1372), b23=pulsing(freqcutoff=11), a29=mockfixedintblock(blocksize=1372), b24=mockvariableintblock(baseblocksize=105), b25=mockfixedintblock(blocksize=1372), b26=standard, b27=mockrandom, b41=mockvariableintblock(baseblocksize=105), b40=mockrandom, c77=simpletext, c76=mocksep, c75=mockrandom, c74=standard, c79=mocksep, c78=pulsing(freqcutoff=11), c80=mocksep, c83=mockrandom, c84=mockvariableintblock(baseblocksize=105), c81=mockfixedintblock(blocksize=1372), b39=mockrandom, c82=pulsing(freqcutoff=11), b37=mockvariableintblock(baseblocksize=105), b38=mockfixedintblock(blocksize=1372), b35=pulsing(freqcutoff=11), b36=mocksep, b33=mocksep, b34=simpletext, b31=standard, b32=mockrandom, str2=standard, b50=mockrandom, b52=simpletext, str3=pulsing(freqcutoff=11), b51=mocksep, c86=mocksep, tvtest=pulsing(freqcutoff=11), c85=pulsing(freqcutoff=11), c88=mockfixedintblock(blocksize=1372), c87=mockvariableintblock(baseblocksize=105), c89=mockrandom, c90=mockrandom, c91=mockvariableintblock(baseblocksize=105), c92=standard, c93=mockrandom, c94=mocksep, c95=simpletext, content1=simpletext, b46=mockrandom, b47=mockvariableintblock(baseblocksize=105), content3=mockrandom, b48=simpletext, content4=standard, b49=standard, content5=mockvariableintblock(baseblocksize=105), b42=pulsing(freqcutoff=11), b43=mocksep, b44=mockvariableintblock(baseblocksize=105), b45=mockfixedintblock(blocksize=1372)}, locale=it_ch, timezone=europe/chisinau     [junit] note: all tests run in this jvm:     [junit] [testmergeschedulerexternal, testtoken, testcodecs, testfieldinfos, testflushbyramorcountspolicy, testindexreaderreopen, testindexwriter]     [junit] note: linux 2.6.37-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=275548088,total=309395456",
        "label": 33
    },
    {
        "text": "port x fieldcache getdocswithfield  to trunk [spinoff from lucene-3390] i think the approach in 3.x for handling un-valued docs, and making it possible to specify how such docs are sorted, is better than the solution we have in trunk. i like that fc has a dedicated method to get the bits for docs with field \u2013 easy for apps to directly use. and i like that the bits have their own entry in the fc. one downside is that it's 2 passes to get values and valid bits, but i think we can fix this by passing optional bool to fc.getxxx methods indicating you want the bits, and the populate the fc entry for the missing bits as well. (we can do that for 3.x and trunk). then it's single pass.",
        "label": 33
    },
    {
        "text": "compile with compact profiles if we clean up the 'alignment' calculator in ramusageestimator, we can compile core with compact1, and the rest of lucene (except tests) with compact2.",
        "label": 40
    },
    {
        "text": "add packedquadprefixtree this task introduces a packedquadprefixtree that includes two things: 1. a packed 8 byte representation for a quadcell, including more efficient implementations of the spt api than the existing quadprefixtree or geohashprefixtree. 2. an alternative implementation to rpt's \"pruneleafybranches\" that streams the cells without buffering them all, which is way more memory efficient. however pruning is limited to the target detail level, where it accomplishes the most good. future improvements over this approach may include the generation of the packed cells using an autoprefixautomaton",
        "label": 10
    },
    {
        "text": "samplecomparable doesn't work well in contrib remote tests as discovered in lucene-1749, when using identical instances of a sortcomparator you get multiple entries in the fieldcache. demonstrating this bug currently requires the patches in lucene-1749. see markmiller's comment here... https://issues.apache.org/jira/browse/lucene-1749?focusedcommentid=12735190#action_12735190",
        "label": 29
    },
    {
        "text": "position increment bug  smartcn if i use lucene_version >= 2.9 with smart chinese analyzer, it will crash indexwriter with any reasonable amount of chinese text. its especially annoying because it happens in 2.9.1 rc as well. this is because the position increments for tokens after stopwords are bogus: here's an example (from test case), where the position increment should be 2, but is instead 91975314!   public void testchinesestopwords2() throws exception {     analyzer ca = new smartchineseanalyzer(version.lucene_current); /* will load stopwords */     string sentence = \"title:san\"; // : is a stopword     string result[] = { \"titl\", \"san\"};     int startoffsets[] = { 0, 6 };     int endoffsets[] = { 5, 9 };     int posincr[] = { 1, 2 };     assertanalyzesto(ca, sentence, result, startoffsets, endoffsets, posincr);   } junit.framework.assertionfailederror: posincrement 1 expected:<2> but was:<91975314> at junit.framework.assert.fail(assert.java:47) at junit.framework.assert.failnotequals(assert.java:280) at junit.framework.assert.assertequals(assert.java:64) at junit.framework.assert.assertequals(assert.java:198) at org.apache.lucene.analysis.basetokenstreamtestcase.asserttokenstreamcontents(basetokenstreamtestcase.java:83) ...",
        "label": 40
    },
    {
        "text": "site should call project  lucene java  not just  lucene  to avoid confusion with the top-level lucene project, the lucene java website should refer to itself as lucene java.",
        "label": 29
    },
    {
        "text": "ant contrib test can fail if there is a space in path to lucene project a couple contrib ant tests get the path to test files through a url object, and so the path is url encoded. normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my ubuntu system) the test will fail with filenotfound. this patch simply replaces all %20 with \" \". not sure if we want/need to take it any further.",
        "label": 33
    },
    {
        "text": "improve arabic analyzer  light8   light10 someone mentioned on the java user list that the arabic analysis was not as good as they would like. this patch adds the \u0644\u0644- prefix (light10 algorithm versus light8 algorithm). in the light10 paper, this improves precision from .390 to .413 they mention this is not statistically significant, but it makes linguistic sense and at least has been shown not to hurt. in the future, i hope openrelevance will allow us to try some more approaches.",
        "label": 40
    },
    {
        "text": "improve infostream class in trunk to be more consistent with logging frameworks like slf4j log4j commons logging followup on a thread by shai erea on java-dev@lao: i already discussed with robert about that, that there is one thing missing. currently the iw only checks if the infostream!=null and then passes the message to the method, and that may ignore it. for your requirement it is the case that this is enabled or disabled dynamically. unfortunately if the construction of the message is heavy, then this wastes resources. i would like to add another method to this class: abstract boolean isenabled() that can also be implemented. i would then replace all null checks in iw by this method. the default config in iw would be changed to use a nooutputinfostream that returns false here and ignores the message. a simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled): loger log = yourloggingframework.getlogger(indexwriter.class); public void message(string component, string message) {   log.debug(component + \": \" + message); } public boolean isenabled(string component) {   return log.isdebugenabled(); } using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling indexwriter.class logging. the changes are really simple: printstreaminfostream returns true, always, mabye make it dynamically enable/disable to allow shai's request infostream.getdefault() is never null and can never be set to null. instead the default is a singleton nooutputinfostream that returns false of isenabled(component). all null checks on infostream should be replaced by infostream.iseanbled(component), this is possible as always != null. there are no slowdowns by this - it's like collections.emptylist() instead stupid null checks.",
        "label": 53
    },
    {
        "text": "tokenstream tokenizer tokenfilter token javadoc improvements some of the javadoc for the new tokenstream/tokenizer/tokenfilter/token apis had javadoc errors. to the best of my knowledge, i corrected these and refined the copy a bit.",
        "label": 33
    },
    {
        "text": "indexreader iscurrent  lies if documents were only removed by latest commit usecase is as following: 1. get indexreader via indexwriter. 2. delete documents by term via indexwriter. 3. commit indexwriter. 4. indexreader.iscurrent() returns true. usually there is a check if index reader is current. if not then it is reopened (re-obtained via writer or ect.). but this causes the problem when documents can still be found through the search after deletion. testcase is attached.",
        "label": 33
    },
    {
        "text": "gdata server   testcase deadlock storagemodifier solfed the racecondition deadlock while closing the storagecontroller. this occured the first time hossman tried to run the test cases. concurrent modification exception while iteration over a collection in a sepereate thread \u2013 modifiedentryfilter replaced list with array. @hossman if you can get to it, could you try the testcases again. @all if you guys do have time you could run the testcases on different environment, that would help to resolve bugs in the test cases and the server. simon",
        "label": 18
    },
    {
        "text": "windowsfs misses to remove open file handle if file is concurrently deleted windowsfs has some race conditions when files are concurrently opened and deleted. a file might be successfully opened while concurrently deleted which should be prevented by the windowsfs with an ioexception / access denied. the problem is that we try to remove the leaked file handle form the internal map on close which fails since we fail to read the key from the filesystem since it has already been deleted. this manifests in subsequent `access denied` exceptions even though all streams on the file are closed.",
        "label": 46
    },
    {
        "text": "make contrib collation icu collationkeyanalyzer constructors public in contrib/collation, the constructors for collationkeyanalyzer and icucollationkeyanalyzer are package private, and so are effectively unusable.",
        "label": 33
    },
    {
        "text": "fully decouple indexwriter from analyzers indexwriter only needs an attributesource to do indexing. yet, today, it interacts with field instances, holds a private analyzers, invokes analyzer.reusabletokenstream, has to deal with a wide variety (it's not analyzed; it is analyzed but it's a reader, string; it's pre-analyzed). i'd like to have iw only interact with attr sources that already arrived with the fields. this would be a powerful decoupling \u2013 it means others are free to make their own attr sources. they need not even use any of lucene's analysis impls; eg they can integrate to other things like openpipeline. or make something completely custom. lucene-2302 is already a big step towards this: it makes iw agnostic about which attr is \"the term\", and only requires that it provide a bytesref (for flex). then i think lucene-2308 would get us most of the remaining way \u2013 ie, if the fieldtype knows the analyzer to use, then we could simply create a getattrsource() method (say) on it and move all the logic iw has today onto there. (we'd still need existing iw code for back-compat).",
        "label": 7
    },
    {
        "text": "checkindex should verify numuniqueterms   recomputednumuniqueterms just glancing at the code it seems to sorta do this check, but only in the hasord==true case maybe (which seems to be testing something else)? it would be nice to verify this also for terms dicts that dont support ord. we should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex",
        "label": 40
    },
    {
        "text": "change payloaditerator to not use top level reader api currently the facet module uses multifields.* to pull the d&penum in payloaditerator, to access the payloads that store the facet ords. it then makes heavy use of .advance and .getpayload to visit all docids in the result set. i think we should get some speedup if we go segment by segment instead ...",
        "label": 43
    },
    {
        "text": "promote solr's prefixfilter into java lucene's core solr's prefixfilter class is not specific to solr and seems to be of interest to core lucene users (pylucene in this case). promoting it into the lucene core would be helpful.",
        "label": 55
    },
    {
        "text": "ensure binary artifacts contain the necessary licensing files lucene src pack contains snowball-license.txt which is not in either of the two binary packages, is this a problem?",
        "label": 40
    },
    {
        "text": "make indexreader decref  call refcount decrementandget instead of getanddecrement indexreader.decref() has this code:     final int rc = refcount.getanddecrement();     if (rc == 1) { i think it will be clearer if it was written like this:     final int rc = refcount.decrementandget();     if (rc == 0) { it's a very simple change, which makes reading the code (at least imo) easier. will post a patch shortly.",
        "label": 43
    },
    {
        "text": "contrib spatial should use docset api rather then deprecated bitset api contrib-spatial should be rewritten to use the new docidset filter api with openbitsets instead of j.u.bitsets. filtereddocidset can be used to replace (i)serialchainfilter.",
        "label": 33
    },
    {
        "text": "don't use memorycodec for nightly runs of testindexsorting nightly runs of testindexsorting fail occasionally with oom (see https://builds.apache.org/job/lucene-solr-nightlytests-7.x/183/ for a recent example, and it's been appearing in erick's badapple report too).  it looks as this is normally due to the combination of a large docset and memorycodec.  we should suppress memorycodec for these tests, on nightly runs only if possible)",
        "label": 2
    },
    {
        "text": "if index is too old you should hit an exception saying so if you create an index in 2.3.x (i used demo's indexfiles) and then try to read it in 4.0.x (i used checkindex), you hit a confusing exception like this: java.io.ioexception: read past eof         at org.apache.lucene.store.bufferedindexinput.refill(bufferedindexinput.java:154)         at org.apache.lucene.store.bufferedindexinput.readbyte(bufferedindexinput.java:39)         at org.apache.lucene.store.checksumindexinput.readbyte(checksumindexinput.java:40)         at org.apache.lucene.store.datainput.readint(datainput.java:76)         at org.apache.lucene.index.segmentinfo.<init>(segmentinfo.java:171)         at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:230)         at org.apache.lucene.index.segmentinfos$1.dobody(segmentinfos.java:269)         at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:649)         at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:484)         at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:265)         at org.apache.lucene.index.checkindex.checkindex(checkindex.java:308)         at org.apache.lucene.index.checkindex.checkindex(checkindex.java:287)         at org.apache.lucene.index.checkindex.main(checkindex.java:930) i think instead we should throw an indextoooldexception or something like that?",
        "label": 53
    },
    {
        "text": "generify fst shortestpaths  to take a comparator not sure we should do this, it costs 5-10% performance for wfstsuggester. but maybe we can optimize something here, or maybe its just no big deal to us. because in general, this could be pretty powerful, e.g. if you needed to store some custom stuff in the suggester, you could use pairoutputs, or whatever. and the possibility we might need shortestpaths for other cool things... at the least i just wanted to have the patch up here. i haven't tested this on pairoutputs... but i've tested it with e.g. floatoutputs and other things and it works fine. i tried to minimize the generics violations, there is only 1 (cannot create generic array).",
        "label": 40
    },
    {
        "text": "allow more than 1b  tail nodes  when building fst we recently relaxed some of the limits for big fsts, but there is one more limit i think we should fix. e.g. aaron hit it in building the world's biggest fst: http://aaron.blog.archive.org/2013/05/29/worlds-biggest-fst/ the issue is nodehash, which currently uses a growablewriter (packed ints impl that can grow both number of bits and number of values): it's indexed by int not long. this is a hash table that's used to share suffixes, so we need random get/put on a long index of long values, i.e. this is logically a long[]. i think one simple way to do this is to make a \"paged\" growablewriter... along with this we'd need to fix the hash codes to be long not int.",
        "label": 33
    },
    {
        "text": "testweakidentitymap testconcurrenthashmap fails periodically in jenkins there is either a bug, a test bug, or a jvm bug. i dont care which one it is, but lets fix the intermittent fail or disable the test.",
        "label": 53
    },
    {
        "text": "pull corereaders out of segmentreader similar to lucene-3117, i think we should pull the corereaders class out of sr, to make it easier to navigate the code.",
        "label": 40
    },
    {
        "text": "add backcompat for tokenfilters with posinc false before in lucene 4.4, a number of token filters supporting the enablepositionincrements=false setting were changed to default to true. however, with lucene 5.0, the setting was removed altogether. we should have backcompat for this setting, as well as work when used with a tokenfilterfactory and match version < 4.4.",
        "label": 47
    },
    {
        "text": "make all classes that have a close  methods instanceof closeable  java  this should be simple.",
        "label": 53
    },
    {
        "text": "lucene native directory implementation need automated build currently the native directory impl in contrib/misc require manual action to compile the c code (partially) documented in https://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/contrib/misc/src/java/overview.html yet it would be nice if we had an ant task and documentation for all platforms how to compile them and set up the prerequisites.",
        "label": 46
    },
    {
        "text": "bounds not properly computed for exact circles hi karl wright, i still get some errors in the tests but luckily it is not related with the last change. the errors always happen between complex polygons and exact circles. i look into it and the problem is that complex polygons require that shapes compute the correct bounds to compute intersections. this is not the case for exact circles. it seems a generic bug so i am not attaching a test case. but if you want i can try to work up something.",
        "label": 25
    },
    {
        "text": "multi threaded spatial search the attached patch is a large refactoring of the spatial search contrib. the primary contribution is the creation of the threadeddistancefilter, which uses an executorservice to filter the documents in multiple threads. as a result of doing the filtering in multiple threads, the time taken to filter 1.2 million documents has been reduced from nearly 3s, to between 500-800ms. as part of this work, the distancequerybuilder has been replaced by the spatialfilter, a lucene filter, some unused functionality has been removed, and the package hierarchy has changed. consequently this patch breaks backwards compatibility with the existing spatial search contrib. also during the process of making these changes, abstractions have been added so that the one implementation of the threadeddistancefilter can work with lat/long and geohash data formats, and so that precise but costly arc distance calculations can be replaced by less precise but much more efficient flat plane calculations if needed. this patch will be used in an upcoming patch for solr which will improve solr's support for spatial search.",
        "label": 7
    },
    {
        "text": "performance optimization when retrieving a single field from a document if you just want to retrieve a single field from a document, the only way to do it is to retrieve all the fields from the document and then search it. this patch is an optimization that allows you retrieve a specific field from a document without instantiating a lot of field and string objects. this reduces our memory consumption on a per query basis by around around 20% when a lot of documents are returned. i've added a lot of comments saying you should only call it if you only ever need one field. there's also a unit test.",
        "label": 38
    },
    {
        "text": "fix lowercasefilter for unicode lowercase suppl. characters correctly. this only fixes the filter, the lowercasetokenizer is part of a more complex issue (chartokenizer)",
        "label": 40
    },
    {
        "text": "instantiatedindex   faster but memory consuming index represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric ramdirectory at the cost of greater ram consumption. performance seems to be a little bit better than log2n (binary search). no real data on that, just my eyes. populated with a single document instantiatedindex is almost, but not quite, as fast as memoryindex. at 20,000 document 10-50 characters long instantiatedindex outperforms ramdirectory some 30x, 15x at 100 documents of 2000 charachters length, and is linear to ramdirectory at 10,000 documents of 2000 characters length. mileage may vary depending on term saturation.",
        "label": 15
    },
    {
        "text": "filters should return null if they don't accept documents today we document that filter#getdocidset can return null if it doesn't accept documents. infact in the code we sometimes return null and sometimes return docidset.empty_docidset. conceptually there is nothing wrong with that but apparently we are not applying our optimisations accordingly ie. some parts of the code check for the empty_docidset and all check for null. this is also a source of potential bugs like in lucene-4940 and i think there are still problems in the tochildblock query. anyways, i think we should be consistent here about when to apply the optimisations and for the sake of caching in cachingwrapperfilter we should make the empty_docidset and impl detail.",
        "label": 46
    },
    {
        "text": "testindexwriterondiskfull testadddocumentondiskfull seed failure version: trunk r1155278 reproduce-able: always     [junit] testsuite: org.apache.lucene.index.testindexwriterondiskfull     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0.847 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testindexwriterondiskfull -dtestmethod=testadddocumentondiskfull -dtests.seed=-3cc23002ebad518d:70ae722281b31c9f:57406021f8789a22     [junit] note: test params are: codec=randomcodecprovider: {content=mockfixedintblock(blocksize=1081)}, locale=hr_hr, timezone=atlantic/jan_mayen     [junit] note: all tests run in this jvm:     [junit] [testindexwriterondiskfull]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=85252968,total=125632512     [junit] ------------- ---------------- ---------------     [junit] testcase: testadddocumentondiskfull(org.apache.lucene.index.testindexwriterondiskfull):     caused an error     [junit] no segments* file found in mockdirwrapper(org.apache.lucene.store.ramdirectory@65dcc2a3 lockfactory=mocklockfactorywrapper(org.apache.lucene.store.singleinstancelockfactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]     [junit] org.apache.lucene.index.indexnotfoundexception: no segments* file found in mockdirwrapper(org.apache.lucene.store.ramdirectory@65dcc2a3 lockfactory=mocklockfactorywrapper(org.apache.lucene.store.singleinstancelockfactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]     [junit]     at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:657)     [junit]     at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:534)     [junit]     at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:284)     [junit]     at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:277)     [junit]     at org.apache.lucene.index.testindexwriter.assertnounreferencedfiles(testindexwriter.java:158)     [junit]     at org.apache.lucene.index.testindexwriterondiskfull.testadddocumentondiskfull(testindexwriterondiskfull.java:114)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1526)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1428)     [junit]      [junit]      [junit] test org.apache.lucene.index.testindexwriterondiskfull failed",
        "label": 46
    },
    {
        "text": "fastvectorhighlighter  support for additional queries i am using fastvectorhighlighter for some strange languages and it is working well! one thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc) here is one thing michael m posted in the original ticket: i think a nice [eventual] model would be if we could simply re-run the scorer on the single document (using instantiatedindex maybe, or simply some sort of wrapper on the term vectors which are already a mini-inverted-index for a single doc), but extend the scorer api to tell us the exact term occurrences that participated in a match (which i don't think is exposed today). due to strange requirements i am using something similar to this (but specialized to our case). i am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted, and flattening multiphrasequeries into boolean or'ed phrasequeries. i do not think these things would be 'fast', but i had a few ideas that might help: looking at contrib/highlighter, you can support filteredquery in flatten() by calling getquery() right? maybe as a last resort, try query.extractterms() ?",
        "label": 26
    },
    {
        "text": "mockrandommergepolicy optimizes segments not in the set passed in the test class mockrandommergepolicy shuffles the whole segmentinfos passed to the optimize callback and returns random segments for optimizing. this is fine, but it also returns segments, that are not listed in the set<segmentinfo> that is also passed in, containing the subset of segments to optimize. this bug was found when writing a testcase for lucene-3082: the wrapper mergepolicy (when wrapped around mockrandommergepolicy) only passes a subset of the segments to the delegate (the ones that are in old index format). but mockrandom created onemerge in its return mergespecification having segments outside this set.",
        "label": 33
    },
    {
        "text": "indexwriter has incomplete javadocs a couple of getter methods in indexwriter have no javadocs.",
        "label": 33
    },
    {
        "text": "create a markdown formatted readme file for lucene solr we have a minimal plain text readme file right now. github is very popular these days and we are even accepting pull requests from there. i think we should add a proper markdown formatted readme file which not only talks about the features of lucene/solr but also include a short tutorial on both lucene and solr.",
        "label": 34
    },
    {
        "text": "make it posible not to include tf information in index term frequency is typically not needed for all fields, some cpu (reading one vint less and one x>>>1...) and io can be spared by making pure boolen fields possible in lucene. this topic has already been discussed and accepted as a part of flexible indexing... this issue tries to push things a bit faster forward as i have some concrete customer demands. benefits can be expected for fields that are typical candidates for filters, enumerations, user rights, ids or very short \"texts\", phone numbers, zip codes, names... status: just passed standard test (compatibility), commited for early review, i have not tried new feature, missing some asserts and one two unit tests complexity: simpler than expected can be used via omittf() (who used omitnorms() will know where to find it",
        "label": 33
    },
    {
        "text": "javadoc mistake in segmentmerger ",
        "label": 33
    },
    {
        "text": "improved compound file handling currently compoundfilereader could use some improvements, i see the following problems its csindexinput extends bufferedindexinput, which is stupid for directories like mmap. it seeks on every readinternal its not possible for a directory to override or improve the handling of compound files. for example: it seems if you were impl'ing this thing from scratch, you would just wrap the ii directly (not extend bufferedindexinput, and add compound file offset x to seek() calls, and override length(). but of course, then you couldnt throw read past eof always when you should, as a user could read into the next file and be left unaware. however, some directories could handle this better. for example mmapdirectory could return an indexinput that simply mmaps the 'slice' of the cfs file. its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(), as its position would just work. so i think we should try to refactor this so that a directory can customize how compound files are handled, the simplest case for the least code change would be to add this to directory.java:   public directory opencompoundinput(string filename) {     return new compoundfilereader(this, filename);   } because most code depends upon the fact compound files are implemented as a directory and transparent. at least then a subclass could override... but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.",
        "label": 46
    },
    {
        "text": "add supported for wikipedia english as a corpus in the benchmarker stuff add support for using wikipedia for benchmarking.",
        "label": 15
    },
    {
        "text": "jrebug causes porter stemmer to sigsegv happens easily on java7: ant test -dtestcase=testporterstemfilter -dtests.iter=100 might happen on 1.6.0_u26 too, a user reported something that looks like the same bug already: http://www.lucidimagination.com/search/document/3beaa082c4d2fdd4/porterstemfilter_kills_jvm",
        "label": 40
    },
    {
        "text": "nullpointerexception during indexwriter mergesegments last commit on culprit org.apache.lucene.index.fieldsreader: sun oct 30 05:38:46 2005. --------------------------------------------------------- offending code in fieldsreader.java: ... final document doc(int n) throws ioexception { indexstream.seek(n * 8l); long position = indexstream.readlong(); fieldsstream.seek(position); document doc = new document(); int numfields = fieldsstream.readvint(); for (int i = 0; i < numfields; i++) { int fieldnumber = fieldsstream.readvint(); fieldinfo fi = fieldinfos.fieldinfo(fieldnumber); // // this apparently returns null, presumably either as a result of: // catch (indexoutofboundsexception ioobe) { // return null; // } // in fieldinfos.fieldinfo(int fieldnumber) // - or - // because there's a null member of member arraylist bynumber of fieldinfos byte bits = fieldsstream.readbyte(); boolean compressed = (bits & fieldswriter.field_is_compressed) != 0; .... field.store store = field.store.yes; // // here --v is where the npe is thrown. if (fi.isindexed && tokenize) index = field.index.tokenized; ... --------------------------------------------------------- proposed patch: i'm not sure what the behavior should be in this case, but if it's no big deal that there's null field info for an index and we should just ignore that index, an obvious patch could be: in fieldsreader.java: ... for (int i = 0; i < numfields; i++) { int fieldnumber = fieldsstream.readvint(); fieldinfo fi = fieldinfos.fieldinfo(fieldnumber); // vvvpatchvvv if(fi == null) {continue;} byte bits = fieldsstream.readbyte(); ... --------------------------------------------------------- other observations: in my search prior to submitting this issue, i found lucene-168, which looks similar, and is perhaps related, but if so, i'm not sure exactly how.",
        "label": 55
    },
    {
        "text": "addindexes does not call maybemerge i don't know why this was removed, but this is buggy and just asking for trouble.",
        "label": 40
    },
    {
        "text": "fvh  slow performance on very large queries the change from hashset to arraylist for flatqueries in lucene-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. our queries sometime contain tens of thousands of terms. as a result, major portion of execution time for such queries is now spent in the flatqueries.contains( sourcequery ) method calls.",
        "label": 26
    },
    {
        "text": "'ant javacc' in root project should also properly create contrib surround java files for consistency after lucene-1829 which did the same for contrib/queryparser",
        "label": 32
    },
    {
        "text": "toggle score normalization in hits the current implementation of the \"hits\" class sometimes performs score normalization. in particular, whenever the top-ranked score is bigger than 1.0, it is normalized to a maximum of 1.0. in this case, hits may return different score results than topdocs-based methods. in my scenario (a federated search system), hits delievered just plain wrong results. i was merging results from several sources, all having homogeneous statistics (similar to multisearcher, but over the internet using http/xml-based protocols). sometimes, some of the sources had a top-score greater than 1, so i ended up with garbled results. i suggest to add a switch to enable/disable this score-normalization at runtime. my patch (attached) has an additional peformance benefit, since score normalization now occurs only when hits#score() is called, not when creating the hits result list. whenever scores are not required, you save one multiplication per retrieved hit (i.e., at least 100 multiplications with the current implementation of hits).",
        "label": 38
    },
    {
        "text": "add support for polygon holes to geo3d real-world polygons (e.g. from esri) have holes in them. we need polygon support in geo3d that works in the same way. the proposal would be to change the geoconvexpolygon constructor to include a number of geopolygon inputs, each of which would specify a hole. then, the geopolygonfactory.makegeopolygon() method would also need to accept a similar list of geopolygon hole descriptions. this change is likely to be fairly complex because of the already tricky algorithm used to create convex polygons from non-convex ones, implemented in geopolygonfactory.",
        "label": 25
    },
    {
        "text": "consolidate near real time and reopen api semantics we should consolidate the indexwriter.getreader and the indexreader.reopen semantics, since most people are already using the ir.reopen() method, we should simply add:: ir.reopen(indexwriter) initially, it could just call the iw.getreader(), but it probably should switch to just using package private methods for sharing the internals",
        "label": 33
    },
    {
        "text": "testperfielddocvaluesformat testrambytesused failure my jenkins found this seed on branch_5x with java8 on linux, but it repros for me with java7 and on trunk on os x - here's the trunk failure:    [junit4] suite: org.apache.lucene.codecs.perfield.testperfielddocvaluesformat    [junit4]   2> note: reproduce with: ant test  -dtestcase=testperfielddocvaluesformat -dtests.method=testrambytesused -dtests.seed=ee5133c78ae812f3 -dtests.slow=true -dtests.locale=es_cl -dtests.timezone=canada/newfoundland -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 2.27s | testperfielddocvaluesformat.testrambytesused <<<    [junit4]    > throwable #1: java.lang.assertionerror: actual ram usage 17156, but got 20616, -20.16787129867102% error    [junit4]    >  at __randomizedtesting.seedinfo.seed([ee5133c78ae812f3:1cf2218740970da5]:0)    [junit4]    >  at org.apache.lucene.index.baseindexfileformattestcase.testrambytesused(baseindexfileformattestcase.java:282)    [junit4]    >  at org.apache.lucene.index.basedocvaluesformattestcase.testrambytesused(basedocvaluesformattestcase.java:78)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=asserting(lucene53): {}, docvalues:{}, sim=defaultsimilarity, locale=es_cl, timezone=canada/newfoundland    [junit4]   2> note: mac os x 10.10.4 x86_64/oracle corporation 1.8.0_20 (64-bit)/cpus=8,threads=1,free=213922040,total=257425408    [junit4]   2> note: all tests run in this jvm: [testperfielddocvaluesformat]",
        "label": 1
    },
    {
        "text": "optimized memory management in automaton builder finish  reworked automaton.builder.finish() to not allocate memory stepwise. added growtransitions(int numtransitions) to be able to resize the transistions array just once.",
        "label": 33
    },
    {
        "text": "problems with ir's readerfinishedlistener there are two major problems: 1. the listener api does not really apply all indexreaders. for example segmentreaders dont fire it on close, only segmentcorereaders. this is wrong, a segmentcorereader is not an indexreader. furthermore, if you register it on a top-level reader you get events for anything under the reader tree (sometimes, unless they are segmentreaders as mentioned above, where it doesnt work correctly at all). 2. furthermore your listener is 'passed along' in a viral fashion from clone() and reopen(). this means for example, if you are trying to listen to readers in nrt search you are just accumulating reader listeners, all potentially keeping references to old indexreaders (because, in order to deal with #1 your listener must 'keep' a reference to the ir it was registered on, so it can check if thats really the one). we should discuss how to fix #1. i will create a patch for #2 shortly and commit it, its just plain wrong.",
        "label": 40
    },
    {
        "text": "applying deletes is sometimes dog slow i hit this while testing various use cases for lucene-6119 (adding auto-throttle to concurrentmergescheduler). when i tested \"always call updatedocument\" (each add buffers a delete term), with many indexing threads, opening an nrt reader once per second (forcing all deleted terms to be applied), i see that bufferedupdatesstream.applydeletes sometimes seems to take a loooong time, e.g.: bd 0 [2015-01-04 09:31:12.597; lucene merge thread #69]: applydeletes took 339 msec for 10 segments, 117 deleted docs, 607333 visited terms bd 0 [2015-01-04 09:31:18.148; thread-4]: applydeletes took 5533 msec for 62 segments, 10989 deleted docs, 8517225 visited terms bd 0 [2015-01-04 09:31:21.463; lucene merge thread #71]: applydeletes took 1065 msec for 10 segments, 470 deleted docs, 1825649 visited terms bd 0 [2015-01-04 09:31:26.301; thread-5]: applydeletes took 4835 msec for 61 segments, 14676 deleted docs, 9649860 visited terms bd 0 [2015-01-04 09:31:35.572; thread-11]: applydeletes took 6073 msec for 72 segments, 13835 deleted docs, 11865319 visited terms bd 0 [2015-01-04 09:31:37.604; lucene merge thread #75]: applydeletes took 251 msec for 10 segments, 58 deleted docs, 240721 visited terms bd 0 [2015-01-04 09:31:44.641; thread-11]: applydeletes took 5956 msec for 64 segments, 15109 deleted docs, 10599034 visited terms bd 0 [2015-01-04 09:31:47.814; lucene merge thread #77]: applydeletes took 396 msec for 10 segments, 137 deleted docs, 719914 visit what this means is even though i want an nrt reader every second, often i don't get one for up to ~7 or more seconds. this is on an ssd, machine has 48 gb ram, heap size is only 2 gb. 12 indexing threads. as hideously complex as this code is, i think there are some inefficiencies, but fixing them could be hard / make code even hairier ... also, this code is mega-locked: holds iw's lock, holds bd's lock. it blocks things like merges kicking off or finishing... e.g., we pull the mergediterator many times on the same set of sub-iterators. maybe we can create the sorted terms up front and reuse that? maybe we should go \"term stride\" (one term visits all n segments) not \"segment stride\" (visit each segment, iterating all deleted terms for it). just iterating the terms to be deleted takes a sizable part of the time, and we now do that once for every segment in the index. also, the \"isunique\" bit in lucene-6005 should help here, since if we know the field is unique, we can stop seekexact once we found a segment that has the deleted term, we can maybe pass false for removeduplicates to mergediterator...",
        "label": 33
    },
    {
        "text": "changes html generation improvements bug fixes for and improvements to changes2html.pl, which generates changes.html from changes.txt: when the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible. properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release \"1.9 final\" was invisible). auto-linkify solr-xxx and infra-xxx jira issues (previously: only lucene-xxx issues). auto-linkify bugzilla bugs prefaced with \"issue\" (previously: only \"bug\" and \"patch\"). auto-linkify bugzilla bugs in the form \"bugs xxxxx and yyyyy\". auto-linkify issues that follow attributions.",
        "label": 33
    },
    {
        "text": "upgrade ivy to ivy 2.4.0 is released. ivy-1489 is likely to still be a problem. i'm not sure whether we have a minimum version check for ivy, or whether we are using any features that require a minimum version check.",
        "label": 47
    },
    {
        "text": "mmapdirectory shouldn't pass along oom wrapped as ioexception the bug here is in java (not mmapdir), but i think we shoudl do something. users get confused when they configure their jvm to trigger something on oom, and then see \"outofmemoryerror: map failed\": but their trigger doesnt fire. thats because in the jdk, when it maps files it catches outofmemoryerror, asks for a garbage collection, sleeps for 100 milliseconds, then tries to map again. if it fails a second time it wraps the oom in a generic ioexception. i think we should add a try/catch to our filechannel.map",
        "label": 53
    },
    {
        "text": "testbkdtree testrandommedium  failure  some hits were wrong my jenkins found a reproducible seed for a failure of testbkdtree.testrandommedium() on branch_5x with java8:   [junit4] suite: org.apache.lucene.bkdtree.testbkdtree    [junit4]   1> t1: id=29784 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29528    [junit4]   1>   lat=86.88086835667491 lon=-8.821268286556005    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=29801 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29545    [junit4]   1>   lat=86.88149104826152 lon=-9.34366637840867    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=29961 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29705    [junit4]   1>   lat=86.8706679996103 lon=-9.38328042626381    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30015 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29759    [junit4]   1>   lat=86.84762765653431 lon=-9.44802425801754    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30017 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29761    [junit4]   1>   lat=86.8753323610872 lon=-9.091365560889244    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30042 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29786    [junit4]   1>   lat=86.85837233439088 lon=-9.127480667084455    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30061 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29805    [junit4]   1>   lat=86.85876209288836 lon=-9.408821929246187    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30077 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29821    [junit4]   1>   lat=86.84681385755539 lon=-8.837449550628662    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30185 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=29929    [junit4]   1>   lat=86.84285902418196 lon=-9.196635894477367    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30457 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=30201    [junit4]   1>   lat=86.85951378196478 lon=-9.43030072376132    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30510 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=30254    [junit4]   1>   lat=86.87674303539097 lon=-9.011097270995378    [junit4]   1>   deleted?=false    [junit4]   1> t1: id=30541 should match but did not    [junit4]   1>   small=true query=bkdpointinpolygonquery: field=point: points: [-9.594408497214317, 86.83882305398583] [-9.594408497214317, 86.8827043287456] [-8.752231243997812, 86.8827043287456] [-8.752231243997812, 86.83882305398583] [-9.594408497214317, 86.83882305398583]  docid=30285    [junit4]   1>   lat=86.8406930565834 lon=-9.17652016505599    [junit4]   1>   deleted?=false    [junit4]   2> 1? 01, 2016 8:21:44 ?? com.carrotsearch.randomizedtesting.randomizedrunner$queueuncaughtexceptionshandler uncaughtexception    [junit4]   2> warning: uncaught exception in thread: thread[t1,5,tgrp-testbkdtree]    [junit4]   2> java.lang.assertionerror: some hits were wrong    [junit4]   2>        at __randomizedtesting.seedinfo.seed([17908caa7fd53e04]:0)    [junit4]   2>        at org.junit.assert.fail(assert.java:93)    [junit4]   2>        at org.apache.lucene.util.basegeopointtestcase$verifyhits.test(basegeopointtestcase.java:547)    [junit4]   2>        at org.apache.lucene.util.basegeopointtestcase$2._run(basegeopointtestcase.java:753)    [junit4]   2>        at org.apache.lucene.util.basegeopointtestcase$2.run(basegeopointtestcase.java:618)    [junit4]   2>     [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testbkdtree -dtests.method=testrandommedium -dtests.seed=17908caa7fd53e04 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=zh_hk -dtests.timezone=europe/jersey -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   7.46s | testbkdtree.testrandommedium <<<    [junit4]    > throwable #1: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=25, name=t1, state=runnable, group=tgrp-testbkdtree]    [junit4]    >        at __randomizedtesting.seedinfo.seed([17908caa7fd53e04:aa4ebb023eb05d62]:0)    [junit4]    > caused by: java.lang.assertionerror: some hits were wrong    [junit4]    >        at __randomizedtesting.seedinfo.seed([17908caa7fd53e04]:0)    [junit4]    >        at org.apache.lucene.util.basegeopointtestcase$verifyhits.test(basegeopointtestcase.java:547)    [junit4]    >        at org.apache.lucene.util.basegeopointtestcase$2._run(basegeopointtestcase.java:753)    [junit4]    >        at org.apache.lucene.util.basegeopointtestcase$2.run(basegeopointtestcase.java:618)    [junit4]   2> note: test params are: codec=asserting(lucene54): {}, docvalues:{}, sim=randomsimilarityprovider(querynorm=true,coord=crazy): {}, locale=zh_hk, timezone=europe/jersey    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=381269328,total=513802240    [junit4]   2> note: all tests run in this jvm: [testbkdtree]    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testbkdtree -dtests.seed=17908caa7fd53e04 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=zh_hk -dtests.timezone=europe/jersey -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   0.00s | testbkdtree (suite) <<<    [junit4]    > throwable #1: java.lang.assertionerror: the test or suite printed 11178 bytes to stdout and stderr, even though the limit was set to 8192 bytes. increase the limit with @limit, ignore it completely with @suppresssysoutchecks or run with -dtests.verbose=true    [junit4]    >        at __randomizedtesting.seedinfo.seed([17908caa7fd53e04]:0)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4] completed [1/1 (1!)] in 7.90s, 1 test, 1 failure, 1 error <<< failures!    [junit4]     [junit4]     [junit4] tests with failures [seed: 17908caa7fd53e04]:    [junit4]   - org.apache.lucene.bkdtree.testbkdtree.testrandommedium    [junit4]   - org.apache.lucene.bkdtree.testbkdtree (suite)    [junit4]     [junit4]     [junit4] jvm j0:     1.00 ..     9.75 =     8.75s    [junit4] execution time total: 9.76 sec.    [junit4] tests summary: 1 suite, 1 test, 1 suite-level error, 1 error",
        "label": 33
    },
    {
        "text": "nullpointerexception thrown by equals method in spanorquery part of our code utilizes the equals method in spanorquery and, in certain cases (details to follow, if necessary), a nullpointerexception gets thrown as a result of the string \"field\" being null. after applying the following patch, the problem disappeared: index: src/java/org/apache/lucene/search/spans/spanorquery.java =================================================================== \u2014 src/java/org/apache/lucene/search/spans/spanorquery.java (revision 465065) +++ src/java/org/apache/lucene/search/spans/spanorquery.java (working copy) @@ -121,7 +121,8 @@ final spanorquery that = (spanorquery) o; if (!clauses.equals(that.clauses)) return false; if (!field.equals(that.field)) return false; + if (field != null && !field.equals(that.field)) return false; + if (field == null && that.field != null) return false; return getboost() == that.getboost(); }",
        "label": 38
    },
    {
        "text": "add create attributefactory  to tokenizerfactory and subclasses with ctors taking attributefactory  and remove tokenizer's and subclasses' ctors taking attributesource all tokenizer implementations have a constructor that takes a given attributesource as parameter (lucene-1826). these should be removed. tokenizerfactory does not provide an api to create tokenizers with a given attributefactory, but quite a few tokenizers have constructors that take an attributefactory. tokenizerfactory should add a create(attributefactory) method, as should subclasses for tokenizers with attributefactory accepting ctors.",
        "label": 47
    },
    {
        "text": "lucene should not call system out println we seem to have accumulated a few random sops... eg, pairoutputs.java (oal.util.fst) and multidocvalues.java, at least. can we somehow detect (eg, have a test failure) if we accidentally leave errant system.out.println's (leftover from debugging)...?",
        "label": 33
    },
    {
        "text": "lazy field loading breaks backward compat document.getfield() and document.getfields() have changed in a non backward compatible manner. simple code like the following no longer compiles: field x = mydoc.getfield(\"x\");",
        "label": 55
    },
    {
        "text": "directory createoutput and openinput should take an iocontext today for merging we pass down a larger readbuffersize than for searching because we get better performance. i think we should generalize this to a class (iocontext), which would hold the buffer size, but then could hold other flags like direct (bypass os's buffer cache), sequential, etc. then, we can make the directiolinuxdirectory fully usable because we would only use direct/sequential during merging. this will require fixing how iw pools readers, so that a reader opened for merging is not then used for searching, and vice/versa. really, it's only all the open file handles that need to be different \u2013 we could in theory share del docs, norms, etc, if that were somehow possible.",
        "label": 54
    },
    {
        "text": "when tests fail  sometimes the testmethod in 'reproduce with' is null an example is the recent fail: https://builds.apache.org/job/lucene-3.x/680/ it would be better to not populate -dtestmethod with anything here...",
        "label": 11
    },
    {
        "text": "add n gram tokenizers to contrib analyzers it would be nice to have some n-gram-capable tokenizers in contrib/analyzers. patch coming shortly.",
        "label": 38
    },
    {
        "text": "sortfield auto doesn't work with long this is actually the same as lucene-463 but i cannot find a way to re-open that issue. i'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it. the problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. with the patch the following parsers are used in this order: int, long, float.",
        "label": 15
    },
    {
        "text": "filteringtokenfilter is double incrementing the position increment in incrementtoken the following code from filteringtokenfilter#incrementtoken() seems wrong.     if (enablepositionincrements) {       int skippedpositions = 0;       while (input.incrementtoken()) {         if (accept()) {           if (skippedpositions != 0) {             posincratt.setpositionincrement(posincratt.getpositionincrement() + skippedpositions);           }           return true;         }         skippedpositions += posincratt.getpositionincrement();       }     } else { the skippedpositions variable should probably be incremented by 1 instead of posincratt.getpositionincrement(). as it is, it seems to be double incrementing, which is a problem if your data is full of stop words and your position increment integer overflows.",
        "label": 53
    },
    {
        "text": "facets should index drill down fields using docs only today we index as docs_and_positions, which is necessary because we stuff the payload into one of those tokens. if we indexed under two fields instead, then we could make the drill-down field docs_only. but ... once/if we cutover to doc values then we could use one field again.",
        "label": 43
    },
    {
        "text": "improved javadocs for priorityqueue lessthan it kills me that i have to inspect the code every time i implement a priorityqueue.",
        "label": 46
    },
    {
        "text": "minimize autoboxing in numericfield dicif you already have a integer/long/double etc. numericfield.setlongvalue(long) causes an unnecessary auto-unbox. actually, since internal to setlongvalue there is: fieldsdata = long.valueof(value); then, there is an explicit box anyway, so this makes setlongvalue(long) with an auto-box of long roughly the same as setlongvalue(long), but better if you started with a long. long being replaceable with integer, float, double etc.",
        "label": 46
    },
    {
        "text": "high frequency terms phrases at the index level we should be able to find the all the high frequency terms/phrases ( where frequency is the search criteria / benchmark)",
        "label": 38
    },
    {
        "text": "deprecation of autocommit in leads to compile problems  when autocommit should be false i am currently changing my code to be most compatible with 2.4. i switched on deprecation warnings and got a warning about the autocommit parameter in indexwriter constructors. my code should use autocommit=false, so i want to use the new semantics. the default of indexwriter is still autocommit=true. my problem now: how to disable autocommit whithout deprecation warnings? maybe, the \"old\" constructors, that are deprecated should use autocommit=true. but there are new constructors with this \"indexwriter.maxfieldlength mfl\" in it, that appear new in 2.4 but are deprecated: indexwriter(directory d, boolean autocommit, analyzer a, boolean create, indexdeletionpolicy deletionpolicy, indexwriter.maxfieldlength mfl) deprecated. this will be removed in 3.0, when autocommit will be hardwired to false. use indexwriter(directory,analyzer,boolean,indexdeletionpolicy,maxfieldlength) instead, and call commit() when needed. what the hell is meant by this, a new constructor that is deprecated? and the hint is wrong. if i use the other constructor in the warning, i get autocommit=true. there is something completely wrong. it should be clear, which constructors set autocommit=true, which set it per default to false (perhaps new ones), and the deprecated text is wrong, if autocommit does not default to false.",
        "label": 33
    },
    {
        "text": "make cfs appendable currently cfs is created once all files are written during a flush / merge. once on disk the files are copied into the cfs format which is basically a unnecessary for some of the files. we can at any time write at least one file directly into the cfs which can save a reasonable amount of io. for instance stored fields could be written directly during indexing and during a codec flush one of the written files can be appended directly. this optimization is a nice sideeffect for lucene indexing itself but more important for docvalues and lucene-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once lucene-3216 is resolved.",
        "label": 46
    },
    {
        "text": "new queryparser fails to set auto rewrite for multi term queries the old queryparser defaults to constant score rewrite for prefix,fuzzy,wildcard,termrangequery, but the new one seems not to.",
        "label": 32
    },
    {
        "text": "infix suggesters' highlighting  alltermsrequired options are hardwired and not configurable for non contextual lookup highlighting and alltermsrequired are hardwired in analyzinginfixsuggester for non-contextual lookup (via lookup) see true, true below: analyzinginfixsuggester.java (extends lookup.java) public list<lookupresult> lookup(charsequence key, set<bytesref> contexts, boolean onlymorepopular, int num) throws ioexception {     return lookup(key, contexts, num, true, true); } /** lookup, without any context. */ public list<lookupresult> lookup(charsequence key, int num, boolean alltermsrequired, boolean dohighlight) throws ioexception {     return lookup(key, null, num, alltermsrequired, dohighlight);   } lookup.java public list<lookupresult> lookup(charsequence key, boolean onlymorepopular, int num) throws ioexception {     return lookup(key, null, onlymorepopular, num); } the above means the majority of the current infix suggester lookup always return highlighted results with alltermsrequired in effect. there is no way to change this despite the options and improvement of lucene-6050, made to incorporate boolean lookup clauses (must/should). this shortcoming has also been reported in solr-6648. the suggesters (analyzinginfixsuggester, blendedinfixsuggester) should provide a proper mechanism to set defaults for highlighting and \"alltermsrequired\", e.g. in constructors (and in solr factories, thus configurable via solrconfig.xml).",
        "label": 52
    },
    {
        "text": "correct minor javadoc mistakes in core  javadoc access private patches token.java and termvectorsreader.java",
        "label": 38
    },
    {
        "text": "testsimpletextpointformat testwithexceptions  failure my jenkins found a reproducible seed:    [junit4] suite: org.apache.lucene.codecs.simpletext.testsimpletextpointformat    [junit4] ignor/a 0.01s j5 | testsimpletextpointformat.testmergestability    [junit4]    > assumption #1: merge is not stable    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testsimpletextpointformat -dtests.method=testwithexceptions -dtests.seed=73b4a097f1853fc0 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=ar-ye -dtests.timezone=america/curacao -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   0.24s j5 | testsimpletextpointformat.testwithexceptions <<<    [junit4]    > throwable #1: java.lang.runtimeexception: mockdirectorywrapper: cannot close: there are still open files: {_0_bkd_0.tmp=1}    [junit4]    >  at __randomizedtesting.seedinfo.seed([73b4a097f1853fc0:d1e73ceb7e0b703a]:0)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:771)    [junit4]    >  at org.apache.lucene.index.basepointformattestcase.testwithexceptions(basepointformattestcase.java:253)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.lang.runtimeexception: unclosed indexoutput: _0_bkd_0.tmp    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.addfilehandle(mockdirectorywrapper.java:659)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.createtempoutput(mockdirectorywrapper.java:631)    [junit4]    >  at org.apache.lucene.store.filterdirectory.createtempoutput(filterdirectory.java:78)    [junit4]    >  at org.apache.lucene.store.trackingdirectorywrapper.createtempoutput(trackingdirectorywrapper.java:51)    [junit4]    >  at org.apache.lucene.store.trackingdirectorywrapper.createtempoutput(trackingdirectorywrapper.java:51)    [junit4]    >  at org.apache.lucene.util.bkd.offlinepointwriter.<init>(offlinepointwriter.java:39)    [junit4]    >  at org.apache.lucene.util.bkd.bkdwriter.switchtooffline(bkdwriter.java:198)    [junit4]    >  at org.apache.lucene.util.bkd.bkdwriter.add(bkdwriter.java:217)    [junit4]    >  at org.apache.lucene.codecs.simpletext.simpletextpointwriter$2.visit(simpletextpointwriter.java:164)    [junit4]    >  at org.apache.lucene.index.pointvalueswriter$1.intersect(pointvalueswriter.java:75)    [junit4]    >  at org.apache.lucene.codecs.simpletext.simpletextpointwriter.writefield(simpletextpointwriter.java:157)    [junit4]    >  at org.apache.lucene.index.pointvalueswriter.flush(pointvalueswriter.java:66)    [junit4]    >  at org.apache.lucene.index.defaultindexingchain.writepoints(defaultindexingchain.java:172)    [junit4]    >  at org.apache.lucene.index.defaultindexingchain.flush(defaultindexingchain.java:107)    [junit4]    >  at org.apache.lucene.index.documentswriterperthread.flush(documentswriterperthread.java:425)    [junit4]    >  at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:502)    [junit4]    >  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:614)    [junit4]    >  at org.apache.lucene.index.indexwriter.doflush(indexwriter.java:3099)    [junit4]    >  at org.apache.lucene.index.indexwriter.flush(indexwriter.java:3074)    [junit4]    >  at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1727)    [junit4]    >  at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1707)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.forcemerge(randomindexwriter.java:422)    [junit4]    >  at org.apache.lucene.index.basepointformattestcase.verify(basepointformattestcase.java:663)    [junit4]    >  at org.apache.lucene.index.basepointformattestcase.testwithexceptions(basepointformattestcase.java:222)    [junit4]    >  ... 36 more    [junit4] ignor/a 0.00s j5 | testsimpletextpointformat.testrandombinarybig    [junit4]    > assumption #1: too slow with simpletext    [junit4]   2> note: leaving temporary files on disk at: /var/lib/jenkins/jobs/lucene-solr-nightly-5.x-java8/workspace/lucene/build/codecs/test/j5/temp/lucene.codecs.simpletext.testsimpletextpointformat_73b4a097f1853fc0-001    [junit4]   2> note: test params are: codec=asserting(lucene60): {}, docvalues:{}, sim=randomsimilarity(querynorm=true,coord=crazy): {}, locale=ar-ye, timezone=america/curacao    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=415913304,total=536346624    [junit4]   2> note: all tests run in this jvm: [testsimpletextpostingsformat, testfixedgappostingsformat, testsimpletextpointformat]    [junit4] completed [17/22 (1!)] on j5 in 47.49s, 16 tests, 1 error, 2 skipped <<< failures!",
        "label": 33
    },
    {
        "text": "deadlock while optimize sometimes after starting the thread with the indexer, the thread will hang in the following threads. thread lucene merge thread #0 (ausgesetzt) indexwriter.commitmerge(mergepolicy$onemerge, segmentmerger, int) line: 3751 indexwriter.mergemiddle(mergepolicy$onemerge) line: 4240 indexwriter.merge(mergepolicy$onemerge) line: 3877 concurrentmergescheduler.domerge(mergepolicy$onemerge) line: 205 concurrentmergescheduler$mergethread.run() line: 260 thread [indexer] (ausgesetzt) object.wait(long) line: not available [native methode] indexwriter.dowait() line: 4491 indexwriter.optimize(int, boolean) line: 2268 indexwriter.optimize(boolean) line: 2203 indexwriter.optimize() line: 2183 indexer.run() line: 263 if you need more informations, please let me know.",
        "label": 33
    },
    {
        "text": "conjunctionscorer tune up i just recently ran a load test on the latest code from lucene , which is using a new booleanscore and noticed the conjunctionscorer was crunching through objects , especially while sorting as part of the skipto call. it turns a linked list into an array, sorts the array, then converts the array back to a linked list for further processing by the scoring engines below. 'm not sure if anyone else is experiencing this as i have a very large index (> 4 million items) and i am issuing some heavily nested queries anyway, i decide to change the link list into an array and use a first and last marker to \"simulate\" a linked list. this scaled much better during my load test as the java gargbage collector was less - umm - virulent",
        "label": 55
    },
    {
        "text": "add more support to validate the  dbootclasspath given for javadocs generate when simon created the nice looking javadocs for lusolr 4.6, he just copypasted the command line from http://wiki.apache.org/lucene-java/howtogeneratenicejavadocs unfortunately this does not work with applejdk6, because it has no rt.jar! the rt.jar file is there in a completely different directory and is named classes.jar. i had a similar problem when i wanted to regenerate the javadocs on my linux box, but specified -dbootclasspath with shell specials (e.g., ~ for homedir). this patch will assist the user and will \"validate\" the given bootclasspath, so it points to a jar file that actually contains the runtime. also to make life easier, instead of -dbootclasspath you can set -dbootjdk to the jdk homefolder (same like java_home) and ant will figure out if it is apple or oracle or maybe only a jre. in the meantime, i regenerated the 4.6 javadocs with correct bootclasspath.",
        "label": 53
    },
    {
        "text": "possible security issue when parsing xml documents containing external entity references it appears that in querytemplatemanager.java lines 149 and 198 and in domutils.java line 204 xml is parsed without disabling external entity references (xxe). this is described in http://cwe.mitre.org/data/definitions/611.html and possible mitigations are listed here: https://www.owasp.org/index.php/xml_external_entity_(xxe)_prevention_cheat_sheet all recent versions of lucene are affected.",
        "label": 53
    },
    {
        "text": "testscoreddocidsutils testwithdeletions test failure ant test -dtestcase=testscoreddocidsutils -dtestmethod=testwithdeletions -dtests.seed=-2216133137948616963:2693740419732273624 -dtests.multiplier=5 in general, on both 3.x and trunk, if you run this test with -dtests.iter=100 it tends to fail 2% of the time.",
        "label": 43
    },
    {
        "text": "rework of the terminfosreader class to remove the terms  terminfos  and the index pointer long  and create a more memory efficient data structure  basically packing those three arrays into a byte array with an int array as an index offset. the performance benefits are stagering on my test index (of size 6.2 gb, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size. from 291.5 mb to 49.7 mb. the random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full gc's on my jvm were made 7 times faster. i have already performed the work and am offering this code as a patch. currently all test in the trunk pass with this new code enabled. i did write a system property switch to allow for the original implementation to be used as well. -dorg.apache.lucene.index.terminfosreader=default or small i have also written a blog about this patch here is the link. http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html",
        "label": 33
    },
    {
        "text": "add a tokenfilter for icu transforms i pulled the icutransformfilter out of lucene-1488 and create an issue for it here. this is a tokenfilter that applies an icu transliterator, which is a context-sensitive way to transform text. these are typically rule-based and you can use ones included with icu (such as traditional-simplified) or you can make your own from your own set of rules. user's guide: http://userguide.icu-project.org/transforms/general rule tutorial: http://userguide.icu-project.org/transforms/general/rules",
        "label": 40
    },
    {
        "text": "missing dependencies in two generated maven pom files there are some missing dependencies in generated maven pom.xml files (benchmark and highlighter)",
        "label": 33
    },
    {
        "text": "indexwriter does not properly account for the ram consumed by pending deletes indexwriter, with autocommit false, is able to carry buffered deletes for quite some time before materializing them to docids (thus freeing up ram used). it's only on triggering a merge (or, commit/close) that the deletes are materialized and the ram is freed. i expect this in practice is a smallish amount of ram, but we should still fix it. i don't have a patch yet so if someone wants to grab this, feel free!!",
        "label": 33
    },
    {
        "text": "pass livedocs bits down in scorercontext  instead of weights pulling from the reader spinoff from lucene-1536, this would allow filters to work in a more flexible way (besides just cleaning up)",
        "label": 40
    },
    {
        "text": "deadlock in documentswriterflushcontrol hi all, we have an obvious deadlock between a \"mayberefreshindexjob\" thread calling referencemanager.mayberefresh(referencemanager.java:204) and a \"rebuildindexjob\" thread calling indexwriter.deleteall(indexwriter.java:2065). lucene wants to flush in the \"mayberefreshindexjob\" thread trying to intrinsically lock the indexwriter instance at documentswriterperthread.java:563 before notifyall()ing the flush. simultaneously the \"rebuildindexjob\" thread who already intrinsically locked the indexwriter instance at indexwriter#deleteall wait()s at documentswriterflushcontrol.java:245 for the flush forever causing a deadlock. \"mayberefreshindexjob thread - 2\" daemon prio=10 tid=0x00007f8fe4006000 nid=0x1ac2 waiting for monitor entry [0x00007f8fa7bf7000]    java.lang.thread.state: blocked (on object monitor)  at org.apache.lucene.index.indexwriter.usecompoundfile(indexwriter.java:2223)  - waiting to lock <0x00000000f1c00438> (a org.apache.lucene.index.indexwriter)  at org.apache.lucene.index.documentswriterperthread.sealflushedsegment(documentswriterperthread.java:563)  at org.apache.lucene.index.documentswriterperthread.flush(documentswriterperthread.java:533)  at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:422)  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:559)  at org.apache.lucene.index.indexwriter.getreader(indexwriter.java:365)  - locked <0x00000000f1c007d0> (a java.lang.object)  at org.apache.lucene.index.standarddirectoryreader.doopenfromwriter(standarddirectoryreader.java:270)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:245)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:235)  at org.apache.lucene.index.directoryreader.openifchanged(directoryreader.java:170)  at org.apache.lucene.search.searchermanager.refreshifneeded(searchermanager.java:118)  at org.apache.lucene.search.searchermanager.refreshifneeded(searchermanager.java:58)  at org.apache.lucene.search.referencemanager.domayberefresh(referencemanager.java:155)  at org.apache.lucene.search.referencemanager.mayberefresh(referencemanager.java:204)  at jobs.mayberefreshindexjob.timeout(mayberefreshindexjob.java:47) \"rebuildindexjob thread - 1\" prio=10 tid=0x00007f903000a000 nid=0x1a38 in object.wait() [0x00007f9037dd6000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000f1c0c240> (a org.apache.lucene.index.documentswriterflushcontrol)  at java.lang.object.wait(object.java:503)  at org.apache.lucene.index.documentswriterflushcontrol.waitforflush(documentswriterflushcontrol.java:245)  - locked <0x00000000f1c0c240> (a org.apache.lucene.index.documentswriterflushcontrol)  at org.apache.lucene.index.documentswriter.abort(documentswriter.java:235)  - locked <0x00000000f1c05370> (a org.apache.lucene.index.documentswriter)  at org.apache.lucene.index.indexwriter.deleteall(indexwriter.java:2065)  - locked <0x00000000f1c00438> (a org.apache.lucene.index.indexwriter)  at jobs.rebuildindexjob.buildindex(rebuildindexjob.java:102)",
        "label": 46
    },
    {
        "text": "very inefficient implementation of multitermdocs skipto in our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average. we would consistently see this drop anytime an index went from an optimized state to an unoptimized state. i tracked down the issue to the implementation of multitermdocs.skipto function (found in multireader.java). optimized indexes do not use this class during search but unoptimized indexes do. the comment on this function even explicitly states 'as yet unoptimized implementation.' it was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not. so i re-implemented the function very similar to how the multitermdocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished. we have already put the new jar onto our production machines. here is my implementation of skipto, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it. /** much more optimized implementation. could be optimized fairly easily to skip entire segments */ public boolean skipto(int target) throws ioexception unknown macro: { if (current != null && current.skipto(target-base)) { return true; } else if (pointer < readers.length) { base = starts[pointer]; current = termdocs(pointer++); return skipto(target); } else return false; }",
        "label": 55
    },
    {
        "text": "fastvectorhighlighter  highlighted term is out of alignment in multi valued not analyzed field ",
        "label": 26
    },
    {
        "text": "cachingwrapperfilter crashes if you call both bits  and getdocidset  cachingwrapperfilter uses only a single cache, so calling bits() after calling getdocidset() will result in a type error. additionally, more code than is necessary is wrapped in the @synchronized blocks.",
        "label": 33
    },
    {
        "text": "charactercache   references deleted charactercache is deprecated by character.valueof(c) . hence the latter is chosen over the former.",
        "label": 53
    },
    {
        "text": "payloadtermquery's explain is broken when span score is not included when setting includespanscore to false with payloadtermquery, the explain is broken.",
        "label": 40
    },
    {
        "text": "spellchecker file descriptor leak   no way to close the indexsearcher used by spellchecker internally i can't find any way to close the indexsearcher (and indexreader) that is being used by spellchecker internally. i've worked around this issue by keeping a single spellchecker open for each index, but i'd really like to be able to close it and reopen it on demand without leaking file descriptors. could we add a close() method to spellchecker that will close the indexsearcher and null the reference to it? and perhaps add some code that reopens the searcher if the reference to it is null? or would that break thread safety of spellchecker? the attached patch adds a close method but leaves it to the user to call setspellindex to reopen the searcher if desired.",
        "label": 46
    },
    {
        "text": "make memoryindex more memory efficient currently memoryindex uses bytesref objects to represent terms and holds an int[] per term per field to represent postings. for highlighting this creates a ton of objects for each search that 1. need to be gced and 2. can't be reused.",
        "label": 46
    },
    {
        "text": "matchalldocsquerynode tostring  creates invalid xml tag matchalldocsquerynode.tostring() returns \"<matchalldocs field='' term=''>\", which is inavlid xml should read \"<matchalldocs field='' term='' />.",
        "label": 40
    },
    {
        "text": "fastvectorhighlighter  aioobe occurs if one phrasequery is contained by another phrasequery i'm very sorry but this is another one. if q=\"a b c d\" or \"b c\", then arrayindexoutofboundsexception occurs in fieldquery.checkoverlap(). i'm working on this and fix with test case soon to be posted. thank you for your patient!",
        "label": 29
    },
    {
        "text": "explore morfologik integration dawid weiss mentioned on lucene-2298 that there is another polish stemmer available: http://sourceforge.net/projects/morfologik/ this works differently than lucene-2298, and ideally would be another option for users.",
        "label": 11
    },
    {
        "text": "remove code that potentially rethrows checked exceptions from methods that don't declare them  sneaky throw  hack  for a long time i considered the \"sneaky\" throw hack to be a nice way of coding around some of java's limitations (especially with invoking methods via reflection or method handles), but with time i started to see how it can be potentially dangerous and is nearly always confusing. if you have a java method and its signature doesn't indicate the possibility of a checked exception you, as a programmer, simply don't expect it to happen. never. so, for example, you could write: try {  luceneapi(); } catch (runtimeexception | error e) {   // handle unchecked exceptions here. } and consider the code above to be absolutely bullet-proof in terms of handling exceptions. unfortunately with sneaky throws anywhere in the \"luceneapi\" this is no longer the case \u2013 you can receive a checked exception that will simply fall through and hit some code frame above. so i suggest we remove sneaky throw from the core entirely. it only exists in two places \u2013 private methods inside snowball programs invoked via method handles (these don't even declare checked exceptions so i assume they can't occur) and attributefactory \u2013 here there is a real possibility somebody could declare an attribute class's constructor that does throw an unchecked exception. in that case i think it is more reasonable to wrap it in a runtimeexception than rethrow it as original. alternatively, we can modify the signature of createattributeinstance and getstaticimplementation to declare some kind of checked exception (either a wrapper or even a throwable), but i see little reason for it and it'd change the api.",
        "label": 11
    },
    {
        "text": "improve default turkishanalyzer add a tokenfilter that strips characters after an apostrophe (including the apostrophe itself).",
        "label": 40
    },
    {
        "text": "spanscorer's assert message strings should reference spans tostring  spanscorer.setfreqcurrentdoc has a bunch of assert statements, and they refer to this.tostring(). i'm pretty confident the intention was for this to actually be spans.tostring(), not \"this\" which is a spanscorer that doesn't even have a custom tostring. it was probably correct once but after some refactoring of spans got messed up, probably in lucene-6919 (lucene 5.5).",
        "label": 10
    },
    {
        "text": "remove version references from the versioned website from the website: lucene java 3.1 has the following minimum requirements: you already had to select the specific version to get to the versioned site, so i think we should consider removing these redundant version references to prevent the possibility they are out of date.",
        "label": 40
    },
    {
        "text": "geo3d test failure  test point is contained by shape but outside the xyzbounds reproduces for me on branch_7x.  /cc karl wright  ignacio vera reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=252b55c41a78f987 -dtests.slow=true -dtests.badapples=true -dtests.locale=th -dtests.timezone=america/virgin -dtests.asserts=true -dtests.file.encoding=utf-8 [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint    [junit4]   1>     doc=639 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=-1.077431832267001, lon=3.141592653589793([x=-0.47288721079787505, y=5.791198090613375e-17, z=-0.8794340737031547])]    [junit4]   1>       quantized=[x=-0.47288721059145067, y=2.3309121299774915e-10, z=-0.8794340734858216]    [junit4]   1>     doc=1079 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=-1.074298280522397, lon=-3.141592653589793([x=-0.4756448135017662, y=-5.824968983859777e-17, z=-0.8779556514050441])]    [junit4]   1>       quantized=[x=-0.4756448134355703, y=-2.3309121299774915e-10, z=-0.8779556514433299]    [junit4]   1>   shape=geocomplexpolygon: {planetmodel=planetmodel.wgs84, number of shapes=1, address=5b34ab34, testpoint=[lat=-0.9074319066955279, lon=2.1047077826887393e-11([x=0.6151745825332513, y=1.2947627315700302e-11, z=-0.7871615107396388])], testpointinset=true, shapes={ {[lat=0.12234154783984401, lon=2.9773900430735544e-11([x=0.9935862314832985, y=2.9582937525533484e-11, z=0.12216699617265761])], [lat=-1.1812619187738946, lon=0.0([x=0.3790909950565304, y=0.0, z=-0.9234617794363308])], [lat=-1.5378336326638269, lon=-2.1777686687777411e-97([x=0.03288309726634029, y=-7.161177895900688e-99, z=-0.9972239126272725])]}}    [junit4]   1>   bounds=xyzbounds: [xmin=0.03288309626634029 xmax=1.0011188549924792 ymin=-1.0e-9 ymax=1.029686850221785e-9 zmin=-0.9972239136272725 zmax=0.12216699717265761]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=252b55c41a78f987 -dtests.slow=true -dtests.badapples=true -dtests.locale=th -dtests.timezone=america/virgin -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 0.16s | testgeo3dpoint.testgeo3drelations <<<    [junit4]    > throwable #1: java.lang.assertionerror: invalid bounds for shape=geocomplexpolygon: {planetmodel=planetmodel.wgs84, number of shapes=1, address=5b34ab34, testpoint=[lat=-0.9074319066955279, lon=2.1047077826887393e-11([x=0.6151745825332513, y=1.2947627315700302e-11, z=-0.7871615107396388])], testpointinset=true, shapes={ {[lat=0.12234154783984401, lon=2.9773900430735544e-11([x=0.9935862314832985, y=2.9582937525533484e-11, z=0.12216699617265761])], [lat=-1.1812619187738946, lon=0.0([x=0.3790909950565304, y=0.0, z=-0.9234617794363308])], [lat=-1.5378336326638269, lon=-2.1777686687777411e-97([x=0.03288309726634029, y=-7.161177895900688e-99, z=-0.9972239126272725])]}}    [junit4]    >  at __randomizedtesting.seedinfo.seed([252b55c41a78f987:955428509535571b]:0)    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:259)    [junit4]    >  at java.lang.thread.run(thread.java:748)    [junit4]   2> note: test params are: codec=asserting(lucene70), sim=randomsimilarity(querynorm=false): {}, locale=th, timezone=america/virgin    [junit4]   2> note: linux 4.15.0-29-generic amd64/oracle corporation 1.8.0_161 (64-bit)/cpus=4,threads=1,free=298939008,total=313524224    [junit4]   2> note: all tests run in this jvm: [testgeo3dpoint]    [junit4] completed [1/1 (1!)] in 0.62s, 1 test, 1 failure <<< failures!",
        "label": 19
    },
    {
        "text": "add suggester that uses shortest path wfst instead of buckets currently the fst suggester (really an fsa) quantizes weights into buckets (e.g. single byte) and puts them in front of the word. this makes it fast, but you lose granularity in your suggestions. lately the question was raised, if you build lucene's fst with positiveintoutputs, does it behave the same as a tropical semiring wfst? in other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the best suggestion (with the highest score). this means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier.",
        "label": 40
    },
    {
        "text": "suspicious condition hi please, look this code fragment:     else if (q1 instanceof andquerynode)       op = andoperation.q1;     else if (q1 instanceof andquerynode)       op = andoperation.q2; (q1 instanceof andquerynode) is checked twice. probably it should be:     else if (q1 instanceof andquerynode)       op = andoperation.q1;     else if (q2 instanceof andquerynode)       op = andoperation.q2; this possible defect found by appchecker",
        "label": 53
    },
    {
        "text": "fieldcache should include a bitset for matching docs the fieldcache returns an array representing the values for each doc. however there is no way to know if the doc actually has a value. this should be changed to return an object representing the values and a bitset for all valid docs.",
        "label": 42
    },
    {
        "text": "mockfilesystemtestcase testuri should be improved to handle cases where os jvm cannot create non ascii filenames ant test -dtestcase=testverbosefs -dtests.method=testuri -dtests.file.encoding=utf-8 fails (for example) with 'oracle corporation 1.8.0_45 (64-bit)' when the default sun.jnu.encoding system property is (for example) ansi_x3.4-1968 [details to follow]",
        "label": 53
    },
    {
        "text": "collationkeyfilter  convert tokens into collationkeys encoded using indexablebinarystringtools converts each token into its collationkey using the provided collator, and then encodes the collationkey with indexablebinarystringtools, to allow it to be stored as an index term. this will allow for efficient range searches and sorts over fields that need collation for proper ordering.",
        "label": 33
    },
    {
        "text": "ioutils spins doesn't work for nvme drives nvme is the faster (than ahci) protocol for newer ssds that plug into the pcie bus. i just built a new beast box with one of these drives, and the partition is named /dev/nvme0n1p1 while the device is /dev/nvme0n1 by linux - this also appears in /sys/block with rotational=0. i think steve rowe also has an nvme drive ... uwe schindler (who got the box working for me: thank you!!!) has ideas on how to fix it!",
        "label": 53
    },
    {
        "text": "the difference between totalhits and scoredocs length of topdocs topdocs hits = searcher.search(query, n); hits.totalhits is not equal to hits.scoredocs.length in lucene whose version is 4.3.0 when i write the fllowing codes: for (int i = 0; i < hits.totalhits; i++) { int id = hits.scoredocs[i].doc; //... } it may throw arrayindexoutofboundsexception, but in version 4.0.0, the value of hits.totalhits and the value of hits.scoredocs.length are qual! and i think they should be qual! thank you!",
        "label": 43
    },
    {
        "text": "similarity score deprecated method   javadoc reference   similaritydelegator old method public float scorepayload(string fieldname, byte [] payload, int offset, int length) has been deprecated by - public float scorepayload(int docid, string fieldname, int start, int end, byte [] payload, int offset, int length) references in payloadnearquery (javadoc) changed. also - similaritydelegator overrides the new method as opposed to the (deprecated) old one.",
        "label": 53
    },
    {
        "text": "mergepolicy onemerge segments should be list segmentinfo  not segmentinfos  remove vector si  subclassing from segmentinfos   more refactoring segmentinfos carries a bunch of fields beyond the list of si, but for merging purposes these fields are unused. we should cutover to list<si> instead. also segmentinfos subclasses vector<si>, this should be removed and the collections be hidden inside the class. we can add unmodifiable views on it (aslist(), asset()).",
        "label": 53
    },
    {
        "text": "revision  lucene  causes ioe  read past eof  when processing older format segmentinfo data when jvm assertion processing is disabled  at revision 949509 in org.apache.lucene.index.segmentinfo at line 155, there is the following code:       if (format > segmentinfos.format_4_0) {       // pre-4.0 indexes write a byte if there is a single norms file       assert 1 == input.readbyte();     } note that the assert statement invokes input.readbyte(). if asserts are disabled for the jvm, input.readbyte() will not be invoked, causing the following readint() to return a bogus value, and then causing an ioe during the (mistakenly entered) loop at line 165. this can occur when processing old format (format \"-9\") index data under tomcat (whose startup scripts by default do not turn on asserts). full stacktrace:   severe: java.lang.runtimeexception: java.io.ioexception: read past eof  at org.apache.solr.core.solrcore.getsearcher(solrcore.java:1066)  at org.apache.solr.core.solrcore.<init>(solrcore.java:581)  at org.apache.solr.core.corecontainer.create(corecontainer.java:431)  at org.apache.solr.core.corecontainer.load(corecontainer.java:286)  at org.apache.solr.core.corecontainer$initializer.initialize(corecontainer.java:125)  at org.apache.solr.servlet.solrdispatchfilter.init(solrdispatchfilter.java:86)  at org.apache.catalina.core.applicationfilterconfig.getfilter(applicationfilterconfig.java:275)  at org.apache.catalina.core.applicationfilterconfig.setfilterdef(applicationfilterconfig.java:397)  at org.apache.catalina.core.applicationfilterconfig.<init>(applicationfilterconfig.java:108)  at org.apache.catalina.core.standardcontext.filterstart(standardcontext.java:3800)  at org.apache.catalina.core.standardcontext.start(standardcontext.java:4450)  at org.apache.catalina.core.containerbase.addchildinternal(containerbase.java:791)  at org.apache.catalina.core.containerbase.addchild(containerbase.java:771)  at org.apache.catalina.core.standardhost.addchild(standardhost.java:526)  at org.apache.catalina.startup.hostconfig.deploywar(hostconfig.java:850)  at org.apache.catalina.startup.hostconfig.deploywars(hostconfig.java:724)  at org.apache.catalina.startup.hostconfig.deployapps(hostconfig.java:493)  at org.apache.catalina.startup.hostconfig.start(hostconfig.java:1206)  at org.apache.catalina.startup.hostconfig.lifecycleevent(hostconfig.java:314)  at org.apache.catalina.util.lifecyclesupport.firelifecycleevent(lifecyclesupport.java:119)  at org.apache.catalina.core.containerbase.start(containerbase.java:1053)  at org.apache.catalina.core.standardhost.start(standardhost.java:722)  at org.apache.catalina.core.containerbase.start(containerbase.java:1045)  at org.apache.catalina.core.standardengine.start(standardengine.java:443)  at org.apache.catalina.core.standardservice.start(standardservice.java:516)  at org.apache.catalina.core.standardserver.start(standardserver.java:710)  at org.apache.catalina.startup.catalina.start(catalina.java:583)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.catalina.startup.bootstrap.start(bootstrap.java:288)  at org.apache.catalina.startup.bootstrap.main(bootstrap.java:413) caused by: java.io.ioexception: read past eof  at org.apache.lucene.store.bufferedindexinput.refill(bufferedindexinput.java:154)  at org.apache.lucene.store.bufferedindexinput.readbyte(bufferedindexinput.java:39)  at org.apache.lucene.store.checksumindexinput.readbyte(checksumindexinput.java:40)  at org.apache.lucene.store.datainput.readint(datainput.java:76)  at org.apache.lucene.store.datainput.readlong(datainput.java:99)  at org.apache.lucene.index.segmentinfo.<init>(segmentinfo.java:165)  at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:230)  at org.apache.lucene.index.directoryreader$1.dobody(directoryreader.java:91)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:649)  at org.apache.lucene.index.directoryreader.open(directoryreader.java:87)  at org.apache.lucene.index.indexreader.open(indexreader.java:415)  at org.apache.lucene.index.indexreader.open(indexreader.java:294)  at org.apache.solr.core.standardindexreaderfactory.newreader(standardindexreaderfactory.java:38)  at org.apache.solr.core.solrcore.getsearcher(solrcore.java:1055)  ... 32 more",
        "label": 43
    },
    {
        "text": "treccontentsource should use a fixed encoding  rather than system dependent treccontentsource opens inputstreamreader w/o a fixed encoding. on windows, this means cp1252 (at least on my machine) which is ok. however, when i opened it on a linux machine w/ a default of utf-8, it failed to read the files. the patch changes it to use iso-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data). patch to follow shortly.",
        "label": 29
    },
    {
        "text": "convert precedencequeryparser to new tokenstream api adriano crestani provided a patch, that updates the pqp to use the new tokenstream api...all tests still pass. i hope this helps to keep the pqp",
        "label": 53
    },
    {
        "text": "allow use of compact docidset in cachingwrapperfilter extends cachingwrapperfilter with a protected method to determine the docidset to be cached.",
        "label": 33
    },
    {
        "text": "leverage multitermawarecomponent in query parsers multitermawarecomponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. however, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.",
        "label": 1
    },
    {
        "text": "interruptible segment merges adds the ability to indexwriter to interrupt an ongoing merge. this might be necessary when lucene is e. g. running as a service and has to stop indexing within a certain period of time due to a shutdown request. a solution would be to add a new method shutdown() to indexwriter which satisfies the following two requirements: if a merge is happening, abort it flush the buffered docs but do not trigger a merge see also discussions about this feature on java-dev: http://www.gossamer-threads.com/lists/lucene/java-dev/49008",
        "label": 32
    },
    {
        "text": "npe in nearspansunordered ispayloadavailable  using rc1 of lucene 2.4 resulted in null pointer exception with some constructed spannearqueries implementation of ispayloadavailable() (results in exception)  public boolean ispayloadavailable() {    spanscell pointer = min();    do {      if(pointer.ispayloadavailable()) {        return true;      }      pointer = pointer.next;    } while(pointer.next != null);    return false;   } \"fixed\" ispayloadavailable()  public boolean ispayloadavailable() {    spanscell pointer = min();    while (pointer != null) {      if(pointer.ispayloadavailable()) {        return true;      }      pointer = pointer.next;    }    return false;   } exception produced:   [junit] java.lang.nullpointerexception     [junit]     at org.apache.lucene.search.spans.nearspansunordered$spanscell.access$300(nearspansunordered.java:65)     [junit]     at org.apache.lucene.search.spans.nearspansunordered.ispayloadavailable(nearspansunordered.java:235)     [junit]     at org.apache.lucene.search.spans.nearspansordered.shrinktoaftershortestmatch(nearspansordered.java:246)     [junit]     at org.apache.lucene.search.spans.nearspansordered.advanceafterordered(nearspansordered.java:154)     [junit]     at org.apache.lucene.search.spans.nearspansordered.next(nearspansordered.java:122)     [junit]     at org.apache.lucene.search.spans.spanscorer.next(spanscorer.java:54)     [junit]     at org.apache.lucene.search.scorer.score(scorer.java:57)     [junit]     at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:137)     [junit]     at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:113)     [junit]     at org.apache.lucene.search.hits.getmoredocs(hits.java:113)     [junit]     at org.apache.lucene.search.hits.<init>(hits.java:80)     [junit]     at org.apache.lucene.search.searcher.search(searcher.java:50)     [junit]     at org.apache.lucene.search.searcher.search(searcher.java:40)     [junit]     at com.attivio.lucene.spanquerytest.search(spanquerytest.java:79)     [junit]     at com.attivio.lucene.spanquerytest.asserthitcount(spanquerytest.java:75)     [junit]     at com.attivio.lucene.spanquerytest.test(spanquerytest.java:67) will attach unit test that causes exception (and passes with updated ispayloadavailable())",
        "label": 33
    },
    {
        "text": "query norm and coordination factor not calculated when perfieldsimilaritywrapper is used if any kind of similarity is defined and therefore the schemasimilarityfactory is defined as global similarity the querynorm is always 1.0 the perfieldsimilaritywrapper delegates some of the methods to the desired similarity but misses to delegate public float querynorm(float valuefornormalization) instead the indexreader calls this method on the base class similarity. the result is that all scores are much higher. i created a custom similarity which extends classicsimilarity. to have the calculation fixed i did a local \"hotfix\" which always uses the default similarity. also wrong for some cases but fine in my scenario. @override public float querynorm(float valuefornormalization) { return get(\"\").querynorm(valuefornormalization); // use default similarity to calculate }",
        "label": 53
    },
    {
        "text": "functionvalues exist int  isn't returning false in cases where it should for many  math  based value sources the functionvalues class contains an exist(int doc) method with a default implementation that returns true - field based docvalues override this method as appropriate, but most of the \"function\" based subclasses in the code (typically anonymous subclasses of \"floatdocvalues\") don't override this method when wrapping other valuesources. so for example: the functionvalues returned by productfloatfunction.getvalues() will say that a value exists for any doc, even if that productfloatfunction wraps two floatfieldsources that don't exist for any docs",
        "label": 18
    },
    {
        "text": "fail smoketester if there is lucene xxxx or solr xxxx in changes txt one of these slipped into the 4.8.1 release in the solr changes. looking at trunk/4.9 in solr/changes.txt, there are quite a few solr_xxxx issues. when the underscore is used, no link to the issue is generated.",
        "label": 33
    },
    {
        "text": "fsdirectory throws accesscontrolexception unless you grant write access to the index hit this during my attempted upgrade to lucene 5.1.0. (yeah, i know 5.2.0 is out, and we'll be using that in production anyway, but the merge takes time.) various tests of ours test directory stuff against methods which the security policy won't allow tests to write to. changes in fsdirectory mean that it now demands write access to the directory. 4.10.4 permitted read-only access.",
        "label": 53
    },
    {
        "text": "add galician analyzer adds analyzer for galician, based upon \"regras do lematizador para o galego\" , and a set of stopwords created in the usual fashion. this is really just an adaptation of the portuguese rslp, so i added that too, and modified our existing hand-coded rslp-s (rslp's plural-only step) to just be a plural-only flow of rslp.",
        "label": 40
    },
    {
        "text": "add asyncfsdirectory to work around windows issues with niofs  lucene only  on lucene-4848 a new directory implementation was proposed that uses asyncfilechannel to make a sync-less directory implementation (only needed for indexinput). the problem on windows is that positional reads are impossible without overlapping (async) i/o, so filechannel in the jdk has to syncronize all reads, because they consist of an atomic seek and atomic read. asyncfsdirectoty would not have this issue, but has to take care of thread management, because you need a separate thread to get notified when the read is done. this involves overhead, but might still be better than the synchronization.",
        "label": 53
    },
    {
        "text": "regexquery matches terms the input regex doesn't actually match i was writing some unit tests for our own wrapper around the lucene regex classes, and got tripped up by something interesting. the regex \"cat.\" will match \"cats\" but also anything with \"cat\" and 1+ following letters (e.g. \"cathy\", \"catcher\", ...) it is as if there is an implicit .* always added to the end of the regex. here's a unit test for the behaviour i would expect myself: @test public void testnecessity() throws exception { file dir = new file(new file(system.getproperty(\"java.io.tmpdir\")), \"index\"); indexwriter writer = new indexwriter(dir, new standardanalyzer(), true); try { document doc = new document(); doc.add(new field(\"field\", \"cat cats cathy\", field.store.yes, field.index.tokenized)); writer.adddocument(doc); } finally { writer.close(); } indexreader reader = indexreader.open(dir); try { termenum terms = new regexquery(new term(\"field\", \"cat.\")).getenum(reader); assertequals(\"wrong term\", \"cats\", terms.term()); assertfalse(\"should have only been one term\", terms.next()); } finally { reader.close(); } } this test fails on the term check with terms.term() equal to \"cathy\". our workaround is to mangle the query like this: string fixed = string.format(\"(?:%s)$\", original);",
        "label": 33
    },
    {
        "text": "commontermsquery highfreq must not applied if lowfreq terms when a commontermsquery has high and low frequency terms, the highfreq terms boolean query is always added as a should clause, even if highfreqoccur is set to must: new commontermsquery(occur.must, occur.must,0.1); my patch sets the top level boolean query's minimum should match to 1 to ensure that the should clause must match. not sure if this is the correct approach, or if it should just add the highfreq query as a must clause instead?",
        "label": 46
    },
    {
        "text": "bring hunspell for lucene into analysis module some time ago i along with robert and uwe, wrote an stemmer which uses the hunspell algorithm. it has the benefit of supporting dictionaries for a wide array of languages. it seems to still be being used but has fallen out of date. i think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added.",
        "label": 7
    },
    {
        "text": "improve spans payload collection spin off from lucene-6308, see the comments there from around 23 march 2015.",
        "label": 2
    },
    {
        "text": "rename atomicreader to leafreader see lucene-5527 for more context: several of us seem to prefer leaf to atomic. talking from my experience, i was a bit confused in the beginning that this thing is named atomicreader, since atomic is otherwise used in java in the context of concurrency. so maybe renaming it to leaf would help remove this confusion and also carry the information that these readers are used as leaves of top-level readers?",
        "label": 41
    },
    {
        "text": "improve javadoc ",
        "label": 18
    },
    {
        "text": "static index pruning by in document term frequency  carmel pruning  this module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. the net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-n results as compared with the original index, but with increased performance. optionally, stored values and term vectors can also be removed. this functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1). as the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). note: especially phrase recall deteriorates significantly at higher threshold values. primary purpose of this class is to produce small first-tier indexes that fit completely in ram, and store these indexes using indexwriter.addindexes(indexreader[]). usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. note: if the input index is optimized (i.e. doesn't contain deletions) then the index produced via indexwriter.addindexes(indexreader[]) will preserve internal document id-s so that they are in sync with the original index. this means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. threshold values can be specified globally (for terms in all fields) using defaultthreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. keys in this map are either field names, or terms in field:text format. the precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold. a command-line tool (pruningtool) is provided for convenience. at this moment it doesn't support all functionality available through api.",
        "label": 12
    },
    {
        "text": "javacc fails to generate queryparser java when generating the java source for queryparser via the ant task 'javacc-queryparser' against subversion trunk (updated jan. 25, 2006), javacc 4.0 gives the following error: javacc-queryparser: [javacc] java compiler compiler version 4.0 (parser generator) [javacc] (type \"javacc\" with no arguments for help) [javacc] reading from file [...]/src/java/org/apache/lucene/queryparser/queryparser.jj . . . [javacc] org.javacc.parser.parseexception: encountered \"<<\" at line 754, column 3. [javacc] was expecting one of: [javacc] <string_literal> ... [javacc] \"<\" ... [javacc] [javacc] detected 1 errors and 0 warnings. build failed",
        "label": 14
    },
    {
        "text": "allow indexwriter to opt out of flushing on indexing threads today indexing / updating threads always help out flushing. experts might want indexing threads to only help flushing if flushes are falling behind. maybe we can allow an expert flag in iwc to opt out of this behavior.",
        "label": 46
    },
    {
        "text": "cleanup test tokenstreams so they are reusable many tokenstreams created in tests are not reusable. some do some really messy things which prevent their reuse so we may have to change the tests themselves. we'll target back porting this to 3x.",
        "label": 7
    },
    {
        "text": "binary packaging  lucene modules contribs that depend on jars are confusing in the binary release, i noticed lucene contribs (for example benchmark) that rely upon jar files, don't include them, nor do they have a readme telling you they depend upon them, nor is there any hint they actually have any dependencies at all! we should improve this either by including the jars you need or by including a readme.txt telling you what a particular module/contrib depends upon.",
        "label": 40
    },
    {
        "text": "elision filter for simple french analyzing if you don't wont to use stemming, standardanalyzer miss some french strangeness like elision. \"l'avion\" wich means \"the plane\" must be tokenized as \"avion\" (plane). this filter could be used with other latin language if elision exists.",
        "label": 38
    },
    {
        "text": "closeablethreadlocal should allow null objects closeablethreadlocal does not allow null objects in its get() method, but does nothing to prevent them in set(object). the comment in get() before assert v != null is irrelevant - the application might have passed null. null is an important value for analyzers. since tokenstreams (a threadlocal private member in analyzer) is not accessible by extending classes, the only way for an analyzer to reset the tokenstreams is by calling setprevioustokenstream(null). i will post a patch w/ a test",
        "label": 33
    },
    {
        "text": "finalize methods of fsdirectory fsindexinput and fsdirectory fsindexoutput try to close already closed file hi all, i found a small problem in fsdirectory: the finalize()-methods of fsdirectory.fsindexinput and fsdirectory.fsindexoutput try to close the underlying file. this is not a problem unless the file has been closed before by calling the close() method. if it has been closed before, the finalize method throws an ioexception saying that the file is already closed. usually this ioexception would go unnoticed, because the garbagecollector, which calls finalize(), just eats it. however, if i use the eclipse debugger the execution of my code will always be suspended when this exception is thrown. even though this exception probably won't cause problems during normal execution of lucene, the code becomes cleaner if we apply this small patch. might this ioexception also have a performance impact, if it is thrown very frequently? i attached the patch which applies cleanly on the current svn head. all testcases pass and i verfied with the eclipse debugger that the ioexception is not longer thrown.",
        "label": 32
    },
    {
        "text": "simplefslockfactory ignores error on deleting the lock file spinoff from here: http://www.gossamer-threads.com/lists/lucene/java-user/54438 the lock.release for simplefslockfactory ignores the return value of lockfile.delete(). i plan to throw a new lockreleasefailedexception, subclassing from ioexception, when this returns false. this is a very minor change to backwards compatibility because all methods in lucene that release a lock already throw ioexception.",
        "label": 15
    },
    {
        "text": "directspellchecker throws npe if field doesn't exist directspellchecker doesn't check that the resulting terms is null, it should return an empty list here.",
        "label": 40
    },
    {
        "text": "wildcards  ors etc inside phrase queries an extension to the default queryparser that overrides the parsing of phrasequeries to allow more complex syntax e.g. wildcards in phrase queries. the implementation feels a little hacky - this is arguably better handled in queryparser itself. this works as a proof of concept for much of the query parser syntax. examples from the junit test include: checkmatches(\"\\\"j* smyth~\\\"\", \"1,2\"); //wildcards and fuzzies are ok in phrases checkmatches(\"\\\"(jo* -john) smith\\\"\", \"2\"); // boolean logic works checkmatches(\"\\\"jo* smith\\\"~2\", \"1,2,3\"); // position logic works. checkbadquery(\"\\\"jo* id:1 smith\\\"\"); //mixing fields in a phrase is bad checkbadquery(\"\\\"jo* \\\"smith\\\" \\\"\"); //phrases inside phrases is bad checkbadquery(\"\\\"jo* [sma to smz]\\\" \\\"\"); //range queries inside phrases not supported code plus junit test to follow...",
        "label": 13
    },
    {
        "text": "documentation bug  the query parser syntax wiki page says it is for this page: http://lucene.apache.org/java/2_4_1/queryparsersyntax.html says this: .bq this page provides the query parser syntax in lucene 1.9. if you are using a different version of lucene, please consult the copy of docs/queryparsersyntax.html that was distributed with the version you are using. this is misleading on a doc page for 2.4.1",
        "label": 33
    },
    {
        "text": "calling indexsearcher searchafter beyond the number of stored documents causes arrayindexoutofboundsexception arrayindexoutofboundsexception makes it harder to reason about the cause. is there a better way to notify programmers of the cause?",
        "label": 43
    },
    {
        "text": "separate backcompat creation script from adding version the recently created bumpversion.py attempts to create a new backcompat index if the default codec has changed. however, we now want to create a backcompat index for every released version, instead of just when there is a change to the default codec. we should have a separate script which creates the backcompat indexes. it can even work directly on the released artifacts (by pulling down from mirrors once released), so that there is no possibility for generating the index from an incorrect svn/git checkout.",
        "label": 41
    },
    {
        "text": "initialization error of junit tests with solr test framework with ides and maven i'm currently developping a new component for solr. and in my netbeans project, i have created two test classes for this component: one class for simple unit tests (derived from solrtestcasej4 class) and a second one for tests with sharding (derived from basedistributedsearchtestcase). when i launch a test with these two classes, i have an error in the initialization of the second class of tests (no matter the class is, this is always the second executed class which fails). the error comes from an \"assert\" which failed in the begining of the function \"initrandom()\" of lucenetestcase class : assert !random.initialized; but, if i launch each test class separatly, all the tests succeed! after a discussion with mr. muir, the problems seems to be related to the incompatibility of the class lucenetestcase with the functioning of maven projects in ides. according to mister muir: \" the problem is that via ant, tests work like this (e.g. for 3 test classes): computetestmethods beforeclass afterclass computetestmethods beforeclass afterclass computetestmethods beforeclass afterclass but via an ide, if you run it from a folder like you did, then it does this: computetestmethods computetestmethods computetestmethods beforeclass afterclass beforeclass afterclass beforeclass afterclass \"",
        "label": 40
    },
    {
        "text": "change datetools to not create a calendar in every call to datetostring or timetostring datetools creates a calendar instance on every call to datetostring and timetostring. specifically: timetostring calls calendar.getinstance on every call. datetostring calls timetostring(date.gettime()), which then instantiates a new date(). i think we should change the order of the calls, or not have each call the other. round(), which is called from timetostring (after creating a calendar instance) creates another calendar instance ... seems that if we synchronize the methods and create the calendar instance once (static), it should solve it.",
        "label": 29
    },
    {
        "text": "change sortfield types to an enum when updating my solr-2533 patch, one issue was that the int value i had given my new type had been used by another change in the mean time. since we don't use these fields in a bitset kind of way, we can convert them to an enum.",
        "label": 7
    },
    {
        "text": "ability to group search results by field it would be awesome to group search results by specified field. some functionality was provided for apache solr but i think it should be done in core lucene. there could be some useful information like total hits about collapsed data like total count and so on. thanks, artyom",
        "label": 33
    },
    {
        "text": "make booleanweight and disjunctionmaxweight protected currently, booleanweight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code i have some use cases where it would be very useful to crawl a booleanweight to get at the sub weight objects however, since booleanweight is private, i have no way of doing this if booleanweight is protected, then i can subclass booleanquery to hook in and wrap booleanweight with a subclass to facilitate this walking of the weight objects would also want disjunctionmaxweight to be protected, along with its \"weights\" member would be even better if these weights were made public with accessors to their sub \"weights\" objects (then no subclassing would be necessary on my part) this should be really trivial and would be great if it can get into 2.9 more generally, it would be nice if all weight classes were public with nice accessors to relevant \"sub weights\"/etc so custom code can get its hooks in where and when desired",
        "label": 33
    },
    {
        "text": "testrandomchains testrandomchainswithlargestrings  failure reproducing seed for testrandomchains.testrandomchainswithlargestrings() failure from https://jenkins.thetaphi.de/job/lucene-solr-7.x-linux/2196/: checking out revision 53ec8224705f4f0d35751b18b3f0168517c43121 (refs/remotes/origin/branch_7x) [...]    [junit4] suite: org.apache.lucene.analysis.core.testrandomchains    [junit4]   2> test fail: usecharfilter=true text='\\ua97b  \\uebcf\\ueb06\\uf85b\\uf649\\uf0b7 esgm s \\uabfd \\ue11c\\udbb4\\udc48\\ue90d\\u0142\\u0014\\u0018 cr \\u30ed\\u30a8\\u30ec\\u30e1 <? gr \\ud835\\udf53\\ud835\\udc58\\ud835\\ude2b \\ueff5\\uda61\\ude33\\ud94d\\udcbb\\udb3b\\uddc8\\u0738 \\ua711\\ua719 xqu ygvfwc ~?\\u0781%'    [junit4]   2> exception from random analyzer:     [junit4]   2> charfilters=    [junit4]   2>   org.apache.lucene.analysis.fa.persiancharfilter(java.io.stringreader@12c9ec6)    [junit4]   2> tokenizer=    [junit4]   2>   org.apache.lucene.analysis.core.lowercasetokenizer()    [junit4]   2> filters=    [junit4]   2>   org.apache.lucene.analysis.hunspell.hunspellstemfilter(validatingtokenfilter@17533c4 term=,bytes=[],startoffset=0,endoffset=0,positionincrement=1,positionlength=1,type=word,termfrequency=1,keyword=false, org.apache.lucene.analysis.hunspell.dictionary@1e0b337, true, false)    [junit4]   2>   conditional:org.apache.lucene.analysis.no.norwegianlightstemfilter(onetimewrapper@3e3989 term=,bytes=[],startoffset=0,endoffset=0,positionincrement=1,positionlength=1,type=word,termfrequency=1,keyword=false)    [junit4]   2>   conditional:org.apache.lucene.analysis.en.englishpossessivefilter(onetimewrapper@96b77b term=,bytes=[],startoffset=0,endoffset=0,positionincrement=1,positionlength=1,type=word,termfrequency=1,keyword=false)    [junit4]   2>   conditional:org.apache.lucene.analysis.shingle.fixedshinglefilter(onetimewrapper@d4fade term=,bytes=[],startoffset=0,endoffset=0,positionincrement=1,positionlength=1,type=word,termfrequency=1,keyword=false, 3)    [junit4]   2> note: reproduce with: ant test  -dtestcase=testrandomchains -dtests.method=testrandomchainswithlargestrings -dtests.seed=8c3cde29c6d4a774 -dtests.multiplier=3 -dtests.slow=true -dtests.locale=ms -dtests.timezone=europe/saratov -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 0.42s j2 | testrandomchains.testrandomchainswithlargestrings <<<    [junit4]    > throwable #1: java.lang.assertionerror: finaloffset expected:<74> but was:<73>    [junit4]    >  at __randomizedtesting.seedinfo.seed([8c3cde29c6d4a774:e66761389f9a8787]:0)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.asserttokenstreamcontents(basetokenstreamtestcase.java:305)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.asserttokenstreamcontents(basetokenstreamtestcase.java:320)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.asserttokenstreamcontents(basetokenstreamtestcase.java:324)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkanalysisconsistency(basetokenstreamtestcase.java:860)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:659)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:561)    [junit4]    >  at org.apache.lucene.analysis.core.testrandomchains.testrandomchainswithlargestrings(testrandomchains.java:893)    [junit4]    >  at java.lang.thread.run(thread.java:748)    [junit4]   2> note: test params are: codec=asserting(lucene70): {dummy=postingsformat(name=memory)}, docvalues:{}, maxpointsinleafnode=1890, maxmbsortinheap=7.329943162959591, sim=randomsimilarity(querynorm=false): {}, locale=ms, timezone=europe/saratov    [junit4]   2> note: linux 4.13.0-41-generic i386/oracle corporation 1.8.0_172 (32-bit)/cpus=8,threads=1,free=313060856,total=533725184",
        "label": 2
    },
    {
        "text": "indexupgrader   fails when  verbose is not set  or when any value of  dir impl is specified here it fails because -verbose is not set: $ java -cp ./lucene-core-4.4-snapshot.jar org.apache.lucene.index.indexupgrader ./index exception in thread \"main\" java.lang.illegalargumentexception: printstream must not be null at org.apache.lucene.index.indexwriterconfig.setinfostream(indexwriterconfig.java:514) at org.apache.lucene.index.indexupgrader.<init>(indexupgrader.java:126) at org.apache.lucene.index.indexupgrader.main(indexupgrader.java:109) here it works with -verbose set: $ java -cp ./lucene-core-4.4-snapshot.jar org.apache.lucene.index.indexupgrader -verbose ./index ifd 0 [mon sep 16 18:25:53 pdt 2013; main]: init: current segments file is \"segments_5\"; deletionpolicy=org.apache.lucene.index.keeponlylastcommitdeletionpolicy@42698403 ... iw 0 [mon sep 16 18:25:53 pdt 2013; main]: at close: _2(4.4):c4",
        "label": 18
    },
    {
        "text": "testfieldcachesort testfieldscorereverse  failure my jenkins found a reproducing seed on branch_6x:   [junit4] suite: org.apache.lucene.uninverting.testfieldcachesort   [junit4]   2> note: reproduce with: ant test  -dtestcase=testfieldcachesort -dtests.method=testfieldscorereverse -dtests.seed=ddd3900d2520b584 -dtests.slow=true -dtests.locale=ko -dtests.timezone=america/adak -dtests.asserts=true -dtests.file.encoding=utf-8   [junit4] failure 0.07s j3 | testfieldcachesort.testfieldscorereverse <<<   [junit4]    > throwable #1: java.lang.assertionerror: expected:<0> but was:<1>   [junit4]    >  at __randomizedtesting.seedinfo.seed([ddd3900d2520b584:a6146b3ad4ed3f07]:0)   [junit4]    >  at org.apache.lucene.uninverting.testfieldcachesort.testfieldscorereverse(testfieldcachesort.java:445)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4]   2> note: test params are: codec=asserting(lucene62): {t=postingsformat(name=asserting), string=postingsformat(name=lucenefixedgap), f=blocktreeords(blocksize=128), id=postingsformat(name=memory dopackfst= false), body=blocktreeords(blocksize=128), value=postingsformat(name=lucenefixedgap), tievalue=postingsformat(name=lucenefixedgap)}, docvalues:{}, maxpointsinleafnode=1889, maxmbsortinheap=6.363051260515641, sim=randomsimilarity(querynorm=false,coord=crazy): {contents=dfr i(f)3(800.0), body=dfr i(f)2, value=org.apache.lucene.search.similarities.booleansimilarity@2f181fb4}, locale=ko, timezone=america/adak   [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=488108520,total=514850816   [junit4]   2> note: all tests run in this jvm: [testmultipassindexsplitter, testlazydocument, testfieldcachesort]   [junit4] completed [18/23 (1!)] on j3 in 3.69s, 60 tests, 1 failure <<< failures!",
        "label": 47
    },
    {
        "text": "calls to segmentinfos message should be wrapped w  infostream   null checks to avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infostream != null check, rather than inside message(). i'll attach a patch which does that.",
        "label": 33
    },
    {
        "text": "bitset cannot be cast to docidset we recently upgraded from lucene 2.3.1 to 2.4.0. we left the code as-is apart from the needed changes to the package name for snowball, everything seems to run as before... until yesterday when we started noticing the following stacktrace: java.lang.classcastexception: java.util.bitset cannot be cast to org.apache.lucene.search.docidset org.apache.lucene.search.cachingwrapperfilter.getdocidset(cachingwrapperfilter.java:76) org.apache.lucene.misc.chainedfilter.getdocidset(chainedfilter.java:200) org.apache.lucene.misc.chainedfilter.getdocidset(chainedfilter.java:145) org.apache.lucene.search.indexsearcher.search(indexsearcher.java:140) org.apache.lucene.search.indexsearcher.search(indexsearcher.java:112) org.apache.lucene.search.searcher.search(searcher.java:136) our index is 15gb in size and was made with lucene 2.4.0. the index holds around 36 million (lucene) documents and 268 million terms.",
        "label": 33
    },
    {
        "text": "add a geo3d shape that models an exact circle  even when the planet model is not a sphere hi [~karl wright], how circles are currently build do not behave very well when the planet model is not an sphere. when you are close to the border in wgs84 you might get false positves or false negatives when checking if a point is within. i think the reason is how the points to generate the circle plane are generated which assumes a sphere. my proposal is the following: add a new method to planetmodel: public geopoint pointonbearing(geopoint from, double dist, double bearing); which uses and algorithm that takes into account that the planet might not be spherical. for example vincenty's formulae (https://en.wikipedia.org/wiki/vincenty%27s_formulae). use this method to generate the points for the circle plane. my experiments shows that this approach removes false negatives in wgs84 meanwhile it works nicely in the sphere. does it make sense?",
        "label": 25
    },
    {
        "text": "hindi analyzer an analyzer for hindi. below are map values on the fire 2008 test collection. qe means expansion with morelikethis, all defaults, on top 5 docs. setup t t(qe) td td(qe) tdn tdn(qe) words only 0.1646 0.1979 0.2241 0.2513 0.2468 0.2735 hindianalyzer 0.2875 0.3071 0.3387 0.3791* 0.3837 0.3810 improvement 74.67% 55.18% 51.14% 50.86% 55.47% 39.31% td was the official measurement, highest score for this collection in fire 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf needs a bit of cleanup and more tests",
        "label": 40
    },
    {
        "text": "instantiatedindex supports non optimized indexreaders instantiatedindex does not currently support non-optimized indexreaders.",
        "label": 24
    },
    {
        "text": "visibility of scorer score collector  int  int  is wrong the method for scoring subsets in scorer has wrong visibility, its marked protected, but protected methods should not be called from other classes. protected methods are intended for methods that should be overridden by subclasses and are called by (often) final methods of the same class. they should never be called from foreign classes. this method is called from another class out-of-scope: booleanscorer(2) - so it must be public, but it's protected. this does not lead to a compiler error because bs(2) is in same package, but may lead to problems if subclasses from other packages override it. when implementing lucene-2838 i hit a trap, as i thought tis method should only be called from the class or scorer itsself, but in fact its called from outside, leading to bugs, because i had not overridden it. as constantscorer did not use it i have overridden it with throw uoe and suddenly booleanquery was broken, which made it clear that it's called from outside (which is not the intention of protected methods). we cannot fix this in 3.x, as it would break backwards for classes that overwrite this method, but we can fix visibility in trunk.",
        "label": 53
    },
    {
        "text": "test mixed binary docvaluesupdates testmanyreopensandfields  failures non-reproducing failure from https://builds.apache.org/job/lucene-solr-smokerelease-master/794/: checking out revision e8057309b90db0c79fc273e2284948b84c16ce4c (refs/remotes/origin/master) [...]    [smoker]    [junit4] suite: org.apache.lucene.index.testmixeddocvaluesupdates    [smoker]    [junit4] ignor/a 0.00s j0 | testmixeddocvaluesupdates.testtonsofupdates    [smoker]    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())    [smoker]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testmixeddocvaluesupdates -dtests.method=testmanyreopensandfields -dtests.seed=69a3133ac96f545a -dtests.multiplier=2 -dtests.locale=es-sv -dtests.timezone=europe/zagreb -dtests.asserts=true -dtests.file.encoding=us-ascii    [smoker]    [junit4] failure 0.04s j0 | testmixeddocvaluesupdates.testmanyreopensandfields <<<    [smoker]    [junit4]    > throwable #1: java.lang.assertionerror: invalid numeric value for doc=0, field=f0, reader=_3(7.0.0):c35/1 expected:<3> but was:<2>    [smoker]    [junit4]    >  at __randomizedtesting.seedinfo.seed([69a3133ac96f545a:5f5f7115489a3746]:0)    [smoker]    [junit4]    >  at org.apache.lucene.index.testmixeddocvaluesupdates.testmanyreopensandfields(testmixeddocvaluesupdates.java:138)    [smoker]    [junit4]    >  at java.lang.thread.run(thread.java:748)    [smoker]    [junit4]   2> note: test params are: codec=lucene70, sim=randomsimilarity(querynorm=true): {}, locale=es-sv, timezone=europe/zagreb    [smoker]    [junit4]   2> note: linux 3.13.0-88-generic amd64/oracle corporation 1.8.0_131 (64-bit)/cpus=4,threads=1,free=177411120,total=287309824    [smoker]    [junit4]   2> note: all tests run in this jvm: [testregexprandom, teststandardanalyzer, testmmapdirectory, testcodecs, testdocvaluesqueries, testneverdelete, testindexwriterconfig, testnodeletionpolicy, testbooleanminshouldmatch, testindexsorting, testdocvaluesindexing, testtragicindexwriterdeadlock, testintblockpool, testbinaryterms, testindexwriter, test4gbstoredfields, testsortedsetselector, testallfilescheckindexheader, testfiltercodecreader, testcachingcollector, testnotdocidset, testquerybuilder, testmaxtermfrequency, testforcemergeforever, testfieldmaskingspanquery, testregexp, testpointvalues, testindexwriteroutoffiledescriptors, test2bterms, testtermsenum, testsloppyphrasequery, testboostquery, testratelimiter, testindexwriterexceptions, testmultiphrasequery, testsimplesearchequivalence, testbinarydocvaluesupdates, testpersegmentdeletes, test2bpoints, testsimpleexplanations, testperfieldpostingsformat, testlucene50termvectorsformat, testsingleinstancelockfactory, testlucene50compoundformat, testmaxposition, testtotalhitcountcollector, testconstantscorequery, testwordlistloader, testthreadedforcemerge, testbytesrefarray, testpointqueries, testcharfilter, testsimilarityprovider, testbytesstore, testintrosorter, testwildcardrandom, testsimilarity, testfieldvaluequery, testomitnorms, testunicodeutil, testlruquerycache, testtermquery, testinplacemergesorter, testnot, testtopfieldcollector, testindexwriterfromreader, testchararraymap, testutf32toutf8, testdocidswriter, testdocsandpositions, testnewestsegment, testterm, testcodecholdsopenfiles, testpagedbytes, testpackedints, testbasics, testnrtthreads, teststressadvance, testsearchafter, testhighcompressionmode, testdocumentswriterstallcontrol, teststressindexing, testsnapshotdeletionpolicy, testnrtreaderwiththreads, testtieredmergepolicy, testlevenshteinautomata, testweakidentitymap, testregexprandom2, testsegmenttermdocs, testperfieldpostingsformat2, testmultidocvalues, testhugeramfile, testlazyproxskipping, testdeterminism, testbytesrefhash, testnearspansordered, testtermrangequery, testdocumentwriter, testcrashcausescorruptindex, testlivefieldvalues, testfuzzyquery, testautomatonquery, testmultilevelskiplist, testcheckindex, testconjunctions, testvirtualmethod, testsearch, testdatetools, testdoccount, testattributesource, testiscurrent, testindexwriterlockrelease, testbyteblockpool, testdemo, testrollback, multicollectortest, testsimpleattributeimpl, testbytearraydatainput, testpackedtokenattributeimpl, testforutil, testlucene50storedfieldsformathighcompression, testfieldtype, test2bsorteddocvaluesfixedsorted, test2bsorteddocvaluesords, testcustomtermfreq, testdocidmerger, testdocvalues, testdocswithfieldset, testexitabledirectoryreader, testfieldinvertstate, testfieldreuse, testfilterdirectoryreader, testindexreaderclose, testindexwriteronvmerror, testinfostream, testmergepolicywrapper, testmixeddocvaluesupdates]",
        "label": 33
    },
    {
        "text": "fieldcachesanitychecker called directly by fieldcache get  as suggested by mccandless in lucene-1749, we can make fieldcacheimpl a client of the fieldcachesanitychecker and have it sanity check itself each time it creates a new cache entry, and log a warning if it thinks there is a problem. (although we'd probably only want to do this if the caller has set some sort of infostream/warningstream type property on the fieldcache object.",
        "label": 33
    },
    {
        "text": "morfologik filter can accept custom dictionary resources i have little proposal for morfologik lucene module. current module is tightly coupled with polish dictionary enumeration. but other people (like me) can build own dictionaries to fsa and use it with lucene. you can find proposal in attachment and also example usage in analyzer (slovaklemmaanalyzer). it uses dictionary property as string resource from classpath, not enumeration. one change is, that dictionary variable must be set in mofologikfilterfactory (no default value).",
        "label": 11
    },
    {
        "text": "equals method of termsfilter might equate two different filters if two terms filters have 1) the same number of terms, 2) use the same field in all these terms and 3) term values happened to have the same hash codes, these two filter are considered to be equal as long as the first term is the same in both filters.",
        "label": 1
    },
    {
        "text": "segmentinfos shouldn't blindly increment version on commit segmentinfos currently increments version on the assumption that there are always changes. but, both dirreader and iw are more careful about tracking whether there are changes. dirreader has haschanges and iw has changecount. i think these classes should notify the sis when there are in fact changes; this will fix the case simon hit on fixing lucene-2082 when the nrt reader thought there were changes, but in fact there weren't because iw simply committed the exact sis it already had.",
        "label": 46
    },
    {
        "text": "rename segmentinfos format and improve description in checkindex a 3.2 user recently asked if something was wrong because checkindex was reporting his (newly built) index version as... segments file=segments_or numsegments=1 version=format_3_1 [lucene 3.1] it seems like there are two very confusing pieces of information here... 1) the variable name of segmentinfos.format_3_1 seems like poor choice. all other format_* constants in segmentinfos are descriptive of the actual change made, and not specific to the version when they were introduced. 2) whatever the name of the format_* variable, checkindex is labeling it \"lucene 3.1\", which is missleading since that format is alwasy used in 3.2 (and probably 3.3, etc...). i suggest: a) rename format_3_1 to something like \"format_segment_records_version\" b) change checkindex so that the label for the \"newest\" format always ends with \" and later\" (ie: \"lucene 3.1 and later\") so when we release versions w/o a format change we don't have to remember to manual list them in checkindex. when we do make format changes and update checkindex \" and later\" can be replaced with \" to x.y\" and the new format can be added",
        "label": 43
    },
    {
        "text": "another highlighter i've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. whitespacetokenizer) also supported. see test code in patch). the idea was inherited from my previous project with my colleague and lucene-644. this approach needs highlight fields to be termvector.with_positions_offsets, but is fast and can support n-grams. this depends on lucene-1448 to get refined term offsets. usage: topdocs docs = searcher.search( query, 10 ); highlighter h = new highlighter(); fieldquery fq = h.getfieldquery( query ); for( scoredoc scoredoc : docs.scoredocs ){   // fieldname=\"content\", fragcharsize=100, numfragments=3   string[] fragments = h.getbestfragments( fq, reader, scoredoc.doc, \"content\", 100, 3 );   if( fragments != null ){     for( string fragment : fragments )       system.out.println( fragment );   } } features: fast for large docs supports not only whitespace-based token stream, but also \"fixed size\" n-gram (e.g. (2,2), not (1,3)) (can solve lucene-1489) supports phrasequery, phrase-unit highlighting with slops q=\"w1 w2\" <b>w1 w2</b> --------------- q=\"w1 w2\"~1 <b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b> highlight fields need to be termvector.with_positions_offsets easy to apply patch due to independent package (contrib/highlighter2) uses java 1.5 looks query boost to score fragments (currently doesn't see idf, but it should be possible) pluggable fraglistbuilder pluggable fragmentsbuilder to do: term positions can be unnecessary when phrasehighlight==false collects performance numbers",
        "label": 29
    },
    {
        "text": "reorganize contrib modules it would be nice to reorganize contrib modules, so that they are bundled together by functionality. for example: the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers there are two highlighters, i think could be one highlighters package. there are many queryparsers and queries in different places in contrib",
        "label": 40
    },
    {
        "text": "move documentstoredfieldsvisitor to o a l document when examining the changes to the field/document api, i noticed this class was in o.a.l.index i think it should be in o.a.l.document, its more intuitive packaging",
        "label": 40
    },
    {
        "text": "chararrayset clear  i needed chararrayset.clear() for something i was working on in solr in a tokenstream. instead i ended up using chararraymap<boolean> because it supported .clear() it would be better to use a set though, currently it will throw uoe for .clear() because abstractset will call iterator.remove() which throws uoe. in solr, the very similar chararraymap.clear() looks like this:   @override   public void clear() {     count = 0;     arrays.fill(keys,null);     arrays.fill(values,null);   } i think we can do a similar thing as long as we throw uoe for the unmodifiablechararrayset will submit a patch later tonight (unless someone is bored and has nothing better to do)",
        "label": 53
    },
    {
        "text": "readerclosedlistener is not invoked for parallelcompositereader's leaves there was a test failure last night: 1 tests failed. regression:  org.apache.lucene.search.grouping.allgroupheadscollectortest.testbasic error message: testbasic(org.apache.lucene.search.grouping.allgroupheadscollectortest): insane fieldcache usage(s) found expected:<0> but was:<2> stack trace: java.lang.assertionerror: testbasic(org.apache.lucene.search.grouping.allgroupheadscollectortest): insane fieldcache usage(s) found expected:<0> but was:<2>         at __randomizedtesting.seedinfo.seed([1f9c2a2ad23a8e02:b466373f0de6082c]:0)         at org.junit.assert.fail(assert.java:93)         at org.junit.assert.failnotequals(assert.java:647)         at org.junit.assert.assertequals(assert.java:128)         at org.junit.assert.assertequals(assert.java:472)         at org.apache.lucene.util.lucenetestcase.assertsanefieldcaches(lucenetestcase.java:592)         at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:55)         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)         at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)         at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:49)         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358)         at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:782)         at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:442)         at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:746)         at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:648)         at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:682)         at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:693)         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)         at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42)         at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)         at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)         at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)         at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:43)         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)         at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358)         at java.lang.thread.run(thread.java:722) build log: [...truncated 6904 lines...] [junit4:junit4] suite: org.apache.lucene.search.grouping.allgroupheadscollectortest [junit4:junit4]   2> *** begin testbasic(org.apache.lucene.search.grouping.allgroupheadscollectortest): insane fieldcache usage(s) *** [junit4:junit4]   2> valuemismatch: multiple distinct value objects for parallelatomicreader(_0(5.0):c3)+id [junit4:junit4]   2>    'parallelatomicreader(_0(5.0):c3)'=>'id',class org.apache.lucene.index.sorteddocvalues,0.5=>org.apache.lucene.search.fieldcacheimpl$sorteddocvaluesimpl#386041791 (size =~ 232 bytes) [junit4:junit4]   2>    'parallelatomicreader(_0(5.0):c3)'=>'id',int,org.apache.lucene.search.fieldcache.default_int_parser=>org.apache.lucene.search.fieldcacheimpl$intsfromarray#140912913 (size =~ 48 bytes) [junit4:junit4]   2>    'parallelatomicreader(_0(5.0):c3)'=>'id',int,null=>org.apache.lucene.search.fieldcacheimpl$intsfromarray#140912913 (size =~ 48 bytes) [junit4:junit4]   2> [junit4:junit4]   2> valuemismatch: multiple distinct value objects for parallelatomicreader(_1(5.0):c5)+id [junit4:junit4]   2>    'parallelatomicreader(_1(5.0):c5)'=>'id',int,null=>org.apache.lucene.search.fieldcacheimpl$intsfromarray#1105632232 (size =~ 56 bytes) [junit4:junit4]   2>    'parallelatomicreader(_1(5.0):c5)'=>'id',int,org.apache.lucene.search.fieldcache.default_int_parser=>org.apache.lucene.search.fieldcacheimpl$intsfromarray#1105632232 (size =~ 56 bytes) [junit4:junit4]   2>    'parallelatomicreader(_1(5.0):c5)'=>'id',class org.apache.lucene.index.sorteddocvalues,0.5=>org.apache.lucene.search.fieldcacheimpl$sorteddocvaluesimpl#27148040 (size =~ 232 bytes) [junit4:junit4]   2> [junit4:junit4]   2> *** end testbasic(org.apache.lucene.search.grouping.allgroupheadscollectortest): insane fieldcache usage(s) *** [junit4:junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory. [junit4:junit4]   2> note: reproduce with: ant test  -dtestcase=allgroupheadscollectortest -dtests.method=testbasic -dtests.seed=1f9c2a2ad23a8e02 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/hudson/lucene-data/enwiki.random.lines.txt -dtests.locale=be_by -dtests.timezone=asia/manila -dtests.file.encoding=us-ascii [junit4:junit4] failure 0.75s j1 | allgroupheadscollectortest.testbasic <<< [junit4:junit4]    > throwable #1: java.lang.assertionerror: testbasic(org.apache.lucene.search.grouping.allgroupheadscollectortest): insane fieldcache usage(s) found expected:<0> but was:<2> [junit4:junit4]    >    at __randomizedtesting.seedinfo.seed([1f9c2a2ad23a8e02:b466373f0de6082c]:0) [junit4:junit4]    >    at org.apache.lucene.util.lucenetestcase.assertsanefieldcaches(lucenetestcase.java:592) [junit4:junit4]    >    at java.lang.thread.run(thread.java:722) [junit4:junit4]   2> note: test params are: codec=lucene42: {sort3=mockfixedintblock(blocksize=733), id=pulsing41(freqcutoff=3 minblocksize=50 maxblocksize=177), content=mockfixedintblock(blocksize=733), author=pulsing41(freqcutoff=3 minblocksize=50 maxblocksize=177), sort2=mockvariableintblock(baseblocksize=71), sort1=pulsing41(freqcutoff=3 minblocksize=50 maxblocksize=177), group=pulsing41(freqcutoff=3 minblocksize=50 maxblocksize=177)}, docvalues:{author_dv=docvaluesformat(name=disk), group_dv=docvaluesformat(name=disk)}, sim=randomsimilarityprovider(querynorm=false,coord=yes): {content=ib ll-l1, author=dfr gbz(0.3)}, locale=be_by, timezone=asia/manila [junit4:junit4]   2> note: freebsd 9.0-release amd64/oracle corporation 1.7.0_17 (64-bit)/cpus=16,threads=1,free=157973280,total=249626624 [junit4:junit4]   2> note: all tests run in this jvm: [groupfacetcollectortest, allgroupscollectortest, allgroupheadscollectortest] it reproduces, and happens because parallelcompositereader isn't invoking the reader listeners on its .leaves() when everything is closed. i made a separate test case to show the issue ...",
        "label": 53
    },
    {
        "text": "missing word  cela  in conf lang stopwords fr txt nb: not sure this defect is assigned to the right component. in file example/solr/collection1/conf/lang/stopwords_fr.txt, there is the word \"cel\u00e0\". though incorrect in french (cf http://fr.wiktionary.org/wiki/cel%c3%a0), it's common, but we may also add the correct spelling (e.g. \"cela\", whitout accent) to that stopwords list. another thing: i noticed that \"cel\u00e0\" is the only word of the list followed by an unbreakable space. is that wanted?",
        "label": 1
    },
    {
        "text": "indexwriter addindexesnooptimize has redundent try catch with the new transaction code, the try/catch clause at the beginning of indexwriter#addindexesnooptimize is redundant.",
        "label": 33
    },
    {
        "text": "termvectoraccessor  transparent vector space access this class visits termvectormapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with field.termvector) or by resolving the inverted index.",
        "label": 24
    },
    {
        "text": "avoid grow of set in analyzingsuggester toposortstates automaton  converted \"visited\" to a bitset and sized it correctly in analyzingsuggester.toposortstates(automaton). this avoids dynamic resizing of the set.",
        "label": 33
    },
    {
        "text": "improve the documentation of version in my opinion, we should elaborate more on the effects of changing the version parameter. particularly, changing this value, even if you recompile your code, likely involves reindexing your data. i do not think this is adequately clear from the current javadocs.",
        "label": 40
    },
    {
        "text": "weight is not serializable for some of the queries in order to work with parallelmultisearcher, query weights need to be serializable. the interface weight extends java.io.serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors.",
        "label": 12
    },
    {
        "text": "decouple filter from bitset package org.apache.lucene.search; public abstract class filter implements java.io.serializable  {   public abstract abstractbitset bits(indexreader reader) throws ioexception; } public interface abstractbitset  {   public boolean get(int index); } it would be useful if the method =filter.bits()= returned an abstract interface, instead of =java.util.bitset=. use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible. sparsely populated =java.util.bitset=s are not efficient and waste lots of memory. it would be desirable to have an alternative bitset implementation with smaller memory footprint. though it is possibly to derive classes from =java.util.bitset=, it was obviously not designed for that purpose. that's why i propose to use an interface instead. the default implementation could still delegate to =java.util.bitset=.",
        "label": 29
    },
    {
        "text": "documentswriter applydeletes should not create termdocs or indexsearcher if not needed documentswriter.applydeletes(indexreader, int) always creates termdocs and indexsearcher, even if there were no deletes by term or by query. the attached patch wraps those creations w/ checks on whether there were any deletes by these two. additionally, the searcher wasn't closed in a finally block, so i fixed that as well. i'll attach a patch shortly.",
        "label": 33
    },
    {
        "text": "stored only fields automatically enable norms and tf when added to document during updating my internal components to the new trieapi, i have seen the following: i index a lot of numeric fields with trie encoding omitting norms and term frequency. this works great. luke shows that both is omitted. as i sometimes also want to have the components of the field stored and want to use the same field name for it. so i add additionally the field again to the document, but stored only (as the field c'tor using a tokenstream cannot additionally store the field). as it is stored only, i thought, that i can left out explicit setting of omitnorms and omittermfreqandpositions. after adding the stored-only-without-omits field, luke shows all fields with norms enabled. i am not sure, if the norms/tf were really added to the index, but luke shows a value for the norms and fieldinfo has it enabled. in my opinion, this is not intuitive, o.a.l.document.field should switch both omit* options on when storing fields only (and also disable other indexing-only options). alternatively the internal fieldinfo.update(boolean isindexed, boolean storetermvector, boolean storepositionwithtermvector, boolean storeoffsetwithtermvector, boolean omitnorms, boolean storepayloads, boolean omittermfreqandpositions) should only change the omit* and other options, if the isindexed parameter (not this.isindexed) is also true, elsewhere leave it as it is. in principle, when adding a stored-only field, any indexing-specific options should not be changed in fieldinfo. if the field was indexed with norms before, norms should stay enabled (but this would be the default as it is).",
        "label": 33
    },
    {
        "text": "improve the edge ngramtokenizer filters our ngram tokenizers/filters could use some love. eg, they output ngrams in multiple passes, instead of \"stacked\", which messes up offsets/positions and requires too much buffering (can hit oome for long tokens). they clip at 1024 chars (tokenizers) but don't (token filters). the split up surrogate pairs incorrectly.",
        "label": 1
    },
    {
        "text": "configurable multitermquery toptermsscoringbooleanrewrite pq size multitermquery has a toptermsscoringbooleanrewrite, that uses a priority queue to expand the query to the top-n terms. currently n is hardcoded at booleanquery.getmaxclausecount(), but it would be nice to be able to set this for top-n multitermqueries: e.g. expand a fuzzy query to at most only the 50 closest terms. at a glance it seems one way would be to expose toptermsscoringbooleanrewrite (it is private right now) and add a ctor to it, so a multitermquery can instantiate one with its own limit.",
        "label": 40
    },
    {
        "text": "add next  and skipto  variants to docidsetiterator that return the current doc  instead of boolean see http://www.nabble.com/another-possible-optimization---now-in-docidsetiterator-p23223319.html for the full discussion. the basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). if there are no more docs, return -1. a summary of what was discussed so far: deprecate those two methods. add nextdoc() and skiptodoc(int) that return doc, with default impl in disi (calls next() and skipto() respectively, and will be changed to abstract in 3.0). i actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target. wherever these are used, do something like '(doc = advance()) >= 0' instead of comparing to -1 for improved performance. i will post a patch shortly",
        "label": 33
    },
    {
        "text": "trunk testrollingupdates testrollingupdates seed failure trunk r1152892 reproducable: always junit-sequential:     [junit] testsuite: org.apache.lucene.index.testrollingupdates     [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 1.168 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testrollingupdates -dtestmethod=testrollingupdates -dtests.seed=-5322802004404580273:-4001225075726350391     [junit] warning: test method: 'testrollingupdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k     [junit] resource leak: test method: 'testrollingupdates' left 1 thread(s) running     [junit] note: test params are: codec=randomcodecprovider: {docid=standard, body=simpletext, title=mocksep, titletokenized=pulsing(freqcutoff=20), date=mockfixedintblock(blocksize=1474)}, locale=lv_lv, timezone=pacific/fiji     [junit] note: all tests run in this jvm:     [junit] [testrollingupdates]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512     [junit] ------------- ---------------- ---------------     [junit] testcase: testrollingupdates(org.apache.lucene.index.testrollingupdates):   failed     [junit] expected:<20> but was:<21>     [junit] junit.framework.assertionfailederror: expected:<20> but was:<21>     [junit]     at org.apache.lucene.index.testrollingupdates.testrollingupdates(testrollingupdates.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1522)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1427)     [junit]      [junit]      [junit] test org.apache.lucene.index.testrollingupdates failed",
        "label": 46
    },
    {
        "text": "random string generation centralized in  testutil the random string generators in basetokenstreamtestcase have wider applicability and should move in with their cousins.",
        "label": 6
    },
    {
        "text": "only reader that contains fields can be added into readercontext when there is only segements in solr core, which means no any indexes, in compositereadercontext.build() method, the atomicreader that has no fields returned should not be added into leaves. otherwise, in solrindexsearcher.getdocsetnc(query query, docset filter), when execute line fields.terms(t.field()), a nullpointerexception will occur since fields variable is null.",
        "label": 53
    },
    {
        "text": "explore facets aggregation during documents collection today the facet module simply gathers all hits (as a bitset, optionally with a float[] to hold scores as well, if you will aggregate them) during collection, and then at the end when you call getfacetsresults(), it makes a 2nd pass over all those hits doing the actual aggregation. we should investigate just aggregating as we collect instead, so we don't have to tie up transient ram (fairly small for the bit set but possibly big for the float[]).",
        "label": 43
    },
    {
        "text": "sloppyphrasescorer sometimes computes infinite freq reported on user list: http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query",
        "label": 12
    },
    {
        "text": "reproducing testlucene docvaluesformat testsortedsetvariablelengthbigvsstoredfields  failures policeman jenkins found a reproducing seed for a testlucene70docvaluesformat.testsortedsetvariablelengthbigvsstoredfields() failure https://jenkins.thetaphi.de/job/lucene-solr-master-linux/22320/; git bisect blames commit 2519025 on lucene-7976: checking out revision 8c714348aeea51df19e7603905f85995bcf0371c (refs/remotes/origin/master) [...]    [junit4] suite: org.apache.lucene.codecs.lucene70.testlucene70docvaluesformat    [junit4]   2> note: reproduce with: ant test  -dtestcase=testlucene70docvaluesformat -dtests.method=testsortedsetvariablelengthbigvsstoredfields -dtests.seed=63a61b46a6934b1a -dtests.multiplier=3 -dtests.slow=true -dtests.locale=sw-tz -dtests.timezone=pacific/pitcairn -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 23.3s j2 | testlucene70docvaluesformat.testsortedsetvariablelengthbigvsstoredfields <<<    [junit4]    > throwable #1: java.lang.assertionerror: limit=4 actual=5    [junit4]    >  at __randomizedtesting.seedinfo.seed([63a61b46a6934b1a:6be93fa35e02851]:0)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.dorandomforcemerge(randomindexwriter.java:372)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.getreader(randomindexwriter.java:386)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.getreader(randomindexwriter.java:332)    [junit4]    >  at org.apache.lucene.index.basedocvaluesformattestcase.dotestsortedsetvsstoredfields(basedocvaluesformattestcase.java:2155)    [junit4]    >  at org.apache.lucene.codecs.lucene70.testlucene70docvaluesformat.testsortedsetvariablelengthbigvsstoredfields(testlucene70docvaluesformat.java:93)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    >  at java.base/java.lang.thread.run(thread.java:844) [...]    [junit4]   2> note: test params are: codec=asserting(lucene70): {}, docvalues:{}, maxpointsinleafnode=693, maxmbsortinheap=5.078503794479895, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@20a604e6), locale=sw-tz, timezone=pacific/pitcairn    [junit4]   2> note: linux 4.13.0-41-generic amd64/oracle corporation 9.0.4 (64-bit)/cpus=8,threads=1,free=352300304,total=518979584",
        "label": 13
    },
    {
        "text": "only a part of categorypath is recorded if i add  me  category  if i add type/chat, me, network/hanirc/chan/#sdgsdgsd as \"categories\" to a document and add the document to a writer as below, only \"network/hanirc/chan\", not \"network/hanirc/chan/#sdgsdgsd \"is\" recorded into a taxonomy directory. type/chat and me are recorded well. i confirmed it with a test. facetfields ff = new facetfields(taxowriter); ff.addfields(doc, categories); nrtwriter.adddocument(doc); if i add type/chat, network/hanirc/chan/#sdgsdgsd to a document, everything is recorded fine. what's wrong with \"me\" category? you could clone git://snowberry.me/zeroirclog.git and read the code if you want.",
        "label": 43
    },
    {
        "text": "morelikethis should use the new token api the morelikethis functionality needs to be converted to use the new tokenstream api. see also lucene-1695.",
        "label": 32
    },
    {
        "text": "fsdirectory copybytes isn't safe for simplefsdirectory the copybytes optimization from lucene-2574 is not safe for simplefsdirectory, but works fine for niofsdirectory. with simplefsdirectory, the copybytes optimization causes index corruption. see http://www.lucidimagination.com/search/document/36d2dbfc691909d5/bug_triggered_by_testindexwriter_testrandomstoredfields for background here are my steps to reproduce (most of the time, at least on windows): 1. edit line 87 of testindexwriter to plugin the seed:     random = newrandom(3312389322103990899l); 2. edit line 5138 of testindexwriter to force simplefsdirectory:     directory dir = new simplefsdirectory(index); 3. run this command:     ant clean test-core -dtestcase=testindexwriter -dtestmethod=testrandomstoredfields -dtests.iter=10 -dtests.codec=\"mockvariableintblock(29)\"",
        "label": 40
    },
    {
        "text": "poll mirrors pl needs fixed i just noticed that poll-mirrors.pl is setup to look for the keys file in the release dir on each mirror \u2013 infra (wisely) tweaked the way mirroring happens recently to ensure that keys files are not mirrored anymore (presumably to help catch bad links advising people to download untrusted keys files) we're going to need to updated poll-mirrors.pl to look for something else in each release dir ... changes/changes.html perhaps?",
        "label": 18
    },
    {
        "text": "fix wrong clover analysis because of backwards tests  upgrade clover to or better this is a followup for http://www.lucidimagination.com/search/document/6248d6eafbe10ef4/build_failed_in_hudson_lucene_trunk_902 the problem with clover running on hudson is, that it does not instrument all tests ran. the autodetection of clover 1.x is not able to find out which files are the correct tests and only instruments the backwards test. because of this, the current coverage report is only from the backwards tests running against the current lucene jar. you can see this, if you install clover and start the tests. during test-core no clover data is added to the db, only when backwards-tests begin, new files are created in the clover db folder. clover 2.x supports a new ant task, <testsources> that can be used to specify the files, that are the tests. it works here locally with clover 2.4.3 and produces a really nice coverage report, also linking with test files work, it tells which tests failed and so on. i will attach a patch, that changes common-build.xml to the new clover version (other initialization resource) and tells clover where to find the tests (using the test folder include/exclude properties). one problem with the current patch: it does not instrument the backwards branch, so you see only coverage of the core/contrib tests. getting the coverage also from the backwards tests is not easy possible because of two things: the tag test dir is not easy to find out and add to <testsources> element (there may be only one of them) the test names in bw branch are identical to the trunk tests. this completely corrupts the linkage between tests and code in the coverage report. in principle the best would be to generate a second coverage report for the backwards branch with a separate clover db. the attached patch does not instrument the bw branch, it only does trunk tests.",
        "label": 53
    },
    {
        "text": "optimize fuzzytermsenum per segment we can make fuzzyquery about 3% faster by not creating dfa(s) for each segment. creating the dfas is still somewhat heavy: i can address this here too, but this is easy.",
        "label": 53
    },
    {
        "text": "fix thread safety in printstreaminfostream  unify logging format with tests noticed while debugging some iw output in an unit test that milliseconds were not output in the date, changed this to reuse the date format used by printstreaminfostream.",
        "label": 53
    },
    {
        "text": "use java enums replace the use of o.a.l.util.parameter with java 5 enums, deprecating parameter. replace other custom enum patterns with java 5 enums.",
        "label": 53
    },
    {
        "text": "improve tests to work easier from ides as reported by paolo castagna on the mailing lists, some tests fail when you run them from eclipse. some of the failures he reports are actually code problems such as base test classes not being abstract when they should be... we should fix things like that.",
        "label": 40
    },
    {
        "text": "contrib benchmark build doesn't handle checking if content is properly extracted the contrib/benchmark build does not properly handle checking to see if the content (such as reuters coll.) is properly extracted. it only checks to see if the directory exists. thus, it is possible that the directory gets created and the extraction fails. then, the next time it is run, it skips the extraction part and tries to continue on running the benchmark. the workaround is to manually delete the extraction directory.",
        "label": 15
    },
    {
        "text": "always apply position increment gap between values i'm doing some fancy stuff with span queries that is very sensitive to term positions. i discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document. i suspect this logic wasn't deliberate, it's just how its always been for no particular reason. i think it should always apply the gap between fields. reference docinverterperfield.java line 82: if (fieldstate.length > 0) fieldstate.position += docstate.analyzer.getpositionincrementgap(fieldinfo.name); this is checking fieldstate.length. i think the condition should simply be: if (i > 0). i don't think this change will affect anyone at all but it will certainly help me. presently, i can either change this line in lucene, or i can put in a hack so that the first value for the document is some dummy value which is wasteful.",
        "label": 26
    },
    {
        "text": "indexed non point shapes index excessive terms indexed non-point shapes are comprised of a set of terms that represent grid cells. cells completely within the shape or cells on the intersecting edge that are at the maximum detail depth being indexed for the shape are denoted as \"leaf\" cells. such cells have a trailing '+' at the end. such tokens are actually indexed twice, one with the leaf byte and one without. the termquery based prefixtree strategy doesn't consider the notion of 'leaf' cells and so the tokens with '+' are completely redundant. the recursive [algorithm] based prefixtree strategy better supports correct search of indexed non-point shapes than termquery does and the distinction is relevant. however, the foundational search algorithms used by this strategy (intersects & contains; the other 2 are based on these) could each be upgraded to deal with this correctly. not trivial but very doable. in the end, spatial non-point indexes can probably be trimmed my ~40% by doing this.",
        "label": 10
    },
    {
        "text": "memory leak per unique thread caused by randomizedcontext contexts static map in digging on the hard-to-understand oomes with testdirectpostingsformat ... i found (thank you yourkit) that randomizedcontext (in randomizedtesting jar) seems to be holding onto all threads created by the test. the test does create many very short lived threads (testing the thread safety of the postings format), in basepostingsformattestcase.testterms), and somehow these seem to tie up a lot (~100 mb) of ram in randomizedcontext.contexts static map. for now i've disabled all thread testing (committed false && inside bpftc.testterms), but hopefully we can fix the root cause here, eg when a thread exits can we clear it from that map?",
        "label": 11
    },
    {
        "text": "rangequery equals method does not compare collator property fully the equals method in the range query has the collator comparison implemented as: (this.collator != null && ! this.collator.equals(other.collator)) when this.collator = null and other.collator = somecollator this method will incorrectly assume they are equal. so adding something like (this.collator == null && other.collator != null) would fix the problem",
        "label": 29
    },
    {
        "text": "convert drilldown to drilldownquery drilldown is a utility class for creating drill-down queries over a base query and a bunch of categories. we've been asked to support and, or and and of ors. the latter is not so simple as a static utility method though, so instead we have some sample code ... rather, i think that we can just create a drilldownquery (extends query) which takes a basequery in its ctor and exposes add(categorypath...), such that every such group of categories is and'ed with other groups, and internally they are or'ed. it's very similar to how you would construct a booleanquery, only simpler and specific to facets. internally, it would build a booleanquery and delegate rewrite, createweight etc to it. that will remove the need for the static utility methods .. or we can keep static term() for convenience.",
        "label": 43
    },
    {
        "text": "arrayindexoutofboundsexception for surround parser i got the following exception when i query solr with \"pcnt(kk w hit) or (ipad) or (iphoine))\" and the deftype is 'surround'. 18:16:45 severe solrcore java.lang.arrayindexoutofboundsexception: 2147483647 at org.apache.lucene.search.similarities.tfidfsimilarity$sloppytfidfdocscorer.score(tfidfsimilarity.java:793) at org.apache.lucene.search.spans.spanscorer.score(spanscorer.java:93) at org.apache.lucene.search.disjunctionsumscorer.afternext(disjunctionsumscorer.java:94) at org.apache.lucene.search.disjunctionsumscorer.nextdoc(disjunctionsumscorer.java:82) at org.apache.lucene.search.booleanscorer2.score(booleanscorer2.java:284) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:573) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:275) at org.apache.solr.search.solrindexsearcher.getdoclistnc(solrindexsearcher.java:1390) at org.apache.solr.search.solrindexsearcher.getdoclistc(solrindexsearcher.java:1265) at org.apache.solr.search.solrindexsearcher.search(solrindexsearcher.java:390) at org.apache.solr.handler.component.querycomponent.process(querycomponent.java:411) at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:206) at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:129) at org.apache.solr.core.solrcore.execute(solrcore.java:1656) at org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:454) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:275) at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1337) at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:484) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:119) at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:524) at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:233) at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1065) at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:413) at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:192) at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:999) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:117) at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:250) at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:149) at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:111) at org.eclipse.jetty.server.server.handle(server.java:351) at org.eclipse.jetty.server.abstracthttpconnection.handlerequest(abstracthttpconnection.java:454) at org.eclipse.jetty.server.blockinghttpconnection.handlerequest(blockinghttpconnection.java:47) at org.eclipse.jetty.server.abstracthttpconnection.content(abstracthttpconnection.java:900) at org.eclipse.jetty.server.abstracthttpconnection$requesthandler.content(abstracthttpconnection.java:954) at org.eclipse.jetty.http.httpparser.parsenext(httpparser.java:857) at org.eclipse.jetty.http.httpparser.parseavailable(httpparser.java:235) at org.eclipse.jetty.server.blockinghttpconnection.handle(blockinghttpconnection.java:66) at org.eclipse.jetty.server.bio.socketconnector$connectorendpoint.run(socketconnector.java:254) at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:599) at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:534) at java.lang.thread.run(thread.java:636) 18:16:45 severe solrdispatchfilter null:java.lang.arrayindexoutofboundsexception: 2147483647 at org.apache.lucene.search.similarities.tfidfsimilarity$sloppytfidfdocscorer.score(tfidfsimilarity.java:793) at org.apache.lucene.search.spans.spanscorer.score(spanscorer.java:93) at org.apache.lucene.search.disjunctionsumscorer.afternext(disjunctionsumscorer.java:94) at org.apache.lucene.search.disjunctionsumscorer.nextdoc(disjunctionsumscorer.java:82) at org.apache.lucene.search.booleanscorer2.score(booleanscorer2.java:284) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:573) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:275) at org.apache.solr.search.solrindexsearcher.getdoclistnc(solrindexsearcher.java:1390) at org.apache.solr.search.solrindexsearcher.getdoclistc(solrindexsearcher.java:1265) at org.apache.solr.search.solrindexsearcher.search(solrindexsearcher.java:390) at org.apache.solr.handler.component.querycomponent.process(querycomponent.java:411) at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:206) at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:129) at org.apache.solr.core.solrcore.execute(solrcore.java:1656) at org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:454) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:275) at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1337) at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:484) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:119) at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:524) at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:233) at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1065) at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:413) at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:192) at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:999) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:117) at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:250) at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:149) at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:111) at org.eclipse.jetty.server.server.handle(server.java:351) at org.eclipse.jetty.server.abstracthttpconnection.handlerequest(abstracthttpconnection.java:454) at org.eclipse.jetty.server.blockinghttpconnection.handlerequest(blockinghttpconnection.java:47) at org.eclipse.jetty.server.abstracthttpconnection.content(abstracthttpconnection.java:900) at org.eclipse.jetty.server.abstracthttpconnection$requesthandler.content(abstracthttpconnection.java:954) at org.eclipse.jetty.http.httpparser.parsenext(httpparser.java:857) at org.eclipse.jetty.http.httpparser.parseavailable(httpparser.java:235) at org.eclipse.jetty.server.blockinghttpconnection.handle(blockinghttpconnection.java:66) at org.eclipse.jetty.server.bio.socketconnector$connectorendpoint.run(socketconnector.java:254) at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:599) at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:534) at java.lang.thread.run(thread.java:636) 18:16:45 severe solrcore org.apache.solr.common.solrexception: org.apache.solr.client.solrj.solrserverexception: no live solrservers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1, http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:300) at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:129) at org.apache.solr.core.solrcore.execute(solrcore.java:1656) at org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:454) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:275) at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1337) at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:484) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:119) at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:524) at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:233) at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1065) at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:413) at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:192) at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:999) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:117) at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:250) at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:149) at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:111) at org.eclipse.jetty.server.server.handle(server.java:351) at org.eclipse.jetty.server.abstracthttpconnection.handlerequest(abstracthttpconnection.java:454) at org.eclipse.jetty.server.blockinghttpconnection.handlerequest(blockinghttpconnection.java:47) at org.eclipse.jetty.server.abstracthttpconnection.headercomplete(abstracthttpconnection.java:890) at org.eclipse.jetty.server.abstracthttpconnection$requesthandler.headercomplete(abstracthttpconnection.java:944) at org.eclipse.jetty.http.httpparser.parsenext(httpparser.java:634) at org.eclipse.jetty.http.httpparser.parseavailable(httpparser.java:230) at org.eclipse.jetty.server.blockinghttpconnection.handle(blockinghttpconnection.java:66) at org.eclipse.jetty.server.bio.socketconnector$connectorendpoint.run(socketconnector.java:254) at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:599) at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:534) at java.lang.thread.run(thread.java:636) caused by: org.apache.solr.client.solrj.solrserverexception: no live solrservers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1, http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.client.solrj.impl.lbhttpsolrserver.request(lbhttpsolrserver.java:324) at org.apache.solr.handler.component.httpshardhandler$1.call(httpshardhandler.java:167) at org.apache.solr.handler.component.httpshardhandler$1.call(httpshardhandler.java:1) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334) at java.util.concurrent.futuretask.run(futuretask.java:166) at java.util.concurrent.executors$runnableadapter.call(executors.java:471) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334) at java.util.concurrent.futuretask.run(futuretask.java:166) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) ... 1 more caused by: org.apache.solr.common.solrexception: server at http://192.168.50.78:8985/solr/ac201209w3_s1 returned non ok status:500, message:server error at org.apache.solr.client.solrj.impl.httpsolrserver.request(httpsolrserver.java:373) at org.apache.solr.client.solrj.impl.httpsolrserver.request(httpsolrserver.java:182) at org.apache.solr.client.solrj.impl.lbhttpsolrserver.request(lbhttpsolrserver.java:289) ... 10 more 18:16:45 severe solrdispatchfilter null:org.apache.solr.common.solrexception: org.apache.solr.client.solrj.solrserverexception: no live solrservers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1, http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:300) at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:129) at org.apache.solr.core.solrcore.execute(solrcore.java:1656) at org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:454) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:275) at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1337) at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:484) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:119) at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:524) at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:233) at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1065) at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:413) at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:192) at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:999) at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:117) at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:250) at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:149) at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:111) at org.eclipse.jetty.server.server.handle(server.java:351) at org.eclipse.jetty.server.abstracthttpconnection.handlerequest(abstracthttpconnection.java:454) at org.eclipse.jetty.server.blockinghttpconnection.handlerequest(blockinghttpconnection.java:47) at org.eclipse.jetty.server.abstracthttpconnection.headercomplete(abstracthttpconnection.java:890) at org.eclipse.jetty.server.abstracthttpconnection$requesthandler.headercomplete(abstracthttpconnection.java:944) at org.eclipse.jetty.http.httpparser.parsenext(httpparser.java:634) at org.eclipse.jetty.http.httpparser.parseavailable(httpparser.java:230) at org.eclipse.jetty.server.blockinghttpconnection.handle(blockinghttpconnection.java:66) at org.eclipse.jetty.server.bio.socketconnector$connectorendpoint.run(socketconnector.java:254) at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:599) at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:534) at java.lang.thread.run(thread.java:636) caused by: org.apache.solr.client.solrj.solrserverexception: no live solrservers available to handle this request:http://192.168.50.78:8985/solr/ac201209w3_s1, http://192.168.50.76:8985/solr/ac201209w3_s1 at org.apache.solr.client.solrj.impl.lbhttpsolrserver.request(lbhttpsolrserver.java:324) at org.apache.solr.handler.component.httpshardhandler$1.call(httpshardhandler.java:167) at org.apache.solr.handler.component.httpshardhandler$1.call(httpshardhandler.java:1) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334) at java.util.concurrent.futuretask.run(futuretask.java:166) at java.util.concurrent.executors$runnableadapter.call(executors.java:471) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334) at java.util.concurrent.futuretask.run(futuretask.java:166) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) ... 1 more caused by: org.apache.solr.common.solrexception: server at http://192.168.50.78:8985/solr/ac201209w3_s1 returned non ok status:500, message:server error at org.apache.solr.client.solrj.impl.httpsolrserver.request(httpsolrserver.java:373) at org.apache.solr.client.solrj.impl.httpsolrserver.request(httpsolrserver.java:182) at org.apache.solr.client.solrj.impl.lbhttpsolrserver.request(lbhttpsolrserver.java:289) ... 10 more",
        "label": 40
    },
    {
        "text": "create test framework src test we have quite a few tests (~30 suites) for test-framework stuff (\"test-the-tester\") but currently they all sit in lucene/core housed with real tests. i think we should just give test-framework a src/test and move these tests there. this makes the build simpler in the future too, because its less \"special\".",
        "label": 40
    },
    {
        "text": "indexwriter doesn't process all events on getreader   close   rollback indexwriter misses to apply all pending events in getreader() as well as close() / rollback(). this can lead to files that never get deleted or only very very late. if you are using ram directories for instance this quickly fills up a huge amount of memory. it might not be super critical in production and it also doesn't cause any data loss or index corruption.",
        "label": 46
    },
    {
        "text": "edgengram  documentation improvement to clarify what \"edge\" means, i added some description. that edge means the beggining edge of a term or ending edge of a term.",
        "label": 15
    },
    {
        "text": "multisearcher explain returns incorrect score explanation relating to docfreq creating 2 different indexes, searching each individually and print score details and compare to searching both indexes with mulitsearcher and printing score details. the \"docfreq\" value printed isn't correct - the values it prints are as if each index was searched individually. code is like: multisearcher multi = new multisearcher(searchables); hits hits = multi.search(query); for(int i=0; i<hits.length(); i++) {   explanation expl = multi.explain(query, hits.id(i));   system.out.println(expl.tostring()); } i raised this in the lucene user mailing list and was advised to log a bug, email thread given below.   -----original message----- from: chris hostetter   sent: friday, december 07, 2007 10:30 pm to: java-user subject: re: does the multisearcher class calculate idf properly? a quick glance at the code seems to indicate that multisearcher has code  for calcuating the docfreq accross all of the searchables when searching  (or when the docfreq method is explicitly called) but that explain method  just delegates to searchable that the specific docid came from. if you compare that explanation score you got with the score returned by  a hitcollector (or topdocs) they probably won't match. so i would say \"yes multisearcher calculates idf properly, but  multiseracher.explain is broken.  please file a bug about this, i can't  think of an easy way to fix it, but it certianly seems broken to me. : subject: does the multisearcher class calculate idf properly? :  : i tried the following.  creating 2 different indexes, search each : individually and print score details and compare to searching both : indexes with mulitsearcher and printing score details.   :  : the \"docfreq\" value printed don't seem right - is this just a problem : with using explain together with the multisearcher? :  :  : code is like: : multisearcher multi = new multisearcher(searchables); : hits hits = multi.search(query); : for(int i=0; i<hits.length(); i++) : { :   explanation expl = multi.explain(query, hits.id(i)); :   system.out.println(expl.tostring()); : } :  :  : output: : id = 14 score = 0.071 : 0.07073946 = (match) fieldweight(contents:climate in 2), product of: :   1.0 = tf(termfreq(contents:climate)=1) :   1.8109303 = idf(docfreq=1) :   0.0390625 = fieldnorm(field=contents, doc=2)",
        "label": 29
    },
    {
        "text": "broken javadocs site docs links see the java-dev mailing list discussion: http://www.nabble.com/broken-javadocs-%3esite-docs-links-to20369092.html. when the lucene java website transitioned to versioning some of the documentation, links from some javadocs were not modified to follow the resources. i found broken links to gettingstarted.html and queryparsersyntax.html. here is one example, to gettingstarted.html (the link text is \"demo\"): http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/package-summary.html the attached patch converts absolute urls from javadocs to versioned docs to be relative, and modifies the \"javadocs-all\" target in build.xml to add a path element named \"all\", so that both versions of the javadocs (all: core+contrib; and separated: core, contribs) can use the same relative urls. adding a path element to the \"javadocs-all\" target is necessary because currently the \"all\" javadocs have one fewer path element than the separated javadocs. i left as-is one absolute url, in the o.a.l.index.segmentinfos javadocs, to fileformats.html, because segmentinfos is a package-private class, and the javadocs targets in build.xml only generate javadocs for public classes.",
        "label": 33
    },
    {
        "text": "simpleanalyzer and whitespaceanalyzer should have version ctors due to the changes to chartokenizer ( lucene-2183 ) whitespaceanalyzer and simpleanalyzer need a version ctor. default ctors must be deprecated",
        "label": 53
    },
    {
        "text": "testdocvaluesindexing testaddindexes failures on docvalues branch doc values branch r1124825, reproducible     [junit] testsuite: org.apache.lucene.index.values.testdocvaluesindexing     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0.716 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testdocvaluesindexing -dtestmethod=testaddindexes -dtests.seed=5939035003978436534:-6429764582682717131     [junit] note: test params are: codec=randomcodecprovider: {id=mockrandom, bytes_var_deref=mockrandom, ints=pulsing(freqcutoff=13)}, locale=da_dk, timezone=asia/macao     [junit] note: all tests run in this jvm:     [junit] [testdocvaluesindexing]     [junit] note: linux 2.6.37-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88582432,total=125632512     [junit] ------------- ---------------- ---------------     [junit] testcase: testaddindexes(org.apache.lucene.index.values.testdocvaluesindexing):     caused an error     [junit] null     [junit] java.nio.channels.closedchannelexception     [junit]     at sun.nio.ch.filechannelimpl.ensureopen(filechannelimpl.java:88)     [junit]     at sun.nio.ch.filechannelimpl.read(filechannelimpl.java:603)     [junit]     at org.apache.lucene.store.niofsdirectory$niofsindexinput.readinternal(niofsdirectory.java:161)     [junit]     at org.apache.lucene.store.bufferedindexinput.refill(bufferedindexinput.java:222)     [junit]     at org.apache.lucene.store.bufferedindexinput.readbyte(bufferedindexinput.java:39)     [junit]     at org.apache.lucene.store.datainput.readint(datainput.java:73)     [junit]     at org.apache.lucene.store.bufferedindexinput.readint(bufferedindexinput.java:162)     [junit]     at org.apache.lucene.store.datainput.readlong(datainput.java:115)     [junit]     at org.apache.lucene.store.bufferedindexinput.readlong(bufferedindexinput.java:175)     [junit]     at org.apache.lucene.store.mockindexinputwrapper.readlong(mockindexinputwrapper.java:136)     [junit]     at org.apache.lucene.index.values.packedintsimpl$intsenumimpl.<init>(packedintsimpl.java:263)     [junit]     at org.apache.lucene.index.values.packedintsimpl$intsenumimpl.<init>(packedintsimpl.java:249)     [junit]     at org.apache.lucene.index.values.packedintsimpl$intsreader.getenum(packedintsimpl.java:239)     [junit]     at org.apache.lucene.index.values.docvalues.getenum(docvalues.java:54)     [junit]     at org.apache.lucene.index.values.testdocvaluesindexing.getvaluesenum(testdocvaluesindexing.java:484)     [junit]     at org.apache.lucene.index.values.testdocvaluesindexing.testaddindexes(testdocvaluesindexing.java:202)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1304)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1233)     [junit]      [junit]      [junit] test org.apache.lucene.index.values.testdocvaluesindexing failed and     [junit] testsuite: org.apache.lucene.index.values.testdocvaluesindexing     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0.94 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testdocvaluesindexing -dtestmethod=testaddindexes -dtests.seed=-3677966427932339626:-4746638811786223564     [junit] note: test params are: codec=randomcodecprovider: {id=standard, bytes_fixed_deref=mocksep, float_64=simpletext}, locale=ca, timezone=asia/novosibirsk     [junit] note: all tests run in this jvm:     [junit] [testdocvaluesindexing]     [junit] note: linux 2.6.37-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88596152,total=125632512     [junit] ------------- ---------------- ---------------     [junit] testcase: testaddindexes(org.apache.lucene.index.values.testdocvaluesindexing):     caused an error     [junit] bad file descriptor     [junit] java.io.ioexception: bad file descriptor     [junit]     at java.io.randomaccessfile.seek(native method)     [junit]     at org.apache.lucene.store.simplefsdirectory$simplefsindexinput.readinternal(simplefsdirectory.java:101)     [junit]     at org.apache.lucene.store.bufferedindexinput.refill(bufferedindexinput.java:222)     [junit]     at org.apache.lucene.store.bufferedindexinput.readbyte(bufferedindexinput.java:39)     [junit]     at org.apache.lucene.store.mockindexinputwrapper.readbyte(mockindexinputwrapper.java:105)     [junit]     at org.apache.lucene.index.values.floats$floatsreader.load(floats.java:281)     [junit]     at org.apache.lucene.index.values.sourcecache$directsourcecache.load(sourcecache.java:101)     [junit]     at org.apache.lucene.index.values.docvalues.getsource(docvalues.java:101)     [junit]     at org.apache.lucene.index.values.testdocvaluesindexing.getsource(testdocvaluesindexing.java:472)     [junit]     at org.apache.lucene.index.values.testdocvaluesindexing.getvaluesenum(testdocvaluesindexing.java:482)     [junit]     at org.apache.lucene.index.values.testdocvaluesindexing.testaddindexes(testdocvaluesindexing.java:203)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1304)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1233)     [junit]      [junit]      [junit] test org.apache.lucene.index.values.testdocvaluesindexing failed",
        "label": 46
    },
    {
        "text": "release resources on uima tokenizers close  baseuimatokenizer should release analysisengine and cas on close()",
        "label": 50
    },
    {
        "text": "allow usage of hyphenationcompoundwordtokenfilter without dictionary we should allow to use the hyphenationcompoundwordtokenfilter without a dictionary. this produces a lot of \"nonword\" tokens but might be useful sometimes.",
        "label": 40
    },
    {
        "text": "add smoketester or test check that indexes are created with the correct codecs  this wasted a bunch of my time this morning. tests should fail.",
        "label": 41
    },
    {
        "text": "remove leafreader fields  fields seems like a pointless intermediary between the leafreader and terms. why not have leafreader.getterms(fieldname) instead? one loses the ability to get the count and iterate over indexed fields, but it's not clear what real use-cases are for that and such rare needs could figure that out with fieldinfos. michael mccandless pointed out that we'd probably need to re-introduce a termvectors class since tv's are row-oriented not column-oriented. imo they should be column-oriented but that'd be a separate issue. (p.s. i'm lacking time to do this w/i the next couple months so if someone else wants to tackle it then great)",
        "label": 10
    },
    {
        "text": "geoexactcircle should not create circles that they do not fit in spheroid hi karl wright, i have seen test fail when we try to create circles that the don' t fit in the planet. i think sectors of the circle start overlapping to each other and the shape becomes invalid. the shape should prevent that to happen. i will attach a test and a proposed solution.",
        "label": 25
    },
    {
        "text": "detach indexwriter from mergepolicy this change introduces a new mergepolicy.mergecontext interface that is easy to mock and cuts over all instances of iw to mergecontext. since iw now implements mergecontext the cut over is straight forward. this reduces the exposed api available in mp dramatically and allows efficient testing without relying on iw to improve the coverage and testability of our mp implementations.",
        "label": 46
    },
    {
        "text": "fix computation of the allowed segment count in tieredmergepolicy lucene-7976 removed the logic that decreases 'totindexbytes` when a segment is graced out because it is too large. this makes 'allowedsegmentcount' overestimated.",
        "label": 13
    },
    {
        "text": "ant precommit should remind people to run clean jars and jar checksums when checksums are not right ivy's bug that fails to remove differently versioned dependencies in (test-)lib/ dirs even though we set sync=\"true\" on <ivy:retrieve> (i couldn't find a jira for this) continues to cause trouble/confusion (see related lucene-5467). we should make the ant precommit target depend on clean-jars, so that people won't think they need to run ant jar-checksums because of stale jars ivy leaves in lib/ or test-lib/ directories, which currently causes ant precommit to bitch that there are missing checksums.",
        "label": 47
    },
    {
        "text": "lucene deletes entire index if and exception is thrown due do toomanyopenfiles and openmode create or append the lucene indexwriter might delete an entire index if it hits a filenotfoundexception triggered by toomanyopenfiles during indexwriter creation. we try to figure out if the index exists already if the openmode.create_or_append is set (which is default). yet, the logic in directoryreader#indexexists(directory) will just return false if we are not able to open the segment file. this will cause the iw to assume there is no index and it will try to create a new index there trashing all existing commit points treating this as a openmode.create.",
        "label": 46
    },
    {
        "text": "replace collation lib icu4j jar with a smaller icu jar collation does not need all the icu data. we can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables.",
        "label": 29
    },
    {
        "text": "enable bypassing docvalues check in doctermords currently, doctermords refuses to build if doc values have been enabled for a field. while good for catching bugs, this disabled what can be legitimate use cases (such as just trying out an alternate method w/o having to re-configure and re-index, or even using consistently in conjunction with uninvertingreader). we should restore the ability to use this class in other scenarios via adding a flag to bypass the check.",
        "label": 55
    },
    {
        "text": "improve bbox areasimilarity algorithm to consider lines and points geoportal's area overlap algorithm didn't consider lines and points; they end up turning the score 0. i've thought about this for a bit and i've come up with an alternative scoring algorithm. (already coded and tested and documented): new javadocs: /**  * the algorithm is implemented as envelope on envelope overlays rather than  * complex polygon on complex polygon overlays.  * <p/>  * <p/>  * spatial relevance scoring algorithm:  * <dl>  *   <dt>queryarea</dt> <dd>the area of the input query envelope</dd>  *   <dt>targetarea</dt> <dd>the area of the target envelope (per lucene document)</dd>  *   <dt>intersectionarea</dt> <dd>the area of the intersection between the query and target envelopes</dd>  *   <dt>querytargetproportion</dt> <dd>a 0-1 factor that divides the score proportion between query and target.  *   0.5 is evenly.</dd>  *  *   <dt>queryratio</dt> <dd>intersectionarea / queryarea; (see note)</dd>  *   <dt>targetratio</dt> <dd>intersectionarea / targetarea; (see note)</dd>  *   <dt>queryfactor</dt> <dd>queryratio * querytargetproportion;</dd>  *   <dt>targetfactor</dt> <dd>targetratio * (1 - querytargetproportion);</dd>  *   <dt>score</dt> <dd>queryfactor + targetfactor;</dd>  * </dl>  * note: the actual computation of queryratio and targetratio is more complicated so that it considers  * points and lines. lines have the ratio of overlap, and points are either 1.0 or 0.0 depending on wether  * it intersects or not.  * <p />  * based on geoportal's  * <a href=\"http://geoportal.svn.sourceforge.net/svnroot/geoportal/geoportal/trunk/src/com/esri/gpt/catalog/lucene/spatialrankingvaluesource.java\">  *   spatialrankingvaluesource</a> but modified. geoportal's algorithm will yield a score of 0  * if either a line or point is compared, and it's doesn't output a 0-1 normalized score (it multiplies the factors).  *  * @lucene.experimental  */",
        "label": 10
    },
    {
        "text": "check legal isn't doing its job in trunk, the check-legal-lucene ant target is not checking any lucene/contrib/**/lib/ directories; the modules/**/lib/ directories are not being checked; and check-legal-solr can't be checking solr/example/lib/**/*.jar, because there are currently .jar files in there that don't have a license. these targets are set up to take in a full list of lib/ directories in which to check, but modules move around, and these lists are not being kept up-to-date. instead, check-legal-* should run for each module, if the module has a lib/ directory, and it should be specialized for modules that have more than one (solr/core/) or that have a lib/ directory in a non-standard place (lucene/core/).",
        "label": 11
    },
    {
        "text": "separate getting directory from indexreader from its concrete subclasses currently only subclasses of directoryreader expose the underlying directory via public final directory(). imho this aspect should be separated from directoryreader so that other indexreader implementations could expose any underlying directory if they wanted to. specifically, i have a use case where i'd like to expose a synthetic directory view of resources used for parallelcompositereader.",
        "label": 53
    },
    {
        "text": "added appendingpackedlongbuffer   extended abstractappendinglongbuffer family  customizable compression ratio   bulk retrieval  made acceptableoverheadratio configurable added bulk get to abstractappendinglongbuffer classes, for faster retrieval. introduced a new variant, appendingpackedlongbuffer which solely relies on packedints as a back-end. this new class is useful where people have non-negative numbers with a fairly uniform distribution over a fixed (limited) range. ex. facets ordinals. to distinguish it from appendingpackedlongbuffer, delta based appendinglongbuffer was renamed to appendingdeltapackedlongbuffer fixed an issue with nullreader where it didn't respect it's valuecount in bulk gets.",
        "label": 1
    },
    {
        "text": "demo html parser gives incorrect summaries when title is repeated as a heading if you have an html document where the title is repeated as a heading at the top of the document, the htmlparser will return the title as the summary, ignoring everything else that was added to the summary. instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). i don't see any benefit to repeating the title in the summary for any case. in htmlparser.jj's getsummary(): string sum = summary.tostring().trim(); string tit = gettitle(); if (sum.startswith(tit) || sum.equals(\"\")) return tit; else return sum; change it to: (* denotes a line that has changed) string sum = summary.tostring().trim(); string tit = gettitle(); if (sum.startswith(tit)) // don't repeat title in summary return sum.substring(tit.length()).trim(); else return sum;",
        "label": 40
    },
    {
        "text": "searchermanager afterrefresh  issues 1) referencemanager.domayberefresh seems to call afterrefresh even if it didn't refresh/swap, (when newreference == null) 2) it would be nice if users were allowed to override searchermanager.afterrefresh() to get notified when a new searcher is in action. but searchermanager and readermanager are final, while nrtmanager is not. the only way to currently hook into when a new searched is created is using the factory, but if you wish to do some async task then, there are no guarantees that acquire() will return the new searcher, so you have to pass it around and incref manually. while if allowed to hook into afterrefresh you can just rely on acquire() & existing infra you have around it to give you the latest one.",
        "label": 33
    },
    {
        "text": "rethink localizedtestcaserunner with junit   clover oom as a spinn off from this conversation we should rethink the way how we execute testcases with different locals since glover reports appears to throw oom errors b/c junit treats each local as a single test case run. here are some options: select the local at random only run the test with a single local set the local via system property -dtest.locale=en.en run with the default locale only -dtest.skiplocale=true one from the above but only if instrumented with clover (let common tests run all the locale)",
        "label": 40
    },
    {
        "text": "dists include analyzer contrib in src dist but not binary dist dists include analyzer contrib in src dist but not binary dist",
        "label": 29
    },
    {
        "text": "port to java5 for my needs i've updated lucene so that it uses java 5 constructs. i know java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. this patch against the trunk includes : most obvious generics usage (there are tons of usages of sets, ... those which are commonly used have been generified) priorityqueue generification replacement of indexed for loops with for each constructs removal of unnececessary unboxing the code is to my opinion much more readable with those features (you actually know what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms. note that this patch also includes an interface for the query class. this has been done for my company's needs for building custom query classes which add some behaviour to the base lucene queries. it prevents multiple unnnecessary casts. i know this introduction is not wanted by the team, but it really makes our developments easier to maintain. if you don't want to use this, replace all /queriable/ calls with standard /query/.",
        "label": 53
    },
    {
        "text": "deleting docs of all returned hits during search causes arrayindexoutofboundsexception for background user discussion: http://www.nabble.com/document-deletion-problem-to14414351.html hits h = m_indexsearcher.search(q); // returns 11475 documents  for(int i = 0; i < h.length(); i++)  {    int doc = h.id(i);    m_indexsearcher.getindexreader().deletedocument(doc);  <-- causes arrayindexoutofboundsexception when i = 6400 } ",
        "label": 12
    },
    {
        "text": "mistake in the comment in source  surround parser queryparser jj the comment in the source regarding surround query parser \"n is ordered, and w is unordered.\" is a mistake. should be the other way around. appears in in org.apache.lucene.queryparser.surround.parser queryparser.jj",
        "label": 14
    },
    {
        "text": "improve basetokenstreamtestcase to test end  if offsetatt/end() is not implemented correctly, then there can be problems with highlighting: see lucene-2207 for an example with cjktokenizer. in my opinion you currently have to write too much code to test this. this patch does the following: adds optional integer finaloffset (can be null for no checking) to asserttokenstreamcontents in assertanalyzesto, automatically fill this with the string length() in my opinion this is correct, for asserttokenstreamcontents the behavior should be optional, it may not even have a tokenizer. if you are using asserttokenstreamcontents with a tokenizer then simply provide the extra expected value to check it. for assertanalyzesto then it is implied there is a tokenizer so it should be checked. the tests pass for core but there are failures in contrib even besides cjktokenizer (apply koji's patch from lucene-2207, it is correct). specifically chinesetokenizer has a similar problem.",
        "label": 40
    },
    {
        "text": "benchmark geospatial performance based on geonames org see comments for details. in particular, the original patch \"benchmark-geo.patch\" is fairly different than lucene-2844.patch",
        "label": 10
    },
    {
        "text": "clover setup currently has some problems (tracking as a bug before it get lost in email... http://www.nabble.com/clover-reports-missing-from-hudson--to15510616.html#a15510616 ) the clover setup for lucene currently has some problems, 3 i think... 1) instrumentation fails on contrib/db/ because it contains java packages the asf clover lscence doesn't allow instrumentation of. i have a patch for this. 2) running instrumented contrib tests for other contribs produce strange errors... monospaced [junit] testsuite: org.apache.lucene.analysis.el.greekanalyzertest [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0.126 sec [junit] [junit] ------------- standard error ----------------- [junit] [clover] fatal error: clover could not be initialised. are you sure you have clover in the runtime classpath? (class java.lang.noclassdeffounderror:com_cenqua_clover/cloverversioninfo) [junit] ------------- ---------------- --------------- [junit] testcase: testanalyzer(org.apache.lucene.analysis.el.greekanalyzertest): caused an error [junit] com_cenqua_clover/g [junit] java.lang.noclassdeffounderror: com_cenqua_clover/g [junit] at org.apache.lucene.analysis.el.greekanalyzer.<init>(greekanalyzer.java:157) [junit] at org.apache.lucene.analysis.el.greekanalyzertest.testanalyzer(greekanalyzertest.java:60) [junit] [junit] [junit] test org.apache.lucene.analysis.el.greekanalyzertest failed monospaced ...i'm not sure what's going on here. the error seems to happen both when trying to run clover on just a single contrib, or when doing the full build ... i suspect there is an issue with the way the batchtests fork off, but i can't see why it would only happen to contribs (the regular tests fork as well) 3) according to grant... quote ...there is also a bit of a change on hudson during the migration to the new servers that needs to be ironed out. quote",
        "label": 15
    },
    {
        "text": "small imprecision in search package javadocs search package javadocs states that scorer#score(collector) will be abstract in lucene 3.0, which is not accurate anymore.",
        "label": 33
    },
    {
        "text": "fieldcache introspection api fieldcache should expose an expert level api for runtime introspection of the fieldcache to provide info about what is in the fieldcache at any given moment. we should also provide utility methods for sanity checking that the fieldcache doesn't contain anything \"odd\"... entries for the same reader/field with different types/parsers entries for the same field/type/parser in a reader and it's subreader(s) etc...",
        "label": 18
    },
    {
        "text": "similaritydelegator broke back compat for subclasses overriding lengthnorm in lucene-1420, we added similarity.computenorm to let the norm computation have access to the raw information (length, boost, etc.). but this class broke back compat with similaritydelegator. we did add computenorm there, but, it's impl just forwards to the delegee's computenorm. in the case where a subclass of similaritydelegator overrides lengthnorm, that method will no longer be invoked. not quite sure how to fix this since, somehow, we have to determine whether the delegee's impl of computenorm should be favored over the subclasses impl of the \"legacy\" lengthnorm.",
        "label": 40
    },
    {
        "text": "rollback doesn't preserve integrity of original index after several \"updatedocuments\" calls a rollback call does not return the index to the prior state. this seems to occur if the number of updates exceeds the ram buffer size i.e. when some flushing of updates occurs. test fails in lucene 2.4, 2.9, 3.0.1 and 3.0.2 junit to follow.",
        "label": 33
    },
    {
        "text": "termvectormapper setdocumentnumber  passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader. see http://www.nabble.com/allowing-ioexceptions-in-termvectormapper--tf4687704.html#a13397341",
        "label": 15
    },
    {
        "text": "post grouping faceting this issues focuses on implementing post grouping faceting. how to handle multivalued fields. what field value to show with the facet. where the facet counts should be based on facet counts can be based on the normal documents. ungrouped counts. facet counts can be based on the groups. grouped counts. facet counts can be based on the combination of group value and facet value. matrix counts. and properly more implementation options. the first two methods are implemented in the solr-236 patch. for the first option it calculates a docset based on the individual documents from the query result. for the second option it calculates a docset for all the most relevant documents of a group. once the docset is computed the facetcomponent and statscomponent use one the docset to create facets and statistics. this last one is a bit more complex. i think it is best explained with an example. lets say we search on travel offers: hotel departure_airport duration hotel a ams 5 hotel a dus 10 hotel b ams 5 hotel b ams 10 if we group by hotel and have a facet for airport. most end users expect (according to my experience off course) the following airport facet: ams: 2 dus: 1 the above result can't be achieved by the first two methods. you either get counts ams:3 and dus:1 or 1 for both airports.",
        "label": 31
    },
    {
        "text": "memoryindex does not allow user to add multiple values for a single field name when using memoryindex.addfield the following operation throws an illegalargumentexception: index.addfield(\"foobar\", \"value1\", luceneanalyzer); index.addfield(\"foobar\", \"value2\", luceneanalyzer); this throws: java.lang.illegalargumentexception: field must not be added more than once according to uwe schindler on the java-user mailing list this violates the indexwriter/indexreader contract.",
        "label": 46
    },
    {
        "text": "example code in overview html uses deprecated syntax the examples should use non-deprecated syntax only. im' attaching a patch, but other parts of that page might also be out-of-date, which i didn't check now.",
        "label": 29
    },
    {
        "text": "hits does not use multisearcher's createweight i am developing a distributed index, using multisearcher and remotesearcher. when investigating some performance issues, i noticed that there is a lot of back-and-forth traffic between the servers during the weight calculation. although multisearcher has a method called createweight that minimizes the calls to the sub-searchers, this method never actually gets called when i call search(query). from what i can tell, this is fixable by changing in hits.java the line: weight = q.weight(s); to: weight = s.createweight(q);",
        "label": 38
    },
    {
        "text": "move spanquery getspans  to spanweight spanquery.getspans() should only be called on rewritten queries, so it seems to make more sense to have this being called from spanweight",
        "label": 2
    },
    {
        "text": "boostingnearquery class  prototype  this patch implements term boosting for spannearquery. refer to: http://www.gossamer-threads.com/lists/lucene/java-user/62779 this patch works but probably needs more work. i don't like the use of 'instanceof', but i didn't want to touch spans or termspans. also, the payload code is mostly a copy of what's in boostingtermquery and could be common-sourced somewhere. feel free to throw darts at it",
        "label": 15
    },
    {
        "text": "remove unused code in smartchineseanalyzer hmm pkg there is some unused code in the hmm package. i would like to remove it before i supply a fix for lucene-1817. only after this can we refactor any of this analyzer, otherwise we risk breaking custom dictionary support.",
        "label": 40
    },
    {
        "text": "coordconstrainedbooleanquery   queryparser support attached 2 new classes: 1) coordconstrainedbooleanquery a boolean query that only matches if a specified number of the contained clauses match. an example use might be a query that returns a list of books where any 2 people from a list of people were co-authors, eg: \"lucene in action\" would match (\"erik hatcher\" \"otis gospodneti\u0107\" \"mark harwood\" \"doug cutting\") with a minrequiredoverlap of 2 because otis and erik wrote that. the book \"java development with ant\" would not match because only 1 element in the list (erik) was selected. 2) customqueryparserexample a customised queryparser that allows definition of coordconstrainedbooleanqueries. the solution (mis)uses fieldnames to pass parameters to the custom query.",
        "label": 55
    },
    {
        "text": "add subset method to bitvector recently i needed the ability to efficiently compute subsets of a bitvector. the method is: public bitvector subset(int start, int end) where \"start\" is the starting index, inclusive and \"end\" is the ending index, exclusive. attached is a patch including the subset method as well as relevant unit tests.",
        "label": 33
    },
    {
        "text": "concurrentschedulemanager addmyself  has wrong inted this method has the wrong index for the 'size' variable, i think it should b allinstances.size. private void addmyself() {     synchronized(allinstances) {       final int size=0;       int upto = 0;       for(int i=0;i<size;i++) {         final concurrentmergescheduler other = (concurrentmergescheduler) allinstances.get(i);         if (!(other.closed && 0 == other.mergethreadcount()))           // keep this one for now: it still has threads or           // may spawn new threads           allinstances.set(upto++, other);       }       allinstances.sublist(upto, allinstances.size()).clear();       allinstances.add(this);     }   }",
        "label": 33
    },
    {
        "text": "exception when facetscollector is used with scorefacetrequest aggregating facets with lucene's score using facetscollector results in an exception (assertion when enabled).",
        "label": 43
    },
    {
        "text": "fsdirectory ctor should use getabsolutepath instead of getrealpath for directory after upgrade from 4.1 to 5.2.1 i found that one of our test failed. appeared the guilty was fsdirectory that converts given path to path.getrealpath. as result the test will fail: path p = paths.get(\"/var/lucene_store\"); fsdirectory d = new fsdirectory(p); assertequals(p.tostring(), d.getdirectory().tostring()); it because /var/lucene_store is a symlink and path directory =path.getrealpath(); resolves it to /private/var/lucene_store i think this is bad design decision because \"direcrory\" isn't just internal state but is exposed in a public interface and \"getdirectory()\" is widely used to initialize other components. it should use paths.getabsolutepath() instead. build and \"ant test\" were successful after fix.",
        "label": 53
    },
    {
        "text": "fix directory implementations to use nio2 apis i have implemented 3 directory subclasses using nio2 api's (available on jdk7). these may be suitable for inclusion in a lucene contrib module. see the mailing list at http://lucene.markmail.org/thread/lrv7miivzmjm3ml5 for more details about this code and the advantages it provides. the code is attached as a zip to this issue. i'll be happy to make any changes requested. i've included some minimal smoke tests, but any help in how to use the normal lucene tests to perform more thorough testing would be appreciated.",
        "label": 53
    },
    {
        "text": "dwpt doesn't see changes to dw infostream dw does not push infostream changes to dwpt since dwpt#infostream is final and initialized on dwptpool initialization (at least for initial dwpt) we should push changes to infostream to dwpt too",
        "label": 46
    },
    {
        "text": "dwptpool shouldnt be public  its methods have pkg private parameters  so you really need to be in the o.a.l.index package anyway... can we just make this pkg-private?",
        "label": 33
    },
    {
        "text": "make it easier to mix different kinds of facetrequests spinoff from lucene-4980, where we added a strange class called rangefacetsaccumulatorwrapper, which takes an incoming fsp, splits out the facetrequests into range and non-range, delegates to two accumulators for each set, and then zips the results back together in order. somehow we should generalize this class and make it work with sortedsetdocvaluesaccumulator as well.",
        "label": 43
    },
    {
        "text": "make tokenstreamcomponents final the current design is a little trappy. any specialised subclasses of tokenstreamcomponents (see standardanalyzer, classicanalyzer, uax29urlemailanalyzer) are discarded by any subsequent analyzers that wrap them (see limittokencountanalyzer, queryautostopwordanalyzer, shingleanalyzerwrapper and other examples in elasticsearch).  the current design means each analyzerwrapper.wrapcomponents() implementation discards any custom tokenstreamcomponents and replaces it with one of its own choosing (a vanilla tokenstreamcomponents class from examples i've seen). this is a trap i fell into when writing a custom tokenstreamcomponents with a custom setreader() and i wondered why it was not being triggered when wrapped by other analyzers. if analyzerwrapper is designed to encourage composition it's arguably a mistake to also permit custom tokenstreamcomponent subclasses  - the composition process does not preserve the choice of custom classes and any behaviours they might add. for this reason we should not encourage extensions to tokenstreamcomponents (or if tsc extensions are required we should somehow mark an analyzer as \"unwrappable\" to prevent lossy compositions).    ",
        "label": 2
    },
    {
        "text": "arrayindexoutofboundsexception in pagedbytes reader fill with a very large index (in our case > 10g), we are seeing exceptions like: java.lang.arrayindexoutofboundsexception: -62400 at org.apache.lucene.util.pagedbytes$reader.fill(pagedbytes.java:116) at org.apache.lucene.search.fieldcacheimpl$binarydocvaluesimpl$1.get(fieldcacheimpl.java:1342) at org.apache.lucene.search.join.termscollector$sv.collect(termscollector.java:106) at org.apache.lucene.search.weight$defaultbulkscorer.scoreall(weight.java:193) at org.apache.lucene.search.weight$defaultbulkscorer.score(weight.java:163) at org.apache.lucene.search.bulkscorer.score(bulkscorer.java:35) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:621) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:309) the code in question is trying to allocate an array with a negative size. we believe the source of the error is in org.apache.lucene.search.fieldcacheimpl$binarydocvaluesimpl$1.get where the following code occurs: final int pointer = (int) doctooffset.get(docid); if (pointer == 0) { term.length = 0; } else { bytes.fill(term, pointer); } the cast to int will break if the (long) result of doctooffset.get is too large, and is unnecessary in the first place since bytes.fill takes a long as its second parameter. proposed fix: final long pointer = doctooffset.get(docid); if (pointer == 0) { term.length = 0; } else { bytes.fill(term, pointer); }",
        "label": 9
    },
    {
        "text": "smoketestrelease py on cygwin  errno  no such file or directory  $ python3 -u dev-tools/scripts/smoketestrelease.py https://dist.apache.org/repos/dist/dev/lucene/lucene-solr-6.3.0-rc2-rev1fe1a54db32b8c27bfae81887cd4d75242090613/ revision: 1fe1a54db32b8c27bfae81887cd4d75242090613 java 1.8 java_home=c:\\program files\\java\\jdk1.8.0_102 traceback (most recent call last): file \"dev-tools/scripts/smoketestrelease.py\", line 1440, in <module> main() file \"dev-tools/scripts/smoketestrelease.py\", line 1377, in main c = parse_config() file \"dev-tools/scripts/smoketestrelease.py\", line 1239, in parse_config c.java = make_java_config(parser, c.test_java8) file \"dev-tools/scripts/smoketestrelease.py\", line 1193, in make_java_config run_java8 = _make_runner(java8_home, '1.8') file \"dev-tools/scripts/smoketestrelease.py\", line 1179, in _make_runner java_home = subprocess.check_output('cygpath -u \"%s\"' % java_home).read().decode('utf-8').strip() file \"/usr/lib/python3.4/subprocess.py\", line 607, in check_output with popen(*popenargs, stdout=pipe, **kwargs) as process: file \"/usr/lib/python3.4/subprocess.py\", line 859, in _init_ restore_signals, start_new_session) file \"/usr/lib/python3.4/subprocess.py\", line 1457, in _execute_child raise child_exception_type(errno_num, err_msg) filenotfounderror: [errno 2] no such file or directory: 'cygpath -u \"c: program files\\\\java jdk1.8.0_102\"' giving the doc path and args should be either supplied as array of terms or supplied as shell=true",
        "label": 35
    },
    {
        "text": "remaining reallocation should use arrayutil getnextsize  see recent discussion on arrayutils.getnextsize().",
        "label": 33
    },
    {
        "text": "new document and field api this is a super rough prototype of how a new document api could look like. it's basically what i came up with during a long flight across the atlantic it is not integrated with anything yet (like indexwriter, documentswriter, etc.) and heavily uses java 1.5 features, such as generics and annotations. the general idea sounds similar to what marvin is doing in ks, which i found out by reading mike's comments on lucene-831, i haven't looked at the ks api myself yet. main ideas: separate a field's value from its configuration; therefore this patch introduces two classes: fielddescriptor and fieldvalue i was thinking that in most cases the documents people add to a lucene index look alike, i.e. they contain mostly the same fields with the same settings. yet, for every field instance the documentswriter checks the settings and calls the right consumers, which themselves check settings and return true or false, indicating whether or not they want to do something with that field or not. so i was thinking we could design the document api similar to the class<->object concept of oo-languages. there a class is a blueprint (as everyone knows ), and an object is one instance of it. so in this patch i introduced a class called documentdescriptor, which contains all fielddescriptors with the field settings. this descriptor is given to the consumer (indexwriter) once in the constructor. then the document \"instances\" are created and added via adddocument(). a document instance allows adding \"variable fields\" in addition to the \"fixed fields\" the documentdescriptor contains. for these fields the consumers have to check the field settings for every document instance (like with the old document api). this is for maintaining lucene's flexibility that everyone loves. disregard the changes to attributesource for now. the code that's worth looking at is contained in a new package \"newdoc\". again, this is not a \"real\" patch, but rather a demo of how a new api could roughly work.",
        "label": 32
    },
    {
        "text": "deprecate spatial contrib the spatial contrib is blighted by bugs. the latest series, found by grant and discussed here shows that we need to re-think the cartesian tier implementation. given the need to create a spatial module containing code taken from both lucene and solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.",
        "label": 10
    },
    {
        "text": "javadoc error when run in private access level javadoc error when run in private access level.",
        "label": 9
    },
    {
        "text": "allow timelimitedcollector timer thread to be shutdown when using tomcat 6 and solr 1.3 (with lucene 2.4) we found that if we caused tomcat to reload our .war files a number of times, we would eventually see permgen memory errors where the jvm' s gc reported that all \"permanent generation\" memory had been consumed and none could be freed. this turns out to be a fairly common issue when using tomcat's autodeploy feature (or similar features of other application servers). see, for example: http://ampedandwired.dreamhosters.com/2008/05/09/causes-of-java-permgen-memory-leaks/ http://cornelcreanga.com/2009/02/how-to-prevent-memory-leaks-when-reloading-web-applications/ http://www.samaxes.com/2007/10/classloader-leaks-and-permgen-space/ http://blogs.sun.com/fkieviet/entry/how_to_fix_the_dreaded my understanding of the issue is that when reloading a webapp, tomcat starts by releasing all of its references to the classloader used to load the previous version of the application. then it creates a new classloader which reloads the application. the old classloader and old version of the app are left to the garbage collector to be cleaned up. however, if the app itself hold references to the classloader, the gc may not be able to ascertain that the classloader is truly unused, in which case, it and the entire old version of app remain in memory. if one causes a sufficient number of app reloads, eventually permgen space is exhausted. the particular issue we had with solr and lucene was that lucene's timelimitedcollector creates a thread which is not shut down anywhere; this in turn seems to prevent tomcat from unloading the classloader. to solve this i applied a minor patch to timelimitedcollector which adds a flag variable controlling the timer thread's loop and some methods to set it so the thread will exit. the stopthread() method can then be called by an application such as solr from a class registered as a servlet context listener; when the server is unloading the application the listener will execute and in turn stop the timer thread. my testing during multiple reloads of solr.war with and without these patches indicates that without them, we consistently get permgen errors, and with them, once the permgen is nearly exhausted (which may take a lot of reloads, e.g., 10-15!), the gc is able to free space and no permgen errors occur.",
        "label": 46
    },
    {
        "text": "update the wiki the wiki needs updating. for starters, the url is still jakarta. i think infrastructure needs to be contacted to do this move. if someone is so inclined, it might be useful to go through and cleanup/organize what is there.",
        "label": 15
    },
    {
        "text": "make index output buffer size configurable currently, the buffered index input class allows sub-classes and users thereof to specify a size for the input buffer, which by default is 1024 bytes. in practice, this option is leveraged by the simple file and compound segment index input sub-classes. by the same token, it would be nice if the buffered index output class could open up it's buffer size for users to configure. in particular, this would allow sub-classes thereof to align the output buffer size, which by default is 16348 bytes, to that of the underlying directory's data unit. for example, a network-based directory might want to buffer data in multiples of it's maximum transmission unit. to use an existing use-case, the file system-based directory could potentially choose to align it's output buffer size to the operating system's file block size. the proposed change to the buffered index output class involves defining a one-arg constructor that takes a user-defined buffer size, and a default constructor that uses the currently defined buffer size.",
        "label": 46
    },
    {
        "text": "indexoutput writestring  should write length in bytes we should change the format of strings written to indexes so that the length of the string is in bytes, not java characters. this issue has been discussed at: http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html we must increment the file format number to indicate this change. at least the format number in the segments file should change. i'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features).",
        "label": 33
    },
    {
        "text": "workaround the xerces j bug in benchmark in becnhmark we have a patched version of xerces which is hard to compile from source. when looking at the code part patched and the source of enwikicontentsource, to simply provide the xml parser a reader instead of inputstream, so the broken code is not triggered. this assumes, that the xml-file is always utf-8.... if not it will no longer work (because the xml parser cannot switch encoding, if it only has a reader).",
        "label": 53
    },
    {
        "text": "trivial fixes to weightedspantermextractor the conversion of phrasequery to spannearquery miscalculates the slop if there are stop words in some cases. the issue only really appears if there is more than one intervening run of stop words: ab the cd the the ef. i also noticed that the inorder determination is based on the newly calculated slop, and it should probably be based on the original phrasequery.getslop() patch and unit tests on way",
        "label": 10
    },
    {
        "text": "chardelimitertokenizer whitespacetokenizer is very useful for space separated languages, but my japanese text is not always separated by a space. so, i created an alternative tokenizer that we can specify the delimiter. the file submitted will be an improvement of the current whitespacetokenizer. i tried to extend it from chartokenizer, but chartokenizer has a limitation that a token can't be longer than 255 chars.",
        "label": 38
    },
    {
        "text": "upgrade standardtokenizer   co to latest unicode rules besides any change in data, the rules have also changed (regional indicators, better handling for hebrew, etc)",
        "label": 47
    },
    {
        "text": "instantiatedindexreader does not implement getfieldnames properly causes error in org.apache.lucene.index.segmentmerger.mergefields",
        "label": 24
    },
    {
        "text": "thaianalyzer assumes things about your jre the thaianalyzer/thaiwordfilter depends on the fact that breakiterator.getwordinstance(new locale(\"th\")) returns a dictionary-based break iterator that can segment thai phrases into words (it does not use whitespace). but this is non-standard that the jre will specialize this locale in this way, its nice, but you can't depend on it. for example, if you are running on ibm jre, this analyzer/wordfilter is completely \"broken\" in the sense it won't do what it claims to do. at the minimum, we need to document this and suggest users look at icutokenizer for thai, which always has this breakiterator and is not jre-dependent. better, would be to check statically that the thing actually works. when creating a new thaiwordfilter we could clone() the breakiterator, which is often cheaper than making a new one anyway. we could throw an exception, if its not supported, and add a boolean so the user knows it works. and we could refer to this boolean with assert.assume in its tests.",
        "label": 40
    },
    {
        "text": "'ant stage maven artifacts' should work from the top level project directory  and should provide a better error message when its 'maven dist dir' param points to a non existent directory ",
        "label": 47
    },
    {
        "text": "illegalstateexception in nrtcachingdirectory listall hey, we are getting illegalstateexception in 2 different circumstances. the first one is on status calls: error - 2016-02-01 22:32:43.164; [   ] org.apache.solr.common.solrexception; org.apache.solr.common.solrexception: error handling 'status' action   at org.apache.solr.handler.admin.coreadminhandler.handlestatusaction(coreadminhandler.java:748)  at org.apache.solr.handler.admin.coreadminhandler.handlerequestinternal(coreadminhandler.java:228)  at org.apache.solr.handler.admin.coreadminhandler.handlerequestbody(coreadminhandler.java:193)  at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:143)  at org.apache.solr.servlet.httpsolrcall.handleadminrequest(httpsolrcall.java:660)  at org.apache.solr.servlet.httpsolrcall.call(httpsolrcall.java:431)  at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:227)  at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:196)  at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1652)  at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:585)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:143)  at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:577)  at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:223)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:143)  at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:97)  at org.eclipse.jetty.server.handler.requestloghandler.handle(requestloghandler.java:95)  at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1129)  at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:515)  at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:185)  at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:1061)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:141)  at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:215)  at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:110)  at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:97)  at org.eclipse.jetty.server.server.handle(server.java:497)  at org.eclipse.jetty.server.httpchannel.handle(httpchannel.java:310)  at org.eclipse.jetty.server.httpconnection.onfillable(httpconnection.java:257)  at org.eclipse.jetty.io.abstractconnection$2.run(abstractconnection.java:540)  at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:635)  at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:555)  at java.lang.thread.run(unknown source) caused by: java.lang.illegalstateexception: file: mmapdirectory@d:\\solr\\server\\solr\\prod_core1_shard1_replica2\\data\\index lockfactory=org.apache.lucene.store.nativefslockfactory@65d307e5 appears both in delegate and in cache: cache=[_a0.fdt, _9t_7.liv, _a0.fdx, _a0_lucene50_0.tip, _a0.nvm, _a0_lucene50_0.doc, _a0_lucene50_0.tim, _a0.fnm, _a0_lucene50_0.pos, _a0.si],delegate=[pending_segments_93, segments_92, write.lock, _9t.fdt, _9t.fdx, _9t.fnm, _9t.nvd, _9t.nvm, _9t.si, _9t_6.liv, _9t_lucene50_0.doc, _9t_lucene50_0.pos, _9t_lucene50_0.tim, _9t_lucene50_0.tip, _9u.fdt, _9u.fdx, _9u.fnm, _9u.nvd, _9u.nvm, _9u.si, _9u_lucene50_0.doc, _9u_lucene50_0.pos, _9u_lucene50_0.tim, _9u_lucene50_0.tip, _9v.fdt, _9v.fdx, _9v.fnm, _9v.nvd, _9v.nvm, _9v.si, _9v_lucene50_0.doc, _9v_lucene50_0.pos, _9v_lucene50_0.tim, _9v_lucene50_0.tip, _9w.fdt, _9w.fdx, _9w.fnm, _9w.nvd, _9w.nvm, _9w.si, _9w_lucene50_0.doc, _9w_lucene50_0.pos, _9w_lucene50_0.tim, _9w_lucene50_0.tip, _9x.fdt, _9x.fdx, _9x.fnm, _9x.nvd, _9x.nvm, _9x.si, _9x_lucene50_0.doc, _9x_lucene50_0.pos, _9x_lucene50_0.tim, _9x_lucene50_0.tip, _9y.fdt, _9y.fdx, _9y.fnm, _9y.nvd, _9y.nvm, _9y.si, _9y_lucene50_0.doc, _9y_lucene50_0.pos, _9y_lucene50_0.tim, _9y_lucene50_0.tip, _9z.fdt, _9z.fdx, _9z.fnm, _9z.nvd, _9z.nvm, _9z.si, _9z_lucene50_0.doc, _9z_lucene50_0.pos, _9z_lucene50_0.tim, _9z_lucene50_0.tip, _a0.nvd, _a0_lucene50_0.pos]  at org.apache.lucene.store.nrtcachingdirectory.listall(nrtcachingdirectory.java:103)  at org.apache.solr.core.directoryfactory.sizeofdirectory(directoryfactory.java:208)  at org.apache.solr.handler.admin.coreadminhandler.getindexsize(coreadminhandler.java:1195)  at org.apache.solr.handler.admin.coreadminhandler.getcorestatus(coreadminhandler.java:1173)  at org.apache.solr.handler.admin.coreadminhandler.handlestatusaction(coreadminhandler.java:736)  ... 30 more and the second one is on some kind of replication related request: null:java.lang.illegalstateexception: file: mmapdirectory@d:\\solr\\server\\solr\\prod_core1_shard1_replica3\\data\\index lockfactory=org.apache.lucene.store.nativefslockfactory@65d307e5 appears both in delegate and in cache: cache=[_g3x.si, _g3x.fdx, _g3x.fdt],delegate=[pending_segments_eia, segments_ei9, write.lock, _g3q.fdt, _g3q.fdx, _g3q.fnm, _g3q.nvd, _g3q.nvm, _g3q.si, _g3q_6.liv, _g3q_7.liv, _g3q_lucene50_0.doc, _g3q_lucene50_0.pos, _g3q_lucene50_0.tim, _g3q_lucene50_0.tip, _g3r.fdt, _g3r.fdx, _g3r.fnm, _g3r.nvd, _g3r.nvm, _g3r.si, _g3r_lucene50_0.doc, _g3r_lucene50_0.pos, _g3r_lucene50_0.tim, _g3r_lucene50_0.tip, _g3s.fdt, _g3s.fdx, _g3s.fnm, _g3s.nvd, _g3s.nvm, _g3s.si, _g3s_lucene50_0.doc, _g3s_lucene50_0.pos, _g3s_lucene50_0.tim, _g3s_lucene50_0.tip, _g3t.fdt, _g3t.fdx, _g3t.fnm, _g3t.nvd, _g3t.nvm, _g3t.si, _g3t_lucene50_0.doc, _g3t_lucene50_0.pos, _g3t_lucene50_0.tim, _g3t_lucene50_0.tip, _g3u.fdt, _g3u.fdx, _g3u.fnm, _g3u.nvd, _g3u.nvm, _g3u.si, _g3u_lucene50_0.doc, _g3u_lucene50_0.pos, _g3u_lucene50_0.tim, _g3u_lucene50_0.tip, _g3v.fdt, _g3v.fdx, _g3v.fnm, _g3v.nvd, _g3v.nvm, _g3v.si, _g3v_lucene50_0.doc, _g3v_lucene50_0.pos, _g3v_lucene50_0.tim, _g3v_lucene50_0.tip, _g3w.fdt, _g3w.fdx, _g3w.fnm, _g3w.nvd, _g3w.nvm, _g3w.si, _g3w_lucene50_0.doc, _g3w_lucene50_0.pos, _g3w_lucene50_0.tim, _g3w_lucene50_0.tip, _g3x.fnm, _g3x.nvd, _g3x.nvm, _g3x.si, _g3x_lucene50_0.doc, _g3x_lucene50_0.pos, _g3x_lucene50_0.tim, _g3x_lucene50_0.tip]  at org.apache.lucene.store.nrtcachingdirectory.listall(nrtcachingdirectory.java:103)  at org.apache.solr.core.directoryfactory.sizeofdirectory(directoryfactory.java:208)  at org.apache.solr.handler.replicationhandler.getindexsize(replicationhandler.java:705)  at org.apache.solr.handler.replicationhandler.getstatistics(replicationhandler.java:741)  at org.apache.solr.handler.admin.solrinfombeanhandler.addmbean(solrinfombeanhandler.java:165)  at org.apache.solr.handler.admin.solrinfombeanhandler.getmbeaninfo(solrinfombeanhandler.java:135)  at org.apache.solr.handler.admin.solrinfombeanhandler.handlerequestbody(solrinfombeanhandler.java:66)  at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:143)  at org.apache.solr.core.solrcore.execute(solrcore.java:2064)  at org.apache.solr.servlet.httpsolrcall.execute(httpsolrcall.java:654)  at org.apache.solr.servlet.httpsolrcall.call(httpsolrcall.java:450)  at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:227)  at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:196)  at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1652)  at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:585)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:143)  at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:577)  at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:223)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:143)  at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:97)  at org.eclipse.jetty.server.handler.requestloghandler.handle(requestloghandler.java:95)  at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1129)  at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:515)  at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:185)  at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:1061)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:141)  at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:215)  at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:110)  at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:97)  at org.eclipse.jetty.server.server.handle(server.java:497)  at org.eclipse.jetty.server.httpchannel.handle(httpchannel.java:310)  at org.eclipse.jetty.server.httpconnection.onfillable(httpconnection.java:257)  at org.eclipse.jetty.io.abstractconnection$2.run(abstractconnection.java:540)  at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:635)  at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:555)  at java.lang.thread.run(unknown source) the environment is a 3 node solr cloud setup running solr 5.2.1. when looking at the code: public synchronized string[] listall() throws ioexception {     final set<string> files = new hashset<>();     for(string f : cache.listall()) {       files.add(f);     }     for(string f : in.listall()) {       if (!files.add(f)) {         throw new illegalstateexception(\"file: \" + in + \" appears both in delegate and in cache: \" +                                         \"cache=\" + arrays.tostring(cache.listall()) + \",delegate=\" + arrays.tostring(in.listall()));       }     }     return files.toarray(new string[files.size()]);   } i can see that the exception is thrown because the file exists in both the cache and the file system. this however doesn't make sense to me, because a file that is in cache will always be in the file system as well.",
        "label": 55
    },
    {
        "text": "add deletealldocuments  method to indexwriter ideally, there would be a deletealldocuments() or clear() method on the indexwriter this method should have the same performance and characteristics as: currentwriter.close() currentwriter = new indexwriter(..., create=true,...) this would greatly optimize a delete all documents case. using deletedocuments(new matchalldocsquery()) could be expensive given a large existing index. indexwriter.deletealldocuments() should have the same semantics as a commit(), as far as index visibility goes (new indexreader opening would get the empty index) i see this was previously asked for in lucene-932, however it would be nice to finally see this added such that the indexwriter would not need to be closed to perform the \"clear\" as this seems to be the general recommendation for working with an indexwriter now deletealldocuments() method should: abort any background merges (they are pointless once a deleteall has been received) write new segments file referencing no segments this method would remove one of the final reasons i would ever need to close an indexwriter and reopen a new one",
        "label": 33
    },
    {
        "text": "geo3dshapewgs84modelrectrelationtest test failure it reproduces with ant test -dtestcase=geo3dshapewgs84modelrectrelationtest -dtests.method=testgeopathrect -dtests.seed=1257a8e5fdb03055 -dtests.slow=true -dtests.locale=sq -dtests.timezone=america/monterrey -dtests.asserts=true -dtests.file.encoding=iso-8859-1:    [junit4] suite: org.apache.lucene.spatial.spatial4j.geo3dshapewgs84modelrectrelationtest    [junit4]   2> note: reproduce with: ant test  -dtestcase=geo3dshapewgs84modelrectrelationtest -dtests.method=testgeopathrect -dtests.seed=1257a8e5fdb03055 -dtests.slow=true -dtests.locale=sq -dtests.timezone=america/monterrey -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] error   0.69s | geo3dshapewgs84modelrectrelationtest.testgeopathrect <<<    [junit4]    > throwable #1: java.lang.runtimeexception: couldn't come up with a plane through three points that included the fourth    [junit4]    >  at __randomizedtesting.seedinfo.seed([1257a8e5fdb03055:5b0183e4dd0431e2]:0)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geostandardpath$segmentendpoint.<init>(geostandardpath.java:603)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geostandardpath.done(geostandardpath.java:186)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geostandardpath.<init>(geostandardpath.java:68)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geopathfactory.makegeopath(geopathfactory.java:39)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3dshapefactory$geo3dlinestringbuilder.build(geo3dshapefactory.java:285)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.shaperectrelationtestcase$4.generaterandomshape(shaperectrelationtestcase.java:179)    [junit4]    >  at org.locationtech.spatial4j.shape.rectintersectiontesthelper.testrelatewithrectangle(rectintersectiontesthelper.java:98)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.shaperectrelationtestcase.testgeopathrect(shaperectrelationtestcase.java:199)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    >  at java.base/java.lang.thread.run(thread.java:844)    [junit4]   2> note: test params are: codec=lucene70, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@3595d2c2), locale=sq, timezone=america/monterrey    [junit4]   2> note: linux 4.4.0-104-generic amd64/oracle corporation 9.0.1 (64-bit)/cpus=12,threads=1,free=134432384,total=260046848    [junit4]   2> note: all tests run in this jvm: [geo3dshapewgs84modelrectrelationtest]    [junit4] completed [1/1 (1!)] in 0.86s, 1 test, 1 error <<< failures!",
        "label": 25
    },
    {
        "text": "duplicate package html files in queryparser and analsysis cn packages these files conflict with eachother when building the javadocs. there can be only one (of each) ... hossman@brunner:~/lucene/java$ find src contrib -name package.html | perl -ple 's{.*src/java/}{}' | sort | uniq -c | grep -v \" 1 \"    2 org/apache/lucene/analysis/cn/package.html    2 org/apache/lucene/queryparser/package.html hossman@brunner:~/lucene/java$ find src contrib -path \\*queryparser/package.html src/java/org/apache/lucene/queryparser/package.html contrib/queryparser/src/java/org/apache/lucene/queryparser/package.html hossman@brunner:~/lucene/java$ find src contrib -path \\*cn/package.html contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html",
        "label": 18
    },
    {
        "text": "namedspiloader reload needs to be synchronized spun off of solr-4373: as discsused with uwe on irc, namedspiloader.reload is not thread safe: it reads from this.services at the beginging of hte method, makes additions based on the method input, and then overwrites this.services at the end of the method. if the method is called by two threads concurrently, the entries added by threadb could be lost if threada enters the method before threadb and exists the method after threadb",
        "label": 53
    },
    {
        "text": "analyzer for preventing overload of search service by queries with common terms in large indexes an analyzer used primarily at query time to wrap another analyzer and provide a layer of protection which prevents very common words from being passed into queries. for very large indexes the cost of reading termdocs for a very common word can be high. this analyzer was created after experience with a 38 million doc index which had a term in around 50% of docs and was causing termqueries for this term to take 2 seconds. use the various \"addstopwords\" methods in this class to automate the identification and addition of stop words found in an already existing index.",
        "label": 15
    },
    {
        "text": "build should allow you  especially hudson  to refer to a local javadocs installation instead of downloading currently, we fail on all javadocs warnings. however, you get a warning if it cannot download the package-list from sun.com so i think we should allow you optionally set a sysprop using linkoffline. then we would get much less hudson \"fake failures\" i feel like mike opened an issue for this already but i cannot find it.",
        "label": 47
    },
    {
        "text": "duplicate hits and missing hits in sorted search if using a searcher that subclasses from indexsearcher i get different result sets (besides the ordering of course). the problem only occurrs if the searcher is wrapped by (parallel)multisearcher and the index is not too small. the number of hits returned by un unsorted and a sorted search are identical but the hits are referencing different documents. a closer look at the result sets revealed that the sorted search returns duplicate hits. i created test cases for lucene 1.4.3 as well as for the head release. the problem showed up for both, the number of duplicates beeing bigger for the head realease. the test cases are written for package org.apache.lucene.search. there are messages describing the problem written to the console. in order to see all those hints the asserts are commented out. so dont't be confused if junit reports no errors. (sorry, beeing a novice user of the bug tracker i don't see any means to attach the test cases on this screen. let's see.)",
        "label": 55
    },
    {
        "text": "remove version from analyzer constructors this has always been a mess: analyzers are easy enough to make on your own, we don't need to \"take responsibility\" for the users analysis chain for 2 major releases. the code maintenance is horrible here. this creates a huge usability issue too, and as seen from numerous mailing list issues, users don't even understand how this versioning works anyway. i'm sure someone will whine if i try to remove these constants, but we can at least make no-arg ctors forwarding to version_current so that people who don't care about back compat (e.g. just prototyping) don't have to deal with the horribly complex versioning system. if you want to make the argument that doing this is \"trappy\" (i heard this before), i think thats bogus, and ill counter by trying to remove them. either way, i'm personally not going to add any of this kind of back compat logic myself ever again. updated: description of the issue updated as expected. we should remove this api completely. no one else on the planet has apis that require a mandatory version parameter.",
        "label": 41
    },
    {
        "text": "allow easy extensions of topdoccollector topdoccollector's members and constructor are declared either private or package visible. it makes it hard to extend it as if you want to extend it you can reuse its hq and totatlhits members, but need to define your own. it also forces you to override gettotalhits() and topdocs(). by changing its members and constructor (the one that accepts a pq) to protected, we allow users to extend it in order to get a different view of 'top docs' (like topfieldcollector does), but still enjoy its gettotalhits() and topdocs() method implementations.",
        "label": 12
    },
    {
        "text": "reduce exposure of nightly build documentation from lucene-1157 - ..the nightly build documentation is too prominent. a search for \"indexwriter api\" on google or yahoo! returns nightly documentation before released documentation. (https://issues.apache.org/jira/browse/lucene-1157?focusedcommentid=12565820#action_12565820)",
        "label": 53
    },
    {
        "text": "lucene's nightly hudson builds don't have svn version in manifest mf solr had the same issue but apparently made a configuration change to the hudson configuration to get it working: https://issues.apache.org/jira/browse/solr-684 also i opened this infra issue: https://issues.apache.org/jira/browse/infra-1721 which says the svnversion exe is located in this path: /opt/subversion-current/bin in that inra issue, /etc/init.d/tomcat was also fixed in theory so that svnversion would be on the path the next time hudson is restarted. still, in case that doesn't work, or it changes in the future, it seems a good idea to make the same change that solr made to lucene's hudson configuration. hoss can you detail what you needed to do for solr? or maybe just do it also for lucene thanks!",
        "label": 18
    },
    {
        "text": "trie range   make trie range indexing more flexible in the current trie range implementation, a single precision step is specified. with a large precision step (say 8), a value is indexed in fewer terms (8) but the number of terms for a range can be large. with a small precision step (say 2), the number of terms for a range is smaller but a value is indexed in more terms (32). we want to add an option that different precision steps can be set for different precisions. an expert can use this option to keep the number of terms for a range small and at the same time index a value in a small number of terms. see the discussion in lucene-1470 that results in this issue.",
        "label": 53
    },
    {
        "text": "things to be done now that filter is independent from bitset (aside: where is the documentation on how to mark up text in jira comments?) the following things are left over after lucene-584 : for lucene 3.0 filter.bits() will have to be removed. there is a checkme in indexsearcher about using conjunctionscorer to have the boolean behaviour of a filter. i have not looked into filter caching yet, but i suppose there will be some room for improvement there. iirc the current core has moved to use openbitsetfilter and that is probably what is being cached. in some cases it might be better to cache a sortedvintlist instead. boolean logic on docidsetiterator is already available for scorers (that inherit from docidsetiterator) in the search package. this is currently implemented by conjunctionscorer, disjunctionsumscorer, reqoptsumscorer and reqexclscorer. boolean logic on bitsets is available in contrib/misc and contrib/queries disjunctionsumscorer calls score() on its subscorers before the score value actually needed. this could be a reason to introduce a disjunctiondocidsetiterator, perhaps as a superclass of disjunctionsumscorer. to fully implement non scoring queries a termdocidsetiterator will be needed, perhaps as a superclass of termscorer. the javadocs in org.apache.lucene.search using matching vs non-zero score: i'll investigate this soon, and provide a patch when necessary. an early version of the patches of lucene-584 contained a class matcher, that differs from the current docidset in that matcher has an explain() method. it remains to be seen whether such a matcher could be useful between docidset and scorer. the semantics of scorer.skipto(scorer.doc()) was discussed briefly. this was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this. skipping on a sortedvintlist is done using linear search, this could be improved by adding multilevel skiplist info much like in the lucene index for documents containing a term. one comment by me of 3 dec 2008: a few complete (test) classes are deprecated, it might be good to add the target release for removal there.",
        "label": 32
    },
    {
        "text": "queries with zero boosts don't work queries consisting of only zero boosts result in incorrect results.",
        "label": 55
    },
    {
        "text": "indexwriter deadlock when using concurrentmergescheduler i'm trying to update our application from compass 2.0.0m1 with lucene 2.2 to compass 2.0.0m3 (latest build) with lucene 2.3.1. i'm holding all other things constant and only changing the compass and lucene jars. i'm recreating the search index for our data and seeing deadlock in lucene's indexwriter. it appears to be waiting on a signal from the merge thread. i've tried creating a simple reproduction case for this but to no avail. doing the exact same steps with compass 2.0.0m1 and lucene 2.2 has no problems and recreates our search index. that is to say, it's not our code. in particular, the main thread performing the commit (lucene document save) from compass is calling lucene's indexwriter.optimize(). we're using compass's executormergescheduler to handle the merging, and it is calling indexwriter.merge(). the main thread in indexwriter.optimize() enters the wait() at the bottom of that method and is never notified. i can't tell if this is because optimizemergespending() is returning true incorrectly, or if indexwriter.merge()'s notifyall() is being called prematurely. looking at the code, it doesn't seem possible for indexwriter.optimize() to be waiting and miss a notifyall(), and lucene's indexwriter.merge() was recently fixed to always call notifyall() even on exceptions \u2013 that is all the relevant indexwriter code looks properly synchronized. nevertheless, i'm seeing the deadlock behavior described, and it's reproducible using our app and our test data set. could someone familiar with indexwriter's synchronization code take another look at it? i'm sorry that i can't give you a simple reproduction test case.",
        "label": 33
    },
    {
        "text": "queryparser getfieldquery string string  doesn't set default slop on multiphrasequery there seems to have been an oversight in calling mph.setslop(phraseslop) in queryparser.getfieldquery(string,string). the result being that in some cases, the \"default slop\" value doesnt' get set right (sometimes, ... see below). when i tried amending testmultianalyzer to demonstrate the problem, i discovered that the grammer aparently always calls getfieldquery(string,string,int) \u2013 even if no \"~slop\" was specified in the text being parsed, in which case it passes the default as if it were specified. (just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test) the problem isn't entirely obvious unless you have a subclasses of queryparser and try to call getfieldquery(string,string) directly. in my case, i had overridden getfieldquery(string,string) to call super.getfieldquery(string,string) and wrap the result in a disjunctionmaxquery ... i don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way queryparser does, because getfieldquery(string,string,int) wasn't getting back something it could call setslop() with the (default) value it got from the javacc generated code. my description may not make much sense, but hopefull the test patch i'm about to attach will. the fix is also in the patch, and is fairly trivial. (disclaimer: i don't have javacc installed, so i tested this patch by manually making the change to both queryparser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)",
        "label": 55
    },
    {
        "text": "testutil randomwhitespace  broken for java  unicode  java 9 changed its character tables to unicode version 7.0. the table updates are listed here: http://hg.openjdk.java.net/jdk9/jdk9/jdk/rev/86206517258d because of this, character \\u180e is no longer treated as whitespace, so testutil#randomwhitespace() fails. i will remove this character from the list and update the documentation jre_version_migration.txt. we should maybe make this character list dynamic (e.g. testutil initializes it on static class init one time by iterating over all 16 bit characters). maybe somebody else has an idea (there is already a todo in the code about that).",
        "label": 53
    },
    {
        "text": "adding filtereddocidset and filtereddocidsetiterator adding 2 convenience classes: filtereddocidset and filtereddocidsetiterator.",
        "label": 33
    },
    {
        "text": "similarity lengthnorm and positionincrement calculation of lengthnorm factor should in some cases take into account the number of tokens with positionincrement=0. this should be made optional, to support two different scenarios: when analyzers insert artificially constructed tokens into tokenstream (e.g. ascii-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthnorm calculation should ignore the tokens with positionincrement=0. when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthnorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionincrement. the default should be backward-compatible, i.e. it should count all tokens. (see also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )",
        "label": 33
    },
    {
        "text": "standardtokenizer disposes of hiragana combining mark dakuten instead of attaching it to the character it belongs to lucene 3.3 (possibly 3.1 onwards) exhibits less than great behaviour for tokenising hiragana, if combining marks are in use. here's a unit test:     @test     public void testhiraganawithcombiningmarkdakuten() throws exception     {         // hiragana 's' following by the combining mark dakuten         tokenstream stream = new standardtokenizer(version.lucene_33, new stringreader(\"\\u3055\\u3099\"));         // should be kept together.         list<string> expectedtokens = arrays.aslist(\"\\u3055\\u3099\");         list<string> actualtokens = new linkedlist<string>();         chartermattribute term = stream.addattribute(chartermattribute.class);         while (stream.incrementtoken())         {             actualtokens.add(term.tostring());         }         assertequals(\"wrong tokens\", expectedtokens, actualtokens);     } this code fails with: java.lang.assertionerror: wrong tokens expected:<[\u3056]> but was:<[\u3055]> it seems as if the tokeniser is throwing away the combining mark entirely. 3.0's behaviour was also undesirable: java.lang.assertionerror: wrong tokens expected:<[\u3056]> but was:<[\u3055, \u3099]> but at least the token was there, so it was possible to write a filter to work around the issue. katakana seems to be avoiding this particular problem, because all katakana and combining marks found in a single run seem to be lumped into a single token (this is a problem in its own right, but i'm not sure if it's really a bug.)",
        "label": 40
    },
    {
        "text": "document maven nightly builds  artifact generation  and using maven to build lucene solr there should be documentation we can point people to when they ask how to use maven with lucene and solr.",
        "label": 47
    },
    {
        "text": "support for new resources model in ant in lucene ant task  ant task for lucene should use modern resource model (not only fileset child element). there is a patch with required changes. supported by old (ant 1.6) and new (ant 1.7) resources model: <index ....> <!-- lucene ant task --> <fileset ... /> </index> supported only by new (ant 1.7) resources model: <index ....> <!-- lucene ant task --> <filelist ... /> </index> <index ....> <!-- lucene ant task --> <userdefinied-filesource ... /> </index>",
        "label": 14
    },
    {
        "text": "indexwriter optimize boolean dowait  ignores dowait parameter indexwriter.optimize(boolean dowait) ignores the dowait parameter and always calls optimize(1, true). that does not seem to be the intended behavior, based on the doc comment.",
        "label": 33
    },
    {
        "text": "geo3dshaperectrelationtest reproducible failure this fails on trunk r1678446 for me: ant test -dtestcase=geo3dshaperectrelationtest -dtests.method=testgeocirclerect -dtests.seed=8032886d7b96249d:c92ad49bdf6566d7 strangely, the test framework fails to print the \"reproduce with\" line: [junit4:pickseed] seed property 'tests.seed' already defined: 8032886d7b96249d:c92ad49bdf6566d7    [junit4] <junit4> says hello! master seed: 8032886d7b96249d:c92ad49bdf6566d7    [junit4] executing 1 suite with 1 jvm.    [junit4]     [junit4] started j0 pid(10530@localhost).    [junit4] suite: org.apache.lucene.spatial.spatial4j.geo3dshaperectrelationtest    [junit4]   1> s-r rel: {}, shape {}, rectangle {} [contains, geo3dshape{geocircle: {center=[x=0.33939366238712926, y=-0.9406444290609368, z=2.8417543640235586e-6], radius=1.5009831567151235(86.00000000000001)}}, rect(minx=-118.0,maxx=-114.0,miny=-2.0,maxy=32.0)](no slf4j subst; sorry)    [junit4] failure 0.37s | geo3dshaperectrelationtest.testgeocirclerect <<<    [junit4]    > throwable #1: java.lang.assertionerror: if not disjoint then the shape's bbox shouldn't be disjoint    [junit4]    >  at __randomizedtesting.seedinfo.seed([8032886d7b96249d:c92ad49bdf6566d7]:0)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.rectintersectiontesthelper.testrelatewithrectangle(rectintersectiontesthelper.java:117)    [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3dshaperectrelationtest.testgeocirclerect(geo3dshaperectrelationtest.java:132)    [junit4] completed [1/1] in 0.41s, 1 test, 1 failure <<< failures!    [junit4]     [junit4]     [junit4] tests with failures:    [junit4]   - org.apache.lucene.spatial.spatial4j.geo3dshaperectrelationtest.testgeocirclerect",
        "label": 10
    },
    {
        "text": "hyper parameter c is ignored in term frequency normalizationh1 unlike normalizationh2, c parameter is not used in term frequency calculation in normalizationh1.",
        "label": 40
    },
    {
        "text": "change contrib tests to use the special lucenetestcase j4  constant for the current version used a matchversion parameter sub issue for contrib changes",
        "label": 53
    },
    {
        "text": "make changes to html target an offline operation currently changes-to-html pulls release dates from jira, and so fails when jira is inaccessible (e.g. from behind a firewall). solr-9711 advocates adding a build sysprop to ignore jira connection failures, but i'd rather make the operation always offline. in an offline discussion, hoss man advocated moving lucene's and solr's doap.rdf files, which contain all of the release dates that the changes-to-html now pulls from jira, from the cms subversion repository (downloadable from the website at http://lucene.apache.org/core/doap.rdf and http://lucene.apache.org/solr/doap.rdf) to the lucene/solr git repository. if we did that, then the process could be entirely offline if release dates were taken from the local doap.rdf files instead of downloaded from jira.",
        "label": 47
    },
    {
        "text": "edgengrams creates invalid offsets a user reported this because it was causing his highlighting to throw an error.",
        "label": 40
    },
    {
        "text": "missing possibility to supply custom fieldparser when sorting search results when implementing the new trierangequery for contrib (lucene-1470), i was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by searcher.search() and sortfield. the problem is: if you use sortfield.long, you get numberformatexceptions. the trie encoded values may be sorted using sortfield.string (as the encoding is in such a way, that they are sortable as strings), but this is very memory ineffective. extendedfieldcache gives the possibility to specify a custom longparser when retrieving the cached values. but you cannot use this during searching, because there is no possibility to supply this custom longparser to the sortfield. i propose a change in the sort classes: include a pointer to the parser instance to be used in sortfield (if not given use the default). my idea is to create a sortfield using a new constructor sortfield(string field, int type, object parser, boolean reverse) the parser is \"object\" because all current parsers have no super-interface. the ideal solution would be to have: sortfield(string field, int type, fieldcache.parser parser, boolean reverse) and fieldcache.parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like longparser...). the sort implementation then must be changed to respect the given parser (if not null), else use the default fieldcache.getxxxx without parser.",
        "label": 33
    },
    {
        "text": "persian analyzer a simple persian analyzer. i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/dbrg/hamshahri/ : simpleanalyzer: summary search seconds: 0.012 docname seconds: 0.020 num points: 981.015 num good points: 33.738 max good points: 36.185 average precision: 0.374 mrr: 0.667 recall: 0.905 precision at 1: 0.585 precision at 2: 0.531 precision at 3: 0.513 precision at 4: 0.496 precision at 5: 0.486 precision at 6: 0.487 precision at 7: 0.479 precision at 8: 0.465 precision at 9: 0.458 precision at 10: 0.460 precision at 11: 0.453 precision at 12: 0.453 precision at 13: 0.445 precision at 14: 0.438 precision at 15: 0.438 precision at 16: 0.438 precision at 17: 0.429 precision at 18: 0.429 precision at 19: 0.419 precision at 20: 0.415 persiananalyzer: summary search seconds: 0.004 docname seconds: 0.011 num points: 987.692 num good points: 36.123 max good points: 36.185 average precision: 0.481 mrr: 0.833 recall: 0.998 precision at 1: 0.754 precision at 2: 0.715 precision at 3: 0.646 precision at 4: 0.646 precision at 5: 0.631 precision at 6: 0.621 precision at 7: 0.593 precision at 8: 0.577 precision at 9: 0.573 precision at 10: 0.566 precision at 11: 0.572 precision at 12: 0.562 precision at 13: 0.554 precision at 14: 0.549 precision at 15: 0.542 precision at 16: 0.538 precision at 17: 0.533 precision at 18: 0.527 precision at 19: 0.525 precision at 20: 0.518",
        "label": 40
    },
    {
        "text": "allow junit4 tests in our environment  now that we're dropping java 1.4 compatibility for 3.0, we can incorporate junit4 in testing. junit3 and junit4 tests can coexist, so no tests should have to be rewritten. we should start this for the 3.1 release so we can get a clean 3.0 out smoothly. it's probably worthwhile to convert a small set of tests as an exemplar.",
        "label": 33
    },
    {
        "text": "add a set iterator to sentinelintset i'm working on code that needs a hash based int set. it will need to iterate over the values, but sentinalintset doesn't have this utility feature. it should be pretty easy to add. fyi this is an out-growth of a question i posed to the dev list, examining 3 different int hash sets out there: sentinalintset, inthashset (in lucene facet module) and the 3rd party intopenhashset (hppc) \u2013 see http://lucene.472066.n3.nabble.com/inthashset-sentinelintset-sortedintdocset-td4037516.html i decided to go for sentinalintset because it's already in lucene-core, adding the method i need should be easy, and it has a nice lean implementation.",
        "label": 10
    },
    {
        "text": " patch  new method expungedeleted  added to indexwriter we make use the docids in lucene. i need a way to compact the docids in segments to remove the \"holes\" created from doing deletes. the only way to do this is by calling indexwriter.optimize(). this is a very heavy call, for the cases where the index is large but with very small number of deleted docs, calling optimize is not practical. i need a new method: expungedeleted(), which finds all the segments that have delete documents and merge only those segments. i have implemented this method and have discussed with otis about submitting a patch. i don't see where i can attached the patch. i will do according to the patch guidleine and email the lucene mailing list. thanks -john i don't see a place where i can",
        "label": 33
    },
    {
        "text": "iw addindexes doesn't prune all deleted segments at the least, this can easily create segments with maxdoc == 0. it seems buggy: elsewhere we prune these segments out, so its expected to have a commit point with no segments rather than a segment with 0 documents...",
        "label": 43
    },
    {
        "text": "highlight fragment does not extend to maxdoccharstoanalyze the current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxdoccharstoanalyze before adding any text remaining after the last token to the fragment. this means that if maxdoccharstoanalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted. for example, consider the phrase \"this is a text with searchterm in it\". \"in\" and \"it\" are not tokenized because they're stopwords. setting maxdoccharstoanalyze to 36 (the length of the sentence) and searching for \"searchterm\" gives a fragment ending in \"searchterm\". the expected behaviour is to have \"in it\" at the end of the fragment, since maxdoccharstoanalyse explicitely states that the whole phrase should be considered.",
        "label": 29
    },
    {
        "text": "iw commit  writes but fails to fsync the n fnx file in making a unit test for nrtcachingdir (lucene-3092) i hit this surprising bug! because the new n.fnx file is written at the \"last minute\" along with the segments file, it's not included in the sis.files() that iw uses to figure out which files to sync. this bug means one could call iw.commit(), successfully, return, and then the machine could crash and when it comes back up your index could be corrupted. we should hopefully first fix testcrash so that it hits this bug (maybe it needs more/better randomization?), then fix the bug....",
        "label": 46
    },
    {
        "text": "allow scorer to expose positions and payloads aka  nuke spans currently we have two somewhat separate types of queries, the one which can make use of positions (mainly spans) and payloads (spans). yet span*query doesn't really do scoring comparable to what other queries do and at the end of the day they are duplicating lot of code all over lucene. span*queries are also limited to other span*query instances such that you can not use a termquery or a booleanquery with spannear or anthing like that. beside of the span*query limitation other queries lacking a quiet interesting feature since they can not score based on term proximity since scores doesn't expose any positional information. all those problems bugged me for a while now so i stared working on that using the bulkpostings api. i would have done that first cut on trunk but termscorer is working on blockreader that do not expose positions while the one in this branch does. i started adding a new positions class which users can pull from a scorer, to prevent unnecessary positions enums i added scorercontext#needspositions and eventually scorere#needspayloads to create the corresponding enum on demand. yet, currently only termquery / termscorer implements this api and other simply return null instead. to show that the api really works and our bulkpostings work fine too with positions i cut over termspanquery to use a termscorer under the hood and nuked termspans entirely. a nice sideeffect of this was that the position bulkreading implementation got some exercise which now work all with positions while payloads for bulkreading are kind of experimental in the patch and those only work with standard codec. so all spans now work on top of termscorer ( i truly hate spans since today ) including the ones that need payloads (standardcodec only)!! i didn't bother to implement the other codecs yet since i want to get feedback on the api and on this first cut before i go one with it. i will upload the corresponding patch in a minute. i also had to cut over spanquery.getspans(ir) to spanquery.getspans(atomicreadercontext) which i should probably do on trunk first but after that pain today i need a break first . the patch passes all core tests (org.apache.lucene.search.highlight.highlightertest still fails but i didn't look into the memoryindex bulkpostings api yet)",
        "label": 40
    },
    {
        "text": "make analyzerwrapper get offset positionincrement gap non final it can sometimes be useful to reconfigure the position and offset gaps of an existing analyzer.",
        "label": 1
    },
    {
        "text": "token implementation needs improvements this was discussed in the thread (not sure which place is best to reference so here are two): http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3c21f67cc2-ebb4-48a0-894e-fba4aecc0d50@gmail.com%3e or to see it all at once: http://www.gossamer-threads.com/lists/lucene/java-dev/62851 issues: 1. javadoc is insufficient, leading one to read the code to figure out how to use the class. 2. deprecations are incomplete. the constructors that take string as an argument and the methods that take and/or return string should all be deprecated. 3. the allocation policy is too aggressive. with large tokens the resulting buffer can be over-allocated. a less aggressive algorithm would be better. in the thread, the python example is good as it is computationally simple. 4. the parts of the code that currently use token's deprecated methods can be upgraded now rather than waiting for 3.0. as it stands, filter chains that alternate between char[] and string are sub-optimal. currently, it is used in core by query classes. the rest are in contrib, mostly in analyzers. 5. some internal optimizations can be done with regard to char[] allocation. 6. tokenstream has next() and next(token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(token) 7. tokens are often stored as a string in a term. it would be good to add constructors that took a token. this would simplify the use of the two together.",
        "label": 33
    },
    {
        "text": "queryscorer and spanregexquery are incompatible  since the resolution of #lucene-1685, users are not supposed to rewrite their queries before submitting them to queryscorer: ------------------------------------------------------------------------ r800796 | markrmiller | 2009-08-04 06:56:11 -0700 (tue, 04 aug 2009) | 1 line lucene-1685: the position aware spanscorer has become the default scorer for highlighting. the spanscorer implementation has replaced queryscorer and the old term highlighting queryscorer has been renamed to querytermscorer. multi-term queries are also now expanded by default. if you were previously rewritting the query for multi-term query highlighting, you should no longer do that (unless you switch to using querytermscorer). the spanscorer api (now queryscorer) has also been improved to more closely match the api of the previous queryscorer implementation. ------------------------------------------------------------------------ this is a great convenience for the most part, but it's causing me difficulties with spanregexquerys, as the weightedspantermextractor uses query.extractterms() to collect the fields used in the query, but spanregexquery does not implement this method, so highlighting any query with a spanregexquery throws an unsupportedopertationexception. if this issue is circumvented, there is still the issue of spanregexquery throwing an exception when someone calls its getspans() method. i can provide the patch that i am currently using, but i'm not sure that my solution is optimal. it adds two methods to spanquery: extractfields(set<string> fields) which is equivalent to fields.add(getfield()) except when maskedfieldquerys get involved, and mustberewrittentogetspans() which returns true for spanquery, false for spantermquery, and is overridden in each composite spanquery to return a value depending on its components. in this way spanregexquery (and any other custom spanquerys) do not need to be adjusted. currently the collection of fields and non-weighted terms are done in a single step. in the proposed patch the weightedspanterm extraction from a spanquery proceeds in two steps. first, if the queryscorer's field is null, then the fields are collected from the spanquery using the extractfields() method. second the terms are collected using extractterms(), rewriting the query for each field if mustberewrittentogetspans() returns true.",
        "label": 29
    },
    {
        "text": "some equals methods do not check for null argument the equals methods in the following classes do not check for a null argument and thus would incorrectly fail with a null pointer exception if passed null: org.apache.lucene.index.segmentinfo org.apache.lucene.search.function.customscorequery org.apache.lucene.search.function.ordfieldsource org.apache.lucene.search.function.reverseordfieldsource org.apache.lucene.search.function.valuesourcequery if a null parameter is passed to equals() then false should be returned.",
        "label": 43
    },
    {
        "text": "extract readerpool from indexwriter readerpool plays a central role in the indexwriter pooling nrt readers and making sure we write buffered deletes and updates to disk. this class used to be a non-static inner class accessing many aspects including locks from the indexwriter itself. this change moves the class outside of iw and defines it's responsiblity in a clear way with respect to locks etc. now indexwriter doesn't need to share readerpool anymore and reacts on writes done inside the pool by checkpointing internally. this also removes acquiring the iw lock inside the reader pool which makes reasoning about concurrency difficult. this change also add javadocs and dedicated tests for the readerpool class. /cc michael mccandless dawid weiss",
        "label": 46
    },
    {
        "text": "queryparserutil escape  does not escape forward slash queryparserutil.escape() and queryparser.escape() have different implementations. most important, the former omit escaping forward slash (\"/\"). this again caused errors in the queryparser when a query ended with forward slash.",
        "label": 47
    },
    {
        "text": "remove facet42docvaluesformat the new directdocvaluesformat is nearly identical to facet42dvf, except that it stores the addresses in direct int[] rather than packedints. on lucene-5296 we measured the performance of directdvf vs facet42dvf and it improves perf for some queries and have negligible effect for others, as well as ram consumption isn't much worse. we should remove facet42dvf and use directdvf instead. i also want to rename facet46codec to facetcodec. there's no need to refactor the class whenever the default codec changes (e.g. from 45 to 46) since it doesn't care about the actual codec version underneath, it only overrides the dvf used for the facet fields. facetcodec should take the dvf from the app (so e.g. the facet/ module doesn't depend on codecs/) and be exposed more as a utility codec rather than a real, versioned, codec.",
        "label": 43
    },
    {
        "text": "add common terms query to gracefully handle very high frequent terms dynamically i had this problem quite a couple of times the last couple of month that searches very often contained super high frequent terms and disjunction queries became way too slow. the main problem was that stopword filtering wasn't really an option since in the domain those high-freq terms where not really stopwords though. so for instance searching for a song title \"this is it\" or for a band \"a\" didn't really fly with stopwords. i thought about that for a while and came up with a query based solution that decides based on a threshold if something is considered a stopword or not and if so it moves the term in two boolean queries one for high-frequent and one for low-frequent such that those high frequent terms are only matched if the low-frequent sub-query produces a match. yet if all terms are high frequent it makes the entire thing a conjunction which gave me reasonable results as well as performance.",
        "label": 46
    },
    {
        "text": "allow to pass an instance of ratelimiter to fsdirectory allowing to rate limit merge io across several directories   instances this can come in handy when running several lucene indices in the same vm, and wishing to rate limit merge across all of them.",
        "label": 46
    },
    {
        "text": "improve pointvalues visitor calls when all docs in a leaf share a value when all the docs in a leaf node have the same value, range queries can waste a lot of processing if the node itself returns cell_crosses_query when compare() is called, in effect performing the same calculation in visit(int, byte[]) over and over again. in the case i'm looking at (very low cardinality indexed longrange fields), this causes something of a perfect storm for performance. pointvalues can detect up front if a given node has a single value (because it's min value and max value will be equal), so this case should be fairly simple to identify and shortcut.",
        "label": 36
    },
    {
        "text": "classic query parser  autogeneratephrasequeries true doesn't work when splitonwhitespace false lucene-2605 introduced the classic query parser option to not split on whitespace prior to performing analysis. from the javadocs for queryparser.setautogeneratephrasequeries(): phrase queries will be automatically generated when the analyzer returns more than one term from whitespace delimited text. when splitonwhitespace=false, the output from analysis can now come from multiple whitespace-separated tokens, which breaks code assumptions when autogeneratephrasequeries=true: for this combination of options, it's not appropriate to auto-quote multiple non-overlapping tokens produced by analysis. e.g. simple whitespace tokenization over the query \"some words\" will produce the token sequence (\"some\", \"words\"), and even when autogeneratephrasequeries=true, we should not be creating a phrase query here.",
        "label": 47
    },
    {
        "text": "add geoencodingutils to core this is part 1 for lucene-7165. this task will add a geoencodingutils helper class to o.a.l.geo in the core module for reusing lat/lon encoding methods. existing encoding methods in latlonpoint will be refactored to the new helper class so a new numerically stable morton encoding can be added that reuses the same encoding methods.",
        "label": 36
    },
    {
        "text": "priorityqueue is inheriently broken if subclass attempts to use  heap  w generic t bound to anything other then  object  as discovered in solr-2410 the fact that the protected \"heap\" variable in priorityqueue is initialized using an object[] makes it impossible for subclasses of priorityqueue to exist and access the \"heap\" array unless they bind the generic to object.",
        "label": 53
    },
    {
        "text": "offsets issues with multiword synonyms as reported on the list, there are some strange offsets with fstsynonyms, in the case of multiword synonyms. as a workaround it was suggested to use the older synonym impl, but it has bugs too (just in a different way).",
        "label": 33
    },
    {
        "text": "move wordnet based synonym code out of contrib memory and into contrib wordnet  or somewhere else  see lucene-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out.",
        "label": 40
    },
    {
        "text": "reduce usage of string intern  performance is terrible i profiled a simple matchalldocsquery() against ~1.5 million documents (8 fields of short text, field.store.yes,field.index.not_analyzed_no_norms), then retrieved all documents via searcher.doc(i, fs). string.intern() showed up as a top hotspot (see attached screenshot), so i implemented a small optimization to not intern() for every new field(), instead forcing the intern in the fieldinfos class and adding a optional \"internname\" constructor to field. this reduced execution time for searching and iterating through all documents by 35%. results were similar for -server and -client. trunk (2.9) w/out patch: matched 1435563 in 8884 ms/search trunk (2.9) w/patch: matched 1435563 in 5786 ms/search",
        "label": 33
    },
    {
        "text": "analyzers common tests fail with jdk9 ea build looks like this:    [junit4] suite: org.apache.lucene.analysis.fr.testfrenchlightstemfilter    [junit4]   2> note: reproduce with: ant test  -dtestcase=testfrenchlightstemfilter -dtests.method=testvocabulary -dtests.seed=4044297f9bfa5e32 -dtests.locale=az-cyrl-az -dtests.timezone=act -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 0.44s j0 | testfrenchlightstemfilter.testvocabulary <<<    [junit4]    > throwable #1: org.junit.comparisonfailure: term 0 expected:<grosi[]> but was:<grosi[er]> so far i see these failing with french and portuguese. it may be a hotspot issue, as these tests stem more than 10,000 words.",
        "label": 53
    },
    {
        "text": "make prefixlength accessible to prefixtermenum subclasses prefixtermenum#difference() offers a way to influence scoring based on the difference between the prefix term and a term in the enumeration. to effectively use this facility the length of the prefix should be accessible to subclasses. currently the prefix term is private to prefixtermenum. i added a getter for the prefix length and made prefixtermenum#endenum(), prefixtermenum#termcompare() final for consistency with other termenum subclasses. patch is attached. simon",
        "label": 33
    },
    {
        "text": "scandinavianfoldingfilterfactory and scandinaviannormalizationfilterfactory this filter is an augmentation of output from asciifoldingfilter, it discriminate against double vowels aa, ae, ao, oe and oo, leaving just the first one. bl\u00e5b\u00e6rsyltet\u00f8j == bl\u00e5b\u00e4rsyltet\u00f6j == blaabaarsyltetoej == blabarsyltetoj r\u00e4ksm\u00f6rg\u00e5s == r\u00e6ksm\u00f8rg\u00e5s == r\u00e6ksm\u00f6rgaos == raeksmoergaas == raksmorgas caveats: since this is a filtering on top of asciifoldingfilter \u00e4\u00f6\u00e5\u00f8\u00e6 already has been folded down to aoaoae when handled by this filter it will cause effects such as: b\u00f8en -> boen -> bon \u00e5ene -> aene -> ane i find this to be a trivial problem compared to not finding anything at all. background: swedish \u00e5\u00e4\u00f6 is in fact the same letters as norwegian and danish \u00e5\u00e6\u00f8 and thus interchangeable in when used between these languages. they are however folded differently when people type them on a keyboard lacking these characters and asciifoldingfilter handle \u00e4 and \u00e6 differently. when a swedish person is lacking umlauted characters on the keyboard they consistently type a, a, o instead of \u00e5, \u00e4, \u00f6. foreigners also tend to use a, a, o. in norway people tend to type aa, ae and oe instead of \u00e5, \u00e6 and \u00f8. some use a, a, o. i've also seen oo, ao, etc. and permutations. not sure about denmark but the pattern is probably the same. this filter solves that problem, but might also cause new.",
        "label": 21
    },
    {
        "text": "spanscorer fails when sloppyfreq  returns i think we should fix this for 2.4 (now back to 10)?",
        "label": 33
    },
    {
        "text": "memoryindex does not support points i realized this glancing at lucene-7091. i think this should have points support or else people cannot move off of the deprecated legacyxxx encodings?",
        "label": 30
    },
    {
        "text": "offset gap should be added regardless of existence of tokens in docinverterperfield problem: if a multivalued field which contains a stop word (e.g. \"will\" in the following sample) only value is analyzed by stopanalyzer when indexing, the offsets of the subsequent tokens are not correct. indexing a multivalued field doc.add( new field( f, \"mike\", store.yes, index.analyzed, termvector.with_offsets ) ); doc.add( new field( f, \"will\", store.yes, index.analyzed, termvector.with_offsets ) ); doc.add( new field( f, \"use\", store.yes, index.analyzed, termvector.with_offsets ) ); doc.add( new field( f, \"lucene\", store.yes, index.analyzed, termvector.with_offsets ) ); in this program (soon to be attached), if you use whitespaceanalyzer, you'll get the offset(start,end) for \"use\" and \"lucene\" will be use(10,13) and lucene(14,20). but if you use stopanalyzer, the offsets will be use(9,12) and lucene(13,19). when searching, since searcher cannot know what analyzer was used at indexing time, this problem causes out of alignment of fvh. cause of the problem: stopanalyzer filters out \"will\", anytoken flag set to false then offset gap is not added in docinverterperfield: docinverterperfield.java if (anytoken)   fieldstate.offset += docstate.analyzer.getoffsetgap(field); i don't understand why the condition is there... if always the gap is added, i think things are simple.",
        "label": 26
    },
    {
        "text": "contrib intelligent analyzer for chinese i wrote a analyzer for apache lucene for analyzing sentences in chinese language. it's called \"imdict-chinese-analyzer\", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/ in chinese, \"\u6211\u662f\u4e2d\u56fd\u4eba\"(i am chinese), should be tokenized as \"\u6211\"(i) \"\u662f\"(am) \"\u4e2d\u56fd\u4eba\"(chinese), not \"\u6211\" \"\u662f\u4e2d\" \"\u56fd\u4eba\". so the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by lucene, and the accuracy of the search engine will be affected seriously! although there are two analyzer packages in apache repository which can handle chinese: chineseanalyzer and cjkanalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly. the algorithm of imdict-chinese-analyzer is based on hidden markov model (hmm), so it can tokenize chinese sentence in a really intelligent way. tokenizaion accuracy of this model is above 90% according to the paper \"hhmm-based chinese lexical analyzer ictclal\" while other analyzer's is about 60%. as imdict-chinese-analyzer is a really fast and intelligent. i want to contribute it to the apache lucene repository.",
        "label": 33
    },
    {
        "text": "spatial extras module is not present under intellij when i run ant clean-idea idea, project compilation fails because modules that depend on spatial-extras can't find it, because it is not defined. the issue appears to be that intellij 2017.1 has dropped support for some vestigial markup present in spatial-extras' .iml file.",
        "label": 47
    },
    {
        "text": "deprecate indexmodifier see discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017 this is to deprecate indexmodifier before 3.0 and remove it in 3.0. this patch includes: 1 indexmodifier and testindexmodifier are deprecated. 2 testindexwritermodify is added. it is similar to testindexmodifer but uses indexwriter and has a few other changes. the changes are because of the difference between indexmodifier and indexwriter. 3 testindexwriterlockrelease and teststressindexing are switched to use indexwriter instead of indexmodifier.",
        "label": 33
    },
    {
        "text": "avoidable synchronization bottleneck in matchalldocsquery matchallscorer the isdeleted() method on indexreader has been mentioned a number of times as a potential synchronization bottleneck. however, the reason this bottleneck occurs is actually at a higher level that wasn't focused on (at least in the threads i read). in every case i saw where a stack trace was provided to show the lock/block, higher in the stack you see the matchallscorer.next() method. in solr paricularly, this scorer is used for \"not\" queries. we saw incredibly poor performance (order of magnitude) on our load tests for not queries, due to this bottleneck. the problem is that every single document is run through this isdeleted() method, which is synchronized. having an optimized index exacerbates this issues, as there is only a single segmentreader to synchronize on, causing a major thread pileup waiting for the lock. by simply having the matchallscorer see if there have been any deletions in the reader, much of this can be avoided. especially in a read-only environment for production where you have slaves doing all the high load searching. i modified line 67 in the matchalldocsquery from: if (!reader.isdeleted(id)) { to: if (!reader.hasdeletions() || !reader.isdeleted(id)) { in our micro load test for not queries only, this was a major performance improvement. we also got the same query results. i don't believe this will improve the situation for indexes that have deletions. please consider making this adjustment for a future bug fix release.",
        "label": 33
    },
    {
        "text": "add ability to not count sub task dologic increment to contri benchmark sometimes, you want to run a sub-task like closeindex, and include the time it takes to run, but not include the count that it returns when reporting rec/s. we could adopt this approach: if a task is preceded by a \"-\" character, then, do not count the value returned by dologic. see discussion leading to this here: http://www.gossamer-threads.com/lists/lucene/java-dev/57081",
        "label": 12
    },
    {
        "text": "remove scorer freq  at the moment, scorer.freq() does different things depending on the scorer implementation: termscorer and the phrase scorers return the frequency of that term or phrase in the current document. termscorer.freq() is not actually called anywhere (apart from in a couple of tests), and xphrasescorer.freq() is only called in phraseweight.explain() the various boolean scorers return the number of matching subscorers, and are used for coord calculations. i think this is confusing. i propose that we instead add a new coord() method to scorer that by default returns 1, and that is overridden by the boolean scorers; and that we just remove freq() entirely. phraseweight.explain() can call a package-private method on xphrasescorer.",
        "label": 2
    },
    {
        "text": "improvements to unifiedhighlighter offsetstrategies this ticket improves several of the unifiedhighlighter fieldoffsetstrategies by reducing reliance on creating or re-creating tokenstreams. the primary changes are as follows: analysisoffsetstrategy - split into two offset strategies memoryindexoffsetstrategy - the primary analysis mode that utilizes a memoryindex for producing offsets tokenstreamoffsetstrategy - an offset strategy that avoids creating a memoryindex. can only be used if the query distills down to terms and automata. tokenstream removal memoryindexoffsetstrategy - previously a tokenstream was created to fill the memory index and then once consumed a new one was generated by uninverting the memoryindex back into a tokenstream if there were automata (wildcard/mtq queries) involved. now this is avoided, which should save memory and avoid a second pass over the data. termvectoroffsetstrategy - this was refactored in a similar way to avoid generating a tokenstream if automata are involved. postingswithtermvectorsoffsetstrategy - similar refactoring compositepostingsenum - aggregates several underlying postingsenums for wildcard/mtq queries. this should improve relevancy by providing unified metrics for a wildcard across all it's term matches added a highlightflag for enabling the newly separated tokenstreamoffsetstrategy since it can adversely affect passage relevancy",
        "label": 10
    },
    {
        "text": "regex support and beyond in javacc queryparser since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. i was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. i added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. i choose the forward slash '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. all chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. this token is subsequently passed to a pluggable \"parser extension\" with builds a query from the embedded string. i do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. such an extension could be another full featured query parser itself or simply a ctor call for regex query. the interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javacc sources. the downsides of this patch is clearly that i introduce a new special char into the syntax but i guess that would not be that much of a deal as it is reflected in the escape method though. it would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though. another way of solving the problem with regexquery would be to move the jdk version of regex into the core and simply have another method like: protected query newregexquery(term t) {   ...  } which i would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser. i will upload a patch in a second which implements the extension based approach i guess i will add a second patch with regex in core soon too.",
        "label": 46
    },
    {
        "text": "timelimitingcollector starts thread in static   with no way to stop them see the comment in lucenetestcase. if you even do class.forname(\"timelimitingcollector\") it starts up a thread in a static method, and there isn't a way to kill it. this is broken.",
        "label": 46
    },
    {
        "text": "add support for developing in netbeans ide it will be nice to have ant target for building netbeans ide project definition.",
        "label": 47
    },
    {
        "text": "build xml in cnotrib benchmark should auto build core java and demo if required currently one needs to build core jar and demo jar before building/running benchmark. this is not very convenient. change it to use core classes and demo classes (instead of jars). build core and demo by dependency if required.",
        "label": 15
    },
    {
        "text": " patch  russiananalyzer's tokenizer skips numbers from input text  russiananalyzer's tokenizer skips numbers from input text, so that resulting token stream miss numbers. problem can be solved by adding numbers to russiancharsets.unicoderussian. see test case below for details. testrussiananalyzer.java public class testrussiananalyzer extends testcase {   reader reader = new stringreader(\"text 1000\");   // test fails   public void teststemmer() {     testanalyzer(new russiananalyzer());   }   // test passes   public void testfixedrussiananalyzer() {     testanalyzer(new russiananalyzer(getrussiancharset()));   }   private void testanalyzer(russiananalyzer analyzer) {     try {       tokenstream stream = analyzer.tokenstream(\"text\", reader);       assertequals(\"text\", stream.next().termtext());       assertnotnull(stream.next());     } catch (ioexception e) {       fail(e.getmessage());     }   }   private char[] getrussiancharset() {     int length = russiancharsets.unicoderussian.length;     final char[] russianchars = new char[length + 10];     system         .arraycopy(russiancharsets.unicoderussian, 0, russianchars, 0, length);     russianchars[length++] = '0';     russianchars[length++] = '1';     russianchars[length++] = '2';     russianchars[length++] = '3';     russianchars[length++] = '4';     russianchars[length++] = '5';     russianchars[length++] = '6';     russianchars[length++] = '7';     russianchars[length++] = '8';     russianchars[length] = '9';     return russianchars;   } }",
        "label": 38
    },
    {
        "text": "constants lucene main version can have broken values constants#lucene_main_version is set to the lucene main version and should not contain minor versions. well this is at least what i thought and to my knowledge what the comments say too. yet in for instance 4.3.1 and 4.5.1 we broke this such that the version from segmentsinfo can not be parsed with version#parseleniently. imo we should really add an assertion that this constant doesn't throw an error and / or make the smoketester catch this. to me this is actually a index bwc break. note that 4.8.1 doesn't have this problem...",
        "label": 41
    },
    {
        "text": "testtermsenum testfloorblocks fail ant test -dtestcase=testtermsenum -dtests.method=testfloorblocks -dtests.seed=cf0dae0d9fb42bc7 -dtests.locale=es_do -dtests.timezone=america/cancun -dtests.multiplier=3 -dargs=\"-dfile.encoding=us-ascii",
        "label": 33
    },
    {
        "text": "refactor segmentinfo   fieldinfo to make them extensible after lucene-4050 is done the resulting segmentinfo / fieldinfo classes should be made abstract so that they can be extended by codec-s.",
        "label": 40
    },
    {
        "text": "remove the oom catching in simplefsdirectory and niofsdirectory followup from lucene-5161: in former times we added the oom cactching in niofsdir and simplefsdir because nobody understand why the oom could happen on filechannel.read() or simplefsdir.read(). by reading the java code its easy to understand (it allocates direct buffers with same size as the requested length to read). as we have chunking now reduce to a few kilobytes it cannot happen anymore that we get spurious ooms. in fact we might hide a real oom! so we should remove it. i am also not sure if we should make chunk size configureable in fsdirectory at all! it makes no sense to me (it was in fact only added for people that hit the oom to fine-tune). in my opinion we should remove the setter in trunk and keep it deprecated in 4.x. the buf size is then in trunk equal to the defaults from lucene-5161.",
        "label": 53
    },
    {
        "text": "build file for highlighter contrib works when run in isolation  but not when core dist is run build.xml for highlighter does not work when compilation is triggered by clean core dist call. patch has changes to fix this by updating build.xml to follow xml-query-parser build.xml",
        "label": 15
    },
    {
        "text": "wildcard query with no wildcard characters in the term throws stringindexoutofbounds exception query q1 = new wildcardquery(new term(\"text\", \"a\")); hits hits = searcher.search(q1); caught exception java.lang.stringindexoutofboundsexception : string index out of range: -1 at java.lang.string.substring(unknown source) at org.apache.lucene.search.wildcardtermenum.<init>(wildcardtermenum.java:65) at org.apache.lucene.search.wildcardquery.getenum (wildcardquery.java:38) at org.apache.lucene.search.multitermquery.rewrite(multitermquery.java:54) at org.apache.lucene.search.indexsearcher.rewrite(indexsearcher.java:137) at org.apache.lucene.search.query.weight (query.java:92) at org.apache.lucene.search.hits.<init>(hits.java:41) at org.apache.lucene.search.searcher.search(searcher.java:44) at org.apache.lucene.search.searcher.search(searcher.java:36) at quicktest.main(quicktest.java:45) from erik hatcher feel free to log this as a bug report in our jira issue tracker. it seems like a reasonable change to make, such that a wildcardquery without a wildcard character would behave like termquery.",
        "label": 32
    },
    {
        "text": "testindexwriter failes for simpletextcodec i just ran into this failure since simpletext obviously takes a lot of disk space though.     [junit] testsuite: org.apache.lucene.index.testindexwriter     [junit] testcase: testcommitonclosediskusage(org.apache.lucene.index.testindexwriter): failed     [junit] writer used too much space while adding documents: mid=608162 start=5293 end=634214     [junit] junit.framework.assertionfailederror: writer used too much space while adding documents: mid=608162 start=5293 end=634214     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:795)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:768)     [junit]  at org.apache.lucene.index.testindexwriter.testcommitonclosediskusage(testindexwriter.java:1047)     [junit]      [junit]      [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 3.281 sec     [junit]      [junit] ------------- standard output ---------------     [junit] note: reproduce with: ant test -dtestcase=testindexwriter -dtestmethod=testcommitonclosediskusage -dtests.seed=-7526585723238322940:-1609544650150801239     [junit] note: test params are: codec=simpletext, locale=th_th, timezone=uct     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.index.testindexwriter failed i did not look into simpletext but i guess we need either change the threshold for this test or exclude simpletext from it. any ideas?",
        "label": 33
    },
    {
        "text": "factory method to create bbox from bounds object hi karl wright, i propose a new factory method in the bounding box factory from a latlonbounds object. it is an utility method but i have the feeling to have to always write it when trying to build a bounding box so it may be worthy to add it. i will attach a patch.",
        "label": 25
    },
    {
        "text": "position increments should be implemented by tokenstream end  if you have pages of a book as multivalued fields, with the default position increment gap of analyzer.java (0), phrase queries won't work across pages if one ends with stopword(s). this is because the 'trailing holes' are not taken into account in end(). so i think in tokenstream.end(), subclasses of filteringtokenfilter (e.g. stopfilter) should do: super.end(); posincatt += skippedpositions; one problem is that these filters need to 'add' to the posinc, but currently nothing clears the attributes for end() [they are dirty, except offset which is set by the tokenizer]. also the indexer should be changed to pull posincatt from end().",
        "label": 33
    },
    {
        "text": "simplify configuration api of contrib query parser the current configuration api is very complicated and inherit the concept used by attribute api to store token information in token streams. however, the requirements for both (qp config and token stream) are not the same, so they shouldn't be using the same thing. i propose to simplify qp config and make it less scary for people intending to use contrib qp. the task is not difficult, it will just require a lot of code change and figure out the best way to do it. that's why it's a good candidate for a gsoc project. i would like to hear good proposals about how to make the api more friendly and less scaring",
        "label": 0
    },
    {
        "text": "multiphrasequery should allow access to terms ",
        "label": 55
    },
    {
        "text": "contrib benchmark   few improvements and a bug fix benchmark bytask was slightly improved: 1. fixed a bug in the \"child-should-not-report\" mechanism. if a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now \"penetrating/inherited\" all the way down. 2. doc size control now possible also for the reuters doc maker. (allowing to index n docs of size c characters each.) 3. trecdocmaker was added - it reads as input the .gz files used in trec - e.g. .gov data - this can be handy to benchmark lucene on these large collections. similar to the reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files. here there are multiple documents in each input file. unlike the reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes. 4. a new basicdocmaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler.",
        "label": 15
    },
    {
        "text": "migrate lucene project from svn to git  see mailing list discussion: http://mail-archives.apache.org/mod_mbox/lucene-dev/201512.mbox/%3ccal8pwkbfvt83zbczm0y-x-mdeth6hyc_xyejrev9fzzk5yxymq%40mail.gmail.com%3e",
        "label": 29
    },
    {
        "text": "npe doing local sensitive sorting when sort field is missing if you do a local sensitive sort against a field that is missing from some documents in the index an npe will get thrown. attached is a patch which resolved the issue and updates the sort test case to give coverage to this issue.",
        "label": 18
    },
    {
        "text": "indexoutofboundsexception at shinglematrixfilter's iterator hasnext method i tried to use the shinglematrixfilter within solr. to test the functionality etc., i first used the built-in field analysis view.the filter was configured to be used only at query time analysis with \"_\" as spacer character and a min. and max. shingle size of 2. the generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. also, each time i tried to query the index the exception was immediately thrown. stacktrace: java.lang.indexoutofboundsexception: index: 1, size: 1  at java.util.arraylist.rangecheck(unknown source)  at java.util.arraylist.get(unknown source)  at org.apache.lucene.analysis.shingle.shinglematrixfilter$matrix$1.hasnext(shinglematrixfilter.java:729)  at org.apache.lucene.analysis.shingle.shinglematrixfilter.next(shinglematrixfilter.java:380)  at org.apache.lucene.analysis.stopfilter.next(stopfilter.java:120)  at org.apache.lucene.analysis.tokenstream.next(tokenstream.java:47)  ... within the hasnext method, there is the s-1-th column from the arraylist columns requested, but there isn't this entry within columns. i created a patch that checks, if columns contains enough entries.",
        "label": 24
    },
    {
        "text": "improve tests to play nicely w intellij ide when running a single test in intellij, something about the way the tests are run causes the ui to display all the tests in the class other than the one that was run as if they had been ignored. in contrast, running a single test in a typical test case class not derived from lucenetestcase shows only the status of that test case method, not all the other methods in the class. this is somewhat irritating since it makes it hard to see what's going on with the test of interest. this is with intellij 2017.1.4, junit 4.12",
        "label": 11
    },
    {
        "text": "long terms should generate a runtimeexception  not just infostream as reported on the solr-user list, when a term is greater then 2^15 bytes it is silently ignored at indexing time \u2013 a message is logged in to infostream if enabled, but no error is thrown. seems like we should change this behavior (if nothing else starting in 5.0) to throw an exception.",
        "label": 18
    },
    {
        "text": "consolidate indexwriter deletedocuments  spinoff from here: http://markmail.org/message/7kjlaizqdh7kst4d. we should consolidate the various iw.deletedocuments().",
        "label": 43
    },
    {
        "text": "add indexreader acquire  and release  methods using indexreader's ref counting from: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3cpine.osx.4.64.0807170752080.1708@c5850-a3-2-62-147-22-102.dial.proxad.net%3e i have a server where a bunch of threads are handling search requests. i have a another process that updates the index used by the search server and that asks the searcher server to reopen its index reader after the updates completed. when i reopen() the index reader, i also close the old one (if the reopen() yielded a new instance). this causes problems for the other threads that are currently in the middle of a search request. i'd like to propose the addition of two methods, acquire() and release() (attached to this bug report), that increment/decrement the ref count that indexreader instances currently maintain for related purposes. that ref count prevents the index reader from being actually closed until it reaches zero. my server's search threads, thus acquiring and releasing the index reader can be sure that the index reader they're currently using is good until they're done with the current request, ie, until they release() it.",
        "label": 33
    },
    {
        "text": "if test has methods with  ignore  we should print out a notice currently these silently pass, but there is usually a reason they are @ignore (sometimes good, sometimes really a todo we should fix) in my opinion we should add reasons for all these current @ignores like mike did with test2bterms. example output: [junit] testsuite: org.apache.lucene.index.test2bterms [junit] tests run: 0, failures: 0, errors: 0, time elapsed: 0.184 sec [junit] [junit] ------------- standard error ----------------- [junit] note: ignoring test method 'test2bterms' takes ~4 hours to run on a fast machine!!  and requires that you don't use preflex codec. [junit] ------------- ---------------- --------------- ... [junit] testsuite: org.apache.solr.handler.dataimport.testmailentityprocessor [junit] tests run: 0, failures: 0, errors: 0, time elapsed: 0.043 sec [junit] [junit] ------------- standard error ----------------- [junit] note: ignoring test method 'testconnection' [junit] note: ignoring test method 'testrecursion' [junit] note: ignoring test method 'testexclude' [junit] note: ignoring test method 'testinclude' [junit] note: ignoring test method 'testincludeandexclude' [junit] note: ignoring test method 'testfetchtimesince' [junit] ------------- ---------------- ---------------",
        "label": 40
    },
    {
        "text": "standardqueryparser doesn't support pure negative clauses at first i thought it's by design but uwe says it's a bug. sqp emits this: standardqueryparser qp = new standardqueryparser(); qp.setdefaultoperator(operator.and);     system.out.println(qp.parse(\"summary:foo\", \"deffld\")); system.out.println(qp.parse(\"-summary:foo\", \"deffld\")); system.out.println(qp.parse(\"!summary:foo\", \"deffld\")); system.out.println(qp.parse(\"not summary:foo\", \"deffld\")); summary:foo summary:foo summary:foo summary:foo",
        "label": 11
    },
    {
        "text": "lucene source build doesn't work correctly by itself from the src dist running 'ant clean' (or any target) from tarsrc/lucene-3.2.0 fails due to dependency in ../common-build.xml. see also lucene-2974 for one idea to prevent this from breaking in the future.",
        "label": 40
    },
    {
        "text": "change testrandomchains to replace the list of broken classes by a list of broken constructors some classes are currently in the list of bad apples although only one constructor is broken. for example, limittokencountfilter has an option to consume the whole stream.",
        "label": 1
    },
    {
        "text": "support array offset  length setters for field with binary data currently field/fieldable interface supports only compact, zero based byte arrays. this forces end users to create and copy content of new objects before passing them to lucene as such fields are often of variable size. depending on use case, this can bring far from negligible performance improvement. this approach extends fieldable interface with 3 new methods getoffset(); gettlenght(); and getbinaryvalue() (this only returns reference to the array)",
        "label": 33
    },
    {
        "text": "postingshighlighter's passageformatter should allow for rendering to arbitrary objects for example, in a server, i may want to render the highlight result to jsonobject to send back to the front-end. today since we render to string, i have to render to json string and then re-parse to jsonobject, which is inefficient... or, if (rob's idea we make a query that's like morelikethis but it pulls terms from snippets instead, so you get proximity-influenced salient/expanded terms, then perhaps that renders to just an array of tokens or fragments or something from each snippet.",
        "label": 33
    },
    {
        "text": "spanmultitermquerywrapper with prefix query issue if we try to do a search with spanquery and a prefixquery this message is returned: \"you can only use spanmultitermquerywrapper with a suitable spanrewritemethod.\" the problem is in the wildcardquery rewrite function. if the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the spanrewritemethod and the prefix query is returned. but, that's the rewritten prefix query which should be returned: return rewritten; + return rewritten.rewrite(reader); i will attach a patch with a unit test included.",
        "label": 40
    },
    {
        "text": "upgrade jflex to jflex 1.7.0, supporting unicode 9.0, was released recently: http://jflex.de/changelog.html#jflex-1.7.0. we should upgrade.",
        "label": 47
    },
    {
        "text": "facets package reorganization facets packages have a weird structure imo. i think that we should organize the packages by feature, and not by functionality (index/search). for example: o.a.l.facet.index \u2013 core facets indexing o.a.l.facet.search \u2013 core facets search o.a.l.facet.params \u2013 all facets params (indexing and search) o.a.l.facet.associations \u2013 all associations code (we can break to sub-index/search packages if needed) o.a.l.facet.partitions \u2013 all partitions related code o.a.l.facet.sampling \u2013 all sampling related code o.a.l.facet.util \u2013 consolidate all utils under that, even those that are currently under o.a.l.util o.a.l.facet.encoding \u2013 move all encoders under it (from o.a.l.util) o.a.l.facet.taxonomy \u2013 all taxonomy related stuff. the motivation \u2013 if i want to handle all associations related code, it should be very easy to locate it.",
        "label": 43
    },
    {
        "text": "brazilian analyzer doesn't remove stopwords when uppercase is given the order of filters matter here, just need to apply lowercase token filter before removing stopwords result = new stopfilter( result, stoptable ); result = new brazilianstemfilter( result, excltable ); // convert to lowercase after stemming! result = new lowercasefilter( result ); lowercase must come before brazilianstemfilter at the end of day i'll attach a patch, it's straightforward",
        "label": 33
    },
    {
        "text": "include junit jar in source dist we recently added the junit jar under \"lib\" so that we can checkout & run tests, but we fail to include it in the source dist.",
        "label": 33
    },
    {
        "text": "in lucenetestcase beforeclass  make a new random  also using the class hashcode  to vary defaults in lucenetestcase, we set many static defaults like: default codec default infostream impl default locale default timezone default similarity currently each test run gets a single seed for the run, which means for example across one test run every single test will have say, simpletext + infostream=off + locale=german + timezone=edt + similarity=bm25 because of that, we lose lots of basic mixed coverage across tests, and it also means the unfortunate individual who gets simpletext or other slow options gets a really slow test run, rather than amortizing this across all test runs. we should at least make a new random (getrandom() ^ classname.hashcode()) to fix this so it works like before, but unfortunately that only fixes it for lucenetestcase. won't any subclasses that make random decisions in @beforeclass (and we have many) still have the same problem? maybe randomizedrunner can instead be improved here?",
        "label": 11
    },
    {
        "text": "javadocs cleanup basic cleanup in core/contrib: typos, apache license header as javadoc, missing periods that screw up package summary, etc.",
        "label": 40
    },
    {
        "text": "add backcompat support for uax29urlemailtokenizer before in lucene-5999 backcompat support was added for standardtokenizer with unicode 6.1, but uax29urlemailtokenizer was overlooked.",
        "label": 41
    },
    {
        "text": "contrary to documentation document get field  on numeric field returns null a call to numeric num = indexablefield.numericvalue() comes up with a correct value, whereas document.get(field) yields null.",
        "label": 40
    },
    {
        "text": "geopolygon test failure  [junit4] suite: org.apache.lucene.spatial3d.geom.randomgeopolygontest    [junit4]   2> note: reproduce with: ant test  -dtestcase=randomgeopolygontest -dtests.method=testcomparebigpolygons -dtests.seed=2c88b3da273be2df -dtests.multiplier=3 -dtests.slow=true -dtests.locale=en-tc -dtests.timezone=europe/budapest -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 0.01s j0 | randomgeopolygontest.testcomparebigpolygons {seed=[2c88b3da273be2df:5742535e2813b1bd]} <<<    [junit4]    > throwable #1: java.lang.assertionerror: polygon failed to build with an exception:    [junit4]    > [[lat=1.5408708232037775e-28, lon=0.0([x=1.0011188539924791, y=0.0, z=1.5425948326762136e-28])], [lat=-0.42051952071345244, lon=-0.043956709579662245([x=0.912503274975597, y=-0.04013649525500056, z=-0.40846219882801177])], [lat=0.6967302798374987, lon=-1.5354076311466454([x=0.027128243251908137, y=-0.7662593106632875, z=0.641541793498374])], [lat=0.6093302043457702, lon=-1.5374202165648532([x=0.02736481119831758, y=-0.8195876964154789, z=0.5723273145651325])], [lat=1.790840712772793e-12, lon=4.742872761198669e-13([x=1.0011188539924791, y=4.748179343323357e-13, z=1.792844402054173e-12])], [lat=-1.4523595845716656e-12, lon=9.592326932761353e-13([x=1.0011188539924791, y=9.603059346047237e-13, z=-1.4539845628913788e-12])], [lat=0.29556330360208455, lon=1.5414988021120735([x=0.02804645884597515, y=0.957023986775941, z=0.2915213382500179])]]    [junit4]    > wkt:polygon((-2.5185339401969213 -24.093993739745027,0.0 8.828539494442529e-27,5.495998489568957e-11 -8.321407453133e-11,2.7174659198424288e-11 1.0260761462208114e-10,88.32137548549387 16.934529875343248,-87.97237709688223 39.91970449365747,-88.0876897472551 34.91204903885665,-2.5185339401969213 -24.093993739745027))    [junit4]    > java.lang.illegalargumentexception: convex polygon has a side that is more than 180 degrees    [junit4]    >        at __randomizedtesting.seedinfo.seed([2c88b3da273be2df:5742535e2813b1bd]:0)    [junit4]    >        at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparepolygons(randomgeopolygontest.java:163)    [junit4]    >        at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparebigpolygons(randomgeopolygontest.java:98)    [junit4]    >        at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >        at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >        at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >        at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    >        at java.base/java.lang.thread.run(thread.java:844)    [junit4]   2> note: test params are: codec=asserting(lucene70): {}, docvalues:{}, maxpointsinleafnode=1403, maxmbsortinheap=5.306984579448146, sim=randomsimilarity(querynorm=false): {}, locale=en-tc, timezone=europe/budapest    [junit4]   2> note: linux 4.15.0-29-generic amd64/oracle corporation 9.0.4 (64-bit)/cpus=8,threads=1,free=296447064,total=536870912    [junit4]   2> note: all tests run in this jvm: [geopointtest, geoexactcircletest, testgeo3ddocvalues, randomgeopolygontest]",
        "label": 25
    },
    {
        "text": "compressed fields should be  externalized   from fields into document  right now, as of 2.0 release, lucene supports compressed stored fields. however, after discussion on java-dev, the suggestion arose, from robert engels, that it would be better if this logic were moved into the document level. this way the indexing level just stores opaque binary fields, and then document handles compress/uncompressing as needed. this approach would have prevented issues like lucene-629 because merging of segments would never need to decompress. see this thread for the recent discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/38836 when we do this we should also work on related issue lucene-648.",
        "label": 33
    },
    {
        "text": "positions are incremented for each ngram in edgengramtokenfilter edge ngrams should be like synonyms, with all the ngrams generated from a token having the same position as that original token. the current code increments position. for the text \"molecular biology\", the query \"mol bio\" should match as a phrase in neighboring positions. it does not. you can see this in the analysis page in the admin ui.",
        "label": 33
    },
    {
        "text": "geoshape intersects filter omitted matching docs spatialprefixtree#recursivegetnodes uses an optimization that prevents recursion into the deepest tree level if a parent node in the penultimate level covers all its children. this produces a bug if the optimization happens both at indexing and at query/filter time. original post",
        "label": 10
    },
    {
        "text": "unifiedhighlighter doesn't handle some automatonquery's with multi byte chars in multitermhighlighting, a characterrunautomaton is being created that takes the result of automatonquery.getautomaton that in turn is byte oriented, not character oriented. for ascii terms, this is safe but it's not for multi-byte characters. this is most likely going to rear it's head with a wildcardquery, but due to special casing in multitermhighlighting, prefixquery isn't affected. nonetheless it'd be nice to get a general fix in so that multitermhighlighting can remove special cases for prefixquery and termrangequery (both subclass automatonquery). afaict, this bug was likely in the postingshighlighter since inception.",
        "label": 10
    },
    {
        "text": "replace stringbuffer with stringbuilder where possible  add to forbidden apis this is pretty minor, but i found a few issues with the tostring implementations while looking through the facet data structures. the most egregious is the use of string concatenation in the intarray class. i have fixed that using stringbuilders. i also noticed that other classes were using stringbuffer instead of stringbuilder. according to the javadoc, \"this class is designed for use as a drop-in replacement for stringbuffer in places where the string buffer was being used by a single thread (as is generally the case). where possible, it is recommended that this class be used in preference to stringbuffer as it will be faster under most implementations.\"",
        "label": 53
    },
    {
        "text": "behavior on hard power shutdown when indexing a large number of documents, upon a hard power failure (e.g. pull the power cord), the index seems to get corrupted. we start a java application as an windows service, and feed it documents. in some cases (after an index size of 1.7gb, with 30-40 index segment .cfs files) , the following is observed. the 'segments' file contains only zeros. its size is 265 bytes - all bytes are zeros. the 'deleted' file also contains only zeros. its size is 85 bytes - all bytes are zeros. before corruption, the segments file and deleted file appear to be correct. after this corruption, the index is corrupted and lost. this is a problem observed in lucene 1.4.3. we are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.",
        "label": 33
    },
    {
        "text": "complete parallelizaton of parallelmultisearcher parallelmultisearcher is parallel only for the method signatures of 'search'. part of a query process calls the method docfreq(). there was a todo comment to parallelize this. parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.",
        "label": 53
    },
    {
        "text": "change all filteredtermsenum impls into termsenum decorators currently, filteredtermsenum has two ctors: filteredtermsenum(indexreader reader, string field) filteredtermsenum(termsenum tenum) but most of our concrete implementations (e.g. termsrangeenum) use the indexreader+field ctor in my opinion we should remove this ctor, and switch over all filteredtermsenum implementations to just take a termsenum. advantages: this simplifies filteredtermsenum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them) removes silly checks such as if (tenum == null) in every next() allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use multifields or not, it shouldnt be buried in filteredtermsenum. i created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.",
        "label": 53
    },
    {
        "text": "use reusable collation keys in icucollationfilter icucollationfilter need not create a new collationkey object for each token. in icu there is a mechanism to use a reusable key.",
        "label": 33
    },
    {
        "text": "cleanup 'good' queries code before moving some of the classes from the queries contrib to the queries module, i want to just pass over them and clean them up, since we want code in modules to be of the same calibre as core code.",
        "label": 7
    },
    {
        "text": "fastvectorhighlighter  latter terms cannot be highlighted if two or more terms are concatenated my customer found a bug in fastvectorhighlighter. i'm working for the fix. i'll post it as soon as possible. we hope the fix in 2.9.",
        "label": 33
    },
    {
        "text": "taxonomyfacetsumint floatassociations should not rollup  stumbled upon this by accident when i reviewed the code. the previous associations impl never rolled-up. the assumption is that association values are given to exact categories and have no hierarchical meaning. for instance if a document is associated with two categories: category/cs/algo and category/cs/datastructure with weights 0.95 and 0.43 respectively, it is not associated with category/cs with weight 1.38! if the app wants to association values to apply to parents in the hierarchy as well, it needs to explicitly specify that (as in passing the hierarchy categories with their own association value). i will fix the bug and also make sure the app cannot trip it by accidentally specifying hierarchical on these categories, or that if it does (cause e.g. it indexes the categories for both counting and assoc values) then we don't apply the association to all the categories in the hierarchy.",
        "label": 43
    },
    {
        "text": "incorrect sort by numeric values for documents missing the sorting field while sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (zero) value in the sort. (tested against double, float, int & long numeric fields ascending and descending order). this behavior is unexpected, as zero is \"comparable\" to the rest of the values. a better solution would either be allowing the user to define such a \"non-value\" default, or always bring those document results as the last ones. example scenario: adding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value. searching with matchalldocsquery, with sort over that field in descending order yields the docid results of 0, 2, 1. asking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?",
        "label": 53
    },
    {
        "text": "return sequence ids from iw update delete add commit to allow total ordering outside of iw based on the discussion on the mailing list iw should return sequence ids from update/delete/add and commit to allow ordering of events for consistent transaction logs and recovery.",
        "label": 46
    },
    {
        "text": "remove unused deprecated token types from standardtokenizer standardtokenizer does not recognize e-mail, company etc. this issue removes those token types.",
        "label": 47
    },
    {
        "text": "add script to attempt to reproduce failing tests from a jenkins log this script will be runnable from a downstream job triggered by an upstream failing jenkins job, passing log location info between the two. the script will also be runnable manually from a developer's cmdline. from the script help: usage:      python3 -u reproducejenkinsfailures.py url must be run from a lucene/solr git workspace. downloads the jenkins log pointed to by the given url, parses it for git revision and failed lucene/solr tests, checks out the git revision in the local workspace, groups the failed tests by module, then runs 'ant test -dtest.dups=5 -dtests.class=\"*.test1[|*.test2[...]]\" ...' in each module of interest, failing at the end if any of the runs fails. to control the maximum number of concurrent jvms used for each module's test run, set 'tests.jvms', e.g. in ~/lucene.build.properties",
        "label": 47
    },
    {
        "text": "fix reversestringfilter for unicode reversestringfilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by u+fffd by the indexer (but not at query time). the wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart. this patch implements in-place reverse with the algorithm from apache harmony abstractstringbuilder.reverse0()",
        "label": 46
    },
    {
        "text": "extend geoshape interfaces so objects can be copied serialized hi david wright, i would like to propose to extends the geoshape interfaces to be able to copy/serialized the objects. the current status and propose change is as following: geopoint: it can be serialized by using x, y, z geocircle: it can be serialized by using getcenter() and getradius() and getplanetmodel() geocompositeshape: it can be serialized by accesing shapes using size() and getshape(int index) geopath: add methods to the interface getpoints() and getcutoffangle() geopolygon: this is the most complicated one as we have different types: 1.- geocompositepolygon is just a composite 2.- geoconcavepolygon and geoconvexpolygon: create a new interface for those polygons which exposes the points, holes, internaledges and concavity/convexity 3.- geocomplexpolygons: do nothing, they are too complex to be serialize?? i am intersested in accesing the discreatization of the polygons into convex and concave ones for other reasons as well. i think this should be expose as they end result can be used for other use cases. cheers, i.",
        "label": 25
    },
    {
        "text": "buildandpushrelease py should fail if the project doap files are missing releases that are less than the release being produced i wrote on the 6.4.1 release vote thread: one minor problem with both changes.html files: there is no release date on the 6.4.0 release section. the 6.4.0 release section was never added to the doap files under dev-tools/doap/ on branch_6_4. i\u2019ll think about a way of automating a test to prevent this in the future. in the meantime, i\u2019ll add a reminder to sanity check the doap files before a release to the releasetodo.",
        "label": 47
    },
    {
        "text": "filteredquery should have getfilter  unless you are in the same package, you can't access the filter in a filteredquery. a getfilter() method should be added.",
        "label": 55
    },
    {
        "text": "blockjoinquery doesn't implement boost after reviewing lucene-3494, i checked other queries and noticed that blockjoinquery currently throws uoe for getboost and setboost: throw new unsupportedoperationexception(\"this query cannot support boosting; please use childquery.setboost instead\"); i don't think we can safely do that in queries, because other parts of lucene rely upon this working... for example bqs rewrite when it has a single clause and erases itself. so i think we should just pass down the boost to the inner weight.",
        "label": 40
    },
    {
        "text": "deprecate remaining unused classes in spatial the major changes to spatial have rendered a few other classes in the module unnecessary. this issue deprecates these classes so they can be removed later on.",
        "label": 7
    },
    {
        "text": "consolidate solr   lucene functionquery into modules spin-off from the dev list",
        "label": 7
    },
    {
        "text": "system reqs page should be release specific the system requirements page, currently under the main->resources section of the website should be part of a given version's documentation, since it will be changing for a given release. i will \"deprecate\" the existing one, but leave it in place(with a message) to cover the existing releases that don't have this, but will also add it to the release docs for future releases.",
        "label": 53
    },
    {
        "text": "collapse common module into lucene core util it was suggested by robert in http://markmail.org/message/wbfuzfamtn2qdvii that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core. given that i haven't added anything to this module except the mutablevalue classes, i'm going to collapse them into the util package, remove the module, and correct the dependencies.",
        "label": 7
    },
    {
        "text": "unexpected search results hello... i'm using lucene search with alfresco 3.3.g (i'm not sure what version of lucene is used), and i'm havin problems when the search get me results... sometimes one search can bring me just 1 result, but when i instantly do the same search at a second time it can bring me a lot of results... sometimes the search takes too much time to bring results... and sometimes the search stops at 1000 results. i'm using simple and boolean searches and both types have the same mistakes. thanks for reading and for your support. alejandro villa betancur",
        "label": 47
    },
    {
        "text": "tests using version lucene current will produce problems in backwards branch  when development for starts a lot of tests for the most-recent functionality in lucene use version.lucene_current, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions. the problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. and we loose backward compatibility testing for older versions. if we would specify a specific version like lucene_31 in our tests, after moving to backwards they must work without any changes! to not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), i propose to do the following: declare a static final version test_version = version.lucene_current (or better) version.lucene_31 in lucenetestcase(4j). change all tests that use version.lucene_current using eclipse refactor to use this constant and remove unneeded import statements. when we then move the tests to backward we must only change one line, depending on how we define this constant: if in trunk lucenetestcase it's version.lucene_current, we just change the backwards branch to use the version numer of the released thing. if trunk already uses the lucene_31 constant (i prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to version enum).",
        "label": 53
    },
    {
        "text": "possible bug in concurrentmergescheduler merge indexwriter  from dev list: \u00a8i suspect that this code is broken. lines 331 - 343 in org.apache.lucene.index.concurrentmergescheduler.merge(indexwriter) mergethreadcount() are currently active merges, they can be at most maxthreadcount, maxmergecount is number of queued merges defaulted with maxthreadcount+2 and it can never be lower then maxthreadcount, which means that condition in while can never become true. synchronized(this) { long startstalltime = 0; while (mergethreadcount() >= 1+maxmergecount) { startstalltime = system.currenttimemillis(); if (verbose()) { message(\" too many merges; stalling...\"); } try { wait(); } catch (interruptedexception ie) { throw new threadinterruptedexception(ie); } } while confusing, i think the code is actually nearly correct... but i would love to find some simplifications of cms's logic (it's really hairy). it turns out mergethreadcount() is allowed to go higher than maxthreadcount; when this happens, lucene pauses mergethreadcount()-maxthreadcount of those merge threads, and resumes them once threads finish (see updatemergethreads). ie, cms will accept up to maxmergecount merges (and launch threads for them), but will only allow maxthreadcount of those threads to be running at once. so what that while loop is doing is preventing more than maxmergecount+1 threads from starting, and then pausing the incoming thread to slow down the rate of segment creation (since merging cannot keep up). but ... i think the 1+ is wrong ... it seems like it should just be mergethreadcount() >= maxmergecount().",
        "label": 33
    },
    {
        "text": "postingshighlighter throws indexoutofbounds exception when using multivalued fields and get to maxlength when using postingshighlighter with multi-valued fields, if the sum of the lengths of the fields is more than maxlength the highlighter throws an indexoutofboundsexception. i got to this error using solr 4.3 error org.apache.solr.core.solrcore  - java.lang.indexoutofboundsexception: start 0, end -1, s.length() 131  at java.lang.abstractstringbuilder.append(abstractstringbuilder.java:476)  at java.lang.stringbuilder.append(stringbuilder.java:191)  at org.apache.lucene.search.postingshighlight.postingshighlighter$limitedstoredfieldvisitor.stringfield(postingshighlighter.java:688)  at org.apache.solr.search.solrindexsearcher.visitfromcached(solrindexsearcher.java:612)  at org.apache.solr.search.solrindexsearcher.doc(solrindexsearcher.java:580)  at org.apache.lucene.search.postingshighlight.postingshighlighter.loadfieldvalues(postingshighlighter.java:385)  at org.apache.lucene.search.postingshighlight.postingshighlighter.highlightfields(postingshighlighter.java:347)  at org.apache.solr.highlight.postingssolrhighlighter.dohighlighting(postingssolrhighlighter.java:172)  at org.apache.solr.handler.component.highlightcomponent.process(highlightcomponent.java:139)  at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:208)  at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:135)  at org.apache.solr.core.solrcore.execute(solrcore.java:1816)  at org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:656)  at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:359)  at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:155)  at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1307)  at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:453)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:137)  at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:560)  at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:231)  at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1072)  at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:382)  at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:193)  at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:1006)  at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:135)  at org.eclipse.jetty.server.handler.contexthandlercollection.handle(contexthandlercollection.java:255)  at org.eclipse.jetty.server.handler.handlercollection.handle(handlercollection.java:154)  at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:116)  at org.eclipse.jetty.server.server.handle(server.java:365)  at org.eclipse.jetty.server.abstracthttpconnection.handlerequest(abstracthttpconnection.java:485)  at org.eclipse.jetty.server.blockinghttpconnection.handlerequest(blockinghttpconnection.java:53)  at org.eclipse.jetty.server.abstracthttpconnection.headercomplete(abstracthttpconnection.java:926)  at org.eclipse.jetty.server.abstracthttpconnection$requesthandler.headercomplete(abstracthttpconnection.java:988)  at org.eclipse.jetty.http.httpparser.parsenext(httpparser.java:635)  at org.eclipse.jetty.http.httpparser.parseavailable(httpparser.java:235)  at org.eclipse.jetty.server.blockinghttpconnection.handle(blockinghttpconnection.java:72)  at org.eclipse.jetty.server.bio.socketconnector$connectorendpoint.run(socketconnector.java:264)  at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:608)  at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:543)  at java.lang.thread.run(thread.java:722)",
        "label": 33
    },
    {
        "text": "deprecate remove language specific tokenizers in favor of standardtokenizer as of lucene 3.1, standardtokenizer implements uax#29 word boundary rules to provide language-neutral tokenization. lucene contains several language-specific tokenizers that should be replaced by uax#29-based standardtokenizer (deprecated in 3.1 and removed in 4.0). the language-specific analyzers, by contrast, should remain, because they contain language-specific post-tokenization filters. the language-specific analyzers should switch to standardtokenizer in 3.1. some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer. for example, persiananalyzer currently uses arabiclettertokenizer, and depends on the fact that this tokenizer breaks tokens on the zwnj character (zero-width non-joiner; u+200c), but in the uax#29 word boundary rules, zwnj is not a word boundary. robert muir has suggested using a char filter converting zwnj to spaces prior to standardtokenizer in the converted persiananalyzer.",
        "label": 40
    },
    {
        "text": "update read before regenerating txt reading the file read_before_regenerating.txt from analysis/common/src/java/org/apache/lucene/analysis/standard tells me to use jflex trunk. ant regenerate already uses ivy to get current jflex (1.6) which should be used - does the text still apply or is it obsolete?",
        "label": 47
    },
    {
        "text": "ramusageestimator has too general exception handlers core util ramusageestimator has several handlers in which \"exception\" was caught and ignored. such a general handler not only includes reflection-related exceptions such as classnotfoundexception, nosuchfieldexception and nosuchmethodexception (which are ok to be ignored) but also harmful exceptions (that should at least be logged), such as nullpointerexception, unsupportedoperationexception, securityexception, etc.",
        "label": 53
    },
    {
        "text": "testcodecloadingdeadlock testdeadlock failure has no  reproduce with  line is it expected that there are test situations where a \"reproduce with\" line is not printed? (reproducejenkinsfailures.py assumes that all failures produce such a line.) here's one from https://builds.apache.org/job/lucene-solr-smokerelease-7.x/172/:    [smoker]    [junit4] suite: org.apache.lucene.codecs.testcodecloadingdeadlock    [smoker]    [junit4] failure 30.4s j0 | testcodecloadingdeadlock.testdeadlock <<<    [smoker]    [junit4]    > throwable #1: java.lang.assertionerror: process did not exit after 30 secs -> classloader deadlock?    [smoker]    [junit4]    >  at __randomizedtesting.seedinfo.seed([88b4fc32922379:de355e834c88eaf]:0)    [smoker]    [junit4]    >  at org.apache.lucene.codecs.testcodecloadingdeadlock.testdeadlock(testcodecloadingdeadlock.java:75)    [smoker]    [junit4] completed [132/466 (1!)] on j0 in 30.45s, 1 test, 1 failure <<< failures! note: git checkout -f d77f618a672d719b971684c55b9fa07d0ba59aaf",
        "label": 11
    },
    {
        "text": "indexwriter close true  should either not be interruptible or should abort background merge threads before returning currently, there is no safe way to close a directory after closing the writer without causing an exception in a merge thread if the #close call is interrupted.",
        "label": 53
    },
    {
        "text": "expose filteredtermsenum from mtq mtq#getenum() is protected and in order to access it you need to be in the o.a.l.search package. here is a relevant snipped from the mailing list discussion getenum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). they are also accessible by other classes from the same package, but that's more a java bug than a feature. the problem with mtq is that rewritemethod is a separate *class* and *not a subclass* of mtq, so the method cannot be called (it can because of the \"java bug\" called from same package). so theoretically it has to be public otherwise you cannot call getenum(). another cleaner fix would be to add a protected final method to rewritemethod that calls this method from mtq. so anything subclassing rewritemethod can get the enum from inside the rewritemethod class which is the \"correct\" way to handle it. delegating to mtq is then \"internal\".",
        "label": 46
    },
    {
        "text": "move smartchineseanalyzer into the smartcn package an offshoot of lucene-1862, org.apache.lucene.analysis.cn.smartchineseanalyzer should become org.apache.lucene.analysis.cn.smartcn.smartchineseanalyzer",
        "label": 40
    },
    {
        "text": "if tests fail  don't report about unclosed resources lucenetestcase ensures in afterclass() if you closed all your directories, which in turn will check if you have closed any open files. this is good, as a test will fail if we have resource leaks. but if a test truly fails, this is just confusing, because its usually not going to make it to the part of its code where it would call .close() so, if any tests fail, i think we should omit this check in afterclass()",
        "label": 40
    },
    {
        "text": "compositestrategytest testoperations  overlaps  should have matched  assertionerror i hit this while doing unrelated beasting: -test:     [mkdir] created dir: /l/trunk/lucene/build/spatial/test [junit4:pickseed] seed property 'tests.seed' already defined: e1320619ec8749df     [mkdir] created dir: /l/trunk/lucene/build/spatial/test/temp     [mkdir] created dir: /l/trunk/.caches/test-stats/spatial    [junit4] <junit4> says hallo! master seed: e1320619ec8749df    [junit4] executing 1 suite with 1 jvm.    [junit4]     [junit4] started j0 pid(32324@localhost).    [junit4] suite: org.apache.lucene.spatial.composite.compositestrategytest    [junit4] ok      0.29s | compositestrategytest.testoperations {#0 seed=[e1320619ec8749df:9319db8f802fc2c7]}    [junit4] ok      0.08s | compositestrategytest.testoperations {#1 seed=[e1320619ec8749df:274f6773b4ed09eb]}    [junit4]   2> note: reproduce with: ant test  -dtestcase=compositestrategytest -dtests.method=testoperations -dtests.seed=e1320619ec8749df -dtests.locale=nl_nl -dtests.timezone=antarctica/casey -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] failure 0.15s | compositestrategytest.testoperations {#2 seed=[e1320619ec8749df:a9a6f1afe5294120]} <<<    [junit4]    > throwable #1: java.lang.assertionerror: [overlaps] qidx:1 should have matched i#2:circle(pt(x=45.0,y=-8.0), d=88.4\u00b0 9827.73km) q:circle(pt(x=-135.0,y=8.0), d=57.5\u00b0 6396.19km)    [junit4]    >  at __randomizedtesting.seedinfo.seed([e1320619ec8749df:a9a6f1afe5294120]:0)    [junit4]    >  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.fail(randomspatialopstrategytestcase.java:127)    [junit4]    >  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperation(randomspatialopstrategytestcase.java:121)    [junit4]    >  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperationrandomshapes(randomspatialopstrategytestcase.java:56)    [junit4]    >  at org.apache.lucene.spatial.composite.compositestrategytest.testoperations(compositestrategytest.java:99)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4] ok      0.04s | compositestrategytest.testoperations {#3 seed=[e1320619ec8749df:98485a4a89d71a09]}    [junit4] ok      0.14s | compositestrategytest.testoperations {#4 seed=[e1320619ec8749df:d489dfe728dfdab2]}    [junit4] ok      0.06s | compositestrategytest.testoperations {#5 seed=[e1320619ec8749df:45730cb8556397b2]}    [junit4] ok      0.04s | compositestrategytest.testoperations {#6 seed=[e1320619ec8749df:7bad683e475387b4]}    [junit4] ok      0.06s | compositestrategytest.testoperations {#7 seed=[e1320619ec8749df:e71ef2446447131a]}    [junit4] ok      0.03s | compositestrategytest.testoperations {#8 seed=[e1320619ec8749df:d5b2172a138c0440]}    [junit4] ok      0.03s | compositestrategytest.testoperations {#9 seed=[e1320619ec8749df:239419077db3312]}    [junit4] ok      0.03s | compositestrategytest.testoperations {#10 seed=[e1320619ec8749df:f778a9cba56711ca]}    [junit4] ok      0.03s | compositestrategytest.testoperations {#11 seed=[e1320619ec8749df:7cdf650e21fab003]}    [junit4] ok      0.02s | compositestrategytest.testoperations {#12 seed=[e1320619ec8749df:1becf0b7c48772f2]}    [junit4] ok      0.04s | compositestrategytest.testoperations {#13 seed=[e1320619ec8749df:74a7d7a85812f382]}    [junit4] ok      0.02s | compositestrategytest.testoperations {#14 seed=[e1320619ec8749df:2939d8308a63b5db]}    [junit4] ok      0.04s | compositestrategytest.testoperations {#15 seed=[e1320619ec8749df:4a8b353498e120ea]}    [junit4] ok      0.02s | compositestrategytest.testoperations {#16 seed=[e1320619ec8749df:cc7092fd53e94b83]}    [junit4] ok      0.03s | compositestrategytest.testoperations {#17 seed=[e1320619ec8749df:783f40e69fdc39f1]}    [junit4] ok      0.04s | compositestrategytest.testoperations {#18 seed=[e1320619ec8749df:674b3fe8e349d6f3]}    [junit4] ok      0.12s | compositestrategytest.testoperations {#19 seed=[e1320619ec8749df:6ddb4c0449a1905e]}    [junit4]   2> note: test params are: codec=asserting(lucene53), sim=defaultsimilarity, locale=nl_nl, timezone=antarctica/casey    [junit4]   2> note: linux 3.13.0-46-generic amd64/oracle corporation 1.8.0_40 (64-bit)/cpus=8,threads=1,free=374435352,total=495976448    [junit4]   2> note: all tests run in this jvm: [compositestrategytest]    [junit4] completed [1/1] in 1.47s, 20 tests, 1 failure <<< failures!    [junit4]     [junit4]     [junit4] tests with failures:    [junit4]   - org.apache.lucene.spatial.composite.compositestrategytest.testoperations {#2 seed=[e1320619ec8749df:a9a6f1afe5294120]}    [junit4]     [junit4]     [junit4] jvm j0:     0.38 ..     2.29 =     1.91s    [junit4] execution time total: 2.29 sec.    [junit4] tests summary: 1 suite, 20 tests, 1 failure",
        "label": 10
    },
    {
        "text": "change attributesource api to use generics the attributesource api will be easier to use with jdk 1.5 generics. uwe, if you started working on a patch for this already feel free to assign this to you.",
        "label": 53
    },
    {
        "text": "directorytaxonomywriter extensions should be able to set internal index writer config attributes such as info stream current protected openindexwriter(directory directory, openmode openmode) does not provide access to the iwc it creates. so extensions must reimplement this method completely in order to set e.f. info stream for the internal index writer. this came up in user question: taxonomy indexer debug",
        "label": 12
    },
    {
        "text": "thaiwordfilter uses attributesource copyto incorrectly the bug can be seen by https://builds.apache.org/hudson/job/lucene-solr-tests-only-3.x/7367/ it looks like the issue is this lazy initialization of the cloned token: if the tokenstream is reused and the consumer is interested in a different set of attributes, it could be a problem. one probably-probably-not-totally-correct fix would be to add 'clonedtoken = null;' to reset(), at least it would solve this case?",
        "label": 40
    },
    {
        "text": "highlighting overlapping tokens outputs doubled words if for the text \"the fox did not jump\" we generate following tokens : (the, 0, 0-3),( {fox} ,0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18) if termvector for field is stored with_offsets and not with_positions_offsets, highlighing would output \"the<em>the fox</em> did not jump\" i join a patch with 2 additive junit tests and a fix of tokensources class where token ordering by offset did'nt manage well overlapping tokens.",
        "label": 40
    },
    {
        "text": "modify spannotquery to act as spannotnearquery too with very small modifications, spannotquery can act as a spannotnearquery. to find \"a\" but not if \"b\" appears 3 tokens before or 4 tokens after \"a\": new spannotquery(\"a\", \"b\", 3, 4) original constructor still exists and calls spannotquery(\"a\", \"b\", 0, 0). patch with tests on way.",
        "label": 10
    },
    {
        "text": "cleanup suggester api currently the suggester api and especially termfreqiterator don't play that nice with bytesref and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with termsenum, bytesref etc. we should try to clean up this api step by step moving over to bytesref including the lookup class and its interface...",
        "label": 46
    },
    {
        "text": "blockjoinquery advance fails on an assert in case of a single parent with child segment the blockjoinquery will fail on an assert when advance in called on a segment with a single parent with a child. the call to parentbits.prevsetbit(parenttarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. just removing the assert fixes the problem, since nextdoc will handle it properly. also, i don't understand the \"assert parenttarget != 0;\", with a comment of each parent must have one child. there isn't really a reason to add this constraint, as far as i can tell..., just call nextdoc in this case, no?",
        "label": 33
    },
    {
        "text": "javacc skeleton files not regenerated copies of the the character stream files for javacc are checked into svn. these files were generated under javacc 3.0 (at least that's what they say, though javacc 3.2 says this too). javacc 4 complains that they are out of date but won't replace them; they must be removed before it will regenerate them. there is one side effect of removing them: local changes are lost. r387550 removed a couple of deprecated methods. by using the files as generated by javacc, these deprecated methods will be readded (at least until the javacc team removes them totally). there are other changes being made to the stream files, so i woudl think it's better to live with them unmodified than to keep local versions just for this change. if we want javacc to recreate the files, the attached patch will remove them before running javacc. all the tests pass using both javacc3.2 and 4.0.",
        "label": 48
    },
    {
        "text": "testpayloadnearquery fails with npe ant test -dtestcase=testpayloadnearquery -dtests.method=test -dtests.seed=24743b1132665845 -dtests.slow=true -dtests.locale=es_ni -dtests.timezone=israel -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] started j0 pid(19895@localhost).    [junit4] suite: org.apache.lucene.search.payloads.testpayloadnearquery    [junit4]   2> note: reproduce with: ant test  -dtestcase=testpayloadnearquery -dtests.method=test -dtests.seed=24743b1132665845 -dtests.slow=true -dtests.locale=es_ni -dtests.timezone=israel -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   0.09s | testpayloadnearquery.test <<<    [junit4]    > throwable #1: java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.nullpointerexception    [junit4]    >  at __randomizedtesting.seedinfo.seed([24743b1132665845:ac2004cb9c9a35bd]:0)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:669)    [junit4]    >  at org.apache.lucene.search.indexsearcher.searchafter(indexsearcher.java:353)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:382)    [junit4]    >  at org.apache.lucene.search.payloads.testpayloadnearquery.test(testpayloadnearquery.java:144)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.util.concurrent.executionexception: java.lang.nullpointerexception    [junit4]    >  at java.util.concurrent.futuretask.report(futuretask.java:122)    [junit4]    >  at java.util.concurrent.futuretask.get(futuretask.java:192)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:665)    [junit4]    >  ... 39 more    [junit4]    > caused by: java.lang.nullpointerexception    [junit4]    >  at org.apache.lucene.search.payloads.payloadnearquery$payloadnearspanscorer.processpayloads(payloadnearquery.java:202)    [junit4]    >  at org.apache.lucene.search.payloads.payloadnearquery$payloadnearspanscorer.setfreqcurrentdoc(payloadnearquery.java:223)    [junit4]    >  at org.apache.lucene.search.spans.spanscorer.ensurefreq(spanscorer.java:65)    [junit4]    >  at org.apache.lucene.search.spans.spanscorer.score(spanscorer.java:118)    [junit4]    >  at org.apache.lucene.search.assertingscorer.score(assertingscorer.java:67)    [junit4]    >  at org.apache.lucene.search.topscoredoccollector$simpletopscoredoccollector$1.collect(topscoredoccollector.java:64)    [junit4]    >  at org.apache.lucene.search.assertingleafcollector.collect(assertingleafcollector.java:53)    [junit4]    >  at org.apache.lucene.search.assertingcollector$1.collect(assertingcollector.java:57)    [junit4]    >  at org.apache.lucene.search.assertingleafcollector.collect(assertingleafcollector.java:53)    [junit4]    >  at org.apache.lucene.search.weight$defaultbulkscorer.scoreall(weight.java:203)    [junit4]    >  at org.apache.lucene.search.weight$defaultbulkscorer.score(weight.java:174)    [junit4]    >  at org.apache.lucene.search.bulkscorer.score(bulkscorer.java:35)    [junit4]    >  at org.apache.lucene.search.assertingbulkscorer.score(assertingbulkscorer.java:69)    [junit4]    >  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:714)    [junit4]    >  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:93)    [junit4]    >  at org.apache.lucene.search.indexsearcher$4.call(indexsearcher.java:656)    [junit4]    >  at org.apache.lucene.search.indexsearcher$4.call(indexsearcher.java:653)    [junit4]    >  at java.util.concurrent.futuretask.run(futuretask.java:265)    [junit4]    >  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)    [junit4]    >  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)    [junit4]    >  ... 1 more    [junit4]   2> note: test params are: codec=fastdecompressioncompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fast_decompression, chunksize=25825, maxdocsperchunk=709, blocksize=459), termvectorsformat=compressingtermvectorsformat(compressionmode=fast_decompression, chunksize=25825, blocksize=459)), sim=defaultsimilarity, locale=es_ni, timezone=israel    [junit4]   2> note: linux 3.13.0-49-generic amd64/oracle corporation 1.9.0-ea (64-bit)/cpus=8,threads=1,free=199771008,total=253231104    [junit4]   2> note: all tests run in this jvm: [testpayloadnearquery]    [junit4] completed [1/1] in 0.71s, 1 test, 1 error <<< failures!",
        "label": 2
    },
    {
        "text": "geoexactcircletest randompointbearingcardinaltest failures i hit some reproducing failures over the weekend: ant test  -dtestcase=geoexactcircletest -dtests.method=randompointbearingcardinaltest -dtests.seed=30b96a8700f32d8f -dtests.slow=true -dtests.locale=ar-sd -dtests.timezone=turkey -dtests.asserts=true -dtests.file.encoding=utf8 [junit4] failure 0.01s j0 | geoexactcircletest.randompointbearingcardinaltest {seed=[30b96a8700f32d8f:475e54a204015a1c]} <<<    [junit4]    > throwable #1: java.lang.assertionerror: planetmodel(ab=1.7929995623606008 c=1.17777596251282) 0.022823921875714692 2.6270976802297388    [junit4]    >  at __randomizedtesting.seedinfo.seed([30b96a8700f32d8f:475e54a204015a1c]:0)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geoexactcircletest.checkbearingpoint(geoexactcircletest.java:117)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geoexactcircletest.randompointbearingcardinaltest(geoexactcircletest.java:109)    [junit4]    >  at java.lang.thread.run(thread.java:748)    [junit4]   2> note: test params are: codec=asserting(lucene70): {}, docvalues:{}, maxpointsinleafnode=478, maxmbsortinheap=5.961909961194244, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@179f38fb), locale=ar-sd, timezone=turkey    [junit4]   2> note: linux 4.4.0-1043-aws amd64/oracle corporation 1.8.0_151 (64-bit)/cpus=4,threads=1,free=269120312,total=319291392    [junit4]   2> note: all tests run in this jvm: [xyzsolidtest, testgeo3ddocvalues, geoexactcircletest] ant test  -dtestcase=geoexactcircletest -dtests.method=randompointbearingcardinaltest -dtests.seed=30b96a8700f32d8f -dtests.slow=true -dtests.locale=ar-sd -dtests.timezone=turkey -dtests.asserts=true -dtests.file.encoding=utf8    [junit4] failure 0.02s j2 | geoexactcircletest.randompointbearingcardinaltest {seed=[8c1e53dfce9646f5:8dcce74adec6d907]} <<<    [junit4]    > throwable #1: java.lang.assertionerror: planetmodel(ab=1.0366200558773102 c=0.6736249299915238) 0.0011591580078804675 2.649410126114567    [junit4]    >  at __randomizedtesting.seedinfo.seed([8c1e53dfce9646f5:8dcce74adec6d907]:0)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geoexactcircletest.checkbearingpoint(geoexactcircletest.java:117)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geoexactcircletest.randompointbearingcardinaltest(geoexactcircletest.java:109)    [junit4]    >  at java.lang.thread.run(thread.java:748)    [junit4]   2> note: test params are: codec=asserting(lucene70): {}, docvalues:{}, maxpointsinleafnode=1185, maxmbsortinheap=5.925083864677718, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@6a7f1f9), locale=en-au, timezone=cnt    [junit4]   2> note: linux 2.6.32-696.6.3.el6.x86_64 amd64/oracle corporation 1.8.0_151 (64-bit)/cpus=4,threads=1,free=207196520,total=251658240    [junit4]   2> note: all tests run in this jvm: [testgeo3ddocvalues, geocircletest, geoexactcircletest]    [junit4] completed [11/16 (1!)] on j2 in 1.60s, 311 tests, 1 failure <<< failures!",
        "label": 25
    },
    {
        "text": "convert build to work with git rather than svn  we assume an svn checkout in parts of our build and will need to move to assuming a git checkout. patches against https://github.com/dweiss/lucene-solr-svn2git from lucene-6933.",
        "label": 29
    },
    {
        "text": "assertion fails for toparentblockjoinquery blockjoinscorer nextdoc after i enable assertion with \"-ea:org.apache...\" i got the stack trace below. i checked that the parent filter only match parent documents and the child filter only match child documents. field \"id\" is unique. 16:55:06,269 error [org.apache.solr.servlet.solrdispatchfilter] (http-127.0.0.1/127.0.0.1:8080-1) null:java.lang.runtimeexception: java.lang.assertionerror at org.apache.solr.servlet.httpsolrcall.senderror(httpsolrcall.java:593) at org.apache.solr.servlet.httpsolrcall.call(httpsolrcall.java:465) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:227) at org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:196) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:246) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:214) at org.apache.catalina.core.standardwrappervalve.invoke(standardwrappervalve.java:230) at org.apache.catalina.core.standardcontextvalve.invoke(standardcontextvalve.java:149) at org.jboss.as.web.security.securitycontextassociationvalve.invoke(securitycontextassociationvalve.java:169) at org.apache.catalina.core.standardhostvalve.invoke(standardhostvalve.java:145) at org.apache.catalina.valves.errorreportvalve.invoke(errorreportvalve.java:97) at org.apache.catalina.valves.accesslogvalve.invoke(accesslogvalve.java:559) at org.apache.catalina.core.standardenginevalve.invoke(standardenginevalve.java:102) at org.apache.catalina.connector.coyoteadapter.service(coyoteadapter.java:336) at org.apache.coyote.http11.http11processor.process(http11processor.java:856) at org.apache.coyote.http11.http11protocol$http11connectionhandler.process(http11protocol.java:653) at org.apache.tomcat.util.net.jioendpoint$worker.run(jioendpoint.java:920) at java.lang.thread.run(thread.java:744) caused by: java.lang.assertionerror at org.apache.lucene.search.join.toparentblockjoinquery$blockjoinscorer.nextdoc(toparentblockjoinquery.java:278) at org.apache.lucene.search.weight$defaultbulkscorer.scoreall(weight.java:204) at org.apache.lucene.search.weight$defaultbulkscorer.score(weight.java:176) at org.apache.lucene.search.bulkscorer.score(bulkscorer.java:35) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:771) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:485) at org.apache.solr.search.solrindexsearcher.buildandruncollectorchain(solrindexsearcher.java:202) at org.apache.solr.search.solrindexsearcher.getdoclistnc(solrindexsearcher.java:1666) at org.apache.solr.search.solrindexsearcher.getdoclistc(solrindexsearcher.java:1485) at org.apache.solr.search.solrindexsearcher.search(solrindexsearcher.java:561) at org.apache.solr.handler.component.querycomponent.process(querycomponent.java:518) at org.apache.solr.handler.component.searchhandler.handlerequestbody(searchhandler.java:255) at org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:143) at org.apache.solr.core.solrcore.execute(solrcore.java:2064) at org.apache.solr.servlet.httpsolrcall.execute(httpsolrcall.java:654) at org.apache.solr.servlet.httpsolrcall.call(httpsolrcall.java:450) ... 16 more without assertions enabled: 17:21:39,008 error [org.apache.solr.servlet.solrdispatchfilter] (http-127.0.0.1/127.0.0.1:8080-1) null:java.lang.illegalstateexception: child query must only match non-parent docs, but parent docid=2147483647 matched childscorer=class org.apache.lucene.search.conjunctionscorer at org.apache.lucene.search.join.toparentblockjoinquery$blockjoinscorer.nextdoc(toparentblockjoinquery.java:334)",
        "label": 35
    },
    {
        "text": "constants lucene main version needs updated on branch current test failure on the 4_3 branch... [junit4:junit4] suite: org.apache.lucene.index.testcheckindex [junit4:junit4] ok      0.37s | testcheckindex.testdeleteddocs [junit4:junit4]   2> note: reproduce with: ant test  -dtestcase=testcheckindex -dtests.method=testluceneconstantversion -dtests.seed=dc6bfdd66878c275 -dtests.slow=true -dtests.locale=be_by -dtests.timezone=asia/baku -dtests.file.encoding=utf-8 [junit4:junit4] failure 0.03s | testcheckindex.testluceneconstantversion <<< [junit4:junit4]    > throwable #1: java.lang.assertionerror: invalid version: 4.3.1-snapshot [junit4:junit4]    >  at __randomizedtesting.seedinfo.seed([dc6bfdd66878c275:1ff50a184e6722ba]:0) [junit4:junit4]    >  at org.apache.lucene.index.testcheckindex.testluceneconstantversion(testcheckindex.java:132) [junit4:junit4]    >  at java.lang.thread.run(thread.java:679) [junit4:junit4] ok      0.02s | testcheckindex.testbogustermvectors [junit4:junit4]   2> note: test params are: codec=appending, sim=defaultsimilarity, locale=be_by, timezone=asia/baku [junit4:junit4]   2> note: linux 3.2.0-43-generic amd64/sun microsystems inc. 1.6.0_27 (64-bit)/cpus=4,threads=1,free=219494672,total=247726080 [junit4:junit4]   2> note: all tests run in this jvm: [testcheckindex] [junit4:junit4] completed in 0.67s, 3 tests, 1 failure <<< failures!",
        "label": 18
    },
    {
        "text": "monotonic packed could maybe be faster this compression is used in lucene for monotonically increasing offsets, e.g. stored fields index, dv binary/sorted_set offsets, ordinalmap (used for merging and faceting dv) and so on. today this stores a +/- deviation from an expected line of y=mx + b, where b is the minvalue for the block and m is the average delta from the previous value. because it can be negative, we have to do some additional work to zigzag-decode. can we just instead waste a bit for every value explicitly (lower the minvalue by the min delta) so that deltas are always positive and we can have a simpler decode? maybe if we do this, the new guy should assert that values are actually monotic at write-time. the current one supports \"mostly monotic\" but do we really need that flexibility anywhere? if so it could always be kept...",
        "label": 1
    },
    {
        "text": "rethink multi term analysis handling the current framework for handling term normalisation works via instanceof checks for multitermawarecomponent and casts.  multitermawarecomponent itself deals in abstractanalysiscomponents, and so callers need to cast to the correct component type before use, which is ripe for misuse. we should re-organise all this to be type-safe and usable without casts.  one possibility is to add `normalize` methods to charfilterfactory and tokenfilterfactory that mirror their existing `create` methods.  the default implementation would return the input unchanged, while filters that should apply at normalization time can delegate to `create`. related to this, we should deprecate and remove lowercasetokenizer, which combines tokenization and normalization in a way that will break this api.",
        "label": 2
    },
    {
        "text": "wrong default attribute factory in use originally reported to the mailing list: http://mail-archives.apache.org/mod_mbox/lucene-java-user/201607.mbox/%3ccaj0vynnmah7n7bypevtv9htxo-nk-b7mwuwrgp4x8gn=v4pybg@mail.gmail.com%3e lucene-7355 made a change to customanalyzer.createcomponents() such that it uses a different attributefactory. https://github.com/apache/lucene-solr/commit/e92a38af90d12e51390b4307ccbe0c24ac7b6b4e#diff-b39a076156e10aa7a4ba86af0357a0fel122 the previous default was tokenstream.default_token_attribute_factory which uses packedtokenattributeimpl while the new default is now attributefactory.default_attribute_factory which does not use packedtokenattributeimpl. uwe schindler asked me to open an issue for this.",
        "label": 53
    },
    {
        "text": " who tests the tester  tests sometimes fail under ibm j9 curiously, the tests only seem to fail when i run \"ant test\" from lucene/core. no \"reproduce with\" line is printed ... and if i run the failing tests alone, they do not fail. the failures look like this: [junit4:junit4] suite: org.apache.lucene.util.junitcompat.testfailifdirectorynotclosed [junit4:junit4] failure 0.02s j2 | testfailifdirectorynotclosed.testfailifdirectorynotclosed <<< [junit4:junit4]    > throwable #1: java.lang.assertionerror: expected:<1> but was:<0> [junit4:junit4]    >  at org.junit.assert.fail(assert.java:93) [junit4:junit4]    >  at org.junit.assert.failnotequals(assert.java:647) [junit4:junit4]    >  at org.junit.assert.assertequals(assert.java:128) [junit4:junit4]    >  at org.junit.assert.assertequals(assert.java:472) [junit4:junit4]    >  at org.junit.assert.assertequals(assert.java:456) [junit4:junit4]    >  at org.apache.lucene.util.junitcompat.testfailifdirectorynotclosed.testfailifdirectorynotclosed(testfailifdirectorynotclosed.java:41) [junit4:junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit4:junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:60) [junit4:junit4]    >  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:37) [junit4:junit4]    >  at java.lang.reflect.method.invoke(method.java:611) [junit4:junit4]    >  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) [junit4:junit4]    >  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) [junit4:junit4]    >  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) [junit4:junit4]    >  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) [junit4:junit4]    >  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28) [junit4:junit4]    >  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:30) [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.systempropertiesrestorerule$1.evaluate(systempropertiesrestorerule.java:53) [junit4:junit4]    >  at org.junit.rules.runrules.evaluate(runrules.java:18) [junit4:junit4]    >  at org.junit.runners.parentrunner.runleaf(parentrunner.java:263) [junit4:junit4]    >  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:68) [junit4:junit4]    >  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:47) [junit4:junit4]    >  at org.junit.runners.parentrunner$3.run(parentrunner.java:231) [junit4:junit4]    >  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:60) [junit4:junit4]    >  at org.junit.runners.parentrunner.runchildren(parentrunner.java:229) [junit4:junit4]    >  at org.junit.runners.parentrunner.access$000(parentrunner.java:50) [junit4:junit4]    >  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:222) [junit4:junit4]    >  at org.junit.runners.parentrunner.run(parentrunner.java:300) [junit4:junit4]    >  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.execute(slavemain.java:180) [junit4:junit4]    >  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.main(slavemain.java:275) [junit4:junit4]    >  at com.carrotsearch.ant.tasks.junit4.slave.slavemainsafe.main(slavemainsafe.java:12) [junit4:junit4] completed on j2 in 0.02s, 1 test, 1 failure <<< failures! and at the end of \"ant test\" i see: [junit4:junit4] tests with failures (first 10 out of 34): [junit4:junit4]   - org.apache.lucene.store.testlockfactory (suite) [junit4:junit4]   - org.apache.lucene.util.junitcompat.testcodecreported.testcorrectcodecreported [junit4:junit4]   - org.apache.lucene.index.testindexwriterreader (suite) [junit4:junit4]   - org.apache.lucene.util.junitcompat.testfailifdirectorynotclosed.testfailifdirectorynotclosed [junit4:junit4]   - org.apache.lucene.util.junitcompat.testsetupteardownchaining.testsetupchaining [junit4:junit4]   - org.apache.lucene.util.junitcompat.testsetupteardownchaining.testteardownchaining [junit4:junit4]   - org.apache.lucene.codecs.compressing.testcompressingstoredfieldsformat (suite) [junit4:junit4]   - org.apache.lucene.util.junitcompat.testleavefilesiftestfails.testleavefilesiftestfails [junit4:junit4]   - org.apache.lucene.util.junitcompat.testreproducemessage.testfailurebeforeclass [junit4:junit4]   - org.apache.lucene.util.junitcompat.testreproducemessage.testfailureinitializer java version is: java version \"1.6.0\" java(tm) se runtime environment (build pxa6460sr9fp2ifix-20111111_05(sr9 fp2+iv03622+iv02378+iz99243+iz97310+iv00707)) ibm j9 vm (build 2.4, jre 1.6.0 ibm j9 2.4 linux amd64-64 jvmxa6460sr9-20111111_94827 (jit enabled, aot enabled) j9vm - 20111111_094827 jit  - r9_20101028_17488ifx45 gc   - 20101027_aa) jcl  - 20110727_04",
        "label": 11
    },
    {
        "text": "unifiedhighlighter doesn't highlight prefixquery with multi byte chars unifiedhighlighter highlighter = new unifiedhighlighter(null, new standardanalyzer()); query query = new prefixquery(new term(\"title\", \"\u044f\")); string testdata = \"\u044f\"; object snippet = highlighter.highlightwithoutsearcher(fieldname, query, testdata, 1); system.out.printf(\"testdata=[%s] query=%s snippet=[%s]\\n\", testdata, query, snippet==null?null:snippet.tostring());",
        "label": 10
    },
    {
        "text": "extended javadocs in spellchecker added some javadocs that explains why the spellchecker does not work as one might expect it to. http://www.nabble.com/spellchecker%3a%3asuggestsimilar%28%29-question-tf3118660.html#a8640395 > without having looked at the code for a long time, i think the problem is what the > lucene scoring consider to be best. first the grams are searched, resulting in a number > of hits. then the edit-distance is calculated on each hit. \"genetics\" is appearently the > third most similar hit according to lucene, but the best according to levenshtein. > > i.e. lucene does not use edit-distance as similarity. you need to get a bunch of best hits > in order to find the one with the smallest edit-distance. i took a look at the code, and my assessment seems to be right.",
        "label": 38
    },
    {
        "text": "documentation of the standardtokenizerinterface getnexttoken  return value documentation of getnexttoken() says that it returns an int, but incorrectly states that it returns the next token. getnexttoken doesn't return the next token, it returns the numerical type of the next token that was found, as defined by constants in the implementing class. that is also useful information that can be taken advantage of in the filtering chain that follows.",
        "label": 53
    },
    {
        "text": "specialize booleanquery if all clauses are termqueries during work on lucene-3319 i ran into issues with booleanquery compared to phrasequery in the exact case. if i disable scoring on phrasequery and bypass the position matching, essentially doing a conjunction match, exactphrasescorer beats plain boolean scorer by 40% which is a sizeable gain. i converted a conjunctionscorer to use docsenum directly but still didn't get all the 40% from phrasequery. yet, it turned out with further optimizations this gets very close to phrasequery. the biggest gain here came from converting the hand crafted loop in conjunctionscorer#donext to a for loop which seems to be less confusing to hotspot. in this particular case i think code specialization makes lots of sense since bq with tq is by far one of the most common queries. i will upload a patch shortly",
        "label": 46
    },
    {
        "text": "impossible to use custom norm encoding decoding although it is possible to override methods encodenorm and decodenorm in a custom similarity class, these methods are not actually used by the query processing and scoring functions, not by the indexing functions. the relevant lucene classes all call \"similarity.decodenorm\" rather than \"similarity.decodenorm\", i.e. the norm encoding/decoding is fixed to use that of the base similarity class. also index writing classes such as documentwriter use \"similarity.decodenorm\" rather than \"similarity.decodenorm\", so we are stuck with the 3 bit mantissa encoding implemented by smallfloat.floattobyte315 and smallfloat.byte315tofloat. this is very restrictive and annoying, since in practice many users would prefer an encoding that allows finer distinctions for boost and normalisation factors close to 1.0. for example. smallfloat.floattobyte52 uses 5 bits of mantissa, and this would be of great help in distinguishing much better between subtly different lengthnorms and fieldboost/documentboost values. it hsould be easy to fix this by changing all instances of \"similarity.decodenorm\" and \"similarity.encodenorm\" to \"similarity.decodenorm\" and \"similarity.encodenorm\" in the lucene code (there are only a few of each).",
        "label": 38
    },
    {
        "text": "intparser and floatparser unused by fieldcacheimpl fieldcacheimpl doesn't use intparser or floatparser to parse values",
        "label": 55
    },
    {
        "text": " ant eclipse  should create an eclipse project the \"eclipse\" ant target creates a .classpath file, but not a .project file, so the user has to create an eclipse project in a separate step. creating a .project file (if it doesn't exist yet) would make it easier for eclipse users to build lucene.",
        "label": 43
    },
    {
        "text": "morfologicfilter doesn't stem legitimate uppercase terms  surnames  proper nouns  etc  morfologic filter search stems in input or lowercase format: org.apache.lucene.analysis.morfologik.morfologikfilter.incrementtoken() if (!keywordattr.iskeyword() && (lookupsurfaceform(termatt) || lookupsurfaceform(tolowercase(termatt)))) {   [...] } in this situation, if input token is sienkiewicza - it isn't stemmed but: sienkiewicza --> sienkiewicz for comparison: produkty --> produkt it should stem also input token with capitalized first letter",
        "label": 11
    },
    {
        "text": "byteshash this issue will have the byteshash separated out from lucene-2186",
        "label": 46
    },
    {
        "text": "add bkddistancequery our bkd tree impl should be very fast at doing \"distance from lat/lon center point < x\" query. i haven't started this ... nicholas knize expressed interest in working on it.",
        "label": 33
    },
    {
        "text": "if setconfig config config  is called in resetinputs  you can turn term vectors off and on by round i want to be able to run one benchmark that tests things using term vectors and not using term vectors. currently this is not easy because you cannot specify term vectors per round. while you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests. if it doesn't affect anything else, it would be great to have setconfig(config config) called in basicdocmaker.resetinputs(). this would keep the term vector options up to date per round if you reset. mark",
        "label": 12
    },
    {
        "text": "new arabic analyzer  apache license  i've noticed there is no arabic analyzer for lucene, most likely because tim buckwalter's morphological dictionary is gpl. however, it is not necessary to have full morphological analysis engine for a quality arabic search. this implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf as you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision. while i personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is trec-tested, so the precision/recall improvements to lucene are already documented. for a stopword list, i used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as bsd-licensed. this implementation (analyzer) consists of above mentioned stopword list plus two filters: arabicnormalizationfilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc) arabicstemfilter: performs arabic light stemming both filters operate directly on termbuffer for maximum performance. there is no object creation in this analyzer. there are no external dependencies. i've indexed about half a billion words of arabic text and tested against that. if there are any issues with this implementation i am willing to fix them. i use lucene on a daily basis and would like to give something back. thanks.",
        "label": 15
    },
    {
        "text": "fix default charset sensitive method calls ",
        "label": 11
    },
    {
        "text": "add checks asserts if you search across a closed reader if you try to search across a closed reader (and/or searcher too), there are no checks, not even assertions statements. this results in crazy scary stacktraces deep inside places like fsts/various term dictionary implementations etc. in some situations, depending on codec, you wont even get an error (i'm sure its fun when you try to retrieve the stored fields!)",
        "label": 33
    },
    {
        "text": "edgengramtokenfilter drops payloads using an edgengramtokenfilter after a delimitedpayloadtokenfilter discards the payloads, where as most other filters copy the payload to the new tokens. i added a test for this issue and a possible fix at https://github.com/xabbu42/lucene-solr/tree/edgepayloads greetings nathan gass",
        "label": 53
    },
    {
        "text": "packaging is sometimes  tar gz  sometimes  tgz in lucene tar-gz format is file.tar.gz while in solr it is file.tgz. i think we would like them to be the same in the future?",
        "label": 40
    },
    {
        "text": "lsh filter i'm planning to implement lsh. which support query like this find similar documents that have 0.8 or higher similar score with a given document. similarity measurement can be cosine, jaccard, euclid.. for example. given following corpus 1. solr is an open source search engine based on lucene 2. solr is an open source enterprise search engine based on lucene 3. solr is an popular open source enterprise search engine based on lucene 4. apache lucene is a high-performance, full-featured text search engine library written entirely in java we wanna find documents that have 0.6 score in jaccard measurement with this doc solr is an open source search engine it will return only docs 1,2 and 3 (morelikethis will also return doc 4)",
        "label": 50
    },
    {
        "text": "facet drilldown should return a constantscorequery drilldown is a helper class which the user can use to convert a facet value that a user selected into a query for performing drill-down or narrowing the results. the api has several static methods that create e.g. a term or query. rather than creating a query, it would make more sense to create a filter i think. in most cases, the clicked facets should not affect the scoring of documents. anyway, even if it turns out that it must return a query (which i doubt), we should at least modify the impl to return a constantscorequery.",
        "label": 43
    },
    {
        "text": "deprecated api called in o a l store directories just ran into niofsdirectory and others still call getfile instead of getdirectory",
        "label": 33
    },
    {
        "text": "geopolygon factory fails in recognize convex polygon when a polygon contains three consecutive points which are nearly co-planar, the polygon factory may fail to recognize the concavity/convexity of the polygon. i think the problem is the way the sideness for a polygon edge is calculated. it relies in the position of the next point in respect of the previous polygon edge which fails on the case explained above because of numerical imprecision. the result is that sideness is messed up.",
        "label": 25
    },
    {
        "text": "minor nitpick terminforeader bug   some code flagged by a bytecode static analyzer - i guess a nitpick, but we should just drop the null check in the if? if its null it will fall to the below code and then throw a nullpointer exception anyway. keeping the nullpointer check implies we expect its possible - but then we don't handle it correctly.   /** returns the nth term in the set. */   final term get(int position) throws ioexception {     if (size == 0) return null;     segmenttermenum enumerator = getthreadresources().termenum;     if (enumerator != null && enumerator.term() != null &&         position >= enumerator.position &&  position < (enumerator.position + totalindexinterval))       return scanenum(enumerator, position);      // can avoid seek     seekenum(enumerator, position/totalindexinterval); // must seek     return scanenum(enumerator, position);   }",
        "label": 32
    },
    {
        "text": "changes html not explicitly included in release none of the release related ant targets explicitly call cahnges-to-html ... this seems like an oversight. (currently it's only called as part of the nightly target)",
        "label": 18
    },
    {
        "text": "explore streaming viterbi search in kuromoji i've been playing with the idea of changing the kuromoji viterbi search to be 2 passes (intersect, backtrace) instead of 4 passes (break into sentences, intersect, score, backtrace)... this is very much a work in progress, so i'm just getting my current state up. it's got tons of nocommits, doesn't properly handle the user dict nor extended modes yet, etc. one thing i'm playing with is to add a double backtrace for the long compound tokens, ie, instead of penalizing these tokens so that shorter tokens are picked, leave the scores unchanged but on backtrace take that penalty and use it as a threshold for a 2nd best segmentation...",
        "label": 8
    },
    {
        "text": "fix segmentinfo attributes when updates are involved today, segmentinfo.attributes are write-once. however, in the presence of field updates (see lucene-5189 and lucene-5215) this creates an issue, in which if a codec decides to alter the attributes when updates are applied, they are silently discarded. this is rather a corner case, though one that should be addressed. there were two solutions to address this: record si.attributes in segmentinfos, so they are written per-commit, instead of the .si file. remove them altogether, as they don't seem to be used anywhere in lucene code today. if we remove them, we basically don't take away special capability from codecs, because they can still write the attributes to a separate file, or even the file they record the other data in. this will work even with updates, as long as codecs respect the given segmentsuffix. if we keep them, i think the simplest solution is to read/write them by segmentinfos. but if we don't see a good use case, i suggest we remove them, as it's just extra code to maintain. i think we can even risk a backwards break and remove them completely from 4x, though if that's a problem, we can deprecate too. if anyone sees a good usage for them, or better - already uses them, please speak up, so we can make the proper decision.",
        "label": 43
    },
    {
        "text": "xmlparser drops user boosting the lucene xml parser seems to convert user defined boosting back to default 1.0 and thus boosting value is dropped from the query... e.g. <booleanquery>  <clause occurs=\"must\">   <booleanquery>    <clause occurs=\"should\">     <userquery fieldname=\"vehicle.colour\">red^66 blue~^8</userquery>    </clause>   </booleanquery>  </clause>  <clause occurs=\"should\">   <booleanquery>    <clause occurs=\"should\">     <userquery fieldname=\"vehicle.colour\">black^0.01</userquery>    </clause>   </booleanquery>  </clause> </booleanquery> produces a lucene query: +( ( vehicle.colour:red^66 vehicle.colour:blue~0.5^8 ) ) ( vehicle.colour:black ) the expected query : +( ( vehicle.colour:red^66 vehicle.colour:blue~0.5^8 ) ) ( vehicle.colour:black^0.01 ) i have developed a work around by modifying line 77 of userinputquerybuilder.java from: q.setboost(domutils.getattribute(e,\"boost\",1.0f)); to: q.setboost( domutils.getattribute( e, \"boost\", q.getboost() ) );",
        "label": 53
    },
    {
        "text": "prepare release fails if run from source package while checking 3.2 rc2 artifacts, i ran \"ant prepare-release\" and it failed because \"get-svn-info\" failed (since this is not a svn checkout). this makes sense indeed, but still annoying, so question is how to overcome it? it attempts to execute \"svn info\" as part of \"package-tgz-src\". when you run the latter to prepare the release, you want it to fail if \"svn\" does not exist, however in the source package there's no '.svn' so it cannot succeed. on the other hand, nobody should \"prepare a release\" from the source package, so perhaps the failure is ok, and we should disable the ant target? i don't know how to solve it yet, so i'm marking it as a blocker for 3.3, so that we at least revisit it before then.",
        "label": 40
    },
    {
        "text": "change latlonshape encoding to use bytes per dimension latlonshape tessellated triangles currently use a relatively naive encoding with the first four dimensions as the bounding box of the triangle and the last three dimensions as the vertices of the triangle. to encode the x,y vertices in the last three dimensions requires bytesperdim to be set to 8, with 4 bytes for the x & y axis, respectively. we can reduce bytesperdim to 4 by encoding the index(es) of the vertices shared by the bounding box along with the orientation of the triangle. this also opens the door for supporting contains queries.",
        "label": 19
    },
    {
        "text": "indexwriter unlock does does nothing if nativefslockfactory is used if nativefslockfactory is used, indexwriter.unlock will return, silently doing nothing. the reason is that nativefslockfactory's makelock always creates a new nativefslock. nativefslock's release first checks if its lock is not null. however, only if obtain() is called, that lock is not null. so release actually does nothing, and so indexwriter.unlock does not delete the lock, or fail w/ exception. this is only a problem in nativefslock, and not in other lock implementations, at least as i was able to see. need to think first how to reproduce in a test, and then fix it. i'll work on it.",
        "label": 53
    },
    {
        "text": "nullpointerexception from highlighter getbestfragment  when testing against the 5.1 nightly snapshots i've come across a nullpointerexception in highlighting when nothing would be highlighted. this does not happen with 5.0. java.lang.nullpointerexception  at __randomizedtesting.seedinfo.seed([3edc6eb0fa552b34:9971866e394f5fd0]:0)  at org.apache.lucene.search.highlight.weightedspantermextractor.extractweightedspanterms(weightedspantermextractor.java:311)  at org.apache.lucene.search.highlight.weightedspantermextractor.extract(weightedspantermextractor.java:151)  at org.apache.lucene.search.highlight.weightedspantermextractor.getweightedspanterms(weightedspantermextractor.java:515)  at org.apache.lucene.search.highlight.queryscorer.initextractor(queryscorer.java:219)  at org.apache.lucene.search.highlight.queryscorer.init(queryscorer.java:187)  at org.apache.lucene.search.highlight.highlighter.getbesttextfragments(highlighter.java:196)  at org.apache.lucene.search.highlight.highlighter.getbestfragments(highlighter.java:156)  at org.apache.lucene.search.highlight.highlighter.getbestfragment(highlighter.java:102)  at org.apache.lucene.search.highlight.highlighter.getbestfragment(highlighter.java:80)  at org.apache.lucene.search.highlight.missestest.testphrasequery(missestest.java:50) i've written a small unit test and used git bisect to narrow the regression to the following commit: commit 24e4eefaefb1837d1d4fa35f7669c2b264f872ac author: michael mccandless <mikemccand@apache.org> date:   tue mar 31 08:48:28 2015 +0000     lucene-6308: cutover spans to disi, reuse conjunctiondisi, use two-phased iteration          git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/branch_5x@1670273 13f79535-47bb-0310-9956-ffa450edef68 the problem looks quite simple, weightedspantermextractor.extractweightedspanterms() needs an early return if spanquery.getspans() returns null. all other callers check against this. unit test and fix (against the regressed commit) attached.",
        "label": 40
    },
    {
        "text": "improve termsenum automaton filtering apis to filter a termsenum by a compiledautomaton, we currently have a number of different possibilities: terms.intersect(compiledautomaton, bytesref) - efficient, but only works on normal type automata compiledautomaton.getterms(terms) - efficient, works on all automaton types, but requires a terms instead of a termsenum, so no use for eg sorteddocvalues.termsenum() automatontermsenum - takes a termsenum, so it's more general than the terms methods above, but agian only works on normal automata it's easy to do the wrong thing here, and at the moment we only guard against incorrect usage via runtime checks (see eg lucene-7576, https://github.com/flaxsearch/marple/issues/24). we should try and clean this up.",
        "label": 2
    },
    {
        "text": "bogus todo inside testbackwardscompatibility in trunk in testbackwardscompatibility in trunk we have:       // todo: these are on 4x, but something is wrong (they seem to be a too old dv format): i put a nocommit above it, to draw as much attention as possible to this. what does this mean, that we aren't testing backwards compatibilty? are tests disabled? i don't see how this is possibly a valid todo.",
        "label": 33
    },
    {
        "text": "random test failure org apache lucene testexternalcodecs testperfieldcodec  from testexternalcodecs  error message state.ord=54 startord=0 ir.isindexterm=true state.docfreq=1 stacktrace junit.framework.assertionfailederror: state.ord=54 startord=0 ir.isindexterm=true state.docfreq=1 at org.apache.lucene.index.codecs.standard.standardtermsdictreader$fieldreader$segmenttermsenum.seek(standardtermsdictreader.java:395) at org.apache.lucene.index.documentswriter.applydeletes(documentswriter.java:1099) at org.apache.lucene.index.documentswriter.applydeletes(documentswriter.java:1028) at org.apache.lucene.index.indexwriter.applydeletes(indexwriter.java:4213) at org.apache.lucene.index.indexwriter.doflushinternal(indexwriter.java:3381) at org.apache.lucene.index.indexwriter.doflush(indexwriter.java:3221) at org.apache.lucene.index.indexwriter.flush(indexwriter.java:3211) at org.apache.lucene.index.indexwriter.optimize(indexwriter.java:2345) at org.apache.lucene.index.indexwriter.optimize(indexwriter.java:2323) at org.apache.lucene.index.indexwriter.optimize(indexwriter.java:2293) at org.apache.lucene.testexternalcodecs.testperfieldcodec(testexternalcodecs.java:645) at org.apache.lucene.util.lucenetestcase.runbare(lucenetestcase.java:381) at org.apache.lucene.util.lucenetestcase.run(lucenetestcase.java:373) standard output note: random codec of testcase 'testperfieldcodec' was: mockfixedintblock(blocksize=1327) note: random locale of testcase 'testperfieldcodec' was: lt_lt note: random timezone of testcase 'testperfieldcodec' was: africa/lusaka note: random seed of testcase 'testperfieldcodec' was: 812019387131615618",
        "label": 46
    },
    {
        "text": "multilingual analyzer based on icu the standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts. this is because it is unaware of unicode bounds properties. i actually couldn't figure out how the thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. while most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). i've got a partially tested standardanalyzer that uses icu rule-based breakiterator instead of jflex. using this method you can define word boundaries according to the unicode bounds properties. after getting it into some good shape i'd be happy to contribute it for contrib but i wonder if theres a better solution so that out of box lucene will be more friendly to non-ascii text. unfortunately it seems jflex does not support use of these properties such as [\\p {word_break = extend} ] so this is probably the major barrier. thanks, robert",
        "label": 40
    },
    {
        "text": "testbalancedsegmentmergepolicy setup hang http://85.25.120.39/job/lucene-trunk-linux-java7-64 was hung for a couple days ... stacktrace: 2012-07-28 13:45:03 full thread dump java hotspot(tm) 64-bit server vm (23.1-b03 mixed mode): \"test-testscope-org.apache.lucene.index.testbalancedsegmentmergepolicy.testforcemerge-seed#[f42ffcd5faee14ff]\" prio=10 tid=0x00007f56e8371800 nid=0x106c runnable [0x00007f56b2130000]    java.lang.thread.state: runnable  at org.apache.lucene.codecs.blocktreetermswriter$termswriter$findblocks.freeze(blocktreetermswriter.java:392)  at org.apache.lucene.util.fst.builder.freezetail(builder.java:212)  at org.apache.lucene.util.fst.builder.add(builder.java:392)  at org.apache.lucene.codecs.blocktreetermswriter$termswriter.finishterm(blocktreetermswriter.java:864)  at org.apache.lucene.codecs.termsconsumer.merge(termsconsumer.java:167)  at org.apache.lucene.codecs.fieldsconsumer.merge(fieldsconsumer.java:65)  at org.apache.lucene.index.segmentmerger.mergeterms(segmentmerger.java:323)  at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:110)  at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:3539)  at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3154)  at org.apache.lucene.index.serialmergescheduler.merge(serialmergescheduler.java:36)  - locked <0x00000000dc0fcfb8> (a org.apache.lucene.index.serialmergescheduler)  at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1727)  at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1721)  at org.apache.lucene.index.indexwriter.preparecommit(indexwriter.java:2612)  - locked <0x00000000dc0fcfc8> (a java.lang.object)  at org.apache.lucene.index.indexwriter.commitinternal(indexwriter.java:2699)  - locked <0x00000000dc0fcfc8> (a java.lang.object)  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2679)  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2663)  at org.apache.lucene.index.randomindexwriter.commit(randomindexwriter.java:346)  at org.apache.lucene.index.testbalancedsegmentmergepolicy.setup(testbalancedsegmentmergepolicy.java:55)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:601)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1995)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$1100(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:875)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:891)  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)  at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)  at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:825)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:671)  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:697)  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:736)  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:747)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)  at org.apache.lucene.util.testruleicuhack$1.evaluate(testruleicuhack.java:51)  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)  at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)  at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551) \"service thread\" daemon prio=10 tid=0x00007f56e81be800 nid=0x1062 runnable [0x0000000000000000]    java.lang.thread.state: runnable \"c2 compilerthread1\" daemon prio=10 tid=0x00007f56e81bc000 nid=0x1060 waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable \"c2 compilerthread0\" daemon prio=10 tid=0x00007f56e81b9800 nid=0x105f waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable \"signal dispatcher\" daemon prio=10 tid=0x00007f56e81b7800 nid=0x105e waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable \"surrogate locker thread (concurrent gc)\" daemon prio=10 tid=0x00007f56e81b5800 nid=0x105c waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable \"finalizer\" daemon prio=10 tid=0x00007f56e8167000 nid=0x1057 in object.wait() [0x00007f56b3127000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000dd21e078> (a java.lang.ref.referencequeue$lock)  at java.lang.ref.referencequeue.remove(referencequeue.java:135)  - locked <0x00000000dd21e078> (a java.lang.ref.referencequeue$lock)  at java.lang.ref.referencequeue.remove(referencequeue.java:151)  at java.lang.ref.finalizer$finalizerthread.run(finalizer.java:177) \"reference handler\" daemon prio=10 tid=0x00007f56e8164800 nid=0x1056 in object.wait() [0x00007f56b3228000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000dd21e068> (a java.lang.ref.reference$lock)  at java.lang.object.wait(object.java:503)  at java.lang.ref.reference$referencehandler.run(reference.java:133)  - locked <0x00000000dd21e068> (a java.lang.ref.reference$lock) \"main\" prio=10 tid=0x00007f56e800c000 nid=0x1028 in object.wait() [0x00007f56edd14000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000dc0f4528> (a com.carrotsearch.randomizedtesting.randomizedrunner$2)  at java.lang.thread.join(thread.java:1258)  - locked <0x00000000dc0f4528> (a com.carrotsearch.randomizedtesting.randomizedrunner$2)  at java.lang.thread.join(thread.java:1332)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:561)  at com.carrotsearch.randomizedtesting.randomizedrunner.run(randomizedrunner.java:521)  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.execute(slavemain.java:153)  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.main(slavemain.java:246)  at com.carrotsearch.ant.tasks.junit4.slave.slavemainsafe.main(slavemainsafe.java:12) \"vm thread\" prio=10 tid=0x00007f",
        "label": 33
    },
    {
        "text": "the token types of the standard tokenizer is not accessible the standardtokenizerimpl not being public, these token types are not accessible : public static final int alphanum          = 0; public static final int apostrophe        = 1; public static final int acronym           = 2; public static final int company           = 3; public static final int email             = 4; public static final int host              = 5; public static final int num               = 6; public static final int cj                = 7; /**  * @deprecated this solves a bug where hosts that end with '.' are identified  *             as acronyms. it is deprecated and will be removed in the next  *             release.  */ public static final int acronym_dep       = 8; public static final string [] token_types = new string [] {     \"<alphanum>\",     \"<apostrophe>\",     \"<acronym>\",     \"<company>\",     \"<email>\",     \"<host>\",     \"<num>\",     \"<cj>\",     \"<acronym_dep>\" }; so no custom tokenfilter can be based of the token type. actually even the standardfilter cannot be writen outside the org.apache.lucene.analysis.standard package.",
        "label": 33
    },
    {
        "text": "directoryreader ignores nrt segmentinfos in  isoptimized  directoryreader only takes shared (with iw) segmentinfos into account in directoryreader#isoptimized(). this can return true even if the actual realtime reader sees more than one segments. public boolean isoptimized() {     ensureopen();    // if segmentsinfos changes in iw this can return false positive     return segmentinfos.size() == 1 && !hasdeletions();   } directoryreader should check if this reader has a non-nul segmentinfosstart and use that instead",
        "label": 46
    },
    {
        "text": "assertion error in termsinfowriter the test passes on my local machine but fails on machine in our lab. both are using jdk 6 and ubuntu 8.04. another interesting difference is that the directory that my input files are read from is an nfs share, but the index is written to a local disk. the indexing process is single-threaded. this test completes successfully with lucene 2.2.0 but fails with 2.3.2 (pulled from the maven2 repository). indexenron(bob.indexingtest) time elapsed: 41.285 sec <<< failure! java.lang.assertionerror: terms are out of order: field=b_contentid (number 7) lastfield=b_contentid (number 7) text=sha_256 lasttext=sha_256 at org.apache.lucene.index.terminfoswriter.add(terminfoswriter.java:154) at org.apache.lucene.index.documentswriter.appendpostings(documentswriter.java:2320) at org.apache.lucene.index.documentswriter.writesegment(documentswriter.java:2015) at org.apache.lucene.index.documentswriter.flush(documentswriter.java:552) at org.apache.lucene.index.indexwriter.doflush(indexwriter.java:2623) at org.apache.lucene.index.indexwriter.flush(indexwriter.java:2523) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1484) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1442)",
        "label": 33
    },
    {
        "text": "trecdocmaker skips over documents when  date  is missing from documents trecdocmaker skips over trec documents if they do not have a \"date\" line. when such a document is encountered, the code may skip over several documents until the next tag that is searched for is found. the result is, instead of reading ~25m documents from the gov2 collection, the code reads only ~23m (don't remember the actual numbers). the fix adds a terminatingtag to read() such that the code looks for prefix, but only until terminatingtag is found. appropriate changes were made in getnextdocdata(). patch to follow",
        "label": 33
    },
    {
        "text": "fuzzytermsenum creates tons of objects i ran into this problem in production using the directspellchecker. the number of objects created by the spellchecker shoot through the roof very very quickly. we ran about 130 queries and ended up with > 2m transitions / states. we spend 50% of the time in gc just because of transitions. other parts of the system behave just fine here. i talked quickly to robert and gave a poc a shot providing a levenshteinautomaton#torunautomaton(prefix, n) method to optimize this case and build a array based strucuture converted into utf-8 directly instead of going through the object based apis. this involved quite a bit of changes but they are all package private at this point. i have a patch that still has a fair set of nocommits but its shows that its possible and imo worth the trouble to make this really useable in production. all tests pass with the patch - its a start....",
        "label": 33
    },
    {
        "text": "fuzzy query scoring issues queries which automatically produce multiple terms (wildcard, range, prefix, fuzzy etc)currently suffer from two problems: 1) scores for matching documents are significantly smaller than term queries because of the volume of terms introduced (a match on query foo~ is 0.1 whereas a match on query foo is 1). 2) the rarer forms of expanded terms are favoured over those of more common forms because of the idf. when using fuzzy queries for example, rare mis- spellings typically appear in results before the more common correct spellings. i will attach a patch that corrects the issues identified above by 1) overriding similarity.coord to counteract the downplaying of scores introduced by expanding terms. 2) taking the idf factor of the most common form of expanded terms as the basis of scoring all other expanded terms.",
        "label": 28
    },
    {
        "text": "add segmentcachable interface following lucene-8017, i tried to add a getcachehelper(leafreadercontext) method to doublevaluessource so that weights that use dvs can delegate on. this ended up with the same method being added to longvaluessource, and some of the similar objects in spatial-extras. i think it makes sense to abstract this out into a separate segmentcachable interface.",
        "label": 2
    },
    {
        "text": "analyzinginfixsuggester should close its indexwriter by default at the end of build  from solr-6246, where analyzinginfixsuggester's write lock on its index is causing trouble when reloading a solr core: grant ingersoll wrote: one suggestion that might minimize the impact: close the writer after build varun thacker wrote: this is what i am thinking - create a lucene issue in which analyzinginfixsuggester#build closes the writer by default at the end. the add and update methods call ensureopen and those who do frequent real time updates directly via lucene won't see any slowdowns. michael mccandless - would this approach have any major drawback from lucene's perspective? else i can go ahead an tackle this in a lucene issue michael mccandless wrote: fixing analyzinginfixsuggester to close the writer at the end of build seems reasonable?",
        "label": 47
    },
    {
        "text": "testmockdirectorywrapper  raminputstream npe  the elasticsearch ci found a reproducing failure in test-framework: [junit4] suite: org.apache.lucene.store.testmockdirectorywrapper    [junit4]   2> note: reproduce with: ant test  -dtestcase=testmockdirectorywrapper -dtests.method=testseekpasteof -dtests.seed=8ca554881b469221 -dtests.slow=true -dtests.badapples=true -dtests.locale=ar -dtests.timezone=africa/niamey -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 0.05s | testmockdirectorywrapper.testseekpasteof <<<    [junit4]    > throwable #1: junit.framework.assertionfailederror: unexpected exception type, expected eofexception but got java.lang.nullpointerexception    [junit4]    >  at __randomizedtesting.seedinfo.seed([8ca554881b469221:6ce39f711bc3ecb0]:0)    [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2679)    [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2668)    [junit4]    >  at org.apache.lucene.store.basedirectorytestcase.testseekpasteof(basedirectorytestcase.java:524)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:566)    [junit4]    >  at java.base/java.lang.thread.run(thread.java:834)    [junit4]    >  suppressed: java.lang.runtimeexception: mockdirectorywrapper: cannot close: there are still 1 open files: {out=1}    [junit4]    >   at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:838)    [junit4]    >   at org.apache.lucene.store.basedirectorytestcase.testseekpasteof(basedirectorytestcase.java:527)    [junit4]    >   ... 35 more    [junit4]    >  caused by: java.lang.runtimeexception: unclosed indexinput: out    [junit4]    >   at org.apache.lucene.store.mockdirectorywrapper.addfilehandle(mockdirectorywrapper.java:730)    [junit4]    >   at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:773)    [junit4]    >   at org.apache.lucene.store.basedirectorytestcase.testseekpasteof(basedirectorytestcase.java:515)    [junit4]    >   ... 35 more    [junit4]    > caused by: java.lang.nullpointerexception    [junit4]    >  at org.apache.lucene.store.raminputstream.readbyte(raminputstream.java:75)    [junit4]    >  at org.apache.lucene.store.mockindexinputwrapper.readbyte(mockindexinputwrapper.java:140)    [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2674)    [junit4]    >  ... 37 more    [junit4]   2> note: leaving temporary files on disk at: /users/romseygeek/projects/lucene-solr-master/lucene/build/test-framework/test/j0/temp/lucene.store.testmockdirectorywrapper_8ca554881b469221-001    [junit4]   2> note: test params are: codec=asserting(lucene80): {}, docvalues:{}, maxpointsinleafnode=1937, maxmbsortinheap=7.245645580981918, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@64aabfe0), locale=ar, timezone=africa/niamey    [junit4]   2> note: mac os x 10.14.1 x86_64/oracle corporation 11 (64-bit)/cpus=8,threads=1,free=240218112,total=268435456    [junit4]   2> note: all tests run in this jvm: [testmockdirectorywrapper]    [junit4] completed [1/1 (1!)] in 0.42s, 1 test, 1 failure <<< failures!    [junit4]    [junit4]    [junit4] tests with failures [seed: 8ca554881b469221]:    [junit4]   - org.apache.lucene.store.testmockdirectorywrapper.testseekpasteof",
        "label": 11
    },
    {
        "text": "many versioned documents could should be in javadocs instead  looking at our forrested site, and trying to think about how we could move our versioned site away from it, i think as a first step we should look at what really needs to be there. i think it easily becomes out of date and we don't have good centralized documentation since stuff is split between javadocs and forrest. couldn't queryparsersyntax.xml simply be in the overview/package for the queryparser? we could just link to that page from the forrest docs menu, then we could link to the syntax from other places in the javadocs. furthermore, in that case we could link to other queryparser impls documentations (e.g. complexphrase) so it would probably be more useful. demo/demo2.xml could just be overview for the demo contrib? currently that one is useless: https://builds.apache.org/job/lucene-trunk/javadoc/demo/index.html scoring.xml could be added to the package documentation of search or similarities or somewhere that makes more sense? currently its \"almost javadocs\" already, except harder to validate none of these links are actually out of date: my best bet is a ton of them already are! i'll leave fileformats.xml aside for now, but with many different codec implementations its something to think about for later too.",
        "label": 53
    },
    {
        "text": "raminputstream and ramoutputstream without further buffering from java-dev, doug's reply of 12 sep 2005 on delaying buffer allocation in bufferedindexinput: paul elschot wrote: ... > i noticed that ramindexinput extends bufferedindexinput. > it has all data in buffers already, so why is there another > layer of buffering? no good reason: it's historical. to avoid this either: (a) the bufferedindexinput api would need to be modified to permit subclasses to supply the buffer; or (b) raminputstream could subclass indexinput directly, using its own buffers. the latter would probably be simpler. end of quote. i made version (b) of raminputstream. using this raminputstream, testtermvectorsreader failed as the only failing test.",
        "label": 32
    },
    {
        "text": "hanging on documentswriterstallcontrol waitifstalled forever in an environment where our underlying storage was timing out on various operations, we find all of our indexing threads eventually stuck in the following state (so far for 4 days): \"thread-0\" daemon prio=5 thread id=556 waiting at java.lang.object.wait(native method) at java.lang.object.wait(object.java:503) at org.apache.lucene.index.documentswriterstallcontrol.waitifstalled(documentswriterstallcontrol.java:74) at org.apache.lucene.index.documentswriterflushcontrol.waitifstalled(documentswriterflushcontrol.java:676) at org.apache.lucene.index.documentswriter.preupdate(documentswriter.java:301) at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:361) at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1484) at ... i have not yet enabled detail logging and tried to reproduce yet, but looking at the code, i see that dwfc.abortpendingflushes does try { dwpt.abort(); doafterflush(dwpt); } catch (throwable ex) { // ignore - keep on aborting the flush queue } (and the same for the blocked ones). since the throwable is ignored, i can't say for sure, but i've seen dwpt.abort thrown in other cases, so if it does throw, we'd fail to call doafterflush and properly decrement flushbytes. this can be a problem, right? is it possible to do this instead: try { dwpt.abort(); } catch (throwable ex) { // ignore - keep on aborting the flush queue } finally { try { doafterflush(dwpt); } catch (throwable ex2) { // ignore - keep on aborting the flush queue } } it's ugly but safer. otherwise, maybe at least add logging for the throwable just to make sure this is/isn't happening.",
        "label": 46
    },
    {
        "text": "remove simscorer computeslopfactor and simscorer computepayloadfactor this supersedes lucene-8013. we should hardcode computeslopfactor to 1/(n+1) in sloppyphrasescorer and move computepayloadfactor to payloadfunction so that all the payload scoring logic is in a single place.",
        "label": 2
    },
    {
        "text": "morfologikfilter shoudn't stem words marked as keyword i added \"agd\" as keyword using solr.keywordmarkerfilterfactory i would be able to add synonyms after solr.morfologikfilterfactory: agd => lod\u00f3wka, zamra\u017carka, ch\u0142odziarka, piekarnik, etc. it's not possible right now. all words (even keywords) are threated same way.",
        "label": 11
    },
    {
        "text": "customscorequery  function query  is broken  due to per segment searching  spinoff from here: http://lucene.markmail.org/message/psw2m3adzibaixbq with the cutover to per-segment searching, customscorequery is not really usable anymore, because the per-doc custom scoring method (customscore) receives a per-segment docid, yet there is no way to figure out which segment you are currently searching. i think to fix this we must also notify the subclass whenever a new segment is switched to. i think if we copy collector.setnextreader, that would be sufficient. it would by default do nothing in customscorequery, but a subclass could override.",
        "label": 53
    },
    {
        "text": "simplify and tidy cartesian tier code in spatial the cartesian tier filtering code in the spatial code can be simplified, tidied and generally improved. improvements include removing default field name support which isn't the responsibility of the code, adding javadoc, making method names more intuitive and trying to make the complex code in cartesianpolyfilterbuilder more understandable. few deprecations have to occur as part of this work, but some public methods in cartesianpolyfilterbuilder will be made private where possible so future improvements of this class can occur.",
        "label": 7
    },
    {
        "text": "refresh uax29urlemailtokenizer's tld list uax_url_email analyzer appears unable to recognize the \".local\" tld among others. bug can be reproduced by curl -xget \"address/index/_analyze?text=first%20last%20lname@section.mycorp.local&pretty&analyzer=uax_url_email\" will parse \"lname@section.my\" and \"corp.local\" as separate tokens, as opposed to curl -xget \"address/index/_analyze?text=first%20last%20lname@section.mycorp.org&pretty&analyzer=uax_url_email\" which will recognize \"lname@section.mycorp.org\". can this be fixed by updating to a newer version? i am running elasticsearch 0.90.5 and whatever lucene version sits underneath that. my suspicion is that the tld list the analyzer relies on (http://www.internic.net/zones/root.zone, i think?) is incomplete and needs updating.",
        "label": 47
    },
    {
        "text": "booleanweight should size the weights vector correctly the weights field on booleanweight uses a vector that will always be sized exactly the same as the outer class' clauses vector, therefore can be sized correctly in the constructor. this is a trivial memory saving enhancement.",
        "label": 38
    },
    {
        "text": "matchalldocsquery tostring string field  does not honor the javadoc contract should be public string tostring(string field){ return \":\"; } queryparser needs to be able to parse the string form of this query.",
        "label": 29
    },
    {
        "text": "geoexactcircle should create circles with right number of planes hi karl wright, there is still a situation when the test can fail. it happens when the planet model is a sphere and the radius is slightly lower than pi. the circle is created with two sectors but the circle plane is too big and the shape is bogus. i will attach a test and a proposed solution. (i hope this is the last issue of this saga)",
        "label": 25
    },
    {
        "text": "matchalldocsquery doesn't honor boost or querynorm matchalldocsquery doesn't pay attention to either it's own boost, or lucene's query normalization factor.",
        "label": 55
    },
    {
        "text": "contrib highlighter javadoc example needs to be updated the javadoc package.html example code is outdated, as it still uses queryparser.parse. http://lucene.zones.apache.org:8080/hudson/job/lucene-nightly/javadoc/contrib-highlighter/index.html",
        "label": 15
    },
    {
        "text": "spatial recursiveprefixtreefilter has some bugs with indexing non point shapes recursiveprefixtreefilter has some bugs that can occur when searching indexed shapes. one bug is an unpositioned termsenum. it through an exception in testing; i'm not sure what its effects would be in production. the other couple bugs are hard to describe here but were rare to occur in extensive testing. the effects were probably a slim chance of matching an indexed shape near the query shape. and spatialprefixtree does not support an indexed shape that covers the entire globe. these bugs were discovered during development of tests for rptf lucene-4419 which i will submit shortly.",
        "label": 10
    },
    {
        "text": "support for index search large numeric field currently if an number is larger than long.max_value, we can't index/search that in lucene as a number. for example, ipv6 address is an 128 bit number, so we can't index that as a numeric field and do numeric range query etc. it would be good to support biginteger / bigdecimal i've tried use biginteger for ipv6 in elasticsearch and that works fine, but there are still lots of things to do https://github.com/elasticsearch/elasticsearch/pull/5758",
        "label": 53
    },
    {
        "text": "code cleanup   tokenstreamfromtermvector   attribute factory at the top of tokenstreamfromtermvector:  //this attribute factory uses less memory when capturestate() is called.   public static final attributefactory attribute_factory =       attributefactory.getstaticimplementation(           attributefactory.default_attribute_factory, packedtokenattributeimpl.class); this is the default if super() was called with no-args from the constructor, so i believe this can go away. cc david smiley",
        "label": 10
    },
    {
        "text": "cut over numeric docvalues to fixed straight bytes currently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant.",
        "label": 46
    },
    {
        "text": "testautoprefixkicksin reproducable oom i hit this one on branch_5x, r1671999    [junit4] suite: org.apache.lucene.search.testtermrangequery    [junit4]   2> note: reproduce with: ant test  -dtestcase=testtermrangequery -dtests.method=testautoprefixtermskickin -dtests.seed=885e97c071968a3e -dtests.locale=es_cr -dtests.timezone=europe/madrid -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   56.5s j2 | testtermrangequery.testautoprefixtermskickin <<<    [junit4]    > throwable #1: java.lang.outofmemoryerror: gc overhead limit exceeded    [junit4]    >  at __randomizedtesting.seedinfo.seed([885e97c071968a3e:ac4a30651871f6fa]:0)    [junit4]    >  at org.apache.lucene.codecs.blocktree.autoprefixtermswriter$prefixterm.tobytesref(autoprefixtermswriter.java:117)    [junit4]    >  at org.apache.lucene.codecs.blocktree.autoprefixtermswriter$prefixterm.<init>(autoprefixtermswriter.java:73)    [junit4]    >  at org.apache.lucene.codecs.blocktree.autoprefixtermswriter.saveprefix(autoprefixtermswriter.java:411)    [junit4]    >  at org.apache.lucene.codecs.blocktree.autoprefixtermswriter.saveprefixes(autoprefixtermswriter.java:381)    [junit4]    >  at org.apache.lucene.codecs.blocktree.autoprefixtermswriter.pushterm(autoprefixtermswriter.java:238)    [junit4]    >  at org.apache.lucene.codecs.blocktree.autoprefixtermswriter.<init>(autoprefixtermswriter.java:200)    [junit4]    >  at org.apache.lucene.codecs.blocktree.blocktreetermswriter.write(blocktreetermswriter.java:416)    [junit4]    >  at org.apache.lucene.codecs.perfield.perfieldpostingsformat$fieldswriter.write(perfieldpostingsformat.java:198)    [junit4]    >  at org.apache.lucene.index.freqproxtermswriter.flush(freqproxtermswriter.java:107)    [junit4]    >  at org.apache.lucene.index.defaultindexingchain.flush(defaultindexingchain.java:112)    [junit4]    >  at org.apache.lucene.index.documentswriterperthread.flush(documentswriterperthread.java:420)    [junit4]    >  at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:512)    [junit4]    >  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:624)    [junit4]    >  at org.apache.lucene.index.indexwriter.preparecommitinternal(indexwriter.java:2699)    [junit4]    >  at org.apache.lucene.index.indexwriter.commitinternal(indexwriter.java:2854)    [junit4]    >  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2821)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.maybecommit(randomindexwriter.java:164)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:155)    [junit4]    >  at org.apache.lucene.search.testtermrangequery.testautoprefixtermskickin(testtermrangequery.java:425)",
        "label": 33
    },
    {
        "text": "change indexsearcher multisegment searches to search each individual segment using a single hitcollector this issue changes how an indexsearcher searches over multiple segments. the current method of searching multiple segments is to use a multisegmentreader and treat all of the segments as one. this causes filters and fieldcaches to be keyed to the multireader and makes reopen expensive. if only a few segments change, the fieldcache is still loaded for all of them. this patch changes things by searching each individual segment one at a time, but sharing the hitcollector used across each segment. this allows fieldcaches and filters to be keyed on individual segmentreaders, making reopen much cheaper. fieldcache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. searching individual segments avoids this cost. the term/document statistics from the multireader are used to score results for each segment. when sorting, its more difficult to use a single hitcollector for each sub searcher. ordinals are not comparable across segments. to account for this, a new field sort enabled hitcollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). this topfieldcollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. this is done lazily. all and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. we were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway). introduces multireaderhitcollector - a hitcollector that can collect across multiple indexreaders. old hitcollectors are wrapped to support multiple indexreaders. topfieldcollector - a hitcollector that can compare values/ordinals across indexreaders and sort on fields. fieldvaluehitqueue - a priority queue that is part of the topfieldcollector implementation. fieldcomparator - a new comparator class that works across indexreaders. part of the topfieldcollector implementation. fieldcomparatorsource - new class to allow for custom comparators. alters indexsearcher uses a single hitcollector to collect hits against each individual segmentreader. all the other changes stem from this deprecates topfielddoccollector fieldsortedhitqueue",
        "label": 33
    },
    {
        "text": "add option for tests to not randomize codec this is particularly useful when creating the backcompat indexes, since it is a pain to figure out which codec you need to specify to avoid being randomized. something like -dtests.codec=default could simply bypass the randomization of the codec.",
        "label": 41
    },
    {
        "text": "using multisearcher and parallelmultisearcher can change the sort order  when using multiple sort criteria the first criterium that indicates a difference should be used. when a field does not exist for a given document, special rules apply. from what i see in the code, it is sorted as 0 for integer and float fields, and null strings are sorted before others. this works correctly in both lucene 1.4.3 and in trunk as long as you use a single indexsearcher (except perhaps in special cases, see other bug reports like lucene-374). however, in multisearcher and parallelmultisearcher, the results of the separate indexsearchers are merged and there an error occurs. the bug is located in fielddocsortedhitqueue. it can even be demonstrated by passing a single indexsearcher to a multisearcher. testcase and patch follow.",
        "label": 55
    },
    {
        "text": "killed jvm when first commit was running will generate a corrupted index 1. start a new indexwriterbuilder on an empty folder, add some documents to the index 2. call commit 3. when the segments_1 file with 0 byte was created, kill the jvm we will end with a corrupted index with an empty segments_1. we only have issue with the first commit crash. also, if you tried to open an indexsearcher on a new index. and the first commit on the index was not finished yet. then you will see exception like: =========================================================================== org.apache.lucene.index.indexnotfoundexception: no segments* file found in org.apache.lucene.store.mmapdirectory@c:\\tmp\\testdir lockfactory=org.apache.lucene.store.nativefslockfactory@6ee00df: files: [write.lock, _0.fdt, _0.fdx] at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:741) at org.apache.lucene.index.standarddirectoryreader.open(standarddirectoryreader.java:52) at org.apache.lucene.index.directoryreader.open(directoryreader.java:65) =========================================================================== so when a new index was created, we should first create an empty index. we should not wait for the commit/close call to create the segment file. if we had an empty index there. it won't leave a corrupted index when there were a power issue on the first commit. and a concurrent indexsearcher can access to the index(no match is better than exception).",
        "label": 33
    },
    {
        "text": "cannot read indexes thank you mike for the smoketester addition. it failed because 4.10.1 was not verifying backwards compatibility against 4.10 however, when i added the indexes (svn copied from branch-4x), the test fails.",
        "label": 41
    },
    {
        "text": "deprecate   remove dutchanalyzer setstemdictionary dutchanalyzer.setstemdictionary(file) prevents reuse of tokenstreams (and also uses a file which isn't ideal). it should be deprecated in 3x, removed in trunk.",
        "label": 7
    },
    {
        "text": "instantiatedindexreader does not handle  termdocs null  correct  alltermdocs  this patch contains core changes so someone else needs to commit it. due to the incompatible #termdocs(null) behaviour at least matchalldocsquery, fieldcacherangefilter and valuesourcequery fails using ii since 2.9. alltermdocs now has a superclass, abstractalltermdocs that also instantiatedalltermdocs extend. also: ii-tests made less plausable to pass on future incompatible changes to termdocs and termenum iitermdocs#skipto and #next mimics the behaviour of document posisioning from segmenttermdocs#dito when returning false ii now uses bitvector rather than sets for deleted documents",
        "label": 33
    },
    {
        "text": "testgrouping failure ant test -dtestcase=testgrouping -dtestmethod=testrandom -dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba fails with this on current trunk:     [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testgrouping -dtestmethod=testrandom -dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba     [junit] note: test params are: codec=randomcodecprovider: {id=mockrandom, content=mocksep, sort2=simpletext, groupend=pulsing(freqcutoff=3 minblocksize=65 maxblocksize=132), sort1=memory, group=memory}, sim=randomsimilarityprovider(querynorm=true,coord=false): {id=dfr i(f)l2, content=dfr bel3(800.0), sort2=dfr gl3(800.0), groupend=dfr g2, sort1=dfr gb3(800.0), group=lm jelinek-mercer(0.700000)}, locale=zh_tw, timezone=america/indiana/indianapolis     [junit] note: all tests run in this jvm:     [junit] [testgrouping]     [junit] note: linux 2.6.33.6-147.fc13.x86_64 amd64/sun microsystems inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800     [junit] ------------- ---------------- ---------------     [junit] testcase: testrandom(org.apache.lucene.search.grouping.testgrouping): failed     [junit] expected:<11> but was:<7>     [junit] junit.framework.assertionfailederror: expected:<11> but was:<7>     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:148)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:50)     [junit]  at org.apache.lucene.search.grouping.testgrouping.assertequals(testgrouping.java:980)     [junit]  at org.apache.lucene.search.grouping.testgrouping.testrandom(testgrouping.java:865)     [junit]  at org.apache.lucene.util.lucenetestcase$2$1.evaluate(lucenetestcase.java:611)     [junit]      [junit]  i dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes. index #1 has no deletions; index #2 has same docs, but organized into doc blocks by group, and has some deletions. in theory (i think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs. here is the explain output of the docs that sorted differently: #1: top hit in the \"has deletes doc-block\" index (id=239): explain: 2.394486 = (match) weight(content:real1 in 292) [dfrsimilarity], result of:  2.394486 = score(dfrsimilarity, doc=292, freq=1.0), computed from:    1.0 = termfreq=1    41.944084 = normalizationh3, computed from:      1.0 = tf      5.3102274 = avgfieldlength      2.56 = len    102.829 = basicmodelbe, computed from:      41.944084 = tfn      880.0 = numberofdocuments      239.0 = totaltermfreq    0.023286095 = aftereffectl, computed from:      41.944084 = tfn #2: hit in the \"no deletes normal index\" (id=229) id=229 explain=2.382285 = (match) weight(content:real1 in 225) [dfrsimilarity], result of:  2.382285 = score(dfrsimilarity, doc=225, freq=1.0), computed from:    1.0 = termfreq=1    41.765594 = normalizationh3, computed from:      1.0 = tf      5.3218827 = avgfieldlength      10.24 = len    101.879845 = basicmodelbe, computed from:      41.765594 = tfn      786.0 = numberofdocuments      215.0 = totaltermfreq    0.023383282 = aftereffectl, computed from:      41.765594 = tfn then i went and called explain on the \"no deletes normal index\" for the top doc (id=239): explain: 2.3822558 = (match) weight(content:real1 in 17) [dfrsimilarity], result of:  2.3822558 = score(dfrsimilarity, doc=17, freq=1.0), computed from:    1.0 = termfreq=1    42.165264 = normalizationh3, computed from:      1.0 = tf      5.3218827 = avgfieldlength      2.56 = len    102.8307 = basicmodelbe, computed from:      42.165264 = tfn      786.0 = numberofdocuments      215.0 = totaltermfreq    0.023166776 = aftereffectl, computed from:      42.165264 = tfn",
        "label": 40
    },
    {
        "text": "make accessible subenums in mappingmultidocsenum the #merge method of the postingsconsumer receives mappingmultidocsenum and mappingmultidocsandpositionsenum as postings enum. in certain case (with specific postings formats), the #merge method needs to be overwritten, and the underlying docsenums wrapped by the mappingmultidocsenum need to be accessed. the mappingmultidocsenum class should provide a method #getsubs similarly to multidocsenum class.",
        "label": 46
    },
    {
        "text": "benchmark cannot parse highlight vs vector highlight alg  but only on x  a new test (testperftasksparse.testparseexamples) was added in lucene-3768 that guarantees all .alg files in the conf/ directory can actually be parsed... but highlight-vs-vector-highlight.alg cannot be parsed on 3.x (numberformatexception), however it works fine on trunk... and the .alg is exactly the same in both cases.     [junit] ------------- standard error -----------------     [junit] java.lang.numberformatexception: for input string: \"maxfrags[3.0],fields[body]\"     [junit]  at sun.misc.floatingdecimal.readjavaformatstring(floatingdecimal.java:1222)     [junit]  at java.lang.float.parsefloat(float.java:422)     [junit]  at org.apache.lucene.benchmark.bytask.tasks.searchtravtask.setparams(searchtravtask.java:76)     [junit]  at org.apache.lucene.benchmark.bytask.tasks.searchtravretvectorhighlighttask.setparams(searchtravretvectorhighlighttask.java:124)     [junit]  at org.apache.lucene.benchmark.bytask.utils.algorithm.<init>(algorithm.java:112)     [junit]  at org.apache.lucene.benchmark.bytask.testperftasksparse.testparseexamples(testperftasksparse.java:132)",
        "label": 43
    },
    {
        "text": "replace deprecated termattribute by new chartermattribute after lucene-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the tokenstreams to the new chartermattribute. we should also think about adding a attributefactory that creates a subclass of chartermattributeimpl that returns collation keys in tobytesref() accessor. collationkeyfilter is then obsolete, instead you can simply convert every tokenstream to indexing only collationkeys by changing the attribute implementation.",
        "label": 53
    },
    {
        "text": "indexsplitter that divides by primary key term index splitter that divides by primary key term. the contrib multipassindexsplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id. i think this implementation is a fairly trivial change.",
        "label": 53
    },
    {
        "text": "in modules analysys icu  ant gennorm2 does not work command to run gennorm2 does not work at present. also, icupkg needs to be called to convert the binary file to big-endian. i will attach a patch.",
        "label": 40
    },
    {
        "text": "spanregexquery and spannearquery is not working with multisearcher multisearcher is using: queries[i] = searchables[i].rewrite(original); to rewrite query and then use combine to combine them. but spanregexquery's rewrite is different from others. after you call it on the same query, it always return the same rewritten queries. as a result, only search on the first indexsearcher work. all others are using the first indexsearcher's rewrite queries. so many terms are missing and return unexpected result. billow",
        "label": 29
    },
    {
        "text": "change contrib qp api that uses charsequence as string identifier there are some api methods on contrib queryparser that expects charsequence as identifier. this is wrong, since it may lead to incorrect or mislead behavior, as shown on lucene-2855. to avoid this problem, these apis will be changed and enforce the use of string instead of charsequence on version 4. this patch already deprecate the old api methods and add new substitute methods that uses only string.",
        "label": 0
    },
    {
        "text": "token of   returns in cjktokenizer   new testcjktokenizer the \"\" string returns as token in the boundary of two byte character and one byte character. there is no problem in cjkanalyzer. when cjktokenizer is used with the unit, it becomes a problem. (use it with solr etc.)",
        "label": 33
    },
    {
        "text": "move slowcompositereaderwrapper and uninverting package to solr sources spinoff from lucene-6766, where we fixed index-time sorting to have first class support in lucene's ore, and no longer use slowcompositereaderwrapper. this is a dangerous, long living class, that tries to pretend a set of n segments is actually just a single segment. it's a leaky abstraction, has poor performance, and puts undue pressure on the apis of new lucene features to try to keep up this illusion. with lucene-6766, finally all usage of this class (except for uninvertedreader tests, which should maybe also move out?) has been removed from lucene, so i think we should move it to solr. this may also lead to a solution for lucene-7086 since e.g. the class could tap into solr's schema to \"know\" how to handle points fields properly.",
        "label": 33
    },
    {
        "text": "normalizecharmap build creates utf32 keyed automaton and uses it with utf16 keys normalizecharmap#build method is inconsistent with later use in mappingcharfilter         final org.apache.lucene.util.fst.builder<charsref> builder = new org.apache.lucene.util.fst.builder<charsref>(fst.input_type.byte2, outputs);         final intsref scratch = new intsref();         for(map.entry<string,string> ent : pendingpairs.entryset()) {           builder.add(util.toutf32(ent.getkey(), scratch),                       new charsref(ent.getvalue())); (note byte2 vs. toutf32 later on).",
        "label": 33
    },
    {
        "text": "simplify hashcode equals for spanquery subclasses spin off from lucene-6308, see the comments there from around 23 march 2015.",
        "label": 1
    },
    {
        "text": "clarify the sort sortfield  constructor  i don't really know which version this affects, but i clarified the documentation of the sort(sortfield...) constructor to ease the understanding for new users. pull request: https://github.com/apache/lucene-solr/pull/20",
        "label": 33
    },
    {
        "text": "geopolygon factory still shows problems with coplanar points the attached patch contains two polygons that still shows problems with co-planar points. to better explain the issue i attached some images: 1) concave.jpg:  this is the polygon we want to build. note that we want to build the concave part, therefore the blue part is actually not part of the shape and the white part is the area cover by the shape. 2) concavewithtiling.jpg: the algorithm of the polygon factory tries to tile the polygon using convex polygons. in our case it creates the three colored polygons on the image. what it remains is a concave polygon. the problem with this polygon is that the right edge of the concave polygon contains co-planar points. these points cannot be merged into a single plane because they have different properties (internal edges or shape edges). because geoconvexpolygon and geoconcavepolygon cannot handle polygons with co-planar points, the polygon cannot be built. karl wright, is it possible to make this polygons support such an extreme case?",
        "label": 25
    },
    {
        "text": "fastvectorhighlighter  idf weighted terms for ordered fragments the fastvectorhighlighter uses for every term found in a fragment an equal weight, which causes a higher ranking for fragments with a high number of words or, in the worst case, a high number of very common words than fragments that contains all of the terms used in the original query. this patch provides ordered fragments with idf-weighted terms: total weight = total weight + idf for unique term per fragment * boost of query; the ranking-formula should be the same, or at least similar, to that one used in org.apache.lucene.search.highlight.querytermscorer. the patch is simple, but it works for us. some ideas: a better approach would be moving the whole fragments-scoring into a separate class. switch scoring via parameter exact phrases should be given a even better score, regardless if a phrase-query was executed or not edismax/dismax-parameters pf, ps and pf^boost should be observed and corresponding fragments should be ranked higher",
        "label": 26
    },
    {
        "text": "wfstcompletionlookup lookup  npe when empty fst the fst builder.finish() returns null when nothing is accepted, this then results in npe in lookup(), see patch for extra nullchecks",
        "label": 33
    },
    {
        "text": "move kuromoji to analysis ja and introduce japanese  naming lucene/solr 3.6 and 4.0 will get out-of-the-box japanese language support through kuromojianalyzer, kuromojitokenizer and various other filters. these filters currently live in org.apache.lucene.analysis.kuromoji. i'm proposing that we move kuromoji to a new japanese package org.apache.lucene.analysis.ja in line with how other languages are organized. as part of this, i also think we should rename kuromojianalyzer to japaneseanalyzer, etc. to further align naming to our conventions by making it very clear that these analyzers are for japanese. (as much as i like the name \"kuromoji\", i think \"japanese\" is more fitting.) a potential issue i see with this that i'd like to raise and get feedback on, is that end-users in japan and elsewhere who use lucene-gosen could have issues after an upgrade since lucene-gosen is in fact releasing its analyzers under the org.apache.lucene.analysis.ja namespace (and we'd have a name clash). i believe users should have the freedom to choose whichever japanese analyzer, filter, etc. they'd like to use, and i don't want to propose a name change that just creates unnecessary problems for users, but i think the naming proposed above is most fitting for a lucene/solr release.",
        "label": 8
    },
    {
        "text": "make the lucene jar an osgi bundle in order to use lucene in an osgi environment, some additional headers are needed in the manifest of the jar. as lucene has no dependency, it is pretty straight forward and it ill be easy to maintain i think.",
        "label": 42
    },
    {
        "text": "lucene benchmark  objective performance test for lucene we need an objective way to measure the performance of lucene, both indexing and querying, on a known corpus. this issue is intended to collect comments and patches implementing a suite of such benchmarking tests. regarding the corpus: one of the widely used and freely available corpora is the original reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20newsgroups/20news-18828.tar.gz. i propose to use this corpus as a base for benchmarks. the benchmarking suite could automatically retrieve it from known locations, and cache it locally.",
        "label": 15
    },
    {
        "text": "add verbose to lucenetestcase and lucenetestcasej4 component-build.xml allows to define tests.verbose as a system property when running tests. both lucenetestcase and lucenetestcasej4 don't read that property. it will be useful for overriding tests to access one place for this setting (i believe currently some tests do it on their own). then (as a separate issue) we can move all tests that don't check the parameter to only print if verbose is true. i will post a patch soon.",
        "label": 53
    },
    {
        "text": " memory leak  in termshashperfield memory tracking i am witnessing an apparent leak in the memory tracking used to determine when a flush is necessary. over time, this will result in every single document being flushed into its own segment as the memusage will remain above the configured buffer size, causing a flush to be triggered after every add/update. best i can figure, this is being caused by termshashperfield's tracking of memory usage for postingshash and/or postingsarray combined with multi-threaded feeding. i suspect that the termshashperfield's postingshash is growing in one thread, then, when a segment is flushed, a single, different thread will merge all termshashperfields in freqproxtermswriter and then call shrinkhash(). i suspect this call of shrinkhash() is seeing an old postingshash array, and subsequently not releasing all the memory that was allocated. if this is the case, i am also concerned that freqproxtermswriter will not write the correct terms into the index, although i have not confirmed that any indexing problem occurs as of yet. note: i am witnessing this growth in a test by subtracting the amount or memory allocated (but in a \"free\" state) by perdocallocator/byteblockallocator/charblocks/intblocks from documentswriter.memusage.get() in indexwriter.doafterflush() i will see this stay at a stable point for a while, then on some flushes, i will see this grow by a couple of bytes, and all subsequent flushes will never go back down the the previous state i will continue to investigate and post any additional findings",
        "label": 33
    },
    {
        "text": "documentstoredfieldvisitor binaryfield ignores offset and length this is no problem with simpletext and lucene40 since in their cases, offset is always 0 and length the length of the byte[] array, but it might break with custom codecs.",
        "label": 1
    },
    {
        "text": "httpreplicator uses a lot of cpu for large files the method responseinputstream of httpclientbase wraps an inputstream in order to close it when it is done reading. however, the wrapper only overwrites the single-byte read() method, every other method is delegated to its parent (java.io.inputstream). therefore, the more efficient read-methods like read(byte[] b) are all implemented by reading one byte after the other. in my test, it took 20 minutes to copy an index of 38 gb. with the provided small patch, this was reduced to less than 10 minutes.",
        "label": 43
    },
    {
        "text": "testnrtthreads hangs in nightly x builds maybe we have a problem, maybe its a bug in the test. but its strange that lately the 3.x nightlies have been hanging here.",
        "label": 33
    },
    {
        "text": "reorder arguments of field constructor to be more intuitive i think field should take (name, value, type) not (name, type, value) ? this seems more intuitive and consistent with previous releases take this change to some code i had for example: -    d1.add(new field(\"foo\", \"bar\", field.store.yes, field.index.analyzed)); +    d1.add(new field(\"foo\", textfield.type_stored, \"bar\")); i think it would be better if it was document.add(new field(\"foo\", \"bar\", textfield.type_stored));",
        "label": 7
    },
    {
        "text": "teststressnrt failures  reproducible  build server logs. reproduces on at least two machines.     [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=teststressnrt -dtestmethod=test -dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -dargs=\"-dfile.encoding=utf-8\"     [junit] note: test params are: codec=lucene3x, sim=randomsimilarityprovider(querynorm=true,coord=false): {}, locale=ro, timezone=etc/gmt+1     [junit] note: all tests run in this jvm:     [junit] [teststressnrt]     [junit] note: linux 3.0.0-16-generic amd64/sun microsystems inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200     [junit] ------------- ---------------- ---------------     [junit] testcase: test(org.apache.lucene.index.teststressnrt): caused an error     [junit] mockdirectorywrapper: cannot close: there are still open files: {_ng.cfs=8}     [junit] java.lang.runtimeexception: mockdirectorywrapper: cannot close: there are still open files: {_ng.cfs=8}     [junit]  at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:555)     [junit]  at org.apache.lucene.index.teststressnrt.test(teststressnrt.java:385)     [junit]  at org.apache.lucene.util.lucenetestcase$subclasssetupteardownrule$1.evaluate(lucenetestcase.java:743)     [junit]  at org.apache.lucene.util.lucenetestcase$internalsetupteardownrule$1.evaluate(lucenetestcase.java:639)     [junit]  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:22)     [junit]  at org.apache.lucene.util.lucenetestcase$testresultinterceptorrule$1.evaluate(lucenetestcase.java:538)     [junit]  at org.apache.lucene.util.lucenetestcase$rememberthreadrule$1.evaluate(lucenetestcase.java:600)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:164)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:57)     [junit]  at org.apache.lucene.util.storeclassnamerule$1.evaluate(storeclassnamerule.java:21)     [junit]  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:22)     [junit] caused by: java.lang.runtimeexception: unclosed indexinput: _ng.cfs     [junit]  at org.apache.lucene.store.mockdirectorywrapper.addfilehandle(mockdirectorywrapper.java:479)     [junit]  at org.apache.lucene.store.mockdirectorywrapper$1.openslice(mockdirectorywrapper.java:777)     [junit]  at org.apache.lucene.store.compoundfiledirectory.openinput(compoundfiledirectory.java:221)     [junit]  at org.apache.lucene.codecs.lucene3x.terminfosreader.<init>(terminfosreader.java:112)     [junit]  at org.apache.lucene.codecs.lucene3x.lucene3xfields.<init>(lucene3xfields.java:84)     [junit]  at org.apache.lucene.codecs.lucene3x.preflexrwpostingsformat$1.<init>(preflexrwpostingsformat.java:51)     [junit]  at org.apache.lucene.codecs.lucene3x.preflexrwpostingsformat.fieldsproducer(preflexrwpostingsformat.java:51)     [junit]  at org.apache.lucene.index.segmentcorereaders.<init>(segmentcorereaders.java:108)     [junit]  at org.apache.lucene.index.segmentreader.<init>(segmentreader.java:51)     [junit]  at org.apache.lucene.index.indexwriter$readersandlivedocs.getmergereader(indexwriter.java:521)     [junit]  at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:3587)     [junit]  at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3257)     [junit]  at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:382)     [junit]  at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:451)     [junit]      [junit]      [junit] test org.apache.lucene.index.teststressnrt failed",
        "label": 33
    },
    {
        "text": "more like this  necessary improvement to run mlt on multi field values  ",
        "label": 46
    },
    {
        "text": "isolatin1accentfilter discards position increments of filtered terms not sure if this is a bug, but looks like one to me...",
        "label": 55
    },
    {
        "text": "remove html verification from checkjavadocs py currently, the broken html verification in checkjavadocs.py has issues in some cases (see solr-6902). on looking further to fix it with the html.parser package instead, noticed that there is broken html verification already present (using html.parser!)in checkjavadoclinks.py anyway which takes care of validation, and probably jtidy does it as well, going by the output (haven't verified it). given this, the validation in checkjavadocs.py doesn't seem to add any further value, so here's a patch to just nuke it instead.",
        "label": 13
    },
    {
        "text": "break out storablefield from indexablefield in the field type branch we have strongly decoupled document/field/fieldtype impl from the indexer, by having only a narrow api (indexablefield) passed to indexwriter. this frees apps up use their own \"documents\" instead of the \"user-space\" impls we provide in oal.document. similarly, with lucene-3309, we've done the same thing on the doc/field retrieval side (from indexreader), with the storedfieldsvisitor. but, maybe we should break out storablefield from indexablefield, such that when you index a doc you provide two iterables \u2013 one for the indexablefields and one for the storablefields. either can be null. one downside is possible perf hit for fields that are both indexed & stored (ie, we visit them twice, lookup their name in a hash twice, etc.). but the upside is a cleaner separation of concerns in api....",
        "label": 37
    },
    {
        "text": "release smoker should fail when changes txt has a release section for a future release in the first 6.3.0 rc, solr's changes.txt had a section for 7.0.0. smoketestrelease.py should add a new check for future release sections and fail if any are found.",
        "label": 47
    },
    {
        "text": "spellchecker not working because of stale indexsearcher the spellchecker unit test did not work, because of a stale indexreader and indexsearcher instance after calling indexdictionary(dictionary).",
        "label": 38
    },
    {
        "text": "move mutablevalues to common module solr makes use of the mutablevalue* series of classes to improve performance of grouping by functionquery (i think). as such they are used in valuesource implementations. consequently we need to move these classes in order to move the valuesources. as yonik pointed out, these classes have use beyond just functionquerys and might be used by both solr and other modules. however i don't think they belong in lucene core, since they aren't really related to search functionality. therefore i think we should put them into a common module, which can serve as a dependency to solr and any module.",
        "label": 7
    },
    {
        "text": "ordinalscache should use reader getcorecachekey  i'm doing some facet performance tests, and i tried using the cachedordscountingfacetsaggregator to cache the decoded ords per doc x field ... but noticed it was generating way too many cache entries, because it's currently using the ndv instance as the cache key. ndv instances are thread-private so this results in way too many entries in the cache.",
        "label": 43
    },
    {
        "text": "remove query getsimilarity  spinoff of lucene-2854. see lucene-2828 and lucene-2854 for reference. in general, the similaritydelegator was problematic with regards to back-compat, and if queries want to score differently, trying to runtime subclass similarity only causes trouble. the reason we could not fix this in lucene-2854 is because: michael mccandless added a comment - 08/jan/11 01:53 pm bq. is it possible to remove this method query.getsimilarity also? i don't understand why we need this method! i would love to! but i think that's for another day... i looked into this and got stuck with boostingquery, which rewrites to an anon  subclass of bq overriding its getsimilarity in turn override its coord method.  rather twisted... if we can do this differently i think we could remove query.getsimilarity. here is the method in question: /** expert: returns the similarity implementation to be used for this query.  * subclasses may override this method to specify their own similarity  * implementation, perhaps one that delegates through that of the searcher.  * by default the searcher's similarity implementation is returned.*/ public similarity getsimilarity(indexsearcher searcher) {   return searcher.getsimilarity(); }",
        "label": 40
    },
    {
        "text": "optimizations for bufferedindexinput along the same lines as lucene-2816: the readvint/readvlong/readshort/readint/readlong are not optimal here since they defer to readbyte. for example this means checking the buffer's bounds per-byte in readvint instead of per-vint. its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)",
        "label": 40
    },
    {
        "text": "topdocscollector should have bounded generic  t extends scoredoc  topdocscollector was changed to be topdocscollector<t>. however it has methods which specifically assume the pq stores scoredoc. therefore, if someone extends it and defines a type which is not scoredoc, things will break. we shouldn't put <t> on topdocscollector at all, but rather change its ctor to protected topdocscollector(priorityqueue<? extends scoredoc> pq). topdocscollector should handle scoredoc types. if we do this, we'll need to change fieldvaluehitqueue's entry to extend scoredoc as well.",
        "label": 53
    },
    {
        "text": "add a link to the release archive it would be nice if the releases page contained a link to the release archive at http://archive.apache.org/dist/lucene/java/.",
        "label": 33
    },
    {
        "text": "lengthfilter and other tokenfilters that skip tokens ignore relative positionincrement see for reference: http://www.nabble.com/worddelimiterfilter%2blenghtfilter-results-in-termposition%3d%3d-1-td16306788.html and http://www.nabble.com/lucene---java-f24284.html it seems that lengthfilter (at least) could produce a stream in which the first token has a positionincrement of 0, which make checkindex and luke function \"reconstruct&edit\" to generate exception. should something be done to avoid this situation, or could the error be ignored (by allowing term with a position of -1, and relaxing checkindex checks?)",
        "label": 53
    },
    {
        "text": "default doc values format should optimize for iterator access in lucene-7407 we switched doc values consumption from random access api to an iterator api, but nothing was done there to improve the codec. we should do that here. at a bare minimum we should fix the existing very-sparse case to be a true iterator, and not wrapped with the silly legacy wrappers. i think we should also increase the threshold (currently 1%?) when we switch from dense to sparse encoding. this should fix lucene-7253, making merging of sparse doc values efficient (\"pay for what you use\"). i'm sure there are many other things to explore to let codecs \"take advantage\" of the fact that they no longer need to offer random access to doc values.",
        "label": 1
    },
    {
        "text": "massive code duplication in contrib analyzers   unifly the analyzer ctors due to the variouse tokenstream apis we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenstream. when you look at the code it appears to be almost identical if both are implemented in the same analyzer. each analyzer defnes the same inner class (savedstreams) which is unnecessary. in contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.",
        "label": 40
    },
    {
        "text": "add tostring  or getname  method to indexreader it would be very useful for debugging if indexreader either had a getname() method, or a tostring() implementation that would get a string identification for the reader. for segmentreader, this would return the same as getsegmentname() for directory readers, this would return the \"generation id\"? for multireader, this could return something like \"multi(sub reader name, sub reader name, sub reader name, ...) right now, i have to check instanceof for segmentreader, then call getsegmentname(), and for all other indexreader types, i would have to do something like get the indexcommit and get the generation off it (and this may throw unsupportedoperationexception, at which point i have would have to recursively walk sub readers and try again) i could work up a patch if others like this idea",
        "label": 33
    },
    {
        "text": "minor changes to simplehtmlformatter i'd like to make few minor changes to simplehtmlformatter. 1. define default_pre_tag and default_post_tag and use them in the default constructor. this will not trigger string lookups by the jvm whenever the highlighter is instantiated. 2. create the stringbuffer in highlightterm with the right number of characters from the beginning. even though stringbuffer's default constructor allocates 16 chars, which will probably be enough for most highlighted terms (pre + post tags are 7 chars, which leaves 9 chars for terms), i think it's better to allocate sb with the right # of chars in advance, to avoid char[] allocations in the middle.",
        "label": 33
    },
    {
        "text": "mappingcharfilter could be improved by switching to an fst  mappingcharfilter stores an overly complex tree-like structure for matching input patterns. the input is a union of fixed strings mapped to a set of fixed strings; an fst matcher would be ideal here and provide both memory and speed improvement i bet.",
        "label": 33
    },
    {
        "text": "update dictionary version for ukrainian analyzer to update morfologik dictionary version to 3.9.0 for ukrainian analyzer. there's 60k of new lemmas there along with some other improvements and fixes, particularly ukrainian town names have been synchronized with official standard.",
        "label": 11
    },
    {
        "text": "iw counts hard deletes as soft deletes in merges if a document is soft-deleted, then hard-deleted, iw, however, accounts that document as soft-deleted when wrapping readers for merges.",
        "label": 46
    },
    {
        "text": "allow defined width gaps in spannearquery spannearquery is not quite an exact spans replacement for phrasequery at the moment, because while you can ask for an overall slop in an ordered match, you can't specify exactly where the gaps should appear.",
        "label": 2
    },
    {
        "text": "joinutil support for numeric docvalues fields while polishing solr-6234 i found that joinutil can't join int dv fields at least. i plan to provide test/patch. it might be important, because solr's join can do that. please vote if you care!",
        "label": 35
    },
    {
        "text": "shinglefilter  don't output all filler shingles unigrams  also  convert from termattribute to chartermattribute when the input token stream to shinglefilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream. as a result, unigrams (if configured) and shingles can be filler-only. filler-only output tokens make no sense - these should be removed. also, because termattribute has been deprecated in favor of chartermattribute, the patch will also convert termattribute usages to chartermattribute in shinglefilter.",
        "label": 53
    },
    {
        "text": "use isbinary cached variable instead of instanceof in field field class can hold three types of values, see: abstractfield.java protected object fieldsdata = null; currently, mainly rtti (instanceof) is used to determine the type of the value stored in particular instance of the field, but for binary value we have mixed rtti and cached variable \"boolean isbinary\" this patch makes consistent use of cached variable isbinary. benefit: consistent usage of method to determine run-time type for binary case (reduces chance to get out of sync on cached variable). it should be slightly faster as well. thinking aloud: would it not make sense to maintain type with some integer/byte\"poor man's enum\" (interface with a couple of constants) code:java{ public static final interface type{ public static final byte boolean = 0; public static final byte string = 1; public static final byte reader = 2; .... } } and use that instead of isbinary + instanceof?",
        "label": 33
    },
    {
        "text": "allow qp subclasses to support wildcard queries with leading   it would be usefull for some users if the logic that prevents queryparser from creating wildcardqueries with leading wildcard characters (\"?\" or \"*\") be moved from the grammer into the base implimentation of getwildcardquery so that it may be overridden in subclasses without needing to modifiy the grammer directly.",
        "label": 38
    },
    {
        "text": "move contribs modules away from queryparser dependency some contribs and modules depend on the core queryparser just for simplicity in their tests. we should apply the same process as i did to the core tests, and move them away from using the queryparser where possible.",
        "label": 7
    },
    {
        "text": "parallelcompositereader does not always call closed listeners compositeparallelreader misses to call closed listeners when the reader which is provided at construction time does not wrap leaf readers directly, such as a multi reader over directory readers.",
        "label": 53
    },
    {
        "text": "make constant score rewrite the default for multi term queries for queries that expand to multiple terms (prefixquery, rangequery, fuzzyquery, wildcardquery), the default now is to rewrite to a booleanquery, which scales poorly, and can hit the dreaded toomanyclauses (ungraceful degradation). except for fuzzyquery (which we should fix with this issue), they all support setconstantscorerewrite, which scales better. in 3.0 we should make constantscorerewrite the default, and leave an option to turn it off. this is a spinoff from lucene-998.",
        "label": 53
    },
    {
        "text": "logmergepolicy findmergestoexpungedeletes need to get deletes from the segmentreader with lucene-1516, deletes are carried over in the segmentreaders which means implementations of mergepolicy.findmergestoexpungedeletes (such as logmergepolicy) need to obtain deletion info from the sr (instead of from the segmentinfo which won't have the information).",
        "label": 33
    },
    {
        "text": "constantscoreautorewrite rewrites prefix queryies that don't match anything before query weight is calculated constantscoreautorewrite rewrites prefix queryies that don't match anything before query weight is calculated. this dramatically changes the resulting score which is bad when comparing scores across different lucene indexes/shards/whatever.",
        "label": 53
    },
    {
        "text": "updated snowball package updated snowball contrib package new org.tartarus.snowball java package with patched snowballprogram to be abstract to avoid using reflection. introducing hungarian, turkish and romanian stemmers introducing constructor snowballfilter(snowballprogram) it is possible there have been some changes made to the some of there stemmer algorithms between this patch and the current svn trunk of lucene, an index might thus not be compatible with new stemmers! the api is backwards compatibile and the test pass.",
        "label": 24
    },
    {
        "text": "build txt instructions wrong for javacc the text in build.txt for javacc says to set the property to the bin directory in the javacc installation. it should actually be set to the javacc installation directory, the directory containing the bin directory. the comments common-build.xml correctly state this.",
        "label": 48
    },
    {
        "text": "make highfreqterms termstats class public it's not possible to use public methods in contrib/misc/... /highfreqterms from outside the package because the return type has package visibility. i propose to move termstats class to a separate file and make it public.",
        "label": 4
    },
    {
        "text": "another case of polygon tessellator going into an infinite loop related to lucene-8454, another case where tesselator never returns when processing a polygon.    ",
        "label": 19
    },
    {
        "text": "improve perf and memory of fieldinfos fieldinfo int  fieldinfos.fieldinfo(int) looks up a field by number and returns its fieldinfo. this method is called per-field-per-doc in things like stored fields and vectors readers. unfortunately, today this method is always backed by a treemap. in most cases a simple array is better, its faster and uses less memory. these changes made significant difference in stored fields checkindex time with my test index (had only 10 fields). maybe it helps merge as well.",
        "label": 33
    },
    {
        "text": "cut norms over to docvalues since ir is now fully r/o and norms are inside codecs we can cut over to use a idv impl for writing norms. lucene-3606 has some ideas about how this could be implemented",
        "label": 46
    },
    {
        "text": "allow to define custom chartokenizer using java lambdas method references as a followup from lucene-6874, i thought about how to generate custom chartokenizers wthout subclassing. i had this quite often and i was a bit annoyed, that you had to create a subclass every time. this issue is using the pattern like threadlocal or many collection methods in java 8: you have the (abstract) base class and you define a factory method named fromxxxpredicate (like threadlocal.withinitial(() -> value). public static chartokenizer fromtokencharpredicate(java.util.function.intpredicate predicate) this would allow to define a new chartokenizer with a single line statement using any predicate: // long variant with lambda: tokenizer tok = chartokenizer.fromtokencharpredicate(c -> !ucharacter.isuwhitespace(c)); // method reference for separator char predicate + normalization by uppercasing: tokenizer tok = chartokenizer.fromseparatorcharpredicate(ucharacter::isuwhitespace, character::touppercase); // method reference to custom function: private boolean mytestfunction(int c) {  return (cracy condition); } tokenizer tok = chartokenizer.fromtokencharpredicate(this::mytestfunction); i know this would not help solr users that want to define the tokenizer in a config file, but for real lucene users this java 8-like way would be easy and elegant to use. it is fast as hell, as it is just a reference to a method and java 8 is optimized for that. the inverted factories fromseparatorcharpredicate() are provided to allow quick definition without lambdas using method references. in lots of cases, like whitespacetokenizer, predicates are on the separator chars (iswhitespace(int), so using the 2nd set of factories you can define them without the counter-intuitive negation. internally it just uses predicate#negate(). the factories also allow to give the normalization function, e.g. to lowercase, you may just give character::tolowercase as intunaryoperator reference.",
        "label": 53
    },
    {
        "text": "track fieldinfo per segment instead of per iw session currently fieldinfo is tracked per iw session to guarantee consistent global field-naming / ordering. iw carries fi instances over from previous segments which also carries over field properties like isindexed etc. while having consistent field ordering per iw session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with lucene's codec support. codecs that rely on consistent properties in fi will fail if fi properties are carried over. the docvaluescodec (docvaluesbranch) for instance writes files per segment and field (using the field id within the file name). yet, if a segment has no docvalues indexed in a particular segment but a previous segment in the same iw session had docvalues, fieldinfo#docvalues will be true since those values are reused from previous segments. we already work around this \"limitation\" in segmentinfo with properties like hasvectors or hasprox which is really something we should manage per codec & segment. ideally fieldinfo would be managed per segment and codec such that its properties are valid per segment. it also seems to be necessary to bind fieldinfos to segmentinfo logically since its really just per segment metadata.",
        "label": 46
    },
    {
        "text": "oom erros with checkindex with indexes containg a lot of fields with norms all index readers have a cache of the last used norms (segmentreader, multireader, multisegmentreader,...). this cache is never cleaned up, so if you access norms of a field, the norm's byte[maxdoc()] array is not freed until you close/reopen the index. you can see this problem, if you create an index with many fields with norms (i tested with about 4,000 fields) and many documents (half a million). if you then call checkindex, that calls norms() for each field in the segment and each of this calls creates a new cache entry, you get outofmemoryexceptions after short time (i tested with the above index: i was not able to do a checkindex even with \"-xmx 16gb\" on 64bit java). checkindex opens and then tests each segment of a index with a separate segmentreader. the big index with the outofmemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. each byte[] array takes about a half mib for this index. the checkindex funtion created the norm for 4000 fields and the segmentreader cached them, which is about 2 gib ram. so ooms are not unusal. in my opinion, the best would be to use a weak- or better a softreference so norms.bytes gets java.lang.ref.softreference<byte[]> and used for caching. with proper synchronization (which is done on the norms cache in segmentreader) you can do the best with softreference, as this reference is garbage collected only when an oom may happen. if the byte[] array is freed (but it is only freed if no other references exist), a lter call to getnorms() creates a new array. when code is hard referencing the norms array, it will not be freed, so no problem. the same could be done for the other indexreaders. fields without norm() do not have this problem, as all these fields share a one-time allocated dummy norm array. so the same index without norms enabled for most of the fields checked perfectly. i will prepare a patch tomorrow. mike proposed another quick fix for checkindex: we could do something first specifically for checkindex (eg it could simply use the 3-arg non-caching bytes method instead) to prevent oom errors when using it.",
        "label": 33
    },
    {
        "text": "dictionarycompoundwordtokenfilter does not properly add tokens from the end compound word  due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream. for example (from the unit test in the attached patch): dictionary: {\"ab\", \"cd\", \"ef\"} input: \"abcdef\" created tokens: {\"abcdef\", \"ab\", \"cd\"} expected tokens: {\"abcdef\", \"ab\", \"cd\", \"ef\"} additionally, it could produce tokens that were shorter than the minsubwordsize due to another off-by-one error. for example (again, from the attached patch): dictionary: {\"abc\", \"d\", \"efg\"} minimum subword length: 2 input: \"abcdefg\" created tokens: {\"abcdef\", \"abc\", \"d\", \"efg\"} expected tokens: {\"abcdef\", \"abc\", \"efg\"}",
        "label": 40
    },
    {
        "text": "bulgarian analyzer someone asked about bulgarian analysis on solr-user today... http://www.lucidimagination.com/search/document/e1e7a5636edb1db2/non_english_languages i was surprised we did not have anything. this analyzer implements the algorithm specified here, http://members.unine.ch/jacques.savoy/papers/buir.pdf in the measurements there, this improves map approx 34%",
        "label": 40
    },
    {
        "text": "shinglefilter should have a way to specify filler token today we have no choice that if pos_inc is > 1 there will be a `_` inserted in between the tokens. we should have the ability to change this character and the char[] that holds it should not be public static since it's mutable.",
        "label": 47
    },
    {
        "text": "speed up automaton seeking in nextstring while testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking. nextstring doesn't handle this particularly well, when it walks the dfa, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely. alternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.",
        "label": 40
    },
    {
        "text": "change file format documentation from  bit for bit  to highlevel while reviewing website docs in lucene-2924, i noticed the the existing fileformats is going to be pretty hopeless for 4.0. before it described the format \"bit-for-bit\", but with flexible indexing this is somewhat silly (and who really wants a bit-for-bit explanation of some of the new formats!) i think it would be much better to give a high-level overview, perhaps linking to javadocs or even source code for the low-level details. we probably should delay this until 4.0 is really close in sight (since things are changing so fast) but we can go ahead and think about it some now. for example: high level explanation of what a codec is, and the various subsystems one is usually composed of (terms index, terms data, skiplist impl, postings impl, etc). we can reiterate that you can make your own, and hopefully this kind of documentation will actually encourage that. high level explanation of what standardcodec is \"composed of\". for example assume its variable terms index, block terms reader, pfordelta docs and freqs and simple64 positions. i think really this is the only codec we should try to \"diagram\" in any way. high level explanation (probably with links) of some of the components. for example we could explain what the purpose of a terms index is, and that this implementation uses a finite state transducer to find the terms block for a given term. in this case maybe we have an image now that dawid made the todot useful. high level explanation (probably with links) of some of the compression algorithms. for example, we could explain the basics of the available algorithms we have (vbyte/simple/for/pfor/...) and what their advantages and disadvantages are. some of the things i mentioned here are probably optional, for instance i think its \"enough\" to give a high-level overview of standardcodec, but i can't help but think that explaining some of the architecture will be useful for new developers.",
        "label": 40
    },
    {
        "text": "add wrapper class constructors to forbiddenapis wrapper classes for the java primitives (boolean, byte, short, character, integer, long, float, double) have constructors which will always create new objects. these constructors are officially deprecated as of java 9 and it is recommended to use the public static methods since these can reuse the same underlying objects. in 99% of cases we should be doing this, so these constructors should be added to forbiddenapis and code corrected to use autoboxing or call the static methods (.valueof, .parse*) explicitly.",
        "label": 53
    },
    {
        "text": "mockdirectorywrapper relies on hashset iteration order mdw relies on hashset iteration order in   public synchronized void corruptfiles(collection<string> files) throws ioexception {     // must make a copy because we change the incoming unsyncedfiles     // when we create temp files, delete, etc., below:     for(string name : new arraylist<>(files)) { // <<<<< this should be sorted       int damage = randomstate.nextint(6); this causes reproducibility issues when files get corrupted.",
        "label": 46
    },
    {
        "text": "raminputstream hits false eof if you seek to eof then seek back then readbytes testlazyloadthreadsafety fails in hudson, possibly an issue with ramdirectory. if you hack lucene testcase to return another directory, the same seed will pass.",
        "label": 33
    },
    {
        "text": "indexwriter should expose field names while working on solr-5944, i needed a way to know whether applying an update to a dv is possible (i.e. the dv exists or not), while deciding upon whether or not to apply the update as an in-place update or a regular full document update. this information is present at the indexwriter in a fieldinfos instance, and can be exposed.",
        "label": 20
    },
    {
        "text": "mocktokenizer throws away the character right after a token even if it is a valid start to a new token mocktokenizer throws away the character right after a token even if it is a valid start to a new token. you won't see this unless you build a tokenizer that can recognize every character like with new regexp(\".\") or regexp(\"...\"). changing this behaviour seems to break a number of tests.",
        "label": 40
    },
    {
        "text": "arabic analyzer  stopwords list needs enhancement the provided arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue.",
        "label": 40
    },
    {
        "text": "parsing hunspell affix rules without regexp condition we found out that some recent dutch hunspell dictionaries contain suffix or prefix rules like the following:   sfx na n 1 sfx na 0 ste the rule on the second line doesn't contain the 5th parameter, which should be the condition (a regexp usually). you can usually see a '.' as condition, meaning always (for every character). as explained in lucene-3976 the readaffix method throws error. i wonder if we should treat the missing value as a kind of default value, like '.'. on the other hand i haven't found any information about this within the spec. any thoughts?",
        "label": 7
    },
    {
        "text": "add 'accountable' interface for various rambytesused currently this is a disaster. there is rambytesused(), sizeinbytes(), etc etc everywhere, with zero consistency, little javadocs, and no structure. for example, look at lucene-5695, where we go back and forth on how to handle \"don't know\". i don't think we should add any more of these methods to any classes in lucene until this has been cleaned up.",
        "label": 1
    },
    {
        "text": "nativelock is release if lock is closed after obtain failed if you obtain the nativefslock and try to obtain it again in the same jvm and close if if it fails another process will be able to obtain it. this is pretty trappy though. if you execute the main class twice the problem becomes pretty obvious. import org.apache.lucene.store.lock; import org.apache.lucene.store.nativefslockfactory; import java.io.file; import java.io.ioexception; public class testlock {  public static void main(string[] foo) throws ioexception, interruptedexception {         nativefslockfactory lockfactory = new nativefslockfactory(new file(\"/tmp\"));         lock lock = lockfactory.makelock(\"lock\");         if (lock.obtain()) {             system.out.println(\"obtained\");         } else {             lock.close();             system.out.println(\"failed\");         }         // try it again and close it if it fails         lock = lockfactory.makelock(\"lock\"); // <<<<==== this is a new lock         if (lock.obtain()) {             system.out.println(\"obtained again\");         } else {             lock.close(); // <<<<==== this releases the lock we obtained             system.out.println(\"failed on second\");         }         thread.sleep(integer.max_value);     } }",
        "label": 46
    },
    {
        "text": "lucene benchmark has some unnecessary files lucene/contrib/benchmark/.rsync-filter is only in the source pack (and in svn), i was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/lucene-848?focusedcommentid=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404 not a blocker for this rc, just interesting to note. maybe this is related to lucene-3155 too, in that we could consider this one for automatic exclusion (like ds_store), but we should fix it if its committed in svn too.",
        "label": 40
    },
    {
        "text": "make nearspansordered use lazy iteration ",
        "label": 2
    },
    {
        "text": "automaton spellchecker the current spellchecker makes an n-gram index of your terms, and queries this for spellchecking. the terms that come back from the n-gram query are then re-ranked by an algorithm such as levenshtein. alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need a separate index to rebuild.",
        "label": 40
    },
    {
        "text": "eliminate unnecessary uses of hashtable and vector lucene uses vector, hashtable and enumeration when it doesn't need to. changing to arraylist and hashmap may provide better performance. there are a few places vector shows up in the api. imho, list should have been used for parameters and return values. there are a few distinct usages of these classes: internal but with arraylist or hashmap would do as well. these can simply be replaced. internal and synchronization is required. either leave as is or use a collections synchronization wrapper. as a parameter to a method where list or map would do as well. for contrib, just replace. for core, deprecate current and add new method signature. generated by javacc. (all *.jj files.) nothing to be done here. as a base class. not sure what to do here. (only applies to segmentinfos extends vector, but it is not used in a safe manner in all places. perhaps, implements list would be better.) as a return value from a package protected method, but synchronization is not used. change return type. as a return value to a final method. change to list or map. in using a vector the following iteration pattern is frequently used. for (int i = 0; i < v.size(); i++) { object o = v.elementat; } this is an indication that synchronization is unimportant. the list could change during iteration.",
        "label": 33
    },
    {
        "text": "disjunctionmaxquery   iterator code to for   a a   container   construct for better readability - converting the iterable<t> to for ( a a : container ) constructs that is more intuitive to read.",
        "label": 53
    },
    {
        "text": "testpointqueries times out on nightly nightlies have failed with a suite timeout on: -dtestcase=testpointqueries -dtests.method=testrandombinarybig  -dtests.seed=81db11c283a04f59 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.asserts=true -dtests.file.encoding=us-ascii this is a result of plain text codec being used and a large volume of repetitions. i'll disable plain text codec on that test.",
        "label": 11
    },
    {
        "text": "haversine  is broken   misdocumented distanceutils.haversine() is coded in a way that is erroneous based on the documented order of the parameters. the parameters are defined as (x1,y1,x2,y2,radius) \u2013 i.e. lon,lat order. the code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior. it turns out that all callers of this method do this! fyi i found out about this bug since it is inherited code in lsp (lucene-spatial-playground) and i have been supplying parameters according to its documented order. apparently i shouldn't do that",
        "label": 10
    },
    {
        "text": "queryparser makes all cjk queries phrase queries regardless of analyzer the queryparser automatically makes all cjk, thai, lao, myanmar, tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off. this completely breaks lucene for these languages, as it treats all queries like 'grep'. example: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of \"a b c d\". if you use cjk analyzer, its no better, its a phrasequery of \"ab bc cd\", and if you use smartchinese analyzer, you get a phrasequery like \"ab cd\". but the user didn't ask for one, and they cannot turn it off. the reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. if more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what. the proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. implementing subclasses can always extend the qp and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose qps should be language independent.",
        "label": 40
    },
    {
        "text": "testlatlonlineshapequeries testrandombig fails with suite timeout  simple text codec used  pr at: https://github.com/apache/lucene-solr/pull/536",
        "label": 11
    },
    {
        "text": "allow other string distance measures in spellchecker updated spelling code to allow for other string distance measures to be used. created stringdistance interface. modified existing levenshtein distance measure to implement interface (and renamed class). verified that change to levenshtein distance didn't impact runtime performance. implemented jaro/winkler distance metric modified spellchecker to take distacne measure as in constructor or in set method and to use interface when calling.",
        "label": 38
    },
    {
        "text": "spannearquery  ordered spans should not overlap while using span queries i think i've found a little bug. with a document like this (from the testnearspansordered unit test) : \"w1 w2 w3 w4 w5\" if i try to search for this span query : spannear([spannear([field:w3, field:w5], 1, true), field:w4], 0, true) the above document is returned and i think it should not because 'w4' is not after 'w5'. the 2 spans are not ordered, because there is an overlap. i will add a test patch in the testnearspansordered unit test. i will add a patch to solve this issue too. basicaly it modifies the two docspansordered functions to make sure that the spans does not overlap.",
        "label": 14
    },
    {
        "text": "give access to members of a composite shape hi karl wright, i hope this is my last point in my wish list. in order to serialize objects i need to access the members of a composite geoshape. this is currently not possible so i was wondering if it is possible to add to more methods to the class geocompositemembershipshape: public int size() public geomembershipshape getshape(int index) thanks, ignacio",
        "label": 25
    },
    {
        "text": "ramusagetester sizeof ignores arrays and collections if  illegal access deny we have a silent fall-through on catch clause there if a field is inaccessible. this causes all collections to not be recursively descended into. we could hack it so that collection objects are counted \"manually\". or we could propagate illegal access exception from ram tester, detect which tests/ objects do try to measure collection objects and either remove them (if it's not possible to measure accurately) or change them (if it is possible to estimate the size in a different way). looking for feedback on this one.",
        "label": 11
    },
    {
        "text": "improve lucenetestcase javadocs now that the lucene test-framework javadocs will be published, they should get some attention.",
        "label": 40
    },
    {
        "text": "implement standardtokenizer with the uax standard it would be really nice for standardtokenizer to adhere straight to the standard as much as we can with jflex. then its name would actually make sense. such a transition would involve renaming the old standardtokenizer to europeantokenizer, as its javadoc claims: this should be a good tokenizer for most european-language documents the new standardtokenizer could then say this should be a good tokenizer for most languages. all the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that europeantokenizer, and it could be used by the european analyzers.",
        "label": 47
    },
    {
        "text": "je directory implementation i've created a port of dbdirectory to je",
        "label": 3
    },
    {
        "text": "be more precise about iocontext for reads set the context as iocontext.read / iocontext.readonce where applicable motivation: custom postingsformat may want to check the context on segmentreadstate and branch differently, but for this to work properly the context has to be specified correctly up the stack. for example, directpostingsformat only loads postings into memory if the context != merge. however a better condition would be context == context.read && !context.readonce.",
        "label": 33
    },
    {
        "text": "dead code in spellchecker java  branch never executes  spellchecker contains the following lines of code: final int goalfreq = (morepopular && ir != null) ? ir.docfreq(new term(field, word)) : 0; // if the word exists in the real index and we don't care for word frequency, return the word itself if (!morepopular && goalfreq > 0) { return new string[] { word } ; } the branch will never execute: the only way for goalfreq to be greater than zero is if morepopular is true, but if morepopular is true, the expression in the if statement evaluates to false.",
        "label": 38
    },
    {
        "text": "balancedsegmentmergepolicy  contributed from the zoie project for realtime indexing written by yasuhiro matsuda for zoie realtime indexing system used to handle high update rates to avoid large segment merges. detailed write-up is at: http://code.google.com/p/zoie/wiki/zoiemergepolicy",
        "label": 33
    },
    {
        "text": "rename rangequery   termrangequery since we now have numericrangequery (lucene-1701) we should rename rangequery to textrangequery to make it clear that textrangequery (termrangequery? stringrangequery) is based entirely on text comparison. and, existing users on upgrading to 2.9 and using rangequery for [slow] numeric searching would realize they now have a good option for numeric range searching.",
        "label": 53
    },
    {
        "text": "htmlstripcharfilter produces invalid final offset nightly build found this... i boiled it down to a small test case that doesn't require the big line file docs.",
        "label": 47
    },
    {
        "text": "deleted nested docs are scored into parent doc  if a nested doc is deleted is still scored into the parent doc, which i think isn't right.",
        "label": 30
    },
    {
        "text": "the disjunctionmaxquery lacks an implementation of extractterms  the disjunctionmaxquery lacks an implementation of extractterms().",
        "label": 55
    },
    {
        "text": "fieldinfos retains garbage if non sparse a heap dump revealed a lot of treemap.entry instances (millions of them) for a system with about ~1000 active searchers.",
        "label": 1
    },
    {
        "text": "asciifoldingfilter that emits both unfolded and folded tokens i've found myself wanting an asciifoldingfilter that emits both the folded tokens and the original, unfolded tokens.",
        "label": 46
    },
    {
        "text": "fieldboostmapattribute in contrib qp is broken  while looking for more suppresswarnings in lucene, i came across two of them in contrib/queryparser. even worse, i found these revolved around using maps with charsequence as key. from the javadocs for charsequence: this interface does not refine the general contracts of the equals and hashcode methods. the result of comparing two objects that implement charsequence is therefore, in general, undefined. each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. it is therefore inappropriate to use arbitrary charsequence instances as elements in a set or as keys in a map.",
        "label": 40
    },
    {
        "text": "mmapdirectory can't create new index on windows when i set the system property to request the use of the mmap directory, and start building a large index, the process dies with an ioexception trying to delete a file. apparently, lucene isn't closing down the memory map before deleting the file.",
        "label": 33
    },
    {
        "text": "standard highlighting doesn't work for toparentblockjoinquery because weightedspantermextractor#extract doesn't check for toparentblockjoinquery, the highlighter class fails to produce highlights for this type of query. at first it may seem like there's no issue, because toparentblockjoinquery only returns parent documents, while the highlighting applies to children. but if a client can directly supply the text from child documents (as elasticsearch does if _source is enabled), then highlighting will unexpectedly fail. a test case that triggers the bug is attached. the same issue exists for tochildblockjoinquery.",
        "label": 33
    },
    {
        "text": "optimize getmergedfieldinfos for one segment fieldinfos fieldinfos.getmergedfieldinfos could trivially return the fieldinfos of the first and only leafreader if there is only one leafreader. also... if there is more than one leafreader, and if fieldinfos & fieldinfo implemented equals() & hashcode() (including a cached hashcode), maybe we could also call equals() iterating through the fieldinfos to see if we should bother adding it to the fieldinfos.builder? admittedly this is speculative; may not be worth the bother.",
        "label": 10
    },
    {
        "text": " reproduce with  on test failure isn't right if you manually overrided anything if you run a test with eg -dtests.codec=simpletext... if it fails, the \"reproduce with\" fails to include that manual override (-dtests.codec=simpletext), ie it only includes the seed / test class / test method. so it won't actually reproduce the fail, in general. we just need to fix the \"reproduce with\" to add any manual overrides....",
        "label": 40
    },
    {
        "text": "provide limit on phrase analysis in fastvectorhighlighter with larger documents, fvh can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document. if one is willing to accept less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible. this is analogous to the highlighter limit on the number of characters to analyze. the patch includes an artifical test case that shows > 1000x speedup. in a more normal test environment, with english documents and random queries, i am seeing speedups of around 3-10x when setting phraselimit=1, which has the effect of selecting the first possible snippet in the document. most of our sites operate in this way (just show the first snippet), so this would be a big win for us. with phraselimit = -1, you get the existing fvh behavior. at larger values of phraselimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.",
        "label": 26
    },
    {
        "text": "improve testallanalyzershavefactories this test wasn't working at all, it would always pass. it is sensitive to the strings inside exception messages, if we change those, it might suddenly stop working. it would be great to improve this thing to be less fragile.",
        "label": 53
    },
    {
        "text": "charfilter   normalize characters before tokenizer this proposes to import charfilter that has been introduced in solr 1.4. please see for the details: solr-822 http://www.nabble.com/proposal-for-introducing-charfilter-to20327007.html",
        "label": 33
    },
    {
        "text": "indexwriter applies wrong deletes during concurrent flush all yonik uncovered this with the testrealtimeget test: if a flush-all is underway, it is possible for an incoming update to pick a dwpt that is stale, ie, not yet pulled/marked for flushing, yet the dw has cutover to a new deletes queue. if this happens, and the deleted term was also updated in one of the non-stale dwpts, then the wrong document is deleted and the test fails by detecting the wrong value. there's a 2nd failure mode that i haven't figured out yet, whereby 2 docs are returned when searching by id (there should only ever be 1 doc since the test uses updatedocument which is atomic wrt commit/reopen). yonik verified the test passes pre-dwpt, so my guess is (but i have yet to verify) this test also passes on 3.x. i'll backport the test to 3.x to be sure.",
        "label": 46
    },
    {
        "text": "testgeo3dpointfield testbasic  failure    [junit4] suite: org.apache.lucene.geo3d.testgeo3dpointfield    [junit4] ignor/a 0.01s j5 | testgeo3dpointfield.testrandombig    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpointfield -dtests.method=testbasic -dtests.seed=f318bb6a659e2e19 -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=ar_dz -dtests.timezone=america/north_dakota/new_salem -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   0.01s j5 | testgeo3dpointfield.testbasic <<<    [junit4]    > throwable #1: java.lang.illegalargumentexception: maxmbsortinheap=0.14646630717120193 only allows for maxpointssortinheap=1066, but this is less than maxpointsinleafnode=1736; either increase maxmbsortinheap or decrease maxpointsinleafnode    [junit4]    >  at __randomizedtesting.seedinfo.seed([f318bb6a659e2e19:58e2a67fba42a837]:0)    [junit4]    >  at org.apache.lucene.util.bkd.bkdwriter.<init>(bkdwriter.java:153)    [junit4]    >  at org.apache.lucene.codecs.lucene60.lucene60dimensionalwriter.writefield(lucene60dimensionalwriter.java:83)    [junit4]    >  at org.apache.lucene.index.dimensionalvalueswriter.flush(dimensionalvalueswriter.java:68)    [junit4]    >  at org.apache.lucene.index.defaultindexingchain.writedimensionalvalues(defaultindexingchain.java:146)    [junit4]    >  at org.apache.lucene.index.defaultindexingchain.flush(defaultindexingchain.java:96)    [junit4]    >  at org.apache.lucene.index.documentswriterperthread.flush(documentswriterperthread.java:423)    [junit4]    >  at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:502)    [junit4]    >  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:614)    [junit4]    >  at org.apache.lucene.index.indexwriter.getreader(indexwriter.java:422)    [junit4]    >  at org.apache.lucene.index.directoryreader.open(directoryreader.java:86)    [junit4]    >  at org.apache.lucene.geo3d.testgeo3dpointfield.testbasic(testgeo3dpointfield.java:112)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=lucene60, sim=classicsimilarity, locale=ar_dz, timezone=america/north_dakota/new_salem    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=402059552,total=521142272    [junit4]   2> note: all tests run in this jvm: [testgeo3dpointfield]    [junit4] completed [9/9] on j5 in 4.25s, 5 tests, 1 error, 1 skipped <<< failures!",
        "label": 33
    },
    {
        "text": "make indexreader open  always return msr to simplify  re opens  as per discussion in mailing list, i'm making directoryindexreader.open() always return msr, even for single-segment indexes. while theoretically valid in the past (if you make sure to keep your index constantly optimized) this feature is made practically obsolete by per-segment collection. the patch somewhat de-hairies (re-)open logic for msr/sr. sr no longer needs an ability to pose as toplevel directory-owning ir. all related logic is moved from dir to msr. dir becomes almost empty, and copying two or three remaining fields over to msr/sr, i remove it. lots of tests fail, as they rely on sr returned from ir.open(), i fix by introducing sr.getonlysegmentreader static package-private method. some previous bugs are uncovered, one is fixed in lucene-1645, another (partially fixed in lucene-1648) is fixed in this patch.",
        "label": 33
    },
    {
        "text": "some lucene tests try and use a junit assert in new threads there are a few cases in lucene tests where junit asserts are used inside a new threads run method - this won't work because junit throws an exception when a call to assert fails - that will kill the thread, but the exception will not propagate to junit - so unless a failure is caused later from the thread termination, the asserts are invalid. testthreadsafe teststressindexing2 teststringintern",
        "label": 53
    },
    {
        "text": "leafreader getfieldinfos should always return the same instance most implementations of the leafreader cache an instance of fieldinfos which is returned in the leafreader.getfieldinfos() method.  there are a few places that currently do not and this can cause performance problems. the most notable example is the lack of caching in solr's slowcompositereaderwrapper which caused unexpected performance slowdowns when trying to use solr's json facets compared to the legacy facets. this proposed change is mostly relevant to solr but touches a few lucene classes.  specifically: 1. adds a check to testutil.checkreader to verify that leafreader.getfieldinfos() returns the same instance:   // fieldinfos should be cached at the reader and always return the same instance  if (reader.getfieldinfos() != reader.getfieldinfos()) {  throw new runtimeexception(\"getfieldinfos() returned different instances for class: \"+reader.getclass());  } i'm not entirely sure this is wanted or needed but adding it uncovered most of the other leafreader implementations that were not caching fieldinfos.  i'm happy to remove this part of the patch though.   2. adds a fieldinfos.empty that can be used in a handful of places   public final static fieldinfos empty = new fieldinfos(new fieldinfo[0]); there are several places in the lucene/solr tests that were creating empty instances of fieldinfos which were causing the check in #1 to fail.  this fixes those failures and cleans up the code a bit. 3. fixes a few leafreader implementations that were not caching fieldinfos specifically: memoryindex.memoryindexreader - the constructor was already looping over the fields so it seemed natural to just create the fieldinfos at that time slowcompositereaderwrapper - this was the one causing me trouble.  i've moved the caching of fieldinfos from solrindexsearcher to slowcompositereaderwrapper. collapsingqparserplugin.readerwrapper - getfieldinfos() is immediately called twice after this is constructed expandcomponent.readerwrapper - getfieldinfos() is immediately called twice after this is constructed   4. minor solr tweak to avoid calling solrindexsearcher.getslowatomicreader in facetfieldprocessorbyhashdv.  this change is now optional since slowcompositereaderwrapper caches fieldinfos.   as suggested by david smiley this takes the place of solr-12878 since it touches some lucene code.  ",
        "label": 10
    },
    {
        "text": "please push maven snapshots to repositories apache org again once upon a time, snapshots of the lucene trunk went into the snapshot repo at repositories.apache.org. no longer. instead, they just sit at: https://builds.apache.org//job/lucene-solr-maven-trunk/lastsuccessfulbuild/artifact/maven_artifacts/ unfortunately, jenkins makes a rather mediocre maven repo. the maven-wagon-plugin can't copy it and nexus can't proxy it.",
        "label": 47
    },
    {
        "text": "improved snowball testing snowball project has test vocabulary files for each language in their svn repository, along with expected output. we should use these tests to ensure all languages are working correctly, and it might be helpful in the future for identifying back breaks/changes if we ever want to upgrade snowball, etc.",
        "label": 40
    },
    {
        "text": "segmenttermenum next  doesn't maintain prevbuffer at end when you're iterating a segmenttermenum and you go past the end of the docs, you end up with a state where the nextbuffer = null and the prevbuffer is the penultimate term, not the last term. this patch fixes it. (it's also required for my prefetching bug lucene-506) index: java/org/apache/lucene/index/segmenttermenum.java =================================================================== \u2014 java/org/apache/lucene/index/segmenttermenum.java (revision 382121) +++ java/org/apache/lucene/index/segmenttermenum.java (working copy) @@ -109,6 +109,7 @@ /** increments the enumeration to the next element. true if one exists.*/ public final boolean next() throws ioexception { if (position++ >= size - 1) { + prevbuffer.set(termbuffer); termbuffer.reset(); return false; }",
        "label": 33
    },
    {
        "text": "parallelreader is now atomic  rename to parallelatomicreader and also add a parallelcompositereader  that requires logdocmergepolicy to have identical subreader structure  the plan is: move all subreaders to ctor (builder-like api. first build reader-set, then call build) rename parallelreader to parallelatomicreader add a parallelcompositereader with same builder api, but taking any compositereader-set and checks them that they are aligned (docstarts identical). the subreaders are parallelatomicreaders.",
        "label": 53
    },
    {
        "text": "source distribution packaging targets should make a tarball from  svn export  instead of picking and choosing which stuff to include from a local working copy, lucene's dist-src/package-tgz-src target and solr's package-src target should simply perform \"svn export\" with the same revision and url as the local working copy.",
        "label": 47
    },
    {
        "text": "build xml  result of  dist src  should support  build contrib  currently the packed src distribution would fail to run \"ant build-contrib\". it would be much nicer if that work. in fact, would be nicer if you could even \"re-pack\" with it. for now i marked this for 2.1, although i am not yet sure if this is a stopper.",
        "label": 12
    },
    {
        "text": "make license checking maintenance easier automated instead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc.",
        "label": 15
    },
    {
        "text": "filteredquery always uses default filterstrategy if the filtered query is rewritten the rewrite method doesn't pass on the filterstrategy in filteredquery and we don't have a test for it. grrr....",
        "label": 46
    },
    {
        "text": "allow variable buffer size on bufferedindexoutput bufferedindexinput allows to set the buffersize but bufferedindexoutput doesn't this could be useful for optimizations related to lucene-4537. we should make the apis here consistent.",
        "label": 46
    },
    {
        "text": "fst save can truncate output  bufferedoutputstream may be closed after the underlying stream  are used in save (path) method of fst class, bufferedoutputstream has not been closed. when create a dictionary, there is a possibility that the buffered data is not written out.",
        "label": 11
    },
    {
        "text": "booleanqueries are not parsed correctly with the flexible query parser hi, i just found another bug in the flexible query parser (together with robert muir, yay!). the following query string works in the standard query parser: (field:[1 to *] and field:[* to 2]) and field2:z yields +(+field:[1 to *] +field:[* to 2]) +field2:z the flexible query parser though yields: +(field:[1 to *] field:[* to 2]) +field2:z test patch is attached (from robert actually). i don't know if it affects earlier versions than 3.5.",
        "label": 40
    },
    {
        "text": "improve basetokenstreamtestcase to uses a fake attribute to check if clearattributes  was called correctly   found bugs in contrib analyzers robert had the idea to use a fake attribute inside basetokenstreamtestcase that records if its clear() method was called. if this is not the case after incrementtoken(), assertokenstreamcontents fails. it also uses the attribute in teesinktokenfilter, because there a lot of copying, capturestate and restorestate() is used. by the attribute, you can track wonderful, if save/restore and clearattributes is correctly implemented. it also verifies that before a capturestate() it was also cleared (as the state will also contain the clear call). because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedstates must also be cleared before. in contrib analyzers are some test that fail to pass this additional assertion. they are not fixed in the attached patch.",
        "label": 53
    },
    {
        "text": "increase default maxfieldlength  to my understanding, lucene 2.3 will easily index large documents. so shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look. a better new default might be integer.max_value.",
        "label": 47
    },
    {
        "text": "fix remaining localization test failures in lucene see also lucene-1836 and lucene-1846 all tests should pass under different locales. the fix is to run 'ant test' under different locales, look and fix problems, and use the localizedtestcase from lucene-1836 to keep them from coming back. the same approach as lucene-1836 fixes the core queryparser, but i am running ant test under a few locales to look for more problems.",
        "label": 32
    },
    {
        "text": "add patternkeywordtokenfilter to marks keywords based on regular expressions today we need to pass in an explicit set of terms that we want to marks as keywords. it might make sense to allow patterns as well to prevent certain suffixes etc. to be keyworded.",
        "label": 46
    },
    {
        "text": "sort order different in branch 4x than trunk i will buy a beer to whoever figures out why +0 sorts before -0 in branch_4x, but works correctly in trunk",
        "label": 53
    },
    {
        "text": " patch  efficiently retrieve sizes of field values sometimes an application would like to know how large a document is before retrieving it. this can be important for memory management or choosing between algorithms, especially in cases where documents might be very large. this patch extends the existing fieldselector mechanism with two new fieldselectorresults: size and size_and_break. size creates fields on the retrieved document that store field sizes instead of actual values. size_and_break is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.",
        "label": 15
    },
    {
        "text": "fastvectorhighlighter  simplefraglistbuilder hard coded char margin is too naive the new fastvectorhighlighter performs extremely well, however i've found in testing that the window of text chosen per fragment is often very poor, as it is hard coded in simplefraglistbuilder to always select starting 6 characters to the left of the first phrase match in a fragment. when selecting long fragments, this often means that there is barely any context before the highlighted word, and lots after; even worse, when highlighting a phrase at the end of a short text the beginning is cut off, even though the entire phrase would fit in the specified fragcharsize. for example, highlighting \"punishment\" in \"crime and punishment\" returns \"e and <b>punishment</b>\" no matter what fragcharsize is specified. i am going to attach a patch that improves the text window selection by recalculating the starting margin once all phrases in the fragment have been identified - this way if a single word is matched in a fragment, it will appear in the middle of the highlight, instead of 6 characters from the beginning. this way one can also guarantee that the entirety of short texts are represented in a fragment by specifying a large enough fragcharsize.",
        "label": 26
    },
    {
        "text": "remove segments with all documents deleted in commit flush close of indexwriter instead of waiting until a merge occurs  i do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed: 4 of 14: name=_dlo doccount=5   compound=true   hasprox=true   numfiles=2   size (mb)=0.059   diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268p - 2009-09-21 10:25:09, os=sunos,      os.arch=amd64, java.vendor=sun microsystems inc., os.version=5.10, source=flush}   has deletions [delfilename=_dlo_1.del]   test: open reader.........ok [5 deleted docs]   test: fields..............ok [136 fields]   test: field norms.........ok [136 fields]   test: terms, freq, prox...ok [1698 terms; 4236 terms/docs pairs; 0 tokens]   test: stored fields.......ok [0 total field count; avg ? fields per doc]   test: term vectors........ok [0 total vector count; avg ? term/freq vector fields per doc] shouldn't such segments not be removed automatically during the next commit/close of indexwriter? mike mccandless: lucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which i agree would be a simple optimization. can you open a new issue? i would think iw can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.",
        "label": 33
    },
    {
        "text": "make contrib analyzers final the analyzers in contrib/analyzers should all be marked final. none of the analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and tokenizers is desired.",
        "label": 46
    },
    {
        "text": "maybe rename field omittf  and strengthen the javadocs spinoff from here: http://www.nabble.com/search-problem-when-indexed-using-field.setomittf()-td22456141.html maybe rename omittf to something like omittermpositions, and make it clear what queries will silently fail to work as a result.",
        "label": 33
    },
    {
        "text": "testcompoundfile fails on windows ant test-core -dtestcase=testcompoundfile -dtestmethod=testreadnestedcfp -dtests.seed=-61cb66ec0d71d1ac:-46685c36ec38fd32:568c63299214892c",
        "label": 46
    },
    {
        "text": "reproducable failure of testmaxfailuresrule james mentioned on the dev list that testmaxfailuresrule fails reliable for him on multiple platforms when using -dtests.seed=3facdc7ebd23cb80:3d65d783617f94f1 i was able to reproduce on my linux box as of trunk r1420486.",
        "label": 18
    },
    {
        "text": "add expectthrows utility to lucenetestcase in junit5, a neat assertion method is added which makes testing expected failures a little more straightforward. the block of code that is expected to throw is passed in with a lambda expression, and the caught exception returned for inspection. the usage looks something like this: ioexception e = expectthrows(ioexception.class, () -> {     throw new ioexception(\"some io error\"); }); // assert stuff here about the exception we should add this to lucenetestcase until junit5 is available.",
        "label": 41
    },
    {
        "text": "default kuromojianalyzer to use search mode kuromoji supports an option to segment text in a way more suitable for search, by preventing long compound nouns as indexing terms. in general 'how you segment' can be important depending on the application (see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese) the current algorithm punishes the cost based on some parameters (search_mode_penalty, search_mode_length, etc) for long runs of kanji. some questions (these can be separate future issues if any useful ideas come out): should these parameters continue to be static-final, or configurable? should pos also play a role in the algorithm (can/should we refine exactly what we decompound)? is the tokenizer the best place to do this, or should we do it in a tokenfilter? or both? with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. abcd -> ab, cd, abcd(posinc=0) from my understanding this tends to help with noun compounds in other languages, because idf of the original term boosts 'exact' compound matches. but does a tokenfilter provide the segmenter enough 'context' to do this properly? either way, i think as a start we should turn on what we have by default: its likely a very easy win.",
        "label": 40
    },
    {
        "text": "remove per document multiply in filteredquery spinoff of lucene-1536. in lucene-1536, uwe suggested using filteredquery under-the-hood to implement filtered search. but this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()). instead, it should just pass the boost down in its weight, like booleanquery does to avoid this per-document multiply.",
        "label": 40
    },
    {
        "text": "indexreader clone based on discussion http://www.nabble.com/indexreader.reopen-issue-td18070256.html. the problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior.",
        "label": 33
    },
    {
        "text": "unmatched right parentheses truncates query the query processor truncates a query when right parentheses are unmatched. e.g.: secret and illegal) and access:confidential will not result in a parseexception instead will run as: secret and illegal",
        "label": 32
    },
    {
        "text": "ngram tokenizer filters create nonsense offsets if followed by a word combiner it seems like maybe its possibly applying the offsets from the wrong token? because after shingling, the resulting token has a startoffset thats after the endoffset.",
        "label": 1
    },
    {
        "text": "system requirements is duplicated across versioned unversioned our system requirements page is located here on the unversioned site: http://lucene.apache.org/core/systemreqs.html but its also in forrest under each release. can we just nuke the forrested one?",
        "label": 53
    },
    {
        "text": "the release smoke tester inappropriately requires back compat index testing for versions not less than the one being smoke tested i ran ant nightly-smoke on my laptop against the lucene_solr_5_3 branch and got the following error:    [smoker] verify...    [smoker]   confirm all releases have coverage in testbackwardscompatibility    [smoker]     find all past lucene releases...    [smoker]     run testbackwardscompatibility..    [smoker] releases that don't seem to be tested:    [smoker]   5.4.0    [smoker] traceback (most recent call last):    [smoker]   file \"/users/sarowe/svn/lucene/dev/branches/lucene_solr_5_3/dev-tools/scripts/smoketestrele    [smoker] ase.py\", line 1449, in <module>    [smoker]     main()    [smoker]   file \"/users/sarowe/svn/lucene/dev/branches/lucene_solr_5_3/dev-tools/scripts/smoketestrelease.py\", line 1394, in main    [smoker]     smoketest(c.java, c.url, c.revision, c.version, c.tmp_dir, c.is_signed, ' '.join(c.test_args))    [smoker]   file \"/users/sarowe/svn/lucene/dev/branches/lucene_solr_5_3/dev-tools/scripts/smoketestrelease.py\", line 1432, in smoketest    [smoker]     unpackandverify(java, 'lucene', tmpdir, 'lucene-%s-src.tgz' % version, svnrevision, version, testargs, baseurl)    [smoker]   file \"/users/sarowe/svn/lucene/dev/branches/lucene_solr_5_3/dev-tools/scripts/smoketestrelease.py\", line 583, in unpackandverify    [smoker]     verifyunpacked(java, project, artifact, unpackpath, svnrevision, version, testargs, tmpdir, baseurl)    [smoker]   file \"/users/sarowe/svn/lucene/dev/branches/lucene_solr_5_3/dev-tools/scripts/smoketestrelease.py\", line 762, in verifyunpacked    [smoker]     confirmallreleasesaretestedforbackcompat(unpackpath)    [smoker]   file \"/users/sarowe/svn/lucene/dev/branches/lucene_solr_5_3/dev-tools/scripts/smoketestrelease.py\", line 1387, in confirmallreleasesaretestedforbackcompat    [smoker]     raise runtimeerror('some releases are not tested by testbackwardscompatibility?')    [smoker] runtimeerror: some releases are not tested by testbackwardscompatibility? here's the relevant section of smoketestrelease.py - getalllucenereleases() fetches all dotted-version entries in the file listing page returned by the web server at https://archive.apache.org/dist/lucene/java/: def confirmallreleasesaretestedforbackcompat(unpackpath):   print('    find all past lucene releases...')   allreleases = getalllucenereleases()   [...]   nottested = []   for x in allreleases:     if x not in testedindices:       if '.'.join(str(y) for y in x) in ('1.4.3', '1.9.1', '2.3.1', '2.3.2'):         # exempt the dark ages indices         continue       nottested.append(x)   if len(nottested) > 0:     nottested.sort()     print('releases that don\\'t seem to be tested:')     failed = true     for x in nottested:       print('  %s' % '.'.join(str(y) for y in x))     raise runtimeerror('some releases are not tested by testbackwardscompatibility?') i think the code above should allow/ignore versions greater than the version being smoke tested. afaik, version 5.3.2 will be the first release where a greater version has been released in the past since full back compat testing started being checked for by the smoke tester. (the last time this happened was when 4.9.1 was released after 4.10.0.)",
        "label": 47
    },
    {
        "text": "make docidsetiterator docid  return  when not positioned today docidsetiterator.docid() can either return -1 or no_more_docs when the enum is not positioned. i would like to only allow it to return -1 so that we can have better assertions. (this proposal is for trunk only.)",
        "label": 1
    },
    {
        "text": "several codecs use the same files   perfieldcodecwrapper can not hold two codec using the same files currently we have a rather simple file naming scheme which prevents us from using more than one codec in a segment that relies on the same file. for instance pulsing and standard codec can not be used together since they both need the .frq .tii .tis etc. to make this work we either need to write distinct per codec files or set a per field / codec file id. while the first solution seems to be quiet verbose the second one seems to be more flexible too. one possibility to do that would be to assign a unique id to each segmentswritestate when opening the fieldsconsumer and write the ids into the segments file to eventually load it once the segment is opened. otherwise our perfieldcodec feature will not be really flexible nor useful though.",
        "label": 46
    },
    {
        "text": "standard polygon is created with bogus tile beasting polygon tests produces the time to time errors due to standard polygons with a bogus tile. we need to improve the checks for those situations and throw a tileexception in those cases so we fall back to complex polygons.",
        "label": 19
    },
    {
        "text": "enhancementspayloaditerator getcategorydata categoryenhancement  problematic usage of object equals  enhancementspayloaditerator has an internal list of category enhancemnets, and in getcategorydata(categoryenhancement) there is a lookup of the given categoryenhancement in the list. in order to make sure this lookup works, categoryenhancement must override object.equals(object).",
        "label": 43
    },
    {
        "text": "solr contrib  map reduce  breaks manifest of all other jar files by adding a broken main class attribute the addition of the solr map-reduce contrib created a new \"main.class\" property, which is used by the jarify task. currently only the map-reduce plugin actually set this property, soall other generated jar files contain the following line: main-class: ${main.class} this happens because the ant property \"main.class\" is undefined for most modules. maybe this was added for one of the modules (i assume that the solr-morphline jars use this attribute?). we should add some if/then/else structure to the <jarify/> task that only sets this property, if it is actually defined. otherwise remove it (i think ant does this automatically if its empty, means string-empty, have to try out) this leads to an error if the file is double-clicked or started via java -jar: c:\\users\\uwe schindler\\desktop>java -jar lucene-core-4.7.0.jar fehler: hauptklasse ${main.class} konnte nicht gefunden oder geladen werden i opened this issue in lucene, because jar files from lucene and solr are affected.",
        "label": 47
    },
    {
        "text": "org apache lucene ant htmldocument creates a fileinputstream in its constructor that it doesn't close a look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me.",
        "label": 29
    },
    {
        "text": "offline sorter wrongly uses min buffer size if there is more memory available the sorter we use for offline sorting seems to use the min_buffer_size as a upper bound even if there is more memory available. see this snippet: long half = free/2; if (half >= absolute_min_sort_buffer_size) {    return new buffersize(math.min(min_buffer_size_mb * mb, half)); }        // by max mem (heap will grow) half = (max - total) / 2; return new buffersize(math.min(min_buffer_size_mb * mb, half)); use use use math.max instead of min here.",
        "label": 46
    },
    {
        "text": "custom similarity is ignored when using multisearcher symptoms: i am using searcher.setsimilarity() to provide a custom similarity that turns off tf() factor. however, somewhere along the way the custom similarity is ignored and the defaultsimilarity is used. i am using multisearcher and booleanquery. problem analysis: the problem seems to be in multisearcher.createweight(query) method. it creates an instance of cacheddfsource but does not set the similarity. as the result cacheddfsource provides defaultsimilarity to queries that use it. potential solution: adding the following line: cachesim.setsimilarity(getsimilarity()); after creating an instance of cachedfsource (line 312) seems to fix the problem. however, i don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.",
        "label": 12
    },
    {
        "text": "add search timeout support to lucene this patch is based on nutch-308. this patch adds support for a maximum search time limit. after this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated. this patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer. this was also discussed in an e-mail thread. http://www.nabble.com/search-timeout-tf3410206.html#a9501029",
        "label": 12
    },
    {
        "text": "lucene should have an entirely memory resident term dictionary fst based termdictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. some environments have enough memory available to keep the entire fst based term dict in memory. we should add a termdictionary implementation that encodes all needed information for each term into the fst (custom fst.output) and builds a fst from the entire term not just the delta.",
        "label": 17
    },
    {
        "text": "config incorrectly handles windows absolute pathnames i have no idea how no one ran into this so far, but i tried to execute an .alg file which used reuterscontentsource and referenced both docs.dir and work.dir as windows absolute pathnames (e.g. d:\\something). surprisingly, the run reported an error of missing content under benchmark\\work\\something. i've traced the problem back to config, where get(string, string) includes the following code:     if (sval.indexof(\":\") < 0) {       return sval;     }     // first time this prop is extracted by round     int k = sval.indexof(\":\");     string colname = sval.substring(0, k);     sval = sval.substring(k + 1);     ... it detects \":\" in the value and so it thinks it's a per-round property, thus stripping \"d:\" from the value ... fix is very simple:     if (sval.indexof(\":\") < 0) {       return sval;     } else if (sval.indexof(\":\\\\\") >= 0) {       // this previously messed up absolute path names on windows. assuming       // there is no real value that starts with \\\\       return sval;     }     // first time this prop is extracted by round     int k = sval.indexof(\":\");     string colname = sval.substring(0, k);     sval = sval.substring(k + 1); i'll post a patch w/ the above fix + test shortly.",
        "label": 43
    },
    {
        "text": "binary docvalues updates lucene-5189 was a great move toward. i wish to continue. the reason for having this feature is to have \"join-index\" - to write children docnums into parent's binarydv. i can try to proceed the implementation, but i'm not so experienced in such deep lucene internals. shai erera, any hint to begin with is much appreciated.",
        "label": 43
    },
    {
        "text": "add infrastructure for longer running nightly test cases i'm spinning this out of lucene-2762... the patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test nrt. i'd like to see some tests run on more substantial indices based on real data... so this is just a start.",
        "label": 33
    },
    {
        "text": "improve spatialexample java to show distance value retrieval i want to have spatialexample.java depict how to get the distance. i also observed that spatialexample.java isn't tested, despite it being a junit test. the reason is its name doesn't begin or end in \"test\". as a small hack, a test method can be added elsewhere to call it. patch to follow...",
        "label": 10
    },
    {
        "text": "sloppyphrasescorer calls docsandpositionsenum advance with target    sloppyphrasescorer calls docsandpositionsenum.advance with target = -1 although the behavior of this method is undefined in such cases.",
        "label": 1
    },
    {
        "text": "testgeo3dpoint testgeo3drelations  failure  invalid hits for shape georectangle policeman jenkins found a seed https://jenkins.thetaphi.de/job/lucene-solr-6.x-linux/1989/, that reproduces for me with java8 on linux, both on master and on branch_6x:    [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint    [junit4] ignor/a 0.01s j1 | testgeo3dpoint.testrandombig    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())    [junit4]   1> doc=23 should match but did not    [junit4]   1>   point=[x=-0.3032237416849336, y=2.3309121299774915e-10, z=-0.9508945608073219]    [junit4]   1>   mappedpoint=[lat=-1.2621073061661279, lon=3.141592653589793([x=-0.3032237417663322, y=3.7134198477962236e-17, z=-0.9508945610068391])]    [junit4]   1> doc=30 should match but did not    [junit4]   1>   point=[x=-0.19482888783564378, y=2.3309121299774915e-10, z=-0.9786855487622393]    [junit4]   1>   mappedpoint=[lat=-1.3742932353673953, lon=3.141592653589793([x=-0.1948288878124478, y=2.3859657384095297e-17, z=-0.9786855486360273])]    [junit4]   1> doc=83 should match but did not    [junit4]   1>   point=[x=-0.2892999513665818, y=2.3309121299774915e-10, z=-0.9551939133132278]    [junit4]   1>   mappedpoint=[lat=-1.2767082336466407, lon=3.141592653589793([x=-0.2892999511883776, y=3.5429025921633215e-17, z=-0.9551939133776381])]    [junit4]   1> doc=204 should match but did not    [junit4]   1>   point=[x=-0.2304883857492458, y=-2.3309121299774915e-10, z=-0.970958461302852]    [junit4]   1>   mappedpoint=[lat=-1.3377279126209909, lon=-3.141592653589793([x=-0.23048838553642823, y=-2.8226686358782794e-17, z=-0.9709584613946622])]    [junit4]   1> doc=295 should match but did not    [junit4]   1>   point=[x=-0.38044050228897974, y=2.3309121299774915e-10, z=-0.9229103565532143]    [junit4]   1>   mappedpoint=[lat=-1.1798015420947494, lon=3.141592653589793([x=-0.38044050220203357, y=4.6590524328773195e-17, z=-0.9229103567469498])]    [junit4]   1> doc=335 should match but did not    [junit4]   1>   point=[x=-0.34552679276447196, y=-2.3309121299774915e-10, z=-0.9364507784902717]    [junit4]   1>   mappedpoint=[lat=-1.2173184081667363, lon=-3.141592653589793([x=-0.3455267925813584, y=-4.2314828055441195e-17, z=-0.9364507786023997])]    [junit4]   1> doc=444 should match but did not    [junit4]   1>   point=[x=-0.35565273191587166, y=2.3309121299774915e-10, z=-0.9326775916369318]    [junit4]   1>   mappedpoint=[lat=-1.2064925301928429, lon=3.141592653589793([x=-0.35565273187036933, y=4.355489796930597e-17, z=-0.9326775917892445])]    [junit4]   1> doc=456 should match but did not    [junit4]   1>   point=[x=-0.24059621278103072, y=2.3309121299774915e-10, z=-0.9685197822476396]    [junit4]   1>   mappedpoint=[lat=-1.3273086505286442, lon=3.141592653589793([x=-0.24059621266986583, y=2.9464538173312706e-17, z=-0.9685197820947206])]    [junit4]   1> doc=749 should match but did not    [junit4]   1>   point=[x=-0.27931563097728074, y=2.3309121299774915e-10, z=-0.9581412458781736]    [junit4]   1>   mappedpoint=[lat=-1.2871390311991375, lon=3.141592653589793([x=-0.27931563076181104, y=3.420629931642758e-17, z=-0.9581412457046323])]    [junit4]   1> doc=978 should match but did not    [junit4]   1>   point=[x=-0.20014590305820745, y=-2.3309121299774915e-10, z=-0.9776192380446992]    [junit4]   1>   mappedpoint=[lat=-1.3688589001639526, lon=-3.141592653589793([x=-0.20014590297436863, y=-2.4510803944001724e-17, z=-0.9776192382200114])]    [junit4]   1> doc=1019 should match but did not    [junit4]   1>   point=[x=-0.1895701514767011, y=-2.3309121299774915e-10, z=-0.9797108368248392]    [junit4]   1>   mappedpoint=[lat=-1.3796623403968673, lon=-3.141592653589793([x=-0.18957015125823026, y=-2.3215647895227127e-17, z=-0.9797108369364872])]    [junit4]   1> doc=1050 should match but did not    [junit4]   1>   point=[x=-0.37669388969865125, y=-2.3309121299774915e-10, z=-0.9244356257338767]    [junit4]   1>   mappedpoint=[lat=-1.1838538424170673, lon=-3.141592653589793([x=-0.37669388956862926, y=-4.613169661185884e-17, z=-0.9244356256615374])]    [junit4]   1> doc=1077 should match but did not    [junit4]   1>   point=[x=-0.281464835919801, y=-2.3309121299774915e-10, z=-0.9575163092226472]    [junit4]   1>   mappedpoint=[lat=-1.2848963877296418, lon=-3.141592653589793([x=-0.2814648357291603, y=-3.4469501014825175e-17, z=-0.957516309437251])]    [junit4]   1> doc=1087 should match but did not    [junit4]   1>   point=[x=-0.22993880934761124, y=-2.3309121299774915e-10, z=-0.9710878847326866]    [junit4]   1>   mappedpoint=[lat=-1.3382936874912785, lon=-3.141592653589793([x=-0.22993880922160065, y=-2.8159382671298716e-17, z=-0.9710878846374904])]    [junit4]   1> doc=1245 should match but did not    [junit4]   1>   point=[x=-0.2977325835509569, y=-2.3309121299774915e-10, z=-0.9526165659644908]    [junit4]   1>   mappedpoint=[lat=-1.2678732944040285, lon=-3.141592653589793([x=-0.2977325833240384, y=-3.6461725516965625e-17, z=-0.9526165658097825])]    [junit4]   1> doc=1274 should match but did not    [junit4]   1>   point=[x=-0.2230123351953449, y=-2.3309121299774915e-10, z=-0.9726911285182859]    [junit4]   1>   mappedpoint=[lat=-1.345418012127493, lon=-3.141592653589793([x=-0.2230123354014425, y=-2.7311134271975252e-17, z=-0.9726911284129601])]    [junit4]   1> doc=1365 should match but did not    [junit4]   1>   point=[x=-0.21337764463280406, y=-2.3309121299774915e-10, z=-0.9748355592865815]    [junit4]   1>   mappedpoint=[lat=-1.3553090486591461, lon=-3.141592653589793([x=-0.21337764449242597, y=-2.6131224933725133e-17, z=-0.9748355591900344])]    [junit4]   1> doc=1421 should match but did not    [junit4]   1>   point=[x=-0.357719833275708, y=-2.3309121299774915e-10, z=-0.9318920262323395]    [junit4]   1>   mappedpoint=[lat=-1.2042771838760873, lon=-3.141592653589793([x=-0.3577198333651691, y=-4.380804489221789e-17, z=-0.9318920261236907])]    [junit4]   1> doc=1567 should match but did not    [junit4]   1>   point=[x=-0.2019549943558293, y=2.3309121299774915e-10, z=-0.977249613653688]    [junit4]   1>   mappedpoint=[lat=-1.367008546421418, lon=3.141592653589793([x=-0.2019549945354198, y=2.4732353762962308e-17, z=-0.9772496136297395])]    [junit4]   1> doc=1695 should match but did not    [junit4]   1>   point=[x=-0.21798690729041048, y=-2.3309121299774915e-10, z=-0.9738220605113527]    [junit4]   1>   mappedpoint=[lat=-1.3505798311224506, lon=-3.141592653589793([x=-0.2179869073694118, y=-2.6695696836598075e-17, z=-0.9738220602853552])]    [junit4]   1> doc=1858 should match but did not    [junit4]   1>   point=[x=-0.2881195858552449, y=2.3309121299774915e-10, z=-0.9555482278071868]    [junit4]   1>   mappedpoint=[lat=-1.2779430458945407, lon=3.141592653589793([x=-0.2881195859095498, y=3.5284472865579097e-17, z=-0.9555482277022879])]    [junit4]   1> doc=2207 should match but did not    [junit4]   1>   point=[x=-0.32595903903655066, y=-2.3309121299774915e-10, z=-0.9433936450151833]    [junit4]   1>   mappedpoint=[lat=-1.2381203695374021, lon=-3.141592653589793([x=-0.3259590392236278, y=-3.991846940383623e-17, z=-0.9433936449254461])]    [junit4]   1> doc=2349 should match but did not    [junit4]   1>   point=[x=-0.20974591943808518, y=-2.3309121299774915e-10, z=-0.9756181369280056]    [junit4]   1>   mappedpoint=[lat=-1.3590319035018428, lon=-3.141592653589793([x=-0.20974591957164881, y=-2.5686466903763788e-17, z=-0.9756181367786224])]    [junit4]   1> doc=2380 should match but did not    [junit4]   1>   point=[x=-0.4875463924984321, y=2.3309121299774915e-10, z=-0.8714467112499943]    [junit4]   1>   mappedpoint=[lat=-1.06071322725801, lon=3.141592653589793([x=-0.48754639238198005, y=5.970721288664315e-17, z=-0.8714467113481478])]    [junit4]   1> doc=2403 should match but did not    [junit4]   1>   point=[x=-0.5307326574182415, y=2.3309121299774915e-10, z=-0.8460130563140477]    [junit4]   1>   mappedpoint=[lat=-1.0105204721369465, lon=3.141592653589793([x=-0.5307326575599536, y=6.499600502837654e-17, z=-0.8460130563310785])]    [junit4]   1> doc=2647 should match but did not    [junit4]   1>   point=[x=-0.41216428024496576, y=-2.3309121299774915e-10, z=-0.9092785241675204]    [junit4]   1>   mappedpoint=[lat=-1.1452121604915835, lon=-3.141592653589793([x=-0.41216428042536374, y=-5.047556667457938e-17, z=-0.9092785241822108])]    [junit4]   1> doc=2672 should match but did not    [junit4]   1>   point=[x=-0.18909431254793352, y=-2.3309121299774915e-10, z=-0.9798021745490172]    [junit4]   1>   mappedpoint=[lat=-1.380147893716371, lon=-3.141592653589793([x=-0.18909431255244671, y=-2.3157374460432306e-17, z=-0.9798021744231401])]    [junit4]   1> doc=2676 should match but did not    [junit4]   1>   point=[x=-0.2859003155588272, y=-2.3309121299774915e-10, z=-0.9562101196759868]    [junit4]   1>   mappedpoint=[lat=-1.2802634653663008, lon=-3.141592653589793([x=-0.2859003156392869, y=-3.501269064228707e-17, z=-0.9562101195662314])]    [junit4]   1> doc=2682 should match but did not    [junit4]   1>   point=[x=-0.5068398365617964, y=-2.3309121299774915e-10, z=-0.86044329743017]    [junit4]   1>   mappedpoint=[lat=-1.038470904936626, lon=-3.141592653589793([x=-0.5068398363428848, y=-6.206997832576823e-17, z=-0.8604432975645862])]    [junit4]   1> doc=2784 should match but did not    [junit4]   1>   point=[x=-0.37840497686340335, y=-2.3309121299774915e-10, z=-0.9237412167084987]    [junit4]   1>   mappedpoint=[lat=-1.1820039671839628, lon=-3.141592653589793([x=-0.37840497689182123, y=-4.6341244373199705e-17, z=-0.92374121663021])]    [junit4]   1> doc=2880 should match but did not    [junit4]   1>   point=[x=-0.2569863542772196, y=-2.3309121299774915e-10, z=-0.9643286344122667]    [junit4]   1>   mappedpoint=[lat=-1.3103565403065773, lon=-3.141592653589793([x=-0.25698635432302114, y=-3.1471751624623544e-17, z=-0.9643286344522917])]    [junit4]   1> doc=2902 should match but did not    [junit4]   1>   point=[x=-0.5314576316943799, y=-2.3309121299774915e-10, z=-0.8455608705492103]    [junit4]   1>   mappedpoint=[lat=-1.0096649353953517, lon=-3.141592653589793([x=-0.5314576318897689, y=-6.508478877762378e-17, z=-0.8455608705872797])]    [junit4]   1> doc=2961 should match but did not    [junit4]   1>   point=[x=-0.22611501186537553, y=-2.3309121299774915e-10, z=-0.9719793532651747]    [junit4]   1>   mappedpoint=[lat=-1.3422281433038927, lon=-3.141592653589793([x=-0.2261150116836731, y=-2.7691102529757665e-17, z=-0.9719793532388703])]    [junit4]   1> doc=3060 should match but did not    [junit4]   1>   point=[x=-0.3694237626445084, y=2.3309121299774915e-10, z=-0.9273452879617076]    [junit4]   1>   mappedpoint=[lat=-1.1916984985376446, lon=3.141592653589793([x=-0.36942376251911235, y=4.524136282980029e-17, z=-0.9273452880788157])]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=7fede7a41eb3b240 -dtests.multiplier=3 -dtests.slow=true -dtests.locale=en-ie -dtests.timezone=america/st_kitts -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 0.21s j1 | testgeo3dpoint.testgeo3drelations <<<    [junit4]    > throwable #1: java.lang.assertionerror: invalid hits for shape=georectangle: {planetmodel=planetmodel.wgs84, toplat=0.017453291479645996(0.9999999403953552), bottomlat=-1.3962634015954636(-80.0), leftlon=2.4457272005608357e-47(1.401298464324817e-45), rightlon=1.869241958456853e-12(1.0709967510834605e-10)}    [junit4]    >  at __randomizedtesting.seedinfo.seed([7fede7a41eb3b240:cf929a3091fe1cdc]:0)    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:464)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=asserting(lucene62): {id=postingsformat(name=lucenefixedgap)}, docvalues:{id=docvaluesformat(name=lucene54), point=docvaluesformat(name=direct)}, maxpointsinleafnode=1896, maxmbsortinheap=5.304289065936136, sim=randomsimilarity(querynorm=true,coord=crazy): {}, locale=en-ie, timezone=america/st_kitts    [junit4]   2> note: linux 4.4.0-42-generic amd64/oracle corporation 1.8.0_102 (64-bit)/cpus=12,threads=1,free=452209936,total=508887040    [junit4]   2> note: all tests run in this jvm: [testgeo3dpoint]    [junit4] completed [10/11 (1!)] on j1 in 68.45s, 14 tests, 1 failure, 1 skipped <<< failures!",
        "label": 25
    },
    {
        "text": "merge segments to sort them it would be awesome if lucene could write the documents out in a segment based on a configurable order. this of course applies to merging segments to. the benefit is increased locality on disk of documents that are likely to be accessed together. this often applies to documents near each other in time, but also spatially.",
        "label": 1
    },
    {
        "text": " patch  ant task  javadocs all  fails with outofmemoryerror hi all, the current nightly build's \"ant dist\" fails with an outofmemoryerror at ant task javadocs-all (see below). apparently javadoc needs more memory. a similar case has been reported in hadoop-5561 (add a maxmemory statement to the javadoc task), and i propose the same change for lucene as well. cheers, christian javadocs-all: [javadoc] generating javadoc [javadoc] javadoc execution [javadoc] loading source files for package org.apache.lucene... [javadoc] loading source files for package org.apache.lucene.analysis... (...) [javadoc] loading source files for package org.apache.lucene.queryparser.standard.config... [javadoc] loading source files for package org.apache.lucene.queryparser.standard.nodes... [javadoc] loading source files for package org.apache.lucene.queryparser.standard.parser... [javadoc] loading source files for package org.apache.lucene.queryparser.standard.processors... [javadoc] constructing javadoc information... [javadoc] standard doclet version 1.6.0_15 [javadoc] building tree for all the packages and classes... (takes a long time here until oome) [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] at java.lang.throwable.getstacktraceelement(native method) [javadoc] at java.lang.throwable.getourstacktrace(throwable.java:591) [javadoc] at java.lang.throwable.printstacktrace(throwable.java:462) [javadoc] at java.lang.throwable.printstacktrace(throwable.java:451) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.abstractbuilder.build(abstractbuilder.java:103) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.abstractmemberbuilder.build(abstractmemberbuilder.java:56) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.buildmembersummary(classbuilder.java:279) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [javadoc] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [javadoc] at java.lang.reflect.method.invoke(method.java:597) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.invokemethod(classbuilder.java:101) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.abstractbuilder.build(abstractbuilder.java:90) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.buildclassdoc(classbuilder.java:124) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [javadoc] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [javadoc] at java.lang.reflect.method.invoke(method.java:597) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.invokemethod(classbuilder.java:101) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.abstractbuilder.build(abstractbuilder.java:90) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.build(classbuilder.java:108) [javadoc] at com.sun.tools.doclets.formats.html.htmldoclet.generateclassfiles(htmldoclet.java:155) [javadoc] at com.sun.tools.doclets.internal.toolkit.abstractdoclet.generateclassfiles(abstractdoclet.java:164) [javadoc] at com.sun.tools.doclets.internal.toolkit.abstractdoclet.startgeneration(abstractdoclet.java:106) [javadoc] at com.sun.tools.doclets.internal.toolkit.abstractdoclet.start(abstractdoclet.java:64) [javadoc] at com.sun.tools.doclets.formats.html.htmldoclet.start(htmldoclet.java:42) [javadoc] at com.sun.tools.doclets.standard.standard.start(standard.java:23) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [javadoc] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [javadoc] at java.lang.reflect.method.invoke(method.java:597) [javadoc] at com.sun.tools.javadoc.docletinvoker.invoke(docletinvoker.java:269) [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] at java.util.arrays.copyofrange(arrays.java:3209) [javadoc] at java.lang.string.<init>(string.java:215) [javadoc] at com.sun.tools.javac.util.convert.utf2string(convert.java:131) [javadoc] at com.sun.tools.javac.util.name.tostring(name.java:164) [javadoc] at com.sun.tools.javadoc.classdocimpl.getclassname(classdocimpl.java:341) [javadoc] at com.sun.tools.javadoc.typemaker.gettypename(typemaker.java:100) [javadoc] at com.sun.tools.javadoc.parameterizedtypeimpl.parameterizedtypetostring(parameterizedtypeimpl.java:117) [javadoc] at com.sun.tools.javadoc.typemaker.gettypestring(typemaker.java:121) [javadoc] at com.sun.tools.javadoc.executablememberdocimpl.makesignature(executablememberdocimpl.java:217) [javadoc] at com.sun.tools.javadoc.executablememberdocimpl.signature(executablememberdocimpl.java:198) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap.getmemberkey(visiblemembermap.java:485) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap.access$1000(visiblemembermap.java:28) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap$classmembers.isoverridden(visiblemembermap.java:442) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap$classmembers.addmembers(visiblemembermap.java:316) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap$classmembers.build(visiblemembermap.java:278) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap$classmembers.access$100(visiblemembermap.java:230) [javadoc] at com.sun.tools.doclets.internal.toolkit.util.visiblemembermap.<init>(visiblemembermap.java:93) [javadoc] at com.sun.tools.doclets.formats.html.constructorwriterimpl.<init>(constructorwriterimpl.java:38) [javadoc] at com.sun.tools.doclets.formats.html.writerfactoryimpl.getconstructorwriter(writerfactoryimpl.java:129) [javadoc] at com.sun.tools.doclets.formats.html.writerfactoryimpl.getmembersummarywriter(writerfactoryimpl.java:141) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.membersummarybuilder.init(membersummarybuilder.java:104) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.membersummarybuilder.getinstance(membersummarybuilder.java:64) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.builderfactory.getmembersummarybuilder(builderfactory.java:191) [javadoc] at com.sun.tools.doclets.formats.html.classwriterimpl.navsummarylinks(classwriterimpl.java:474) [javadoc] at com.sun.tools.doclets.formats.html.classwriterimpl.printsummarydetaillinks(classwriterimpl.java:456) [javadoc] at com.sun.tools.doclets.formats.html.htmldocletwriter.navlinks(htmldocletwriter.java:462) [javadoc] at com.sun.tools.doclets.formats.html.classwriterimpl.writefooter(classwriterimpl.java:146) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.buildclassfooter(classbuilder.java:330) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [javadoc] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [javadoc] at java.lang.reflect.method.invoke(method.java:597) [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] at java.util.linkedhashmap.createentry(linkedhashmap.java:424) [javadoc] at java.util.linkedhashmap.addentry(linkedhashmap.java:406) [javadoc] at java.util.hashmap.put(hashmap.java:385) [javadoc] at sun.util.resources.openlistresourcebundle.loadlookup(openlistresourcebundle.java:118) [javadoc] at sun.util.resources.openlistresourcebundle.loadlookuptablesifnecessary(openlistresourcebundle.java:97) [javadoc] at sun.util.resources.openlistresourcebundle.handlegetobject(openlistresourcebundle.java:58) [javadoc] at sun.util.resources.timezonenamesbundle.handlegetobject(timezonenamesbundle.java:59) [javadoc] at java.util.resourcebundle.getobject(resourcebundle.java:378) [javadoc] at java.util.resourcebundle.getobject(resourcebundle.java:381) [javadoc] at java.util.resourcebundle.getstringarray(resourcebundle.java:361) [javadoc] at sun.util.timezonenameutility.retrievedisplaynames(timezonenameutility.java:100) [javadoc] at sun.util.timezonenameutility.retrievedisplaynames(timezonenameutility.java:81) [javadoc] at java.util.timezone.getdisplaynames(timezone.java:399) [javadoc] at java.util.timezone.getdisplayname(timezone.java:350) [javadoc] at java.util.date.tostring(date.java:1025) [javadoc] at com.sun.tools.doclets.formats.html.markup.htmldocwriter.today(htmldocwriter.java:337) [javadoc] at com.sun.tools.doclets.formats.html.htmldocletwriter.printhtmlheader(htmldocletwriter.java:281) [javadoc] at com.sun.tools.doclets.formats.html.classwriterimpl.writeheader(classwriterimpl.java:122) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.buildclassheader(classbuilder.java:164) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [javadoc] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [javadoc] at java.lang.reflect.method.invoke(method.java:597) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.invokemethod(classbuilder.java:101) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.abstractbuilder.build(abstractbuilder.java:90) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.buildclassdoc(classbuilder.java:124) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [javadoc] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [javadoc] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [javadoc] at java.lang.reflect.method.invoke(method.java:597) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.classbuilder.invokemethod(classbuilder.java:101) [javadoc] at com.sun.tools.doclets.internal.toolkit.builders.abstractbuilder.build(abstractbuilder.java:90) [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] java.lang.outofmemoryerror: java heap space [javadoc] java.lang.outofmemoryerror: java heap space",
        "label": 33
    },
    {
        "text": "improve  ant eclipse  to select right jre for building whenever i run \"ant eclipse\" the setting choosing the right jvm is lost and has to be reassigned in the project properties. in fact the classpath generator writes a new classpath file (as it should), but this onl ycontains the \"default\" entry: <classpathentry kind=\"con\" path=\"org.eclipse.jdt.launching.jre_container\"/> instead it should preserve something like: <classpathentry kind=\"con\" path=\"org.eclipse.jdt.launching.jre_container/org.eclipse.jdt.internal.debug.ui.launcher.standardvmtype/jdk1.8.0_25\"/> we can either path this by a ant property via command line or user can do this with \"lucene/build.properties\" or per user. an alternative would be to generate the name \"jdk1.8.0_25\" by guessing from ant's \"java.home\". if this name does not exist in eclipse it would produce an error and user would need to add the correct jdk. i currently have the problem that my eclipse uses java 7 by default and whenever i rebuild the eclipse project, the change to java 8 in trunk is gone. when this is fixed, i could easily/automatically have the \"right\" jdk used by eclipse for trunk (java 8) and branch_5x (java 7).",
        "label": 11
    },
    {
        "text": "remove unnecessary string concatenation in indexwriter i've noticed a couple of places in indexwriter where a boolean string is created by bool + \"\", or integer by int + \"\". there are some places (in setdiagonstics) where a string is concatenated with an empty string ... the patch uses boolean.tostring and integer.tostring, as well as remove the unnecessary str + \"\".",
        "label": 33
    },
    {
        "text": "error with codec on android hello, with the latest version of lucene 4.0, i have an error : \"codec with name \u2018lucene40\u2032 does not exist. you need to add the corresponding jar file supporting this spi to your classpath. the current classpath supports the following names: []\" classpath is ok, but the method for initialise codec don't work. this error does not append with lucene 3.6.",
        "label": 53
    },
    {
        "text": "unable to generate a x back compat test index it seems the issue is an iwc cannot be used across 2 different indexwriters, because the dwptthreadpool is created on the indexwriterconfig, and you hit a setonce exception... seems like the exception is correct and it would be bad to have two indexwriters sharing the same dwptthreadpool? stacktrace to follow in a comment.",
        "label": 33
    },
    {
        "text": "build failed in the flexscoring branch because of javadoc warnings ant build log: [javadoc] standard doclet version 1.6.0_24 [javadoc] building tree for all the packages and classes... [javadoc] /home/savior/development/workspaces/java/lucene-gsoc/lucene/src/java/org/apache/lucene/search/similarity.java:93: warning - tag @link: can't find tf(float) in org.apache.lucene.search.similarity [javadoc] /home/savior/development/workspaces/java/lucene-gsoc/lucene/src/java/org/apache/lucene/search/tfidfsimilarity.java:588: warning - @param argument \"term\" is not a parameter name. [javadoc] /home/savior/development/workspaces/java/lucene-gsoc/lucene/src/java/org/apache/lucene/search/tfidfsimilarity.java:588: warning - @param argument \"docfreq\" is not a parameter name. [javadoc] /home/savior/development/workspaces/java/lucene-gsoc/lucene/src/java/org/apache/lucene/search/tfidfsimilarity.java:618: warning - @param argument \"terms\" is not a parameter name. [javadoc] generating /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html... [javadoc] copying file /home/savior/development/workspaces/java/lucene-gsoc/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files... [javadoc] copying file /home/savior/development/workspaces/java/lucene-gsoc/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/hitcollectionbench.jpg to directory /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files... [javadoc] copying file /home/savior/development/workspaces/java/lucene-gsoc/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files... [javadoc] generating /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/serialized-form.html... [javadoc] copying file /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/stylesheet+prettify.css... [javadoc] building index for all the packages and classes... [javadoc] building index for all classes... [javadoc] generating /home/savior/development/workspaces/java/lucene-gsoc/lucene/build/docs/api/all/help-doc.html... [javadoc] 4 warnings",
        "label": 40
    },
    {
        "text": "support protected words in stemming tokenfilters this is from lucene-1515 i propose that all stemming tokenfilters have an 'exclusion set' that bypasses any stemming for words in this set. some stemming tokenfilters have this, some do not. this would be one way for karl to implement his new swedish stemmer (as a text file of ignore words). additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality. finally, i think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming. as an alternative design i considered a case where we generalized this to chararraymap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. but i think this is too expert, could be its own filter, and the only example of this i can find is in the dutch stemmer. so i think we should just provide ignore with chararrayset, but if you feel otherwise please comment.",
        "label": 53
    },
    {
        "text": "testparalleltermenum fails with sep codec reproduceable in the 'preflexfixes' branch (since we test all codecs there) with: ant test-core -dtestcase=testparalleltermenum -dtests.codec=sep but i think there are probably more tests like this that have only been run with standard and we might find more like this. i don't think this should block lucene-2554.     [junit] testsuite: org.apache.lucene.index.testparalleltermenum     [junit] testcase: test1(org.apache.lucene.index.testparalleltermenum):      caused an error     [junit] read past eof     [junit] java.io.ioexception: read past eof     [junit]     at org.apache.lucene.store.bufferedindexinput.refill(bufferedindexinput.java:154)     [junit]     at org.apache.lucene.store.bufferedindexinput.readbyte(bufferedindexinput.java:39)     [junit]     at org.apache.lucene.store.datainput.readvint(datainput.java:86)     [junit]     at org.apache.lucene.index.codecs.sep.singleintindexinput$reader.next(singleintindexinput.java:64)     [junit]     at org.apache.lucene.index.codecs.sep.seppostingsreaderimpl$sepdocsenum.nextdoc(seppostingsreaderimpl.java:316)     [junit]     at org.apache.lucene.index.testparalleltermenum.test1(testparalleltermenum.java:188)     [junit]     at org.apache.lucene.util.lucenetestcase.runbare(lucenetestcase.java:316)     [junit]     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0.009 sec     [junit]     [junit] ------------- standard output ---------------     [junit] note: random codec of testcase 'test1' was: sep     [junit] ------------- ---------------- ---------------",
        "label": 33
    },
    {
        "text": "reduce memory allocated by compressingstoredfieldswriter to write large strings in solr-7927, i am trying to reduce the memory required to index very large documents (between 10 to 100mb) and one of the places which allocate a lot of heap is the utf8 encoding in compressingstoredfieldswriter. the same problem existed in javabincodec and we reduced its memory allocation by falling back to a double pass approach in solr-7971 when the utf8 size of the string is greater than 64kb. i propose to make the same changes to compressingstoredfieldswriter as we made to javabincodec in solr-7971.",
        "label": 44
    },
    {
        "text": "indexwriter might delete dv update files if addindices are invovled the attached test fails with this output: /library/java/javavirtualmachines/jdk-9.0.1.jdk/contents/home/bin/java -ea -djava.security.egd=file:/dev/./urandom -didea.test.cyclic.buffer.size=1048576 -dfile.encoding=utf-8 -classpath \"/applications/intellij idea.app/contents/lib/idea_rt.jar:/applications/intellij idea.app/contents/plugins/junit/lib/junit-rt.jar:/applications/intellij idea.app/contents/plugins/junit/lib/junit5-rt.jar:/users/simonw/projects/lucene-solr/idea-build/lucene/test-framework/classes/test:/users/simonw/projects/lucene-solr/idea-build/lucene/test-framework/classes/java:/users/simonw/projects/lucene-solr/lucene/test-framework/lib/junit-4.10.jar:/users/simonw/projects/lucene-solr/lucene/test-framework/lib/randomizedtesting-runner-2.5.3.jar:/users/simonw/projects/lucene-solr/idea-build/lucene/codecs/classes/java:/users/simonw/projects/lucene-solr/idea-build/lucene/core/classes/java:/users/simonw/projects/lucene-solr/idea-build/lucene/core/classes/test\" com.intellij.rt.execution.junit.junitstarter -ideversion5 -junit4 org.apache.lucene.index.testaddindexes,testaddindexesdvupdate ifd 0 [2018-04-06t19:27:27.176036z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: current segments file is \"segments_1\"; deletionpolicy=org.apache.lucene.index.keeponlylastcommitdeletionpolicy@27cf18f0 ifd 0 [2018-04-06t19:27:27.188066z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: load commit \"segments_1\" ifd 0 [2018-04-06t19:27:27.189800z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: seg=_0 set nextwritedelgen=2 vs current=1 ifd 0 [2018-04-06t19:27:27.190053z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: removing unreferenced file \"_0_1_lucene70_0.dvd\" ifd 0 [2018-04-06t19:27:27.190224z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: removing unreferenced file \"_0_1.fnm\" ifd 0 [2018-04-06t19:27:27.190371z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: removing unreferenced file \"_0_1_lucene70_0.dvm\" ifd 0 [2018-04-06t19:27:27.190528z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: delete [_0_1_lucene70_0.dvd, _0_1.fnm, _0_1_lucene70_0.dvm] ifd 0 [2018-04-06t19:27:27.192558z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: now checkpoint \"_0(8.0.0):c1:fieldinfosgen=1:dvgen=1\" [1 segments ; iscommit = false] ifd 0 [2018-04-06t19:27:27.192806z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: 0 msec to checkpoint iw 0 [2018-04-06t19:27:27.193012z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: init: create=false iw 0 [2018-04-06t19:27:27.193428z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]:  dir=mockdirectorywrapper(ramdirectory@79d3c690 lockfactory=org.apache.lucene.store.singleinstancelockfactory@795a0c8b) index=_0(8.0.0):c1:fieldinfosgen=1:dvgen=1 version=8.0.0 analyzer=org.apache.lucene.analysis.mockanalyzer rambuffersizemb=16.0 maxbuffereddocs=503 mergedsegmentwarmer=null delpolicy=org.apache.lucene.index.keeponlylastcommitdeletionpolicy commit=null openmode=create_or_append similarity=org.apache.lucene.search.similarities.assertingsimilarity mergescheduler=org.apache.lucene.index.serialmergescheduler@2f3feff6 codec=fastdecompressioncompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fast_decompression, chunksize=8, maxdocsperchunk=6, blocksize=201), termvectorsformat=compressingtermvectorsformat(compressionmode=fast_decompression, chunksize=8, blocksize=201)) infostream=org.apache.lucene.util.printstreaminfostream mergepolicy=[tieredmergepolicy: maxmergeatonce=41, maxmergeatonceexplicit=44, maxmergedsegmentmb=6.255859375, floorsegmentmb=0.38671875, forcemergedeletespctallowed=4.456652110760543, segmentspertier=31.0, maxcfssegmentsizemb=8.796093022207999e12, nocfsratio=0.877330376985384 indexerthreadpool=org.apache.lucene.index.documentswriterperthreadpool@257ebcdb readerpooling=true perthreadhardlimitmb=1945 usecompoundfile=true commitonclose=true indexsort=null checkpendingflushonupdate=true softdeletesfield=null writer=org.apache.lucene.index.indexwriter@17b77ed5 iw 0 [2018-04-06t19:27:27.194085z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: mmapdirectory.unmap_supported=true iw 0 [2018-04-06t19:27:27.194347z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: now flush at close iw 0 [2018-04-06t19:27:27.194630z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]:   start flush: applyalldeletes=true iw 0 [2018-04-06t19:27:27.194864z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]:   index before flush _0(8.0.0):c1:fieldinfosgen=1:dvgen=1 dw 0 [2018-04-06t19:27:27.194964z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: startfullflush dw 0 [2018-04-06t19:27:27.195096z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd] finishfullflush success=true iw 0 [2018-04-06t19:27:27.195181z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: now apply all deletes for all segments buffered updates bytesused=0 reader pool bytesused=0 bd 0 [2018-04-06t19:27:27.195260z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: waitapply: no deletes to apply iw 0 [2018-04-06t19:27:27.195330z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: waitformerges iw 0 [2018-04-06t19:27:27.195385z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: waitformerges done iw 0 [2018-04-06t19:27:27.195448z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: commit: start iw 0 [2018-04-06t19:27:27.195504z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: commit: enter lock iw 0 [2018-04-06t19:27:27.195560z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: commit: now prepare iw 0 [2018-04-06t19:27:27.195779z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: preparecommit: flush iw 0 [2018-04-06t19:27:27.195873z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]:   index before flush _0(8.0.0):c1:fieldinfosgen=1:dvgen=1 dw 0 [2018-04-06t19:27:27.195954z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: startfullflush iw 0 [2018-04-06t19:27:27.196076z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: now apply all deletes for all segments buffered updates bytesused=0 reader pool bytesused=0 bd 0 [2018-04-06t19:27:27.196170z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: waitapply: no deletes to apply dw 0 [2018-04-06t19:27:27.196299z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd] finishfullflush success=true iw 0 [2018-04-06t19:27:27.196382z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: startcommit(): start iw 0 [2018-04-06t19:27:27.196455z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]:   skip startcommit(): no changes pending iw 0 [2018-04-06t19:27:27.196542z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: commit: pendingcommit == null; skip iw 0 [2018-04-06t19:27:27.196717z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: commit: took 0.8 msec iw 0 [2018-04-06t19:27:27.196791z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: commit: done iw 0 [2018-04-06t19:27:27.196950z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: rollback iw 0 [2018-04-06t19:27:27.197032z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: all running merges have aborted iw 0 [2018-04-06t19:27:27.197326z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: rollback: done finish merges dw 0 [2018-04-06t19:27:27.197411z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: abort dw 0 [2018-04-06t19:27:27.197486z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: done abort success=true iw 0 [2018-04-06t19:27:27.197605z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: rollback: infos=_0(8.0.0):c1:fieldinfosgen=1:dvgen=1 ifd 0 [2018-04-06t19:27:27.197706z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: now checkpoint \"_0(8.0.0):c1:fieldinfosgen=1:dvgen=1\" [1 segments ; iscommit = false] ifd 0 [2018-04-06t19:27:27.197837z; test-testaddindexes.testaddindexesdvupdate-seed#[9f04ee6b720b6bfd]]: 0 msec to checkpoint java.nio.file.nosuchfileexception: _0_1.fnm in dir=ramdirectory@79d3c690 lockfactory=org.apache.lucene.store.singleinstancelockfactory@795a0c8b  at __randomizedtesting.seedinfo.seed([9f04ee6b720b6bfd:e6ec23304e67e247]:0)  at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:750)  at org.apache.lucene.store.directory.openchecksuminput(directory.java:119)  at org.apache.lucene.store.mockdirectorywrapper.openchecksuminput(mockdirectorywrapper.java:1072)  at org.apache.lucene.codecs.lucene60.lucene60fieldinfosformat.read(lucene60fieldinfosformat.java:113)  at org.apache.lucene.index.indexwriter.readfieldinfos(indexwriter.java:1207)  at org.apache.lucene.index.indexwriter.getfieldnumbermap(indexwriter.java:1227)  at org.apache.lucene.index.indexwriter.<init>(indexwriter.java:1114)  at org.apache.lucene.index.testaddindexes.testaddindexesdvupdate(testaddindexes.java:1364)  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.base/java.lang.reflect.method.invoke(method.java:564)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1737)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:934)  at com.carrotsearch.randomizedtesting.randomizedrunner$9.evaluate(randomizedrunner.java:970)  at com.carrotsearch.randomizedtesting.randomizedrunner$10.evaluate(randomizedrunner.java:984)  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:49)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:64)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:47)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:368)  at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:817)  at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:468)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:943)  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:829)  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:879)  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:890)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:41)  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:40)  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:40)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:53)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:47)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:64)  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:54)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:368)  at java.base/java.lang.thread.run(thread.java:844) note: reproduce with: ant test  -dtestcase=testaddindexes -dtests.method=testaddindexesdvupdate -dtests.seed=9f04ee6b720b6bfd -dtests.slow=true -dtests.badapples=true -dtests.locale=es-hn -dtests.timezone=asia/singapore -dtests.asserts=true -dtests.file.encoding=utf-8 note: test params are: codec=fastdecompressioncompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fast_decompression, chunksize=8, maxdocsperchunk=6, blocksize=201), termvectorsformat=compressingtermvectorsformat(compressionmode=fast_decompression, chunksize=8, blocksize=201)), sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@55f3c81b), locale=es-hn, timezone=asia/singapore note: mac os x 10.13.3 x86_64/oracle corporation 9.0.1 (64-bit)/cpus=4,threads=1,free=231833088,total=268435456 note: all tests run in this jvm: [testaddindexes]",
        "label": 33
    },
    {
        "text": "two same new field with and without term vector make an illegalstateexception on a empty index, adding a document with two fields with the same name but with different term vector option fail. the field with termvector.with_positions_offsets is correctly indexed, as the offset are correclty extracted. the field with termvector.no is not. the termvectorswriter tries to add offset info given to the data of the filedinfo from the \"fnm\" file, but the documentwriter didn't prepared offset datas as it gets its info from the field itself, not from the fieldinfo. attaching a patch with a test. the test without the fix make this stack trace : java.lang.illegalstateexception: trying to write offsets that are null! at org.apache.lucene.index.termvectorswriter.writefield(termvectorswriter.java:311) at org.apache.lucene.index.termvectorswriter.closefield(termvectorswriter.java:142) at org.apache.lucene.index.termvectorswriter.closedocument(termvectorswriter.java:100) at org.apache.lucene.index.termvectorswriter.close(termvectorswriter.java:240) at org.apache.lucene.index.documentwriter.writepostings(documentwriter.java:365) at org.apache.lucene.index.documentwriter.adddocument(documentwriter.java:114) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:618) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:601) at org.apache.lucene.index.testdocumentwriter.testtermvector(testdocumentwriter.java:147)",
        "label": 12
    },
    {
        "text": "testindexwriterwiththreads hung in documentswriterflushcontrol  i've hit this when trying out the new runner with suite timeouts. and voila, it works output below. suite: org.apache.lucene.index.testindexwriterwiththreads    > (@afterclass output)   2> 31.7.2012 15:28:45 com.carrotsearch.randomizedtesting.threadleakcontrol$2 evaluate   2> warning: suite execution timed out: org.apache.lucene.index.testindexwriterwiththreads   2> ==== jstack at approximately timeout time ====   2> \"thread-54\" id=98 waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a owned by \"thread-53\" id=97   2>  at sun.misc.unsafe.park(native method)   2>  - waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a   2>  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>  at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>  at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>  at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    2> \"thread-53\" id=97 waiting on org.apache.lucene.index.documentswriterflushcontrol@5d4d1093   2>  at java.lang.object.wait(native method)   2>  - waiting on org.apache.lucene.index.documentswriterflushcontrol@5d4d1093   2>  at java.lang.object.wait(object.java:485)   2>  at org.apache.lucene.index.documentswriterflushcontrol.waitforflush(documentswriterflushcontrol.java:221)   2>  at org.apache.lucene.index.documentswriter.abort(documentswriter.java:233)   2>  - locked org.apache.lucene.index.documentswriter@548ae184   2>  at org.apache.lucene.index.indexwriter.rollbackinternal(indexwriter.java:1845)   2>  at org.apache.lucene.index.indexwriter.rollback(indexwriter.java:1813)   2>  - locked java.lang.object@69dbcc78   2>  at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:537)   2>  locked synchronizers:   2>  - java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a   2>    2> \"thread-52\" id=96 waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a owned by \"thread-53\" id=97   2>  at sun.misc.unsafe.park(native method)   2>  - waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a   2>  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>  at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>  at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>  at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    2> \"thread-51\" id=95 waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a owned by \"thread-53\" id=97   2>  at sun.misc.unsafe.park(native method)   2>  - waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a   2>  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>  at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>  at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>  at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    2> \"thread-50\" id=94 waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a owned by \"thread-53\" id=97   2>  at sun.misc.unsafe.park(native method)   2>  - waiting on java.util.concurrent.locks.reentrantlock$nonfairsync@2103217a   2>  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>  at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>  at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>  at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    2> \"test-testindexwriterwiththreads.testrollbackandcommitwiththreads-seed#[844703faaafa8855]\" id=87 waiting on org.apache.lucene.index.testindexwriterwiththreads$1@358ce15a   2>  at java.lang.object.wait(native method)   2>  - waiting on org.apache.lucene.index.testindexwriterwiththreads$1@358ce15a   2>  at java.lang.thread.join(thread.java:1143)   2>  at java.lang.thread.join(thread.java:1196)   2>  at org.apache.lucene.index.testindexwriterwiththreads.testrollbackandcommitwiththreads(testindexwriterwiththreads.java:590)   2>  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)   2>  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)   2>  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)   2>  at java.lang.reflect.method.invoke(method.java:597)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1569)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:81)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:739)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:775)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:789)   2>  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)   2>  at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)   2>  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   2>  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   2>  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)   2>  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)   2>  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   2>  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:345)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:763)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:424)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:748)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:650)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:684)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:695)   2>  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   2>  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)   2>  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   2>  at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)   2>  at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)   2>  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)   2>  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   2>  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)   2>  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)   2>  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:345)   2>  at java.lang.thread.run(thread.java:619)   2>    2> \"suite-testindexwriterwiththreads-seed#[844703faaafa8855]\" id=86 runnable   2>  at sun.management.threadimpl.dumpthreads0(native method)   2>  at sun.management.threadimpl.dumpallthreads(threadimpl.java:374)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol.formatthreadstacksfull(threadleakcontrol.java:636)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol.access$800(threadleakcontrol.java:60)   2>  at com.carrotsearch.randomizedtesting.threadleakcontrol$2.evaluate(threadleakcontrol.java:379)   2>  - locked java.lang.object@6e7b52bf   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:557)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.access$200(randomizedrunner.java:81)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner$1.run(randomizedrunner.java:491)   2>    2> \"signal dispatcher\" id=4 runnable   2>    2> \"finalizer\" id=3 waiting on java.lang.ref.referencequeue$lock@6275b243   2>  at java.lang.object.wait(native method)   2>  - waiting on java.lang.ref.referencequeue$lock@6275b243   2>  at java.lang.ref.referencequeue.remove(referencequeue.java:118)   2>  at java.lang.ref.referencequeue.remove(referencequeue.java:134)   2>  at java.lang.ref.finalizer$finalizerthread.run(finalizer.java:159)   2>    2> \"reference handler\" id=2 waiting on java.lang.ref.reference$lock@1d0a7f35   2>  at java.lang.object.wait(native method)   2>  - waiting on java.lang.ref.reference$lock@1d0a7f35   2>  at java.lang.object.wait(object.java:485)   2>  at java.lang.ref.reference$referencehandler.run(reference.java:116)   2>    2> \"main\" id=1 waiting on com.carrotsearch.randomizedtesting.randomizedrunner$1@3f1074b4   2>  at java.lang.object.wait(native method)   2>  - waiting on com.carrotsearch.randomizedtesting.randomizedrunner$1@3f1074b4   2>  at java.lang.thread.join(thread.java:1143)   2>  at java.lang.thread.join(thread.java:1196)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:501)   2>  at com.carrotsearch.randomizedtesting.randomizedrunner.run(randomizedrunner.java:398)   2>  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.execute(slavemain.java:153)   2>  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.main(slavemain.java:246)   2>  at com.carrotsearch.ant.tasks.junit4.slave.slavemainsafe.main(slavemainsafe.java:12)   2>    2> ^^==============================================   2>    2> 31.7.2012 15:29:05 com.carrotsearch.randomizedtesting.threadleakcontrol checkthreadleaks   2> severe: 6 threads leaked from suite scope at org.apache.lucene.index.testindexwriterwiththreads:    2>    1) thread[id=87, name=test-testindexwriterwiththreads.testrollbackandcommitwiththreads-seed#[844703faaafa8855], state=waiting, group=tgrp-testindexwriterwiththreads]   2>         at java.lang.object.wait(native method)   2>         at java.lang.thread.join(thread.java:1143)   2>         at java.lang.thread.join(thread.java:1196)   2>         at org.apache.lucene.index.testindexwriterwiththreads.testrollbackandcommitwiththreads(testindexwriterwiththreads.java:590)   2>         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)   2>         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)   2>         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)   2>         at java.lang.reflect.method.invoke(method.java:597)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1569)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:81)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:739)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:775)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:789)   2>         at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)   2>         at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)   2>         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   2>         at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   2>         at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)   2>         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)   2>         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:345)   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:763)   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:424)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:748)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:650)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:684)   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:695)   2>         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)   2>         at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)   2>         at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)   2>         at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)   2>         at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)   2>         at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)   2>         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)   2>         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70)   2>         at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:345)   2>         at java.lang.thread.run(thread.java:619)   2>    2) thread[id=96, name=thread-52, state=waiting, group=tgrp-testindexwriterwiththreads]   2>         at sun.misc.unsafe.park(native method)   2>         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>         at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>         at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>         at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    3) thread[id=94, name=thread-50, state=waiting, group=tgrp-testindexwriterwiththreads]   2>         at sun.misc.unsafe.park(native method)   2>         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>         at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>         at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>         at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    4) thread[id=98, name=thread-54, state=waiting, group=tgrp-testindexwriterwiththreads]   2>         at sun.misc.unsafe.park(native method)   2>         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>         at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>         at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>         at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    5) thread[id=95, name=thread-51, state=waiting, group=tgrp-testindexwriterwiththreads]   2>         at sun.misc.unsafe.park(native method)   2>         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:747)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:778)   2>         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)   2>         at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)   2>         at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)   2>         at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:532)   2>    6) thread[id=97, name=thread-53, state=waiting, group=tgrp-testindexwriterwiththreads]   2>         at java.lang.object.wait(native method)   2>         at java.lang.object.wait(object.java:485)   2>         at org.apache.lucene.index.documentswriterflushcontrol.waitforflush(documentswriterflushcontrol.java:221)   2>         at org.apache.lucene.index.documentswriter.abort(documentswriter.java:233)   2>         at org.apache.lucene.index.indexwriter.rollbackinternal(indexwriter.java:1845)   2>         at org.apache.lucene.index.indexwriter.rollback(indexwriter.java:1813)   2>         at org.apache.lucene.index.testindexwriterwiththreads$1.run(testindexwriterwiththreads.java:537)   2> 31.7.2012 15:29:05 com.carrotsearch.randomizedtesting.threadleakcontrol trytointerruptall   2> info: starting to interrupt leaked threads:   2>    1) thread[id=87, name=test-testindexwriterwiththreads.testrollbackandcommitwiththreads-seed#[844703faaafa8855], state=waiting, group=tgrp-testindexwriterwiththreads]   2>    2) thread[id=96, name=thread-52, state=waiting, group=tgrp-testindexwriterwiththreads]   2>    3) thread[id=94, name=thread-50, state=waiting, group=tgrp-testindexwriterwiththreads]   2>    4) thread[id=98, name=thread-54, state=waiting, group=tgrp-testindexwriterwiththreads]   2>    5) thread[id=95, name=thread-51, state=waiting, group=tgrp-testindexwriterwiththreads]   2>    6) thread[id=97, name=thread-53, state=waiting, group=tgrp-testindexwriterwiththreads]   2> 31.7.2012 15:29:07 com.carrotsearch.randomizedtesting.threadleakcontrol trytointerruptall   2> info: all leaked threads terminated.   1> org.apache.lucene.store.lockobtainfailedexception: lock obtain timed out: org.apache.lucene.store.singleinstancelock@7685706f: write.lock   1>  at org.apache.lucene.store.lock.obtain(lock.java:84)   1>  at org.apache.lucene.index.indexwriter.<init>(indexwriter.java:595)   1>  at org.apache.lucene.index.testindexwriterwiththreads$delayedindexandcloserunnable.run(testindexwriterwiththreads.java:490)   2> note: test params are: codec=lucene40: {field=postingsformat(name=memory dopackfst= true), docid=postingsformat(name=mocksep), body=postingsformat(name=memory dopackfst= true), title=postingsformat(name=lucene40withords), titletokenized=postingsformat(name=nestedpulsing), date=postingsformat(name=memory dopackfst= true)}, sim=randomsimilarityprovider(querynorm=false,coord=false): {field=ib spl-d3(800.0), body=dfr i(ne)2, titletokenized=ib ll-l3(800.0)}, locale=cs_cz, timezone=america/anguilla   2> note: linux 2.6.32-38-server amd64/sun microsystems inc. 1.6.0_20 (64-bit)/cpus=4,threads=2,free=122913408,total=155123712   2> note: all tests run in this jvm: [testfieldmaskingspanquery, testdocvaluestypecompatibility, testtermscorer, testsnapshotdeletionpolicy, testlongpostings, testsort, testindexwriter, testcopybytes, testdocumentwriter, testdatefilter, testparallelreaderemptyindex, testdatetools, testmergeschedulerexternal, testprefixinbooleanquery, testindexwriterwiththreads]   1>    2>  error   0.00s j0 | testindexwriterwiththreads (suite)    > throwable #1: java.lang.exception: suite timeout exceeded (>= 600000 msec).    >  at __randomizedtesting.seedinfo.seed([844703faaafa8855]:0)    > completed on j0 in 622.52s, 3 tests, 1 error <<< failures!",
        "label": 46
    },
    {
        "text": "enable docvalues by default for every codec currently docvalues are enable with a wrapper codec so each codec which needs docvalues must be wrapped by docvaluescodec. the docvalues writer and reader should be moved to codec to be enabled by default.",
        "label": 46
    },
    {
        "text": "remove remaining  author references $ find . -name *.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/ @author.*//'",
        "label": 38
    },
    {
        "text": "simplefslockfactory  access denied  on windows  this happened twice in jenkins: [lockstresstest2] exception in thread \"main\" java.io.ioexception: access is denied [lockstresstest2] at java.io.winntfilesystem.createfileexclusively(native method) [lockstresstest2] at java.io.file.createnewfile(file.java:1012) [lockstresstest2] at org.apache.lucene.store.simplefslock.obtain(simplefslockfactory.java:135) my windows machine got struck by lightning, so i cannot fix this easily.",
        "label": 53
    },
    {
        "text": "standardcodec sometimes supplies skip pointers past eof pretty sure this is 4.0-only: i added an assertion, the test to reproduce is: ant test-core -dtestcase=testpayloadnearquery -dtestmethod=testminfunction -dtests.seed=4841190615781133892:3888521539169738727 -dtests.multiplier=3     [junit] testcase: testminfunction(org.apache.lucene.search.payloads.testpayloadnearquery):  failed     [junit] invalid skip pointer: 404, length=337     [junit] junit.framework.assertionfailederror: invalid skip pointer: 404, length=337     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1127)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1059)     [junit]     at org.apache.lucene.index.codecs.multilevelskiplistreader.init(multilevelskiplistreader.java:176)     [junit]     at org.apache.lucene.index.codecs.standard.defaultskiplistreader.init(defaultskiplistreader.java:50)     [junit]     at org.apache.lucene.index.codecs.standard.standardpostingsreader$segmentdocsandpositionsandpayloadsenum.advance(standardpostingsreader.java:742)     [junit]     at org.apache.lucene.search.spans.termspans.skipto(termspans.java:72)",
        "label": 33
    },
    {
        "text": "ioexeception can cause loss of data due to premature segment deletion if you hit an ioexception, e.g., disk full, while making a cfs from its constituent parts, you may not be able to rollback to the before-merge process. this happens via addindexes. i don't have a nice easy test for this; generating ioes ain't so easy. but it does happen in the patch for the factored merge policy with the existing tests because the pseudo-randomly generated ioes fall in a different place.",
        "label": 33
    },
    {
        "text": "smoketestrelease should test solr example i think most anyone reviewing the solr artifacts will do this, so really the rm has to do it manually: but we can test 'ant example' from the source dist + java -jar start.jar from solr/example (or/and 'ant run-example'), and also java -jar start.jar from the binary distribution. some basic checks we can do are to run the test_utf8.sh, and to index the example docs (post.jar/post.sh the docs in exampledocs) then do a search.",
        "label": 33
    },
    {
        "text": "long overflow in lucenexxskipwriter can corrupt skip data i've been iterating with tom on this corruption that checkindex detects in his rather large index (720 gb in a single segment):  java -xmx16g -xms16g -cp $jar -ea:org.apache.lucene... org.apache.lucene.index.checkindex /xxxx/shards/4/core-1/data/test_index -verbose 2>&1 |tee -a shard4_reoptimizednewjava opening index @ /htsolr/lss-reindex/shards/4/core-1/data/test_index segments file=segments_e numsegments=1 version=4.10.2 format= userdata={committimemsec=1421479358825}   1 of 1: name=_8m8 doccount=1130856     version=4.10.2     codec=lucene410     compound=false     numfiles=10     size (mb)=719,967.32     diagnostics = {timestamp=1421437320935, os=linux, os.version=2.6.18-400.1.1.el5, mergefactor=2, source=merge, lucene.version=4.10.2, os.arch=amd64, mergemaxnumsegments=1, java.version=1.7.0_71, java.vendor=oracle corporation}     no deletions     test: open reader.........ok     test: check integrity.....ok     test: check live docs.....ok     test: fields..............ok [80 fields]     test: field norms.........ok [23 fields]     test: terms, freq, prox...error: java.lang.assertionerror: -96 java.lang.assertionerror: -96         at org.apache.lucene.codecs.lucene41.forutil.skipblock(forutil.java:228)         at org.apache.lucene.codecs.lucene41.lucene41postingsreader$blockdocsandpositionsenum.skippositions(lucene41postingsreader.java:925)         at org.apache.lucene.codecs.lucene41.lucene41postingsreader$blockdocsandpositionsenum.nextposition(lucene41postingsreader.java:955)         at org.apache.lucene.index.checkindex.checkfields(checkindex.java:1100)         at org.apache.lucene.index.checkindex.testpostings(checkindex.java:1357)         at org.apache.lucene.index.checkindex.checkindex(checkindex.java:655)         at org.apache.lucene.index.checkindex.main(checkindex.java:2096)     test: stored fields.......ok [67472796 total field count; avg 59.665 fields per doc]     test: term vectors........ok [0 total vector count; avg 0 term/freq vector fields per doc]     test: docvalues...........ok [0 docvalues fields; 0 binary; 0 numeric; 0 sorted; 0 sorted_numeric; 0 sorted_set] failed     warning: fixindex() would remove reference to this segment; full exception: java.lang.runtimeexception: term index test failed         at org.apache.lucene.index.checkindex.checkindex(checkindex.java:670)         at org.apache.lucene.index.checkindex.main(checkindex.java:2096) warning: 1 broken segments (containing 1130856 documents) detected warning: would write new segments file, and 1130856 documents would be lost, if -fix were specified and rob spotted long -> int casts in our skip list writers that look like they could cause such corruption if a single high-freq term with many positions required > 2.1 gb to write its positions into .pos.",
        "label": 33
    },
    {
        "text": "testnrtthreads test failure hit a fail in testnrtthreads running tests over and over:",
        "label": 33
    },
    {
        "text": "indexreader isindexcurrent false   indexreader reopen    still index not current i found a strange error occurring with indexreader.reopen. it is not always reproduceable, it only happens sometimes, but strangely on all my computers with different platforms at the same time. maybe has something to to with the timestamp used in index versions. i have a search server using an indexreader, that is openend in webapp startup and should stay open. every half an hour this web application checks, if the index is still current using indexreader.iscurrent(). when a parallel job that indexes documents (in another virtual machine) and modifies the indexes, iscurrent() return true. the half-hourly cron-job then uses indexreader.reopen() to reopen the index. but sometimes, directly after reopen() the index is still not current (and no updates occur). again calling reopen does not change it, too. searching on the index shows all new/updated documents, but iscurrent() still return false. the problem with this is, that now the index is reopened all the time, because the detection of a current index does not work any more. i have now a workaround in my code to handle this: after calling indexreader.reopen(), i test for indexreader.iscurrent(), and if not, i close it hard and open a new instance. most times indexreader.reopen works correct, but sometimes this error occurs. looking into the code of reopen(), i realized, that there is some extra check, if the index has modifications, and if yes the reopen call returns the original reader (this maybe the problem i have). but the indexreader is only used for searching, no updates occur. my questions: why is there this check for modifications in reopen()? why does this happen only at certain times on all my servers with different platforms? i want to use reopen, because in future, when the new fieldcache will be reopen-aware and does not everytime rebuild the full cache, it will be very important, to have this fixed. at the moment, i have no problem with the case, that reopen may fail and i have to do a rough reopen.",
        "label": 33
    },
    {
        "text": "mmapdirectory chunking is buggy mmapdirectory uses chunking with multimmapindexinput. because java's bytebuffer uses an int to address the values, it's necessary to access a file > integer.max_value in size using multiple byte buffers. but i noticed from the clover report the entire multimmapindexinput class is completely untested: no surprise since all tests make tiny indexes.",
        "label": 40
    },
    {
        "text": "int overflow in bytebuffersdatainput slicebufferlist hi, once i fixed the bug here, https://issues.apache.org/jira/browse/lucene-8624, i encountered another integer overflow error in bytebuffersdatainput: exception in thread \"lucene merge thread #1540\" exception in thread \"main\" org.apache.lucene.index.mergepolicy$mergeexception: java.lang.arithmeticexception: integer overflow at org.apache.lucene.index.concurrentmergescheduler.handlemergeexception(concurrentmergescheduler.java:705) at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:685) caused by: java.lang.arithmeticexception: integer overflow at java.lang.math.tointexact(math.java:1011) at org.apache.lucene.store.bytebuffersdatainput.slicebufferlist(bytebuffersdatainput.java:299) at org.apache.lucene.store.bytebuffersdatainput.slice(bytebuffersdatainput.java:223) at org.apache.lucene.store.bytebuffersindexinput.clone(bytebuffersindexinput.java:186) at org.apache.lucene.store.bytebuffersdirectory$fileentry.openinput(bytebuffersdirectory.java:254) at org.apache.lucene.store.bytebuffersdirectory.openinput(bytebuffersdirectory.java:223) at org.apache.lucene.store.filterdirectory.openinput(filterdirectory.java:100) at org.apache.lucene.store.filterdirectory.openinput(filterdirectory.java:100) at org.apache.lucene.store.filterdirectory.openinput(filterdirectory.java:100) at org.apache.lucene.store.directory.openchecksuminput(directory.java:157) at org.apache.lucene.codecs.lucene50.lucene50compoundformat.write(lucene50compoundformat.java:89) at org.apache.lucene.index.indexwriter.createcompoundfile(indexwriter.java:5004) at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4517) at org.apache.lucene.index.indexwriter.merge(indexwriter.java:4075) at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:626) at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:663) the exception is caused by a math.tointexact in slicebufferlist in bytebuffersdatainput. bytebuffersdatainput.java private static list<bytebuffer> slicebufferlist(list<bytebuffer> buffers, long offset, long length) {  ensureassumptions(buffers);  if (buffers.size() == 1) {  bytebuffer cloned = buffers.get(0).asreadonlybuffer();  cloned.position(math.tointexact(cloned.position() + offset));  cloned.limit(math.tointexact(length + cloned.position()));  return arrays.aslist(cloned);  } else {  long absstart = buffers.get(0).position() + offset;  long absend = math.tointexact(absstart + length);  // throws integer overflow  ...   removing the math.tointexact works but i'm not sure if the logic will still be right since absend is used to calculate endoffset after a few lines: int endoffset = (int) absend & blockmask;   thanks,",
        "label": 11
    },
    {
        "text": "maven artifacts for lucene are not stored in the correct path hello, i would like to use the maven artifacts for lucene 4.0 produced by the hudson build machine. the artifacts are correctly produced (http://hudson.zones.apache.org/hudson/view/lucene/job/lucene-trunk/lastsuccessfulbuild/artifact/maven_artifacts/lucene/). however, the artifacts which should be stored under the path \"org/apache/lucene/\" are currently stored under \"lucene\" which prevents a project using maven to correctly download the lucene 4.0 artifacts. thanks again for your help.",
        "label": 53
    },
    {
        "text": "remove deprecated classes from spatial spatial has not been released, so we can remove the deprecated classes",
        "label": 33
    },
    {
        "text": "docvalues infinite loop caused by   a call to getminvalue   getmaxvalue   getaveragevalue org.apache.lucene.search.function.docvalues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. a call to one of the methods will result in an infinite loop. the internal counter is not incremented. i added a testcase, javadoc and a slightly different implementation to it. i guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway. i changed the return value of all of those methods to float.nan if the docvalues implementation does not contain any values. it might be considerable to fix this in 2.4.2 and 2.3.3",
        "label": 29
    },
    {
        "text": "fastvectorhighlighter  enable fraglistbuilder and fragmentsbuilder to be set per field override ",
        "label": 26
    },
    {
        "text": "add nrt support to facets currently lucenetaxonomyreader does not support nrt - i.e., on changes to lucenetaxonomywriter, you cannot have the reader updated, like indexreader/writer. in order to do that we need to do the following: add ctor to lucenetaxonomyreader to allow you to instantiate it with lucenetaxonomywriter. add api to lucenetaxonomywriter to expose its internal indexreader change ltr.refresh() to return an ltr, rather than void. this is actually not strictly related to that issue, but since we'll need to modify refresh() impl, i think it'll be good to change its api as well. since all of facet api is @lucene.experimental, no backwards issues here (and the sooner we do it, the better).",
        "label": 43
    },
    {
        "text": "remove references to contribs that dont exist from docs pretty sure this is because i forgot to come around and re-run forrest... i think in the actual source tree its fixed and we just need a forrest regen. i didn't have java 5 on my mac and forgot that you can use forrest now with java 6.",
        "label": 40
    },
    {
        "text": "move 'good' contrib queries classes to queries module with the queries module now filled with the functionquery stuff, we should look at closing down contrib/queries. while not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere. heres my proposed plan: similar.* -> suggest module regex.* -> queries module booleanfilter -> queries module under .filters package boostingquery -> queries module chainedfilter -> queries module under .filters package duplicatefilter -> queries module under .filters package fieldcacherewritemethod -> this doesn't belong in this contrib or the queries module. i think we should push it to contrib/misc for the time being. it seems to have quite a few constraints on when its useful. if indeed constant_score_auto rewrite is better, then i dont see a purpose for it. filterclause -> class inside booleanfilter fuzzylikethisquery -> suggest module. this class seems a mess with its similarity hardcoded. with all that said, it does seem to do what it claims and with some cleanup, it could be good. termsfilter -> queries module under .filters package slowcollated* -> they can stay in the module till we have a better place to nuke them. one of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module. but that seems unavoidable at this stage.",
        "label": 7
    },
    {
        "text": "occasional mergeexception while indexing teststressindexing2.testmulticonfig occasionally hits merge exceptions",
        "label": 33
    },
    {
        "text": "niofsdirectory  simplefsdirectory  others  don't properly handle valid file and filechannel read conditions around line 190 of niofsdirectory, the loop to read in bytes doesn't properly handle the -1 condition that can be returned from filechannel.read(). if it returns -1, then it will move the file pointer back and you will enter an infinite loop. simplefsdirectory displays the same characteristics, although i have only seen the issue on niofsdirectory. the code in question from niofsdirectory: try {         while (readlength > 0) {           final int limit;           if (readlength > chunksize) {             // lucene-1566 - work around jvm bug by breaking             // very large reads into chunks             limit = readoffset + chunksize;           } else {             limit = readoffset + readlength;           }           bb.limit(limit);           int i = channel.read(bb, pos);           pos += i;           readoffset += i;           readlength -= i;         }",
        "label": 15
    },
    {
        "text": "npe in multireader iscurrent  and getversion  i'm attaching a fix for the npe in multireader.iscurrent() plus a testcase. for getversion(), we should throw a better exception that npe. i will commit unless someone objects or has a better idea.",
        "label": 32
    },
    {
        "text": "indexcommit equals  bug indexcommit.equals() checks for equality of directories and versions, but it doesn't check imho the more important generation numbers. it looks like commits are really identified by a combination of directory and segments_xxx, which means the generation number, because that's what the directoryreader.open() checks for. this bug leads to an unexpected behavior when the only change to be committed is in userdata - we get two commits then that are declared equal, they have the same version but they have different generation numbers. i have no idea how this situation is treated in a few dozen references to indexcommit.equals() across lucene... on the surface the fix is trivial - either add the gen number to equals(), or use gen number instead of version. however, it's puzzling why these two would ever get out of sync??? and if they are always supposed to be in sync then maybe we don't need both of them at all, maybe just generation or version is sufficient?",
        "label": 33
    },
    {
        "text": "bugs in memorycodec with lots of docs while working on lucene-3290, i noticed a readvint that i thought should be a readvlong, so i wrote a test (test2bpostings) to try to catch things like this... it takes about 5 minutes to run with memorycodec. the problem is, it dies on some other bug in fsts first!",
        "label": 33
    },
    {
        "text": "testcollectionutil fails on ibm jre [junit] testcase: testemptyarraysort(org.apache.lucene.util.testcollectionutil): caused an error [junit] collectionutil can only sort random access lists in-place.",
        "label": 53
    },
    {
        "text": "make storedfieldsformat more configurable the current storedfieldsformat are implemented with the assumption that only one type of storedfieldsformat is used by the index. we would like to be able to configure a storedfieldsformat per field, similarly to the postingsformat. there is a few issues that need to be solved for allowing that: 1) allowing to configure a segment suffix to the storedfieldsformat 2) implement spi interface in storedfieldsformat 3) create a perfieldstoredfieldsformat we are proposing to start first with 1) by modifying the signature of storedfieldsformat#fieldsreader and storedfieldsformat#fieldswriter so that they use segmentreadstate and segmentwritestate instead of the current set of parameters. let us know what you think about this idea. if this is of interest, we can contribute with a first path for 1).",
        "label": 1
    },
    {
        "text": "avoidcodecs  and maybe similar nearby logic  still runs beforeclass ant test-core -dtestcase=testbackwardscompatibility -dtests.codec=lucene3x this test isnt allowed to run lucene3x, but the assumption stuff is per-method, it still loads things up in beforeclass, meaning we checkindex() real 3.x segments with the \"fake\" 3.x impersonator and it looks like corruption, which is really scary if you are just trying to run all the tests with -dtests.codec=lucene3x to verify a change doesnt break backwards compat.",
        "label": 11
    },
    {
        "text": "customscorequery should support multiple valuesourcequeries customscorequery's constructor currently accepts a subquery, and a valuesourcequery. i would like it to accept multiple valuesourcequeries. the workaround of nested customscorequeries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function. this patch implements custommultiscorequery with my desired functionality, and refactors customscorequery to implement the special case of a custommultiscorequery with 0 or 1 valuesourcequeries. this keeps the customscorequery api intact. this patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases.",
        "label": 12
    },
    {
        "text": "testgeo3dpoint testgeo3drelations  reproducing failures three failures: two npes and one assert \"assess edge that ends in a crossing can't both up and down\": 1.a. (npe) from https://builds.apache.org/job/lucene-solr-nightlytests-master/1512/:    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=c1f88333ec85eae0 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-master/test-data/enwiki.random.lines.txt -dtests.locale=ga -dtests.timezone=america/ojinaga -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   10.4s j1 | testgeo3dpoint.testgeo3drelations <<<    [junit4]    > throwable #1: java.lang.nullpointerexception    [junit4]    >  at __randomizedtesting.seedinfo.seed([c1f88333ec85eae0:7187fea763c8447c]:0)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$dualcrossingedgeiterator.countcrossingpoint(geocomplexpolygon.java:1382)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$dualcrossingedgeiterator.matches(geocomplexpolygon.java:1283)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:564)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:572)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:569)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.traverse(geocomplexpolygon.java:660)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.traverse(geocomplexpolygon.java:646)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon.iswithin(geocomplexpolygon.java:370)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geobasemembershipshape.iswithin(geobasemembershipshape.java:36)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geobaseshape.getbounds(geobaseshape.java:35)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon.getbounds(geocomplexpolygon.java:440)    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:225)    [junit4]    >  at java.lang.thread.run(thread.java:748) 1.b. (npe) from https://builds.apache.org/job/lucene-solr-smokerelease-7.x/184/:    [smoker]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=f2a368ab96a2fd75 -dtests.multiplier=2 -dtests.locale=fr-ml -dtests.timezone=america/godthab -dtests.asserts=true -dtests.file.encoding=utf-8    [smoker]    [junit4] error   0.99s j0 | testgeo3dpoint.testgeo3drelations <<<    [smoker]    [junit4]    > throwable #1: java.lang.nullpointerexception    [smoker]    [junit4]    >  at __randomizedtesting.seedinfo.seed([f2a368ab96a2fd75:42dc153f19ef53e9]:0)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$dualcrossingedgeiterator.countcrossingpoint(geocomplexpolygon.java:1382)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$dualcrossingedgeiterator.matches(geocomplexpolygon.java:1283)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:564)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:572)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:572)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.traverse(geocomplexpolygon.java:660)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.traverse(geocomplexpolygon.java:646)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon.iswithin(geocomplexpolygon.java:370)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geobasemembershipshape.iswithin(geobasemembershipshape.java:36)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geobaseshape.getbounds(geobaseshape.java:43)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon.getbounds(geocomplexpolygon.java:440)    [smoker]    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:224)    [smoker]    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [smoker]    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [smoker]    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [smoker]    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:564)    [smoker]    [junit4]    >  at java.base/java.lang.thread.run(thread.java:844) 2. (both up&down) from https://jenkins.thetaphi.de/job/lucene-solr-7.x-linux/1598/:    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=bacc479cc2d38cca -dtests.multiplier=3 -dtests.slow=true -dtests.badapples=true -dtests.locale=lv -dtests.timezone=systemv/ast4adt -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] failure 3.01s j2 | testgeo3dpoint.testgeo3drelations <<<    [junit4]    > throwable #1: java.lang.assertionerror: assess edge that ends in a crossing can't both up and down    [junit4]    >  at __randomizedtesting.seedinfo.seed([bacc479cc2d38cca:ab33a084d9e2256]:0)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$dualcrossingedgeiterator.countcrossingpoint(geocomplexpolygon.java:1438)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$dualcrossingedgeiterator.matches(geocomplexpolygon.java:1283)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$node.traverse(geocomplexpolygon.java:564)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.traverse(geocomplexpolygon.java:660)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.traverse(geocomplexpolygon.java:646)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon.iswithin(geocomplexpolygon.java:370)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geobasemembershipshape.iswithin(geobasemembershipshape.java:36)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geobaseshape.getbounds(geobaseshape.java:43)    [junit4]    >  at org.apache.lucene.spatial3d.geom.geocomplexpolygon.getbounds(geocomplexpolygon.java:440)    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testgeo3drelations(testgeo3dpoint.java:224)    [junit4]    >  at java.lang.thread.run(thread.java:748)",
        "label": 25
    },
    {
        "text": "deadlock in testindexwriterexceptions [junit] 2012-01-18 18:18:16 [junit] full thread dump java hotspot(tm) 64-bit server vm (19.1-b02 mixed mode): [junit] [junit] \"indexer 3\" prio=10 tid=0x0000000041b9b800 nid=0x6291 waiting for monitor entry [0x00007f7e8868f000] [junit] java.lang.thread.state: blocked (on object monitor) [junit] at org.apache.lucene.index.documentswriterflushqueue.innerpurge(documentswriterflushqueue.java:118) [junit] - waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.documentswriterflushqueue) [junit] at org.apache.lucene.index.documentswriterflushqueue.trypurge(documentswriterflushqueue.java:141) [junit] at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:439) [junit] at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:317) [junit] at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:390) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1534) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1506) [junit] at org.apache.lucene.index.testindexwriterexceptions$indexerthread.run(testindexwriterexceptions.java:187) [junit] [junit] \"indexer 2\" prio=10 tid=0x0000000041b9b000 nid=0x6290 waiting on condition [0x00007f7e8838c000] [junit] java.lang.thread.state: waiting (parking) [junit] at sun.misc.unsafe.park(native method) [junit] - parking to wait for <0x00000000e4103100> (a org.apache.lucene.index.documentswriterstallcontrol$sync) [junit] at java.util.concurrent.locks.locksupport.park(locksupport.java:158) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:811) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.doacquireshared(abstractqueuedsynchronizer.java:941) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.acquireshared(abstractqueuedsynchronizer.java:1261) [junit] at org.apache.lucene.index.documentswriterstallcontrol.waitifstalled(documentswriterstallcontrol.java:115) [junit] at org.apache.lucene.index.documentswriterflushcontrol.waitifstalled(documentswriterflushcontrol.java:591) [junit] at org.apache.lucene.index.documentswriter.preupdate(documentswriter.java:302) [junit] at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:362) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1534) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1506) [junit] at org.apache.lucene.index.testindexwriterexceptions$indexerthread.run(testindexwriterexceptions.java:187) [junit] [junit] \"indexer 1\" prio=10 tid=0x0000000042500000 nid=0x628f waiting for monitor entry [0x00007f7e8858e000] [junit] java.lang.thread.state: blocked (on object monitor) [junit] at org.apache.lucene.index.documentswriterflushqueue.addsegment(documentswriterflushqueue.java:84) [junit] - waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.documentswriterflushqueue) [junit] at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:424) [junit] at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:317) [junit] at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:390) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1534) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1506) [junit] at org.apache.lucene.index.testindexwriterexceptions$indexerthread.run(testindexwriterexceptions.java:187) [junit] [junit] \"indexer 0\" prio=10 tid=0x0000000041508000 nid=0x628d waiting on condition [0x00007f7e8848d000] [junit] java.lang.thread.state: waiting (parking) [junit] at sun.misc.unsafe.park(native method) [junit] - parking to wait for <0x00000000e414b408> (a java.util.concurrent.locks.reentrantlock$nonfairsync) [junit] at java.util.concurrent.locks.locksupport.park(locksupport.java:158) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:811) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:842) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1178) [junit] at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186) [junit] at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262) [junit] at org.apache.lucene.index.documentswriterflushqueue.forcepurge(documentswriterflushqueue.java:130) [junit] at org.apache.lucene.index.documentswriterflushqueue.adddeletesandpurge(documentswriterflushqueue.java:50) [junit] - locked <0x00000000e40ff2a8> (a org.apache.lucene.index.documentswriterflushqueue) [junit] at org.apache.lucene.index.documentswriter.applyalldeletes(documentswriter.java:179) [junit] at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:460) [junit] at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:317) [junit] at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:390) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1534) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1506) [junit] at org.apache.lucene.index.testindexwriterexceptions$indexerthread.run(testindexwriterexceptions.java:187) [junit] [junit] \"low memory detector\" daemon prio=10 tid=0x00007f7e84025800 nid=0x6003 runnable [0x0000000000000000] [junit] java.lang.thread.state: runnable [junit] [junit] \"compilerthread1\" daemon prio=10 tid=0x00007f7e84022800 nid=0x6002 waiting on condition [0x0000000000000000] [junit] java.lang.thread.state: runnable [junit] [junit] \"compilerthread0\" daemon prio=10 tid=0x00007f7e8401f800 nid=0x6001 waiting on condition [0x0000000000000000] [junit] java.lang.thread.state: runnable [junit] [junit] \"signal dispatcher\" daemon prio=10 tid=0x00007f7e8401d800 nid=0x6000 waiting on condition [0x0000000000000000] [junit] java.lang.thread.state: runnable [junit] [junit] \"finalizer\" daemon prio=10 tid=0x00007f7e84001000 nid=0x5ffa in object.wait() [0x00007f7e8961b000] [junit] java.lang.thread.state: waiting (on object monitor) [junit] at java.lang.object.wait(native method) [junit] - waiting on <0x00000000e2ece380> (a java.lang.ref.referencequeue$lock) [junit] at java.lang.ref.referencequeue.remove(referencequeue.java:118) [junit] - locked <0x00000000e2ece380> (a java.lang.ref.referencequeue$lock) [junit] at java.lang.ref.referencequeue.remove(referencequeue.java:134) [junit] at java.lang.ref.finalizer$finalizerthread.run(finalizer.java:159) [junit] [junit] \"reference handler\" daemon prio=10 tid=0x000000004101e000 nid=0x5ff9 in object.wait() [0x00007f7e8971c000] [junit] java.lang.thread.state: waiting (on object monitor) [junit] at java.lang.object.wait(native method) [junit] - waiting on <0x00000000e2ece318> (a java.lang.ref.reference$lock) [junit] at java.lang.object.wait(object.java:485) [junit] at java.lang.ref.reference$referencehandler.run(reference.java:116) [junit] - locked <0x00000000e2ece318> (a java.lang.ref.reference$lock) [junit] [junit] \"main\" prio=10 tid=0x0000000040fb2000 nid=0x5fe2 in object.wait() [0x00007f7e8ecc1000] [junit] java.lang.thread.state: waiting (on object monitor) [junit] at java.lang.object.wait(native method) [junit] - waiting on <0x00000000e4105080> (a org.apache.lucene.index.testindexwriterexceptions$indexerthread) [junit] at java.lang.thread.join(thread.java:1186) [junit] - locked <0x00000000e4105080> (a org.apache.lucene.index.testindexwriterexceptions$indexerthread) [junit] at java.lang.thread.join(thread.java:1239) [junit] at org.apache.lucene.index.testindexwriterexceptions.testrandomexceptionsthreads(testindexwriterexceptions.java:286) [junit] at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit] at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [junit] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [junit] at java.lang.reflect.method.invoke(method.java:597) [junit] at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44) [junit] at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) [junit] at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41) [junit] at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) [junit] at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48) [junit] at org.apache.lucene.util.lucenetestcase$3$1.evaluate(lucenetestcase.java:528) [junit] at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28) [junit] at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31) [junit] at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76) [junit] at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:165) [junit] at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:57) [junit] at org.junit.runners.parentrunner$3.run(parentrunner.java:193) [junit] at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52) [junit] at org.junit.runners.parentrunner.runchildren(parentrunner.java:191) [junit] at org.junit.runners.parentrunner.access$000(parentrunner.java:42) [junit] at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184) [junit] at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28) [junit] at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31) [junit] at org.junit.runners.parentrunner.run(parentrunner.java:236) [junit] at junit.framework.junit4testadapter.run(junit4testadapter.java:39) [junit] at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420) [junit] at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911) [junit] at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:743) [junit] [junit] \"vm thread\" prio=10 tid=0x0000000041017800 nid=0x5fef runnable [junit] [junit] \"gc task thread#0 (parallelgc)\" prio=10 tid=0x0000000040fc5000 nid=0x5fe3 runnable [junit] [junit] \"gc task thread#1 (parallelgc)\" prio=10 tid=0x0000000040fc7000 nid=0x5fe4 runnable [junit] [junit] \"gc task thread#2 (parallelgc)\" prio=10 tid=0x0000000040fc9000 nid=0x5fe5 runnable [junit] [junit] \"gc task thread#3 (parallelgc)\" prio=10 tid=0x0000000040fca800 nid=0x5fe6 runnable [junit] [junit] \"gc task thread#4 (parallelgc)\" prio=10 tid=0x0000000040fcc800 nid=0x5fe7 runnable [junit] [junit] \"gc task thread#5 (parallelgc)\" prio=10 tid=0x0000000040fce800 nid=0x5fe8 runnable [junit] [junit] \"gc task thread#6 (parallelgc)\" prio=10 tid=0x0000000040fd0000 nid=0x5fe9 runnable [junit] [junit] \"gc task thread#7 (parallelgc)\" prio=10 tid=0x0000000040fd2000 nid=0x5fea runnable [junit] [junit] \"vm periodic task thread\" prio=10 tid=0x00007f7e84030000 nid=0x6004 waiting on condition [junit] [junit] jni global references: 1578 [junit] [junit] [junit] found one java-level deadlock: [junit] ============================= [junit] \"indexer 3\": [junit] waiting to lock monitor 0x0000000041477498 (object 0x00000000e40ff2a8, a org.apache.lucene.index.documentswriterflushqueue), [junit] which is held by \"indexer 0\" [junit] \"indexer 0\": [junit] waiting for ownable synchronizer 0x00000000e414b408, (a java.util.concurrent.locks.reentrantlock$nonfairsync), [junit] which is held by \"indexer 3\" [junit] [junit] java stack information for the threads listed above: [junit] =================================================== [junit] \"indexer 3\": [junit] at org.apache.lucene.index.documentswriterflushqueue.innerpurge(documentswriterflushqueue.java:118) [junit] - waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.documentswriterflushqueue) [junit] at org.apache.lucene.index.documentswriterflushqueue.trypurge(documentswriterflushqueue.java:141) [junit] at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:439) [junit] at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:317) [junit] at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:390) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1534) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1506) [junit] at org.apache.lucene.index.testindexwriterexceptions$indexerthread.run(testindexwriterexceptions.java:187) [junit] \"indexer 0\": [junit] at sun.misc.unsafe.park(native method) [junit] - parking to wait for <0x00000000e414b408> (a java.util.concurrent.locks.reentrantlock$nonfairsync) [junit] at java.util.concurrent.locks.locksupport.park(locksupport.java:158) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:811) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:842) [junit] at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1178) [junit] at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186) [junit] at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262) [junit] at org.apache.lucene.index.documentswriterflushqueue.forcepurge(documentswriterflushqueue.java:130) [junit] at org.apache.lucene.index.documentswriterflushqueue.adddeletesandpurge(documentswriterflushqueue.java:50) [junit] - locked <0x00000000e40ff2a8> (a org.apache.lucene.index.documentswriterflushqueue) [junit] at org.apache.lucene.index.documentswriter.applyalldeletes(documentswriter.java:179) [junit] at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:460) [junit] at org.apache.lucene.index.documentswriter.postupdate(documentswriter.java:317) [junit] at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:390) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1534) [junit] at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1506) [junit] at org.apache.lucene.index.testindexwriterexceptions$indexerthread.run(testindexwriterexceptions.java:187) [junit] [junit] found 1 deadlock. [junit] [junit] heap [junit] psyounggen total 67136k, used 4647k [0x00000000f5560000, 0x00000000fbc60000, 0x0000000100000000) [junit] eden space 65792k, 5% used [0x00000000f5560000,0x00000000f58a5e10,0x00000000f95a0000) [junit] from space 1344k, 96% used [0x00000000f9740000,0x00000000f98840a0,0x00000000f9890000) [junit] to space 19840k, 0% used [0x00000000fa900000,0x00000000fa900000,0x00000000fbc60000) [junit] psoldgen total 171392k, used 66868k [0x00000000e0000000, 0x00000000ea760000, 0x00000000f5560000) [junit] object space 171392k, 39% used [0x00000000e0000000,0x00000000e414d080,0x00000000ea760000) [junit] pspermgen total 21248k, used 14733k [0x00000000dae00000, 0x00000000dc2c0000, 0x00000000e0000000) [junit] object space 21248k, 69% used [0x00000000dae00000,0x00000000dbc635a8,0x00000000dc2c0000) [junit]",
        "label": 46
    },
    {
        "text": "testlucene54docvaluesformat testsparsedocvaluesvsstoredfields  failure  unstable advanceexact i reproduced https://builds.apache.org/job/lucene-solr-nightlytests-master/1141/ on macos:    [junit4]   2> note: reproduce with: ant test  -dtestcase=testlucene54docvaluesformat -dtests.method=testsparsedocvaluesvsstoredfields -dtests.seed=376025608e59a340 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.locale=zh-hk -dtests.timezone=america/goose_bay -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error    216s | testlucene54docvaluesformat.testsparsedocvaluesvsstoredfields <<<    [junit4]    > throwable #1: java.lang.runtimeexception: dv iterator field=numeric: doc=71089 has unstable advanceexact    [junit4]    >  at __randomizedtesting.seedinfo.seed([376025608e59a340:63bbf97374d5850f]:0)    [junit4]    >  at org.apache.lucene.index.checkindex.checkdviterator(checkindex.java:2118)    [junit4]    >  at org.apache.lucene.index.checkindex.checkdocvalues(checkindex.java:2291)    [junit4]    >  at org.apache.lucene.index.checkindex.testdocvalues(checkindex.java:2039)    [junit4]    >  at org.apache.lucene.util.testutil.checkreader(testutil.java:340)    [junit4]    >  at org.apache.lucene.util.testutil.checkreader(testutil.java:319)    [junit4]    >  at org.apache.lucene.codecs.lucene54.testlucene54docvaluesformat.dotestsparsedocvaluesvsstoredfields(testlucene54docvaluesformat.java:204)    [junit4]    >  at org.apache.lucene.codecs.lucene54.testlucene54docvaluesformat.testsparsedocvaluesvsstoredfields(testlucene54docvaluesformat.java:149)",
        "label": 1
    },
    {
        "text": "longbistset can't have long size private final long[] bits; // array of longs holding the bits ===> bits.length is small for bit number having long.max so you can not call \"longbitset.set(long.max-1)\"",
        "label": 33
    },
    {
        "text": "deprecate all string file ctors opens in indexreader indexwriter indexsearcher during investigation of lucene-1658, i found out, that even lucene-1453 is not completely fixed. as 1658 deprecates all fsdirectory.getdirectory() static factories, we should not use them anymore. as the user is now free to choose the correct directory implementation using direct instantiation or using fsdir.open() he should no longer use all ctors/methods in indexwriter/indexreader/indexsearcher & co. that simply take path names as string or file and always instantiate the directory himself. lucene-1453 currently works for the cached directory implementations from fsdir.getdirectory, but not with uncached, non refcounting fsdirs. sometime reopen() closes the directory (as far as i see, when a segmentreader changes to a multisegmentreader and/or deletes apply). this is hard to track. in lucene 3.0 we then can remove the whole bunch of closedirectory parameters/fields in these classes and simply do not care anymore about closing directories. to remove this closedirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the file/string to return the indexreader wrapped by a filteredindexreader, that keeps track on closing the underlying directory after close and reopen. this is simplier than passing this boolean between different directoryindexreader instances. the small performance impact by wrapping with filterindexreader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with directory parameter.",
        "label": 53
    },
    {
        "text": "new serbian filter this is a new serbian filter that works with regular latin text (the current filter works with \"bald\" latin). i described in detail what does it do and why is it necessary at the wiki.",
        "label": 11
    },
    {
        "text": "index sorter a tool to sort index according to a float document weight. documents with high weight are given low document numbers, which means that they will be first evaluated. when using a strategy of \"early termination\" of queries (see timelimitedcollector) such sorting significantly improves the quality of partial results. (originally this tool was created by doug cutting in nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. this is a pure lucene version of the tool, and it uses arbitrary floats from a specified stored field).",
        "label": 4
    },
    {
        "text": "spatial filters not serializable i am using lucene in a distributed setup. the filters in the spatial project aren't serializable even though it inherits it from filter. filter is a serializable class. distancefilter contains the non-serializable class weakhashmap. cartesianshapefilter contains the non-serializable class java.util.logging.logger",
        "label": 46
    },
    {
        "text": "fastvectorhighlighter adds a multi value separator  space  to the end of the highlighted text the fvh adds an additional ' ' (the multi value separator) to the end of the highlighted text.",
        "label": 26
    },
    {
        "text": "experiment with placing poms outside of src recent work in lucene-3944 has changed how our generated pom.xml files are handled during release preparation, placing them in build/ instead. however get-maven-poms still places the poms inside src/ so you can use them to drive a build. what i think would be ideal is if we could unify the release handling of the poms and the normal building handling, so that the poms can sit outside of src and serve both purposes. some time ago i investigated how the ant project handles its own maven integration and it has its poms sitting in their own directory. they then reference the actual src locations inside the poms. this works for ant but with a warning since some of their tests don't work due to how the maven surefire plugin works, so they skip their tests. i have done some quick testing of my own and this process does seem to work for our poms and tests. i now want to take this to a full scale poc and see if it works fully.",
        "label": 47
    },
    {
        "text": "optimize runs forever if you keep deleting docs at the same time because we \"cascade\" merges for an optimize... if you also delete documents while the merges are running, then the merge policy will see the resulting single segment as still not optimized (since it has pending deletes) and do a single-segment merge, and will repeat indefinitely (as long as your app keeps deleting docs).",
        "label": 33
    },
    {
        "text": "mp does not drop fully soft deleted segments fully soft-deleted segments should be dropped as fully hard-deleted segments if softdeletesfield is provided and mp is configured not to retain fully deleted segments. a failed test is attached. /cc simon willnauer",
        "label": 46
    },
    {
        "text": "make not yet final core contrib tokenstream filter implementations final lucene's analysis package is designed in a way, that you can plug different implementations of analysis in chains of tokenstreams and tokenfilters. an analyzer is build of several tokenstreams/filters that do the tokenization of text. if you want to modify the behaviour of tokenization, you implement a new subclass of tokenstream/-filter/tokenizer. most classes in the core are correctly implemented like that. they are itsself final or their implementation methods are final (chartokenizer). a lot of problems with backwards-compatibility of lucene-1693 are some classes in lucene's core/contrib not yet final: keywordtokenizer should be declared final or its implementation methods should be final standardtokenizer should be declared final or its implementation methods should be final isolatin1filter is deprecated, so it will be removed in 3.0, nothing to do. chartokenizer is the abstract base class of several other classes. the design is correct: child classes cannot override the implementation, they can only change the behaviour of this final implementation. contrib should be checked, that all implementation classes are at least final or they are designed in the same way like chartokenizer.",
        "label": 53
    },
    {
        "text": "rm or formalize dealing with  general  keys files in our dist dir at some point in the past, we started creating a snapshots of keys (taken from the auto-generated data from id.apache.org) in the release dir of each release... http://www.apache.org/dist/lucene/solr/4.4.0/keys http://www.apache.org/dist/lucene/java/4.4.0/keys http://archive.apache.org/dist/lucene/java/4.3.0/keys http://archive.apache.org/dist/lucene/solr/4.3.0/keys etc... but we also still have some \"general\" keys files... https://www.apache.org/dist/lucene/keys https://www.apache.org/dist/lucene/java/keys https://www.apache.org/dist/lucene/solr/keys ...which (as i discovered when i went to add my key to them today) are stale and don't seem to be getting updated. i vaguely remember someone (rmuir?) explaining to me at one point the reason we started creating a fresh copy of keys in each release dir, but i no longer remember what they said, and i can't find any mention of a reason in any of the release docs, or in any sort of comment in buildandpushrelease.py we probably do one of the following: remove these \"general\" keys files add a disclaimer to the top of these files that they are legacy files for verifying old releases and are no longer used for new releases ensure these files are up to date stop generating per-release keys file copies update our release process to ensure that the general files get updated on each release as well",
        "label": 21
    },
    {
        "text": "implementing divergence from independence  dfi  term weighting for lucene solr as explained in the write-up, many state-of-the-art ranking model implementations are added to apache lucene. this issue aims to include dfi model, which is the non-parametric counterpart of the divergence from randomness (dfr) framework. dfi is both parameter-free and non-parametric: parameter-free: it does not require any parameter tuning or training. non-parametric: it does not make any assumptions about word frequency distributions on document collections. it is highly recommended not to remove stopwords (very common terms: the, of, and, to, a, in, for, is, on, that, etc) with this similarity. for more information see: a nonparametric term weighting method for information retrieval based on measuring the divergence from independence",
        "label": 40
    },
    {
        "text": "numericrange support for new query parser it would be good to specify some type of \"schema\" for the query parser in future, to automatically create numericrangequery for different numeric types? it would then be possible to index a numeric value (double,float,long,int) using numericfield and then the query parser knows, which type of field this is and so it correctly creates a numericrangequery for strings like \"[1.567..*]\" or \"(1.787..19.5]\". there is currently no way to extract if a field is numeric from the index, so the user will have to configure the fieldconfig objects in the confighandler. but if this is done, it will not be that difficult to implement the rest. the only difference between the current handling of rangequery is then the instantiation of the correct query type and conversion of the entered numeric values (simple number.valueof(...) cast of the user entered numbers). evenerything else is identical, numericrangequery also supports the mtq rewrite modes (as it is a mtq). another thing is a change in date semantics. there are some strange flags in the current parser that tells it how to handle dates.",
        "label": 53
    },
    {
        "text": "rangequery   rangefilter used with collation seek to lowerterm using compareto  the constructor for rangetermenum initializes a termenum starting with lowertermtext, but when a collator is defined, all terms in the given field need to be checked, since collation can introduce non-unicode orderings. instead, the rangetermenum constructor should test for a non-null collator, and if there is one, point the termenum at the first term in the given field. lucene-1424 introduced this bug.",
        "label": 33
    },
    {
        "text": "icucollationkeyanalyzer fail to tokenize thai word current implementation is: keywordtokenizer tokenizer = new keywordtokenizer(factory, reader, keywordtokenizer.default_buffer_size); return new tokenstreamcomponents(tokenizer, tokenizer); i tried change tokenizer from keywordtokenizer to icutokenizer. it return a proper tokenized word.",
        "label": 53
    },
    {
        "text": "allow indexwriter to commit  even just commitdata spinoff from here http://lucene.472066.n3.nabble.com/commit-with-only-commitdata-td4022155.html. in some cases, it is valuable to be able to commit changes to the index, even if the changes are just commitdata. such data is sometimes used by applications to register in the index some global application information/state. the proposal is: add a setcommitdata() api and separate it from commit() and preparecommit() (simplify their api) when that api is called, flip on the dirty/changes bit, so that this gets committed even if no other changes were made to the index. i will work on a patch a post.",
        "label": 43
    },
    {
        "text": "investigate solr test failures using flex we have a branch of solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr currently all the tests pass with lucene trunk jars. i plopped in the flex jars and they do not, so i thought these might be interesting to look at.",
        "label": 33
    },
    {
        "text": "timelimitingcollector should check timeout also when leafcollector is pulled timelimitingcollector only check the timeout if there is actually a hit. if you never hit anything but have a damn slow query for whatever reason the timeout is never hit.",
        "label": 46
    },
    {
        "text": "trstringdistance uses way too much memory  with patch  the implementation of trstringdistance is based on version 2.1 of org.apache.commons.lang.stringutils#getlevenshteindistance(string, string), which uses an un-optimized implementation of the levenshtein distance algorithm (it uses way too much memory). please see bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information. the commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). i have reported the new implementation to trstringdistance.",
        "label": 40
    },
    {
        "text": "add logisticregressiondocumentclassifier add logisticregressiondocumentclassifier for lucene.",
        "label": 50
    },
    {
        "text": "move contrib benchmark to modules benchmark i think we should move lucene/contrib/benchmark to a shared modules/benchmark, so you can easily benchmark anything (lucene, solr, other modules like analysis or whatever). for example, if you want to do some benchmarking of something in solr (lucene-2844) you should be able to do this. another example is simply being able to benchmark an analyzer definition from a schema.xml, its more convenient than writing the equivalent java analyzer just for benchmarking.",
        "label": 40
    },
    {
        "text": "testindexwriter failure  aioobe trunk: r1133486     [junit] testsuite: org.apache.lucene.index.testindexwriter     [junit] testcase: testemptyfieldname(org.apache.lucene.index.testindexwriter):      caused an error     [junit] checkindex failed     [junit] java.lang.runtimeexception: checkindex failed     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:158)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:144)     [junit]     at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:477)     [junit]     at org.apache.lucene.index.testindexwriter.testemptyfieldname(testindexwriter.java:857)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1362)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1280)     [junit]      [junit]      [junit] tests run: 39, failures: 0, errors: 1, time elapsed: 17.634 sec     [junit]      [junit] ------------- standard output ---------------     [junit] checkindex failed     [junit] segments file=segments_1 numsegments=1 version=format_4_0 [lucene 4.0]     [junit]   1 of 1: name=_0 doccount=1     [junit]     codec=segmentcodecs [codecs=[preflex], provider=org.apache.lucene.index.codecs.corecodecprovider@3f78807]     [junit]     compound=false     [junit]     hasprox=true     [junit]     numfiles=8     [junit]     size (mb)=0     [junit]     diagnostics = {os.version=2.6.39-gentoo, os=linux, lucene.version=4.0-snapshot, source=flush, os.arch=amd64, java.version=1.6.0_25, java.vendor=sun microsystems inc.}     [junit]     no deletions     [junit]     test: open reader.........ok     [junit]     test: fields..............ok [1 fields]     [junit]     test: field norms.........ok [1 fields]     [junit]     test: terms, freq, prox...error: java.lang.arrayindexoutofboundsexception: -1     [junit] java.lang.arrayindexoutofboundsexception: -1     [junit]     at org.apache.lucene.index.codecs.preflex.terminfosreader.seekenum(terminfosreader.java:212)     [junit]     at org.apache.lucene.index.codecs.preflex.terminfosreader.seekenum(terminfosreader.java:301)     [junit]     at org.apache.lucene.index.codecs.preflex.terminfosreader.get(terminfosreader.java:234)     [junit]     at org.apache.lucene.index.codecs.preflex.terminfosreader.terms(terminfosreader.java:371)     [junit]     at org.apache.lucene.index.codecs.preflex.preflexfields$pretermsenum.reset(preflexfields.java:719)     [junit]     at org.apache.lucene.index.codecs.preflex.preflexfields$preterms.iterator(preflexfields.java:249)     [junit]     at org.apache.lucene.index.perfieldcodecwrapper$fieldsreader$fieldsiterator.terms(perfieldcodecwrapper.java:147)     [junit]     at org.apache.lucene.index.checkindex.testtermindex(checkindex.java:610)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:495)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:154)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:144)     [junit]     at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:477)     [junit]     at org.apache.lucene.index.testindexwriter.testemptyfieldname(testindexwriter.java:857)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1362)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1280)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:422)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:931)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:758)     [junit]     test: stored fields.......ok [0 total field count; avg 0 fields per doc]     [junit]     test: term vectors........ok [0 total vector count; avg 0 term/freq vector fields per doc]     [junit] failed     [junit]     warning: fixindex() would remove reference to this segment; full exception:     [junit] java.lang.runtimeexception: term index test failed     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:508)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:154)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:144)     [junit]     at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:477)     [junit]     at org.apache.lucene.index.testindexwriter.testemptyfieldname(testindexwriter.java:857)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1362)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1280)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:422)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:931)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:758)     [junit]      [junit] warning: 1 broken segments (containing 1 documents) detected     [junit]      [junit] ------------- ---------------- ---------------     [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testindexwriter -dtestmethod=testemptyfieldname -dtests.seed=-3770357642070518646:-3121175410586002489 -dtests.multiplier=3     [junit] note: test params are: codec=preflex, locale=zh, timezone=indian/antananarivo     [junit] note: all tests run in this jvm:     [junit] [testdatetools, testdeletionpolicy, testdocsandpositions, testflex, testindexreaderclonenorms, testindexwriter]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=85972280,total=232521728     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.index.testindexwriter failed",
        "label": 33
    },
    {
        "text": "shinglefilter should make shingles from trailing holes when shinglefilter hits a hole, it uses _ as the token, e.g. bigrams for \"the dog barked\", if you have a stopfilter removing the, would be: \"_ dog\", \"dog barked\". but if the input ends with a stopword, e.g. \"wizard of\", shinglefilter fails to produce \"wizard _\" due to lucene-3849 ... once we fix that i think we should fix shinglefilter to make shingles for trailing holes too ...",
        "label": 33
    },
    {
        "text": "toparentblockjoincollector provides no way to access computed scores and the maxscore the constructor of toparentblockjoincollector allows to turn on the tracking of parent scores and the maximum parent score, however there is no way to access those scores because: maxscore is a private field, and there is no getter topgroups / groupdocs does not provide access to the scores for the parent documents, only the children",
        "label": 33
    },
    {
        "text": "lucene40codec methods should be final i think all methods but getpostingsformatforfield should be made final so that users can't create a codec that redefines any of the formats of lucene40 by subclassing (since the codec name can't be overriden by subclassing, lucene will fail at loading segments that use such codecs).",
        "label": 1
    },
    {
        "text": "prepare chararrayset for unicode chararrayset does lowercaseing if created with the correspondent flag. this causes that string / char[] with uncode 4 chars which are in the set can not be retrieved in \"ignorecase\" mode.",
        "label": 53
    },
    {
        "text": "3x lucene contrib changes txt has two  api changes  subsections for there are two \"api changes\" sections which is confusing when looking at the txt version of the file. the html expands only the first of the two, unless expand-all is clicked.",
        "label": 47
    },
    {
        "text": "add new snowball languages snowball added new languages. this patch adds support for them. http://snowball.tartarus.org/algorithms/armenian/stemmer.html http://snowball.tartarus.org/algorithms/catalan/stemmer.html http://snowball.tartarus.org/algorithms/basque/stemmer.html",
        "label": 40
    },
    {
        "text": "payloadtermquery refers to a deprecated documentation for redirection when payloadtermquery refers to the function for scoring similarity - it refers to override the deprecated method - similarity#scorepayload(string, byte[],int,int) . that method has been deprecated by similarity#scorepayload(int, string, int, int, byte[],int,int) . this javadoc patch addresses the class level javadoc for the class to provide the right signature in similarity to be overridden.",
        "label": 33
    },
    {
        "text": "speedup chararrayset if set is empty chararrayset#contains(...) always creates a hashcode of the string, char[] or charsequence even if the set is empty. contains should return false if set it empty",
        "label": 40
    },
    {
        "text": "improve performance of contrib testcompoundwordtokenfilter contrib/analyzers/compound has some tests that use a hyphenation grammar file. the tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem. the issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html so the test must download the entire offo zip file from sourceforge to execute. i happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed. this way it could be included in the source with the test and would be more practical.",
        "label": 40
    },
    {
        "text": "allow mergepolicy to select non contiguous merges i started work on this but with lucene-1044 i won't make much progress on it for a while, so i want to checkpoint my current state/patch. for backwards compatibility we must leave the default mergepolicy as selecting contiguous merges. this is necessary because some applications rely on \"temporal monotonicity\" of doc ids, which means even though merges can re-number documents, the renumbering will always reflect the order in which the documents were added to the index. still, for those apps that do not rely on this, we should offer a mergepolicy that is free to select the best merges regardless of whether they are continuguous. this requires fixing indexwriter to accept such a merge, and, fixing logmergepolicy to optionally allow it the freedom to do so.",
        "label": 33
    },
    {
        "text": "add delete term and query need to more precisely record the bytes used documentswriter's add delete query and add delete term add to the number of bytes used regardless of the query or term already existing in the respective map.",
        "label": 43
    },
    {
        "text": "npe in stopfilter caused by standardanalyzer boolean replaceinvalidacronym  constructor i think that i found a problem with the new code (https://issues.apache.org/jira/browse/lucene-1068). usage of the new constructor standardanalyzer(boolean replaceinvalidacronym) causes npe in stopfilter: java.lang.nullpointerexception at org.apache.lucene.analysis.stopfilter.<init>(stopfilter.java:74) at org.apache.lucene.analysis.stopfilter.<init>(stopfilter.java:86) at org.apache.lucene.analysis.standard.standardanalyzer.tokenstream(standardanalyzer.java:151) at org.apache.lucene.queryparser.queryparser.getfieldquery(queryparser.java:452) at org.apache.lucene.queryparser.queryparser.term(queryparser.java:1133) at org.apache.lucene.queryparser.queryparser.clause(queryparser.java:1020) at org.apache.lucene.queryparser.queryparser.query(queryparser.java:948) at org.apache.lucene.queryparser.queryparser.clause(queryparser.java:1024) at org.apache.lucene.queryparser.queryparser.query(queryparser.java:948) at org.apache.lucene.queryparser.queryparser.toplevelquery(queryparser.java:937) at org.apache.lucene.queryparser.queryparser.parse(queryparser.java:147) the reason is that new constructor forgets to initialize the stopset field: public standardanalyzer(boolean replaceinvalidacronym) { this.replaceinvalidacronym = replaceinvalidacronym; } i guess this should be changed to something like this: public standardanalyzer(boolean replaceinvalidacronym) { this(stop_words); this.replaceinvalidacronym = replaceinvalidacronym; } the bug is present in rc3. fix is one line, it'll be great to have it in 2.3 release.",
        "label": 15
    },
    {
        "text": "create or append mode determined before obtaining write lock if an indexwriter(\"writer1\") is opened in create_or_append mode, it determines whether to create or append before obtaining the write lock. when another indexwriter(\"writer2\") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock. when writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2. this bug was probably effected by lucene-2386 as prior to that lucene generated an empty commit when a new index was created. i think the issue could still have occurred prior to that but the two indexwriters would have needed to be opened nearly simultaneously and the first indexwriter would need to release the lock before the second timed out.",
        "label": 46
    },
    {
        "text": "vector isnumericallyidentical  can produce unexpected results the method vector.isnumericallyidentical() seems to produce wrong results in some cases. for example it considers that vectors (1,0,0) and (-1,0,0) are numerically identical whichi is probably wrong. this behavior might produce unexpected behaviors down the line.  ",
        "label": 25
    },
    {
        "text": "changes txt has no release date for changes.txt has no release date for 3.1.0 - although we've stopped putting dates on rcs' changes.txt for the release-to-be, we should add dates to changes.txt for past releases. alternatively we could remove releases dates for past release completely.",
        "label": 40
    },
    {
        "text": "improve fieldoffsetstrategy javadoc post offsetsenum change as part of lucene-8145, the fieldoffsetstrategy abstract class was changed to return an offsetsenum rather than a list<offsetsenum> for getoffsetsenum(indexreader reader, int docid, string content). however, the javadoc at the top of the class currently reads: ultimately returns a list of {@link offsetsenum} yielding potentially highlightable words in the text.... \"a list\" should be replaced with \"an\" to be more accurate now that this method has changed signature.",
        "label": 10
    },
    {
        "text": "wildcardquery rewrite improvements wildcardquery has logic to rewrite to termquery if there is no wildcard character, but it needs to pass along the boost if it does this if the user asked for a 'constant score' rewritemethod, it should rewrite to a constant score query for consistency. additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query. both will enumerate the same number of terms, but prefixquery has a simpler comparison function.",
        "label": 33
    },
    {
        "text": "resolve junit assert deprecations many tests use assertequals methods which have been deprecated. the culprits are assertequals(float, float), assertequals(double, double) and assertequals(object[], object[]). although not a big issue, they annoy me every time i see them so i'm going to fix them.",
        "label": 7
    },
    {
        "text": "core tests should call version based ctors instead of deprecated default ctors lucene-2183 introduced new ctors for all chartokenizer subclasses. core - tests should use those ctors with version.lucene_current instead of the the deprecated ctors. yet, lucene-2240 introduces more version ctors for whitespaceanalyzer and simpleanalyzer. test should also use their version ctors instead the default ones.",
        "label": 53
    },
    {
        "text": "custom sort broken if is uses executorservice ",
        "label": 33
    },
    {
        "text": "lucene document classification currently the lucene classification module supports the classification for an input text using the lucene index as a trained model. this improvement is adding to the module a set of components to provide document classification ( where the document is a lucene document ). all selected fields from the document will have their part in the classification ( including the use of the proper analyzer per field).",
        "label": 50
    },
    {
        "text": "int overflow in bytebuffersdataoutput size  hi, when indexing large data sets with bytebuffersdirectory, an exception like the below is thrown: caused by: java.lang.illegalargumentexception: cannot write negative vlong (got: -4294888321) at org.apache.lucene.store.dataoutput.writevlong(dataoutput.java:225) at org.apache.lucene.codecs.lucene50.lucene50skipwriter.writeskipdata(lucene50skipwriter.java:182) at org.apache.lucene.codecs.multilevelskiplistwriter.bufferskip(multilevelskiplistwriter.java:143) at org.apache.lucene.codecs.lucene50.lucene50skipwriter.bufferskip(lucene50skipwriter.java:162) at org.apache.lucene.codecs.lucene50.lucene50postingswriter.startdoc(lucene50postingswriter.java:228) at org.apache.lucene.codecs.pushpostingswriterbase.writeterm(pushpostingswriterbase.java:148) at org.apache.lucene.codecs.blocktree.blocktreetermswriter$termswriter.write(blocktreetermswriter.java:865) at org.apache.lucene.codecs.blocktree.blocktreetermswriter.write(blocktreetermswriter.java:344) at org.apache.lucene.codecs.fieldsconsumer.merge(fieldsconsumer.java:105) at org.apache.lucene.codecs.perfield.perfieldpostingsformat$fieldswriter.merge(perfieldpostingsformat.java:169) at org.apache.lucene.index.segmentmerger.mergeterms(segmentmerger.java:244) at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:139) at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4453) at org.apache.lucene.index.indexwriter.merge(indexwriter.java:4075) the exception is caused by an integer overflow while calling getfilepointer() in lucene50postingswriter, which eventually calls the size() method in bytebuffersdataoutput. bytebuffersdataoutput.java public long size() {  long size = 0;  int blockcount = blocks.size();  if (blockcount >= 1) {  int fullblocksize = (blockcount - 1) * blocksize(); // throws integer overflow  int lastblocksize = blocks.getlast().position();  size = fullblocksize + lastblocksize;  }  return size; } in my case, i had a blockcount = 65 and a blocksize() = 33554432 which overflows fullblocksize. the fix: bytebuffersdataoutput.java public long size() { long size = 0; int blockcount = blocks.size(); if (blockcount >= 1) { long fullblocksize = 1l * (blockcount - 1) * blocksize(); int lastblocksize = blocks.getlast().position(); size = fullblocksize + lastblocksize; } return size; }   thanks",
        "label": 11
    },
    {
        "text": "javadocs very very ugly if you generate with java7 java7 changes its javadocs to look much nicer, but this involves different css styles. lucene overrides the css with stylesheet+prettify.css which is a combination of java5/6 stylesheet + google prettify: but there are problems because java7 has totally different styles. so if you generate javadocs with java7, its like you have no stylesheet at all. a solution might be to make stylesheet7+prettify.css and conditionalize a property in ant based on java version.",
        "label": 53
    },
    {
        "text": "allow packed ints norms i was curious what the performance would be, because it might be useful option to use packedints for norms if you have lots of fields and still want good scoring: today the smallest norm per-field-per-doc you can use is a single byte, and if you have f fields with norms enabled and n docs, it uses f * n bytes of space in ram. especially if you aren't using index-time boosting (or even if you are, but not with ridiculous values), this could be wasting a ton of ram. but then i noticed there was no clean way to allow you to do this in your similarity: its a trivial patch.",
        "label": 40
    },
    {
        "text": "allow setting the indexwriter docstore to be a different directory add an indexwriter.setdocstoredirectory method that allows doc stores to be placed in a different directory than the iw default dir.",
        "label": 33
    },
    {
        "text": "flexible standardqueryparser behaves differently than classicqueryparser analyzerquerynodeprocessor creates a booleanquerynode instead of a multiphrasequerynode for some circumstances. classic query parser output: +content:a +content:320 (correct) queryparser classicqueryparser; classicqueryparser = new queryparser(version.lucene_45, \"content\", anaylzer); classicqueryparser.setdefaultoperator(operator.and); classicqueryparser.parse(\"a320\")); flexible query parser output: content:a content:320 (wrong) standardqueryparser flexiblequeryparser; flexiblequeryparser = new standardqueryparser(anaylzer); flexiblequeryparser.setdefaultoperator(operator.and); flexiblequeryparser.parse(\"a320\", \"content\")); the used analyzer: analyzer anaylzer = return new analyzer() {   protected tokenstreamcomponents createcomponents(string field, reader in) {   tokenizer   src = new whitespacetokenizer(version.lucene_45, in);   tokenstream tok = new worddelimiterfilter(src,      worddelimiterfilter.split_on_numerics |      worddelimiterfilter.generate_word_parts |      worddelimiterfilter.generate_number_parts,      chararrayset.empty_set);    return new tokenstreamcomponents(src, tok); };",
        "label": 0
    },
    {
        "text": " patch  booleanscorer2 arrayindexoutofboundsexception   alternative nearspans from erik's post at java-dev: >      [java] caused by: java.lang.arrayindexoutofboundsexception: 4 >      [java]     at org.apache.lucene.search.booleanscorer2 > $coordinator.coordfactor(booleanscorer2.java:54) >      [java]     at org.apache.lucene.search.booleanscorer2.score > (booleanscorer2.java:292) ... and my answer: probably nrmatchers is increased too often in score() by calling score() more than once.",
        "label": 55
    },
    {
        "text": "improve booleanquery rewrite documentation while looking over booleanquery#rewrite, i found a couple of things confusing. why, in the case of a single clause, is the boost set as it is, and whats going on with the lazy initialisation of the cloned booleanquery. i'm just adding a few lines of documentation to both situations to clarify this.",
        "label": 46
    },
    {
        "text": "indexupgradertool should rewrite segments rather than forcemerge spinoff from lucene-7976. we help users get themselves into a corner by using forcemerge on an index to rewrite all segments in the current lucene format. we should rewrite each individual segment instead. this would also help with upgrading x-2->x-1, then x-1->x. of course the preferred method is to re-index from scratch.",
        "label": 13
    },
    {
        "text": "cleanup and improvement of spatial contrib the current spatial contrib can be improved by adding documentation, tests, removing unused classes and code, repackaging the classes and improving the performance of the distance filtering. the latter will incorporate the multi-threaded functionality introduced in lucene-1732. other improvements involve adding better support for different distance units, different distance calculators and different data formats (whether it be lat/long fields, geohashes, or something else in the future). patch to be added soon.",
        "label": 7
    },
    {
        "text": "fieldinfo omitterms bug around line 95 you have: if (this.omittf != omittf) { this.omittf = true; // if one require omittf at least once, it remains off for life } both references of the omittf booleans in the if statement refer to the same field. i am guessing its meant to be other.omittf like the norms code above it.",
        "label": 33
    },
    {
        "text": "support non static methods in the javascript compiler allow methods such as date.getmonth() or string.getordinal() to be added in the same way expression variables are now (forwarded to the bindings for processing). this change will only allow non-static methods that have zero arguments due to current limitations in the architecture, and to keep the change simple.",
        "label": 41
    },
    {
        "text": "numericutils floattosortableint doubletosortablelong does not sort certain nan ranges correctly and numericrangequery produces wrong results for nans with half open ranges the current implementation of floattosortableint does not account for different nan ranges which may result in nans sorted before -infinity and after +infinity. the default java ordering is: all nans after infinity. a possible fix is to make all nans canonic \"quiet nan\" as in: // canonicalize nan ranges. i assume this check will be faster here than  // (v == v) == false on the fpu? we don't distinguish between different // flavors of nans here (see http://en.wikipedia.org/wiki/nan). i guess // in java this doesn't matter much anyway. if ((v & 0x7fffffff) > 0x7f800000) {   // apply the logic below to a canonical \"quiet nan\"   return 0x7fc00000 ^ 0x80000000; } i don't commit because i don't know how much of the existing stuff relies on this (nobody should be keeping different nans in their indexes, but who knows...).",
        "label": 53
    },
    {
        "text": "toblockjoinfieldcomparator wrapping is illegal the following test case triggers an assertionerror:   public void testmissingvalues() throws ioexception {     final directory dir = newdirectory();     final randomindexwriter w = new randomindexwriter(random(), dir, newindexwriterconfig(new mockanalyzer(random()))         .setmergepolicy(nomergepolicy.instance));     w.adddocument(new document());     w.getreader().close();     w.adddocument(new document());     indexreader reader = w.getreader();     w.close();     indexsearcher searcher = newsearcher(reader);     // all docs are parent     bitdocidsetfilter parentfilter = new bitdocidsetcachingwrapperfilter(new querywrapperfilter(new matchalldocsquery()));     bitdocidsetfilter childfilter = new bitdocidsetcachingwrapperfilter(new querywrapperfilter(new matchnodocsquery()));     toparentblockjoinsortfield sortfield = new toparentblockjoinsortfield(         \"some_random_field\", sortfield.type.string, false, parentfilter, childfilter     );     sort sort = new sort(sortfield);     topfielddocs topdocs = searcher.search(new matchalldocsquery(), 1, sort);     searcher.getindexreader().close();     dir.close();   } java.lang.assertionerror  at __randomizedtesting.seedinfo.seed([e9d45d81f597ae4b:83490fc7d11d9aba]:0)  at org.apache.lucene.search.fieldcomparator$termordvalcomparator.setbottom(fieldcomparator.java:800)  at org.apache.lucene.search.fieldcomparator$termordvalcomparator.getleafcomparator(fieldcomparator.java:783)  at org.apache.lucene.search.join.toparentblockjoinfieldcomparator.dosetnextreader(toparentblockjoinfieldcomparator.java:83)  at org.apache.lucene.search.simplefieldcomparator.getleafcomparator(simplefieldcomparator.java:36)  at org.apache.lucene.search.fieldvaluehitqueue.getcomparators(fieldvaluehitqueue.java:183)  at org.apache.lucene.search.topfieldcollector$nonscoringcollector.getleafcollector(topfieldcollector.java:141)  at org.apache.lucene.search.filtercollector.getleafcollector(filtercollector.java:40)  at org.apache.lucene.search.assertingcollector.getleafcollector(assertingcollector.java:48)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:611)  at org.apache.lucene.search.assertingindexsearcher.search(assertingindexsearcher.java:92)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:424)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:543)  at org.apache.lucene.search.indexsearcher.searchafter(indexsearcher.java:528)  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:455)  at org.apache.lucene.search.join.testblockjoinsorting.testmissingvalues(testblockjoinsorting.java:347)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:483)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1627)  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:836)  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:872)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:886)  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:49)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:65)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:365)  at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:798)  at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:458)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:845)  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:747)  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:781)  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:792)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42)  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:54)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:65)  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:365)  at java.lang.thread.run(thread.java:745) the reason is that when a parent document does not have children, toparentblockjoincomparator simply omits to forward calls to copy to the wrapped comparator. so the wrapped comparator ends up with allocated slots that have 0 as an ordinal (the default value in an array) and a null value, which is illegal since 0 is a legal ordinal which can't map to null. this can't be fixed without adding new methods to the already crazy comparator api, so i think there is nothing we can do but remove this comparator. it would be possible to achieve the same functionnality by implementing something similar to sortednumericselector, except that it would have to select across several docs instead of values.",
        "label": 1
    },
    {
        "text": "drop java  support  its been discussed here and there, but i think we need to drop java 5 \"support\", for these reasons: its totally untested by any continual build process. testing java5 only when there is a release candidate ready is not enough. if we are to claim \"support\" then we need a hudson actually running the tests with java 5. its now unmaintained, so bugs have to either be hacked around, tests disabled, warnings placed, but some things simply cannot be fixed... we cannot actually \"support\" something that is no longer maintained: we do find jre bugs (http://wiki.apache.org/lucene-java/sunjavabugs) and its important that bugs actually get fixed: cannot do everything with hacks. because of its limitations, we do things like allow 20% slower grouping speed. i find it hard to believe we are sacrificing performance for this. so, in summary: because we don't test it at all, because its buggy and unmaintained, and because we are sacrificing performance, i think we need to cutover the build system for the next release to require java 6.",
        "label": 46
    },
    {
        "text": "make oal document field reuse its internal stringtokenstream followup from lucene-4930: field.java has a private stringtokenstream which is used as tokenstream implementation for stringfield (single value string tokens). unfortunately this tokenstream is created on every new document/field while indexing, making the cost of creating the ts a significant time. with very old java versions this also involves a lock in referencequeue.poll() when called from addattribute(). in lucene 3.x, docinverterperthread has a private thread-local attributesource for reusing, but because this was factored out to field.java, we can no longer use closeablethreadlocal (because field are not closeable). we should maybe move the special one-token tokenstream back to docinverterperthread and just let field.java delegate there. i know this would let us move back to 3.x where we had special handling of single token fields in the indexer.... another approach would be to make field.java use a static keywordanalyzer (it needs then be moved to core) or we add a threadlocal to field.java (which may be expensive). unfortunately this makes it hard to maintain, as the thread-localness is also needed to be bound to the indexwriter instance. because you could have 2 indexwriters open at same time and add documents to both of them from one thread... this brings us back to my previous solution.",
        "label": 53
    },
    {
        "text": "numericrangequery errors with endpoints near long min and max values this problem first reported in solr: http://lucene.472066.n3.nabble.com/range-query-on-trielongfield-strange-result-tt970974.html#a970974",
        "label": 53
    },
    {
        "text": "mtq rewrite   weight scorer init should be single pass spinoff of lucene-2690 (see the hacked patch on that issue)... once we fix mtq rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite.",
        "label": 46
    },
    {
        "text": "upgrade antlr to version simple upgrade to antlr 4.5.1 which includes numerous bug fixes: https://github.com/antlr/antlr4/releases/tag/4.5.1 note this does not change the grammar itself, only small pieces of the generated code.",
        "label": 53
    },
    {
        "text": "attributesource's methods for accessing attributes should be final  else its easy to corrupt the internal states the methods that operate and modify the internal maps of attributesource should be final, which is a backwards break. but anybody that overrides such methods simply creates a buggy as either case. i want to makeall impls final (in general the class should be final at all, but it is made for extension in tokenstream). so its important that the implementations are final!",
        "label": 53
    },
    {
        "text": "separately specify a field's type this came up from dicussions on irc. i'm summarizing here... today when you make a field to add to a document you can set things index or not, stored or not, analyzed or not, details like omittfap, omitnorms, index term vectors (separately controlling offsets/positions), etc. i think we should factor these out into a new class (fieldtype?). then you could re-use this fieldtype instance across multiple fields. the field instance would still hold the actual value. we could then do per-field analyzers by adding a setanalyzer on the fieldtype, instead of the separate perfieldanalzyerwrapper (likewise for per-field codecs (with flex), where we now have perfieldcodecwrapper). this would not be a schema! it's just refactoring what we already specify today. eg it's not serialized into the index. this has been discussed before, and i know michael busch opened a more ambitious (i think?) issue. i think this is a good first baby step. we could consider a hierarchy of fieldtype (numericfieldtype, etc.) but maybe hold off on that for starters...",
        "label": 33
    },
    {
        "text": "teststressindexing has intermittent failures see http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below: ok, i have seen this twice in the last two days: testsuite: org.apache.lucene.index.teststressindexing [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 18.58 sec [junit] [junit] ------------- standard output --------------- [junit] java.lang.nullpointerexception [junit] at org.apache.lucene.store.raminputstream.readbyte(raminputstream.java:67) [junit] at org.apache.lucene.store.indexinput.readint(indexinput.java:66) [junit] at org.apache.lucene.index.segmentinfos $findsegmentsfile.run(segmentinfos.java:544) [junit] at org .apache .lucene.index.directoryindexreader.open(directoryindexreader.java:63) [junit] at org.apache.lucene.index.indexreader.open(indexreader.java:209) [junit] at org.apache.lucene.index.indexreader.open(indexreader.java:192) [junit] at org.apache.lucene.search.indexsearcher.<init>(indexsearcher.java:56) [junit] at org.apache.lucene.index.teststressindexing $searcherthread.dowork(teststressindexing.java:111) [junit] at org.apache.lucene.index.teststressindexing $timedthread.run(teststressindexing.java:55) [junit] ------------- ---------------- --------------- [junit] testcase: teststressindexandsearching (org.apache.lucene.index.teststressindexing): failed [junit] hit unexpected exception in search1 [junit] junit.framework.assertionfailederror: hit unexpected exception in search1 [junit] at org .apache .lucene.index.teststressindexing.runstresstest(teststressindexing.java: 159) [junit] at org .apache .lucene .index .teststressindexing .teststressindexandsearching(teststressindexing.java:187) [junit] [junit] [junit] test org.apache.lucene.index.teststressindexing failed subsequent runs have, however passed. has anyone else hit this on trunk? i am running using \"ant clean test\" i'm on a mac pro 4 core, 4gb machine, if that helps at all. not sure how to reproduce at this point, but strikes me as a threading issue. oh joy! i'll try to investigate more tomorrow to see if i can dream up a test case. -grant",
        "label": 33
    },
    {
        "text": "improve phrasequery tostring  phrasequery.tostring() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. this may be misleading when presenting phrase queries built using complex analyzers and filters.",
        "label": 33
    },
    {
        "text": "add license headers to prettify js files followup after lucene-5055: the prettify js files have no license header (should be asf according to google code). the minimized javascript files were compressed with suppressing license headers (yui-compressor has an option to remove it, and this was done here, which does not conform to apaches source code requirements).",
        "label": 53
    },
    {
        "text": "add analyzinginfixsuggester our current suggester impls do prefix matching of the incoming text against all compiled suggestions, but in some cases it's useful to allow infix matching. e.g, netflix does infix suggestions in their search box. i did a straightforward impl, just using a normal lucene index, and using postingshighlighter to highlight matching tokens in the suggestions. i think this likely only works well when your suggestions have a strong prior ranking (weight input to build), eg netflix knows the popularity of movies.",
        "label": 33
    },
    {
        "text": "uploade lucene to ibiblio please uploaded lucene (specifically lucene-core) 2.1.0 to ibiblio. i see 2.0.0 but not 2.1.0. thanks!",
        "label": 32
    },
    {
        "text": "improve build system when tests hang currently, if tests hang in hudson it can go hung for days until we manually kill it. the problem is that when a hang happens its probably serious, what we want to do (i think), is: time out the build. ensure we have enough debugging information to hopefully fix any hang. so i think the ideal solution would be: add a sysprop \"-d\" that lucenetestcase respects, it could default to no timeout at all (some value like zero). when a timeout is set, lucenetestcase spawns an additional timer thread for the test class? method? if the timeout is exceeded, lucenetestcase dumps all thread/stack information, random seed information to hopefully reproduce the hang, and fails the test. nightly builds would pass some reasonable -d for each test. separately, i think we should have an \"ant-level\" timeout for the whole build, in case it goes completely crazy (e.g. jvm completely hangs or something else), just as an additional safety.",
        "label": 11
    },
    {
        "text": "remove old static main methods in core we have a few random static main methods that i think are very rarely used... we should remove them (indexreader, utf32toutf8, english). the indexreader main lets you list / extract the sub-files from a cfs... i think we should move this to a new tool in contrib/misc.",
        "label": 33
    },
    {
        "text": "tokenfilters with a null value in the constructor fail while migrating from 2.4.x to 2.9-dev i found a lot of failing unittests. one problem is with tokenfilters that do a super(null) in the constructor. i fixed it by changing the constructor to super(new emptytokenstream()) this will cause problems and frustration to others while migrating to 2.9.",
        "label": 33
    },
    {
        "text": " patch  kstem for lucene september 10th 2003 contributionn from \"sergio guzman-lara\" <guzman@cs.umass.edu> original email: hi all, i have ported the kstem stemmer to java and incorporated it to lucene. you can get the source code (kstem.jar) from the following website: http://ciir.cs.umass.edu/downloads/ just click on \"kstem java implementation\" (you will need to register your e-mail, for free of course, with the ciir --center for intelligent information retrieval, umass \u2013 and get an access code). content of kstem.jar: java/org/apache/lucene/analysis/kstemdata1.java java/org/apache/lucene/analysis/kstemdata2.java java/org/apache/lucene/analysis/kstemdata3.java java/org/apache/lucene/analysis/kstemdata4.java java/org/apache/lucene/analysis/kstemdata5.java java/org/apache/lucene/analysis/kstemdata6.java java/org/apache/lucene/analysis/kstemdata7.java java/org/apache/lucene/analysis/kstemdata8.java java/org/apache/lucene/analysis/kstemfilter.java java/org/apache/lucene/analysis/kstemmer.java kstemdata1.java, ..., kstemdata8.java contain several lists of words used by kstem kstemmer.java implements the kstem algorithm kstemfilter.java extends tokenfilter applying kstem to compile unjar the file kstem.jar to lucene's \"src\" directory, and compile it there. what is kstem? a stemmer designed by bob krovetz (for more information see http://ciir.cs.umass.edu/pubfiles/ir-35.pdf). copyright issues this is open source. the actual license agreement is included at the top of every source file. any comments/questions/suggestions are welcome, sergio guzman-lara senior research fellow ciir umass",
        "label": 40
    },
    {
        "text": "indexwriter deletes non lucene files carl austin raised a good issue in a comment on my lucene 4.0.0 alpha blog post: http://blog.mikemccandless.com/2012/07/lucene-400-alpha-at-long-last.html indexwriter will now (as of 4.0) delete all foreign files from the index directory. we made this change because codecs are free to write to any files now, so the space of filenames is hard to \"bound\". but if the user accidentally uses the wrong directory (eg c:/) then we will in fact delete important stuff. i think we can at least use some simple criteria (must start with _, maybe must fit certain pattern eg _<base36>(_x).y), so we are much less likely to delete a non-lucene file....",
        "label": 33
    },
    {
        "text": "support sortedsource in multidocvalues multidocvalues doesn't support sorted variant ie. sortedsource but throws unsupportedoperationexception. this forces users to work per segment. for consistency we should support sortedsource also if we wrap the docvalues in mdv.",
        "label": 46
    },
    {
        "text": "add kamikaze into lucene kamikaze 3.0.1 is the updated version of kamikaze 2.0.0. it can achieve significantly better performance then kamikaze 2.0.0 in terms of both compressed size and decompression speed. the main difference between the two versions is kamikaze 3.0.x uses the much more efficient implementation of the pfordelta compression algorithm. my goal is to integrate the highly efficient pfordelta implementation into lucene codec.",
        "label": 1
    },
    {
        "text": "improve exception message  child query must only match non parent docs  but parent docid  when parent filter intersects with child query the exception exposes internal details: docnum and scorer class. i propose an exception message to suggest to execute a query intersecting them both. there is an opinion to add this suggestion in addition to existing details. my main concern against is, when index is constantly updated even solr-9582 allows to search for docnum it would be like catching the wind, also think about cloud case. but, user advised with executing query intersection can catch problem documents even if they occurs sporadically.",
        "label": 35
    },
    {
        "text": "reopen on nrt reader should share readers w  unchanged segments a repoen on an nrt reader doesn't seem to share readers for those segments that are unchanged. http://search.lucidimagination.com/search/document/9f0335d480d2e637/nrt_and_caching_based_on_indexreader",
        "label": 33
    },
    {
        "text": "refactor spatial contrib  filter   query  classes from erik's comments in lucene-1387 distancequery is awkwardly named. it's not an (extends) query.... it's a pojo with helpers. maybe distancequeryfactory? (but it creates a filter also) cartesianpolyfilter is not a filter (but cartesianshapefilter is)",
        "label": 42
    },
    {
        "text": "getpayloadspans on org apache lucene search spans spanquery should be abstract i just spent a long time tracking down a bug resulting from upgrading to lucene 2.4.1 on a project that implements some spanquerys of its own and was written against 2.3. since the project's spanquerys didn't implement getpayloadspans, the call to that method went to spanquery.getpayloadspans which returned null and caused a nullpointerexception in the lucene code, far away from the actual source of the problem. it would be much better for this kind of thing to show up at compile time, i think. thanks!",
        "label": 29
    },
    {
        "text": "extendablequeryparser should allow extensions to access the toplevel parser settings  properties based on the latest discussions in lucene-2039 this issue will expose the toplevel parser via the extensionquery so that extensionparsers can access properties like getallowleadingwildcards() from the top level parser.",
        "label": 46
    },
    {
        "text": "longvaluessource doublevaluessource impls should implement equals hashcode given that we embed values sources in queries and sorts, from which we expect that they have working equals/hashcode, we should also implement equals/hashcode on longvaluessource/doublevaluessource impls.",
        "label": 2
    },
    {
        "text": "add a jruby interface and a luke like utiltiy app still pretty early, but available for anyone with sufficient interest and bravado.",
        "label": 48
    },
    {
        "text": "exception during indexwriter close  prevents release of the write lock after encountering a case of index corruption - see http://issues.apache.org/jira/browse/lucene-140 - when the close() method encounters an exception in the flushramsegments() method, the index write.lock is not released (ie. it is not really closed). the writelock is only released when the indexwriter is gc'd and finalize() is called.",
        "label": 33
    },
    {
        "text": "public api inconsistency org.apache.lucene.index.segmentinfos is public, and contains public methods (which is good for expert-level index manipulation tools such as luke). however, segmentinfo class has package visibility. this leads to a strange result that it's possible to read segmentinfos, but it's not possible to access its details (segmentinfos.info(int)) from a user application. the solution is to make segmentinfo class public.",
        "label": 33
    },
    {
        "text": "memoryindex tostring is broken if you enable payloads noticed this as we use luwak which creates a memoryindex(true, true) storing both offsets and payloads (though in reality we never put any payloads in it). we used to use memoryindex.tostring() for debugging and noticed it broke in lucene 5.x and beyond. i think lucene-6155 broke it when it added support for payloads? creating default memoryindex (as all the tests currently do) works fine, as does one with just offsets, it is just the payload version which is broken.",
        "label": 10
    },
    {
        "text": "integrate snowball stopword lists the snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup this patch includes the following: snowball stopword lists for 13 languages in contrib/snowball/resources all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to utf-8 added getsnowballwordset to wordlistloader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments. i did not add any changes to snowballanalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.",
        "label": 40
    },
    {
        "text": "java lang arrayindexoutofboundsexception on reading data java.lang.arrayindexoutofboundsexception at org.apache.lucene.codecs.compressing.lz4.decompress(lz4.java:132) at org.apache.lucene.codecs.compressing.compressionmode$4.decompress(compressionmode.java:135) at org.apache.lucene.codecs.compressing.compressingstoredfieldsreader.visitdocument(compressingstoredfieldsreader.java:336) at org.apache.lucene.index.segmentreader.document(segmentreader.java:133) at org.apache.lucene.index.basecompositereader.document(basecompositereader.java:110) at org.apache.lucene.index.slowcompositereaderwrapper.document(slowcompositereaderwrapper.java:212) at org.apache.lucene.index.filteratomicreader.document(filteratomicreader.java:365) at org.apache.lucene.index.basecompositereader.document(basecompositereader.java:110) at org.apache.lucene.index.indexreader.document(indexreader.java:447) at org.apache.lucene.search.indexsearcher.doc(indexsearcher.java:204)",
        "label": 1
    },
    {
        "text": " patch  some field methods use classcast check instead of instanceof which is slow i am not sure if this is because lucene historically needed to work with older jvm's but with modern jvm's, instanceof is much quicker. the field.stringvalue(), .readervalue(), and .binaryvalue() methods all use classcastexception checking. using the following test-bed class, you will see that instanceof is miles quicker: package com.aconex.index; public class classcastexceptiontest { private static final long iterations = 100000; /** @param args */ public static void main(string[] args) { runclasscasttest(1); // once for warm up runclasscasttest(2); runinstanceofcheck(1); runinstanceofcheck(2); } private static void runinstanceofcheck(int run) { long start = system.currenttimemillis(); object foo = new foo(); for (int i = 0; i < iterations; i++) { string test; if(foo instanceof string) { system.out.println(\"is a string\"); // should never print } } long end = system.currenttimemillis(); long diff = end - start; system.out.println(\"instanceof checking run #\" + run + \": \" + diff + \"ms\"); } private static void runclasscasttest(int run) { long start = system.currenttimemillis(); object foo = new foo(); for (int i = 0; i < iterations; i++) { string test; try { test = (string)foo; } catch (classcastexception c) { // ignore } } long end = system.currenttimemillis(); long diff = end - start; system.out.println(\"classcast checking run #\" + run + \": \" + diff + \"ms\"); } private static final class foo { } } results ======= run #1 classcast checking run #1: 1660ms classcast checking run #2: 1374ms instanceof checking run #1: 8ms instanceof checking run #2: 4ms run #2 classcast checking run #1: 1280ms classcast checking run #2: 1344ms instanceof checking run #1: 7ms instanceof checking run #2: 2ms run #3 classcast checking run #1: 1347ms classcast checking run #2: 1250ms instanceof checking run #1: 7ms instanceof checking run #2: 2ms this could explain why documents with more fields scales worse, as in, for lots of documents with lots of fields, the effect is exacerbated.",
        "label": 55
    },
    {
        "text": "add contrib fast vector highlighter to maven central repo i'm not at all familiar with the lucene build/deployment process, but it would be very nice if releases of the fast vector highlighter were pushed to the maven central repository, as is done with other contrib modules. (issue filed at the request of grant ingersoll.)",
        "label": 46
    },
    {
        "text": "cached filter for a single term field these classes implement inexpensive range filtering over a field containing a single term. they do this by building an integer array of term numbers (storing the term->number mapping in a treemap) and then implementing a fast integer comparison based docsetiditerator. this code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. i have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. the code here is fairly rough; it works but lacks javadocs and tostring() and hashcode() methods etc. i'm posting it here to discover if there is other interest in this feature; i don't mind fixing it up but would hate to go to the effort if it's not going to make it into lucene.",
        "label": 53
    },
    {
        "text": "indexreader iscurrent race revision: 1431169 ant test -dtestcase=testnrtmanager -dtests.method=testthreadstarvationnodeletenrtreader -dtests.seed=925ecd106fbfa3ff -dtests.slow=true -dtests.locale=fr_ca -dtests.timezone=america/kentucky/louisville -dtests.file.encoding=us-ascii -dtests.dups=500",
        "label": 46
    },
    {
        "text": "javadoc of searchermanager close is not crystal clear raised from http://www.mail-archive.com/java-user@lucene.apache.org/msg40064.html the javadoc says:   /**    * close this referencemanager to future {@link #acquire() acquiring}. any    * references that were previously {@link #acquire() acquired} won't be    * affected, and they should still be {@link #release released} when they are    * not needed anymore.    */ the first sentence is not really clear. i would expect something like: close this referencemanager when the application is shutting down or the  underlying index will be disposed. any references that were previously  {@link #acquire() acquired} won't be affected, and they should still be  {@link #release released} when they are not needed anymore. further more, the javadoc does not declare that an exception will be thrown on any method.",
        "label": 46
    },
    {
        "text": "make topdocs constructor public topdocs constructor is package visible. this prevents instantiating it from outside this package. for example, i wrote a hitcolletor that couldn't extend directly from topdoccollector. i need to create a new topdocs instance, however since the c'tor is package visible, i can't do that. for now, i completely duplicated the code, but i hope you'll fix it soon.",
        "label": 32
    },
    {
        "text": "resurrect org apache lucene facet util ordinalmappingatomicreader from lucene > 4.6.1 the class: org.apache.lucene.facet.util.ordinalmappingatomicreader was removed; resurrect it because used merging indexes related to merged taxonomies.",
        "label": 43
    },
    {
        "text": "fail the build on wrong svn eol style i'm tired of fixing this before releases. jenkins should detect and fail on this.",
        "label": 53
    },
    {
        "text": "remove deprecated methods in booleanquery remove deprecated methods setusescorer14 and getusescorer14 in booleanquery, and adapt javadocs.",
        "label": 32
    },
    {
        "text": "payloadnearquery has hardwired explanation for 'averagepayloadfunction' the 'explain' method in payloadnearspanscorer assumes the averagepayloadfunction was used. this patch adds the 'explain' method to the 'payloadfunction' interface, where the scorer can call it. added unit tests for 'explain' and for {min,max} payloadfunction.",
        "label": 15
    },
    {
        "text": "suggeststopfilter should have a factory while trying to use the new suggester in solr i realized that suggeststopfilter did not have a factory. we should add one.",
        "label": 47
    },
    {
        "text": "make the build more friendly to apache harmony as part of improved testing, i thought it would be a good idea to make the build (ant test) more friendly to working under apache harmony. i'm not suggesting we de-optimize code for sun jvms or anything crazy like that, only use it as a tool. for example: bugs in tests/code: for example i found a test that expected arrayioobe when really the javadoc contract for the method is just ioobe... it just happens to pass always on sun jvm because thats the implementation it always throws. better reproduction of bugs: for example 2 months out of the year it seems testqueryparser fails with thai locale in a difficult-to-reproduce way. but i always get similar failures like this with harmony for this test class. better stability and portability: we should try (if reasonable) to avoid depending upon internal details. the same kinds of things that fail in harmony might suddenly fail in a future sun jdk. because its such a different impl, it brings out a lot of interesting stuff. at the moment there are currently a lot of failures, i think a lot might be caused by this: http://permalink.gmane.org/gmane.comp.java.harmony.devel/39484",
        "label": 40
    },
    {
        "text": "nativefslockfactory loses the channel when a thread is interrupted and the solrcore becomes unusable after the condition is rare for us and seems basically a race.  if a thread that is running just happens to have the filechannel open for nativefslockfactory and is interrupted, the channel is closed since it extends abstractinterruptiblechannel unfortunately this means the solr core has to be unloaded and reopened to make the core usable again as the ensurevalid check forever throws an exception after. org.apache.lucene.store.alreadyclosedexception: filelock invalidated by an external force: nativefslock(path=data/index/write.lock,impl=sun.nio.ch.filelockimpl[0:9223372036854775807 exclusive invalid],creationtime=2018-04-06t21:45:11z) at org.apache.lucene.store.nativefslockfactory$nativefslock.ensurevalid(nativefslockfactory.java:178) at org.apache.lucene.store.lockvalidatingdirectorywrapper.createoutput(lockvalidatingdirectorywrapper.java:43) at org.apache.lucene.store.trackingdirectorywrapper.createoutput(trackingdirectorywrapper.java:43) at org.apache.lucene.codecs.compressing.compressingstoredfieldswriter.<init>(compressingstoredfieldswriter.java:113) at org.apache.lucene.codecs.compressing.compressingstoredfieldsformat.fieldswriter(compressingstoredfieldsformat.java:128) at org.apache.lucene.codecs.lucene50.lucene50storedfieldsformat.fieldswriter(lucene50storedfieldsformat.java:183)   proposed solution is using asynchronousfilechannel instead, since this is only operating on a lock and .size method",
        "label": 13
    },
    {
        "text": "suggest fst sort buffersize should not automatically fail just because of freememory  follow up op dev thread: fstcompletiontest failure \"at least 0.5mb ram buffer is needed\"",
        "label": 12
    },
    {
        "text": "geocomplexpolygon failures i have tightened a bit more the random test for polygons and geocomplexpolygons still shows issues when traveling planes that are cutting the world near the pole. i could identify three cases:   case 1) it happens when the check point is on aone of the test point planes but the planes is close to the pole and cannot be traversed. in that case we hit the following part of the code: } else if (testpointfixedyplane.evaluateiszero(x, y, z) || testpointfixedxplane.evaluateiszero(x, y, z) || testpointfixedzplane.evaluateiszero(x, y, z)) {   throw new illegalargumentexception(\"can't compute iswithin for specified point\"); } else {   it seems this check is unnecesary. if removed then a traversal is choosen and evrything works as expected.   case 2) in this case a dualcrossingedgeiterator is used with one of the planes being close to the pole but inside current restricutions (is a valid traversal). i think the problem happens when computing the intersection points for above and below plane in computeinsideoutside: final geopoint[] outsideoutsidepoints = testpointoutsideplane.findintersections(planetmodel, traveloutsideplane);  //these don't add anything: , checkpointcutoffplane, testpointcutoffplane); final geopoint outsideoutsidepoint = pickproximate(outsideoutsidepoints); the intersection above results in two points close to each other and close to the intersection point, and therefore pickproximate fails in choosing the right one. case 3) in this case a linearcrossingedgeiterator is used with the plane being close to the pole. in this case when evaluating the intersection between an edge and the plane, we get two intersections (because are very close together) inside the bounds instead of one. the result is too many crossings.   after evaluating this errors i think we should really prevent using planes that are near a pole. i attached a new version of geocomplexpolygon that seems to solve this issues. the approach is the following:  near_edge_cutoff is not expressed as a linear distance but as a percentage, curerntly 0.75 . if ab is the value of a semiaxis, the logic disallows to travel a plane if the distance between the plane and the center of the world is bigger that  near_edge_cutoff * pole.                ",
        "label": 25
    },
    {
        "text": "org apache lucene util fst fst should skip over outputs it is not interested in currently the fst uses the read(datainput) method from the outputs class to skip over outputs it actually is not interested in. for most use cases this just creates some additional objects that are immediately destroyed again. when traversing an fst with non-trivial data however this can easily add up to several excess objects that nobody actually ever read.",
        "label": 33
    },
    {
        "text": "support grouping by indexdocvalues although idv is not yet finalized (more particular the sortedsource). i think we already can discuss / investigate implementing grouping by idv.",
        "label": 31
    },
    {
        "text": "tessellator  isintersectingpolygon method skip polygon edges the following condition seems wrong: if(node.getx() != x0 && node.gety() != y0 && nextnode.getx() != x0     && nextnode.gety() != y0 && node.getx() != x1 && node.gety() != y1     && nextnode.getx() != x1 && nextnode.gety() != y1) {    //check intersection } any node with the same x or y is skipped. ",
        "label": 19
    },
    {
        "text": "set default precisionstep for numericfield and numericrangefilter this is a spinoff from lucene-1701. a user using numeric* should not need to understand what's \"under the hood\" in order to do their indexing & searching. they should be able to simply: doc.add(new numericfield(\"price\", 15.50); and have a decent default precisionstep selected for them. actually, if we add ctors to numericfield for each of the supported types (so the above code works), we can set the default per-type. i think we should do that? 4 for int and 6 for long was proposed as good defaults. the default need not be \"perfect\", as advanced users can always optimize their precisionstep, and for users experiencing slow rangequery performance, numericrangequery with any of the defaults we are discussing will be much faster.",
        "label": 53
    },
    {
        "text": "iw deadlocks if commit and reopen happens concurrently while exception is hit i just hit this while working on an elasticseach test using a lucene 5.1 snapshot (5.1.0-snapshot-1654549). the test throws random exceptions via mockdirwrapper and deadlocks, jstack says: found one java-level deadlock: ============================= \"elasticsearch[node_2][refresh][t#2]\":   waiting to lock monitor 0x00007fe51314c098 (object 0x00000007018ee8d8, a java.lang.object),   which is held by \"elasticsearch[node_2][generic][t#1]\" \"elasticsearch[node_2][generic][t#1]\":   waiting to lock monitor 0x00007fe512d74b68 (object 0x00000007018ee8e8, a java.lang.object),   which is held by \"elasticsearch[node_2][refresh][t#2]\" java stack information for the threads listed above: =================================================== \"elasticsearch[node_2][refresh][t#2]\":  at org.apache.lucene.index.indexwriter.tragicevent(indexwriter.java:4441)  - waiting to lock <0x00000007018ee8d8> (a java.lang.object)  at org.apache.lucene.index.indexwriter.getreader(indexwriter.java:436)  - locked <0x00000007018ee8e8> (a java.lang.object)  at org.apache.lucene.index.standarddirectoryreader.doopenfromwriter(standarddirectoryreader.java:281)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:256)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:246)  at org.apache.lucene.index.filterdirectoryreader.doopenifchanged(filterdirectoryreader.java:104)  at org.apache.lucene.index.directoryreader.openifchanged(directoryreader.java:123)  at org.apache.lucene.search.searchermanager.refreshifneeded(searchermanager.java:137)  at org.apache.lucene.search.searchermanager.refreshifneeded(searchermanager.java:58)  at org.apache.lucene.search.referencemanager.domayberefresh(referencemanager.java:176)  at org.apache.lucene.search.referencemanager.mayberefreshblocking(referencemanager.java:253)  at org.elasticsearch.index.engine.internal.internalengine.refresh(internalengine.java:703)  at org.elasticsearch.index.shard.indexshard.refresh(indexshard.java:500)  at org.elasticsearch.index.shard.indexshard$enginerefresher$1.run(indexshard.java:954)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:745) \"elasticsearch[node_2][generic][t#1]\":  at org.apache.lucene.index.indexwriter.preparecommitinternal(indexwriter.java:2730)  - waiting to lock <0x00000007018ee8e8> (a java.lang.object)  - locked <0x00000007018ee8d8> (a java.lang.object)  at org.apache.lucene.index.indexwriter.commitinternal(indexwriter.java:2888)  - locked <0x00000007018ee8d8> (a java.lang.object)  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2855)  at org.elasticsearch.index.engine.internal.internalengine.commitindexwriter(internalengine.java:722)  at org.elasticsearch.index.engine.internal.internalengine.flush(internalengine.java:800)  at org.elasticsearch.index.engine.internal.internalengine$recoverycounter.endrecovery(internalengine.java:1520)  at org.elasticsearch.index.engine.internal.internalengine$recoverycounter.close(internalengine.java:1533)  at org.elasticsearch.common.lease.releasables.close(releasables.java:45)  at org.elasticsearch.common.lease.releasables.closewhilehandlingexception(releasables.java:70)  at org.elasticsearch.common.lease.releasables.closewhilehandlingexception(releasables.java:75)  at org.elasticsearch.index.engine.internal.internalengine.recover(internalengine.java:1048)  at org.elasticsearch.index.shard.indexshard.recover(indexshard.java:635)  at org.elasticsearch.indices.recovery.recoverysource.recover(recoverysource.java:120)  at org.elasticsearch.indices.recovery.recoverysource.access$200(recoverysource.java:48)  at org.elasticsearch.indices.recovery.recoverysource$startrecoverytransportrequesthandler.messagereceived(recoverysource.java:141)  at org.elasticsearch.indices.recovery.recoverysource$startrecoverytransportrequesthandler.messagereceived(recoverysource.java:127)  at org.elasticsearch.transport.local.localtransport$2.dorun(localtransport.java:287)  at org.elasticsearch.common.util.concurrent.abstractrunnable.run(abstractrunnable.java:36)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:745) found 1 deadlock.",
        "label": 33
    },
    {
        "text": "add tool to upgrade all segments of an index to last recent supported index format without optimizing currently if you want to upgrade an old index to the format of your current lucene version, you have to optimize your index or use addindexes(indexreader...) [see lucene-2893] to copy to a new directory. the optimize() approach fails if your index is already optimized. i propose to add a custom mergepolicy to upgrade all segments to the last format. this mergepolicy could simply also ignore all segments already up-to-date. all segments in prior formats would be merged to a new segment using another mergepolicy's optimize strategy. this issue is different from lucene-2893, as it would only support upgrading indexes from previous lucene versions in-place using the official path. its a tool for the end user, not a developer tool. this addition should also go to lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. with this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes.",
        "label": 53
    },
    {
        "text": "npe in geo3drpttest testoperations  my jenkins found a reproducing seed on master: checking out revision 23422908165f62581c271524955af2ab0e6e069f (refs/remotes/origin/master) [...]   [junit4] suite: org.apache.lucene.spatial.spatial4j.geo3drpttest   [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.   [junit4]   2> note: reproduce with: ant test  -dtestcase=geo3drpttest -dtests.method=testoperations -dtests.seed=7658c0b00acb7faf -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=en-ie -dtests.timezone=mexico/general -dtests.asserts=true -dtests.file.encoding=iso-8859-1   [junit4] error   0.05s j5 | geo3drpttest.testoperations { seed=[7658c0b00acb7faf:9553873991970562]} <<<   [junit4]    > throwable #1: java.lang.nullpointerexception   [junit4]    >  at __randomizedtesting.seedinfo.seed([7658c0b00acb7faf:9553873991970562]:0)   [junit4]    >  at org.apache.lucene.spatial3d.geom.geobasebbox.isshapeinsidebbox(geobasebbox.java:49)   [junit4]    >  at org.apache.lucene.spatial3d.geom.georectangle.getrelationship(georectangle.java:212)   [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3dshape.relate(geo3dshape.java:85)   [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3dshape.relate(geo3dshape.java:71)   [junit4]    >  at org.locationtech.spatial4j.shape.impl.rectangleimpl.relate(rectangleimpl.java:165)   [junit4]    >  at org.apache.lucene.spatial.query.spatialoperation$4.evaluate(spatialoperation.java:86)   [junit4]    >  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperation(randomspatialopstrategytestcase.java:102)   [junit4]    >  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperationrandomshapes(randomspatialopstrategytestcase.java:55)   [junit4]    >  at org.apache.lucene.spatial.spatial4j.geo3drpttest.testoperations(geo3drpttest.java:114)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4]   2> note: test params are: codec=asserting(lucene60): {geo3drpttest_rpt=lucene50(blocksize=128), id=lucene50(blocksize=128)}, docvalues:{geo3drpttest_sdv=docvaluesformat(name=memory)}, maxpointsinleafnode=170, maxmbsortinheap=4.381892892935566, sim=classicsimilarity, locale=en-ie, timezone=mexico/general   [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=426660176,total=514850816   [junit4]   2> note: all tests run in this jvm: [randomspatialopfuzzyprefixtreetest, geo3drpttest]   [junit4] completed [18/23 (1!)] on j5 in 2.08s, 12 tests, 1 error <<< failures!",
        "label": 25
    },
    {
        "text": "spanorquery skipto  doesn't always move forwards in spanorquery the skipto() method is improperly implemented if the target doc is less than or equal to the current doc, since skipto() may not be called for any of the clauses' spans: public boolean skipto(int target) throws ioexception { if (queue == null) { return initspanqueue(target); } while (queue.size() != 0 && top().doc() < target) { if (top().skipto(target)) { queue.adjusttop(); } else { queue.pop(); } } return queue.size() != 0; } this violates the correct behavior (as described in the spans interface documentation), that skipto() should always move forwards, in other words the correct implementation would be: public boolean skipto(int target) throws ioexception { if (queue == null) { return initspanqueue(target); } boolean skipcalled = false; while (queue.size() != 0 && top().doc() < target) { if (top().skipto(target)) { queue.adjusttop(); } else { queue.pop(); } skipcalled = true; } if (skipcalled) { return queue.size() != 0; } return next(); }",
        "label": 29
    },
    {
        "text": "stopfilter should have option to incr positionincrement after stop word i've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it... stopfilter should have an option that if set, records how many stop words are \"skipped\" in a row, and then sets that value as the positionincrement on the \"next\" token that stopfilter does return.",
        "label": 12
    },
    {
        "text": "planes constructed with two points are wrong when points are close whenever a plane is constructed with two points (and the center of the planet), and those points are close to each other,it might happen that the final plane does not contain all points used for the construction. the issue seems to happen when the resulting magnitude of the normal vector of the new plane (calculated using cross product) is lower that 1e-5. this is a follow up of issue lucene-8133.",
        "label": 25
    },
    {
        "text": "closeablethreadlocal is now obsolete since lucene 3 depends on java 5, we can use threadlocal#remove() to take care or resource management.",
        "label": 33
    },
    {
        "text": "it is not possible to sort with sorttype auto on a field that does not exist when sorting with type auto on a field that does not exist, a runtimeexception is thrown from fieldcacheimpl.java line 346. this is especially a problem when using multisearcher. this issue is related to: http://issues.apache.org/jira/browse/lucene-374",
        "label": 29
    },
    {
        "text": "org apache lucene analysis hunspell hunspelldictionary should implement iconv and oconv lines in the affix file there are some hunspell dictionaries that need to emulate unicode normalization and collation in order to get the correct stem of a word. the original hunspell provides a way to do this with the iconv and oconv lines in the affix file. the lucene hunspelldictionary ignores these lines right now. please support these keys in the affix file. this bit of functionality is briefly described in the hunspell man page http://manpages.ubuntu.com/manpages/lucid/man4/hunspell.4.html this functionality is practically required in order to use a korean dictionary because you want only some of the jamos of a hangul character (grapheme cluster) when using stemming. other languages will find this to be helpful functionality. here is an example for a .aff file: iconv \uac01 \u1100\u1161\u11a8 ... oconv \u1100\u1161\u11a8 \uac01 here is the same example escaped. iconv \\uac01 \\u1100\\u1161\\u11a8 ... oconv \\u1100\\u1161\\u11a8 \\uac01",
        "label": 40
    },
    {
        "text": "enable bzip compression in benchmark bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. the plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams. it will add a dependency on ant.jar which contains two classes similar to gzipoutputstream and gzipinputstream which compress/decompress files using the bzip algorithm. bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower. i wil post a patch which adds this parameter and implement it in linedocmaker, enwikidocmaker and writelinedoc task. maybe even add the capability to docmaker or some of the super classes, so it can be inherited by all sub-classes.",
        "label": 29
    },
    {
        "text": "disjunctionsumscorer small tweak move scorerdocqueue initialization from next() and skipto() methods to the constructor. makes disjunctionsumscorer a bit faster (less than 1% on my tests). downside (if this is one, i cannot judge) would be throwing ioexception from disjunctionsumscorer constructors as we touch harddisk there. i see no problem as this ioexception does not propagate too far (the only modification i made is in booleanscorer2) if (scorerdocqueue == null) { initscorerdocqueue(); } attached test is just quick & dirty rip of testscorerperf from standard lucene test package. not included as patch as i do not like it. all test pass, patch made on trunk revision 613923",
        "label": 33
    },
    {
        "text": "fuzzyquery should not be final i am trying to extend the fuzzyquery to further filter the termenum. (i am indexing stem forms and original forms, but i only want to match original forms with a fuzzy term, otherwise i get to much noise). however, fuzzyquery is a final class and i cannot extend it. as discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected. i am attaching a patch for fuzzyquery.java that implements this. i ran all unit tests and they passed without errors. andreas.",
        "label": 48
    },
    {
        "text": "remove field param from computenorm  scorepayload   remove uoe'd lengthnorm  switch sweetspot to per field in lucene-2236 we switched sim to per field (similarityprovider returns a per-field similarity). but we didn't completely cleanup there... i think we should now do this: sweetspotsimilarity loses all its hashmaps. instead, just configure one per field and return it in your similarityprovider. this means for example, all its tf factors can now be configured per-field too, not just the length normalization factors. computenorm and scorepayload lose their field parameter, as its redundant and confusing. the uoe'd obselete lengthnorm is removed. i also updated javadocs that were pointing to it (this is bad!).",
        "label": 40
    },
    {
        "text": "analyzing suggester since we added shortest-path wfsa search in lucene-3714, and generified the comparator in lucene-3801, i think we should look at implementing suggesters that have more capabilities than just basic prefix matching. in particular i think the most flexible approach is to integrate with analyzer at both build and query time, such that we build a wfst with: input: analyzed text such as ghost0christmas0past <-- byte 0 here is an optional token separator output: surface form such as \"the ghost of christmas past\" weight: the weight of the suggestion we make an fst with pairoutputs<weight,output>, but only do the shortest path operation on the weight side (like the test in lucene-3801), at the same time accumulating the output (surface form), which will be the actual suggestion. this allows a lot of flexibility: using even standardanalyzer means you can offer suggestions that ignore stopwords, e.g. if you type in \"ghost of chr...\", it will suggest \"the ghost of christmas past\" we can add support for synonyms/wdf/etc at both index and query time (there are tradeoffs here, and this is not implemented!) this is a basis for more complicated suggesters such as japanese suggesters, where the analyzed form is in fact the reading, so we would add a tokenfilter that copies readingattribute into term text to support that... other general things like offering suggestions that are more \"fuzzy\" like using a plural stemmer or ignoring accents or whatever. according to my benchmarks, suggestions are still very fast with the prototype (e.g. ~ 100,000 qps), and the fst size does not explode (its short of twice that of a regular wfst, but this is still far smaller than tst or jaspell, etc).",
        "label": 33
    },
    {
        "text": "openbitset ensurecapacity does not modify numbits it's a simple bug, reproduced by this simple test:   public void testensurecapacity() {     openbitset bits = new openbitset(1);     bits.fastset(0);     bits.ensurecapacity(5); // make room for more bits     bits.fastset(2);   } the problem is that numbits which is used only for assrets isn't modified by ensurecapacity and so the next fastset trips the assert. i guess we should also fix ensurecapacitywords and test it too. i may not be able to fix this until sunday though, so if anyone wants to fix it before (maybe it can make it into 4.5.1), feel free.",
        "label": 43
    },
    {
        "text": "booleanquery hashcode and equals ignore iscoorddisabled booleanquery.iscoorddisabled() is not considered by booleanquery's hashcode() or equals() methods ... this can cause serious badness to happen when caching booleanqueries. bug traces back to at least 1.9",
        "label": 33
    },
    {
        "text": "only assign scoredoc shardindex if it was already assigned to non default   value when you use topdocs.merge today it always overrides the scoredoc#shardindex value. the assumption that is made here is that all shard results are merges at once which is not necessarily the case. if for instance incremental merge phases are applied the shard index doesn't correspond to the index in the outer topdocs array. to make this a backwards compatible but yet non-controversial change we could change the internals of topdocs#merge to only assign this value unless it's not been assigned before to a non-default (-1) value to allow multiple or sparse top docs merging.",
        "label": 46
    },
    {
        "text": "queryparser can produce empty sub booleanqueries when analyzer proudces no tokens for input as triggered by solr-261, if you have a query like this... +foo:bbb +(yak:aaa baz:ccc) ...where the analyzer produces no tokens for the \"yak:aaa\" or \"baz:ccc\" portions of the query (posisbly because they are stop words) the resulting query produced by the queryparser will be... +foo:bbb +() ...that is a booleanquery with two required clauses, one of which is an empty booleanquery with no clauses. this does not appear to be \"good\" behavior. in general, queryparser should be smarter about what it does when parsing encountering parens whose contents result in an empty booleanquery \u2013 but what exactly it should do in the following situations... a) +foo:bbb +() b) +foo:bbb () c) +foo:bbb -() ...is up for interpretation. i would think situation (b) clearly lends itself to dropping the sub-booleanquery completely. situation (c) may also lend itself to that solution, since semanticly it means \"don't allow a match on any queries in the empty set of queries\". .... i have no idea what the \"right\" thing to do for situation (a) is.",
        "label": 12
    },
    {
        "text": "when a test assume fails  display information currently if a test uses assume.assumetrue, it silently passes. i think we should output something, at least if you have verbose set, maybe always. here's an example of what the output might look like: junit-sequential:     [junit] testsuite: org.apache.solr.servlet.solrrequestparsertest     [junit] tests run: 4, failures: 0, errors: 0, time elapsed: 1.582 sec     [junit]     [junit] ------------- standard error -----------------     [junit] note: teststreamurl assume failed (ignored):     [junit] org.junit.internal.assumptionviolatedexception: got: <java.io.filenotfoundexception: http://www.apdfgdfgache .org/dist/lucene/solr/>, expected: null     [junit]     at org.junit.assume.assumethat(assume.java:70)     [junit]     at org.junit.assume.assumenoexception(assume.java:92)     [junit]     at org.apache.solr.servlet.solrrequestparsertest.teststreamurl(solrrequestparsertest.java:123)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     [junit]     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     [junit]     at java.lang.reflect.method.invoke(method.java:597)     [junit]     at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)     [junit]     at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]     at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)     [junit]     at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]     at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:802)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:775)     [junit]     at org.junit.runners.parentrunner$3.run(parentrunner.java:193)     [junit]     at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)     [junit]     at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)     [junit]     at org.junit.runners.parentrunner.access$000(parentrunner.java:42)     [junit]     at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)     [junit]     at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]     at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)     [junit]     at org.junit.runners.parentrunner.run(parentrunner.java:236)     [junit]     at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:420)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:911)     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:768)     [junit] caused by: java.io.filenotfoundexception: http://www.apdfgdfgache.org/dist/lucene/solr/     [junit]     at sun.net.www.protocol.http.httpurlconnection.getinputstream(httpurlconnection.java:1311)     [junit]     at org.apache.solr.servlet.solrrequestparsertest.teststreamurl(solrrequestparsertest.java:120)     [junit]     ... 26 more     [junit] ------------- ---------------- ---------------",
        "label": 40
    },
    {
        "text": "analysis for irish adds analysis for irish. the stemmer is generated from a snowball stemmer. i've sent it to martin porter, who says it will be added during the week.",
        "label": 40
    },
    {
        "text": "fastvectorhighlighter  make fragmentsbuilder use encoder make fragmentsbuilder use encoder, as highlighter does.",
        "label": 26
    },
    {
        "text": "normalize \u0491 to \u0433 in ukrainian analyzer letter \u0491 was re-introduced into ukrainian alphabet in 1990 and many ukrainian texts don't use this letter consistently so the search will benefit if we normalize it to \u0433.",
        "label": 11
    },
    {
        "text": "enwikicontentsource silently swallows the last wiki doc last wiki doc is never returned",
        "label": 12
    },
    {
        "text": "make fieldsortedhitqueue public currently, those who utilize the \"advanced\" search api cannot sort results using the handy fieldsortedhitqueue. i suggest making this class public to facilitate this use, as i can't think of a reason not to.",
        "label": 55
    },
    {
        "text": "add namespaces to expressions javascript compiler i would like to add the concept of namespaces to the expressions javascript compiler using '.' as the operator. example of namespace usage in functions: accuratemath.sqrt(field) fastmath.sqrt(field) example of namespace usage in variables: location.latitude location.longitude",
        "label": 41
    },
    {
        "text": "sweetspotsimiliarity this is a new similarity implimention for the contrib/miscellaneous/ package, it provides a similiarty designed for people who know the \"sweetspot\" of their data. three major pieces of functionality are included: 1) a lengthnorm which creates a \"platuea\" of values. 2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function. 3) a hyperbolic tf function which is best explained by graphing the equation. this isn't used by default, but is available for subclasses to call from their own tf functions. all constants used in all functions are configurable. in the case of lengthnorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields.",
        "label": 18
    },
    {
        "text": "remove english stopwords default from standardanalyzer in lucene core yonik said on lucene-7318: i think it would make a good default for most lucene users, and we should graduate it from the analyzers module into core, and make it the default for indexwriter. this \"standardanalyzer\" is specific to english, as it removes english stopwords. that seems to be an odd choice now for a few reasons: it was argued in the past (rather vehemently) that solr should not prefer english in it's default \"text\" field afaik, removing stopwords is no longer considered best practice. given that removal of english stopwords is the only thing that really makes this analyzer english-centric (and given the negative impact that can have on other languages), it seems like the stopword filter should be removed from standardanalyzer. when trying to fix the backwards incompatibility issues in lucene-7318, it looks like most unrelated code moved from analysis module to core (and changing package names!!!! ) was related to word list loading, chararraysets, and superclasses of stopfilter. if we follow yonik's suggestion, we can revert all those changes. i agree with hin, an \"universal\" analyzer should not have any language specific stop-words. the other thing is lowercasefilter, but i'd suggest to simply add a clone of it to lucene core and leave the analysis-module self-contained.",
        "label": 2
    },
    {
        "text": "facet user guide for lucene has deleted classes methods and needs more explanations  concurrent indexing and search there is no clear explanation regarding those points below. 1. taxonomyreader should be reopened after indexreader. 2. taxonomywriter should be committed before indexwriter. taxonomywriter should be closed before indexwriter. the rationale is that it's ok to see categories to which no document belongs but not ok the other way around. beginners may not be able to see this until after they think through it for a long time, which most of them including me wouldn't do. however, facet user guide doesn't explain the rationale clearly. documentbuilder & facetsearchparams.addfacetrequest they no longer exist in lucene 4.2, but they are used in code examples. facetresultnode.getlabel(taxonomyreader) it doesn't eixst in 4.2, but it is mentioned in \"multiple facet requests\" etc i don't know if there are other points to be made.",
        "label": 43
    },
    {
        "text": "add new latlonshapepolygonquery this feature will provide the ability to query indexed latlonshape fields with an arbitrary polygon. initial implementation will support intersect queries only and future enhancements will add other relations (e.g., contains, within)",
        "label": 36
    },
    {
        "text": "highlighting exact phrase with overlapping tokens fails  fields with overlapping token are not highlighted in search results when searching exact phrases, when using termvector.with_offset. the document builded in memoryindex for highlight does not preserve positions of tokens in this case. overlapping tokens get \"flattened\" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift. i corrected this by adding a position increment calculation in sub class storedtokenstream. i added junit test covering this case. i used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. i tried to reduce them, but some linewrapping rules still doesn't match. correction patch joined",
        "label": 33
    },
    {
        "text": " ant regenerate  causes compilation errors the following is the output of ant -diagnostics followed by ant regenerate on a clean checkout of trunk. ------- ant diagnostics report ------- apache ant(tm) version 1.9.3 compiled on april 8 2014 -------------------------------------------  implementation version ------------------------------------------- core tasks     : 1.9.3 in file:/usr/share/ant/lib/ant.jar -------------------------------------------  ant properties ------------------------------------------- ant.version: apache ant(tm) version 1.9.3 compiled on april 8 2014 ant.java.version: 1.7 is this the apache harmony vm? no is this the kaffe vm? no is this gij/gcj? no ant.core.lib: /usr/share/ant/lib/ant.jar ant.home: /usr/share/ant -------------------------------------------  ant_home/lib jar listing ------------------------------------------- ant.home: /usr/share/ant ant-apache-oro.jar (9750 bytes) junit.jar (108762 bytes) ant-jdepend.jar (13865 bytes) ant-launcher.jar (18382 bytes) ant-apache-bsf.jar (9786 bytes) ant-swing.jar (13285 bytes) ant-antlr.jar (11605 bytes) ant.jar (2008109 bytes) ant-apache-log4j.jar (8634 bytes) ant-testutil.jar (21042 bytes) ant-junit4.jar (13104 bytes) ant-apache-resolver.jar (9681 bytes) ant-junit.jar (114902 bytes) ant-commons-logging.jar (9758 bytes) ant-apache-bcel.jar (15114 bytes) ant-javamail.jar (13847 bytes) ant-jmf.jar (12317 bytes) ant-jsch.jar (46562 bytes) ant-commons-net.jar (91319 bytes) ant-apache-regexp.jar (9610 bytes) ant-apache-xalan2.jar (8141 bytes) -------------------------------------------  user_home/.ant/lib jar listing ------------------------------------------- user.home: /home/mdrob ivy.jar (1222059 bytes) -------------------------------------------  tasks availability ------------------------------------------- image : not available (the implementation class is not present) sshexec : missing dependency com.jcraft.jsch.logger scp : missing dependency com.jcraft.jsch.logger sshsession : missing dependency com.jcraft.jsch.logger netrexxc : not available (the implementation class is not present) jdepend : missing dependency jdepend.xmlui.jdepend gjdoc : not available (the implementation class is not present) a task being missing/unavailable should only matter if you are trying to use it -------------------------------------------  org.apache.env.which diagnostics ------------------------------------------- not available. download it at http://xml.apache.org/commons/ -------------------------------------------  xml parser information ------------------------------------------- xml parser : org.apache.xerces.jaxp.saxparserimpl xml parser location: file:/usr/share/java/xercesimpl-2.11.0.jar namespace-aware parser : org.apache.xerces.jaxp.saxparserimpl$jaxpsaxparser namespace-aware parser location: file:/usr/share/java/xercesimpl-2.11.0.jar -------------------------------------------  xslt processor information ------------------------------------------- xslt processor : com.sun.org.apache.xalan.internal.xsltc.trax.transformerimpl xslt processor location: unknown -------------------------------------------  system properties ------------------------------------------- java.runtime.name : java(tm) se runtime environment sun.boot.library.path : /usr/lib/jvm/java-7-oracle/jre/lib/amd64 java.vm.version : 24.72-b04 ant.library.dir : /usr/share/ant/lib java.vm.vendor : oracle corporation java.vendor.url : http://java.oracle.com/ path.separator : : java.vm.name : java hotspot(tm) 64-bit server vm file.encoding.pkg : sun.io user.country : us sun.java.launcher : sun_standard sun.os.patch.level : unknown java.vm.specification.name : java virtual machine specification user.dir : /home/mdrob/workspace/lucene-solr java.runtime.version : 1.7.0_72-b14 java.awt.graphicsenv : sun.awt.x11graphicsenvironment java.endorsed.dirs : /usr/lib/jvm/java-7-oracle/jre/lib/endorsed os.arch : amd64 java.io.tmpdir : /tmp line.separator :  java.vm.specification.vendor : oracle corporation os.name : linux ant.home : /usr/share/ant sun.jnu.encoding : utf-8 java.library.path : /usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib java.specification.name : java platform api specification java.class.version : 51.0 sun.management.compiler : hotspot 64-bit tiered compilers os.version : 3.13.0-39-generic user.home : /home/mdrob user.timezone : america/chicago java.awt.printerjob : sun.print.psprinterjob file.encoding : utf-8 java.specification.version : 1.7 user.name : mdrob java.class.path : /usr/share/ant/lib/ant-launcher.jar:/usr/share/java/xmlparserapis.jar:/usr/share/java/xercesimpl.jar:/home/mdrob/.ant/lib/ivy.jar:/usr/share/ant/lib/ant-apache-oro.jar:/usr/share/ant/lib/junit.jar:/usr/share/ant/lib/ant-jdepend.jar:/usr/share/ant/lib/ant-launcher.jar:/usr/share/ant/lib/ant-apache-bsf.jar:/usr/share/ant/lib/ant-swing.jar:/usr/share/ant/lib/ant-antlr.jar:/usr/share/ant/lib/ant.jar:/usr/share/ant/lib/ant-apache-log4j.jar:/usr/share/ant/lib/ant-testutil.jar:/usr/share/ant/lib/ant-junit4.jar:/usr/share/ant/lib/ant-apache-resolver.jar:/usr/share/ant/lib/ant-junit.jar:/usr/share/ant/lib/ant-commons-logging.jar:/usr/share/ant/lib/ant-apache-bcel.jar:/usr/share/ant/lib/ant-javamail.jar:/usr/share/ant/lib/ant-jmf.jar:/usr/share/ant/lib/ant-jsch.jar:/usr/share/ant/lib/ant-commons-net.jar:/usr/share/ant/lib/ant-apache-regexp.jar:/usr/share/ant/lib/ant-apache-xalan2.jar:/usr/lib/jvm/java-7-oracle/lib/tools.jar java.vm.specification.version : 1.7 sun.arch.data.model : 64 java.home : /usr/lib/jvm/java-7-oracle/jre sun.java.command : org.apache.tools.ant.launch.launcher -cp  regenerate -diagnostics java.specification.vendor : oracle corporation user.language : en awt.toolkit : sun.awt.x11.xtoolkit java.vm.info : mixed mode java.version : 1.7.0_72 java.ext.dirs : /usr/lib/jvm/java-7-oracle/jre/lib/ext:/usr/java/packages/lib/ext sun.boot.class.path : /usr/lib/jvm/java-7-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-7-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-7-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-7-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-7-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-7-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-7-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-7-oracle/jre/classes java.vendor : oracle corporation file.separator : / java.vendor.url.bug : http://bugreport.sun.com/bugreport/ sun.cpu.endian : little sun.io.unicode.encoding : unicodelittle sun.desktop : gnome sun.cpu.isalist :  -------------------------------------------  temp dir ------------------------------------------- temp dir is /tmp temp dir is writeable temp dir alignment with system clock is -897 ms -------------------------------------------  locale information ------------------------------------------- timezone central standard time offset=-21600000 -------------------------------------------  proxy information ------------------------------------------- java1.5+ proxy settings: direct connection buildfile: /home/mdrob/workspace/lucene-solr/build.xml regenerate: regenerate: check-moman: download-moman: createlevautomata:      [exec] wrote lev1tparametricdescription.java [104 lines; 3.8 kb]      [exec] wrote lev1parametricdescription.java [102 lines; 3.7 kb]      [exec] wrote lev2tparametricdescription.java [163 lines; 13.1 kb]      [exec] wrote lev2parametricdescription.java [147 lines; 9.6 kb] createpackedintsources: regenerate: regenerate: regenerate: ivy-availability-check: ivy-fail: ivy-configure: [ivy:configure] :: apache ivy 2.3.0 - 20130110142753 :: http://ant.apache.org/ivy/ :: [ivy:configure] :: loading settings :: file = /home/mdrob/workspace/lucene-solr/lucene/ivy-settings.xml -install-jflex: [ivy:cachepath] :: resolving dependencies :: de.jflex#jflex-caller;working [ivy:cachepath]         confs: [default] [ivy:cachepath]         found de.jflex#jflex;1.6.0 in public [ivy:cachepath]         found org.apache.ant#ant;1.7.0 in public [ivy:cachepath]         found org.apache.ant#ant-launcher;1.7.0 in public [ivy:cachepath] :: resolution report :: resolve 195ms :: artifacts dl 4ms         ---------------------------------------------------------------------         |                  |            modules            ||   artifacts   |         |       conf       | number| search|dwnlded|evicted|| number|dwnlded|         ---------------------------------------------------------------------         |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |         --------------------------------------------------------------------- clean-jflex: resolve: common.init: compile-lucene-core: ivy-availability-check: ivy-fail: ivy-configure: [ivy:configure] :: loading settings :: file = /home/mdrob/workspace/lucene-solr/lucene/ivy-settings.xml resolve: init: -clover.disable: -clover.load: -clover.classpath: -clover.setup: clover: compile-core:     [javac] compiling 50 source files to /home/mdrob/workspace/lucene-solr/lucene/build/core/classes/java     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct16.java:76: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct16.java:81: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct32.java:76: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct32.java:81: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct64.java:71: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct64.java:76: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct8.java:74: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] /home/mdrob/workspace/lucene-solr/lucene/core/src/java/org/apache/lucene/util/packed/direct8.java:79: error: method does not override or implement a method from a supertype     [javac]   @override     [javac]   ^     [javac] 8 errors    [subant] failure for target 'regenerate' of: /home/mdrob/workspace/lucene-solr/lucene/build.xml    [subant] the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/build.xml:521: the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/common-build.xml:2140: the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/analysis/build.xml:143: the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/analysis/build.xml:38: the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/common-build.xml:720: the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/common-build.xml:510: the following error occurred while executing this line:    [subant] /home/mdrob/workspace/lucene-solr/lucene/common-build.xml:1865: compile failed; see the compiler error output for details. build successful total time: 3 seconds there are two concerns that i see: the build result is shown as \"successful\" despite the compilation errors. future invocations of ant compile fail as expected. the build failure itself.",
        "label": 47
    },
    {
        "text": "fsdirectory list  is inconsistent lucene-638 added a check to the fsdirectory.list() method to only return files that are lucene related. i think this change made the fsdirectory implementation inconsistent with all other methods in directory. e.g. you can create a file with an arbitrary name using fsdirectory, fileexists() will report that it is there, deletefile() will remove it, but the array returned by list() will not contain the file. the actual issue that was reported in lucene-638 was about sub directories. those should clearly not be listed, but imo it is not the responsibility of a directory implementation to decide what kind of files can be created or listed. the directory class is an abstraction of a directory and it should't to more than that.",
        "label": 33
    },
    {
        "text": "two unused variables in analysis stempel src java org egothor stemmer compile java compile.java public static void main(java.lang.string[] args) throws exception { ...   for (int i = 1; i < args.length; i++) {       // system.out.println(\"[\" + args[i] + \"]\");       diff diff = new diff();       int stems = 0;       int words = 0; ... in the file compile.java, the variables stems and words are unused. although words gets incremented further in the file, it does not get referenced or used elsewhere. stems is neither incremented nor used elsewhere in the project. are these variables redundant?",
        "label": 9
    },
    {
        "text": "factor out searchermanager from nrtmanager currently we have nrtmanager and searchermanager while nrtmanager contains a big piece of the code that is already in searchermanager. users are kind of forced to use nrtmanager if they want to have searchermanager goodness with nrt. the integration into nrtmanager also forces you to maintain two instances even if you know you always want deletes. to me nrtmanager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. nrtmanager should use a searchermanager by aggregation rather than duplicate a lot of logic. searchermanager should have a nrt and directory based implementation users can simply choose from.",
        "label": 46
    },
    {
        "text": "some terms incorrectly highlighted in complex spanquery modassar ather initially raised this on lucene-5205. i'm opening this as a separate issue. if a spannear is within a spanor, it looks like the child terms within the spannear query are getting highlighted even if there is no match on that spannear query...in some special cases. specifically, in the format of the parser in lucene-5205 \"(b [c z]) d\\\"~2\", which is equivalent to: find \"b\" or the phrase \"c z\" within two words of \"d\" either direction this affects trunk.",
        "label": 10
    },
    {
        "text": "highlighter doesn't support constantscorequery if you wrap a query into a constant score the highlighter fails to highlight since constantscorequery is not recognized in weightedspantermextractor",
        "label": 46
    },
    {
        "text": "possibly improve includes excludes for packages files a great example would be to add excludes to the zipping/tgzing to exclude things like ds_store automatically.",
        "label": 40
    },
    {
        "text": "testautoprefixterms testbinarynumericranges failure my jenkins found the following seed on lucene_solr_5_3, but it also reproduces on trunk and branch_5x:    [junit4] suite: org.apache.lucene.codecs.autoprefix.testautoprefixterms    [junit4]   2> note: reproduce with: ant test  -dtestcase=testautoprefixterms -dtests.method=testbinarynumericranges -dtests.seed=3b0e81e508a4a9 -dtests.slow=true -dtests.locale=es -dtests.timezone=kwajalein -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 1.46s | testautoprefixterms.testbinarynumericranges <<<    [junit4]    > throwable #1: java.lang.assertionerror: tottermcount=17 is > allowedmaxterms=16    [junit4]    >  at __randomizedtesting.seedinfo.seed([3b0e81e508a4a9:ae5c77e440454dbb]:0)    [junit4]    >  at org.apache.lucene.codecs.autoprefix.testautoprefixterms$verifyautoprefixterms.finish(testautoprefixterms.java:577)    [junit4]    >  at org.apache.lucene.codecs.autoprefix.testautoprefixterms.testbinarynumericranges(testautoprefixterms.java:318)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=asserting(lucene53): {}, docvalues:{}, sim=randomsimilarityprovider(querynorm=true,coord=crazy): {}, locale=es, timezone=kwajalein    [junit4]   2> note: mac os x 10.10.4 x86_64/oracle corporation 1.8.0_20 (64-bit)/cpus=8,threads=1,free=207523744,total=257425408    [junit4]   2> note: all tests run in this jvm: [testautoprefixterms]    [junit4] completed [1/1] in 2.00s, 1 test, 1 failure <<< failures!",
        "label": 33
    },
    {
        "text": "refactor expressions module to use doublevaluessource with doublevaluessource in core, we can refactor the expressions module to use these instead of valuesource, and remove the dependency of expressions on the queries module in master.",
        "label": 2
    },
    {
        "text": "enable passing a config into pkindexsplitter i need to be able to pass the indexwriterconfig into the iw used by pkindexsplitter.",
        "label": 46
    },
    {
        "text": "add source packaging targets that make a tarball from a local working copy i am adding back targets that were removed in <https://issues.apache.org/jira/browse/lucene-2973> that are used to create source distribution packaging from a local working copy as new ant targets. 2 things to note about the patch: 1) for package-local-src-tgz in solr/build.xml, i had to specify additional directories under solr/ that have been added since lucene-2973. 2) i couldn't get the package-tgz-local-src in lucene/build.xml to generate the docs folder, which does get added by package-tgz-src. the patch is against the trunk.",
        "label": 47
    },
    {
        "text": "when reopen returns a new indexreader  both indexreaders may now control the lifecycle of the underlying directory which is managed by reference counting rough summary. basically, fsdirectory tracks references to fsdirectory and when indexreader.reopen shares a directory with a created indexreader and closedirectory is true, fsdirectory's ref management will see two decrements for one increment. you can end up getting an alreadyclosed exception on the directory when the indexreader is open. i have a test i'll put up. a solution seems fairly straightforward (at least in what needs to be accomplished).",
        "label": 53
    },
    {
        "text": "remove synchronization in compoundfilereader currently there is what seems to be unnecessary synchronization in compoundfilereader. this is solved by cloning the base indexinput. synchronization in low level io classes creates lock contention on highly multi threaded lucene installations, so much so that in many cases the cpu utilization never reaches the maximum without using something like parallelmultisearcher.",
        "label": 33
    },
    {
        "text": "opennlpopsfactory leaks filehandles of models i appears that all methods in opennlpopsfactory which use a resourceloader to get an inputstream to use for building a model are not closing those inputstreams this doesn't seem to negatively affect any existing lucene/analysis/opennlp tests, because the jvm doesn't know/care that there is a filehandle still open at the end of the test (is there a way to make the test complain?) but it does seem to cause a solr level test failure on windows (solr-12046) because the solr tests create a temp dir where pre-built models are copied for use, and when the test completes the cleanup attempts to delete those copies of the files but windows won't let it because they are still open. presumably if a lucene/analysis/opennlp test also made a copy of the files a similar failure would be triggered \u2013 but only on windows",
        "label": 18
    },
    {
        "text": "first cut at column stride fields  index values storage  i created an initial basic impl for storing \"index values\" (ie column-stride value storage). this is still a work in progress... but the approach looks compelling. i'm posting my current status/patch here to get feedback/iterate, etc. the code is standalone now, and lives under new package oal.index.values (plus some util changes, refactorings) \u2013 i have yet to integrate into lucene so eg you can mark that a given field's value should be stored into the index values, sorting will use these values instead of field cache, etc. it handles 3 types of values: six variants of byte[] per doc, all combinations of fixed vs variable length, and stored either \"straight\" (good for eg a \"title\" field), \"deref\" (good when many docs share the same value, but you won't do any sorting) or \"sorted\". integers (variable bit precision used as necessary, ie this can store byte/short/int/long, and all precisions in between) floats (4 or 8 byte precision) string fields are stored as the utf8 byte[]. this patch adds a bytesref, which does the same thing as flex's termref (we should merge them). this patch also adds basic initial impl of packedints (lucene-1990); we can swap that out if/when we get a better impl. this storage is dense (like field cache), so it's appropriate when the field occurs in all/most docs. it's just like field cache, except the reading api is a get() method invocation, per document. next step is to do basic integration with lucene, and then compare sort performance of this vs field cache. for the \"sort by string value\" case, i think ram usage & gc load of this index values api should be much better than field caache, since it does not create object per document (instead shares big long[] and byte[] across all docs), and because the values are stored in ram as their utf8 bytes. there are abstract writer/reader classes. the current reader impls are entirely ram resident (like field cache), but the api is (i think) agnostic, ie, one could make an mmap impl instead. i think this is the first baby step towards lucene-1231. ie, it cannot yet update values, and the reading api is fully random-access by docid (like field cache), not like a posting list, though i do think we should add an iterator() api (to return flex's docsenum) \u2013 eg i think this would be a good way to track avg doc/field length for bm25/lnu.ltc scoring.",
        "label": 46
    },
    {
        "text": "clean up redundant throws clauses examples are things like ctors that list throws xyzexception but actually dont, and things like 'throws corruptindex, lockobtainedfailed, ioexception' when all of these are actually ioexception.",
        "label": 47
    },
    {
        "text": "taxonomyreader drops empty string component from categorypath i ran the new printtaxonomystats on a wikipedia facets index, and it hit an aioobe because there was a child of the /categories path that had only one component ... this was created because i had added new categorypath(\"categories\", \"\") during indexing. i think taxoreader should preserve and return that empty string from .getpath?",
        "label": 43
    },
    {
        "text": "chainedfilter does not work well in the event of filters in andnot first andnot operation takes place against a completely false bitset and will always return zero results.",
        "label": 12
    },
    {
        "text": "term improvement term is designed for reuse of the supplied filter, to minimize intern(). one of the common use patterns is to create a term with the txt field being an empty string. to simplify this pattern and to document it's usefulness, i suggest adding a constructor: public term(string fld) with the obvious implementation and use it throughout core and contrib as a replacement.",
        "label": 33
    },
    {
        "text": "make creation of fixedbitset in facetscollector overridable in facetscollector, creation of bits in matchingdocs are allocated per query. for large indexes where maxdocs are large creating a bitset of maxdoc bits will be expensive and would great a lot of garbage. attached patch is to allow for this allocation customizable while maintaining current behavior.",
        "label": 43
    },
    {
        "text": "'ant get maven poms' should support custom version formats lucene-5217 changed the way ant get-maven-poms works, so that dependencies are pulled from the ant build, instead of being hard-coded in the pom templates. to parse the versions from internal module dependencies, the new internal ant task getmavendependenciestask uses a regex that expects a dotted/slashed/underscored numeric + optional -snapshot version format. as a result, non-conforming versions trigger a build failure - see the lucene-dev mailing list thread \"maven build issues with non-numeric custom version\": <http://mail-archives.apache.org/mod_mbox/lucene-dev/201401.mbox/%3ccaf=pa5-0sxe9su1pgf5m+f0t+g3q=fetwgxdh5ry1ab3zrhquq@mail.gmail.com%3e> this is a regression, since previously, custom version strings worked properly, e.g. ant -dversion=my-custom-version get-maven-poms.",
        "label": 47
    },
    {
        "text": "garbage data when reading a compressed  text field  lazily lazy compressed text fields is a case that was neglected during lazy field implementation. testcase and patch provided.",
        "label": 15
    },
    {
        "text": "cartesianpolyfilterbuilder has problems around the poles test lat/lon: 89.9, 50 and 89.9, -130.0. try a point near those two points, like 89.8,50 and a distance of 200.",
        "label": 15
    },
    {
        "text": "unable to set lockfactory implementation via  org apache lucene store fsdirectorylockfactoryclass  while trying to move from lucene 2.0 to lucene 2.1 i noticed a problem with the lockfactory instantiation code. during previous tests we successfully specified the lockfactory implementation by setting the property ${org.apache.lucene.store.fsdirectorylockfactoryclass} to \"org.apache.lucene.store.nativefslockfactory\". this does no longer work due to a bug in the fsdirectory class. the problem is caused from the fact that this class tries to invoke the default constructor of the specified lockfactory class. however neither nativefslockfactory nor simplefslockfactory do have a default constructor. fsdirectory, line 285: try { lockfactory = (lockfactory) c.newinstance(); } catch (illegalaccessexception e) { throw new ioexception(\"illegalaccessexception when instantiating lockclass \" + lockclassname); } catch (instantiationexception e) { throw new ioexception(\"instantiationexception when instantiating lockclass \" + lockclassname); } catch (classcastexception e) { throw new ioexception(\"unable to cast lockclass \" + lockclassname + \" instance to a lockfactory\"); } a possible workaround is to not set the property at all and call fsdirectory.setlockfactory(...) instead.",
        "label": 33
    },
    {
        "text": "geo3d  arcdistancetoshape  method may be useful i've got an application that seems like it may need the ability to compute a new kind of arc distance, from a geopoint to the nearest edge/point of a geoshape. adding this method to the interface, and corresponding implementations, would increase the utility of the package for ranking purposes.",
        "label": 10
    },
    {
        "text": "tieredmergepolicy getfloorsegmentmb returns the wrong value public double getfloorsegmentmb() {   return floorsegmentbytes/1024*1024.; } this is clearly wrong. it should be either \"/1024/1024.\" or \"/(1024*1024.)\". as written, the / and * operations offset, so the value gets returned in bytes. the merge policy itself uses the value directly rather than calling this getter, so only the return value itself is wrong.",
        "label": 33
    },
    {
        "text": "queryparser does not correctly handle escaped characters within quoted strings the lucene query parser incorrectly handles escaped characters inside quoted strings; specifically, a quoted string that ends with an (escaped) backslash followed by any additional quoted string will not be properly tokenized. consider the following example: (name:\"///mike\\\\\\\\\\\\\") or (name:\"alphonse\") this is not a contrived example \u2013 it derives from an actual bug we've encountered in our system. running this query will throw an exception, but removing the second clause resolves the problem. after some digging i've found that the problem is with the way quoted strings are processed by the lexer: you'll notice that mike's name is followed by three escaped backslashes right before the ending quote; looking at the javacc code for the query parser highlights the problem: queryparser.jj <default> token : {   <and:       (\"and\" | \"&&\") > | <or:        (\"or\" | \"||\") > | <not:       (\"not\" | \"!\") > | <plus:      \"+\" > | <minus:     \"-\" > | <lparen:    \"(\" > | <rparen:    \")\" > | <colon:     \":\" > | <star:      \"*\" > | <carat:     \"^\" > : boost | <quoted:     \"\\\"\" (~[\"\\\"\"] | \"\\\\\\\"\")* \"\\\"\"> ... take a look at the way the quoted token is constructed \u2013 there is no lexical processing of the escaped characters within the quoted string itself. in the above query the lexer matches everything from the first quote through all the backslashes, treating the end quote as an escaped character, thus also matching the starting quote of the second term. this causes a lexer error, because the last quote is then considered the start of a new match. i've come to understand that the lucene query handler is supposed to be able to handle unsanitized human input; indeed the lexer above would handle a query like \"blah\\\" without complaining, but that's a \"best-guess\" approach that results in bugs with legal, automatically generated queries. i've attached a patch that fixes the erroneous behavior but does not maintain leniency with malformed queries; i believe this is the correct approach because the two design goals are fundamentally at odds. i'd appreciate any comments.",
        "label": 32
    },
    {
        "text": "binary field content lost during optimize scenario: create an index with arbitrary content, and close it open indexwriter again, and add a document with binary field (stored but not compressed) close indexwriter without optimizing, so that the new document is in a separate segment. open indexreader. you can read the last document and its binary field just fine. open indexwriter, optimize the index, close indexwriter open indexreader. now the field is still present (not null) and is marked as binary, but the data is not there - field.getbinarylength() returns 0.",
        "label": 33
    },
    {
        "text": "precedence query parser using the contrib queryparser framework extend the current standardqueryparser on contrib so it supports boolean precedence",
        "label": 40
    },
    {
        "text": "analysis factories should use resourceloader  not class forname this affects snowballporterfilterfactory and phoneticfilterfactory. in solr i encountered this problem when i specified an encoder and i was forced to put the library in web-inf/lib instead of /solr/lib/.",
        "label": 10
    },
    {
        "text": "standard analyzer does not correctly tokenize combining character u combining latin small lettre e standard analyzer does not correctly tokenize combining character u+0364 combining latin small lettre e. the word \"mo\u0364chte\" is incorrectly tokenized into \"mo\" \"chte\", the combining character is lost. expected result is only on token \"mo\u0364chte\".",
        "label": 47
    },
    {
        "text": "make geoutils orientation method more stable the method geoutils#orient is problematic when called with points that are almost collinear but not quite. in that case the sign of the determinant is not reliable, so for example calling the method with points (a, b, c) and with (c, b, a) gives the same orientation. there is a complex implementation described here (https://www.cs.cmu.edu/~quake/robust.html) where the method becomes more reliable. i have been playing with such implementation and still is not 100% reliable. my proposal is not to be fully precise and define a precision constant for this method. therefore whenever the value of determinant is small to that precision, we consider the points to be collinear. in this case the results of the method are reliable.",
        "label": 19
    },
    {
        "text": "gdata server   website sandbox part added gdata-server to the sandbox part of the website \u2013 xdocs/sandbox/ build of website is fine.",
        "label": 15
    },
    {
        "text": "stopfillcacheexception is defined twice and the one is actually thrown is private and not catchable lucene/core/src/java/org/apache/lucene/search/fieldcacheimpl.java 145:  static final class stopfillcacheexception extends runtimeexception { 383:        } catch (stopfillcacheexception stop) { 456:        } catch (stopfillcacheexception stop) { 560:        } catch (stopfillcacheexception stop) { 710:        } catch (stopfillcacheexception stop) { 798:        } catch (stopfillcacheexception stop) { 887:        } catch (stopfillcacheexception stop) { lucene/core/src/java/org/apache/lucene/search/fieldcache.java 57:  public static final class stopfillcacheexception extends runtimeexception { 216:        throw new fieldcacheimpl.stopfillcacheexception(); 232:        throw new fieldcacheimpl.stopfillcacheexception(); 248:        throw new fieldcacheimpl.stopfillcacheexception(); 264:        throw new fieldcacheimpl.stopfillcacheexception();",
        "label": 53
    },
    {
        "text": "intersects for geoexactcircle does not trigger in some cases hi karl wright, it seems there is another issue with exact circles that shows up in the tests. it seems that in some cases the method insertsects() do not provide the right results so the relationship is wrongly computed. i attach a test showing the issue in a moment. cheers!",
        "label": 25
    },
    {
        "text": "discrepancy in gettermfreqvector methods gettermfreqvector(int, termvectormapper) never calls the mapper if there is no term vector, consitent with all the other gettermfreqvector methods that returns null. gettermfreqvector(int, string, termvectormapper) throws an ioexception when a field does not contain the term vector. my suggestion: index: src/java/org/apache/lucene/index/segmentreader.java =================================================================== --- src/java/org/apache/lucene/index/segmentreader.java (revision 590149) +++ src/java/org/apache/lucene/index/segmentreader.java (working copy) @@ -648,7 +648,7 @@      ensureopen();      fieldinfo fi = fieldinfos.fieldinfo(field);      if (fi == null || !fi.storetermvector || termvectorsreaderorig == null) -      throw new ioexception(\"field does not contain term vectors\"); +      return; ",
        "label": 15
    },
    {
        "text": "overview html needs help when you open javadocs, this is the very first thing you see (index.html). it has a bunch of sample code and links that are broken. in some situations i can see it was updated here and there, but others (like javadocs links) link to java 1.2 javadocs no longer online, etc.",
        "label": 40
    },
    {
        "text": "uax29urlemailtokenizer is not detecting some tokens as url type we are using the uax29urlemailtokenizer so we can use the token types in our plugins. however, i noticed that the tokenizer is not detecting certain urls as <url> but <alphanum> instead. examples that are not working: example.com is <alphanum> example.net is <alphanum> but: https://example.com is <url> as is https://example.net examples that work: example.ch is <url> example.co.uk is <url> example.nl is <url> i have checked this jira, and could not find an issue. i have tested this on lucene (solr) 6.4.1 and 7.3. could someone confirm my findings and advise what i could do to (help) resolve this issue?",
        "label": 47
    },
    {
        "text": "possible synonymfilter bug  hudson fail see https://builds.apache.org/job/lucene-trunk/1867/consoletext (no seed)",
        "label": 33
    },
    {
        "text": "remove  system properties  page from release specific docs we no longer use system properties to configure lucene in version 3.0, the page is obsolete and should be removed before release.",
        "label": 53
    },
    {
        "text": "simplify directory lock api see lucene-6507 for some background. in general it would be great if you can just acquire an immutable lock (or you get a failure) and then you close that to release it. today the api might be too much for what is needed by iw.",
        "label": 53
    },
    {
        "text": "intermittent failure in testindexwriter  testrandomiwreader rarely, this test (which was added with lucene-1516) fails in mockramdirectory.close because some files were not closed, eg:    [junit] note: random seed of testcase 'testrandomiwreader' was: -5001333286299627079    [junit] ------------- ---------------- ---------------    [junit] testcase: testrandomiwreader(org.apache.lucene.index.teststressindexing2):        caused an error    [junit] mockramdirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}    [junit] java.lang.runtimeexception: mockramdirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}    [junit]     at org.apache.lucene.store.mockramdirectory.close(mockramdirectory.java:292)    [junit]     at org.apache.lucene.index.teststressindexing2.testrandomiwreader(teststressindexing2.java:66)    [junit]     at org.apache.lucene.util.lucenetestcase.runtest(lucenetestcase.java:88)",
        "label": 33
    },
    {
        "text": "lucene requirements   remove all deprecated code per the move to lucene 2.0 from 1.9, remove all deprecated code and update documentation, etc. patch to follow shortly.",
        "label": 55
    },
    {
        "text": "trunk  testdocumentswriterdeletequeue teststressdeletequeue seed failure fails 100% of the time for me, trunk r1152089     [junit] testsuite: org.apache.lucene.index.testdocumentswriterdeletequeue     [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 0.585 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testdocumentswriterdeletequeue -dtestmethod=teststressdeletequeue -dtests.seed=724635056932528964:-56 53725200660632980     [junit] note: test params are: codec=randomcodecprovider: {}, locale=en_us, timezone=pacific/port_moresby     [junit] note: all tests run in this jvm:     [junit] [testdocumentswriterdeletequeue]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=86067624,total=125632512     [junit] ------------- ---------------- ---------------     [junit] testcase: teststressdeletequeue(org.apache.lucene.index.testdocumentswriterdeletequeue):    failed",
        "label": 46
    },
    {
        "text": "geodegeneratepoint in pole hi karl wright, this a small thing that shows up during the effort of integrating the library with spatial4j. if we have a degenerated point in the pole and you create a point in the pole, it can happen that the new point is not within the degenerated point. i attach a test case. thanks, i.",
        "label": 25
    },
    {
        "text": "kuromoji code donation   a new japanese morphological analyzer atilika inc. (\u30a2\u30c6\u30a3\u30ea\u30ab\u682a\u5f0f\u4f1a\u793e) would like to donate the kuromoji japanese morphological analyzer to the apache software foundation in the hope that it will be useful to lucene and solr users in japan and elsewhere. the project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use java-based japanese morphological analyzers, and these become many of our design goals for kuromoji. kuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest lucene and solr users. compound-nouns, such as \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f (kansai international airport) and \u65e5\u672c\u7d4c\u6e08\u65b0\u805e (nikkei newspaper), are segmented as one token with most analyzers. as a result, a search for \u7a7a\u6e2f (airport) or \u65b0\u805e (newspaper) will not give you a for in these words. kuromoji can segment these words into \u95a2\u897f \u56fd\u969b \u7a7a\u6e2f and \u65e5\u672c \u7d4c\u6e08 \u65b0\u805e, which is generally what you would want for search and you'll get a hit. we also wanted to make sure the technology has a license that makes it compatible with other apache software foundation software to maximize its usefulness. kuromoji has an apache license 2.0 and all code is currently owned by atilika inc. the software has been developed by my good friend and ex-colleague masaru hasegawa and myself. kuromoji uses the so-called ipadic for its dictionary/statistical model and its license terms are described in notice.txt. i'll upload code distributions and their corresponding hashes and i'd very much like to start the code grant process. i'm also happy to provide patches to integrate kuromoji into the codebase, if you prefer that. please advise on how you'd like me to proceed with this. thank you.",
        "label": 40
    },
    {
        "text": "failed attempt of downloading javax activation javadoc i have checked out the trunk source code. but then running the following commands fails: ant clean test ant idea the failure message says the following: ivy-fail: resolve: [ivy:retrieve] [ivy:retrieve] :: problems summary :: [ivy:retrieve] :::: warnings [ivy:retrieve]          [failed     ] javax.activation#activation;1.1.1!activation.jar(javadoc):  (0ms) [ivy:retrieve]  ==== shared: tried [ivy:retrieve]    c:\\users\\ilia\\.ivy2\\shared\\javax.activation\\activation\\1.1.1\\javadocs\\activation.jar [ivy:retrieve]  ==== public: tried [ivy:retrieve]    http://repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1-javadoc.jar [ivy:retrieve]          :::::::::::::::::::::::::::::::::::::::::::::: [ivy:retrieve]          ::              failed downloads            :: [ivy:retrieve]          :: ^ see resolution messages for details  ^ :: [ivy:retrieve]          :::::::::::::::::::::::::::::::::::::::::::::: [ivy:retrieve]          :: javax.activation#activation;1.1.1!activation.jar(javadoc) [ivy:retrieve]          :::::::::::::::::::::::::::::::::::::::::::::: [ivy:retrieve] [ivy:retrieve] [ivy:retrieve] :: use verbose or debug message level for more details build failed c:\\users\\ilia\\documents\\ideaprojects\\svn.apache.org\\lucene\\build.xml:61: the following error occurred while executing this line: c:\\users\\ilia\\documents\\ideaprojects\\svn.apache.org\\lucene\\extra-targets.xml:39: the following error occurred while executing this line: c:\\users\\ilia\\documents\\ideaprojects\\svn.apache.org\\lucene\\solr\\build.xml:209: the following error occurred while executing this line: c:\\users\\ilia\\documents\\ideaprojects\\svn.apache.org\\lucene\\solr\\common-build.xml:440: the following error occurred while executing this line: c:\\users\\ilia\\documents\\ideaprojects\\svn.apache.org\\lucene\\solr\\common-build.xml:496: the following error occurred while executing this line: c:\\users\\ilia\\documents\\ideaprojects\\svn.apache.org\\lucene\\solr\\contrib\\contrib-build.xml:52: impossible to resolve dependencies:         resolve failed - see output for details total time: 53 minutes 19 seconds there was a javadoc file for javax:activation:1.1-rev-1, but none for javax:activation:1.1.1, which might be the cause. a work arround for people who encounter this problem specifically with javadoc jar files in older versions of lucene/solr is to create an empty fake file to satisfy the dependency checker at the \"shared\" path. in the above error, the expected shared path is c:\\users\\ilia\\.ivy2\\shared\\javax.activation\\activation\\1.1.1\\javadocs\\activation.jar so create an empty file at that path.",
        "label": 47
    },
    {
        "text": "speed up merging segments of points with data dimensions currently when merging segments of points with data dimensions, all dimensions are sorted and carried over down the tree even though only indexing dimensions are needed to build the bkd tree. this is needed so leaf node data can be compressed by common prefix. but when using mutablepointvalues, this ordering is done at the leaf level so we can se a similar approach from data dimensions and delay the sorting at leaf level. this seems to speed up indexing time as well as reduce the storage needed for building the index.      ",
        "label": 19
    },
    {
        "text": "improve test coverage of multi segment indices simple patch that adds a test-only helper class, randomindexwriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of ram, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices).",
        "label": 40
    },
    {
        "text": "memoryindex doesn't call tokenstream reset  and tokenstream end  memoryindex from contrib/memory does not honor the contract for a consumer of a tokenstream will work up a patch right quick",
        "label": 32
    },
    {
        "text": "unifiedhighlighter doesn't highlight mtqs wrapped in boostquery unifiedhighlighter doesn't highlight mtq wrapped in boostquery. for example, suppose we have a doc with a field 'f' contains data 'lucene'. unifiedhighlighter highlights query (f:lucene*), but query (f:lucene*)^1 doesn't. test code: string field = \"f\"; string content = \"lucene\"; term term = new term(field, content); unifiedhighlighter highlighter = new unifiedhighlighter(null, new standardanalyzer()); query[] queries = {new prefixquery(term), new boostquery(new prefixquery(term), 1.0f)}; object fragobj;   for (query query : queries) {  fragobj = highlighter.highlightwithoutsearcher(field, query, content, 1);  system.out.printf(\"content=[%s]  query=%s  frag=[%s]\\n\", content, query, fragobj); } my opinion it's because multitermhighlighting.extractautomata() returns an empty automaton for boostquery. i think, should add some thing like: if (query instanceof boostquery)  {       list.addall(arrays.aslist(extractautomata(((boostquery) query).getquery(), fieldmatcher, lookinspan, prerewritefunc))) ;  }  to multitermhighlighting.extractautomata() thanks.",
        "label": 10
    },
    {
        "text": "testsortingmergepolicy testforcemergenotneeded fails testsortingmergepolicy.testforcemergenotneeded (recently added by lucene-7006) regularly and reproducibly fails e.g.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testsortingmergepolicy -dtests.method=testforcemergenotneeded -dtests.seed=baf97ed1d97ddeee -dtests.slow=true -dtests.locale=nl-nl -dtests.timezone=cst6cdt -dtests.asserts=true -dtests.file.encoding=iso-8859-1 stack trace: java.lang.assertionerror     at __randomizedtesting.seedinfo.seed([baf97ed1d97ddeee:a6da06cb422235fb]:0)     at org.apache.lucene.index.basemergepolicytestcase$1.merge(basemergepolicytestcase.java:44)     at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:1931)     at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1764)     at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1721)     at org.apache.lucene.index.basemergepolicytestcase.testforcemergenotneeded(basemergepolicytestcase.java:63)     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)     at java.lang.reflect.method.invoke(method.java:498)     at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1764)     at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:871)     at com.carrotsearch.randomizedtesting.randomizedrunner$9.evaluate(randomizedrunner.java:907)     at com.carrotsearch.randomizedtesting.randomizedrunner$10.evaluate(randomizedrunner.java:921)     at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)     at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)     at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:49)     at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:65)     at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)     at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)     at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:367)     at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:809)     at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:460)     at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:880)     at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:781)     at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:816)     at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:827)     at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46)     at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)     at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42)     at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:40)     at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:40)     at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)     at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)     at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)     at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:54)     at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)     at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:65)     at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55)     at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36)     at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:367)     at java.lang.thread.run(thread.java:745)",
        "label": 43
    },
    {
        "text": "cosmetic javadoc updates i've taken the liberty of making a few cosmetic updates to various javadocs: mergepolicy (minor cosmetic change) logmergepolicy (minor cosmetic change) indexwriter (major cleanup in class description, changed anchors to javadoc links [now works in eclipse], no content change) attached diff from svn r780545. i would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my ocd), and if there's a more practical/convenient way to send these in, please let me know",
        "label": 33
    },
    {
        "text": "clarify documentation of clone  in indexinput here is a snippet from indexinput's documentation: the original instance must take care that cloned instances throw alreadyclosedexception when the original one is closed. but concrete implementations don't throw this alreadyclosedexception (this would break the contract on closeable). for example, see niofsdirectory:     public void close() throws ioexception {       if (!isclone) {         channel.close();       }     } what trapped me was that the abstract class indexinput overrides the default implementation of clone(), but doesn't do anything useful... i guess you could make it final and provide the tracking for cloned instances in this class rather than reimplementing it everywhere else (iscloned() would be a superclass method then too). thoughts?",
        "label": 11
    },
    {
        "text": "ngramphrasequery is not boosted  if i apply setboost() method to ngramphrasequery, score will not change. i think, setboost() is forgatten after optimized in rewrite() method.",
        "label": 1
    },
    {
        "text": "deadlock  while indexing in cascaded threads apparently i found a deadlock problem with indexwriter in a cascaded thread design to add documents (i am working on an application integrating tika, which has the capability to add embedded documents to the index as independent documents as they are found). the attached code illustrates the problem. sometimes it stops processing, at least one of the threads remains in waiting state. it must be executed no more than 5 times in my environment to trigger the problem.",
        "label": 46
    },
    {
        "text": "can't create niofsdirectory w o setting a system property niofsdirectory.getdirectory() returns a fsdirectory object",
        "label": 55
    },
    {
        "text": "make basecharfilter more efficient in performance performance degradation in solr 1.4 was reported. see: http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4 the inefficiency has been pointed out in basecharfilter javadoc by mike: note: this class is not particularly efficient. for example, a new class instance is created for every call to addoffcorrectmap(int, int), which is then appended to a private list.",
        "label": 40
    },
    {
        "text": "docvaluesrangequery newlongrange behaves incorrectly for long max value and long min value it seems that the following queries return all documents, which is unexpected: docvaluesrangequery.newlongrange(\"dv\", long.max_value, long.max_value, false, true); docvaluesrangequery.newlongrange(\"dv\", long.min_value, long.min_value, true, false); in solr, floats and doubles are converted to longs and -0d gets converted to long.min_value, and queries like {-0d to 0d] could fail due to this, returning all documents in the index.",
        "label": 47
    },
    {
        "text": "concurrency issues in segmentinfo files  could lead to concurrentmodificationexception the multi-threaded call of the files() in segmentinfo could lead to the concurrentmodificationexception if one thread is not finished additions to the arraylist (files) yet while the other thread already obtained it as cached (see below). this is a rare exception, but it would be nice to fix. i see the code is no longer problematic in the trunk (and others ported from flex_1458), looks it was fixed while implementing post 3.x features. the fix to 3.x and 2.9.x branches could be the same - create the files set first and populate it, and then assign to the member variable at the end of the method. this will resolve the issue. i could prepare the patch for 2.9.4 and 3.x, if needed. \u2013 info: [19] webapp= path=/replication params= {command=fetchindex&wt=javabin} status=0 qtime=1 jul 30, 2010 9:13:05 am org.apache.solr.core.solrcore execute info: [19] webapp= path=/replication params= {command=details&wt=javabin} status=0 qtime=24 jul 30, 2010 9:13:05 am org.apache.solr.handler.replicationhandler dofetch severe: snappull failed java.util.concurrentmodificationexception at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372) at java.util.abstractlist$itr.next(abstractlist.java:343) at java.util.abstractcollection.addall(abstractcollection.java:305) at org.apache.lucene.index.segmentinfos.files(segmentinfos.java:826) at org.apache.lucene.index.directoryreader$readercommit.<init>(directoryreader.java:916) at org.apache.lucene.index.directoryreader.getindexcommit(directoryreader.java:856) at org.apache.solr.search.solrindexreader.getindexcommit(solrindexreader.java:454) at org.apache.solr.handler.snappuller.fetchlatestindex(snappuller.java:261) at org.apache.solr.handler.replicationhandler.dofetch(replicationhandler.java:264) at org.apache.solr.handler.replicationhandler$1.run(replicationhandler.java:146)",
        "label": 43
    },
    {
        "text": "move core queryparsers to queryparser module move the contents of lucene/src/java/org/apache/lucene/queryparser to the queryparser module. to differentiate these parsers from the others, they are going to be placed a 'classic' package. we'll rename queryparser to classicqueryparser as well.",
        "label": 7
    },
    {
        "text": "testbytebuffersdirectory testseekpasteof  failures with bytearrayindexinput two reproducing seeds below. in both cases: the indexinput implementation is bytearrayindexinput seeking to exactly eof does not throw an exception bytearrayindexinput.readbyte() throws aioobe instead of the expected eofexception from https://jenkins.thetaphi.de/job/lucene-solr-master-macosx/4903: checking out revision 856e28d8cf07cc34bc1361784bf00e7aceb3af97 (refs/remotes/origin/master) [...]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testbytebuffersdirectory -dtests.method=testseekpasteof -dtests.seed=bdfa8cedb7c93ac1 -dtests.slow=true -dtests.locale=sr-rs -dtests.timezone=europe/astrakhan -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 0.00s j0 | testbytebuffersdirectory.testseekpasteof {impl=byte array (heap)} <<<    [junit4]    > throwable #1: junit.framework.assertionfailederror: unexpected exception type, expected eofexception but got java.lang.arrayindexoutofboundsexception: 1770    [junit4]    >  at __randomizedtesting.seedinfo.seed([bdfa8cedb7c93ac1:5dbc4714b74c4450]:0)    [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2680)    [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2669)    [junit4]    >  at org.apache.lucene.store.basedirectorytestcase.testseekpasteof(basedirectorytestcase.java:516)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    >  at java.base/java.lang.thread.run(thread.java:844)    [junit4]    > caused by: java.lang.arrayindexoutofboundsexception: 1770    [junit4]    >  at org.apache.lucene.store.bytearrayindexinput.readbyte(bytearrayindexinput.java:145)    [junit4]    >  at org.apache.lucene.store.basedirectorytestcase.lambda$testseekpasteof$12(basedirectorytestcase.java:518)    [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2675)    [junit4]    >  ... 37 more [...]    [junit4]   2> note: test params are: codec=lucene80, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@2c972cf9), locale=sr-rs, timezone=europe/astrakhan    [junit4]   2> note: mac os x 10.11.6 x86_64/oracle corporation 9 (64-bit)/cpus=3,threads=1,free=157933784,total=235929600 also (older) from https://builds.apache.org/job/lucene-solr-nightlytests-master/1645:   [junit4]   2> note: reproduce with: ant test  -dtestcase=testbytebuffersdirectory -dtests.method=testseekpasteof -dtests.seed=90b07b6267e63464 -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-master/test-data/enwiki.random.lines.txt -dtests.locale=es-pr -dtests.timezone=australia/currie -dtests.asserts=true -dtests.file.encoding=utf-8   [junit4] failure 0.01s j1 | testbytebuffersdirectory.testseekpasteof {impl=byte array (heap)} <<<   [junit4]    > throwable #1: junit.framework.assertionfailederror: unexpected exception type, expected eofexception but got java.lang.arrayindexoutofboundsexception: 1881   [junit4]    >  at __randomizedtesting.seedinfo.seed([90b07b6267e63464:70f6b09b67634af5]:0)   [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2683)   [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2672)   [junit4]    >  at org.apache.lucene.store.basedirectorytestcase.testseekpasteof(basedirectorytestcase.java:516)   [junit4]    >  at java.lang.thread.run(thread.java:748)   [junit4]    > caused by: java.lang.arrayindexoutofboundsexception: 1881   [junit4]    >  at org.apache.lucene.store.bytearrayindexinput.readbyte(bytearrayindexinput.java:145)   [junit4]    >  at org.apache.lucene.store.basedirectorytestcase.lambda$testseekpasteof$12(basedirectorytestcase.java:518)   [junit4]    >  at org.apache.lucene.util.lucenetestcase.expectthrows(lucenetestcase.java:2678)   [junit4]    >  ... 38 more [...]   [junit4]   2> note: test params are: codec=asserting(lucene80): {content=postingsformat(name=mockrandom)}, docvalues:{}, maxpointsinleafnode=118, maxmbsortinheap=7.2252603736984025, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@213e9607), locale=es-pr, timezone=australia/currie   [junit4]   2> note: linux 4.4.0-130-generic amd64/oracle corporation 1.8.0_172 (64-bit)/cpus=4,threads=1,free=165931680,total=326631424",
        "label": 11
    },
    {
        "text": "add dobeforeflush to indexwriter indexwriter has doafterflush which can be overridden by extensions in order to perform operations after flush has been called. since flush is final, one can only override doafterflush. this issue will handle two things: make doafterflush protected, instead of package-private, to allow for easier extendability of iw. add dobeforeflush which will be called by flush before it starts, to allow extensions to perform any operations before flush begings. will post a patch shortly. btw, any chance to get it out in 3.0.1?",
        "label": 33
    },
    {
        "text": "geocomplexpolygon throws error when checking within  for a point parallel to test point if a geocomplexpolygon is created with test point (x, y, z) and we try to check if point(-x,-y,-z) is {{within()}}then an error is thrown. it seems we need to handle the case when a point is parallel to the test point but not identical.  ",
        "label": 25
    },
    {
        "text": "testparser testspantermxml fails with some sims here is why this test sometimes fails (my explanation in the test i wrote):   /** make sure all sims work with spanor(termx, termy) where termy does not exist */   public void testcrazyspans() throws exception {     // the problem: \"normal\" lucene queries create scorers, returning null if terms dont exist     // this means they never score a term that does not exist.     // however with spans, there is only one scorer for the whole hierarchy:     // inner queries are not real queries, their boosts are ignored, etc. basically, spanqueries aren't really queries, you just get one scorer. it calls extractterms on the whole hierarchy and computes weights (e.g. idf) on the whole bag of terms, even if they don't exist. this is fine, we already have tests that sim's won't bug-out in computestats() here: however they don't expect to actually score documents based on these terms that don't exist... however this is exactly what happens in spans because it doesn't use sub-scorers. lucene's sim avoids this with the (docfreq + 1)",
        "label": 40
    },
    {
        "text": "replace segmentreader ref with atomicinteger i think the patch should be applied to backcompat tag in its entirety.",
        "label": 33
    },
    {
        "text": "memoryindexreader fields  performance regression while upgrading our codebase from lucene 4 to lucene 6 we found a significant performance regression - a 5x slowdown on profiling the code, the method memoryindexreader.fields() shows up as one of the hottest methods looking at the method, it just creates a copy of the inner fields map before passing it to memoryfields. it does this so that it can filter out fields with numtokens <= 0. the simplest \"fix\" would be to just remove the copying of the map completely, and pass fields directly to memoryfields. it's simple and removes any slowdown caused by this method. it does potentially change behaviour though, but none of the unit tests seem to test that behaviour so i wonder whether it's necessary (i looked at the original ticket lucene-7091 that introduced this code, i can't find much in way of an explanation). i'm going to attach a patch to this effect anyway and we can take things from there",
        "label": 10
    },
    {
        "text": "adding a should clause to a bq over an empty field clears the score when using defaultsimilarity patch with unit test to show the bug will be attached. i've narrowed this change in behavior with git bisect to the following commit: commit 698b4b56f0f2463b21c9e3bc67b8b47d635b7d1f author: robert muir <rmuir@apache.org> date:   thu aug 13 17:37:15 2015 +0000     lucene-6711: use collectionstatistics.doccount() for idf and average field length computations          git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1695744 13f79535-47bb-0310-9956-ffa450edef68",
        "label": 40
    },
    {
        "text": "arrayindexoutofboundsexception in unifiedhighlighter we see arrayindexoutofboundsexceptions coming out of the unifiedhighlighter in our production logs from time to time: java.lang.arrayindexoutofboundsexception  at java.base/java.lang.system.arraycopy(native method)  at org.apache.lucene.search.uhighlight.phrasehelper$spancollectedoffsetsenum.add(phrasehelper.java:386)  at org.apache.lucene.search.uhighlight.phrasehelper$offsetspancollector.collectleaf(phrasehelper.java:341)  at org.apache.lucene.search.spans.termspans.collect(termspans.java:121)  at org.apache.lucene.search.spans.nearspansordered.collect(nearspansordered.java:149)  at org.apache.lucene.search.spans.nearspansunordered.collect(nearspansunordered.java:171)  at org.apache.lucene.search.spans.filterspans.collect(filterspans.java:120)  at org.apache.lucene.search.uhighlight.phrasehelper.createoffsetsenumsforspans(phrasehelper.java:261) ... it turns out that there is an \"off by one\" error in the unifiedhighlighter's code that, as far as i can tell, is only triggered when two nested spannearqueries contain the same term. the resulting behaviour depends on the content of the highlighted document. either, some highlighted terms go missing or an arrayindexoutofboundsexception is thrown.",
        "label": 46
    },
    {
        "text": "print out where tests failed at the end of running the test suite it would be nice if, at the end of running ant test, it spit out the names of which tests failed so that one doesn't have to go scrolling up through the output or go run grep on the test-reports as a separate step. for another project, i use: <target name=\"test-summary\">     <echo>looking for summaries in: ${build.dir}/test-reports with basedir: ${basedir}</echo>     <echo>errors:</echo>     <exec executable=\"grep\">             <arg value=\"-r\"/>             <arg value=\"-rl\"/>             <arg value=\"errors=\\&quot;[1-9]\\&quot;\"/>             <arg value=\"${build.dir}/test-reports\"/>     </exec>     <echo>failures:</echo>     <exec executable=\"grep\">             <arg value=\"-r\"/>             <arg value=\"-rl\"/>             <arg value=\"failures=\\&quot;[1-9]\\&quot;\"/>             <arg value=\"${build.dir}/test-reports\"/>     </exec>   </target> which can likely be modified for lucene. i can do it, but wanted to see if others had an opinion.",
        "label": 11
    },
    {
        "text": "improve payload error handling reporting if you try to load a payload more than once you get the exception: ioexception(\"payload cannot be loaded more than once for the same term position.\"); you also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem.",
        "label": 33
    },
    {
        "text": "move dictionary for ukrainian analyzer to external dependency currently the dictionary for ukrainian analyzer is a blob in the source tree. we should move it out to external dependency, this allows: to have less binaries in the source easier to update the dictionary and track updates",
        "label": 11
    },
    {
        "text": "using constantscorequery on a remotesearchable throws java io notserializableexception using a constantscorequery through a multisearcher on a searchable obtained through rmi (remotesearchable) will throw a java.io.notserializableexception the problem seems to be the fact that the constantscorequery.constantweight has a searcher member variable which is not serializable. keeping a reference to the searcher does not seem to be required: the fix seems trivial. i've created the testcase to reproduce the issue and the patch to fix it.",
        "label": 55
    },
    {
        "text": "add more methods to manipulate querynodeprocessorpipeline elements querynodeprocessorpipeline allows the user to define a list of processors to process a query tree. however, it's not very flexible when the user wants to extend/modify an already created pipeline, because it only provides an add method, which only allows the user to append a new processor to the pipeline. so, i propose to add new methods to manipulate the processor in a pipeline. i think the methods should not consider an index position when modifying the pipeline, hence the index position in a pipeline does not mean anything, a processor has a meaning when it's after or before another processor. therefore, i suggest the methods should always consider another processor when inserting/modifying the pipeline. for example, insertafter(processor, newprocessor), which will insert the \"newprocessor\" after the \"processor\".",
        "label": 40
    },
    {
        "text": "lucenetaxonomyreader  decref  may close the inner ir  renderring the ltr in a limbo  taxonomyreader which supports ref-counting, has a decref() method which delegates to an inner indexreader and calls its .decref(). the latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls. also, the ltr's .close() method does not work in the same manner as indexreader's - which calls decref(), and leaves the real closing logic to the decref(). i believe this should be the right approach for the fix.",
        "label": 43
    },
    {
        "text": "tessellator  polygons can fail when using morton optimisation i experience some errors when processing complex polygons. i realised that if i disable the morton optimisation, then the errors go away. i studied one of the cases and it seems that when using the optimisation, it is possible to create triangles with points inside of them (see picture attached). there is a point just on the edge of the triangle. when disabling the optimisation, such a triangle is not created.  ",
        "label": 19
    },
    {
        "text": "packedints does not support structures above 256mb the packedints packed32 and packed64 fails when the internal structure exceeds 256mb. this is due to a missing cast that results in the bit position calculation being limited by integer.max_value (256mb * 8 = 2gb).",
        "label": 33
    },
    {
        "text": "lucene java site docs it would be really nice if the java site docs where consistent with the rest of the lucene family (namely, with navigation tabs, etc.) so that one can easily go between nutch, hadoop, etc.",
        "label": 14
    },
    {
        "text": "testfsdirectory fails on windows \"ant test\" generates the following error consistently when run on a windows machine even when run as user with administrator privileges [junit] testcase: testtmpdirisplainfile(org.apache.lucene.index.store.testfsdirectory): caused an error [junit] access is denied [junit] java.io.ioexception: access is denied [junit] at java.io.winntfilesystem.createfileexclusively(native method) [junit] at java.io.file.createnewfile(file.java:828) [junit] at org.apache.lucene.index.store.testfsdirectory.testtmpdirisplainfile(testfsdirectory.java:66)",
        "label": 55
    },
    {
        "text": "document issues involved in building your index with one jdk version and then searching updating with another i think this needs to go in something of a permenant spot - this isn't a one time release type issues - its going to present over multiple release. if there is nothing we can do here, then we just have to do the best we can - such as a prominent notice alerting that if you transition jvm's between building and searching the index and you are using or doing x, things will break. we should put this in a spot that is always pretty visible - perhaps even a new readme file titlted something like indexbackwardcompatibility or something, to which we can add other tips and gotchyas as they come up. or maintainingindicesacrossversions, or fancywhatevergetsyourattentionaboutupgradingstuff. or a permanent entry/sticky entry at the top of changes.",
        "label": 40
    },
    {
        "text": "eliasfanodocidset docidset in elias-fano encoding",
        "label": 1
    },
    {
        "text": "improve java packages  remove shared split packages  refactore naming scheme  i recently prepared lucene osgi bundles for the eclipse orbit repository. during the preparation i discovered that some packages (eg. org.apache.lucene.search) are shared between different jars, i.e. the package is in lucene core and in a contrib lib. while this is perfectly fine, it just makes osgi packaging more complex and complexity also has a higher potential for errors. thus, my wish for a lucene 3.0 would be to rename some packages. for example, all contribs/extensions could be moved into their own package namespace. (apologize if this has been reported elsewhere. i did a search in jira but did not find a similar issue.)",
        "label": 47
    },
    {
        "text": "queryparser doesn't call analyzer i'm trying to escape czech characters thorough the asciifoldingfilter this works fine in indexing since i can retrieve the non-diacritic version of the content i indexed. but trying to retrieve with diacritics returns always 0 results in debug mode i can clearly see that the analyzer wasn't called (in addition to that i've put a breakpoint in my analyser to check if it is not called later, and it never passes in) searchtext = \"p\u0159\u00edli\u0161*\"; analyzer analyzer = (analyzer) factory.getbean(\"analyzer\"); query q = new queryparser((version) factory.getbean(\"version\"), destinationplaceproperties.name, analyzer).parse(searchtext); the query q has these values in debug: prefix term (id=90) field \"name\" (id=100) text \"p\u0159\u00edli\u0161\" (id=101) \u2014 more details ---- q prefixquery (id=65) boost 1.0 numberofterms 0 prefix term (id=90) rewritemethod multitermquery$2 (id=92) --------------------- my analyser is quite simple: i put its code just for reference public class destinationanalyser extends analyzer { /** */ private final version luceneversion; public destinationanalyser(version lucene_version) { super(); this.luceneversion = lucene_version; } /* (non-javadoc) @see org.apache.lucene.analysis.analyzer#tokenstream(java.lang.string, java.io.reader) */ @override public tokenstream tokenstream(string fieldname, reader reader) { tokenstream result = new standardtokenizer(luceneversion, reader); result = new standardfilter(luceneversion, result); result = new lowercasefilter(luceneversion, result); result = new asciifoldingfilter(result); return result; } } --------- workaround --------- to avoid the problem, i'm actually using this method to transform the search text /** uses {@link asciifoldingfilter} to transform diacritical text to its ascii counterpart @param text to transform @return ascii text */ public static string foldtoascii(string text) { int length = text.length(); char[] toreturn = new char[length]; asciifoldingfilter.foldtoascii(text.tochararray(), 0, toreturn, 0, length); return new string(toreturn); }",
        "label": 53
    },
    {
        "text": "ant nightly smoke leaves a dirty checkout ? dev-tools/scripts/_pycache_ can we not leave this around?",
        "label": 33
    },
    {
        "text": "allow to plug in a cache eviction listener to indexreader to eagerly clean custom caches that use the indexreader  getfieldcachekey  allow to plug in a cache eviction listener to indexreader to eagerly clean custom caches that use the indexreader (getfieldcachekey). a spin of: https://issues.apache.org/jira/browse/lucene-2468. basically, its make a lot of sense to cache things based on indexreader#getfieldcachekey, even lucene itself uses it, for example, with the cachingwrapperfilter. fieldcache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the \"outside\", especially when using nrt - reader attack of the clones). the provided patch allows to plug a cacheevictionlistener which will be called when the cache should be purged for an indexreader.",
        "label": 33
    },
    {
        "text": "make reqexclscorer package private  and use docidsetiterator for excluded part  ",
        "label": 33
    },
    {
        "text": "cleanup contrib demo i don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case. i think we should also use a buffered reader in filedocument? and... i'm tempted to remove indexhtml (and the html parser) entirely. it's ancient, and we now have tika to extract text from many doc formats.",
        "label": 47
    },
    {
        "text": "pre analyzed fields adds the possibility to set a tokenstream at field constrution time, available as tokenstreamvalue in addition to stringvalue, readervalue and binaryvalue. there might be some problems with mixing stored fields with the same name as a field with tokenstreamvalue.",
        "label": 32
    },
    {
        "text": "make it possible to use searchermanager with distributed stats lucene-3555 added explicit stats methods to indexsearcher, but you must subclass to override this (e.g. populate with distributed stats). its also impossible to then do this with searchermanager. one idea is make this a factory method (or similar) on indexsearcher instead, so you don't need to subclass it to override. then you can initialize this in a searcherwarmer, except there is currently a lot of hair in what this warming should be. this is a prime example where searcher has different meaning from reader, we should clean this up. otherwise, lets make nrt/searchermanager subclassable in such a way that you can return a custom indexsearcher.",
        "label": 40
    },
    {
        "text": "add support for ideographic space to the queryparser   also know as fullwith space and wide space the ideographic space is a space character that is as wide as a normal cjk character cell. it is also known as wide-space or fullwith space.this type of space is used in cjk languages. this patch adds support for the wide space, making the queryparser component more friendly to queries that contain cjk text. reference: 'http://en.wikipedia.org/wiki/space_(punctuation)' - see table of spaces, char u+3000. i also added a new testcase that fails before the patch. after the patch is applied all junits pass.",
        "label": 32
    },
    {
        "text": "provide more of lucene for maven please provide javadoc & source jars for lucene-core. also, please provide the rest of lucene (the jars inside of \"contrib\" in the download bundle) if possible.",
        "label": 32
    },
    {
        "text": "cleanup xml queryparser code before i move the xml queryparser to the queryparser module, i want to pass over it and bring it up to module standards.",
        "label": 7
    },
    {
        "text": "multifieldqueryparser getfieldquery  drops queries that are neither booleanquery nor termquery from http://mail-archives.apache.org/mod_mbox/lucene-java-user/201609.mbox/%3c944985a6ac27425681bd27abe9d90602@ska-wn-e132.ptvag.ptv.de%3e, oliver kaleske reports: hi, in updating lucene from 6.1.0 to 6.2.0 i came across the following: we have a subclass of multifieldqueryparser (mfqp) for creating a custom type of query, which calls getfieldquery() on its base class (mfqp). for each of its search fields, this method has a query created by calling getfieldquery() on queryparserbase. ultimately, we wind up in querybuilder's createfieldquery() method, which depending on the number of tokens (etc.) decides what type of query to return: a termquery, booleanquery, phrasequery, or multiphrasequery. back in mfqp.getfieldquery(), a variable maxterms is determined depending on the type of query returned: for a termquery or a booleanquery, its value will in general be nonzero, clauses are created, and a non-null query is returned. however, other query subclasses result in maxterms=0, an empty list of clauses, and finally null is returned. to me, this seems like a bug, but i might as well be missing something. the comment \"// happens for stopwords\" on the return null statement, however, seems to suggest that query types other than termquery and booleanquery were not considered properly here. i should point out that our custom mfqp subclass so far does some rather unsophisticated tokenization before calling getfieldquery() on each token, so characters like '*' may still slip through. so perhaps with proper tokenization, it is guaranteed that only termquery and booleanquery can come out of the chain of getfieldquery() calls, and not handling (multi)phrasequery in mfqp.getfieldquery() can never cause trouble? the code in mfqp.getfieldquery dates back to lucene-2605: add classic queryparser option setsplitonwhitespace() to control whether to split on whitespace prior to text analysis. default behavior remains unchanged: split-on-whitespace=true. (06 jul 2016), when it was substantially expanded. best regards, oliver",
        "label": 47
    },
    {
        "text": "add a waitformerges  method to indexwriter it would be very useful to have a waitformerges() method on the indexwriter. right now, the only way i can see to achieve this is to call indexwriter.close() ideally, there would be a method on the indexwriter to wait for merges without actually closing the index. this would make it so that background merges (or optimize) can be waited for without closing the indexwriter, and then reopening a new indexwriter the close() reopen indexwriter method can be problematic if the close() fails as the write lock won't be released this could then result in the following sequence: close() - fails force unlock the write lock (per close() documentation) new indexwriter() (acquires write lock) finalize() on old indexwriter releases the write lock index is now not locked, and another indexwriter pointing to the same directory could be opened if you don't force unlock the write lock, opening a new indexwriter will fail until garbage collection calls finalize() the old indexwriter if the waitformerges() method is available, i would likely never need to close() the indexwriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitformerges() fails)",
        "label": 33
    },
    {
        "text": "order of stored fields not maintained as noted in these threads... http://www.nabble.com/order-of-fields-returned-by-document.getfields%28%29-to21034652.html http://www.nabble.com/order-of-fields-within-a-document-in-lucene-2.4%2b-to24210597.html somewhere prior to lucene 2.4.1 a change was introduced that prevents the stored fields of a document from being returned in same order that they were originally added in. this can cause serious performance problems for people attempting to use loadfirstfieldselector or a custom fieldselector with the load_and_break, or the size_and_break options (since the fields don't come back in the order they expect) speculation in the email threads is that the origin of this bug is code introduced by lucene-1301 \u2013 but the purpose of that issue was refactoring, so if it really is the cause of the change this would seem to be a bug, and not a side affect of a conscious implementation change. someone who understands indexing internals should investigate this. at a minimum, if it's decided that this is not actual a bug, then prior to resolving this bug the wiki docs and some of the fieldselector javadocs should be updated to make it clear what order fields will be returned in.",
        "label": 33
    },
    {
        "text": "reproducible failure in testlatlonshapequeries ant test -dtestcase=testlatlonshapequeries -dtests.method=testrandombig -dtests.seed=c3b6bf358f61417c -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/lucene-solr-nightlytests-master/test-data/enwiki.random.lines.txt -dtests.locale=be -dtests.timezone=antarctica/palmer -dtests.asserts=true -dtests.file.encoding=utf-8",
        "label": 36
    },
    {
        "text": "scale moderator not in line with sinusoidal projector current implementation in spatial lucene does : public double gettierboxid (double latitude, double longitude) { double[] coords = projector.coords(latitude, longitude); double id = getboxid(coords[0]) + (getboxid(coords[1]) / tierverticalposdivider); return id ; } private double getboxid (double coord){ return math.floor(coord / (idd / this.tierlength)); } with double idd = new double(180); in the cartesiantierplotter constructor. but current sinusoidal projector set and used in initialisation of cartesiantierplotter does : public double[] coords(double latitude, double longitude) { double rlat = math.toradians(latitude); double rlong = math.toradians(longitude); double nlat = rlong * math.cos(rlat); double r[] = {nlat, rlong} ; return r; } thus we moderate with idd = 180 a coord that is in a radian space. things to do : 1\u00b0) fix idd to : double idd= pi; 2\u00b0) move idd definition to iprojector interface : the coord space should belong to the projector doing the job. change the code from ctp to use that new interface.",
        "label": 7
    },
    {
        "text": "simplespanfragmenter can create very short fragments line 74 of simplespanfragmenter returns true when the current token is the start of a hit on a span or phrase, thus starting a new fragment. two problems occur: the previous fragment may be very short, but if it contains a hit it will be combined with the new fragment later so this disappears. if the token is close to a natural fragment boundary the new fragment will end up very short; possibly even as short as just the span or phrase itself. this is the result of creating a new fragment without incrementing currentnumfrags. to fix, remove or comment out line 74. the result is that fragments average to the fragment size unless a span or phrase hit is towards the end of the fragment - that fragment is made larger and the following fragment shorter to accommodate the hit.",
        "label": 29
    },
    {
        "text": "dwpt assert tripped again testbagofpositions tripped the spooky dwpt ram used on flush assert in http://jenkins.sd-datasolutions.de/job/lucene-solr-4.x-linux/2472/ it reproduces for me: ant test  -dtestcase=testbagofpositions -dtests.method=test -dtests.seed=730e05d38a0e4afa -dtests.multiplier=3 -dtests.slow=true -dtests.locale=de_de_preeuro -dtests.timezone=america/metlakatla -dtests.file.encoding=utf-8 full failure: [junit4:junit4]   2> note: reproduce with: ant test  -dtestcase=testbagofpositions -dtests.method=test -dtests.seed=730e05d38a0e4afa -dtests.multiplier=3 -dtests.slow=true -dtests.locale=de_de_preeuro -dtests.timezone=america/metlakatla -dtests.file.encoding=utf-8 [junit4:junit4] error   63.2s j0 | testbagofpositions.test <<< [junit4:junit4]    > throwable #1: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=2163, name=thread-1670, state=runnable, group=tgrp-testbagofpositions] [junit4:junit4]    > caused by: java.lang.assertionerror: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending dwpt: 0, flushing dwpt: 2, blocked dwpt: 0, peakdelta mem: 67152 byte [junit4:junit4]    >  at __randomizedtesting.seedinfo.seed([730e05d38a0e4afa]:0) [junit4:junit4]    >  at org.apache.lucene.index.documentswriterflushcontrol.assertmemory(documentswriterflushcontrol.java:120) [junit4:junit4]    >  at org.apache.lucene.index.documentswriterflushcontrol.doafterdocument(documentswriterflushcontrol.java:187) [junit4:junit4]    >  at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:384) [junit4:junit4]    >  at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1451) [junit4:junit4]    >  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1126) [junit4:junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:201) [junit4:junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:160) [junit4:junit4]    >  at org.apache.lucene.index.testbagofpositions$1.run(testbagofpositions.java:111) [junit4:junit4]    > throwable #2: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=2164, name=thread-1671, state=runnable, group=tgrp-testbagofpositions] [junit4:junit4]    > caused by: java.lang.assertionerror: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending dwpt: 0, flushing dwpt: 2, blocked dwpt: 0, peakdelta mem: 67152 byte [junit4:junit4]    >  at __randomizedtesting.seedinfo.seed([730e05d38a0e4afa]:0) [junit4:junit4]    >  at org.apache.lucene.index.documentswriterflushcontrol.assertmemory(documentswriterflushcontrol.java:120) [junit4:junit4]    >  at org.apache.lucene.index.documentswriterflushcontrol.doafterdocument(documentswriterflushcontrol.java:187) [junit4:junit4]    >  at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:384) [junit4:junit4]    >  at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1451) [junit4:junit4]    >  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1126) [junit4:junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:201) [junit4:junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:160) [junit4:junit4]    >  at org.apache.lucene.index.testbagofpositions$1.run(testbagofpositions.java:111) [junit4:junit4]    > throwable #3: com.carrotsearch.randomizedtesting.uncaughtexceptionerror: captured an uncaught exception in thread: thread[id=2166, name=thread-1673, state=runnable, group=tgrp-testbagofpositions] [junit4:junit4]    > caused by: java.lang.assertionerror: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending dwpt: 0, flushing dwpt: 2, blocked dwpt: 0, peakdelta mem: 67152 byte [junit4:junit4]    >  at __randomizedtesting.seedinfo.seed([730e05d38a0e4afa]:0) [junit4:junit4]    >  at org.apache.lucene.index.documentswriterflushcontrol.assertmemory(documentswriterflushcontrol.java:120) [junit4:junit4]    >  at org.apache.lucene.index.documentswriterflushcontrol.doafterdocument(documentswriterflushcontrol.java:187) [junit4:junit4]    >  at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:384) [junit4:junit4]    >  at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1451) [junit4:junit4]    >  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1126) [junit4:junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:201) [junit4:junit4]    >  at org.apache.lucene.index.randomindexwriter.adddocument(randomindexwriter.java:160) [junit4:junit4]    >  at org.apache.lucene.index.testbagofpositions$1.run(testbagofpositions.java:111) [junit4:junit4]   2> note: test params are: codec=simpletext, sim=randomsimilarityprovider(querynorm=false,coord=no): {field=dfr i(ne)1}, locale=de_de_preeuro, timezone=america/metlakatla [junit4:junit4]   2> note: linux 3.2.0-32-generic amd64/ibm corporation 1.6.0 (64-bit)/cpus=8,threads=1,free=55064736,total=63793152 [junit4:junit4]   2> note: all tests run in this jvm: [nested1, testindexwriterwiththreads, testlevenshteinautomata, testbasicoperations, teststressnrt, testindexwriterexceptions, testconjunctions, testmultilevelskiplist, testconcurrentmergescheduler, testindexwriterondiskfull, testbytesref, testthreadedforcemerge, testdocument, testcopybytes, testindexwriternrtiscurrent, testscorecachingwrappingscorer, testscorerperf, testdocvaluestypecompatibility, testprefixcodedterms, testrollingbuffer, testphraseprefixquery, test4gbstoredfields, testcustomsearchersort, testexplanations, testindexinput, testmultithreadtermvectors, testterminfosreaderindex, testsearchafter, test2bpositions, testfield, testsimpleattributeimpl, testflushbyramorcountspolicy, testsimilaritybase, testbyteslices, testflex, testrecyclingbyteblockallocator, testcrash, test2bterms, testsearchermanager, testdeterminism, testdemo, testspanexplanationsofnonmatches, testsubscorerfreqs, testdocidset, testfieldvaluefilter, testspanmultitermquerywrapper, testtermvectors, testsurrogates, testsimilarity2, nested1, testparallelreaderemptyindex, testshardsearching, testblockpostingsformat3, testpagedbytes, testcustomnorms, before3, before3, testsegmentreader, testmatchalldocsquery, testtopdocscollector, testnomergescheduler, testdeletionpolicy, nested1, throwinuncaught, testmultitermconstantscore, testphrasequery, testgraphtokenizers, testbitvector, testperfieldpostingsformat2, testsegmenttermenum, testversion, testngramphrasequery, testbackwardscompatibility3x, testregexprandom2, testdirectoryreaderreopen, testcompoundfile, testblockpostingsformat, testnrtthreads, testpayloads, testtransactions, testtermrangequery, testsmallfloat, testfsts, testnorms, testlookaheadtokenfilter, testduelingcodecs, testatomicupdate, testtermsenum, testmultimmap, testtimelimitingcollector, testtopdocsmerge, testnrtmanager, testarrayutil, testbufferedindexinput, testindexwriterforcemerge, testindexwritercommit, testweakidentitymap, testtypepromotion, testsimpleexplanations, teststressindexing, testsnapshotdeletionpolicy, testnrtreaderwiththreads, testtieredmergepolicy, testconsistentfieldnumbers, testcrashcausescorruptindex, testnumericutils, testmultivaluednumericrangequery, testchartermattributeimpl, testrollingupdates, testprefixinbooleanquery, testbytesrefhash, testramusageestimatoronwildanimals, testfieldcacherangefilter, testpayloadspans, testmixedcodecs, testsegmenttermdocs, testfieldcachesanitychecker, testdoc, testmergeschedulerexternal, testomittf, testdisjunctionmaxquery, testsimplesearchequivalence, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, nested, testpayloadnearquery, testfilteredquery, testpayloadexplanations, testdocsandpositions, testtransactionrollback, testcompiledautomaton, testsentinelintset, testindexablefield, testbooleanquery, testautomatonquery, testcomplexexplanationsofnonmatches, testregexpquery, testdoccount, testsearchforduplicates, testfilteredsearch, nestedsetupchain, nestedteardownchain, nested, nested, testdatefilter, testspansadvanced, testconstantscorequery, testdatetools, testsearch, testreaderclosed, testelevationcomparator, testautomatonqueryunicode, testdatesort, testbinarydocument, testspanfirstquery, testpriorityqueue, testdocboost, testmockcharfilter, testiscurrent, testprefixfilter, testbitutil, testversioncomparator, testcachingtokenfilter, testtermdocperf, testterm, testlucene40postingsformat, testallfileshavecodecheader, testbagofpositions]",
        "label": 46
    },
    {
        "text": "improve benchmark benchmark can be improved by incorporating recent suggestions posted on java-dev. m. mccandless' python scripts that execute multiple rounds of tests can either be incorporated into the codebase or converted to java.",
        "label": 33
    },
    {
        "text": "add example for retrieving facet counts without retrieving documents in the examples of facetting the facetscollector.search() is used. there are use cases where you do not need the documents that match the search. it would be nice if there is an example showing this. basically, it comes down to using searcher.search(query, null /* filter */, facetcollector)",
        "label": 43
    },
    {
        "text": "testindexwriter testthreadinterruptdeadlock failed  can't reproduce  trunk: r1134163 ran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.     [junit] testsuite: org.apache.lucene.index.testindexwriter     [junit] testcase: testthreadinterruptdeadlock(org.apache.lucene.index.testindexwriter):     failed     [junit]     [junit] junit.framework.assertionfailederror:     [junit]     at org.apache.lucene.index.testindexwriter.testthreadinterruptdeadlock(testindexwriter.java:1203)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1403)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1321)     [junit]     [junit]     [junit] tests run: 40, failures: 1, errors: 0, time elapsed: 23.79 sec     [junit]     [junit] ------------- standard output ---------------     [junit] checkindex failed     [junit] error: could not read any segments file in directory     [junit] java.io.filenotfoundexception: segments_2w     [junit]     at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:407)     [junit]     at org.apache.lucene.index.codecs.defaultsegmentinfosreader.openinput(defaultsegmentinfosreader.java:112)     [junit]     at org.apache.lucene.index.codecs.defaultsegmentinfosreader.read(defaultsegmentinfosreader.java:45)     [junit]     at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:257)     [junit]     at org.apache.lucene.index.segmentinfos$1.dobody(segmentinfos.java:287)     [junit]     at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:698)     [junit]     at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:533)     [junit]     at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:283)     [junit]     at org.apache.lucene.index.checkindex.checkindex(checkindex.java:311)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:154)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:144)     [junit]     at org.apache.lucene.index.testindexwriter$indexerthreadinterrupt.run(testindexwriter.java:1154)     [junit]     [junit] checkindex failed: unexpected exception     [junit] java.lang.runtimeexception: checkindex failed     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:158)     [junit]     at org.apache.lucene.util._testutil.checkindex(_testutil.java:144)     [junit]     at org.apache.lucene.index.testindexwriter$indexerthreadinterrupt.run(testindexwriter.java:1154)     [junit] indexreader.open failed: unexpected exception     [junit] java.io.filenotfoundexception: segments_2w     [junit]     at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:407)     [junit]     at org.apache.lucene.index.codecs.defaultsegmentinfosreader.openinput(defaultsegmentinfosreader.java:112)     [junit]     at org.apache.lucene.index.codecs.defaultsegmentinfosreader.read(defaultsegmentinfosreader.java:45)     [junit]     at org.apache.lucene.index.segmentinfos.read(segmentinfos.java:257)     [junit]     at org.apache.lucene.index.directoryreader$1.dobody(directoryreader.java:88)     [junit]     at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:698)     [junit]     at org.apache.lucene.index.directoryreader.open(directoryreader.java:84)     [junit]     at org.apache.lucene.index.indexreader.open(indexreader.java:500)     [junit]     at org.apache.lucene.index.indexreader.open(indexreader.java:293)     [junit]     at org.apache.lucene.index.testindexwriter$indexerthreadinterrupt.run(testindexwriter.java:1161)     [junit] ------------- ---------------- ---------------     [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testindexwriter -dtestmethod=testthreadinterruptdeadlock -dtests.seed=6733070832417768606:3130345095020099096     [junit] note: test params are: codec=randomcodecprovider: {=mockrandom, f6=simpletext, f7=mockrandom, f8=mocksep, f9=standard, f1=simpletext, f0=standard, f3=standard, f2=mocksep, f5=pulsing(freqcutoff=12),  f4=mockfixedintblock(blocksize=552), c=mockvariableintblock(baseblocksize=43), d9=mockvariableintblock(baseblocksize=43), d8=mockrandom, d5=mocksep, d4=pulsing(freqcutoff=12), d7=mockfixedintblock(blocksize=55 2), d6=mockvariableintblock(baseblocksize=43), d25=mocksep, d0=mockvariableintblock(baseblocksize=43), c29=mockfixedintblock(blocksize=552), d24=pulsing(freqcutoff=12), d1=mockfixedintblock(blocksize=552), c28= standard, d23=simpletext, d2=simpletext, c27=mocksep, d22=standard, d3=mockrandom, d21=mockrandom, d20=simpletext, c22=mocksep, c21=pulsing(freqcutoff=12), c20=simpletext, d29=simpletext, c26=mockvariableintblo ck(baseblocksize=43), d28=standard, c25=mockrandom, d27=mockvariableintblock(baseblocksize=43), c24=pulsing(freqcutoff=12), d26=mockrandom, c23=mockfixedintblock(blocksize=552), e9=pulsing(freqcutoff=12), e8=st andard, e7=mocksep, e6=mockrandom, e5=simpletext, c17=mockfixedintblock(blocksize=552), e3=mockfixedintblock(blocksize=552), d12=pulsing(freqcutoff=12), c16=mockvariableintblock(baseblocksize=43), e4=pulsing(fr eqcutoff=12), d11=mockfixedintblock(blocksize=552), c19=mockrandom, e1=mocksep, d14=mockvariableintblock(baseblocksize=43), c18=simpletext, e2=standard, d13=mockrandom, e0=simpletext, d10=mocksep, d19=mockvaria bleintblock(baseblocksize=43), c11=mockvariableintblock(baseblocksize=43), c10=mockrandom, d16=standard, c13=mockrandom, c12=simpletext, d15=mocksep, d18=pulsing(freqcutoff=12), c15=standard, d17=mockfixedintbl ock(blocksize=552), c14=mocksep, b3=standard, b2=mocksep, b5=pulsing(freqcutoff=12), b4=mockfixedintblock(blocksize=552), b7=mockfixedintblock(blocksize=552), b6=mockvariableintblock(baseblocksize=43), d50=mock random, b9=mockrandom, b8=simpletext, d43=simpletext, d42=standard, d41=mockvariableintblock(baseblocksize=43), d40=mockrandom, d47=pulsing(freqcutoff=12), d46=mockfixedintblock(blocksize=552), b0=mockrandom, d 45=standard, b1=mockvariableintblock(baseblocksize=43), d44=mocksep, d49=mockrandom, d48=simpletext, c6=simpletext, c5=standard, c4=mockvariableintblock(baseblocksize=43), c3=mockrandom, c9=mockfixedintblock(bl ocksize=552), c8=standard, c7=mocksep, d30=standard, d32=pulsing(freqcutoff=12), d31=mockfixedintblock(blocksize=552), c1=pulsing(freqcutoff=12), d34=mockfixedintblock(blocksize=552), c2=mocksep, d33=mockvariab leintblock(baseblocksize=43), d36=mockrandom, c0=simpletext, d35=simpletext, d38=mocksep, d37=pulsing(freqcutoff=12), d39=mockvariableintblock(baseblocksize=43), e92=mockfixedintblock(blocksize=552), e93=pulsin g(freqcutoff=12), e90=mocksep, e91=standard, e89=standard, e88=mockvariableintblock(baseblocksize=43), e87=mockrandom, e86=mockfixedintblock(blocksize=552), e85=mockvariableintblock(baseblocksize=43), e84=mocks ep, e83=pulsing(freqcutoff=12), e80=mockfixedintblock(blocksize=552), e81=simpletext, e82=mockrandom, e77=standard, e76=mocksep, e79=pulsing(freqcutoff=12), e78=mockfixedintblock(blocksize=552), e73=mockvariabl eintblock(baseblocksize=43), e72=mockrandom, e75=simpletext, e74=standard, binary=mocksep, f98=mockrandom, f97=simpletext, f99=mocksep, f94=pulsing(freqcutoff=12), f93=mockfixedintblock(blocksize=552), f96=mock variableintblock(baseblocksize=43), f95=mockrandom, e95=mockrandom, e94=simpletext, e97=standard, e96=mocksep, e99=mocksep, e98=pulsing(freqcutoff=12), id=standard, f34=simpletext, f33=standard, f32=mockvariabl eintblock(baseblocksize=43), f31=mockrandom, f30=mockfixedintblock(blocksize=552), f39=simpletext, f38=mockvariableintblock(baseblocksize=43), f37=mockrandom, f36=pulsing(freqcutoff=12), f35=mockfixedintblock(b locksize=552), f43=mocksep, f42=pulsing(freqcutoff=12), f45=mockfixedintblock(blocksize=552), f44=mockvariableintblock(baseblocksize=43), f41=standard, f40=mocksep, f47=simpletext, f46=standard, f49=mocksep, f4 8=pulsing(freqcutoff=12), content=standard, e19=standard, e18=mocksep, e17=simpletext, f12=mockrandom, e16=standard, f11=simpletext, f10=mockfixedintblock(blocksize=552), e15=mockvariableintblock(baseblocksize= 43), e14=mockrandom, f16=mockfixedintblock(blocksize=552), e13=mocksep, e12=pulsing(freqcutoff=12), f15=mockvariableintblock(baseblocksize=43), e11=simpletext, f14=mocksep, e10=standard, f13=pulsing(freqcutoff= 12), f19=standard, f18=mockvariableintblock(baseblocksize=43), f17=mockrandom, e29=mockrandom, e26=mocksep, f21=standard, e25=pulsing(freqcutoff=12), f20=mocksep, e28=mockfixedintblock(blocksize=552), f23=pulsi ng(freqcutoff=12), e27=mockvariableintblock(baseblocksize=43), f22=mockfixedintblock(blocksize=552), f25=mockrandom, e22=mockfixedintblock(blocksize=552), f24=simpletext, e21=mockvariableintblock(baseblocksize= 43), f27=standard, e24=mockrandom, f26=mocksep, e23=simpletext, f29=mocksep, f28=pulsing(freqcutoff=12), e20=pulsing(freqcutoff=12), field=mocksep, string=mockvariableintblock(baseblocksize=43), e30=mockfixedin tblock(blocksize=552), e31=pulsing(freqcutoff=12), a98=mocksep, e34=simpletext, a99=standard, e35=mockrandom, f79=mocksep, e32=mockvariableintblock(baseblocksize=43), e33=mockfixedintblock(blocksize=552), b97=m ockrandom, f77=mockrandom, e38=mockvariableintblock(baseblocksize=43), b98=mockvariableintblock(baseblocksize=43), f78=mockvariableintblock(baseblocksize=43), e39=mockfixedintblock(blocksize=552), b99=standard,  f75=mockfixedintblock(blocksize=552), e36=pulsing(freqcutoff=12), f76=pulsing(freqcutoff=12), e37=mocksep, f73=pulsing(freqcutoff=12), f74=mocksep, f71=standard, f72=simpletext, f81=standard, f80=mocksep, e40= mockvariableintblock(baseblocksize=43), e41=standard, e42=simpletext, e43=mocksep, e44=standard, e45=mockfixedintblock(blocksize=552), e46=pulsing(freqcutoff=12), f86=standard, e47=simpletext, f87=simpletext, e 48=mockrandom, f88=pulsing(freqcutoff=12), e49=mocksep, f89=mocksep, f82=mockvariableintblock(baseblocksize=43), f83=mockfixedintblock(blocksize=552), f84=simpletext, f85=mockrandom, f90=pulsing(freqcutoff=12),  f92=mockvariableintblock(baseblocksize=43), f91=mockrandom, str=mockrandom, a76=standard, e56=standard, f59=pulsing(freqcutoff=12), a77=simpletext, e57=simpletext, a78=pulsing(freqcutoff=12), e54=mockrandom, f 57=standard, a79=mocksep, e55=mockvariableintblock(baseblocksize=43), f58=simpletext, e52=mockvariableintblock(baseblocksize=43), e53=mockfixedintblock(blocksize=552), e50=pulsing(freqcutoff=12), e51=mocksep, f 51=mocksep, f52=standard, f50=mockrandom, f55=mockvariableintblock(baseblocksize=43), f56=mockfixedintblock(blocksize=552), f53=pulsing(freqcutoff=12), e58=mockfixedintblock(blocksize=552), f54=mocksep, e59=pul sing(freqcutoff=12), a80=pulsing(freqcutoff=12), e60=pulsing(freqcutoff=12), a82=mockvariableintblock(baseblocksize=43), a81=mockrandom, a84=mockrandom, a83=simpletext, a86=standard, a85=mocksep, a89=simpletext , f68=mockvariableintblock(baseblocksize=43), e65=pulsing(freqcutoff=12), f69=mockfixedintblock(blocksize=552), e66=mocksep, a87=mockvariableintblock(baseblocksize=43), e67=mockvariableintblock(baseblocksize=43 ), a88=mockfixedintblock(blocksize=552), e68=mockfixedintblock(blocksize=552), e61=simpletext, e62=mockrandom, e63=mocksep, e64=standard, f60=mockfixedintblock(blocksize=552), f61=pulsing(freq cutoff=12), f62=mockrandom, f63=mockvariableintblock(baseblocksize=43), e69=standard, f64=simpletext, f65=mockrandom, f66=mocksep, f67=standard, f70=mockfixedintblock(blocksize=552), a93=mocksep, a92=pulsing(freqcutoff=12), a91=simpletext, e71=simpletext, a90=standard, e70=standard, a97=mockvariableintblock(baseblocksize=43), a96=mockrandom, a95=pulsing(freqcutoff=12), a94=mockfixedintblock(blocksize=552), c58=mockrandom, a63=mockfixedintblock(blocksize=552), a64=pulsing(freqcutoff=12), c59=mockvariableintblock(baseblocksize=43), c56=mockfixedintblock(blocksize=552), d59=mockrandom, a61=mocksep, c57=pulsing(freqcutoff=12), a62=standard, c54=pulsing(freqcutoff=12), c55=mocksep, a60=simpletext, c52=standard, c53=simpletext, d53=simpletext, d54=mockrandom, d51=mockvariableintblock(baseblocksize=43), d52=mockfixedintblock(blocksize=552), d57=pulsing(freqcutoff=12), b62=standard, d58=mocksep, b63=simpletext, d55=standard, b60=mockrandom, d56=simpletext, b61=mockvariableintblock(baseblocksize=43), b56=standard, b55=mocksep, b54=mockrandom, b53=simpletext, d61=mockvariableintblock(baseblocksize=43), b59=mockvariableintblock(baseblocksize=43), d60=mockrandom, b58=mocksep, b57=pulsing(freqcutoff=12), c62=standard, c61=mocksep, a59=mockvariableintblock(baseblocksize=43), c60=mockrandom, a58=mockrandom, a57=mockfixedintblock(blocksize=552), a56=mockvariableintblock(baseblocksize=43), a55=mocksep, a54=pulsing(freqcutoff=12), a72=mockrandom, c67=standard, a73=mockvariableintblock(baseblocksize=43), c68=simpletext, a74=standard, c69=pulsing(freqcutoff=12), a75=simpletext, c63=mockvariableintblock(baseblocksize=43), c64=mockfixedintblock(blocksize=552), a70=mockvariableintblock(baseblocksize=43), c65=simpletext, a71=mockfixedintblock(blocksize=552), c66=mockrandom, d62=mocksep, d63=standard, d64=mockfixedintblock(blocksize=552), b70=standard, d65=pulsing(freqcutoff=12), b71=pulsing(freqcutoff=12), d66=mockvariableintblock(baseblocksize=43), b72=mocksep, d67=mockfixedintblock(blocksize=552), b73=mockvariableintblock(baseblocksize=43), d68=simpletext, b74=mockfixedintblock(blocksize=552), d69=mockrandom, b65=pulsing(freqcutoff=12), b64=mockfixedintblock(blocksize=552), b67=mockvariableintblock(baseblocksize=43), b66=mockrandom, d70=simpletext, b69=mockrandom, b68=simpletext, d72=mocksep, d71=pulsing(freqcutoff=12), c71=pulsing(freqcutoff=12), c70=mockfixedintblock(blocksize=552), a69=pulsing(freqcutoff=12), c73=mockvariableintblock(baseblocksize=43), c72=mockrandom, a66=mockrandom, a65=simpletext, a68=standard, a67=mocksep, c32=mocksep, c33=standard, c30=simpletext, c31=mockrandom, c36=mockvariableintblock(baseblocksize=43), a41=pulsing(freqcutoff=12), c37=mockfixedintblock(blocksize=552), a42=mocksep, a0=mockrandom, c34=pulsing(freqcutoff=12), c35=mocksep, a40=simpletext, b84=mocksep, d79=mockfixedintblock(blocksize=552), b85=standard, b82=simpletext, d77=mocksep, c38=standard, b83=mockrandom, d78=standard, c39=simpletext, b80=mockrandom, d75=standard, b81=mockvariableintblock(baseblocksize=43), d76=simpletext, d73=mockrandom, d74=mockvariableintblock(baseblocksize=43), d83=mockrandom, a9=mockfixedintblock(blocksize=552), d82=simpletext, d81=mockfixedintblock(blocksize=552), d80=mockvariableintblock(baseblocksize=43), b79=mockfixedintblock(blocksize=552), b78=mocksep, b77=pulsing(freqcutoff=12), b76=simpletext, b75=standard, a1=pulsing(freqcutoff=12), a35=pulsing(freqcutoff=12), a2=mocksep, a34=mockfixedintblock(blocksize=552), a3=mockvariableintblock(baseblocksize=43), a33=standard, a4=mockfixedintblock(blocksize=552), a32=mocksep, a5=mockrandom, a39=mockrandom, c40=simpletext, a6=mockvariableintblock(baseblocksize=43), a38=simpletext, a7=standard, a37=mockfixedintblock(blocksize=552), a8=simpletext, a36=mockvariableintblock(baseblocksize=43), c41=mockfixedintblock(blocksize=552), c42=pulsing(freqcutoff=12), c43=mockrandom, c44=mockvariableintblock(baseblocksize=43), c45=simpletext, a50=mockvariableintblock(baseblocksize=43), c46=mockrandom, a51=mockfixedintblock(blocksize=552), c47=mocksep, a52=simpletext, c48=standard, a53=mockrandom, b93=mockfixedintblock(blocksize=552), d88=mockrandom, c49=mockvariableintblock(baseblocksize=43), b94=pulsing(freqcutoff=12), d89=mockvariableintblock(baseblocksize=43), b95=mockrandom, b96=mockvariableintblock(baseblocksize=43), d84=pulsing(freqcutoff=12), b90=simpletext, d85=mocksep, b91=pulsing(freqcutoff=12), d86=mockvariableintblock(baseblocksize=43), b92=mocksep, d87=mockfixedintblock(blocksize=552), d92=standard, d91=mocksep, d94=pulsing(freqcutoff=12), d93=mockfixedintblock(blocksize=552), b87=mockfixedintblock(blocksize=552), b86=mockvariableintblock(baseblocksize=43), d90=simpletext, b89=mockrandom, b88=simpletext, a44=mockvariableintblock(baseblocksize=43), a43=mockrandom, a46=simpletext, a45=standard, a48=standard, a47=mocksep, c51=mockfixedintblock(blocksize=552), a49=mockfixedintblock(blocksize=552), c50=mockvariableintblock(baseblocksize=43), d98=mockfixedintblock(blocksize=552), d97=mockvariableintblock(baseblocksize=43), d96=mocksep, d95=pulsing(freqcutoff=12), d99=mockrandom, a20=mockvariableintblock(baseblocksize=43), c99=simpletext, c98=standard, c97=mockvariableintblock(baseblocksize=43), c96=mockrandom, b19=mockvariableintblock(baseblocksize=43), a16=pulsing(freqcutoff=12), a17=mocksep, b17=pulsing(freqcutoff=12), a14=standard, b18=mocksep, a15=simpletext, a12=simpletext, a13=mockrandom, a10=mockvariableintblock(baseblocksize=43), a11=mockfixedintblock(blocksize=552), b11=mockfixedintblock(blocksize=552), b12=pulsing(freqcutoff=12), b10=standard, b15=simpletext, b16=mockrandom, a18=mockrandom, b13=mockvariableintblock(baseblocksize=43), a19=mockvariableintblock(baseblocksize=43), b14=mockfixedintblock(blocksize=552), b30=mockrandom, a31=mocksep, a30=pulsing(freqcutoff=12), b28=simpletext, a25=mockvariableintblock(baseblocksize=43), b29=mockrandom, a26=mockfixedintblock(blocksize=552), a27=simpletext, a28=mockrandom, a21=mocksep, a22=standard, a23=mockfixedintblock(blocksize=552), a24=pulsing(freqcutoff=12), b20=mockrandom, b21=mockvariableintblock(baseblocksize=43), b22=standard, b23=simpletext, a29=pulsing(freqcutoff=12), b24=mocksep, b25=standard, b26=mockfixedintblock(blocksize=552), b27=pulsing(freqcutoff=12), b41=pulsing(freqcutoff=12), b40=mockfixedintblock(blocksize=552), c77=mockrandom, c76=simpletext, c75=mockfixedintblock(blocksize=552), c74=mockvariableintblock(baseblocksize=43), c79=simpletext, c78=standard, c80=mocksep, c83=mockrandom, c84=mockvariableintblock(baseblocksize=43), c81=mockfixedintblock(blocksize=552), b39=mockfixedintblock(blocksize=552), c82=pulsing(freqcutoff=12), b37=standard, b38=simpletext, b35=mockrandom, b36=mockvariableintblock(baseblocksize=43), b33=mockvariableintblock(baseblocksize=43), b34=mockfixedintblock(blocksize=552), b31=pulsing(freqcutoff=12), b32=mocksep, str2=mocksep, b50=mockvariableintblock(baseblocksize=43), b52=simpletext, str3=mockvariableintblock(baseblocksize=43), b51=standard, c86=standard, tvtest=mocksep, c85=mocksep, c88=pulsing(freqcutoff=12), c87=mockfixedintblock(blocksize=552), c89=mockvariableintblock(baseblocksize=43), c90=simpletext, c91=mockrandom, c92=standard, c93=simpletext, c94=pulsing(freqcutoff=12), c95=mocksep, content1=mockrandom, b46=pulsing(freqcutoff=12), b47=mocksep, content3=mockfixedintblock(blocksize=552), b48=mockvariableintblock(baseblocksize=43), content4=mockvariableintblock(baseblocksize=43), b49=mockfixedintblock(blocksize=552), content5=pulsing(freqcutoff=12), b42=simpletext, b43=mockrandom, b44=mocksep, b45=standard}, locale=sk, timezone=america/rainy_river     [junit] note: all tests run in this jvm:     [junit] [testdatetools, testdeletionpolicy, testdocsandpositions, testflex, testindexreaderclonenorms, testindexwriter]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.index.testindexwriter failed",
        "label": 33
    },
    {
        "text": "termattribute termlength  optimization public int termlength() { inittermbuffer(); // this patch removes this method call return termlength; } i see no reason to inittermbuffer() in termlength()... all tests pass, but i could be wrong?",
        "label": 33
    },
    {
        "text": "toparentblockjoincollector gettopgroups returns empty groups a bug is observed to cause unstable results returned by the gettopgroups function of class toparentblockjoincollector. in the scorer generation stage, the toparentblockjoincollector will automatically rewrite all the associated toparentblockjoinquery (and their subqueries), and save them into its in-memory look-up table, namely joinqueryid (see enroll() method for detail). unfortunately, in the gettopgroups method, the new toparentblockjoinquery parameter is not rewritten (at least users are not expected to do so). when the new one is searched in the old lookup table (considering the impact of rewrite() on hashcode()), the lookup will largely fail and eventually end up with a topgroup collection consisting of only empty groups (their hitcounts are guaranteed to be zero). an easy fix would be to rewrite the original blockjoinquery before invoking gettopgroups method. however, the computational cost of this is not optimal. a better but slightly more complex solution would be to save unrewrited queries into the lookup table.",
        "label": 33
    },
    {
        "text": "more tests of tochildblockjoinscorer advance i recently helped diagnose some strange errors with tochildblockjoinquery in an older version of solr which lead me to realize that the problem seemed to have been fixed by lucene-6593 \u2013 however the tests adrien added in that issue focused specifically the interaction of tochildblockjoinscorer with with the (fairly new) aproximations support in scorers (evidently that was trigger that caused adrien to investigate and make the fixes). however, in my initial diagnoses / testing, there were at least 2 (non aproximation based) situations where the old code was problematic: tochildblockjoinscorer.advance didn't satisfy the \"nextdoc equivilent behavior\" contract in the special case where the first doc in a segment was a parent w/o any kids in indexes that used multiple levels of hierarchy, a booleanquery that combined multiple tochildblockjoinqueries using different parent filters \u2013 ie: \"find docs that are children of x and grandchildren of y\" as mentioned, adrien's changes in lucene-6593 seemed to fix both of these problematic situations, but i'm opening this issue to track the addition of some new tests to explicitly cover these situations to protect us against future regression.",
        "label": 18
    },
    {
        "text": "polygons with holes don't compute bounds properly in certain situations hi karl wright, i thought it would be useful to create a random test for bounds. the test have shown problems with polygon with holes. in this ticket i will submit a random test for bounds and propose a fix for bounds for polygons.",
        "label": 25
    },
    {
        "text": "separate segmentreaders  and other atomic readers  from composite indexreaders with current trunk, whenever you open an indexreader on a directory you get back a directoryreader which is a composite reader. the interface of indexreader has now lots of methods that simply throw uoe (in fact more than 50% of all methods that are commonly used ones are unuseable now). this confuses users and makes the api hard to understand. this issue should split \"atomic readers\" from \"reader collections\" with a separate api. after that, you are no longer able, to get termsenum without wrapping from those composite readers. we currently have helper classes for wrapping (slowmultireaderwrapper - please rename, the name is really ugly; or multi*), those should be retrofitted to implement the correct classes (slowmultireaderwrapper would be an atomic reader but takes a composite reader as ctor param, maybe it could also simply take a list<atomicreader>). in my opinion, maybe composite readers could implement some collection apis and also have the readerutil method directly built in (possibly as a \"view\" in the util.collection sense). in general composite readers do not really need to look like the previous indexreaders, they could simply be a \"collection\" of segmentreaders with some functionality like reopen. on the other side, atomic readers do not need reopen logic anymore? when a segment changes, you need a new atomic reader? - maybe because of deletions thats not the best idea, but we should investigate. maybe make the whole reopen logic simplier to use (ast least on the collection reader level). we should decide about good names, i have no preference at the moment.",
        "label": 53
    },
    {
        "text": "remove dependency of lucene spatial on oal search filter we should try to remove usage of oal.search.filter in lucene/spatial. i gave it a try but this module makes non-trivial use of filters so i wouldn't mind some help here.",
        "label": 10
    },
    {
        "text": "adding numericdocvaluesfields is slowing down the indexing process significantly the indexing time for my ~2m documents has gone up significantly when i started adding fields of type numericdocvaluesfield.   upon debugging found the bottleneck to be in the perfieldmergestate#filterfieldinfos constructor. the contains check in the below code snippet was the culprit.  this.filterednames = new hashset<>(filterfields); this.filtered = new arraylist<>(filterfields.size()); for (fieldinfo fi : src) {   if (filterfields.contains(fi.name)) { a simple change as below seems to have fixed my issue this.filterednames = new hashset<>(filterfields); this.filtered = new arraylist<>(filterfields.size()); for (fieldinfo fi : src) {   if (this.filterednames.contains(fi.name)) {  ",
        "label": 46
    },
    {
        "text": "javadoc for facet user guide does not display because of saxparseexception  eclipse  maven  i have opened javadoc for facet api while using eclipse, which downloaded the javadocs using maven m2e plugin. when i click on facet user guide on the overview page i get the following exception in firefox: http://127.0.0.1:49231/help/nftopic/jar:file:/c:/users/karl/.m2/repository/org/apache/lucene/lucene-facet/4.0.0-alpha/ lucene-facet-4.0.0-alpha-javadoc.jar!/org/apache/lucene/facet/doc-files/userguide.html an error occured while processing the requested document: org.xml.sax.saxparseexception; linenumber: 121; columnnumber: 16; the element type \"br\" must be terminated by the matching end-tag \"</br>\". at com.sun.org.apache.xerces.internal.parsers.domparser.parse(unknown source) at com.sun.org.apache.xerces.internal.jaxp.documentbuilderimpl.parse(unknown source) the link, or requested document is: http://127.0.0.1:49231/help/nftopic/jar:file:/c:/users/karl/.m2/repository/org/apache/lucene/lucene-facet/4.0.0-alpha/ lucene-facet-4.0.0-alpha-javadoc.jar!/org/apache/lucene/facet/doc-files/userguide.html",
        "label": 53
    },
    {
        "text": "nullpointerexception in postingshighlighter in case an index segment does not have any docs with the field requested for highlighting indexed, there should be a null check immediately following this line (in postingshighlighter.java): terms t = r.terms(field); looks like the null check was moved in the 5.1 release and this is occasionally causing a nullpointerexception in my near-realtime searcher.",
        "label": 33
    },
    {
        "text": "nosuchelementexception occurs when org apache lucene facet index facetfields is used  hi, when i use the api as below : list<categorypath> categories = collections.<categorypath>singletonlist(new categorypath(path.toarray(new string[path.size()]))); facetfields facetfields = new facetfields(taxonomywriter); facetfields.addfields(document, categories); taxonomywriter.commit(); an exception occurs : java.util.nosuchelementexception at java.util.collections$1.next(collections.java:3302) at org.apache.lucene.facet.index.drilldownstream.reset(drilldownstream.java:78) at org.apache.lucene.index.docinverterperfield.processfields(docinverterperfield.java:97) at org.apache.lucene.index.docfieldprocessor.processdocument(docfieldprocessor.java:248) at org.apache.lucene.index.documentswriterperthread.updatedocument(documentswriterperthread.java:253) at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:453) at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1520) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1190) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1171) seems likes this is due to multiple calls to org.apache.lucene.facet.index.drilldownstream#reset which invoques #next() on an 'used' iterator. regards, lucien",
        "label": 43
    },
    {
        "text": "inefficient growth of openbitset hi, i found a potentially serious efficiency problem with openbitset. one typical (i think) way to build a bit set is to set() the bits one by one - e.g., have a hitcollector set() the bit for each matching document. the underlying array of longs needs to grow as more as more bits are set, of course. but looking at the code, it appears to me that the array grows very ineefficiently - in the worst case (when doc ids are sorted, as they would normally be in the hitcollector case for example), copying the array again and again for every added bit... the relevant code in openbitset.java is: public void set(long index) { int wordnum = expandingwordnum(index); ... } protected int expandingwordnum(long index) { int wordnum = (int)(index >> 6); if (wordnum>=wlen) { ensurecapacity(index+1); ... } public void ensurecapacitywords(int numwords) { if (bits.length < numwords) { long[] newbits = new long[numwords]; system.arraycopy(bits,0,newbits,0,wlen); bits = newbits; } } as you can see, if the bits array is not long enough, a new one is allocated at exactly the right size - and in the worst case it can grow just one word every time... shouldn't the growth be more exponential in nature, e.g., grow to the maximum of index+1 and twice the existing size? alternatively, if the growth is so inefficient, this should be documented, and it should be recommended to use the variant of the constructor with the correct initial size (e.g., in the hitcollector case, the number of documents in the index). and the fastset() method instead of set(). thanks, nadav.",
        "label": 33
    },
    {
        "text": "bufferdeleteterm in indexwriter might flush prematurely successive calls to remove-by-the-same-term would increment numbuffereddeleteterms although all but the first are no op if no docs were added in between. hence deletes would be flushed too soon. it is a minor problem, should be rare, but it seems cleaner to fix this. attached patch also fixes testindexwriterdelete.testnonramdelete() which somehow relied on this behavior. all tests pass.",
        "label": 12
    },
    {
        "text": "whitespacetokenizer should tokenize on nbsp whitespacetokenizer uses character.iswhitespace to decide what is whitespace. here's a pertinent excerpt: it is a unicode space character (space_separator, line_separator, or paragraph_separator) but is not also a non-breaking space ('\\u00a0', '\\u2007', '\\u202f') perhaps character.iswhitespace should have been called islinebreakablewhitespace? i think whitespacetokenizer should tokenize on this. i am aware it's easy to work around but why leave this trap in by default?",
        "label": 53
    },
    {
        "text": "move functionquery  valuesources and docvalues to queries module having resolved the functionquery sorting issue and moved the mutablevalue classes, we can now move functionquery, valuesources and docvalues to a queries module.",
        "label": 7
    },
    {
        "text": "index too old new is not a corruption indexformattoooldexception and indexformattoonewexception both extend from corruptindexexception. but this is not a corruption, it is simply an unsupported version of an index. they should just extend ioexception.",
        "label": 41
    },
    {
        "text": "large lucene index can hit false oom due to sun jre issue this is not a lucene issue, but i want to open this so future google diggers can more easily find it. there's this nasty bug in sun's jre: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546 the gist seems to be, if you try to read a large (eg 200 mb) number of bytes during a single randomaccessfile.read call, you can incorrectly hit oom. lucene does this, with norms, since we read in one byte per doc per field with norms, as a contiguous array of length maxdoc(). the workaround was a custom patch to do large file reads as several smaller reads. background here: http://www.nabble.com/problems-with-large-lucene-index-td22347854.html",
        "label": 46
    },
    {
        "text": "enable access to the freq information in a query's sub scorers the ability to gather more details than just the score, of how a given doc matches the current query, has come up a number of times on the user's lists. (most recently in the thread \"query match count\" by ryan mcv on java-user). eg if you have a simple termquery \"foo\", on each hit you'd like to know how many times \"foo\" occurred in that doc; or a booleanquery +foo +bar, being able to separately see the freq of foo and bar for the current hit. lucene doesn't make this possible today, which is a shame because lucene in fact does compute exactly this information; it's just not accessible from the collector.",
        "label": 46
    },
    {
        "text": "flexibility to turn on off any flush triggers see discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186 provide the flexibility to turn on/off any flush triggers - rambuffersize, maxbuffereddocs and maxbuffereddeleteterms. one of rambuffersize and maxbuffereddocs must be enabled.",
        "label": 33
    },
    {
        "text": "a similarity class which has unique length norms for numterms   a similarity class which extends defaultsimilarity and simply overrides lengthnorm. lengthnorm is implemented as a lookup for numterms <= 10, else as 1/sqrt(numterms). this is to avoid term counts below 11 from having the same lengthnorm after stored as a single byte in the index. this is useful if your search is only on short fields such as titles or product descriptions. see mailing list discussion: http://www.nabble.com/how-to-boost-the-score-higher-in-case-user-query-matches-entire-field-value-than-just-some-words-within-a-field-td19079221.html",
        "label": 38
    },
    {
        "text": "convert all lucene web properties to use the asf cms the new cms has a lot of nice features (and some kinks to still work out) and forrest just doesn't cut it anymore, so we should move to the asf cms: http://apache.org/dev/cms.html",
        "label": 15
    },
    {
        "text": "testdoctermords test failure hit this while beasting...: ant test -dtestcase=testdoctermords -dtests.method=testrandomwithprefix -dtests.seed=6d1f357b70583434 -dtests.locale=en_us -dtests.timezone=indian/christmas -dtests.codec=lucene40 -dargs=\"-dfile.encoding=utf-8\" here's the root cause:    [junit4] error   0.44s | testdoctermords.testrandomwithprefix    [junit4]    > throwable #1: java.lang.nullpointerexception    [junit4]    >  at org.apache.lucene.index.doctermords$termordsiterator.reset(doctermords.java:616)    [junit4]    >  at org.apache.lucene.index.doctermords.lookup(doctermords.java:642)    [junit4]    >  at org.apache.lucene.index.testdoctermords.verify(testdoctermords.java:364)    [junit4]    >  at org.apache.lucene.index.testdoctermords.testrandomwithprefix(testdoctermords.java:282)    [junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)    [junit4]    >  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.lang.reflect.method.invoke(method.java:601)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1961)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$1100(randomizedrunner.java:132)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:806)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:867)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:881)    [junit4]    >  at org.apache.lucene.util.lucenetestcase$subclasssetupteardownrule$1.evaluate(lucenetestcase.java:772)    [junit4]    >  at org.apache.lucene.util.lucenetestcase$internalsetupteardownrule$1.evaluate(lucenetestcase.java:694)    [junit4]    >  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:69)    [junit4]    >  at org.apache.lucene.util.lucenetestcase$testresultinterceptorrule$1.evaluate(lucenetestcase.java:627)    [junit4]    >  at org.apache.lucene.util.uncaughtexceptionsrule$1.evaluate(uncaughtexceptionsrule.java:75)    [junit4]    >  at org.apache.lucene.util.lucenetestcase$savethreadandtestnamerule$1.evaluate(lucenetestcase.java:666)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:813)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:669)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:688)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:724)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:735)    [junit4]    >  at org.apache.lucene.util.uncaughtexceptionsrule$1.evaluate(uncaughtexceptionsrule.java:75)    [junit4]    >  at org.apache.lucene.util.storeclassnamerule$1.evaluate(storeclassnamerule.java:38)    [junit4]    >  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:69)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)    [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551)",
        "label": 33
    },
    {
        "text": "searchermanager misses to close ir if manager is closed during reopen if we close sm while there is a thread calling maybreopen() and swapsearcher throws already closed exception we miss to close the searcher / reader.",
        "label": 46
    },
    {
        "text": "incorrect results from uax url email tokenizer i'm using an analyzer based on the uax_url_email, with a maximum token length of 64 characters. i expect the analyzer to discard any text in the url beyond those 64 characters, but the actual results yield ordinary terms from the tail-end of the url. for example, curl -xget http://localhost:9200/my_index/_analyze?analyzer=uax_url_email_analyzer -d \"hey, check out http://edge.org/conversation/yuval_noah_harari-daniel_kahneman-death-is-optional for some light reading.\" the results look like this: {     \"tokens\": [         {             \"token\": \"hey\",             \"start_offset\": 0,             \"end_offset\": 3,             \"type\": \"<alphanum>\",             \"position\": 1         },         {             \"token\": \"check\",             \"start_offset\": 5,             \"end_offset\": 10,             \"type\": \"<alphanum>\",             \"position\": 2         },         {             \"token\": \"out\",             \"start_offset\": 11,             \"end_offset\": 14,             \"type\": \"<alphanum>\",             \"position\": 3         },         {             \"token\": \"http://edge.org/conversation/yuval_noah_harari-daniel_kahneman-d\",             \"start_offset\": 15,             \"end_offset\": 79,             \"type\": \"<url>\",             \"position\": 4         },         {             \"token\": \"eath\",             \"start_offset\": 79,             \"end_offset\": 83,             \"type\": \"<alphanum>\",             \"position\": 5         },         {             \"token\": \"is\",             \"start_offset\": 84,             \"end_offset\": 86,             \"type\": \"<alphanum>\",             \"position\": 6         },         {             \"token\": \"optional\",             \"start_offset\": 87,             \"end_offset\": 95,             \"type\": \"<alphanum>\",             \"position\": 7         },         {             \"token\": \"for\",             \"start_offset\": 96,             \"end_offset\": 99,             \"type\": \"<alphanum>\",             \"position\": 8         },         {             \"token\": \"some\",             \"start_offset\": 100,             \"end_offset\": 104,             \"type\": \"<alphanum>\",             \"position\": 9         },         {             \"token\": \"light\",             \"start_offset\": 105,             \"end_offset\": 110,             \"type\": \"<alphanum>\",             \"position\": 10         },         {             \"token\": \"reading\",             \"start_offset\": 111,             \"end_offset\": 118,             \"type\": \"<alphanum>\",             \"position\": 11         }     ] } the term from the extracted url is correct, and correctly truncated at 64 characters. but as you can see, the analysis pipeline also creates three spurious terms [ \"eath\", \"is\" \"optional\" ] which come from the discarded portion of the url.",
        "label": 47
    },
    {
        "text": "ramdirectory close  should have a comment about not releasing any resources i wrongly assumed that calling ramdirectory.close() would free up the memory occupied by the ramdirectory. it might be helpful to add a javadoc comment that warns users that ramdirectory.close() is a no-op, since it might be a common assumption that close() would release resources.",
        "label": 18
    },
    {
        "text": "filesystems do not guarantee order of directories updates currently when index is written to disk the following sequence of events is taking place: write segment file sync segment file write segment file sync segment file ... write list of segments sync list of segments rename list of segments sync index directory this sequence leads to potential window of opportunity for system to crash after 'rename list of segments' but before 'sync index directory' and depending on exact filesystem implementation this may potentially lead to 'list of segments' being visible in directory while some of the segments are not. solution to this is to sync index directory after all segments have been written. this commit shows idea implemented. i'm fairly certain that i didn't find all the places this may be potentially happening.",
        "label": 13
    },
    {
        "text": "create searchertaxonomymanager over directory searchertaxonomymanager now only allows working in nrt mode. it could be useful to have an stm which allows reopening a searcherandtaxonomy pair over directories, e.g. for replication. the problem is that if the thread that calls mayberefresh() is not the one that does the commit(), it could lead to a pair that is not synchronized. perhaps at first we could have a simple version that works under some assumptions, i.e. that the app does the commit + reopen in the same thread in that order, so that it can be used by such apps + when replicating the indexes, and later we can figure out how to generalize it to work even if commit + reopen are done by separate threads/jvms. i'll see if searchertaxonomymanager can be extended to support it, or a new stm is required.",
        "label": 43
    },
    {
        "text": "inconsistency between weight scorer documentation and constantscorequery on top of a filter weight.scorer states that if topscorer == true, scorer.collect will be called and that otherwise scorer.nextdoc/advance will be called. this is a problem when constantscorequery is used on top of a querywrapperfilter: 1. constantscoreweight calls getdocidset on the filter to know which documents to collect. 2. querywrapperfilter.getdocidset returns a scorer created with topscorer == false so that nextdoc/advance are supported. 3. but then constantscorer.score(collector) has the following optimization:     // this optimization allows out of order scoring as top scorer!     @override     public void score(collector collector) throws ioexception {       if (docidsetiterator instanceof scorer) {         ((scorer) docidsetiterator).score(wrapcollector(collector));       } else {         super.score(collector);       }     } so the filter iterator is a scorer which was created with topscorer = false but parentscorer ends up using its score(collector) method, which is illegal. (i found this out because assertingsearcher has some checks to make sure scorers are used accordingly to the value of topscorer.) i can imagine several fixes, including: removing this optimization when working on top of a filter relaxing weight documentation to allow for using score(collector) when topscorer == false but i'm not sure which one is the best one. what do you think?",
        "label": 53
    },
    {
        "text": "highlighting with geopointinbboxquery can result in exception highlighter and geopointinbboxquery don't play well together. i wrote a test here that throws an exception which i think it should not: https://github.com/brwe/lucene-solr/commit/311f5527ffb6f3ef50e3f74b54456aa7b29d8898 the problem seems to be that geopointinbboxquery calls rewrite with a reader that contains a text field (which we want to highlight) and therefore has the wrong encoding. this is from an elasticsearch issue: https://github.com/elastic/elasticsearch/issues/17537",
        "label": 53
    },
    {
        "text": "more like this  ensures selection of best terms is indeed o n  ",
        "label": 46
    },
    {
        "text": "facetsaccumulator java throws nullpointerexception if it's given an empty categorypath  when i wanted to count root categories, i used to pass \"new categorypath(new string[0])\" to a countfacetrequest. since upgrading lucene from 4.1 to 4.2, that threw arrayindexofoutboundsexception, so i passed categorypath.empty to a countfacetrequest instead, and this time i got nullpointerexception. it all originates from facetsaccumulator.java:185 does someone conspire to prevent others from counting root categories?",
        "label": 43
    },
    {
        "text": "randompolygontest times out i saw a failure on the elastic ci that i can reproduce locally. the test either never finishes or is very very slow. this is due to the fact that the do ... polygon = ... while(polygon.getclass().equals(largepolygon.getclass())); never returns. note: the test result below was on branch_7x 21:13:34    [junit4] suite: org.apache.lucene.spatial3d.geom.randomgeopolygontest 21:13:34    [junit4]   2> jun 08, 2018 12:13:10 am com.carrotsearch.randomizedtesting.threadleakcontrol$2 evaluate 21:13:34    [junit4]   2> advertencia: suite execution timed out: org.apache.lucene.spatial3d.geom.randomgeopolygontest 21:13:34    [junit4]   2>    1) thread[id=23, name=test-randomgeopolygontest.testcomparebigpolygons-seed#[e636ffe9e01130d7], state=runnable, group=tgrp-randomgeopolygontest] 21:13:34    [junit4]   2>         at java.util.timsort.mergelo(timsort.java:730) 21:13:34    [junit4]   2>         at java.util.timsort.mergeat(timsort.java:514) 21:13:34    [junit4]   2>         at java.util.timsort.mergecollapse(timsort.java:439) 21:13:34    [junit4]   2>         at java.util.timsort.sort(timsort.java:245) 21:13:34    [junit4]   2>         at java.util.arrays.sort(arrays.java:1438) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geocomplexpolygon$tree.<init>(geocomplexpolygon.java:918) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geocomplexpolygon$xtree.<init>(geocomplexpolygon.java:1048) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geocomplexpolygon.<init>(geocomplexpolygon.java:129) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geopolygonfactory$bestshape.creategeocomplexpolygon(geopolygonfactory.java:463) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geopolygonfactory.makelargegeopolygon(geopolygonfactory.java:389) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geopolygonfactory.makegeopolygon(geopolygonfactory.java:226) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.geopolygonfactory.makegeopolygon(geopolygonfactory.java:142) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparepolygons(randomgeopolygontest.java:156) 21:13:34    [junit4]   2>         at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparebigpolygons(randomgeopolygontest.java:98) 21:13:34    [junit4]   2>         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) 21:13:34    [junit4]   2>         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) 21:13:34    [junit4]   2>         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) 21:13:34    [junit4]   2>         at java.lang.reflect.method.invoke(method.java:498) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1737) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:934) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$9.evaluate(randomizedrunner.java:970) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$10.evaluate(randomizedrunner.java:984) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:49) 21:13:34    [junit4]   2>         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:64) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:47) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:368) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:817) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:468) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:943) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:829) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:879) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:890) 21:13:34    [junit4]   2>         at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:41) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:40) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:40) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:53) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:47) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:64) 21:13:34    [junit4]   2>         at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:54) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:368) 21:13:34    [junit4]   2>         at java.lang.thread.run(thread.java:748) 21:13:34    [junit4]   2>    2) thread[id=1, name=main, state=waiting, group=main] 21:13:34    [junit4]   2>         at java.lang.object.wait(native method) 21:13:34    [junit4]   2>         at java.lang.thread.join(thread.java:1252) 21:13:34    [junit4]   2>         at java.lang.thread.join(thread.java:1326) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:636) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.run(randomizedrunner.java:493) 21:13:34    [junit4]   2>         at com.carrotsearch.ant.tasks.junit4.slave.slavemain.execute(slavemain.java:251) 21:13:34    [junit4]   2>         at com.carrotsearch.ant.tasks.junit4.slave.slavemain.main(slavemain.java:368) 21:13:34    [junit4]   2>         at com.carrotsearch.ant.tasks.junit4.slave.slavemainsafe.main(slavemainsafe.java:13) 21:13:34    [junit4]   2>    3) thread[id=18, name=junit4-serializer-daemon, state=timed_waiting, group=main] 21:13:34    [junit4]   2>         at java.lang.thread.sleep(native method) 21:13:34    [junit4]   2>         at com.carrotsearch.ant.tasks.junit4.events.serializer$1.run(serializer.java:50) 21:13:34    [junit4]   2>    4) thread[id=22, name=suite-randomgeopolygontest-seed#[e636ffe9e01130d7], state=runnable, group=tgrp-randomgeopolygontest] 21:13:34    [junit4]   2>         at java.lang.thread.getstacktrace(thread.java:1559) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$4.run(threadleakcontrol.java:696) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$4.run(threadleakcontrol.java:693) 21:13:34    [junit4]   2>         at java.security.accesscontroller.doprivileged(native method) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol.getstacktrace(threadleakcontrol.java:693) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol.getthreadswithtraces(threadleakcontrol.java:709) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol.formatthreadstacksfull(threadleakcontrol.java:689) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol.access$1000(threadleakcontrol.java:65) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.threadleakcontrol$2.evaluate(threadleakcontrol.java:415) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:705) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner.access$200(randomizedrunner.java:139) 21:13:34    [junit4]   2>         at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:626) 21:13:34    [junit4]   2> note: reproduce with: ant test  -dtestcase=randomgeopolygontest -dtests.method=testcomparebigpolygons -dtests.seed=e636ffe9e01130d7 -dtests.slow=true -dtests.badapples=true -dtests.locale=es-do -dtests.timezone=asia/tel_aviv -dtests.asserts=true -dtests.file.encoding=utf8 21:13:34    [junit4] error   7193s j1 | randomgeopolygontest.testcomparebigpolygons {seed=[e636ffe9e01130d7:7e192d1921bdfe08]} <<< 21:13:34    [junit4]    > throwable #1: java.lang.exception: test abandoned because suite timeout was reached. 21:13:34    [junit4]    >  at __randomizedtesting.seedinfo.seed([e636ffe9e01130d7]:0)",
        "label": 19
    },
    {
        "text": "cachingspanfilter synchronizing on a none final protected object cachingspanfilter and cachingwrapperfilter expose their internal cache via a protected member which is lazily instantiated in the getdocsetid method. the current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. my first guess is that this member was exposed for testing purposes so it should rather be changed to package private. this patch breaks backwards compat while i guess the cleanup is kind of worth breaking it.",
        "label": 53
    },
    {
        "text": "speed up mmapdirectory seek  for traditional lucene access which is mostly sequential, occasional advance(), i think this method gets drowned out in noise. but for access like docvalues, its important. unfortunately seek() is complex today because of mapping multiple buffers. however, the very common case is that only one map is used for a given clone or slice. when there is the possibility to use only a single mapped buffer, we should instead take advantage of bytebuffer.slice(), which will adjust the internal mmap address and remove the offset calculation. furthermore we don't need the shift/mask or even the negative check, as they are then all handled with the bytebuffer api: seek is a one-liner (with try/catch of course to convert exceptions). this makes docvalues access 20% faster, i havent tested conjunctions or anyhting like that.",
        "label": 53
    },
    {
        "text": "minhashfilter's ctor should validate its args my jenkins found this reproducing branch_6x seed:    [junit4] suite: org.apache.lucene.analysis.core.testrandomchains    [junit4]   2> exception from random analyzer:     [junit4]   2> charfilters=    [junit4]   2> tokenizer=    [junit4]   2>   org.apache.lucene.analysis.standard.standardtokenizer()    [junit4]   2> filters=    [junit4]   2>   org.apache.lucene.analysis.minhash.minhashfilter(validatingtokenfilter@6ae99167 term=,bytes=[],startoffset=0,endoffset=0,positionincrement=1,positionlength=1,type=word, 5, 5, -3, true)    [junit4]   2>   org.apache.lucene.analysis.bg.bulgarianstemfilter(validatingtokenfilter@40844352 term=,bytes=[],startoffset=0,endoffset=0,positionincrement=1,positionlength=1,type=word,keyword=false)    [junit4]   2> offsetsarecorrect=true    [junit4]   2> note: reproduce with: ant test  -dtestcase=testrandomchains -dtests.method=testrandomchainswithlargestrings -dtests.seed=4733e677ebdc28fc -dtests.slow=true -dtests.locale=ar-om -dtests.timezone=atlantic/south_georgia -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   3.18s j4 | testrandomchains.testrandomchainswithlargestrings <<<    [junit4]    > throwable #1: java.util.nosuchelementexception    [junit4]    >  at __randomizedtesting.seedinfo.seed([4733e677ebdc28fc:2d685966b292080f]:0)    [junit4]    >  at java.util.treemap.key(treemap.java:1323)    [junit4]    >  at java.util.treemap.lastkey(treemap.java:297)    [junit4]    >  at java.util.treeset.last(treeset.java:401)    [junit4]    >  at org.apache.lucene.analysis.minhash.minhashfilter$fixedsizetreeset.add(minhashfilter.java:325)    [junit4]    >  at org.apache.lucene.analysis.minhash.minhashfilter.incrementtoken(minhashfilter.java:159)    [junit4]    >  at org.apache.lucene.analysis.validatingtokenfilter.incrementtoken(validatingtokenfilter.java:67)    [junit4]    >  at org.apache.lucene.analysis.bg.bulgarianstemfilter.incrementtoken(bulgarianstemfilter.java:48)    [junit4]    >  at org.apache.lucene.analysis.validatingtokenfilter.incrementtoken(validatingtokenfilter.java:67)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkresetexception(basetokenstreamtestcase.java:405)    [junit4]    >  at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:510)    [junit4]    >  at org.apache.lucene.analysis.core.testrandomchains.testrandomchainswithlargestrings(testrandomchains.java:959)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=asserting(lucene62): {dummy=lucene50(blocksize=128)}, docvalues:{}, maxpointsinleafnode=252, maxmbsortinheap=5.297834377897023, sim=classicsimilarity, locale=ar-om, timezone=atlantic/south_georgia    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=395080152,total=465567744    [junit4]   2> note: all tests run in this jvm: [testdecimaldigitfilterfactory, testmultiwordsynonyms, testreversepathhierarchytokenizer, testdoubleescape, testhunspellstemfilterfactory, testarabicnormalizationfilter, testuax29urlemailanalyzer, testswedishlightstemfilterfactory, testbulgarianstemmer, testasciifoldingfilter, testdelimitedpayloadtokenfilterfactory, testindonesianstemmer, testcircumfix, edgengramtokenfiltertest, testpatterntokenizer, testscandinavianfoldingfilter, testignore, testrandomchains]    [junit4] completed [130/272 (1!)] on j4 in 9.85s, 2 tests, 1 error <<< failures!",
        "label": 47
    },
    {
        "text": "remove ignoreincompatiblegeometry for spatialstrategys silently not indexing anything for a shape is not okay. users should get an exception and then they can decide how to proceed.",
        "label": 10
    },
    {
        "text": "standardtokenizer performance bug  buffer is unnecessarily copied when maxtokenlength doesn't change from piotr idzikowski on java-user mailing list http://markmail.org/message/af26kr7fermt2tfh: i am developing own analyzer based on standardanalyzer. i realized that tokenizer.setmaxtokenlength is called many times. protected tokenstreamcomponents createcomponents(final string fieldname, final reader reader) {     final standardtokenizer src = new standardtokenizer(getversion(), reader);     src.setmaxtokenlength(maxtokenlength);     tokenstream tok = new standardfilter(getversion(), src);     tok = new lowercasefilter(getversion(), tok);     tok = new stopfilter(getversion(), tok, stopwords);     return new tokenstreamcomponents(src, tok) {       @override       protected void setreader(final reader reader) throws ioexception {         src.setmaxtokenlength(standardanalyzer.this.maxtokenlength);         super.setreader(reader);       }     };   } does it make sense if length stays the same? i see it finally calls this one( in standardtokenizerimpl ): public final void setbuffersize(int numchars) {      zz_buffersize = numchars;      char[] newzzbuffer = new char[zz_buffersize];      system.arraycopy(zzbuffer, 0, newzzbuffer, 0, math.min(zzbuffer.length, zz_buffersize));      zzbuffer = newzzbuffer;    } so it just copies old array content into the new one.",
        "label": 47
    },
    {
        "text": "weak references cause extreme gc churn we are running a set of independent search machines, running our custom software using lucene as a search library. we recently upgraded from lucene 3.0.3 to 3.6.1 and noticed a severe degradation of performance. after doing some heap dump digging, it turns out the process is stalling because it's spending so much time in gc. we noticed about 212 million weakreference, originating from weakidentitymap, originating from mmapindexinput. our problem completely went away after removing the clones weakhashmap from mmapindexinput, and as a side-effect, disabling support for explictly unmapping the mmapped data.",
        "label": 53
    },
    {
        "text": "datainput readvint  supports negative numbers although not documented readvint() has to return positive numbers (and zero), throw some exception in case of negative numbers. while for the sequence of bytes [-1, -1, -1, -1, 15] it returns -1. simplifying readvint up to last readbyte (exclusive): int i = ((byte)-1) & 0x7f; i |= (((byte)-1) & 0x7f) << 7; i |= (((byte)-1) & 0x7f) << 14; i |= (((byte)-1) & 0x7f) << 21; here i = 268435455 or in binary format is 00001111_11111111_11111111_11111111 keeping in mind that int is a signed type we have only 3 more bits before overflow happens or in another words (integer.max_value - i) >> 28 == 7 - that's max value could be stored in 5th byte to avoid overflow. instead of i |= (b & 0x0f) << 28; if ((b & 0xf0) == 0) return i; has to be i |= (b & 0x07) << 28; if ((b & 0xf8) == 0) return i;",
        "label": 53
    },
    {
        "text": "multiple filtered subsets of the same underlying index passed to iw addindexes  can produce an index with bad sorteddocvalues were hit by this in a custom index splitter implementation that showed no problems with lucene 4.8. after upgrading to 4.10 documents started having wrong sorteddocvalues after splitting.",
        "label": 1
    },
    {
        "text": "highlighter depends on analyzers common this is a huge wtf, just for \"limittokenoffsetfilter\" which is only useful for highlighting. adding all these intermodule dependencies makes things too hard to use. this is a 5.3 release blocker.",
        "label": 46
    },
    {
        "text": "a slightly more accurate sloppymath distance sloppymath, intriduced in lucene-5258, uses earth's avg. (according to wgs84) ellipsoid radius as an approximation for computing the \"spherical\" distance. (the to_kilometers constant). while this is pretty accurate for long distances (latitude wise) this may introduce some small errors while computing distances close to the equator (as the earth radius there is larger than the avg.) a more accurate approximation would be taking the avg. earth radius at the source and destination points. but computing an ellipsoid radius at any given point is a heavy function, and this distance should be used in a scoring function.. so two optimizations are optional - pre-compute a table with an earth radius per latitude (the longitude does not affect the radius) instead of using two point radius avg, figure out the avg. latitude (exactly between the src and dst points) and get its radius.",
        "label": 41
    },
    {
        "text": "indexreader reopen  this is robert engels' implementation of indexreader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).",
        "label": 32
    },
    {
        "text": "false assertion of  position delta in standardpostingswriterimpl standardpostingswriterimpl line 159 is:     assert delta > 0 || position == 0 || position == -1: \"position=\" + position + \" lastposition=\" + lastposition;            // not quite right (if pos=0 is repeated twice we don't catch it) i enable assertions when i run my unit tests and i've found this assertion to fail when delta is 0 which occurs when the same position value is sent in twice in arrow. once i added removeduplicatestokenfilter, this problem went away. should i really be forced to add this filter? i think delta >= 0 would be a better assertion.",
        "label": 33
    },
    {
        "text": "position checking span queries i've created a bunch of new spanquery classes that allow one to do things like check to see if a spanquery falls between two positions (which is a more general form of spanfirstquery) and i've also added one that only includes a match if the payload located at the span match also matches a given payload. with the latter, one can do queries for items w/ specific payloads.",
        "label": 15
    },
    {
        "text": "allow fuzzyslop customization in classic queryparser it turns out searching arbitrary fields with define fuzzy_slop values could be problematic on some types of values. for example a fuzzy_slop on dates is ambiguous and needs a definition of a unit like months, days, minutes, etc. an extension on the query grammar that allows some arbitrary text behind the values in combination with a possibility to override the method parsing those values could solve these kinds of problems.",
        "label": 46
    },
    {
        "text": "something wrong with how  file formats  link is generated in docs index html   can cause precommit to fail on some systems i'm not sure what's going on, but here's what i've figured out while poking at things with ishan to try and figure out why ant precommit fails for him on a clean checkout of master... on my machine, with a clean checkout, the generated index.html file has lines that look like this... <li> <a href=\"core/org/apache/lucene/codecs/lucene62 /package-summary.html#package.description\">file formats</a>: guide to the supported index format used by lucene.  this can be customized by using <a href=\"core/org/apache/lucene/codecs/package-summary.html#package.description\">an alternate codec</a>.</li> <li> ...note there is a newline in the href after lucene62 on ishan's machine, with a clean checkout, the same line looks like this... <li> <a href=\"core/org/apache/lucene/codecs/lucene62%0a/package-summary.html#package.description\">file formats</a>: guide to the supported index format used by lucene.  this can be customized by using <a href=\"core/org/apache/lucene/codecs/package-summary.html#package.description\">an alternate codec</a>.</li> <li> ...note that he has a url escaped 'no-break space' (u+00a0) character in href attribute. on my machine, ant documentation-lint doesn't complain about the newline in the href attribute when checking links. on ishan's machine, ant documentation-lint most certainly complains about the 'no-break space'... ... -documentation-lint:      [echo] checking for broken html...     [jtidy] checking for broken html (such as invalid tags)...    [delete] deleting directory /home/ishan/code/chatman-lucene-solr/lucene/build/jtidy_tmp      [echo] checking for broken links...      [exec]       [exec] crawl/parse...      [exec]       [exec] verify...      [exec]       [exec] file:///build/docs/index.html      [exec]   broken link: file:///build/docs/core/org/apache/lucene/codecs/lucene62%0a/package-summary.html      [exec]       [exec] broken javadocs links were found! build failed raising the following questions... how is either a newline or a 'no-break space' getting introduced into the $defaultcodecpackage variable that index.xsl uses to generate that href attribute? why doesn't documentation-lint complain that the href has a newline in it on my system?",
        "label": 18
    },
    {
        "text": "patch mrjar classes fails if an old version of asm is on the ant classpath if some optional tasks that depend on an old version of asm are installed, patching fails with the following error: /home/jpountz/src/lucene-solr/lucene/common-build.xml:565: java.lang.incompatibleclasschangeerror: class org.objectweb.asm.commons.classremapper has interface org.objectweb.asm.classvisitor as super class the reason is that classremapper is loaded from the right place, but classvisitor, its parent class, is loaded from the parent classpath which may be a different version. it is easy to reproduce: download and extract ant-1.10.1 (latest version) run bin/ant -f fetch.xml -ddest=system, this will add lib/asm-2.2.3.jar among other files run ant clean test at the root of lucene-solr.",
        "label": 53
    },
    {
        "text": "potential resource leak in bigramdictionary java the input and output object streams are being closed in the try block. these resources will not be closed if an exception occurs in the try block we can use the finally block to explicitly close these resources or use the new try-with-resources construct where they are implicitly closed.",
        "label": 53
    },
    {
        "text": "add log step support per task following lucene-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. the .alg file will support: log.step - for all tasks. log.step.<task class name> - for a specific task. for example, log.step.adddoc, or log.step.deletedoc i will post the patch soon",
        "label": 29
    },
    {
        "text": "queryparser parses on whitespace the queryparser parses input on whitespace, and sends each whitespace separated term to its own independent token stream. this breaks the following at query-time, because they can't see across whitespace boundaries: n-gram analysis shingles synonyms (especially multi-word for whitespace-separated languages) languages where a 'word' can contain whitespace (e.g. vietnamese) its also rather unexpected, as users think their charfilters/tokenizers/tokenfilters will do the same thing at index and querytime, but in many cases they can't. instead, preferably the queryparser would parse around only real 'operators'.",
        "label": 47
    },
    {
        "text": "merging multiple indexes does not maintain document order  when i merge multiple indexes into a single, empty index, the document addition order is not being maintained. self contained test case coming (as soon as i figure out how to attach it)",
        "label": 55
    },
    {
        "text": "indexreader readerclosedlistener is not always called on indexreader close  today indexreader#readerclosedlistener might not be called if the last indexreader#decref() call runs into an exception on indexreader#doclose(). today we just reset the refcount and never go and call the listeners. there seem to be a bunch of problems here along the same lines but imo if we close a reader it should close all resources no matter what exception it runs into. what this should do is call the close listeners in a finally block and then rethrow the exception. the real problem here for apps relying on the listener to release resources is that you might leak memory or file handles or whatnot which i think is a bug how we handle closing the ir. as a side-note i think we should never reset the reference here to be honest. along the same lines i think we need to fix the loop in indexreader#notifyreaderclosedlisteners() to make sure we call all of them in the case any of them throws an exception. it also seems that segmentcorereaders#decref() has a similar problem where for instance a fieldsreader can throw an exception on close and we never call the core listeners. imo we need to fix this for 4.7.1",
        "label": 46
    },
    {
        "text": "disable atime for directiolinuxdirectory in linux's open(): o_noatime (since linux 2.6.8) do not update the file last access time (st_atime in the inode) when the file is read(2). this flag is intended for use by indexing or backup programs, where its use can significantly reduce the amount of disk activity. this flag may not be effective on all filesystems. one example is nfs, where the server maintains the access time. so we should do this in our linux-specific directiolinuxdirectory. separately (offtopic), it would be better if this was a linuxdirectory that only uses o_direct when it should it would be nice to think about an optional modules/native for common platforms similar to what tomcat provides its easier to test directories like this now (-dtests.directory)...",
        "label": 40
    },
    {
        "text": "fuzzyquery  matching terms must be longer than maxedits fuzzyquery's maxedit value must be larger than the length of both terms for there to be a match. based on a response from the java-user list, it looks like i wasn't the only one surprised by this. let's document this design choice more clearly in the documentation or modify the behavior. apologies if i missed the documentation of this.",
        "label": 33
    },
    {
        "text": "core test should not have dependencies on the demo code the testdoc.java test file has a dependency on the demo filedocument code. some of us don't keep the demo code around after downloading, so this breaks the build. patch will be along shortly",
        "label": 32
    },
    {
        "text": "fail fsync immediately while analysing a build issue in elasticsearch i stumpled upon org.apache.lucene.util.ioutils.fsync. it has a retry loop in fsync whenever an ioexception occurs. however, there are lots of instances where a retry is not useful, e.g. when a channel has been closed, a closedchannelexception is thrown and ioutils#fsync still tries to fsync multiple times on the closed channel. after bringing the issue to robert's attention, he even opted for removing the retry logic entirely for fsyncing. please find attached a patch that removes the retry logic.",
        "label": 53
    },
    {
        "text": "revisit segments gen sleeping in lucene-3601, i worked up a change where we intentionally crash() all un-fsynced files in tests to ensure that we are calling sync on files when we should. i think this would be nice to do always (and with some fixes all tests pass). but this is super-slow sometimes because when we corrupt the unsynced segments.gen, it causes sis.read to take 500ms each time (and in checkindex for some reason we do this twice, which seems wrong). i can workaround this for now for tests (just do a partial crash that avoids corrupting the segments.gen), but i wanted to create this issue for discussion about the sleeping/non-fsyncing of segments.gen, just because i guess its possible someone could hit this slowness.",
        "label": 33
    },
    {
        "text": "remove all interning of field names from flex api in previous versions of lucene, interning of fields was important to minimize string comparison cost when iterating termenums, to detect changes in field name. as we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. i will start with doing this, but we need to carefully review some places e.g. in preflex codec. maybe before this issue we should remove the term class completely. robert?",
        "label": 33
    },
    {
        "text": "    operators allow any amount of whitespace as an example, (foo - bar) is treated like (foo -bar). it seems like for +- to be treated as unary operators, they should be immediately followed by the operand.",
        "label": 21
    },
    {
        "text": "make fieldselector usable from searchable seems reasonable that you would want to be able to specify a fieldselector from searchable because many systems do not use indexsearcher (where you can get a reader), but instead use searchable or searcher so that searchers and multisearchers can be used in a polymorphic manner.",
        "label": 15
    },
    {
        "text": "sortingatomicreadertest failure fails 100% for me on trunk, whether or not i give the seed below, but appears to be trunk only - the test passes on branch_4x: ant test -dtestcase=sortingatomicreadertest -dtests.seed=fdbc5417f9f1ec45 -dtests.slow=true -dtests.locale=ja_jp -dtests.timezone=africa/juba -dtests.file.encoding=utf-8 common.test: [junit4:pickseed] seed property 'tests.seed' already defined: fdbc5417f9f1ec45 [junit4:junit4] <junit4> says cze\u015b\u0107! master seed: fdbc5417f9f1ec45 [junit4:junit4] executing 1 suite with 1 jvm. [junit4:junit4]  [junit4:junit4] started j0 pid(82211@smb.local). [junit4:junit4] suite: org.apache.lucene.index.sorter.sortingatomicreadertest [junit4:junit4]   1> checkreader failed [junit4:junit4]   1>     test: field norms.........ok [3 fields] [junit4:junit4]   1>     test: terms, freq, prox...error: java.lang.arrayindexoutofboundsexception: -1 [junit4:junit4]   1> java.lang.arrayindexoutofboundsexception: -1 [junit4:junit4]   1>  at org.apache.lucene.index.sorter.sortingatomicreader$sortingdocsandpositionsenum.docid(sortingatomicreader.java:562) [junit4:junit4]   1>  at org.apache.lucene.search.docidsetiterator.slowadvance(docidsetiterator.java:99) [junit4:junit4]   1>  at org.apache.lucene.index.sorter.sortingatomicreader$sortingdocsandpositionsenum.advance(sortingatomicreader.java:557) [junit4:junit4]   1>  at org.apache.lucene.index.checkindex.checkfields(checkindex.java:942) [junit4:junit4]   1>  at org.apache.lucene.index.checkindex.testpostings(checkindex.java:1200) [junit4:junit4]   1>  at org.apache.lucene.index.checkindex.testpostings(checkindex.java:1177) [junit4:junit4]   1>  at org.apache.lucene.util._testutil.checkreader(_testutil.java:243) [junit4:junit4]   1>  at org.apache.lucene.util._testutil.checkreader(_testutil.java:234) [junit4:junit4]   1>  at org.apache.lucene.index.sorter.sortingatomicreadertest.beforeclasssortingatomicreadertest(sortingatomicreadertest.java:77) [junit4:junit4]   1>  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit4:junit4]   1>  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) [junit4:junit4]   1>  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) [junit4:junit4]   1>  at java.lang.reflect.method.invoke(method.java:601) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1559) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:79) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:677) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:693) [junit4:junit4]   1>  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46) [junit4:junit4]   1>  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]   1>  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:43) [junit4:junit4]   1>  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48) [junit4:junit4]   1>  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70) [junit4:junit4]   1>  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358) [junit4:junit4]   1>  at java.lang.thread.run(thread.java:722) [junit4:junit4]   1>     test: stored fields.......ok [21 total field count; avg 1 fields per doc] [junit4:junit4]   1>     test: term vectors........error [-1] [junit4:junit4]   1> java.lang.arrayindexoutofboundsexception: -1 [junit4:junit4]   1>  at org.apache.lucene.index.sorter.sortingatomicreader$sortingdocsandpositionsenum.docid(sortingatomicreader.java:562) [junit4:junit4]   1>  at org.apache.lucene.search.docidsetiterator.slowadvance(docidsetiterator.java:99) [junit4:junit4]   1>  at org.apache.lucene.index.sorter.sortingatomicreader$sortingdocsandpositionsenum.advance(sortingatomicreader.java:557) [junit4:junit4]   1>  at org.apache.lucene.index.checkindex.testtermvectors(checkindex.java:1564) [junit4:junit4]   1>  at org.apache.lucene.util._testutil.checkreader(_testutil.java:245) [junit4:junit4]   1>  at org.apache.lucene.util._testutil.checkreader(_testutil.java:234) [junit4:junit4]   1>  at org.apache.lucene.index.sorter.sortingatomicreadertest.beforeclasssortingatomicreadertest(sortingatomicreadertest.java:77) [junit4:junit4]   1>  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit4:junit4]   1>  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) [junit4:junit4]   1>  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) [junit4:junit4]   1>  at java.lang.reflect.method.invoke(method.java:601) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1559) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:79) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:677) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:693) [junit4:junit4]   1>  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:46) [junit4:junit4]   1>  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:42) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]   1>  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:43) [junit4:junit4]   1>  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48) [junit4:junit4]   1>  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70) [junit4:junit4]   1>  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) [junit4:junit4]   1>  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:358) [junit4:junit4]   1>  at java.lang.thread.run(thread.java:722) [junit4:junit4]   1>     test: docvalues...........ok [0 total doc count; 4 docvalues fields] [junit4:junit4]   1>  [junit4:junit4]   2> note: test params are: codec=lucene42: {id=postingsformat(name=testbloomfilteredlucene41postings), positions=postingsformat(name=memory dopackfst= true), norm=mockvariableintblock(baseblocksize=104), docs=postingsformat(name=testbloomfilteredlucene41postings), term_vectors=postingsformat(name=testbloomfilteredlucene41postings)}, docvalues:{binary=docvaluesformat(name=cheapbastard), numeric=docvaluesformat(name=cheapbastard), sorted=docvaluesformat(name=cheapbastard), sorted_set=docvaluesformat(name=lucene42)}, sim=defaultsimilarity, locale=ja_jp, timezone=africa/juba [junit4:junit4]   2> note: mac os x 10.8.3 x86_64/oracle corporation 1.7.0_13 (64-bit)/cpus=8,threads=1,free=87000136,total=117702656 [junit4:junit4]   2> note: all tests run in this jvm: [sortingatomicreadertest] [junit4:junit4]   2> note: reproduce with: ant test  -dtestcase=sortingatomicreadertest -dtests.seed=fdbc5417f9f1ec45 -dtests.slow=true -dtests.locale=ja_jp -dtests.timezone=africa/juba -dtests.file.encoding=utf-8 [junit4:junit4] error   0.00s | sortingatomicreadertest (suite) <<< [junit4:junit4]    > throwable #1: java.lang.runtimeexception: checkreader failed [junit4:junit4]    >  at __randomizedtesting.seedinfo.seed([fdbc5417f9f1ec45]:0) [junit4:junit4]    >  at org.apache.lucene.util._testutil.checkreader(_testutil.java:255) [junit4:junit4]    >  at org.apache.lucene.util._testutil.checkreader(_testutil.java:234) [junit4:junit4]    >  at org.apache.lucene.index.sorter.sortingatomicreadertest.beforeclasssortingatomicreadertest(sortingatomicreadertest.java:77) [junit4:junit4]    >  at java.lang.thread.run(thread.java:722) [junit4:junit4] completed in 1.12s, 0 tests, 1 error <<< failures! [junit4:junit4]  [junit4:junit4]  [junit4:junit4] tests with failures: [junit4:junit4]   - org.apache.lucene.index.sorter.sortingatomicreadertest (suite) [junit4:junit4]  [junit4:junit4]  [junit4:junit4] jvm j0:     0.63 ..     2.33 =     1.70s [junit4:junit4] execution time total: 2.33 sec. [junit4:junit4] tests summary: 1 suite, 0 tests, 1 suite-level error build failed",
        "label": 1
    },
    {
        "text": "optimization for indexwriter addindexes  one big performance problem with indexwriter.addindexes() is that it has to optimize the index both before and after adding the segments. when you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addindexes() impossible. it makes parallel updates very frustrating. here is an optimized function that helps out by calling mergesegments only on the newly added documents. it will try to avoid calling mergesegments until the end, unless you're adding a lot of documents at once. i also have an extensive unit test that verifies that this function works correctly if people are interested. i gave it a different name because it has very different performance characteristics which can make querying take longer.",
        "label": 55
    },
    {
        "text": "documentexpressiondictionarytest failure reproduce with: ant test -dtestcase=documentexpressiondictionarytest -dtests.method=testbasic -dtests.seed=42efe66b86db4740 -dtests.slow=true -dtests.locale=es_bo -dtests.timezone=us/central -dtests.file.encoding=utf-8 common.test: [junit4:pickseed] seed property 'tests.seed' already defined: 42efe66b86db4740 [junit4] <junit4> says \u043f\u0440\u0438\u0432\u0435\u0442! master seed: 42efe66b86db4740 [junit4] executing 1 suite with 1 jvm. [junit4] [junit4] started j0 pid(2144@a8206617c3d8.ant.amazon.com). [junit4] suite: org.apache.lucene.search.suggest.documentexpressiondictionarytest [junit4] 2> note: reproduce with: ant test -dtestcase=documentexpressiondictionarytest -dtests.method=testbasic -dtests.seed=42efe66b86db4740 -dtests.slow=true -dtests.locale=es_bo -dtests.timezone=us/central -dtests.file.encoding=utf-8 [junit4] error 0.65s | documentexpressiondictionarytest.testbasic <<< [junit4] > throwable #1: java.lang.illegalargumentexception: compositereader is not supported [junit4] > at __randomizedtesting.seedinfo.seed([42efe66b86db4740:e915fb7e5907c16e]:0) [junit4] > at org.apache.lucene.search.suggest.documentexpressiondictionary$documentexpressioninputiterator.<init>(documentexpressiondictionary.java:110) [junit4] > at org.apache.lucene.search.suggest.documentexpressiondictionary.getwordsiterator(documentexpressiondictionary.java:98) [junit4] > at org.apache.lucene.search.suggest.documentexpressiondictionarytest.testbasic(documentexpressiondictionarytest.java:92) [junit4] > at java.lang.thread.run(thread.java:724) [junit4] 2> note: test params are: codec=lucene41, sim=randomsimilarityprovider(querynorm=false,coord=yes): {f1=dfr i(f)3(800.0)} , locale=es_bo, timezone=us/central [junit4] 2> note: mac os x 10.8.5 x86_64/oracle corporation 1.7.0_25 (64-bit)/cpus=8,threads=1,free=121040408,total=161349632 [junit4] 2> note: all tests run in this jvm: [documentexpressiondictionarytest] [junit4] completed in 1.05s, 1 test, 1 error <<< failures! [junit4] [junit4] [junit4] tests with failures: [junit4] - org.apache.lucene.search.suggest.documentexpressiondictionarytest.testbasic [junit4] [junit4] [junit4] jvm j0: 0.66 .. 2.36 = 1.70s [junit4] execution time total: 2.37 sec. [junit4] tests summary: 1 suite, 1 test, 1 error build failed /users/jdconrad/code/lucenesolr/lucene-solr/lucene/module-build.xml:60: the following error occurred while executing this line: /users/jdconrad/code/lucenesolr/lucene-solr/lucene/common-build.xml:1259: the following error occurred while executing this line: /users/jdconrad/code/lucenesolr/lucene-solr/lucene/common-build.xml:902: there were test failures: 1 suite, 1 test, 1 error",
        "label": 33
    },
    {
        "text": "spi  allow fallback to default classloader if thread getcontextclassloader fails note: this issue has been renamed from: \"replace calls to thread#getcontextclassloader with the classloader of the current class\" because the revised patch provides a clean fallback path. i am not sure whether it is a design decision or if we can indeed consider this a bug: in core and analysis-common some classes provide on-demand class loading using spi. in namedspiloader, spiclassiterator, classpathresourceloader and analysisspiloader there are constructors that use the thread's context classloader by default whenever no particular other classloader was specified. unfortunately this does not work as expected when the thread's classloader can't see the required classes that are instantiated downstream with the help of class.forname (e.g., codecs, analyzers, etc.). that's what happened to us here. we currently experiment with running lucene 2.9 and 4.x in one jvm, both being separated by custom classloaders, each seeing only the corresponding lucene version and the upstream classpath. while namedspiloader and company get successfully loaded by our custom classloader, their instantiation fails because our thread's context-classloader cannot find the additionally required classes. we could probably work-around this by using thread#setcontextclassloader at construction time (and quickly reverting back afterwards), but i have the impression this might just hide the actual problem and cause further trouble when lazy-loading classes later on, and potentially from another thread. removing the call to thread#getcontextclassloader would also align with the behavior of attributesource.default_attribute_factory, which in fact uses attribute#getclass().getclassloader() instead. a simple patch is attached. all tests pass.",
        "label": 53
    },
    {
        "text": "interface termfreqvector has incomplete javadocs we should improve the javadocs of org.apache.lucene.index.termfreqvector",
        "label": 15
    },
    {
        "text": "fst cannot be loaded if it's larger than integer max value   bytes this is really quite awful, but the test i created for > 2.1 gb fsts never tested save/load ... and ... it doesn't work.",
        "label": 1
    },
    {
        "text": "nightly 'test lock factory' may leak file handles https://builds.apache.org/job/lucene-solr-nightlytests-trunk/556/console [lockstresstest1] exception in thread \"main\" java.nio.file.filesystemexception: /usr/home/hudson/hudson-slave/workspace/lucene-solr-nightlytests-trunk/lucene/build/core/lockfactorytest/test-test.lock: too many open files in system [lockstresstest1]  at sun.nio.fs.unixexception.translatetoioexception(unixexception.java:91) [lockstresstest1]  at sun.nio.fs.unixexception.rethrowasioexception(unixexception.java:102) [lockstresstest1]  at sun.nio.fs.unixexception.rethrowasioexception(unixexception.java:107) [lockstresstest1]  at sun.nio.fs.unixfilesystemprovider.newfilechannel(unixfilesystemprovider.java:176) [lockstresstest1]  at java.nio.channels.filechannel.open(filechannel.java:287) [lockstresstest1]  at java.nio.channels.filechannel.open(filechannel.java:334) [lockstresstest1]  at org.apache.lucene.store.nativefslock.obtain(nativefslockfactory.java:149) [lockstresstest1]  at org.apache.lucene.store.verifyinglockfactory$checkedlock.obtain(verifyinglockfactory.java:65) [lockstresstest1]  at org.apache.lucene.store.lock.obtain(lock.java:77) [lockstresstest1]  at org.apache.lucene.store.lockstresstest.main(lockstresstest.java:114)",
        "label": 40
    },
    {
        "text": "refactoring multidocvalues ordinalmap to clarify api and internal structure  i refactored multidocvalues.ordinalmap, removing one unused parameter and renaming some methods to more clearly communicate what they do. also i renamed subindex references to segmentindex.",
        "label": 1
    },
    {
        "text": "sortingatomicreader should implement getsortednumericdocvalues as i reviewed sortingatomicreader i noticed it doesn't override getsortednumericdocvalues. i think we should fix this for 4.9?",
        "label": 43
    },
    {
        "text": "support member methods in variablecontext the javascript compiler now supports simple member methods being processed by expression bindings. the variablecontext should also support being able to parse member methods.",
        "label": 41
    },
    {
        "text": "add utility method to analyzer  public final tokenstream tokenstream string fieldname string text  it might be a good idea to remove tons of useless code from tests: most people use tokenstreams and analyzers by only passing a string, wrapped by a stringreader. it would make life easier, if analyzer would have an additional public (and final!!!) method that simply does the wrapping with stringreader by itsself. it might maybe not even needed to throw ioexception (not sure)",
        "label": 53
    },
    {
        "text": "optimize docvalues update datastructures today we are using a linkedhashmap to buffer doc-values updates in bufferedupdates. this on the one hand uses an object based datastructure and on the other requires re-encoding the data into a more compact representation once the bufferedupdates are frozen. this change uses a more compact represenation for the updates already in the bufferedupdates in a parallel-array like datastructure that can be reused in frozenbuffereddeletes. it also adds an much simpler to use api to consume the updates and allows for internal memory optimization for common case updates.",
        "label": 46
    },
    {
        "text": "testspancollection testorquery  failure  missing term field w3 asf & my jenkins found reproducing seeds: from asf jenkins a week ago https://builds.apache.org/job/lucene-solr-nightlytests-master/985/: checking out revision 67f6283ce418357938fc12d82783a3504ba700d7 (refs/remotes/origin/master) [...]  [junit4] suite: org.apache.lucene.search.spans.testspancollection   [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.   [junit4]   2> note: reproduce with: ant test  -dtestcase=testspancollection -dtests.method=testorquery -dtests.seed=994c4307a532054c -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.linedocsfile=/x1/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=fr-ch -dtests.timezone=america/winnipeg -dtests.asserts=true -dtests.file.encoding=us-ascii   [junit4] failure 0.08s j0 | testspancollection.testorquery <<<   [junit4]    > throwable #1: java.lang.assertionerror: missing term field:w3   [junit4]    >  at __randomizedtesting.seedinfo.seed([994c4307a532054c:f928083254607004]:0)   [junit4]    >  at org.apache.lucene.search.spans.testspancollection.checkcollectedterms(testspancollection.java:103)   [junit4]    >  at org.apache.lucene.search.spans.testspancollection.testorquery(testspancollection.java:147)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4]   2> note: test params are: codec=lucene60, sim=randomsimilarity(querynorm=false,coord=crazy): {field=ib spl-dz(0.3)}, locale=fr-ch, timezone=america/winnipeg   [junit4]   2> note: linux 3.13.0-52-generic amd64/oracle corporation 1.8.0_74 (64-bit)/cpus=4,threads=1,free=243932576,total=456654848   [junit4]   2> note: all tests run in this jvm: [testspiclassiterator, testdocvaluesindexing, testtragicindexwriterdeadlock, testtermvectorswriter, testbooleanqueryvisitsubscorers, testfortoomuchcloning, testnewestsegment, testtermsenum, testtransactionrollback, testsearchermanager, testweakidentitymap, testrecyclingbyteblockallocator, testneverdelete, testindexwriterthreadstosegments, testsubscorerfreqs, testdemoparallelleafreader, testqueryrescorer, testsingleinstancelockfactory, testtimsorterworstcase, testpointvalues, testindexwritermaxdocs, testperfieldpostingsformat, testpagedbytes, testparallelcompositereader, testpriorityqueue, testdirectmonotonic, testnumericrangequery64, testsparsefixedbitset, testmmapdirectory, testindexwriterlockrelease, testdeterminizelexicon, testhighcompressionmode, testutf32toutf8, testlucene50compoundformat, testshardsearching, testsynonymquery, testconstantscorequery, testcustomnorms, testautomatonquery, testfixedbitdocidset, testspansenum, testpersegmentdeletes, testreusablestringreader, testregexprandom, testearlytermination, testphrasequery, testindexwritercommit, testsortrandom, testtimelimitingcollector, testmultifields, fuzzytermonshorttermstest, testlsbradixsorter, testnodeletionpolicy, test4gbstoredfields, testsumdocfreq, testmulticollector, testautomaton, testspancollection]   [junit4] completed [192/419 (1!)] on j0 in 0.32s, 3 tests, 1 failure <<< failures! from my jenkins last night: checking out revision d914ec4a67c42466f19e46169754965a7d5af35c (refs/remotes/origin/branch_6x) [...]   [junit4] suite: org.apache.lucene.search.spans.testspancollection   [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.   [junit4]   2> note: reproduce with: ant test  -dtestcase=testspancollection -dtests.method=testorquery -dtests.seed=e0d51ae930685152 -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=sq-al -dtests.timezone=indian/mauritius -dtests.asserts=true -dtests.file.encoding=us-ascii   [junit4] failure 0.01s j1 | testspancollection.testorquery <<<   [junit4]    > throwable #1: java.lang.assertionerror: missing term field:w3   [junit4]    >  at __randomizedtesting.seedinfo.seed([e0d51ae930685152:80b151dcc13a241a]:0)   [junit4]    >  at org.apache.lucene.search.spans.testspancollection.checkcollectedterms(testspancollection.java:103)   [junit4]    >  at org.apache.lucene.search.spans.testspancollection.testorquery(testspancollection.java:147)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4]   2> note: test params are: codec=fastcompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fast, chunksize=6915, maxdocsperchunk=370, blocksize=1), termvectorsformat=compressingtermvectorsformat(compressionmode=fast, chunksize=6915, blocksize=1)), sim=classicsimilarity, locale=sq-al, timezone=indian/mauritius   [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=104831872,total=325058560   [junit4]   2> note: all tests run in this jvm: [testflushbyramorcountspolicy, testdirectoryreader, testsortedsetsortfield, testconjunctions, testrollingupdates, testfrequencytrackingringbuffer, testngramphrasequery, testnumericrangequery64, testsegmentmerger, testbagofpositions, testmultithreadtermvectors, testnumericutils, test2bsorteddocvaluesfixedsorted, testpagedbytes, testdocvaluesindexing, testallfilescheckindexheader, testsmallfloat, testindexwriteroutoffiledescriptors, testspansearchequivalence, testtieredmergepolicy, testsort, testsortedsetdocvalues, testreadonlyindex, testcheckindex, testbytesstore, testutf32toutf8, testindexwriteronvmerror, testlucene50compoundformat, fuzzytermonshorttermstest, teststressindexing, testcharsref, testelevationcomparator, testintsref, testtermvectorsreader, teststringhelper, testmixeddocvaluesupdates, testdeterminizelexicon, testfortoomuchcloning, testforutil, testqueryrescorer, testperfieldpostingsformat, testweakidentitymap, testsearchafter, testbkd, testindexwriterreader, testomitnorms, testfixedbitset, teststressnrt, testnewestsegment, test4gbstoredfields, testspantermquery, testgrowablebytearraydataoutput, testindexreaderclose, testfastdecompressionmode, testtragicindexwriterdeadlock, testbooleanscorer, testcharsrefbuilder, testprefixquery, testpersistentsnapshotdeletionpolicy, testallfileshavechecksumfooter, testparallelleafreader, testindexwriterlockrelease, testindexablefield, testspanorquery, testsimpleexplanationsofnonmatches, testpayloadsonvectors, testfieldtype, testnumericdocvaluesupdates, testdocumentswriterdeletequeue, testsubscorerfreqs, testusagetrackingfiltercachingpolicy, testcharfilter, testtoken, testpostingsoffsets, testlsbradixsorter, testhighcompressionmode, testpositivescoresonlycollector, testindexwriterforcemerge, testreaderclosed, testspansenum, testnrtreadercleanup, testspancollection]   [junit4] completed [181/419 (1!)] on j1 in 0.08s, 3 tests, 1 failure <<< failures!",
        "label": 2
    },
    {
        "text": "incomplete lucene core in maven2 repository i'm new to lucene and am setting up a project using v1.9.1 to use maven2 instead of ant. the project would not build with maven2 due to lacking lucene classes. i tracked the problem down to that the lucene-core-1.9.1 jar file that maven2 downloaded from the repository was smaller (2.3kb) than the one i got from the local ant repository (408kb). can you please update the v1.9.1 file on the maven2 [1], [2] repositories so other developers don't get frustrated by the incomplete jar? [1] http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/1.9.1/ [2] http://mirrors.ibiblio.org/pub/mirrors/maven2/org/apache/lucene/lucene-core/1.9.1/ this issue is a copy of a mail sent to the java-dev@lucene.apache.org list april 4. 2007.",
        "label": 32
    },
    {
        "text": "contrib queries package query implementations do not override equals  query implementations should override equals() so that query instances can be cached and that filters can know if a query has been used before. see the discussion in this thread. http://www.mail-archive.com/java-user@lucene.apache.org/msg13061.html following 3 contrib query implementations do no override equals() org.apache.lucene.search.boostingquery; org.apache.lucene.search.fuzzylikethisquery; org.apache.lucene.search.similar.morelikethisquery; test cases below show the problem. package com.teamware.office.lucene.search; import static org.junit.assert.*; import org.apache.lucene.analysis.standard.standardanalyzer; import org.apache.lucene.index.term; import org.apache.lucene.search.boostingquery; import org.apache.lucene.search.fuzzylikethisquery; import org.apache.lucene.search.termquery; import org.apache.lucene.search.similar.morelikethisquery; import org.junit.after; import org.junit.before; import org.junit.test; public class contribqueriesequalstest { /** @throws java.lang.exception */ @before public void setup() throws exception { } /** @throws java.lang.exception */ @after public void teardown() throws exception { } /** show that the boostingquery in the queries contrib package does not implement equals() correctly. */ @test public void testboostingqueryequals() { termquery q1 = new termquery(new term(\"subject:\", \"java\")); termquery q2 = new termquery(new term(\"subject:\", \"java\")); assertequals(\"two termqueries with same attributes should be equal\", q1, q2); boostingquery bq1 = new boostingquery(q1, q2, 0.1f); boostingquery bq2 = new boostingquery(q1, q2, 0.1f); assertequals(\"boostingquery with same attributes is not equal\", bq1, bq2); } /** show that the morelikethisquery in the queries contrib package does not implement equals() correctly. */ @test public void testmorelikethisqueryequals() { string morelikefields[] = new string[] {\"subject\", \"body\"} ; morelikethisquery mltq1 = new morelikethisquery(\"java\", morelikefields, new standardanalyzer()); morelikethisquery mltq2 = new morelikethisquery(\"java\", morelikefields, new standardanalyzer()); assertequals(\"morelikethisquery with same attributes is not equal\", mltq1, mltq2); } /** show that the fuzzylikethisquery in the queries contrib package does not implement equals() correctly. */ @test public void testfuzzylikethisqueryequals() { fuzzylikethisquery fltq1 = new fuzzylikethisquery(10, new standardanalyzer()); fltq1.addterms(\"javi\", \"subject\", 0.5f, 2); fuzzylikethisquery fltq2 = new fuzzylikethisquery(10, new standardanalyzer()); fltq2.addterms(\"javi\", \"subject\", 0.5f, 2); assertequals(\"fuzzylikethisquery with same attributes is not equal\", fltq1, fltq2); } }",
        "label": 29
    },
    {
        "text": "scorer should not extend postingsenum scorer currently has to implement a whole bunch of methods that are never called. the only method that scorer uses in addition to the methods on docidsetiterator is freq(), and as currently implemented this means different things on different scorers: termscorer returns its underlying termfreq minshouldmatchscorer returns how many of its subscorers are matching {exact|sloppy} phrasescorer returns how many phrases it has found on a document in addition, freq() is never actually called on termscorer, and it's only used in explain() on the phrase scorers. we should make scorer extend docidsetiterator instead. in place of freq(), scorer would have a coord() method that by default returns 1, and for boolean scorers returns how many subscorers are matching.",
        "label": 2
    },
    {
        "text": "contrib  main memory based synonymmap and synonymtokenfilter contrib: main memory based synonymmap and synonymtokenfilter applies to svn trunk as well as 1.4.3",
        "label": 56
    },
    {
        "text": "faceting module faceting is a hugely important feature, available in solr today but not [easily] usable by lucene-only apps. we should fix this, by creating a shared faceting module. ideally, we factor out solr's faceting impl, and maybe poach/merge from other impls (eg bobo browse). hoss describes some important challenges we'll face in doing this (http://markmail.org/message/5w35c2fr4zkiwsz6), copied here: to look at \"faceting\" as a concrete example, there are big the reasons  faceting works so well in solr: solr has total control over the  index, knows exactly when the index has changed to rebuild caches, has a  strict schema so it can make sense of field types and  pick faceting algos accordingly, has multi-phase distributed search  approach to get exact counts efficiently across multiple shards, etc... (and there are still a lot of additional enhancements and improvements  that can be made to take even more advantage of knowledge solr has because  it \"owns\" the index that we no one has had time to tackle) this is a great list of the things we face in refactoring. it's also important because, if solr needed to be so deeply intertwined with caching, schema, etc., other apps that want to facet will have the same \"needs\" and so we really have to address them in creating the shared module. i think we should get a basic faceting module started, but should not cut solr over at first. we should iterate on the module, fold in improvements, etc., and then, once we can fully verify that cutting over doesn't hurt solr (ie lose functionality or performance) we can later cutover.",
        "label": 43
    },
    {
        "text": "make getattribute class attclass  generic org.apache.lucene.util.attributesource current: public attribute getattribute(class attclass) { final attribute att = (attribute) this.attributes.get(attclass); if (att == null) { throw new illegalargumentexception(\"this attributesource does not have the attribute '\" + attclass.getname() + \"'.\"); } return att; } sample usage: termattribute termatt = (termattribute)ts.getattribute(termattribute.class) my improvment: @suppresswarnings(\"unchecked\") public <t> t getattribute2(class<? extends attribute> attclass) { final t att = (t) this.attributes.get(attclass); if (att == null) { throw new illegalargumentexception(\"this attributesource does not have the attribute '\" + attclass.getname() + \"'.\"); } return att; } sample usage: termattribute termatt = ts.getattribute(termattribute.class)",
        "label": 53
    },
    {
        "text": "allow using fst to hold terms data in docvalues bytes sorted ",
        "label": 33
    },
    {
        "text": "method in planetmodel to describe type of ellipsoid hi karl wright, in order to decide which type of circle to use (geostandardcircle or geoexactcircle), users might need to know what kind of planet are dealing with (if planet is a sphere or not). it might have sense to add a method in planetmodel where the planet describes it self. thanks!",
        "label": 25
    },
    {
        "text": "jenkins should randomize  dtests asserts false true spinoff from lucene-6019. it defaults to true, but we should sometimes pass false.",
        "label": 53
    },
    {
        "text": "unexpected terms are highlighted within nested spanquery instances i haven't yet been able to resolve why i'm seeing spurious highlighting in nested spanquery instances. briefly, the issue is illustrated by the second instance of \"lucene\" being highlighted in the test below, when it doesn't satisfy the inner span. there's been some discussion about this on the java-dev list, and i'm opening this issue now because i have made some initial progress on this. this new test, added to the highlightertest class in lucene_2_9_1, illustrates this: /* ref: http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/ */ public void testhighlightingnestedspans2() throws exception { string thetext = \"the lucene was made by doug cutting and lucene great hadoop was\"; // problem //string thetext = \"the lucene was made by doug cutting and the great hadoop was\"; // works okay string fieldname = \"some_field_name\"; spannearquery spannear = new spannearquery(new spanquery[] { new spantermquery(new term(fieldname, \"lucene\")), new spantermquery(new term(fieldname, \"doug\")) } , 5, true); query query = new spannearquery(new spanquery[] { spannear, new spantermquery(new term(fieldname, \"hadoop\")) } , 4, true); string expected = \"the <b>lucene</b> was made by <b>doug</b> cutting and lucene great <b>hadoop</b> was\"; //string expected = \"the <b>lucene</b> was made by <b>doug</b> cutting and the great <b>hadoop</b> was\"; string observed = highlightfield(query, fieldname, thetext); system.out.println(\"expected: \\\"\" + expected + \"\\n\" + \"observed: \\\"\" + observed); assertequals(\"why is that second instance of the term \\\"lucene\\\" highlighted?\", expected, observed); } is this an issue that's arisen before? i've been reading through the source to queryscorer, weightedspanterm, weightedspantermextractor, spans, and nearspansordered, but haven't found the solution yet. initially, i thought that the extractweightedspanterms method in weightedspantermextractor should be called on each clause of a spannearquery or spanorquery, but that didn't get me too far.",
        "label": 10
    },
    {
        "text": "remove priority queue size trap in multitermquery toptermsbooleanqueryrewrite these apis are new in 3.x, so we can do this with no backwards-compatibility issue: before 3.1, fuzzyquery had its own internal rewrite method. we exposed this in 3.x as toptermsbooleanqueryrewrite, and then as subclasses for scoring and boost-only variants. the problem i have is that the pq has a default (large) size of integer.max_value... of course its later limited by the value of booleanquery's maxclausecount, but i think this is a trap. instead its better to simply remove these defaults and force the user to provide a default (reasonable) size.",
        "label": 40
    },
    {
        "text": "allow directory copy  to accept a collection of file names to be copied par example, i want to copy files pertaining to a certain commit, and not everything there is in a directory.",
        "label": 33
    },
    {
        "text": "problem with ngramanalyzer  phrasequery and highlighter using the highlighter with n-gramanalyzer and phrasequery and searching for a substring with length = n yields the following exception: java.lang.illegalargumentexception: less than 2 subspans.size():1 at org.apache.lucene.search.spans.conjunctionspans.<init>(conjunctionspans.java:40) at org.apache.lucene.search.spans.nearspansordered.<init>(nearspansordered.java:56) at org.apache.lucene.search.spans.spannearquery$spannearweight.getspans(spannearquery.java:232) at org.apache.lucene.search.highlight.weightedspantermextractor.extractweightedspanterms(weightedspantermextractor.java:292) at org.apache.lucene.search.highlight.weightedspantermextractor.extract(weightedspantermextractor.java:137) at org.apache.lucene.search.highlight.weightedspantermextractor.getweightedspanterms(weightedspantermextractor.java:506) at org.apache.lucene.search.highlight.queryscorer.initextractor(queryscorer.java:219) at org.apache.lucene.search.highlight.queryscorer.init(queryscorer.java:187) at org.apache.lucene.search.highlight.highlighter.getbesttextfragments(highlighter.java:196) below is a junit-test reproducing this behavior. in case of searching for a string with more than n characters or using ngramphrasequery this problem doesn't occur. why is it that more than 1 subspans are required? public class highlightertest {    @rule    public final expectedexception exception = expectedexception.none();    @test    public void testhighlighterwithphrasequerythrowsexception() throws ioexception, invalidtokenoffsetsexception {        final analyzer analyzer = new ngramanalyzer(4);        final string fieldname = \"substring\";        final list<bytesref> list = new arraylist<>();        list.add(new bytesref(\"uchu\"));        final phrasequery query = new phrasequery(fieldname, list.toarray(new bytesref[list.size()]));        final queryscorer fragmentscorer = new queryscorer(query, fieldname);        final simplehtmlformatter formatter = new simplehtmlformatter(\"<b>\", \"</b>\");        exception.expect(illegalargumentexception.class);        exception.expectmessage(\"less than 2 subspans.size():1\");        final highlighter highlighter = new highlighter(formatter,textencoder.none.getencoder(), fragmentscorer);        highlighter.settextfragmenter(new simplefragmenter(100));        final string fragment = highlighter.getbestfragment(analyzer, fieldname, \"buchung\");        assertequals(\"b<b>uchu</b>ng\",fragment);    } public final class ngramanalyzer extends analyzer {    private final int minngram;    public ngramanalyzer(final int minngram) {        super();        this.minngram = minngram;    }    @override    protected tokenstreamcomponents createcomponents(final string fieldname) {        final tokenizer source = new ngramtokenizer(minngram, minngram);        return new tokenstreamcomponents(source);    } } }",
        "label": 2
    },
    {
        "text": "sort missing string fields last a sortcomparatorsource for string fields that orders documents with the sort field missing after documents with the field. this is the reverse of the default lucene implementation. the concept and first-pass implementation was done by chris hostetter.",
        "label": 18
    },
    {
        "text": "cjk char list seems the character list in the cjk section of the standardtokenizer.jj is not quite complete. following is a more complete list: < cjk: // non-alphabets [ \"\\u1100\"-\"\\u11ff\", \"\\u3040\"-\"\\u30ff\", \"\\u3130\"-\"\\u318f\", \"\\u31f0\"-\"\\u31ff\", \"\\u3300\"-\"\\u337f\", \"\\u3400\"-\"\\u4dbf\", \"\\u4e00\"-\"\\u9fff\", \"\\uac00\"-\"\\ud7a3\", \"\\uf900\"-\"\\ufaff\", \"\\uff65\"-\"\\uffdc\" ] >",
        "label": 38
    },
    {
        "text": "expose version and resource description in corruptindexexception and friends it would be nice to access the minversion, maxversion in indextoonewexception and indextoooldexception as well as the resoruce description in corruptindexexception programmatically. i'd love to use this to support better serialization on top of those exception as well as structured responses from the individual values.",
        "label": 46
    },
    {
        "text": "spanpayloadcheckquery and payloadscorequery are missing rewrite methods if used with a wildcard query, the result is a failure saying: \"rewrite query first\" the spannearquery has the rewrite method; however the spanpayloadcheckquery just returns the query itself. this works: ``` spannear([vectrfield:ebyuugz, spanmultitermquerywrapper(vectrfield:e*), spanmultitermquerywrapper(vectrfield:m*), spanmultitermquerywrapper(vectrfield:f*)], 0, true) ``` code to generate the query: ``` private query getspanquery(string[] parts, int howmany, boolean truncate) throws unsupportedencodingexception { spanquery[] clauses = new spanquery[howmany+1]; clauses[0] = new spantermquery(new term(\"vectrfield\", parts[0])); // surname for (int i = 0; i < howmany; i++) { if (truncate) { spanmultitermquerywrapper<wildcardquery> q = new spanmultitermquerywrapper<wildcardquery>(new wildcardquery(new term(\"vectrfield\", parts[i+1].substring(0, 1) + \"*\"))); clauses[i+1] = q; } else { clauses[i+1] = new spantermquery(new term(\"vectrfield\", parts[i+1])); } } spannearquery sq = new spannearquery(clauses, 0, true); // match in order return sq; } ``` and this fails: ``` spanpaycheck(spannear([vectrfield:ebyuugz, spanmultitermquerywrapper(vectrfield:e*), spanmultitermquerywrapper(vectrfield:m*), spanmultitermquerywrapper(vectrfield:f*)], 1, true), payloadref: 0;1;2;3 ``` each clause is made of: ``` new spanmultitermquerywrapper<wildcardquery>(new wildcardquery(new term(\"vectrfield\", parts[i+1].substring(0, 1) + \"*\"))); ``` it is a regression; the code was working well in solr4.x",
        "label": 14
    },
    {
        "text": "tokensources gettokenstream  doesn't return correctly for termvectors with positions but no offsets the javadocs for tokensources.gettokenstream(terms, boolean) state: \"low level api. returns a token stream or null if no offset info available in index. this can be used to feed the highlighter with a pre-parsed token stream\" however, if the terms instance passed in has positions but no offsets stored, a tokenstream is incorrectly returned, rather than null. this has the effect of incorrectly highlighting fields with term vectors and positions, but no offsets. all highlighting markup is prepended to the beginning of the field.",
        "label": 2
    },
    {
        "text": "shinglefilter improvements shinglefilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams. the token separator used in composing shingles should be configurable too.",
        "label": 40
    },
    {
        "text": "kuromojitokenizer fails with large docs just shoving largeish random docs triggers asserts like:     [junit] caused by: java.lang.assertionerror: backpos=4100 vs lastbacktracepos=5120     [junit]  at org.apache.lucene.analysis.kuromoji.kuromojitokenizer.backtrace(kuromojitokenizer.java:907)     [junit]  at org.apache.lucene.analysis.kuromoji.kuromojitokenizer.parse(kuromojitokenizer.java:756)     [junit]  at org.apache.lucene.analysis.kuromoji.kuromojitokenizer.incrementtoken(kuromojitokenizer.java:403)     [junit]  at org.apache.lucene.analysis.basetokenstreamtestcase.checkrandomdata(basetokenstreamtestcase.java:404) but, you get no seed... i'll commit the test case and @ignore it.",
        "label": 8
    },
    {
        "text": "parallelmultisearcher should shut down thread pool on close parallelmultisearcher does not shut down its internal thread pool on close. as a result, programs that create multiple instances of this class over their lifetime end up \"leaking\" threads.",
        "label": 53
    },
    {
        "text": "merge docsenum and docsandpositionsenum into postingsenum spinnoff from http://www.gossamer-threads.com/lists/lucene/java-dev/172261 hey folks,  i have spend a hell lot of time on the positions branch to make  positions and offsets working on all queries if needed. the one thing  that bugged me the most is the distinction between docsenum and  docsandpositionsenum. really when you look at it closer docsenum is a  docsandfreqsenum and if we omit freqs we should return a docidsetiter.  same is true for  docsandpostionsandpayloadsandoffsets*yourfancyfeaturehere*enum. i  don't really see the benefits from this. we should rather make the  interface simple and call it something like postingsenum where you  have to specify flags on the termsiterator and if we can't provide the  sufficient enum we throw an exception?  i just want to bring up the idea here since it might simplify a lot  for users as well for us when improving our positions / offset etc.  support.  thoughts? ideas?  simon ",
        "label": 2
    },
    {
        "text": "split monster tests in test2bsorteddocvalues out into their own suites so that they can be run in parallel the two monster tests in test2bsorteddocvalues each take a long time to run, e.g. from http://jenkins.sarowe.net/job/lucene-core-nightly-monster-trunk/23/consoletext:    [junit4] heartbeat j0 pid(29118@localhost): 2015-05-31t07:41:18, stalled for 5231s at: test2bsorteddocvalues.test2bords [...]    [junit4] heartbeat j0 pid(29118@localhost): 2015-05-31t08:04:18, stalled for 1329s at: test2bsorteddocvalues.testfixedsorted if each of these tests were in its own suite, then when run with multiple jvms, they could be run in parallel rather than serially. when i do this locally using 4 jvms, the lucene core nightly+monster tests complete about 20-25 minutes faster (~95 minutes vs. ~120 minutes).",
        "label": 47
    },
    {
        "text": "clean up serialization in the codebase we removed contrib/remote, but forgot to cleanup serialization hell everywhere. this is no longer needed, never really worked (e.g. across versions), and slows development (e.g. i wasted a long time debugging stupid serialization of similarity.idfexplain when trying to make a patch for the scoring system).",
        "label": 40
    },
    {
        "text": "fix or deprecate termsenum skipto this method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the termsenum base class since none of the concrete impls override it with a more efficient implementation). the least we should do for 2.9 is deprecate the method with a strong warning about its performance. see here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup and, here for historical context: http://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target",
        "label": 53
    },
    {
        "text": "fix changes2html to link pull requests if someone submits a pull request, i think we should put it in changes.txt in some way similar to the jira issues: e.g. for a jira issue we do: * lucene-xxxx: add foobar.  (joe contributor via john committer) changes2html recognizes and expands these to jira issue links. so i think we should be able to do something like: * pull request #xxx: add foobar. (joe contributor via john committer) and have it link to the request, too.",
        "label": 47
    },
    {
        "text": "further parallelizaton of parallelmultisearcher when calling search(query, filter, int) on a parallelmultisearcher, the createweights function of multisearcher is called, and sequentially calls docfreqs() on every sub-searcher. this can take a significant amount of time when there are lots of remote sub-searchers.",
        "label": 53
    },
    {
        "text": "testcheckindex testchecksumsonlyverbose  failure  document contains at least one immense term my jenkins found the following seed that reproduces for me 100% on branch_5x, both java7&8, and on trunk:    [junit4] suite: org.apache.lucene.index.testcheckindex    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testcheckindex -dtests.method=testchecksumsonlyverbose -dtests.seed=1b39bc3f6e1634f -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=hu -dtests.timezone=america/havana -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   0.22s | testcheckindex.testchecksumsonlyverbose <<<    [junit4]    > throwable #1: java.lang.illegalargumentexception: document contains at least one immense term in field=\"body\" (whose utf8 encoding is longer than the max length 32766), all of which were skipped.  please correct the analyzer to not produce such terms.  the prefix of the first immense term is: '[125, 125, 123, 123, 123, 123, 123, 115, 117, 98, 115, 116, 99, 124, 125, 125, 125, 123, 123, 123, 49, 125, 125, 125, 124, 123, 123, 123, 112, 49]...', original message: bytes can be at most 32766 in length; got 94384    [junit4]    >        at __randomizedtesting.seedinfo.seed([1b39bc3f6e1634f:c5ff0bd55ae404ad]:0)    [junit4]    >        at org.apache.lucene.index.defaultindexingchain$perfield.invert(defaultindexingchain.java:726)    [junit4]    >        at org.apache.lucene.index.defaultindexingchain.processdocument(defaultindexingchain.java:347)    [junit4]    >        at org.apache.lucene.index.documentswriterperthread.updatedocument(documentswriterperthread.java:234)    [junit4]    >        at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:449)    [junit4]    >        at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1461)    [junit4]    >        at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1240)    [junit4]    >        at org.apache.lucene.index.testcheckindex.testchecksumsonlyverbose(testcheckindex.java:156)    [junit4]    >        at java.lang.thread.run(thread.java:745)    [junit4]    >        suppressed: java.lang.illegalstateexception: close() called in wrong state: increment    [junit4]    >                at org.apache.lucene.analysis.mocktokenizer.fail(mocktokenizer.java:126)    [junit4]    >                at org.apache.lucene.analysis.mocktokenizer.close(mocktokenizer.java:293)    [junit4]    >                at org.apache.lucene.analysis.tokenfilter.close(tokenfilter.java:58)    [junit4]    >                at org.apache.lucene.index.defaultindexingchain$perfield.invert(defaultindexingchain.java:742)    [junit4]    >                ... 42 more    [junit4]    > caused by: org.apache.lucene.util.bytesrefhash$maxbyteslengthexceededexception: bytes can be at most 32766 in length; got 94384    [junit4]    >        at org.apache.lucene.util.bytesrefhash.add(bytesrefhash.java:284)    [junit4]    >        at org.apache.lucene.index.termshashperfield.add(termshashperfield.java:150)    [junit4]    >        at org.apache.lucene.index.defaultindexingchain$perfield.invert(defaultindexingchain.java:716)    [junit4]    >        ... 42 more    [junit4]   2> note: test params are: codec=asserting(lucene54): {}, docvalues:{}, sim=classicsimilarity, locale=hu, timezone=america/havana    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=412272632,total=514850816    [junit4]   2> note: all tests run in this jvm: [testcheckindex]    [junit4] completed [1/1] in 0.45s, 1 test, 1 error <<< failures!",
        "label": 33
    },
    {
        "text": "should bkd cells store their min max packed values  the index of the bkd tree already allows to know lower and upper bounds of values in a given dimension. however the actual range of values might be more narrow than what the index tells us, especially if splitting on one dimension reduces the range of values in at least one other dimension. for instance this tends to be the case with range fields: since we enforce that lower bounds are less than upper bounds, splitting on one dimension will also affect the range of values in the other dimension. so i'm wondering whether we should store the actual range of values for each dimension in leaf blocks, this will hopefully allow to figure out that either none or all values match in a block without having to check them all.",
        "label": 19
    },
    {
        "text": "add instantiatedindexwriter addindexes indexreader  readers  enable instantiatedindexwriter to have indexreaders passed in like indexwriter and merged into the index. karl mentioned: bq: it's doable. the simplest solution i can think of is to reconstruct all the documents in one single enumeration of the source index and then add them to the writer. i'm however not certain this is the best way nor if instantiatedindexwriter is the place for the code. how would the documents be reconstructed without creating a lot of overhead? it seems like instantiatedindexwriter is the right place, given it is presumably more efficient to recreate all the indexreaders and then commit?",
        "label": 24
    },
    {
        "text": "support unicode now that unicode 6.1.0 has been released, lucene/solr should support it. jflex trunk now supports unicode 6.1.0. tasks include: upgrade icu4j to v49 (after it's released, on 2012-03-21, according to http://icu-project.org). use icu module tools to regenerate the supplementary character additions to jflex grammars. version the jflex grammars: copy the current implementations to *impl3<x>; cause the versioning tokenizer wrappers to instantiate this version when the version c-tor param is in the range 3.1 to the version in which these changes are released (excluding the range endpoints); then change the specified unicode version in the non-versioned jflex grammars from 6.0 to 6.1. regenerate jflex scanners, including standardtokenizerimpl, uax29urlemailtokenizerimpl, and htmlstripcharfilter. using generatejavaunicodewordbreaktest.pl, generate and then run wordbreaktestunicode_6_1_0.java under modules/analysis/common/src/test/org/apache/lucene/analysis/core/",
        "label": 47
    },
    {
        "text": "fuzzylikethisquery should set maxnoncompetitiveboost for faster speed fuzzylikethisquery uses fuzzytermsenum directly, and maintains a priority queue for its purposes. just like toptermsrewrite method, it should set the maxnoncompetitiveboost attribute, so that fuzzytermsenum can run faster. its already tracking the minscore, just not updating the attribute. this would be especially nice as it appears to have nice defaults already (pq size of 50)",
        "label": 40
    },
    {
        "text": "pattern token filter which emits a token for every capturing group the patterntokenizer either functions by splitting on matches, or allows you to specify a single capture group. this is insufficient for my needs. quite often i want to capture multiple overlapping tokens in the same position. i've written a pattern token filter which accepts multiple patterns and emits tokens for every capturing group that is matched in any pattern. patterns are not anchored to the beginning and end of the string, so each pattern can produce multiple matches. for instance a pattern like :     \"(([a-z]+)(\\d*))\" when matched against:     \"abc123def456\" would produce the tokens:     abc123, abc, 123, def456, def, 456 multiple patterns can be applied, eg these patterns could be used for camelcase analysis:     \"([a-z]{2,})\",     \"(?<![a-z])([a-z][a-z]+)\",     \"(?:^|\\\\b|(?<=[0-9_])|(?<=[a-z]{2}))([a-z]+)\",     \"([0-9]+)\" when matched against the string \"letspartylikeits1999_dude\", they would produce the tokens:     lets, party, like, its, 1999, dude if no token is emitted, the original token is preserved. if the preserveoriginal flag is true, it will output the full original token (ie \"letspartylikeits1999_dude\") in addition to any matching tokens (but in this case, if a matching token is identical to the original, it will only emit one copy of the full token). multiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand. this is my first java code, so apologies if i'm doing something stupid.",
        "label": 46
    },
    {
        "text": "add queryparser newfieldquery note: this patch changes no behavior, just makes qp more subclassable. currently we have query getfieldquery(string field, string querytext, boolean quoted) this contains very hairy methods for producing a query from qp's analyzer. i propose we factor this into newfieldquery(analyzer analyzer, string field, string querytext, boolean quoted) then getfieldquery just calls newfieldquery(this.analyzer, field, querytext, quoted); the reasoning is: it can be quite useful to consider the double quote as more than phrases, but a \"more exact\" search. in the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that: doesn't produce synonyms, doesnt decompose compounds, doesn't use worddelimiterfilter (you would need to be using preserveoriginal=true at index time for the wdf one), etc etc. this is similar to the way google's double quote operator works, its not defined as phrase but \"this exact wording or phrase\". for example compare results to a query of tests versus \"tests\". currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful), and make your custom qp lie about its field... in the examples i listed above you can do this with a single field, yet still have a more exact phrase search.",
        "label": 40
    },
    {
        "text": "check legal lucene always checks contrib queries lib i've been noticing for awhile that the check-legal-lucene always checks /contrib/queries/lib, no matter where it is. consequently it never finds the directory. this seems like a waste in our build and for the life of me i have no idea why it is necessary. offending line is: <arg value=\"${basedir}/contrib/queries/lib\" /> in check-legal-lucene patch will remove this.",
        "label": 7
    },
    {
        "text": "add norms patched compression which uses table for most common values we have added the patched norms sub format in lucene 50, which uses a bitset to mark documents that have the most common value (when >97% of the documents have that value). this works well for fields that have a predominant value length, and then a small number of docs with some other random values. but another common case is having a handful of very common value lengths, like with a title field. we can use a table (see table_compression) to store the most common values, and save an oridinal for the \"other\" case, at which point we can lookup in the secondary patch table.",
        "label": 41
    },
    {
        "text": "maven build fails for target javadoc jar we currently disable missing checks for doclint, but the maven poms don't have it, as a result javadoc:jar fails. (thanks to daniel collins for spotting this)",
        "label": 47
    },
    {
        "text": "search vs explain   score discrepancies i'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc.",
        "label": 18
    },
    {
        "text": "make frozenbuffereddeletes more efficient for terms when looking at lucene-3340, i thought its also ridiculous how much ram we use for delete by term. so we can save a lot of memory, especially object overhead by being a little more efficient.",
        "label": 40
    },
    {
        "text": "detect when index is on ssd and set dynamic defaults e.g. concurrentmergescheduler should default maxmergethreads to 3 if it's on ssd and 1 if it's on spinning disks. i think the new nio2 apis can let us figure out which device we are mounted on, and from there maybe we can do os-specific stuff e.g. look at /sys/block/dev/queue/rotational to see if it's spinning storage or not ...",
        "label": 33
    },
    {
        "text": "unifiedhighlighter  add requirefieldmatch false support the unifiedhighlighter (like the postingshighlighter) only supports highlighting queries for the same fields that are being highlighted. the original highlighter and fvh support loosening this, aka requirefieldmatch=false.",
        "label": 10
    },
    {
        "text": "filtereddocidset does not handle a case where the inner set iterator is null docidset#iterator is allowed to return null, when used in filtereddocidset, if null is returned from the inner set, the filtereddocidsetiterator fails since it does not allow for nulls to be passed to it. the fix is simple, return null in filtereddocidset in the iterator method is the iterator is null.",
        "label": 53
    },
    {
        "text": "field specified norms in matchalldocumentsscorer this patch allows for optionally setting a field to use for norms factoring when scoring a matchingalldocumentsquery. from the test case: .     ramdirectory dir = new ramdirectory();     indexwriter iw = new indexwriter(dir, new standardanalyzer(), true, indexwriter.maxfieldlength.limited);     iw.setmaxbuffereddocs(2);  // force multi-segment     adddoc(\"one\", iw, 1f);     adddoc(\"two\", iw, 20f);     adddoc(\"three four\", iw, 300f);     iw.close();     indexreader ir = indexreader.open(dir);     indexsearcher is = new indexsearcher(ir);     scoredoc[] hits;     // assert with norms scoring turned off     hits = is.search(new matchalldocsquery(), null, 1000).scoredocs;     assertequals(3, hits.length);     assertequals(\"one\", ir.document(hits[0].doc).get(\"key\"));     assertequals(\"two\", ir.document(hits[1].doc).get(\"key\"));     assertequals(\"three four\", ir.document(hits[2].doc).get(\"key\"));     // assert with norms scoring turned on     matchalldocsquery normsquery = new matchalldocsquery(\"key\");     assertequals(3, hits.length); //    is.explain(normsquery, hits[0].doc);     hits = is.search(normsquery, null, 1000).scoredocs;     assertequals(\"three four\", ir.document(hits[0].doc).get(\"key\"));         assertequals(\"two\", ir.document(hits[1].doc).get(\"key\"));     assertequals(\"one\", ir.document(hits[2].doc).get(\"key\"));",
        "label": 33
    },
    {
        "text": "adding empty parallelreader indexes to an indexwriter may cause arrayindexoutofboundsexception or nosuchelementexception hi, i recently stumbled upon this: it is possible (and perfectly legal) to add empty indexes (indexreaders) to an indexwriter. however, when using parallelreaders in this context, in two situations runtimeexceptions may occur for no good reason. condition 1: the indexes within the parallelreader are just empty. when adding them to the indexwriter, we get a java.util.nosuchelementexception triggered by paralleltermenum's constructor. the reason for that is the treemap#firstkey() method which was assumed to return null if there is no entry (which is not true, apparently \u2013 it only returns null if the first key in the map is null). condition 2 (assuming the aforementioned bug is fixed): the indexes within the parallelreader originally contained one or more fields with termvectors, but all documents have been marked as deleted. when adding the indexes to the indexwriter, we get a java.lang.arrayindexoutofboundsexception triggered by termvectorswriter#addalldocvectors. the reason here is that termvectorswriter assumes that if the index is marked to have termvectors, at least one field actually exists for that. this unfortunately is not true, either. patches and a testcase demonstrating the two bugs are provided. cheers, christian",
        "label": 33
    },
    {
        "text": "add generics to documentswriterdeletequeue node documentswriterdeletequeue.note should be generic as the subclasses hold different types of items. this generification is a little bit tricks, but the generics policeman can't wait to fix this g.",
        "label": 53
    },
    {
        "text": "allow cfs be empty since we changed cfs semantics slightly closing a cfs directory on an error can lead to an exception. yet, an empty cfs is still a valid cfs so for consistency we should allow cfs to be empty. here is an example: 1 tests failed. regression:  org.apache.lucene.index.testindexwriterondiskfull.testadddocumentondiskfull error message: cfs has no entries stack trace: java.lang.illegalstateexception: cfs has no entries        at org.apache.lucene.store.compoundfilewriter.close(compoundfilewriter.java:139)        at org.apache.lucene.store.compoundfiledirectory.close(compoundfiledirectory.java:181)        at org.apache.lucene.store.defaultcompoundfiledirectory.close(defaultcompoundfiledirectory.java:58)        at org.apache.lucene.index.segmentmerger.createcompoundfile(segmentmerger.java:139)        at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4252)        at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3863)        at org.apache.lucene.index.serialmergescheduler.merge(serialmergescheduler.java:37)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:2715)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:2710)        at org.apache.lucene.index.indexwriter.maybemerge(indexwriter.java:2706)        at org.apache.lucene.index.indexwriter.flush(indexwriter.java:3513)        at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:2064)        at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:2031)        at org.apache.lucene.index.testindexwriterondiskfull.adddoc(testindexwriterondiskfull.java:539)        at org.apache.lucene.index.testindexwriterondiskfull.testadddocumentondiskfull(testindexwriterondiskfull.java:74)        at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1277)        at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1195)",
        "label": 46
    },
    {
        "text": "in indexsearcher class  make subreader and doccount arrays protected so sub classes can access them please make these two member variables protected so subclasses can access them, e.g.: protected indexreader[] subreaders; protected int[] docstarts; thanks",
        "label": 33
    },
    {
        "text": "remove benchmark lib xml apis jar   jvm already contains the required jaxp implementation on lucene-2957, uwe wrote: xml-apis.jar is not needed with xerces-2.9 and java 5, as java 5 already has these interface classes (jaxp 1.3). xerces 2.11 needs it, because it ships with java 6's jaxp release (containing stax & co. not available in java 5). on the #lucene irc channel, uwe also wrote: since we are on java 5 since 3.0 we have the javax apis already available in the jvm xerces until 2.9.x only needs jaxp 1.3 so the only thing you need is xercesimpl.jar and serializer.jar serializer.jar is shared between all apache xml projects, dont know the exact version number ok you dont need it whan you only parse xml as soon as you want to serialize a dom tree or result of an xsl transformation you need it [...] but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6 so the one shipped with xerces 2.11 is the 1.4 one because xerces 2.11 supports stax",
        "label": 47
    },
    {
        "text": "boosting by geo distance similarly to lucene-8340 it would be nice to have an easy and efficient way to fold geo distance into scoring formulas in order to boost by proximity.",
        "label": 19
    },
    {
        "text": "use float floattorawintbits over float floattointbits copied from my email: float.floattorawintbits (in java1.4) gives the raw float bits without normalization (like (int)&floatvar would in c). since it doesn't do normalization of nan values, it's faster (and hopefully optimized to a simple inline machine instruction by the jvm). on my pentium4, using floattorawintbits is over 5 times as fast as floattointbits. that can really add up in something like similarity.floattobyte() for encoding norms, especially if used as a way to compress an array of float during query time as suggested by doug.",
        "label": 55
    },
    {
        "text": "trapping overloaded ctors setters in field numericfield docvaluesfield in trunk, these apis let you easily create a field, but my concern is this: public numericfield(string name, int value) public numericfield(string name, long value) .. public field(string name, int value, fieldtype type) public field(string name, long value, fieldtype type) .. public void setvalue(int value) public void setvalue(long value) .. public docvaluesfield(string name, int value, docvalues.type docvaluetype) public docvaluesfield(string name, long value, docvalues.type docvaluetype) i really don't like overloaded ctors/setters where the compiler can type-promote you, i think it makes the apis hard to use. instead for the setters i think we sohuld have setintvalue, setlongvalue, ... for the ctors, i see two other options: factories like docvaluesfield.newintfield() subclasses like intfield i don't have any patch for this, but i think we should discuss and fix before these apis are released.",
        "label": 33
    },
    {
        "text": "spatial  enhance rpt to differentiate confirmed from non confirmed hits  then validate with sdv if a cell is within the query shape (doesn't straddle the edge), then you can be sure that all documents it matches are a confirmed hit. but if some documents are only on the edge cells, then those documents could be validated against serializeddvstrategy for precise spatial search. this should be much faster than using rpt and serializeddvstrategy independently on the same search, particularly when a lot of documents match. perhaps this'll be a new rpt subclass, or maybe an optional configuration of rpt. this issue is just for the intersects predicate, which will apply to disjoint. until resolved in other issues, the other predicates can be handled in a naive/slow way by creating a filter that combines rpt's filter and serializeddvstrategy's filter using bitsfiltereddocidset. one thing i'm not sure of is how to expose to lucene-spatial users the underlying functionality such that they can put other query/filters in-between rpt and the serializeddvstrategy. maybe that'll be done by simply ensuring the predicate filters have this capability and are public. it would be ideal to implement this capability after the prefixtree term encoding is modified to differentiate edge leaf-cells from non-edge leaf cells. this distinction will allow the code here to make more confirmed matches.",
        "label": 10
    },
    {
        "text": " patch  nullpointerexception when using nested spanorquery in spannotquery overview description: i'm using the span query classes in lucene to generate higher scores for search results where the search terms are closer together. in certain situations i want to exclude terms from the span. when i attempt to exclude more than one term i get an error. the example query i'm using is: 'brighton and tourism' -pier -contents i construct the query objects and the tostring() version is: spannot(spannear([contents:brighton contents:tourism], 10, false), spanor([contents:pier, contents:road])) steps to reproduce: 1. construct a spannearquery (must have at least one term, but at least two makes more sense) 2. construct a spanorquery containing two or more terms 3. construct a spannotquery to include the first query object and exclude the second (spanorquery) 4. execute the search actual results: a null pointer exception is thrown while generating the scores within the search. stack trace: java.lang.nullpointerexception at org.apache.lucene.search.spans.spanorquery$1.doc(spanorquery.java:174) at org.apache.lucene.search.spans.spannotquery$1.next(spannotquery.java:75) at org.apache.lucene.search.spans.spanscorer.next(spanscorer.java:50) at org.apache.lucene.search.scorer.score(scorer.java:37) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:92) at org.apache.lucene.search.hits.getmoredocs(hits.java:64) at org.apache.lucene.search.hits.<init>(hits.java:43) at org.apache.lucene.search.searcher.search(searcher.java:33) at org.apache.lucene.search.searcher.search(searcher.java:27) at com.runtimecollective.search.lucenesearch.search(lucenesearch.java:362) expected resuts: it executes the search and results where the first search terms (near query) are close together but without the second terms (or query) appearing.",
        "label": 18
    },
    {
        "text": "directoryreader openifchanged oldreader  commit  incorrectly assumes given commit point has deletes field updates standarddirectoryreader assumes that the segments from commit point have deletes, when they may not, yet the original segmentreader for the segment that we are trying to reuse does. this is evident when running attached junit test case with asserts enabled (default): java.lang.assertionerror  at org.apache.lucene.index.standarddirectoryreader.open(standarddirectoryreader.java:188)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:326)  at org.apache.lucene.index.standarddirectoryreader$2.dobody(standarddirectoryreader.java:320)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:702)  at org.apache.lucene.index.standarddirectoryreader.doopenfromcommit(standarddirectoryreader.java:315)  at org.apache.lucene.index.standarddirectoryreader.doopennowriter(standarddirectoryreader.java:311)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:262)  at org.apache.lucene.index.directoryreader.openifchanged(directoryreader.java:183) or, if asserts are disabled then it falls through into npe: java.lang.nullpointerexception  at java.io.file.<init>(file.java:305)  at org.apache.lucene.store.niofsdirectory.openinput(niofsdirectory.java:80)  at org.apache.lucene.codecs.lucene40.bitvector.<init>(bitvector.java:327)  at org.apache.lucene.codecs.lucene40.lucene40livedocsformat.readlivedocs(lucene40livedocsformat.java:90)  at org.apache.lucene.index.segmentreader.<init>(segmentreader.java:131)  at org.apache.lucene.index.standarddirectoryreader.open(standarddirectoryreader.java:194)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:326)  at org.apache.lucene.index.standarddirectoryreader$2.dobody(standarddirectoryreader.java:320)  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:702)  at org.apache.lucene.index.standarddirectoryreader.doopenfromcommit(standarddirectoryreader.java:315)  at org.apache.lucene.index.standarddirectoryreader.doopennowriter(standarddirectoryreader.java:311)  at org.apache.lucene.index.standarddirectoryreader.doopenifchanged(standarddirectoryreader.java:262)  at org.apache.lucene.index.directoryreader.openifchanged(directoryreader.java:183)",
        "label": 33
    },
    {
        "text": "apply graph articulation points optimization to phrase graph queries follow-up to lucene-7638 that applies the same articulation point logic to graph phrases using span queries.",
        "label": 22
    },
    {
        "text": "grouped total count when grouping currently you can get two counts: total hit count. which counts all documents that matched the query. total grouped hit count. which counts all documents that have been grouped in the top n groups. since the end user gets groups in his search result instead of plain documents with grouping. the total number of groups as total count makes more sense in many situations.",
        "label": 33
    },
    {
        "text": "lazy field loading has edge case bug causing read past eof while trying to run some benchmarking of lazy filed loading, i discovered there seems to be an edge case when accessing the last field of the last doc of an index. the problem seems to only happen when the doc has been accessed after at least one other doc. i have not tried to dig into the code to find the root cause, testcase to follow...",
        "label": 55
    },
    {
        "text": "benchmark contrib should allow multiple locations in ext classpath when ant run-task is invoked with the -dbenchmark.ext.classpath=... option, only a single location may be specified. if a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked jvm.",
        "label": 33
    },
    {
        "text": "fvh  uncontrollable color tags the multi-colored tags is a feature of fvh. but it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms.",
        "label": 26
    },
    {
        "text": "disjunctionmaxscorer initializes scoremax to zero preventing from using negative scores we are using a log of probability for scoring, which gives us negative scores. disjunctionmaxscorer initializes scoremax in the score(...) function to zero preventing us from using negative scores. is there a reason it couldn't be initialized to something like this: float scoremax = float.max_value * -1;",
        "label": 53
    },
    {
        "text": "if a filter can support random access api  we should use it i ran some performance tests, comparing applying a filter via random-access api instead of current trunk's iterator api. this was inspired by lucene-1476, where we realized deletions should really be implemented just like a filter, but then in testing found that switching deletions to iterator was a very sizable performance hit. some notes on the test: index is first 2m docs of wikipedia. test machine is mac os x 10.5.6, quad core intel cpu, 6 gb ram, java 1.6.0_07-b06-153. i test across multiple queries. 1-x means an or query, eg 1-4 means 1 or 2 or 3 or 4, whereas +1-4 is an and query, ie 1 and 2 and 3 and 4. \"u s\" means \"united states\" (phrase search). i test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90, 95, 98, 99, 99.99999 (filter is non-null but all bits are set), 100 (filter=null, control)). method high means i use random-access filter api in indexsearcher's main loop. method low means i use random-access filter api down in segmenttermdocs (just like deleted docs today). baseline (qps) is current trunk, where filter is applied as iterator up \"high\" (ie in indexsearcher's search loop).",
        "label": 53
    },
    {
        "text": "pointrangequery equals does not work here is a simple unit test that fails:   public void testpointrangeequals() {     query q1 = longpoint.newrangequery(\"a\", 0, 1000);     query q2 = longpoint.newrangequery(\"a\", 0, 1000);     assertequals(q1, q2);   }",
        "label": 33
    },
    {
        "text": "auto warming from apache solr causes null pointer sep 6, 2009 12:48:07 pm org.apache.solr.common.solrexception log severe: error during auto-warming of key:org.apache.solr.search.queryresultkey@b00371eb:java.lang.nullpointerexception at org.apache.lucene.spatial.tier.distancefieldcomparatorsource$distancescoredoclookupcomparator.copy(distancefieldcomparatorsource.java:101) at org.apache.lucene.search.topfieldcollector$multicomparatorscoringmaxscorecollector.collect(topfieldcollector.java:554) at org.apache.solr.search.docsetdelegatecollector.collect(docsethitcollector.java:98) at org.apache.lucene.search.indexsearcher.dosearch(indexsearcher.java:281) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:253) at org.apache.lucene.search.searcher.search(searcher.java:171) at org.apache.solr.search.solrindexsearcher.getdoclistandsetnc(solrindexsearcher.java:1088) at org.apache.solr.search.solrindexsearcher.getdoclistc(solrindexsearcher.java:876) at org.apache.solr.search.solrindexsearcher.access$000(solrindexsearcher.java:53) at org.apache.solr.search.solrindexsearcher$3.regenerateitem(solrindexsearcher.java:328) at org.apache.solr.search.lrucache.warm(lrucache.java:194) at org.apache.solr.search.solrindexsearcher.warm(solrindexsearcher.java:1468) at org.apache.solr.core.solrcore$3.call(solrcore.java:1142) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:885) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907) at java.lang.thread.run(thread.java:619)",
        "label": 16
    },
    {
        "text": "npe in nearspansunordered from payloadnearquery the following query causes a npe in nearspansunordered, and is reproducible with the the attached unit test. the failure occurs on the last document scored.",
        "label": 33
    },
    {
        "text": "geo3d circle creation that covers whole globe throws an illegalargumentexception the following geocircle construction: new geocircle(planetmodel.sphere, -20.0 * radians_per_degree, -20.0 * radians_per_degree, math.pi); ... fails as follows: degenerate/parallel vector constructed the reason is that the plane normal vector cannot be computed in that case. a special case is warranted for circles that cover the whole globe.",
        "label": 10
    },
    {
        "text": "smartcn hhmm doc translation my coworker patricia peng translated the documentation and code comments for smartcn hhmm package.",
        "label": 40
    },
    {
        "text": "escaped quotes inside a phrase cause a parseexception queryparser cannot handle escaped quotes when inside a phrase. escaped quotes not in a phrase are not a problem. this can be added to testqueryparser.testescaped() to demonstrate the issue - the second assert throws an exception: assertqueryequals(\"a \\\\\\\"b c\\\\\\\" d\", a, \"a \\\"b c\\\" d\"); assertqueryequals(\"\\\"a \\\\\\\"b c\\\\\\\" d\\\"\", a, \"\\\"a \\\"b c\\\" d\\\"\"); see also this thread: http://www.nabble.com/parseexception-with-escaped-quotes-in-a-phrase-t1647115.html",
        "label": 32
    },
    {
        "text": " patch  terminfosreader  segmenttermenum out of memory exception we've been experiencing terrible memory problems on our production search server, running lucene (1.4.3). our live app regularly opens new indexes and, in doing so, releases old indexreaders for garbage collection. but...there appears to be a memory leak in org.apache.lucene.index.terminfosreader.java. under certain conditions (possibly related to jvm version, although i've personally observed it under both linux jvm 1.4.2_06, and 1.5.0_03, and sunos jvm 1.4.1) the threadlocal member variable, \"enumerators\" doesn't get garbage-collected when the terminfosreader object is gc-ed. looking at the code in terminfosreader.java, there's no reason why it shouldn't be gc-ed, so i can only presume (and i've seen this suggested elsewhere) that there could be a bug in the garbage collector of some jvms. i've seen this problem briefly discussed; in particular at the following url: http://java2.5341.com/msg/85821.html the patch that doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. doug's patch only clears the threadlocal variable for the thread running the finalizer (my knowledge of java breaks down here - i'm not sure which thread actually runs the finalizer). in our situation, the terminfosreader is (potentially) used by more than one thread, meaning that doug's patch doesn't allow the affected jvms to correctly collect garbage. so...i've devised a simple patch which, from my observations on linux jvms 1.4.2_06, and 1.5.0_03, fixes this problem. kieran ps thanks to daniel naber for pointing me to jira/lucene @@ -19,6 +19,7 @@ import java.io.ioexception; import org.apache.lucene.store.directory; +import java.util.hashtable; /** this stores a monotonically increasing set of <term, terminfo> pairs in a directory. pairs are accessed either by term or by ordinal position the @@ -29,7 +30,7 @@ private string segment; private fieldinfos fieldinfos; private threadlocal enumerators = new threadlocal(); + private final hashtable enumeratorsbythread = new hashtable(); private segmenttermenum origenum; private long size; @@ -60,10 +61,10 @@ } private segmenttermenum getenum() { segmenttermenum termenum = (segmenttermenum)enumerators.get(); + segmenttermenum termenum = (segmenttermenum)enumeratorsbythread.get(thread.currentthread()); if (termenum == null) { termenum = terms(); - enumerators.set(termenum); + enumeratorsbythread.put(thread.currentthread(), termenum); } return termenum; } @@ -195,5 +196,15 @@ public segmenttermenum terms(term term) throws ioexception { get(term); return (segmenttermenum)getenum().clone(); + } + + /* some jvms might have trouble gc-ing enumeratorsbythread */ + protected void finalize() throws throwable unknown macro: {+ try { + // make sure gc can clear up. + enumeratorsbythread.clear(); + } finally { + super.finalize(); + } } } terminfosreader.java, full source: ====================================== package org.apache.lucene.index; /** copyright 2004 the apache software foundation * licensed under the apache license, version 2.0 (the \"license\"); you may not use this file except in compliance with the license. you may obtain a copy of the license at * http://www.apache.org/licenses/license-2.0 * unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an \"as is\" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. */ import java.io.ioexception; import org.apache.lucene.store.directory; import java.util.hashtable; /** this stores a monotonically increasing set of <term, terminfo> pairs in a directory. pairs are accessed either by term or by ordinal position the set. */ final class terminfosreader { private directory directory; private string segment; private fieldinfos fieldinfos; private final hashtable enumeratorsbythread = new hashtable(); private segmenttermenum origenum; private long size; terminfosreader(directory dir, string seg, fieldinfos fis) throws ioexception { directory = dir; segment = seg; fieldinfos = fis; origenum = new segmenttermenum(directory.openfile(segment + \".tis\"), fieldinfos, false); size = origenum.size; readindex(); } public int getskipinterval() { return origenum.skipinterval; } final void close() throws ioexception { if (origenum != null) origenum.close(); } /** returns the number of term/value pairs in the set. */ final long size() { return size; } private segmenttermenum getenum() { segmenttermenum termenum = (segmenttermenum)enumeratorsbythread.get(thread.currentthread()); if (termenum == null) { termenum = terms(); enumeratorsbythread.put(thread.currentthread(), termenum); } return termenum; } term[] indexterms = null; terminfo[] indexinfos; long[] indexpointers; private final void readindex() throws ioexception { segmenttermenum indexenum = new segmenttermenum(directory.openfile(segment + \".tii\"), fieldinfos, true); try { int indexsize = (int)indexenum.size; indexterms = new term[indexsize]; indexinfos = new terminfo[indexsize]; indexpointers = new long[indexsize]; for (int i = 0; indexenum.next(); i++) { indexterms[i] = indexenum.term(); indexinfos[i] = indexenum.terminfo(); indexpointers[i] = indexenum.indexpointer; } } finally { indexenum.close(); } } /** returns the offset of the greatest index entry which is less than or equal to term.*/ private final int getindexoffset(term term) throws ioexception { int lo = 0; // binary search indexterms[] int hi = indexterms.length - 1; while (hi >= lo) { int mid = (lo + hi) >> 1; int delta = term.compareto(indexterms[mid]); if (delta < 0) hi = mid - 1; else if (delta > 0) lo = mid + 1; else return mid; } return hi; } private final void seekenum(int indexoffset) throws ioexception { getenum().seek(indexpointers[indexoffset], (indexoffset * getenum().indexinterval) - 1, indexterms[indexoffset], indexinfos[indexoffset]); } /** returns the terminfo for a term in the set, or null. */ terminfo get(term term) throws ioexception { if (size == 0) return null; // optimize sequential access: first try scanning cached enum w/o seeking segmenttermenum enumerator = getenum(); if (enumerator.term() != null // term is at or past current && ((enumerator.prev != null && term.compareto(enumerator.prev) > 0) term.compareto(enumerator.term()) >= 0)) { int enumoffset = (int)(enumerator.position/enumerator.indexinterval)+1; if (indexterms.length == enumoffset // but before end of block || term.compareto(indexterms[enumoffset]) < 0) return scanenum(term); // no need to seek } // random-access: must seek seekenum(getindexoffset(term)); return scanenum(term); } /** scans within block for matching term. */ private final terminfo scanenum(term term) throws ioexception { segmenttermenum enumerator = getenum(); while (term.compareto(enumerator.term()) > 0 && enumerator.next()) {} if (enumerator.term() != null && term.compareto(enumerator.term()) == 0) return enumerator.terminfo(); else return null; } /** returns the nth term in the set. */ final term get(int position) throws ioexception { if (size == 0) return null; segmenttermenum enumerator = getenum(); if (enumerator != null && enumerator.term() != null && position >= enumerator.position && position < (enumerator.position + enumerator.indexinterval)) return scanenum(position); // can avoid seek seekenum(position / enumerator.indexinterval); // must seek return scanenum(position); } private final term scanenum(int position) throws ioexception { segmenttermenum enumerator = getenum(); while(enumerator.position < position) if (!enumerator.next()) return null; return enumerator.term(); } /** returns the position of a term in the set or -1. */ final long getposition(term term) throws ioexception { if (size == 0) return -1; int indexoffset = getindexoffset(term); seekenum(indexoffset); segmenttermenum enumerator = getenum(); while(term.compareto(enumerator.term()) > 0 && enumerator.next()) {} if (term.compareto(enumerator.term()) == 0) return enumerator.position; else return -1; } /** returns an enumeration of all the terms and terminfos in the set. */ public segmenttermenum terms() { return (segmenttermenum)origenum.clone(); } /** returns an enumeration of terms starting at or after the named term. */ public segmenttermenum terms(term term) throws ioexception { get(term); return (segmenttermenum)getenum().clone(); } /* some jvms might have trouble gc-ing enumeratorsbythread */ protected void finalize() throws throwable { try { // make sure gc can clear up. enumeratorsbythread.clear(); } finally { super.finalize(); } } }",
        "label": 38
    },
    {
        "text": "fail tests if they print over the given limit of bytes to system out or system err some tests print so much stuff they are now undebuggable (see lucene-5612). from now on, when tests.verbose is false, the number of bytes printed to standard output and error streams will be accounted for and if it exceeds a given limit an assertion will be thrown. the limit is adjustable per-suite using limit annotation, with the default of 8kb per suite. the check can be suppressed entirely by specifying suppresssysoutchecks.",
        "label": 11
    },
    {
        "text": "tokenstreamtoautomaton doesn't ignore trailing posinc when preservepositionincrements false tokenstreamtoautomaton in lucene core is used by the analyzingsuggester (incl. fuzzysuggester subclass ) and nrt document suggester and soon the solrtexttagger.  it has a setting preservepositionincrements defaulting to true. if it's set to false (e.g. to ignore stopwords) and if there is a trailing position increment greater than 1, ts2a will still add position increments (holes) into the automata even though it was configured not to. i'm filing this issue separate from lucene-8332 where i first found it. the fix is very simple but i'm concerned about back-compat ramifications so i'm filing it separately. i'll attach a patch to show the problem.",
        "label": 10
    },
    {
        "text": "testperftaskslogic testbgsearchtaskthreads assertion error build 06-aug-2012 19:45:55 [junit4:junit4] failure 1.44s | testperftaskslogic.testbgsearchtaskthreads build 06-aug-2012 19:45:55 [junit4:junit4]    > throwable #1: java.lang.assertionerror build 06-aug-2012 19:45:55 [junit4:junit4]    >  at __randomizedtesting.seedinfo.seed([73a6da79edd783f8:ae931fa55514525a]:0) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.junit.assert.fail(assert.java:92) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.junit.assert.asserttrue(assert.java:43) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.junit.assert.asserttrue(assert.java:54) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.benchmark.bytask.testperftaskslogic.testbgsearchtaskthreads(testperftaskslogic.java:159) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at java.lang.reflect.method.invoke(method.java:597) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1559) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:79) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:737) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:773) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:787) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:345) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.threadleakcontrol.forktimeoutingtask(threadleakcontrol.java:769) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.threadleakcontrol$3.evaluate(threadleakcontrol.java:429) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:746) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:648) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:682) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:693) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.noshadowingoroverridesonmethodsrule$1.evaluate(noshadowingoroverridesonmethodsrule.java:39) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:40) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testruleignoreaftermaxfailures$1.evaluate(testruleignoreaftermaxfailures.java:70) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:55) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.rules.statementadapter.evaluate(statementadapter.java:36) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at com.carrotsearch.randomizedtesting.threadleakcontrol$statementrunner.run(threadleakcontrol.java:345) build 06-aug-2012 19:45:55 [junit4:junit4]    >  at java.lang.thread.run(thread.java:662) build 06-aug-2012 19:45:55 [junit4:junit4]    >  build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> report sum by prefix (x) (1 about 1 out of 1012) build 06-aug-2012 19:45:55 [junit4:junit4]   1> operation     round   runcnt   recsperrun        rec/s  elapsedsec    avgusedmem    avgtotalmem build 06-aug-2012 19:45:55 [junit4:junit4]   1> xsearch_2_par     0        1            0         0.00        0.00    15,377,688    179,961,856 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   2> note: reproduce with: ant test  -dtestcase=testperftaskslogic -dtests.method=testbgsearchtaskthreads -dtests.seed=73a6da79edd783f8 -dtests.slow=true -dtests.locale=iw_il -dtests.timezone=europe/san_marino -dtests.file.encoding=utf-8 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   2> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.36s | testperftaskslogic.testdisablecounting build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: rounds build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> report sum by (any) name (4 about 4 out of 5) build 06-aug-2012 19:45:55 [junit4:junit4]   1> operation       round   runcnt   recsperrun        rec/s  elapsedsec    avgusedmem    avgtotalmem build 06-aug-2012 19:45:55 [junit4:junit4]   1> rounds              0        1           20       256.41        0.08     5,550,800    180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1> createindex -  -  - 0 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   0.00 -   2,530,536 -  180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1> adddocs_exhaust     0        1           20     1,333.33        0.01     2,530,536    180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1> closeindex -  -  -  0 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   0.00 -   5,550,800 -  180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: rounds build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> report sum by (any) name (4 about 4 out of 5) build 06-aug-2012 19:45:55 [junit4:junit4]   1> operation       round   runcnt   recsperrun        rec/s  elapsedsec    avgusedmem    avgtotalmem build 06-aug-2012 19:45:55 [junit4:junit4]   1> rounds              0        1           22       282.05        0.08     5,988,616    180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1> createindex -  -  - 0 -  -   1 -  -  -  - 1 -   1,000.00 -  -   0.00 -   2,965,432 -  180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1> adddocs_exhaust     0        1           20    20,000.00        0.00     2,965,432    180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1> closeindex -  -  -  0 -  -   1 -  -  -  - 1 -  -   66.67 -  -   0.01 -   5,988,616 -  180,027,392 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.75s | testperftaskslogic.testlocale build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: null build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: root locale build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: de build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: en_us build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: no_no_ny build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.28s | testperftaskslogic.testmergescheduler build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.20s | testperftaskslogic.testreadtokens build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.20s | testperftaskslogic.testexhaustcontentsource build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.48s | testperftaskslogic.testparalleldocmaker build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.97s | testperftaskslogic.testhighlighting build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.86s | testperftaskslogic.testcollator build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: root locale build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: org.apache.lucene.collation.collationkeyanalyzer() build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: de build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: org.apache.lucene.collation.collationkeyanalyzer(de) build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: en_us build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: org.apache.lucene.collation.collationkeyanalyzer(en_us) build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed locale to: no_no_ny build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: org.apache.lucene.collation.collationkeyanalyzer(no_no_ny) build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1 build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.50s | testperftaskslogic.testindexwritersettings build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 0-->1:   compound:true-->false  doc.term.vector:false-->true build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> --> round 1-->2:   compound:false-->true  doc.term.vector:true-->false build 06-aug-2012 19:45:55 [junit4:junit4]   1>  build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.53s | testperftaskslogic.testhighlightingnotvnostore build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1>    0.25 sec --> test-testperftaskslogic.testhighlightingnotvnostore-seed#[73a6da79edd783f8] added      1000 docs build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.52s | testperftaskslogic.testshingleanalyzer build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: shingleanalyzerwrapper, wrapping shinglefilter over org.apache.lucene.analysis.standard.standardanalyzer build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: shingleanalyzerwrapper, wrapping shinglefilter over org.apache.lucene.analysis.standard.standardanalyzer build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: shingleanalyzerwrapper, wrapping shinglefilter over org.apache.lucene.analysis.core.whitespaceanalyzer build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> changed analyzer to: shingleanalyzerwrapper, wrapping shinglefilter over org.apache.lucene.analysis.core.whitespaceanalyzer build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.33s | testperftaskslogic.testexhaustedlooped build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: rounds_2 build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.73s | testperftaskslogic.testhighlightingtv build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1>    0.20 sec --> test-testperftaskslogic.testhighlightingtv-seed#[73a6da79edd783f8] added      1000 docs build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.55s | testperftaskslogic.testdocmakerthreadsafety build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.36s | testperftaskslogic.testforcemerge build 06-aug-2012 19:45:55 [junit4:junit4] ok      0.70s | testperftaskslogic.testtimedsearchtask build 06-aug-2012 19:45:55 [junit4:junit4]   1> ------------> starting task: seq build 06-aug-2012 19:45:55 [junit4:junit4]   1> build 06-aug-2012 19:45:55 [junit4:junit4]    > (@afterclass output) build 06-aug-2012 19:45:55 [junit4:junit4]   2> note: test params are: codec=lucene40: {body=lucene40(minblocksize=90 maxblocksize=264), docdate=lucene40(minblocksize=90 maxblocksize=264), docdatenum=postingsformat(name=memory dopackfst= true), docname=postingsformat(name=mockrandom), sort_field=postingsformat(name=memory dopackfst= false), country=lucene40(minblocksize=90 maxblocksize=264), docid=postingsformat(name=mockrandom), $facets=postingsformat(name=memory dopackfst= true), random_string=postingsformat(name=memory dopackfst= false), $full_path$=postingsformat(name=mockrandom), doctitle=postingsformat(name=memory dopackfst= true), doctimesecnum=lucene40(minblocksize=90 maxblocksize=264), $payloads$=postingsformat(name=mockrandom)}, sim=randomsimilarityprovider(querynorm=true,coord=true): {}, locale=iw_il, timezone=europe/san_marino build 06-aug-2012 19:45:55 [junit4:junit4]   2> note: windows 7 6.1 amd64/sun microsystems inc. 1.6.0_33 (64-bit)/cpus=2,threads=1,free=65982616,total=180551680 build 06-aug-2012 19:45:55 [junit4:junit4]   2> note: all tests run in this jvm: [streamutilstest, testhtmlparser, testperftasksparse, perftasktest, altpackagetasktest, testperftaskslogic] build 06-aug-2012 19:45:55 [junit4:junit4]   2>  build 06-aug-2012 19:45:55 [junit4:junit4] completed in 16.22s, 23 tests, 1 failure <<< failures!",
        "label": 33
    },
    {
        "text": "analyzingsuggester sort order doesn't respect the actual weight uwe would say: \"sorry but your code is wrong\". we don't actually read the weight value in analyzingcomparator which can cause really odd suggestions since we read parts of the input as the weight. non of our tests catches that so i will go ahead and add some tests for it as well.",
        "label": 46
    },
    {
        "text": "column stride fields  aka per document payloads  this new feature has been proposed and discussed here: http://markmail.org/search/?q=per-document+payloads#query:per-document%20payloads+page:1+mid:jq4g5myhlvidw3oc+state:results currently it is possible in lucene to store data as stored fields or as payloads. stored fields provide good performance if you want to load all fields for one document, because this is an sequential i/o operation. if you however want to load the data from one field for a large number of documents, then stored fields perform quite badly, because lot's of i/o seeks might have to be performed. a better way to do this is using payloads. by creating a \"special\" posting list that has one posting with payload for each document you can \"simulate\" a column- stride field. the performance is significantly better compared to stored fields, however still not optimal. the reason is that for each document the freq value, which is in this particular case always 1, has to be decoded, also one position value, which is always 0, has to be loaded. as a solution we want to add real column-stride fields to lucene. a possible format for the new data structure could look like this (csd stands for column- stride data, once we decide for a final name for this feature we can change this): csdlist --> fixedlengthlist | <variablelengthlist, skiplist> fixedlengthlist --> <payload>^segsize variablelengthlist --> <docdelta, payloadlength?, payload> payload --> byte^payloadlength payloadlength --> vint skiplist --> see frq.file we distinguish here between the fixed length and the variable length cases. to allow flexibility, lucene could automatically pick the \"right\" data structure. this could work like this: when the documentswriter writes a segment it checks whether all values of a field have the same length. if yes, it stores them as fixedlengthlist, if not, then as variablelengthlist. when the segmentmerger merges two or more segments it checks if all segments have a fixedlengthlist with the same length for a column-stride field. if not, it writes a variablelengthlist to the new segment. once this feature is implemented, we should think about making the column- stride fields updateable, similar to the norms. this will be a very powerful feature that can for example be used for low-latency tagging of documents. other use cases: replace norms allow to store boost values separately from norms as input for the fieldcache, thus providing significantly improved loading performance (see lucene-831) things that need to be done here: decide for a name for this feature - i think \"column-stride fields\" was liked better than \"per-document payloads\" design an api for this feature. we should keep in mind here that these fields are supposed to be updateable. define datastructures. i would like to get this feature into 2.4. feedback about the open questions is very welcome so that we can finalize the design soon and start implementing.",
        "label": 46
    },
    {
        "text": "testindexwritermaxdocs testexactlyattruelimit  failure from my jenkins http://jenkins.sarowe.net/job/lucene-core-weekly-monster-master-hdd/365/, reproduces for me locally: checking out revision 964cc88cee7d62edf03a923e3217809d630af5d5 (refs/remotes/origin/master) [...]    [junit4] suite: org.apache.lucene.index.testindexwritermaxdocs    [junit4]   2> note: reproduce with: ant test  -dtestcase=testindexwritermaxdocs -dtests.method=testexactlyattruelimit -dtests.seed=4544f11b4d9bb14c -dtests.nightly=true -dtests.slow=true -dtests.monster=true -dtests.locale=ko-kr -dtests.timezone=atlantic/jan_mayen -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 1597s j0 | testindexwritermaxdocs.testexactlyattruelimit <<<    [junit4]    > throwable #1: java.lang.assertionerror: expected:<2147483519> but was:<2165>    [junit4]    >  at __randomizedtesting.seedinfo.seed([4544f11b4d9bb14c:aae52deaf347c094]:0)    [junit4]    >  at org.apache.lucene.index.testindexwritermaxdocs.testexactlyattruelimit(testindexwritermaxdocs.java:71)    [junit4]    >  at java.lang.thread.run(thread.java:748)    [junit4]   2> note: leaving temporary files on disk at: /slow/jenkins/hdd-workspaces/lucene-core-weekly-monster-master-hdd/lucene/build/core/test/j0/temp/lucene.index.testindexwritermaxdocs_4544f11b4d9bb14c-001    [junit4]   2> sep 30, 2018 6:47:23 pm com.carrotsearch.randomizedtesting.threadleakcontrol checkthreadleaks    [junit4]   2> warning: will linger awaiting termination of 1 leaked thread(s).    [junit4]   2> sep 30, 2018 6:47:43 pm com.carrotsearch.randomizedtesting.threadleakcontrol checkthreadleaks    [junit4]   2> severe: 1 thread leaked from suite scope at org.apache.lucene.index.testindexwritermaxdocs:     [junit4]   2>    1) thread[id=2505, name=lucene merge thread #13, state=runnable, group=tgrp-testindexwritermaxdocs]    [junit4]   2>         at org.apache.lucene.codecs.pushpostingswriterbase.writeterm(pushpostingswriterbase.java:148)    [junit4]   2>         at org.apache.lucene.codecs.blocktree.blocktreetermswriter$termswriter.write(blocktreetermswriter.java:865)    [junit4]   2>         at org.apache.lucene.codecs.blocktree.blocktreetermswriter.write(blocktreetermswriter.java:344)    [junit4]   2>         at org.apache.lucene.codecs.fieldsconsumer.merge(fieldsconsumer.java:105)    [junit4]   2>         at org.apache.lucene.codecs.perfield.perfieldpostingsformat$fieldswriter.merge(perfieldpostingsformat.java:165)    [junit4]   2>         at org.apache.lucene.index.segmentmerger.mergeterms(segmentmerger.java:244)    [junit4]   2>         at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:139)    [junit4]   2>         at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4453)    [junit4]   2>         at org.apache.lucene.index.indexwriter.merge(indexwriter.java:4075)    [junit4]   2>         at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:625)    [junit4]   2>         at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:662)    [junit4]   2> sep 30, 2018 6:47:43 pm com.carrotsearch.randomizedtesting.threadleakcontrol trytointerruptall    [junit4]   2> info: starting to interrupt leaked threads:    [junit4]   2>    1) thread[id=2505, name=lucene merge thread #13, state=runnable, group=tgrp-testindexwritermaxdocs]    [junit4]   2> sep 30, 2018 6:47:43 pm com.carrotsearch.randomizedtesting.threadleakcontrol trytointerruptall    [junit4]   2> info: all leaked threads terminated.    [junit4]   2> note: test params are: codec=lucene80, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@72a4d8a0), locale=ko-kr, timezone=atlantic/jan_mayen    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_151 (64-bit)/cpus=16,threads=1,free=616857624,total=1299185664    [junit4]   2> note: all tests run in this jvm: [testbasicmodelin, testsparsefixedbitdocidset, testautomaton, testregexp, testreaderwrapperdvtypecheck, testmultimmap, testlucene50storedfieldsformathighcompression, testnomergescheduler, testlazyproxskipping, testpayloads, test4gbstoredfields, testshardsearching, testtwophasecommittool, testtermvectorsreader, testfloatrangefieldqueries, testlsbradixsorter, testsloppyphrasequery2, testindexmanydocuments, testprefixcodedterms, testsoftdeletesretentionmergepolicy, testindexwriterdelete, testfixedbitdocidset, testconjunctions, testbagofpostings, testmultitermsenum, testindexwriterthreadstosegments, testramdirectory, testmatchnodocsquery, testfuturearrays, testlatlondocvaluesfield, testdatetools, testbufferedindexinput, testassertions, testindexwriterforcemerge, testphrasequery, testdeterminizelexicon, testdocvaluesrewritemethod, testarrayutil, testdelegatinganalyzerwrapper, testcheckindex, testcodecutil, testdatesort, testdirectoryreaderreopen, testdocumentswriterstallcontrol, testdocsandpositions, testindexwriteronjrecrash, testfiltermergepolicy, testneverdelete, testboolean2scorersupplier, testtopfieldcollectorearlytermination, testchartermattributeimpl, teststressindexing, testindexeddisi, testspanexplanations, testomitpositions, testreqoptsumscorer, testintarraydocidset, testindependencestandardized, testforutil, testquerybuilder, testmixeddocvaluesupdates, testcustomtermfreq, testbooleanqueryvisitsubscorers, testmulticollector, testlucene70segmentinfoformat, testperfielddocvaluesformat, testperfieldpostingsformat2, testbinarydocument, testfield, testfieldtype, testfloatrange, testintrange, testlongdistancefeaturequery, testpolygon, testpolygon2d, test2bbinarydocvalues, test2bsorteddocvaluesords, testallfileshavechecksumfooter, testallfileshavecodecheader, testcodecs, testconcurrentmergescheduler, testconsistentfieldnumbers, testcrash, testdemoparallelleafreader, testdirectoryreader, testdocinverterperfielderrorinfo, testdocvalues, testexitabledirectoryreader, testfieldinfos, testfieldinvertstate, testfieldreuse, testfieldsreader, testfiltercodecreader, testflushbyramorcountspolicy, testfortoomuchcloning, testforcemergeforever, testindexcommit, testindexfiledeleter, testindexinput, testindextoomanydocs, testindexwriter, testindexwritercommit, testindexwriterconfig, testindexwriterexceptions, testindexwriterexceptions2, testindexwriterlockrelease, testindexwritermaxdocs]    [junit4]   2> note: reproduce with: ant test  -dtestcase=testindexwritermaxdocs -dtests.seed=4544f11b4d9bb14c -dtests.nightly=true -dtests.slow=true -dtests.monster=true -dtests.locale=ko-kr -dtests.timezone=atlantic/jan_mayen -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] error   0.00s j0 | testindexwritermaxdocs (suite) <<<    [junit4]    > throwable #1: com.carrotsearch.randomizedtesting.threadleakerror: 1 thread leaked from suite scope at org.apache.lucene.index.testindexwritermaxdocs:     [junit4]    >    1) thread[id=2505, name=lucene merge thread #13, state=runnable, group=tgrp-testindexwritermaxdocs]    [junit4]    >         at org.apache.lucene.codecs.pushpostingswriterbase.writeterm(pushpostingswriterbase.java:148)    [junit4]    >         at org.apache.lucene.codecs.blocktree.blocktreetermswriter$termswriter.write(blocktreetermswriter.java:865)    [junit4]    >         at org.apache.lucene.codecs.blocktree.blocktreetermswriter.write(blocktreetermswriter.java:344)    [junit4]    >         at org.apache.lucene.codecs.fieldsconsumer.merge(fieldsconsumer.java:105)    [junit4]    >         at org.apache.lucene.codecs.perfield.perfieldpostingsformat$fieldswriter.merge(perfieldpostingsformat.java:165)    [junit4]    >         at org.apache.lucene.index.segmentmerger.mergeterms(segmentmerger.java:244)    [junit4]    >         at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:139)    [junit4]    >         at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4453)    [junit4]    >         at org.apache.lucene.index.indexwriter.merge(indexwriter.java:4075)    [junit4]    >         at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:625)    [junit4]    >         at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:662)    [junit4]    >  at __randomizedtesting.seedinfo.seed([4544f11b4d9bb14c]:0)    [junit4] completed [321/510 (1!)] on j0 in 1634.90s, 21 tests, 1 failure, 1 error <<< failures!",
        "label": 19
    },
    {
        "text": "multiple facetrequest with the same path creates inconsistent results multiple facetrequest are getting merged into one creating wrong results in this case: facetsearchparams facetsearchparams = new facetsearchparams(); facetsearchparams.addfacetrequest(new countfacetrequest(new categorypath(\"author\"), 10)); facetsearchparams.addfacetrequest(new countfacetrequest(new categorypath(\"author\"), 10)); problem can be fixed by defining hashcode and equals in certain way that lucene recognize we are talking about different requests. attached test case.",
        "label": 43
    },
    {
        "text": "improve spatial point2d and rectangle classes the point2d and rectangle classes have alot of duplicate, redundant and used functionality. this issue cleans them both up and simplifies the functionality they provide. subsequent to this, eclipse and linesegment, which depend on point2d, are not used anywhere in the contrib, therefore rather than trying to update them to use the improved point2d, they will be removed.",
        "label": 7
    },
    {
        "text": "all spatial contrib shape classes implement equals but not hashcode violates contract - at a min, need to implement return constant.",
        "label": 29
    },
    {
        "text": "support updatedocument  with dwpts with separate documentswriterperthreads (dwpt) it can currently happen that the delete part of an updatedocument() is flushed and committed separately from the corresponding new document. we need to make sure that updatedocument() is always an atomic operation from a iw.commit() and iw.getreader() perspective. see lucene-2324 for more details.",
        "label": 46
    },
    {
        "text": "pathhierarchytokenizer adaptation for urls  splits reversed pathhierarchytokenizer should be usable to split urls the a \"reversed\" way (useful for faceted search against urls): www.site.com -> www.site.com, site.com, com moreover, it should be able to skip a given number of first (or last, if reversed) tokens: /usr/share/doc/somesoftware/interesting/part should give with 4 tokens skipped: interesting interesting/part",
        "label": 42
    },
    {
        "text": "ant clean should remove pom xml's currently once the pom.xml's are in place, its hard to get them out. having them can be a little trappy when you're trying to debug the bug. we should facilitate their removal during clean.",
        "label": 47
    },
    {
        "text": "fst has hard limit max size of gb the fst uses a single contiguous byte[] under the hood, which in java is indexed by int so we cannot grow this over integer.max_value. it also internally encodes references to this array as vint. we could switch this to a paged byte[] and make the far larger. but i think this is low priority... i'm not going to work on it any time soon.",
        "label": 33
    },
    {
        "text": "it is impossible to use a custom dictionary for smartchineseanalyzer it is not possible to use a custom dictionary, even though there is a lot of code and javadocs to allow this. this is because the custom dictionary is only loaded if it cannot load the built-in one (which is of course, in the jar file and should load) public synchronized static worddictionary getinstance() {     if (singleinstance == null) {       singleinstance = new worddictionary(); // load from jar file       try {         singleinstance.load();       } catch (ioexception e) { // loading from jar file must fail before it checks the analyzerprofile (where this can be configured)         string worddictroot = analyzerprofile.analysis_data_dir;         singleinstance.load(worddictroot);       } catch (classnotfoundexception e) {         throw new runtimeexception(e);       }     }     return singleinstance;   } i think we should either correct this, document this, or disable custom dictionary support...",
        "label": 40
    },
    {
        "text": "idea modules settings   verify and fix idea's settings for modules/queries and modules/queryparser refer to lucene/contrib instead of modules.",
        "label": 47
    },
    {
        "text": "passageformatter in postingshighlighter trunk the message returned when i try to highlight the word zero [0] in the file : org\\apache\\lucene\\search\\postingshighlight\\package.html the 2 last lines weren't return. there are 4 passages : 2-65 277-434 434-735 735-968 but the length of the file is 984. in the file : passageformatter.format(...) it should return all the original content with the words highlighted. patch need to add this at the end of the method // at line : 91 add this if(pos<content.length()){ sb.append(content.substring(pos)); } return sb.tostring();",
        "label": 33
    },
    {
        "text": "benchmark pkg  specify trec eval submission output from the command line the querydriver for the trec benchmark currently requires 4 command line arguments. the third argument is ignored (i typically populate this with \"bogus\") instead, allow the third argument to specify the submission.txt file for trec_eval. while i am here, add a usage() documenting what the arguments to this driver program do.",
        "label": 40
    },
    {
        "text": "xa resource transaction support please add xaresoure/xatransaction support into lucene core.",
        "label": 40
    },
    {
        "text": "add  noisy annotation for uncontrollably noisy tests   /**    * annotation for test classes that are uncontrollably loud, and you     * only want output if they actually fail, error, or verbose is enabled.    * @deprecated fix your test to properly use {@link #verbose} !    */   @documented   @deprecated   @target(elementtype.type)   @retention(retentionpolicy.runtime)   public @interface noisy {}",
        "label": 11
    },
    {
        "text": "allow terms dictionary lookups to be lazy when scores are not needed lucene-7311 made it possible to avoid loading termstates in cached termqueries. it would be useful to extend this to other queries that use the terms dictionary.",
        "label": 2
    },
    {
        "text": "instantiatedindexreader clone this patch will implement indexreader.clone for instantiatedindexreader.",
        "label": 24
    },
    {
        "text": "add trierangefilter to contrib according to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), i want to include my fast numerical range query implementation into lucene contrib-queries. i implemented (based on rangefilter) another approach for faster rangequeries, based on longs stored in index in a special format. the idea behind this is to store the longs in different precision in index and partition the query range in such a way, that the outer boundaries are search using terms from the highest precision, but the center of the search range with lower precision. the implementation stores the longs in 8 different precisions (using a class called trieutils). it also has support for doubles, using the ieee 754 floating-point \"double format\" bit layout with some bit mappings to make them binary sortable. the approach is used in rather big indexes, query times are even on low performance desktop computers <<100 ms for very big ranges on indexes with 500000 docs. i called this rangequery variant and format \"trierangerange\" query because the idea looks like the well-known trie structures (but it is not identical to real tries, but algorithms are related to it).",
        "label": 53
    },
    {
        "text": "store lucene version in segment n it would be nice to have the version of lucene that wrote segments_n, so that we can use this to determine which major version an index was written with (for upgrading across major versions). i think this could be squeezed in just after the segments_n header.",
        "label": 33
    },
    {
        "text": "specialize payload processing from of docsandpositionsenum in lucene-2760 i started working to try to improve the speed of a few spanqueries. in general the trick there is to avoid processing positions if you dont have to. but, we can improve queries that read lots of positions further by cleaning up segmentdocsandpositionsenum, in nextposition() this has no less than 3 payloads-related checks. however, a large majority of users/fields have no payloads at all. i think we should specialize this case into a separate implementation and speed up the common case. edit: dyslexia with the jira issue number.",
        "label": 40
    },
    {
        "text": "nomergepolicy should have one singleton   nomergepolicy instance currently there are two singletons available - mergepolicy.no_compound_files and mergepolicy.compound_files and it's confusing to distinguish on compound files when the merge policy never merges segments. we should have one singleton - nomergepolicy.instance post to the relevant discussion - http://mail-archives.apache.org/mod_mbox/lucene-java-user/201404.mbox/%3ccaodyfzxxyvsf9%2bxyarhr5v2o4mc6s2v-qwut112_cjfyhwtpqw%40mail.gmail.com%3e",
        "label": 43
    },
    {
        "text": "testpackedints test failure ant test -dtestcase=testpackedints -dtests.method=testbulkset -dtests.seed=abda58fe51efa72b -dtests.locale=lt -dtests.timezone=europe/athens -dargs=\"-dfile.encoding=utf-8\"",
        "label": 1
    },
    {
        "text": "englishpossessivefilter should work with unicode right single quotation mark the current englishpossessivefilter (used in englishanalyzer) removes possessives using only the '\\'' character (plus 's' or 's'), but some common systems (german?) insert the unicode \"\\u2019\" (right single quotation mark) instead and this is not removed when processing utf-8 text. i propose to change englishpossesivefilter to support '\\u2019' as an alternative to '\\''.",
        "label": 40
    },
    {
        "text": "improved analyzingsuggester replacesep  reworked analyzingsuggester.replacesep() to use automaton.builder instead of automaton. this avoids most of the unnecessary allocation of memory via grow*().",
        "label": 33
    },
    {
        "text": "test failed  randomgeoshaperelationshiptest testrandomcontains https://jenkins.thetaphi.de/job/lucene-solr-master-linux/20800/ error message: geoareashape: geoexactcircle: {planetmodel=planetmodel.wgs84, center=[lat=-0.00871130560892533, lon=2.3029626482941588([x=-0.6692047265792528, y=0.7445316825911176, z=-0.008720939756154669])], radius=3.038428918538668(174.0891533827647), accuracy=2.111101444186927e-4} shape: georectangle: {planetmodel=planetmodel.wgs84, toplat=0.18851664435052304(10.801208089253723), bottomlat=-1.4896034997154073(-85.34799368160976), leftlon=-1.4970589804391838(-85.7751612613233), rightlon=1.346321571653886(77.13854392318753)} expected:<0> but was:<2> stack trace: java.lang.assertionerror: geoareashape: geoexactcircle: {planetmodel=planetmodel.wgs84, center=[lat=-0.00871130560892533, lon=2.3029626482941588([x=-0.6692047265792528, y=0.7445316825911176, z=-0.008720939756154669])], radius=3.038428918538668(174.0891533827647), accuracy=2.111101444186927e-4} shape: georectangle: {planetmodel=planetmodel.wgs84, toplat=0.18851664435052304(10.801208089253723), bottomlat=-1.4896034997154073(-85.34799368160976), leftlon=-1.4970589804391838(-85.7751612613233), rightlon=1.346321571653886(77.13854392318753)} expected:<0> but was:<2>         at __randomizedtesting.seedinfo.seed([87612c9805977c6f:b087e212a0c8db25]:0)         at org.junit.assert.fail(assert.java:93)         at org.junit.assert.failnotequals(assert.java:647)         at org.junit.assert.assertequals(assert.java:128)         at org.junit.assert.assertequals(assert.java:472)         at org.apache.lucene.spatial3d.geom.randomgeoshaperelationshiptest.testrandomcontains(randomgeoshaperelationshiptest.java:225)",
        "label": 25
    },
    {
        "text": "spellchecker clearindex calls unlock inappropriately as noted in lucene-1050, fixing a bug in simplelockfactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in spellchecker.clearindex... grant... it seems the spellchecker is telling the indexreader to delete the lockfile, but the lockfile doesn't exist. ... i don't know much about the locking mechanism, but it seems like this should check to see if the lockfile exists before trying to delete it. hoss... grant: my take on this is that spellchecker.clearindex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a \"stale lock\" that needs to be closed - ie: this is a bug in spellchecker that you have only discovered because this bug lucene-1050 was fixed. i would suggest a new issue for tracking, and a patch in which spellchecker.clearindex doesn't call unlock unless islocked returns true. even then, it might make sense to catch and ignore lockreleasefailedexception and let whatever resulting exception may originate from \"new indexwriter\" be returned. marking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.",
        "label": 15
    },
    {
        "text": "factor out a shared spellchecking module in lucene's contrib we have spellchecking support (index-based spellchecker, directspellchecker, etc). we also have some things like pluggable comparators. in solr we have auto-suggest support (with two implementations it looks like), some good utilities like highfrequencydictionary, etc. i think spellchecking is really important... google has upped the ante to what users expect. so i propose we combine all this stuff into a shared modules/spellchecker, which will make it easier to refactor and improve the quality.",
        "label": 40
    },
    {
        "text": "handle nosuchfileexception with java 7 we now must handle either nosuchfileexception (thrown by nio) or filenotfoundexception (thrown by pre-nio io apis) coming out of some directory methods. we did this on trunk already but we need to do it for 4.x as well... including fixing mockdirwrapper to randomly pick one to throw. uwe said: i did a grep for nosuchfileexception on trunk and 4.x. in trunk we have many more of this excepotion, especially also randomization in mockdirectory to throw this one or fnfe. we should backport those fixes: branch_4x: ./lucene/core/src/java/org/apache/lucene/store/directory.java: * (not {@code java.nio.file.nosuchfileexception} of java 7). ./lucene/core/src/java/org/apache/lucene/store/directory.java: * (not {@code java.nio.file.nosuchfileexception} of java 7). ./lucene/core/src/java/org/apache/lucene/store/directory.java: * (not {@code java.nio.file.nosuchfileexception} of java 7). ./lucene/replicator/src/test/org/apache/lucene/replicator/localreplicatortest.java:import java.nio.file.nosuchfileexception; ./lucene/replicator/src/test/org/apache/lucene/replicator/localreplicatortest.java: } catch (filenotfoundexception | nosuchfileexception e) { trunk: ./lucene/core/src/java/org/apache/lucene/index/directoryreader.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/java/org/apache/lucene/index/directoryreader.java: } catch (filenotfoundexception | nosuchfileexception fnfe) { ./lucene/core/src/java/org/apache/lucene/index/indexfiledeleter.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/java/org/apache/lucene/index/indexfiledeleter.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/core/src/java/org/apache/lucene/store/directory.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/java/org/apache/lucene/store/directory.java: * <li>throws {@link filenotfoundexception} or {@link nosuchfileexception} ./lucene/core/src/java/org/apache/lucene/store/directory.java: * <p>throws {@link filenotfoundexception} or {@link nosuchfileexception} ./lucene/core/src/java/org/apache/lucene/store/directory.java: * <p>throws {@link filenotfoundexception} or {@link nosuchfileexception} ./lucene/core/src/test/org/apache/lucene/index/testaddindexes.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/test/org/apache/lucene/index/testaddindexes.java: } else if (t instanceof filenotfoundexception || t instanceof nosuchfileexception) { ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java: fail(\"opening directoryreader on empty directory failed to produce filenotfoundexception/nosuchfileexception\"); ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java: fail(\"expected filenotfoundexception/nosuchfileexception\"); ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java: fail(\"expected filenotfoundexception/nosuchfileexception\"); ./lucene/core/src/test/org/apache/lucene/index/testdirectoryreader.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/core/src/test/org/apache/lucene/index/testindexwriterexceptions.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/test/org/apache/lucene/index/testindexwriterexceptions.java: } catch (filenotfoundexception | nosuchfileexception ex) { ./lucene/core/src/test/org/apache/lucene/index/testindexwriterlockrelease.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/test/org/apache/lucene/index/testindexwriterlockrelease.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/core/src/test/org/apache/lucene/index/testindexwriterlockrelease.java: } catch (filenotfoundexception | nosuchfileexception e1) { ./lucene/core/src/test/org/apache/lucene/store/testdirectory.java:import java.nio.file.nosuchfileexception; ./lucene/core/src/test/org/apache/lucene/store/testdirectory.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/replicator/src/test/org/apache/lucene/replicator/localreplicatortest.java:import java.nio.file.nosuchfileexception; ./lucene/replicator/src/test/org/apache/lucene/replicator/localreplicatortest.java: } catch (filenotfoundexception | nosuchfileexception e) { ./lucene/test-framework/src/java/org/apache/lucene/store/mockdirectorywrapper.java:import java.nio.file.nosuchfileexception; ./lucene/test-framework/src/java/org/apache/lucene/store/mockdirectorywrapper.java: throw randomstate.nextboolean() ? new filenotfoundexception(\"a random ioexception (\" + name + \")\") : new nosuchfileexception(\"a random ioexception (\" + name + \")\"); ./lucene/test-framework/src/java/org/apache/lucene/store/mockdirectorywrapper.java: throw randomstate.nextboolean() ? new filenotfoundexception(name + \" in dir=\" + in) : new nosuchfileexception(name + \" in dir=\" + in); ./lucene/test-framework/src/java/org/apache/lucene/store/mockdirectorywrapper.java: throw randomstate.nextboolean() ? new filenotfoundexception(name) : new nosuchfileexception(name); this is important to fix before we release 4.8 for java 7",
        "label": 53
    },
    {
        "text": "we should change the suggested search in the demo docs because the lucene code base is full of swear words the javadocs for the lucene demo say... you'll be prompted for a query. type in a swear word and press the enter key. you'll see that the lucene developers are very well mannered and get no results. now try entering the word \"string\". that should return a whole bunch of documents. the results will page at every tenth result and ask you whether you want more results. ...but thanks to files like \"kstemdata*.java\" and \"top50kwiki.utf8\" i was really hard pressed to find an (english) swear word that didn't result in a match in any of the files in the lucene code base (and i have a pretty extensive breadth of knowledge of profanity) we should change this paragraph to refer to something that is total giberish (\"supercalifragilisticexpialidocious\")... or maybe just \"nocommit\" (side note: since this para exists in the javadoc package comments, it will get picked up when they index the source \u2013 so we should include an html comment in the middle of whatever word is picked)",
        "label": 18
    },
    {
        "text": "analyzinginfixsuggester should return structured highlighted results instead of single string per result today it renders to an html string (<b>..</b> for hits) in protected methods that one can override to change the highlighting, but this is hard/inefficient to use for search servers that want to e.g. return json representation of the highlighted result. this is the same issue as lucene-4906 (postingshighlighter) but for analyzinginfixsuggester's highlights instead.",
        "label": 33
    },
    {
        "text": "wildcardquery do not find documents if leading and trailing   is used i indexed a document which contains the word \"business\". if i use query \"business\" then document will be found. if i use query \"busines*\" then document will be found. if i use query \"*usiness\" then document will be found. if i use query \"*usines?\" then document will be found. if i use query \"?usines?\" then document will be found. but if i use query \"usines\" then document will not be found. if i use query \"usi*nes\" then document will be found.",
        "label": 12
    },
    {
        "text": "eliminate synchronization contention on initial index reading in terminfosreader ensureindexisread synchronized method ensureindexisread in terminfosreader causes contention under heavy load simple to reproduce: e.g. under solr, with all caches turned off, do a simple range search e.g. id:[0 to 999999] on even a small index (in my case 28k docs) and under a load/stress test application, and later, examining the thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method. rather than using double-checked locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from indexnotread state to indexread, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. in my particular test, this uncreased throughput at least 30 times.",
        "label": 33
    },
    {
        "text": "spellchecker  make hard coded values configurable the class org.apache.lucene.search.spell.spellchecker uses the following hard-coded values in its method indexdictionary: writer.setmergefactor(300); writer.setmaxbuffereddocs(150); this poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix environments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428). there are several ways to circumvent this: 1. add another indexdictionary method with additional parameters: public void indexdictionary (dictionary dict, int mergefactor, int maxbuffereddocs) throws ioexception 2. add setter methods for mergefactor and maxbuffereddocs (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 ) 3. make spellchecker subclassing easier as suggested by chris hostetter (see reply http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463) thanx, karin",
        "label": 38
    },
    {
        "text": "fielddoc tostring only returns super tostring the fielddoc.tostring method very carefully builds a stringbuffer sb containing the information for the fielddoc instance and then just returns super.tostring() instead of sb.tostring()",
        "label": 33
    },
    {
        "text": "querytreebuilder getbuilder  only finds interfaces on the most derived class querybuilder implementations registered with querytreebuilder.setbuilder() are not recognized by querytreebuilder.getbuilder() if they are registered for an interface implemented by a superclass. registering them for a concrete query node class or an interface implemented by the most-derived class do work. example.java /* our custom query builder */ class customquerytreebuilder extends querytreebuilder {   public customquerytreebuilder() {     /* turn field:\"value\" into an application-specific object */     setbuilder(fieldquerynode.class, new querybuilder() {       @override       public object build(querynode querynode) {         fieldquerynode node = (fieldquerynode) querynode;         return new applicationspecificclass(node.getfieldasstring());       }     });     /* ignore all other query node types */     setbuilder(querynode.class, new  querybuilder() {       @override       public object build(querynode querynode) {         return null;       }     });   } } /* assume this is in the main program: */ standardqueryparser queryparser = new standardqueryparser(); queryparser.setquerybuilder(new customquerytreebuilder()); /* the following line will throw an exception because it can't find a builder for booleanquerynode.class */ object queryobject = queryparser.parse(\"field:\\\"value\\\" field2:\\\"value2\\\"\", \"field\");",
        "label": 0
    },
    {
        "text": "ant test command fails under lucene tools the ant test command executed under lucene/tools folder fails because it does not have junit.classpath property. since the module does not have any test folder we could override the -test and -check-totals targets. bash-3.2$ pwd /users/peter.somogyi/repos/lucene-solr/lucene/tools bash-3.2$ ant test buildfile: /users/peter.somogyi/repos/lucene-solr/lucene/tools/build.xml ... -test:    [junit4] <junit4> says ciao! master seed: 9a2acc9b4a3c8553 build failed /users/peter.somogyi/repos/lucene-solr/lucene/common-build.xml:1567: the following error occurred while executing this line: /users/peter.somogyi/repos/lucene-solr/lucene/common-build.xml:1092: reference junit.classpath not found. total time: 1 second i ran into this issue when uploaded a patch where i removed an import from this module. this triggered a module-level build during precommit that failed with this error.",
        "label": 53
    },
    {
        "text": "fastvectorhighlighter  add a method to set an arbitrary char that is used when concatenating multivalued data if the following multivalued names are in authors field: michael mccandless erik hatcher otis gospodneti\u0107 since fragmentsbuilder concatenates multivalued data with a space in basefragmentsbuilder.getfragmentsource(): while( buffer.length() < endoffset && index[0] < values.length ){   if( index[0] > 0 && values[index[0]].istokenized() && values[index[0]].stringvalue().length() > 0 )     buffer.append( ' ' );   buffer.append( values[index[0]++].stringvalue() ); } an entire field snippet (using lucene-2464) will be \"michael mccandless erik hatcher otis gospodneti\u0107\". there is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. \"michael mccandless/erik hatcher/otis gospodneti\u0107\"",
        "label": 26
    },
    {
        "text": "review semantics of spatialoperation predicates spatialoperation (which i wish was named spatialpredicate) is a bunch of predicates \u2013 methods that return true/false based on a pair of shapes. some of them don't seem to be defined in a way consistent with their definitions on esri's site: http://edndoc.esri.com/arcsde/9.1/general_topics/understand_spatial_relations.htm (which is linked as a reference, and is in turn equivalent to ogc spec definitions, i believe). problems: the definitions make no mention of needing to have area or not, yet some of our predicates are defined as to require area on either the indexed or query shape. the definitions make a distinction of the boundary of a shape, yet in lucene-spatial, there is none. that suggests our predicates are wrongly chosen since there are official predicates that are boundary-neutral \u2013 namely \"covers\" and \"coveredby\" in lieu of contains and within, respectively. if we don't rename our predicates, we should at least support the correct predicates names! overlaps appears totally wrong. it should be defined as indexedshape.relate(queryshape) == intersects (and thus not within or contains or disjoint). it's presently defined as the same as intersects plus the query shape needing area.",
        "label": 10
    },
    {
        "text": "let the naivebayes classifier have a fallback doccount method if codec doesn't support terms doccount  in simplenaivebayesclassifier docswithclasssize variable is initialized to multifields.getterms(this.atomicreader, this.classfieldname).getdoccount() which may be -1 if the codec doesn't support doc counts, therefore there should be an alternative way to initialize such a variable with the documents count.",
        "label": 50
    },
    {
        "text": "buildandpushrelease py script should refer to working tree instead of directory as per this commit, https://github.com/git/git/commit/2a0e6cdedab306eccbd297c051035c13d0266343 the git status no longer reports: nothing to commit, working directory clean but reports: nothing to commit, working tree clean",
        "label": 20
    },
    {
        "text": "replacement for termattribute impl with extended capabilities  byte  support  charsequence  appendable  for flexible indexing terms can be simple byte[] arrays, while the current termattribute only supports char[]. this is fine for plain text, but e.g numerictokenstream should directly work on the byte[] array. also termattribute lacks of some interfaces that would make it simplier for users to work with them: appendable and charsequence i propose to create a new interface \"chartermattribute\" with a clean new api that concentrates on charsequence and appendable. the implementation class will simply support the old and new interface working on the same term buffer. default_attribute_factory will take care of this. so if somebody adds a termattribute, he will get an implementation class that can be also used as chartermattribute. as both attributes create the same impl instance both calls to addattribute are equal. so a tokenfilter that adds chartermattribute to the source will work with the same instance as the tokenizer that requested the (deprecated) termattribute. to also support byte[] only terms like collation or numericfield needs, a separate getter-only interface will be added, that returns a reusable bytesref, e.g. bytesrefgetterattribute. the default implementation class will also support this interface. for backwards compatibility with old self-made-termattribute implementations, the indexer will check with hasattribute(), if the bytesref getter interface is there and if not will wrap a old-style termattribute (a deprecated wrapper class will be provided): new bytesrefgetterattributewrapper(termattribute), that is used by the indexer then.",
        "label": 53
    },
    {
        "text": "change primitive data types from int to long in class segmentmerger java hi we are getting an exception while optimize. we are getting this exception \"mergefields produced an invalid result: doccount is 385282378 but fdx file size is 3082259028; now aborting this merge to prevent index corruption\" i have checked the code for class segmentmerger.java and found this check *********************************************************************************************************************************************************************** if (4+doccount*8 != fdxfilelength) // this is most likely a bug in sun jre 1.6.0_04/_05; // we detect that the bug has struck, here, and // throw an exception to prevent the corruption from // entering the index. see lucene-1282 for // details. throw new runtimeexception(\"mergefields produced an invalid result: doccount is \" + doccount + \" but fdx file size is \" + fdxfilelength + \"; now aborting this merge to prevent index corruption\"); } *********************************************************************************************************************************************************************** in our case doccount is 385282378 and fdxfilelength size is 3082259028, even though 4+385282378*8 is equal to 3082259028, the above code will not work because number 3082259028 is out of int range. so type of variable doccount needs to be changed to long i have written a small test for this ************************************************************************************************************************************************************************ public class segmentmergertest { public static void main(string[] args) { int doccount = 385282378; long fdxfilelength = 3082259028l; if(4+doccount*8 != fdxfilelength) system.out.println(\"no match\" + (4+doccount*8)); else system.out.println(\"match\" + (4+doccount*8)); } } ************************************************************************************************************************************************************************ above test will print no match but if you change the data type of doccount to long, it will print match can you please advise us if this issue will be fixed in next release? regards deepak",
        "label": 33
    },
    {
        "text": "booleanquery scores can be diff for same docs sim when using coord  disagree with explanation which doesn't change  some of the folks reported that sometimes explain's score can be different than the score requested by fields parameter. interestingly, explain's scores would create a different ranking than the original result list. this is something users experience, but it cannot be re-produced deterministically.",
        "label": 47
    },
    {
        "text": "make pkindexsplitter and multipassindexsplitter work per segment spinoff from lucene-3624: docvaluesw merger throws exception on iw.addindexes(slowmultireaderwrapper) as string-index like docvalues cannot provide assortedsource.",
        "label": 53
    },
    {
        "text": "randomgeopolygontest testcomparebigpolygons  failure failure from https://jenkins.thetaphi.de/job/lucene-solr-master-linux/22590/, reproduces for me on java8: checking out revision 2a41cbd1924510000f6e69ae2e6cccb7b2e26af2 (refs/remotes/origin/master) [...]    [junit4] suite: org.apache.lucene.spatial3d.geom.randomgeopolygontest    [junit4]   2> note: reproduce with: ant test  -dtestcase=randomgeopolygontest -dtests.method=testcomparebigpolygons -dtests.seed=5444688174504c79 -dtests.multiplier=3 -dtests.slow=true -dtests.locale=pt-lu -dtests.timezone=pacific/pago_pago -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 0.23s j1 | randomgeopolygontest.testcomparebigpolygons {seed=[5444688174504c79:cc6bba71b5fc82a6]} <<<    [junit4]    > throwable #1: java.lang.assertionerror:     [junit4]    > standard polygon: geocompositepolygon: {[geoconvexpolygon: {planetmodel=planetmodel.sphere, points=[[lat=1.0468214627857893e-8, lon=8.413079957136915e-7([x=0.9999999999996461, y=8.413079957135923e-7, z=1.0468214627857893e-8])], [lat=-0.3036468642757333, lon=0.5616500855257733([x=0.807657732111119, y=0.508219108660839, z=-0.29900221630132817])], [lat=-0.17226782498440368, lon=0.8641157866087514([x=0.6397020656700857, y=0.7492646151846353, z=-0.1714170458549729])], [lat=0.5917632222073597, lon=1.0258877306398073([x=0.43020057589183536, y=0.7097594028504629, z=0.5578252903622132])], [lat=0.16341821264361944, lon=0.04608724380526752([x=0.9856292512291138, y=0.04545712432110151, z=0.16269182207472105])]], internaledges={4}}, geoconvexpolygon: {planetmodel=planetmodel.sphere, points=[[lat=1.0468214627857893e-8, lon=8.413079957136915e-7([x=0.9999999999996461, y=8.413079957135923e-7, z=1.0468214627857893e-8])], [lat=0.16341821264361944, lon=0.04608724380526752([x=0.9856292512291138, y=0.04545712432110151, z=0.16269182207472105])], [lat=1.5452567609928165e-12, lon=5.5280224842135794e-12([x=1.0, y=5.5280224842135794e-12, z=1.5452567609928165e-12])]], internaledges={0, 2}}, geoconvexpolygon: {planetmodel=planetmodel.sphere, points=[[lat=1.0468214627857893e-8, lon=8.413079957136915e-7([x=0.9999999999996461, y=8.413079957135923e-7, z=1.0468214627857893e-8])], [lat=1.5452567609928165e-12, lon=5.5280224842135794e-12([x=1.0, y=5.5280224842135794e-12, z=1.5452567609928165e-12])], [lat=-1.0e-323, lon=0.0([x=1.0, y=0.0, z=-1.0e-323])]], internaledges={0}}]}    [junit4]    > large polygon: geocomplexpolygon: {planetmodel=planetmodel.sphere, number of shapes=1, address=e0a76761, testpoint=[lat=0.04032281608974351, lon=0.33067345007473165([x=0.945055084899262, y=0.3244161494642355, z=0.040311889968686655])], testpointinset=true, shapes={ {[lat=1.0468214627857893e-8, lon=8.413079957136915e-7([x=0.9999999999996461, y=8.413079957135923e-7, z=1.0468214627857893e-8])], [lat=-0.3036468642757333, lon=0.5616500855257733([x=0.807657732111119, y=0.508219108660839, z=-0.29900221630132817])], [lat=-0.17226782498440368, lon=0.8641157866087514([x=0.6397020656700857, y=0.7492646151846353, z=-0.1714170458549729])], [lat=0.5917632222073597, lon=1.0258877306398073([x=0.43020057589183536, y=0.7097594028504629, z=0.5578252903622132])], [lat=0.16341821264361944, lon=0.04608724380526752([x=0.9856292512291138, y=0.04545712432110151, z=0.16269182207472105])], [lat=1.5452567609928165e-12, lon=5.5280224842135794e-12([x=1.0, y=5.5280224842135794e-12, z=1.5452567609928165e-12])], [lat=-1.0e-323, lon=0.0([x=1.0, y=0.0, z=-1.0e-323])]}}    [junit4]    > point: [lat=-8.763997112262326e-13, lon=3.14159265358979([x=-1.0, y=3.2310891488651735e-15, z=-8.763997112262326e-13])]    [junit4]    > wkt: polygon((32.18017946378854 -17.397683785381247,49.51018758330871 -9.870219317504647,58.77903721991479 33.90553510354402,2.640604559432277 9.363173880050821,3.1673235739886286e-10 8.853669066894417e-11,0.0 -5.7e-322,4.820339742500488e-5 5.99784517213369e-7,32.18017946378854 -17.397683785381247))    [junit4]    > wkt: point(179.99999999999983 -5.021400461974724e-11)    [junit4]    > normal polygon: true    [junit4]    > large polygon: false    [junit4]    >  at __randomizedtesting.seedinfo.seed([5444688174504c79:cc6bba71b5fc82a6]:0)    [junit4]    >  at org.apache.lucene.spatial3d.geom.randomgeopolygontest.checkpoint(randomgeopolygontest.java:228)    [junit4]    >  at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparepolygons(randomgeopolygontest.java:203)    [junit4]    >  at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparebigpolygons(randomgeopolygontest.java:98)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    >  at java.base/java.lang.thread.run(thread.java:844)    [junit4]   2> note: test params are: codec=asserting(lucene70): {}, docvalues:{}, maxpointsinleafnode=20, maxmbsortinheap=5.528098665421503, sim=asserting(org.apache.lucene.search.similarities.assertingsimilarity@1922a624), locale=pt-lu, timezone=pacific/pago_pago    [junit4]   2> note: linux 4.15.0-29-generic amd64/oracle corporation 9.0.4 (64-bit)/cpus=8,threads=1,free=450991728,total=536870912",
        "label": 19
    },
    {
        "text": "remove  author tags in lucene solr sources lucene/solr sources should not include @author tags. see the solr-dev@l.a.o and java-dev@l.a.o threads in which this was discussed. the jenkins builds should fail if they are found, in the same way that nocommit's are currently handled",
        "label": 47
    },
    {
        "text": "lowercasefilter for turkish language java.lang.character.tolowercase() converts 'i' to 'i' however in turkish alphabet lowercase of 'i' is not 'i'. it is latin small letter dotless i.",
        "label": 46
    },
    {
        "text": "automaton termsenum bug when running with multithreaded search this one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results) however, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.",
        "label": 40
    },
    {
        "text": "indexreader lastmodified   throws npe indexreader.lastmodified(string dir) or its variants always return npe on 2.3, perhaps something to do with segmentinfo.",
        "label": 33
    },
    {
        "text": "czech stemmer currently, the czechanalyzer is merely stopwords, and there isn't a czech stemmer in snowball. this patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600 in their measurements, it improves map 42% the analyzer does not use this stemmer if lucene_version <= 3.0, for back compat.",
        "label": 40
    },
    {
        "text": "optional norms for applications with many indexed fields, the norms cause memory problems both during indexing and querying. this patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field. overview of changes: field.omitnorms that defaults to false backward compatible lucene file format change: fieldinfos.fieldbits has a bit for omitnorms indexreader.hasnorms() method during merging, if any segment includes norms, then norms are included. methods to get norms return the equivalent 1.0f array for backward compatibility the patch was designed for backward compatibility: all current unit tests pass w/o any modifications required compatible with old indexes since the default is omitnorms=false compatible with older/custom subclasses of indexreader since a default hasnorms() is provided compatible with older/custom users of indexreader such as weight/scorer/explain since a norm array is produced on demand, even if norms were not stored if this patch is accepted (or if the direction is acceptable), performance for scoring could be improved by assuming 1.0f when hasnorms(field)==false.",
        "label": 55
    },
    {
        "text": "testbooleanminshouldmatch test failure ant test -dtestcase=testbooleanminshouldmatch -dtestmethod=testrandomqueries -dtests.seed=505d62a62e9f90d0:-60daa428161b404b:-406411290a98f416 i think its an absolute/relative epsilon issue",
        "label": 40
    },
    {
        "text": "groupingsearcher setallgroups true  always returns count spinoff from java-user thread \"groupingsearch return 0 when setallgroups(true)\" ( http://markmail.org/message/cv52f4nr3t5be5xw ).",
        "label": 33
    },
    {
        "text": "lucene classification score calculation normalize and return lists now the classifiers can return only the \"best matching\" classes. if somebody want it to use more complex tasks he need to modify these classes for get second and third results too. if it is possible to return a list and it is not a lot resource why we dont do that? (we iterate a list so also.) the bayes classifier get too small return values, and there were a bug with the zero floats. it was fixed with logarithmic. it would be nice to scale the class scores sum vlue to one, and then we coud compare two documents return score and relevance. (if we dont do this the wordcount in the test documents affected the result score.) with bulletpoints: in the bayes classification normalized score values, and return with result lists. in the knn classifier possibility to return a result list. make the classificationresult comparable for list sorting.",
        "label": 50
    },
    {
        "text": "hotspot bug in readvint gives wrong results when testing the 3.1-rc1 made by yonik on the pangaea (www.pangaea.de) productive system i figured out that suddenly on a large segment (about 5 gib) some stored fiels suddenly produce a strange deflate decompression problem (compressiontools) although the stored fields are no longer pre-3.0 compressed. it seems that the header of the stored field is read incorrectly at the buffer boundary in multimmapdir and then fieldsreader just incorrectly detects a deflate-compressed field (compressiontools). the error occurs reproducible on checkindex with mmapdirectory, but not with niodir or simpledir. the fdt file of that segment is 2.6 gib, on solaris the chunk size is integer.max_value, so we have 2 multimmap indexinputs. robert and me have the index ready as a tar file, we will do tests on our local machines and hopefully solve the bug, maybe introduced by robert's recent changes to mmap.",
        "label": 53
    },
    {
        "text": "test2bpostingsbytes sometimes ooms even with  dtests heapsize 30g here's an example: http://jenkins.sarowe.net/job/lucene-core-nightly-monster-5.x-java7/22/console; the seed still ooms for me when -dtests.heapsize=60g: ant test  -dtestcase=test2bpostingsbytes -dtests.method=test -dtests.seed=f99606d852db1420 -dtests.nightly=true -dtests.slow=true -dtests.locale=sr_rs_#latn -dtests.timezone=america/port-au-prince -dtests.asserts=true -dtests.file.encoding=iso-8859-1 (although the repro line doesn't include all of them - dawid weiss is this expected? - the above-linked job runs with -dtests.jvms=4 -dtests.nightly=true -dtests.monster=true -dtests.heapsize=30g.) i narrowed the problem down to compressingcodec - any of the 4 subclasses - with maxdocsperchunk * blocksize less than 16. the other variable randomized in compressingcodec.randominstance() is chunksize, which doesn't seem to be implicated in the ooms. i guess we could suppress compressingcodec, or just expect ooms from time to time (the above condition happens fairly rarely). but i'd rather continue to exercise compressingcodec as much as possible. here are the ooms i saw with -dtests.heapsize=30g and -dtests.codec=compressingcodec: compressionmode chunksize maxdocsperchunk blocksize dummy 3 2 1 dummy 7 1 1 fast_decompression 10 9 1 fast_decompression 31614 3 4 fast 21879 1 9 dummy 3 3 1 high_compression 4167 10 1 high_compression 12437 2 5 dummy 4 3 2 high_compression 3 3 2 fast_decompression 3339 10 1 dummy 2 5 3 here are some that did not oom: compressionmode chunksize maxdocsperchunk blocksize dummy 4 4 4 dummy 4 2 246 fast 30677 907 2 fast_decompression 15165 2 10 fast_decompression 28633 67 1 dummy 7 3 9 dummy 2835 9 3 dummy 25785 4 7",
        "label": 47
    },
    {
        "text": "checkindex api changed without backwards compaitibility the api of checkindex changed. the check function returns a checkindexstatus and not boolean. and javadocs notes the boolean return value. i am not sure if it works, but it would be good to have the check method that returns boolean available @deprecated, i.e. @deprecated public static checkindexstatus check(directory dir, boolean dofix) throws ioexception { final checkindexstatus stat=this.check(dir,dofix); return stat.clean; } i am not sure, if it can be done with the same method name, but it prevents drop-in-replacements of lucene to work.",
        "label": 33
    },
    {
        "text": "lucenesuggester does not work on android i'm developing an application on android and i'm using lucene for indexing and searching. when i try to use analyzingsuggester (even the fuzzy version) i got an exception the bufferedoutputstream is already closed. i tracked the problem and it seems that in org.apache.lucene.search.suggest.sort and in org.apache.lucene.search.suggest.analyzing.analyzingsuggester the outputstream is closed twice hence the exception on android. the same code on windows runs without a problem. it seems that the android jvm does some additional checks. i attach two patche files, the classes close the output stream once. (check for writerclosed in the code to see what i did)",
        "label": 53
    },
    {
        "text": "make query createweight public  or add back query createqueryweight  now that the queryweight class has been removed, the public queryweight createqueryweight() method on query was also removed i have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think booleanquery outside of the o.a.l.search package) in order to do this, i have to create a static utils class inside o.a.l.search, pass in the query and searcher, and have the static method call the protected createweight method this should not be necessary this could be fixed in one of 2 ways: 1. make createweight() public on query (breaks back compat) 2. add the following method: public weight createqueryweight(searcher searcher) throws ioexception {   return createweight(searcher); } createweight(searcher) should then be deprectated in favor of the publicly accessible method",
        "label": 29
    },
    {
        "text": "when concurrentmergescheduler stalls incoming threads it has unexpected hysteresis eg if you set maxmergecount to 2, as soon as a 3rd merge need to kick off, we stall incoming segment-creating threads. then we wait ... and we are supposed to resume the threads when the merge count drops back to 2, but instead we are only resuming when merge count gets to 1. ie, we stall for too long (= unexpected hysteresis).",
        "label": 33
    },
    {
        "text": "changes to html  better handling of bulleted lists in changes txt bulleted lists should be rendered as such in output html",
        "label": 53
    },
    {
        "text": "segment size limit for compound files hello everyone, i implemented an improvement targeting compound file usage. compound files are used to decrease the number of index files, because operating systems can't handle too many open file descriptors. on the other hand, a disadvantage of compound file format is the worse performance compared to multi-file indexes: http://www.gossamer-threads.com/lists/lucene/java-user/8950 in the book \"lucene in action\" it's said that compound file format is about 5-10% slower than multi-file format. the patch i'm proposing here adds the ability to the indexwriter to use compound format only for segments, that do not contain more documents than a specific limit \"compoundfilesegmentsizelimit\", which the user can set. due to the exponential merges, a lucene index usually contains only a few very big segments, but much more small segments. the best performance is actually just needed for the big segments, whereas a slighly worse performance for small segments shouldn't play a big role in the overall search performance. consider the following example: index size: 1,500,000 merge factor: 10 max buffered docs: 100 number of indexed fields: 10 max. os file descriptors: 1024 in the worst case a not-optimized index could contain the following amount of segments: 1 x 1,000,000 9 x 100,000 9 x 10,000 9 x 1,000 9 x 100 that's 37 segments. a multi-file format index would have: 37 segments * (7 files per segment + 10 files for indexed fields) = 629 files ==> only about 2 open indexes per machine could be handled by the operating system a compound-file format index would have: 37 segments * 1 cfs file = 37 files ==> about 27 open indexes could be handled by the operating system, but performance would be 5-10% worse. a compound-file format index with compoundfilesegmentsizelimit = 1,000,000 would have: 36 segments * 1 cfs file + 1 segment * (7 + 10 files) = 53 ==> about 20 open indexes could be handled by the os the os can handle now 20 instead of just 2 open indexes, while maintaining the multi-file format performance. i'm going to create diffs on the current head and will attach the patch files soon. please let me know what you think about this improvement.",
        "label": 32
    },
    {
        "text": "implement tostring  method in termsfilter lucene-1049 introduced a enhanced implementation of the tostring() method in the booleanfilter clause. this was an improvement, however i'm still seeing a lot lucene filter classes not overriding the tostring method resulting in a tostring returning the classname and the hashcode of the object. this can be useful sometimes, but it's totally not useful in my case. i want to see the properties set in the filters so i know which lucene query was created. now: booleanfilter(+booleanfilter(booleanfilter(+org.apache.lucene.search.termsfilter@ea81ba60 +org.apache.lucene.search.termsfilter@26ea3cbc) booleanfilter(+org.apache.lucene.search.termsfilter@df621f09 +org.apache.lucene.search.termsfilter@2f712446))) wanted behavior: booleanfilter(+booleanfilter(booleanfilter(+instock:y +barcode:12345678) booleanfilter(+isheavy:n +isdamaged:y)))",
        "label": 33
    },
    {
        "text": "merge error during add to index  indexoutofboundsexception  i've been batch-building indexes, and i've build a couple hundred indexes with a total of around 150 million records. this only happened once, so it's probably impossible to reproduce, but anyway... i was building an index with around 9.6 million records, and towards the end i got this: java.lang.indexoutofboundsexception: index: 54, size: 24 at java.util.arraylist.rangecheck(arraylist.java:547) at java.util.arraylist.get(arraylist.java:322) at org.apache.lucene.index.fieldinfos.fieldinfo(fieldinfos.java:155) at org.apache.lucene.index.fieldinfos.fieldname(fieldinfos.java:151) at org.apache.lucene.index.segmenttermenum.readterm(segmenttermenum.java :149) at org.apache.lucene.index.segmenttermenum.next (segmenttermenum.java:115) at org.apache.lucene.index.segmentmergeinfo.next (segmentmergeinfo.java:52) at org.apache.lucene.index.segmentmerger.mergeterminfos (segmentmerger.java:294) at org.apache.lucene.index.segmentmerger.mergeterms (segmentmerger.java:254) at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:93) at org.apache.lucene.index.indexwriter.mergesegments (indexwriter.java:487) at org.apache.lucene.index.indexwriter.maybemergesegments (indexwriter.java:458) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:310) at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:294)",
        "label": 55
    },
    {
        "text": "fsa nooutputs should implement merge  allowing duplicate keys the nooutput object throws notimplemented if you try to add the same input twice. this can easily be implemented",
        "label": 42
    },
    {
        "text": "corrupted segment file not detected and wipes index contents lucene will happily wipe an existing index if presented with a latest generation segments_n file of all zeros. file format documentation says segments_n files should start with a format of -9 but segmentinfos.read accepts >=0 as valid for backward compatibility reasons.",
        "label": 33
    }
]