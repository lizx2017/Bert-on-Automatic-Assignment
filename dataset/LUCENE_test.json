[
    {
        "text": "fix ramdirectory's indexinput to not double buffer on slice  after lucene-4371, we still have a non-optimal implementation of indexinput#slice() in ramdirectory. we should fix that to use the cloning approach like other directories do",
        "label": 53
    },
    {
        "text": "while you could use a custom sort comparator source with remote searchable before  you can no longer do so with fieldcomparatorsource fieldcomparatorsource is not serializable, but can live on a sortfield",
        "label": 29
    },
    {
        "text": "remove contrib deprecations there aren't too many deprecations in contrib to remove for 3.0, but we should get rid of them.",
        "label": 40
    },
    {
        "text": "datetools java general improvements applying the attached patch shows the improvements to datetools.java that i think should be done. all logic that does anything at all is moved to instance methods of the inner class resolution. i argue this is more object-oriented. 1. in cases where resolution is an argument to the method, i can simply invoke the appropriate call on the resolution object. formerly there was a big branch if/else. 2. instead of \"synchronized\" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a dateformat or calendar instance. 3. since different dateformat and calendar instances are created per-resolution, there is now less lock contention since threads using different resolutions will not use the same locks. 4. the old implementation of timetostring rounded the time before formatting it. that's unnecessary since the format only includes the resolution desired. 5. round() now uses a switch statement that benefits from fall-through (no break). another debatable improvement that could be made is putting the resolution instances into an array indexed by format length. this would mean i could remove the switch in lookupresolutionbylength() and avoid the length constants there. maybe that would be a bit too over-engineered when the switch is fine.",
        "label": 47
    },
    {
        "text": "allow chartokenizer derived tokenizers and keywordtokenizer to configure the max token length solr-10186 erick erickson: is there a good reason that we hard-code a 256 character limit for the chartokenizer? in order to change this limit it requires that people copy/paste the incrementtoken into some new class since incrementtoken is final. keywordtokenizer can easily change the default (which is also 256 bytes), but to do so requires code rather than being able to configure it in the schema. for keywordtokenizer, this is solr-only. for the chartokenizer classes (whitespacetokenizer, unicodewhitespacetokenizer and lettertokenizer) (factories) it would take adding a c'tor to the base class in lucene and using it in the factory. any objections?",
        "label": 13
    },
    {
        "text": "x index cannot be opened with dev sorry for the lack of proper testcase. in 2.4.1, if you created an index with the (stupid) options below, then it will not create a .prx file. 2.9 expects this file and will not open the index. the reason i used these stupid options is because i changed the field from indexed=yes to indexed=no, but forgot to remove the .setomittf() public class testcase {  public static void main(string args[]) throws exception {   /* run this part with lucene 2.4.1 */   indexwriter iw = new indexwriter(\"test\", new whitespaceanalyzer(), indexwriter.maxfieldlength.limited);   iw.setusecompoundfile(false);   document doc = new document();   field field1 = new field(\"field1\", \"foo\", field.store.yes, field.index.no);   field1.setomittf(true); // 2.9 will create a 0-byte .prx file, but 2.4.x will not. this is the problem. 2.9 expects this file!   doc.add(field1);   iw.adddocument(doc);   iw.close();    /* run this with lucene 2.9 */   indexreader ir = indexreader.open(fsdirectory.getdirectory(\"test\"), true);   } }",
        "label": 33
    },
    {
        "text": "negativearraysizeexception caused by ff addfields i have a server/client software that i have created which has a server process that accepts connections from clients that transmit data about local connection information. this data is than buffered and a threadpoolexecutor runs to take the data and put it into a lucene index as well as a facet index. this works perfect for the lucene index, but the facet index randomly generates a negativearraysizeexception. i cannot find any reason why the exception would be caused because lines with the same type of data do not throw it, then all of a sudden the exception is thrown, typically 4 of them in a row. i talked with mikemccand on irc and he requested i submit this issue. after some discussion, he seems to think it's because some of the values i am using are rather large. here is the exception... java.lang.negativearraysizeexception at java.lang.abstractstringbuilder.<init>(abstractstringbuilder.java:64) at java.lang.stringbuilder.<init>(stringbuilder.java:97) at org.apache.lucene.facet.taxonomy.writercache.cl2o.charblockarray.subsequence(charblockarray.java:164) at org.apache.lucene.facet.taxonomy.writercache.cl2o.categorypathutils.hashcodeofserialized(categorypathutils.java:50) at org.apache.lucene.facet.taxonomy.writercache.cl2o.compactlabeltoordinal.stringhashcode(compactlabeltoordinal.java:294) at org.apache.lucene.facet.taxonomy.writercache.cl2o.compactlabeltoordinal.grow(compactlabeltoordinal.java:184) at org.apache.lucene.facet.taxonomy.writercache.cl2o.compactlabeltoordinal.addlabel(compactlabeltoordinal.java:116) at org.apache.lucene.facet.taxonomy.writercache.cl2o.cl2otaxonomywritercache.put(cl2otaxonomywritercache.java:84) at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.addtocache(directorytaxonomywriter.java:592) at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.addcategorydocument(directorytaxonomywriter.java:551) at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.internaladdcategory(directorytaxonomywriter.java:501) at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.internaladdcategory(directorytaxonomywriter.java:494) at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.addcategory(directorytaxonomywriter.java:468) at org.apache.lucene.facet.index.facetfields.addfields(facetfields.java:175) at net.domain.netstatindexer.indexjob.run(indexjob.java:73) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:722) here is an example data entry which appears when the exception occurs... location: nj localip: 10.1.200.187 remoteip: 41.161.197.166 localports: [443] connections: 1 times: [120] timestamp: 2013-06-09t12:51:00.000-07:00 states: [\"established\"] and here is the the code stripped down to provide an example of how i am handling the facet/doc code. doc.add(new textfield(\"location\", ehost[0], field.store.yes)); cats.add(new categorypath(\"location\", doc.get(\"location\"))); doc.add(new textfield(\"localip\", (string) stat.get(\"localip\"), field.store.yes)); cats.add(new categorypath(\"localip\", doc.get(\"localip\"))); doc.add(new textfield(\"remoteip\", (string) stat.get(\"remoteip\"), field.store.yes)); cats.add(new categorypath(\"remoteip\", doc.get(\"remoteip\"))); doc.add(new textfield(\"localports\", stringutils.join(stat.get(\"localports\"), \",\"), field.store.yes)); cats.add(new categorypath(\"localports\", doc.get(\"localports\"))); doc.add(new textfield(\"remoteports\", stringutils.join(stat.get(\"remoteports\"), \",\"), field.store.yes)); cats.add(new categorypath(\"remoteports\", doc.get(\"remoteports\"))); doc.add(new longfield(\"connections\", (long) stat.get(\"connections\"), field.store.yes)); cats.add(new categorypath(\"connections\", doc.get(\"connections\"))); doc.add(new textfield(\"times\", stringutils.join(stat.get(\"times\"), \",\"), field.store.yes)); cats.add(new categorypath(\"times\", doc.get(\"times\"))); doc.add(new textfield(\"timestamp\", (string) stat.get(\"timestamp\"), field.store.yes)); cats.add(new categorypath(\"timestamp\", doc.get(\"timestamp\"))); doc.add(new textfield(\"states\", stringutils.join(stat.get(\"states\"), \",\"), field.store.yes)); cats.add(new categorypath(\"states\", doc.get(\"states\"))); system.out.println(\"location: \"doc.get(\"location\")\" localip: \"doc.get(\"localip\")\" remoteip: \"doc.get(\"remoteip\")\" localports: \"doc.get(\"localports\")\" connections: \"doc.get(\"connections\")\" times: \"doc.get(\"times\")\" timestamp: \"doc.get(\"timestamp\")\" states: \"+doc.get(\"states\")); if (cats.size()!=0) { facetfields ff = new facetfields(main.twriter); ff.addfields(doc, cats); // <-- exception occurs here randomly }",
        "label": 43
    },
    {
        "text": "do not allow changing soft deletes field today we do not enforce an index to use a single soft-deletes field. a user can create an index with one soft-deletes field then open an iw with another field or add an index with a different soft-deletes field. this should not be allowed and reported the error to users as soon as possible.",
        "label": 46
    },
    {
        "text": "0xffff char is not a string terminator current trunk index.documentwriter uses \"\\uffff\" as a string terminator, but it should not to be for some reasons. \\uffff is not a terminator char itself and we can't handle a string that really contains \\uffff. and also, we can calculate the end char position in a character sequence from the string length that we already know. however, i agree with the usage for assertion, that \"\\uffff\" is placed after at the end of a string in a char sequence.",
        "label": 33
    },
    {
        "text": "illegalstateexception when trying mvn org apache lucene lucene core install hi everyone, i hope this is the right place to post this... i'm trying to get lucene files from this repository : http://repo1.maven.org/maven2 but i get this exception. did anyone already got this error ? d:\\.m2\\repository>mvn org.apache.lucene:lucene-core:3.0.0:install [info] scanning for projects... downloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/3.0.0/l ucene-core-3.0.0.pom downloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-parent/3.0.0 /lucene-parent-3.0.0.pom downloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/3.0.0/l ucene-core-3.0.0.jar [info] ------------------------------------------------------------------------ [error] fatal error [info] ------------------------------------------------------------------------ [info] the plugin descriptor for the plugin plugin [org.apache.lucene:lucene-cor e] was not found. please verify that the plugin jar d:\\.m2\\repository\\org\\apache \\lucene\\lucene-core\\3.0.0\\lucene-core-3.0.0.jar is intact. [info] ------------------------------------------------------------------------ [info] trace java.lang.illegalstateexception: the plugin descriptor for the plugin plugin [or g.apache.lucene:lucene-core] was not found. please verify that the plugin jar d: \\.m2\\repository\\org\\apache\\lucene\\lucene-core\\3.0.0\\lucene-core-3.0.0.jar is int act. at org.apache.maven.plugin.defaultpluginmanager.addplugin(defaultpluginm anager.java:360) at org.apache.maven.plugin.defaultpluginmanager.verifyversionedplugin(de faultpluginmanager.java:224) at org.apache.maven.plugin.defaultpluginmanager.verifyplugin(defaultplug inmanager.java:184) at org.apache.maven.plugin.defaultpluginmanager.loadplugindescriptor(def aultpluginmanager.java:1642) at org.apache.maven.lifecycle.defaultlifecycleexecutor.verifyplugin(defa ultlifecycleexecutor.java:1540) at org.apache.maven.lifecycle.defaultlifecycleexecutor.getmojodescriptor (defaultlifecycleexecutor.java:1851) at org.apache.maven.lifecycle.defaultlifecycleexecutor.segmenttasklistby aggregationneeds(defaultlifecycleexecutor.java:462) at org.apache.maven.lifecycle.defaultlifecycleexecutor.execute(defaultli fecycleexecutor.java:175) at org.apache.maven.defaultmaven.doexecute(defaultmaven.java:328) at org.apache.maven.defaultmaven.execute(defaultmaven.java:138) at org.apache.maven.cli.mavencli.main(mavencli.java:362) at org.apache.maven.cli.compat.compatiblemain.main(compatiblemain.java:6 0) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl. java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodacces sorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.codehaus.classworlds.launcher.launchenhanced(launcher.java:315) at org.codehaus.classworlds.launcher.launch(launcher.java:255) at org.codehaus.classworlds.launcher.mainwithexitcode(launcher.java:430) at org.codehaus.classworlds.launcher.main(launcher.java:375) [info] ------------------------------------------------------------------------ [info] total time: 7 seconds [info] finished at: sun jan 24 19:35:21 cet 2010 [info] final memory: 1m/15m [info] ------------------------------------------------------------------------",
        "label": 53
    },
    {
        "text": "forcemergedeletes does not merge soft deleted segments indexwriter#forcemergedeletes should merge segments having soft-deleted documents as hard-deleted documents if we configured \"softdeletesfield\" in an indexwriterconfig. attached is a failed test.",
        "label": 46
    },
    {
        "text": "support payload scoring for all spanqueries i need a way to have payloads influence the score of spanorquery's.",
        "label": 2
    },
    {
        "text": "generate jar containing test classes  the test classes are useful for writing unit tests for code external to the lucene project. it would be helpful to build a jar of these classes and publish them as a maven dependency.",
        "label": 47
    },
    {
        "text": "indexupgrader doesn't upgrade an index if it has zero segments indexupgrader uses merges to do its job. therefore, if you use it to upgrade an index with no segments, it will do nothing - it won't even update the version numbers in the segments file, meaning that later versions of lucene will fail to open the index, despite the fact that you \"upgraded\" it. the suggested workaround when this was raised on the mailing list in january seems to be to use filesystem magic to look at the files, figure out whether there are any segments, and write a new empty index if there are none. this sounds easy, but there are probably traps. for instance, there might be files in the directory which don't really belong to the index. earlier versions of lucene used to have a filenamefilter which was usable to distinguish one from the other, but that seems to have disappeared, making it less obvious how to do this. this issue is presumed to exist in 3.x as well, i just haven't encountered it yet because the only empty indices i have hit have been later versions.",
        "label": 53
    },
    {
        "text": "trunk tests hang deadlock testindexwriterwiththreads trunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:",
        "label": 46
    },
    {
        "text": "the repeats mechanism in sloppyphrasescorer is broken when doc has tokens at same position in lucene-736 we made fixes to sloppyphrasescorer, because it was matching docs that it shouldn't; but i think those changes caused it to fail to match docs that it should, specifically when the doc itself has tokens at the same position.",
        "label": 12
    },
    {
        "text": "more performance improvements for snowball i took a more serious look at snowball after lucene-2194. this gives greatly improved performance, but note it has some minor breaks to snowball internals: among.s becomes a char[] instead of a string snowballprogram.current becomes a char[] instead of a stringbuilder snowballprogram.eq_s(int, string) becomes eq_s(int, charsequence), so that eq_v(stringbuilder) doesnt need to create an extra string. same as the above with eq_s_b and eq_v_b replace_s(int, int, string) becomes replace_s(int, int, charsequence), so that stringbuilder-based slice and insertion methods don't need to create an extra string. all of these \"breaks\" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals. the performance improvement here depends heavily upon the snowball language in use, but its way more significant than lucene-2194.",
        "label": 40
    },
    {
        "text": "multiphrasequery has incorrect hashcode  implementation   leads to solr cache misses i found this while hunting for the cause of solr cache misses. the multiphrasequery class hashcode() implementation is non-deterministic. it uses termarrays.hashcode() in the computation. the contents of that arraylist are actually arrays themselves, which return there reference id as a hashcode instead of returning a hashcode which is based on the contents of the array. i would suggest an implementation involving the arrays.hashcode() method. i will try to submit a patch soon, off for today.",
        "label": 55
    },
    {
        "text": "token div exceeds length of provided text sized i have a doc which contains html codes. i want to strip html tags and make the test clear after then apply highlighter on the clear text . but highlighter throws an exceptions if i strip out the html characters , if i don't strip out , it works fine. it just confuses me at the moment i copy paste 3 thing here from the console as it may contain special characters which might cause the problem. 1 -) here is the html text <h2>starter</h2> <div id=\"tab1-content\" class=\"tabcontent selected\"> <div class=\"head\"></div> <div class=\"body\"> <div class=\"subject-header\">learning path: history</div> <h3>key question</h3> <p>did transport fuel the industrial revolution?</p> <h3>learning objective</h3> <ul> <li>to categorise points as for or against an argument</li> </ul> <p> <h3>what to do?</h3> <ul> <li>watch the clip: <em>transport fuelled the industrial revolution.</em></li> </ul> <p>the clips claims that transport fuelled the industrial revolution. some historians argue that the industrial revolution only happened because of developments in transport.</p> <ul> <li>read the statements below and decide which points are <em>for</em> and which points are <em>against</em> the argument that industry expanded in the 18th and 19th centuries because of developments in transport.</li> </ul> <ol type=\"a\"> <li>industry expanded because of inventions and the discovery of steam power.</li> <li>improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for.</li> <li>developments in transport allowed resources, such as coal from mines and cotton from america to come together to manufacture products.</li> <li>transport only developed because industry needed it. it was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry.</li> </ol> <p>now try to think of 2 more statements of your own.</p> </div> <div class=\"foot\"></div> </div> <h2>main activity</h2> <div id=\"tab2-content\" class=\"tabcontent\"> <div class=\"head\"></div> <div class=\"body\"><div class=\"subject-header\">learning path: history</div> <h3>learning objective</h3> <ul> <li>to select evidence to support points</li> </ul> <h3>what to do?</h3> <!--<ul> <li>watch the clip: <em>windmill and water mill</em></li> </ul>--> <ul><li>choose the 4 points that you think are most important - try to be balanced by having two <strong>for</strong> and two <strong>against</strong>.</li> <li>write one in each of the point boxes of the paragraphs on the sheet <a href=\"lp_history_industry_transport_ws1.html\" class=\"link-internal\">constructing a balanced argument</a>.</li></ul> <p>you might like to re write the points in your own words and use connectives to link the paragraphs.</p> <p>in history and in any argument, you need evidence to support your points.</p> <ul><li>find evidence from these sources and from your own knowledge to support each of your points:</li></ul> <ol> <li><a href=\"../servlet/link?template=vid&macro=setresource&resourceid=2044\" class=\"link-internal\">at a toll gate</a></li> <li><a href=\"../servlet/link?macro=setresource&template=vid&resourceid=2046\" class=\"link-internal\">canals</a></li> <li><a href=\"../servlet/link?macro=setresource&template=vid&resourceid=2043\" class=\"link-internal\">growing cities: traffic</a></li> <li><a href=\"../servlet/link?macro=setresource&template=vid&resourceid=2047\" class=\"link-internal\">impact of the railway</a> </li> <li><a href=\"../servlet/link?macro=setresource&template=vid&resourceid=2048\" class=\"link-internal\">sailing ships</a> </li> <li><a href=\"../servlet/link?macro=setresource&template=vid&resourceid=2050\" class=\"link-internal\">liverpool: capital of culture</a> </li> </ol> <p>try to be specific in your evidence - use named examples of places or people. use dates if you can.</p> </div> <div class=\"foot\"></div> </div> <h2>plenary</h2> <div id=\"tab3-content\" class=\"tabcontent\"> <div class=\"head\"></div> <div class=\"body\"><div class=\"subject-header\">learning path: history</div> <h3>learning objective</h3> <ul> <li>to judge which of the arguments is most valid</li> </ul> <h3>what to do?</h3> <!-- <ul> <li>watch the clip: <em>food of the rich</em></li> </ul>--> <p>in order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. having been through the evidence which point do you think is most important? why? is there more evidence? is the evidence more convincing?</p> <ul><li>in the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.</li></ul> </div> <div class=\"foot\"></div> </div> <h2>extension</h2> <div id=\"tab4-content\" class=\"tabcontent\"> <div class=\"head\"></div> <div class=\"body\"><div class=\"subject-header\">learning path: history</div> <h3>what to do?</h3> <p>watch the clip <em>stress in a ski resort</em></p> <p>new industries, such as tourism, can now be said to be fuelled by transport improvements.</p> <ul><li>search clipbank, using the related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.</li></ul> </div> <div class=\"foot\"></div> </div> 2-) here is the text after stripped html tags out starter learning path: history key question did transport fuel the industrial revolution? learning objective to categorise points as for or against an argument what to do? watch the clip: transport fuelled the industrial revolution. the clips claims that transport fuelled the industrial revolution. some historians argue that the industrial revolution only happened because of developments in transport. read the statements below and decide which points are for and which points are against the argument that industry expanded in the 18th and 19th centuries because of developments in transport. industry expanded because of inventions and the discovery of steam power. improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for. developments in transport allowed resources, such as coal from mines and cotton from america to come together to manufacture products. transport only developed because industry needed it. it was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry. now try to think of 2 more statements of your own. main activity learning path: history learning objective to select evidence to support points what to do? choose the 4 points that you think are most important - try to be balanced by having two for and two against . write one in each of the point boxes of the paragraphs on the sheet constructing a balanced argument . you might like to re write the points in your own words and use connectives to link the paragraphs. in history and in any argument, you need evidence to support your points. find evidence from these sources and from your own knowledge to support each of your points: at a toll gate canals growing cities: traffic impact of the railway sailing ships liverpool: capital of culture try to be specific in your evidence - use named examples of places or people. use dates if you can. plenary learning path: history learning objective to judge which of the arguments is most valid what to do? in order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. having been through the evidence which point do you think is most important? why? is there more evidence? is the evidence more convincing? in the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution. extension learning path: history what to do? watch the clip stress in a ski resort new industries, such as tourism, can now be said to be fuelled by transport improvements. search clipbank, using the related clip lists as well as the search function, to find examples from around the world of how transport has helped industry. 3-) here is the exception i get org.apache.lucene.search.highlight.invalidtokenoffsetsexception: token div exceeds length of provided text sized 4114 at org.apache.lucene.search.highlight.highlighter.getbesttextfragments(highlighter.java:228) at org.apache.lucene.search.highlight.highlighter.getbestfragments(highlighter.java:158) at org.apache.lucene.search.highlight.highlighter.getbestfragments(highlighter.java:462)",
        "label": 47
    },
    {
        "text": "add some ligatures  ff  fi  fl  ft  st  to isolatin1accentfilter isolatin1accentfilter removes common diacritics and some ligatures. this patch adds support for additional common ligatures: ff, fi, fl, ft, st.",
        "label": 15
    },
    {
        "text": "optimization of the direct packedints readers given that the initial focus for packedints readers was more on in-memory readers (for storing stuff like the mapping from old to new doc ids at merging time), i never spent time trying to optimize the direct readers although it could be beneficial now that they are used for disk-based doc values.",
        "label": 41
    },
    {
        "text": "poor performance race condition in fieldcacheimpl a race condition exists in fieldcacheimpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. the degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value. for the full discussion see the mailing list thread 'poor performance \"race condition\" in fieldsortedhitqueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717).",
        "label": 38
    },
    {
        "text": "add lusql project to  apache lucene   contributions  wiki page add lusql to the apache lucene - contributions page http://lucene.apache.org/java/2_9_0/contributions.html i am the author of lusql. i can supply any text needed. perhaps a new heading is needed to capture database/jdbc oriented lucene tools (there are others out there)?",
        "label": 33
    },
    {
        "text": "create enwiki indexable data as line per article rather than file per article create a line per article rather than a file. consume with indexlinefile task.",
        "label": 48
    },
    {
        "text": "allow rang faceting on any valuesource today rangeaccumulator assumes the ranges should be read from a numericdocvalues field. would be good if we can modify it, or introduce a new valuesourceaccumulator which allows you to range-facet on any valuesource, e.g. one that is generated from an expression.",
        "label": 43
    },
    {
        "text": "modify confusing javadoc for querynorm see http://markmail.org/message/arai6silfiktwcer the javadoc confuses me as well.",
        "label": 29
    },
    {
        "text": "parallelize tests the lucene tests can be parallelized to make for a faster testing system. this task from ant can be used: http://ant.apache.org/manual/coretasks/parallel.html previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669 notes from mike m.: i'd love to see a clean solution here (the tests are embarrassingly parallelizable, and we all have machines with good concurrency these days)... i have a rather hacked up solution now, that uses \"-dtestpackage=xxx\" to split the tests up. ideally i would be able to say \"use n threads\" and it'd do the right thing... like the -j flag to make.",
        "label": 40
    },
    {
        "text": "update overview example code see http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended api usage. also, double-check that the demo app works as documented.",
        "label": 29
    },
    {
        "text": "frenchanalyzer's tokenstream method does not honour the contract of analyzer in analyzer : /** creates a tokenstream which tokenizes all the text in the provided     reader.  default implementation forwards to tokenstream(reader) for      compatibility with older version.  override to allow analyzer to choose      strategy based on document and/or field.  must be able to handle null     field name for backward compatibility. */   public abstract tokenstream tokenstream(string fieldname, reader reader); and in frenchanalyzer public final tokenstream tokenstream(string fieldname, reader reader) {     if (fieldname == null) throw new illegalargumentexception(\"fieldname must not be null\");     if (reader == null) throw new illegalargumentexception(\"reader must not be null\");",
        "label": 40
    },
    {
        "text": "include geo3d package  along with lucene integration to make it useful i would like to explore contributing a geo3d package to lucene. this can be used in conjunction with lucene search, both for generating geohashes (via spatial4j) for complex geographic shapes, as well as limiting results resulting from those queries to those results within the exact shape in highly performant ways. the package uses 3d planar geometry to do its magic, which basically limits computation necessary to determine membership (once a shape has been initialized, of course) to only multiplications and additions, which makes it feasible to construct a performant boostsource-based filter for geographic shapes. the math is somewhat more involved when generating geohashes, but is still more than fast enough to do a good job.",
        "label": 10
    },
    {
        "text": "classic  flexible  and solr's standard lucene  query parsers  range queries should require   to   and accept to as range endpoints a post on the solr-user mailing list drew my attention to the fact that this is apparently a valid range query under the queryparser.jj grammer (both for the classic parser and the solr variant \u2013 i didn't check flexible)... [x z]       // parsed the same as [x to z] it's parsed as a valid range query even though there is no {{ to }} \u2013 even though there is nothing in the docs to suggest that the {{ to }} is intended to be optional. furthermore, in my experimenting i realized that how the grammer looks for the {{ to }} keyword seems to be a bit sloppy, making some range queries that should be easy to validate (because they are unambiguous) fail to parse... [to to z]     // fails [a to to]     // fails [a to \"to\"]   // works, but why should quoting be neccessary here? if the \"sloppy\" parsing behavior is intentional, then the docs should reflect that the {{ to }} is optional \u2013 but it seems like in general we should make these other unambiguous cases parse cleanly.",
        "label": 47
    },
    {
        "text": "testcontextquery testrandomcontextqueryscoring failure    [junit4] started j0 pid(8355@localhost).    [junit4] suite: org.apache.lucene.search.suggest.document.testcontextquery    [junit4]   2> note: reproduce with: ant test  -dtestcase=testcontextquery -dtests.method=testrandomcontextqueryscoring -dtests.seed=f3a3a7e94ac9cb6d -dtests.slow=true -dtests.locale=es_es -dtests.timezone=zulu -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   0.74s | testcontextquery.testrandomcontextqueryscoring <<<    [junit4]    > throwable #1: java.lang.assertionerror: expected: key:sugg_yafiszhkyq2 score:859398.0 context:evoyj6 actual: key:sugg_mfbt11 score:841758.0 context:evoyj6    [junit4]    > expected: \"sugg_yafiszhkyq2\"    [junit4]    >      got: \"sugg_mfbt11\"    [junit4]    >  at org.apache.lucene.search.suggest.document.testsuggestfield.assertsuggestions(testsuggestfield.java:608)    [junit4]    >  at org.apache.lucene.search.suggest.document.testcontextquery.testrandomcontextqueryscoring(testcontextquery.java:528)    [junit4]    >  at java.lang.thread.run(thread.java:745)throwable #2: java.lang.runtimeexception: mockdirectorywrapper: cannot close: there are still open files: {_0.cfs=1}    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:749)    [junit4]    >  at org.apache.lucene.search.suggest.document.testcontextquery.after(testcontextquery.java:56)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: java.lang.runtimeexception: unclosed indexinput: _0.cfs    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.addfilehandle(mockdirectorywrapper.java:624)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:668)    [junit4]    >  at org.apache.lucene.codecs.lucene50.lucene50compoundreader.<init>(lucene50compoundreader.java:71)    [junit4]    >  at org.apache.lucene.codecs.lucene50.lucene50compoundformat.getcompoundreader(lucene50compoundformat.java:71)    [junit4]    >  at org.apache.lucene.index.segmentcorereaders.<init>(segmentcorereaders.java:93)    [junit4]    >  at org.apache.lucene.index.segmentreader.<init>(segmentreader.java:65)    [junit4]    >  at org.apache.lucene.index.readersandupdates.getreader(readersandupdates.java:132)    [junit4]    >  at org.apache.lucene.index.readersandupdates.getreadonlyclone(readersandupdates.java:184)    [junit4]    >  at org.apache.lucene.index.standarddirectoryreader.open(standarddirectoryreader.java:99)    [junit4]    >  at org.apache.lucene.index.indexwriter.getreader(indexwriter.java:433)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.getreader(randomindexwriter.java:342)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.getreader(randomindexwriter.java:279)    [junit4]    >  at org.apache.lucene.search.suggest.document.testcontextquery.testrandomcontextqueryscoring(testcontextquery.java:521)    [junit4]    >  ... 28 more    [junit4]   2> note: test params are: codec=asserting(lucene50), sim=randomsimilarityprovider(querynorm=false,coord=crazy): {suggest_field=dfr gbz(0.3)}, locale=es_es, timezone=zulu    [junit4]   2> note: linux 3.13.0-46-generic amd64/oracle corporation 1.8.0_40 (64-bit)/cpus=8,threads=1,free=388652544,total=504889344    [junit4]   2> note: all tests run in this jvm: [testcontextquery]    [junit4] completed [1/1] in 1.14s, 1 test, 1 error <<< failures!",
        "label": 5
    },
    {
        "text": "similarity java javadocs and simplifications for as part of adding additional scoring systems to lucene, we made a lower-level similarity and the existing stuff became e.g. tfidfsimilarity which extends it. however, i always feel bad about the complexity introduced here (though i do feel there are some \"excuses\", that its a difficult challenge). in order to try to mitigate this, we also exposed an easier api (similaritybase) on top of it that makes some assumptions (and trades off some performance) to try to provide something consumable for e.g. experiments. still, we can cleanup a few things with the low-level api: fix outdated documentation and shoot for better/clearer naming etc.",
        "label": 40
    },
    {
        "text": "improve random testing we have quite a few random tests, but there's no way to \"crank\" them. the idea here is to add a multiplier which can be increased by a sysprop. for example, we could set this to something higher than 1 for hudson.",
        "label": 33
    },
    {
        "text": "geopath behavior with identical points geopath has the current behavior: when provided a path with two consecutive identical points: in all cases it generates an illegalargumentexception because it tries to build a plane with those two points. when provided a path with two consecutive numerical identical points: in case of geostandardpath it throws an illegalargumentexception because the path is too short. the case of geodegeneratepath is more complicated as it builds the path but the plane can be bogus. in some cases points on the other side of the world can be \"in set\". i think the factory should filter out these points, in the same way it is done for geopolygon. if this is not the desired behavior then the factory  should throw a consistent illegalargumentexception in all cases.",
        "label": 19
    },
    {
        "text": "convert lucene core tests over to a simple mockqueryparser most tests use lucene core's queryparser for convenience. we want to consolidate it into a qp module which we can't have as a dependency. we should add a simple mockqueryparser which does string.split() on the query string, analyzers the terms and builds a booleanquery if necessary. any more complex queries (such as phrases) should be done programmatically.",
        "label": 7
    },
    {
        "text": "two ancient classes renamed to be less peculiar   testhelper and  testutil _testutil and _testhelper begin with _ for historical reasons that don't apply any longer. lets eliminate those _'s.",
        "label": 6
    },
    {
        "text": "replace maven pom templates with full poms  and change documentation accordingly the current maven pom templates only contain dependency information, the bare bones necessary for uploading artifacts to the maven repository. the full maven poms in the attached patch include the information necessary to run a multi-module maven build, in addition to serving the same purpose as the current pom templates. several dependencies are not available through public maven repositories. a profile in the top-level pom can be activated to install these dependencies from the various lib/ directories into your local repository. from the top-level directory: mvn -n -pbootstrap install once these non-maven dependencies have been installed, to run all lucene/solr tests via maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run: mvn install when one lucene/solr module depends on another, the dependency is declared on the artifact(s) produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the build/ directory, so you must run mvn install on the other module before its changes are visible to the module that depends on it. to create all the artifacts without running tests: mvn -dskiptests install i almost always include the clean phase when i do a build, e.g.: mvn -dskiptests clean install",
        "label": 47
    },
    {
        "text": "spellchecker  suggest mode  support this is a spin-off from solr-2585. currently o.a.l.s.s.spellchecker and o.a.l.s.s.directspellchecker support two \"suggest modes\": 1. suggest for terms that are not in the index. 2. suggest \"more popular\" terms. this issue is to add a third suggest mode: 3. suggest always. this will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions. see solr-2585 for a full discussion. note that o.a.l.s.s.spellchecker already can support this functionality, if the user passes in a null term & indexreader. this, however, is not intutive. o.a.l.s.s.directspellchecker currently has no support for this third suggest mode.",
        "label": 40
    },
    {
        "text": "self comparison bug in geocomplexpolygon equals method geocomplexpolygon.equals method checks equality with own testpoint1 field instead of the other.testpoint1.",
        "label": 25
    },
    {
        "text": "smoketestrelease py's maven checker needs to switch from svn to git the checkmaven function in the smoke tester seems to be loading known branches from svn to locate the branch currently being released and then crawling for pom.xml.template files from the svn server. we need to switch this to crawling git instead, but i'm not too familiar with what's happening here ... maybe steve rowe can help?",
        "label": 47
    },
    {
        "text": "remove custom encoding support in greek russian analyzers the greek and russian analyzers support custom encodings such as koi-8, they define things like lowercase and tokenization for these. i think that analyzers should support unicode and that conversion/handling of other charsets belongs somewhere else. i would like to deprecate/remove the support for these other encodings.",
        "label": 40
    },
    {
        "text": "facetsaccumulator set incorrect value for facetresult numvaliddescendants this is cheap to compute, since the topkfrh already must visit all non-zero-count ords under the facetrequest.categorypath. this can be useful to a front end, eg to know whether to present a \"more...\" under that dimension or not, whether to use a suggester like linkedin's facet ui, etc.",
        "label": 33
    },
    {
        "text": "migrate solr analysis factories to analyzers module in lucene-2413 all tokenstreams were consolidated into the analyzers module. this is a good step, but i think the next step is to put the solr factories into the analyzers module, too. this would make analyzers artifacts plugins to both lucene and solr, with benefits such as: users could use the old analyzers module with solr, too. this is a good step to use real library versions instead of version for backwards compat. analyzers modules such as smartcn and icu, that aren't currently available to solr users due to large file sizes or dependencies, would be simple optional plugins to solr and easily available to users that want them. rough sketch in this thread: http://www.lucidimagination.com/search/document/3465a0e55ba94d58/solr_and_analyzers_module practically, i havent looked much and don't really have a plan for how this will work yet, so ideas are very welcome.",
        "label": 53
    },
    {
        "text": "add facetscollector based on sortedsetdocvalues recently (lucene-4765) we added multi-valued docvalues field (sortedsetdocvaluesfield), and this can be used for faceting in solr (solr-4490). i think we should also add support in the facet module? it'd be an option with different tradeoffs. eg, it wouldn't require the taxonomy index, since the main index handles label/ord resolving. there are at least two possible approaches: on every reopen, build the seg -> global ord map, and then on every collect, get the seg ord, map it to the global ord space, and increment counts. this adds cost during reopen in proportion to number of unique terms ... on every collect, increment counts based on the seg ords, and then do a \"merge\" in the end just like distributed faceting does. the first approach is much easier so i built a quick prototype using that. the prototype does the counting, but it does not do the top k facets gathering in the end, and it doesn't \"know\" parent/child ord relationships, so there's tons more to do before this is real. i also was unsure how to properly integrate it since the existing classes seem to expect that you use a taxonomy index to resolve ords. i ran a quick performance test. base = trunk except i disabled the \"compute top-k\" in facetsaccumulator to make the comparison fair; comp = using the prototype collector in the patch:                     task    qps base      stddev    qps comp      stddev                pct diff                orhighlow       18.79      (2.5%)       14.36      (3.3%)  -23.6% ( -28% -  -18%)                 highterm       21.58      (2.4%)       16.53      (3.7%)  -23.4% ( -28% -  -17%)                orhighmed       18.20      (2.5%)       13.99      (3.3%)  -23.2% ( -28% -  -17%)                  prefix3       14.37      (1.5%)       11.62      (3.5%)  -19.1% ( -23% -  -14%)                  lowterm      130.80      (1.6%)      106.95      (2.4%)  -18.2% ( -21% -  -14%)               orhighhigh        9.60      (2.6%)        7.88      (3.5%)  -17.9% ( -23% -  -12%)              andhighhigh       24.61      (0.7%)       20.74      (1.9%)  -15.7% ( -18% -  -13%)                   fuzzy1       49.40      (2.5%)       43.48      (1.9%)  -12.0% ( -15% -   -7%)          medsloppyphrase       27.06      (1.6%)       23.95      (2.3%)  -11.5% ( -15% -   -7%)                  medterm       51.43      (2.0%)       46.21      (2.7%)  -10.2% ( -14% -   -5%)                   intnrq        4.02      (1.6%)        3.63      (4.0%)   -9.7% ( -15% -   -4%)                 wildcard       29.14      (1.5%)       26.46      (2.5%)   -9.2% ( -13% -   -5%)         highsloppyphrase        0.92      (4.5%)        0.87      (5.8%)   -5.4% ( -15% -    5%)              medspannear       29.51      (2.5%)       27.94      (2.2%)   -5.3% (  -9% -    0%)             highspannear        3.55      (2.4%)        3.38      (2.0%)   -4.9% (  -9% -    0%)               andhighmed      108.34      (0.9%)      104.55      (1.1%)   -3.5% (  -5% -   -1%)          lowsloppyphrase       20.50      (2.0%)       20.09      (4.2%)   -2.0% (  -8% -    4%)                lowphrase       21.60      (6.0%)       21.26      (5.1%)   -1.6% ( -11% -   10%)                   fuzzy2       53.16      (3.9%)       52.40      (2.7%)   -1.4% (  -7% -    5%)              lowspannear        8.42      (3.2%)        8.45      (3.0%)    0.3% (  -5% -    6%)                  respell       45.17      (4.3%)       45.38      (4.4%)    0.5% (  -7% -    9%)                medphrase      113.93      (5.8%)      115.02      (4.9%)    1.0% (  -9% -   12%)               andhighlow      596.42      (2.5%)      617.12      (2.8%)    3.5% (  -1% -    8%)               highphrase       17.30     (10.5%)       18.36      (9.1%)    6.2% ( -12% -   28%) i'm impressed that this approach is only ~24% slower in the worst case! i think this means it's a good option to make available? yes it has downsides (nrt reopen more costly, small added ram usage, slightly slower faceting), but it's also simpler (no taxo index to manage).",
        "label": 33
    },
    {
        "text": "smartcn analyzer throw nullpointer exception when the length of analysed text over that's all because of org.apache.lucene.analysis.cn.smart.hhmm.seggraph's makeindex() method: public list<segtoken> makeindex() { list<segtoken> result = new arraylist<segtoken>(); int s = -1, count = 0, size = tokenlisttable.size(); list<segtoken> tokenlist; short index = 0; while (count < size) { if (isstartexist(s)) { tokenlist = tokenlisttable.get(s); for (segtoken st : tokenlist) { st.index = index; result.add(st); index++; } count++; } s++; } return result; } here 'short index = 0;' should be 'int index = 0;'. and that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author xiaopinggao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/seggraph.java",
        "label": 40
    },
    {
        "text": "testterminfosreaderindex failing  always reproducible  always fails on branch (use reproduce string below): git clone --depth 1 -b rr git@github.com:dweiss/lucene_solr.git [junit4] running org.apache.lucene.codecs.lucene3x.testterminfosreaderindex [junit4] failure 0.04s j0 | testterminfosreaderindex.testseekenum [junit4]    > throwable #1: java.lang.assertionerror: expected:<field9:z91ob3wozm6d> but was:<:> [junit4]    >  at __randomizedtesting.seedinfo.seed([c7597dfbbe0b3d7d:c6d9cedd0700aaff]:0) [junit4]    >  at org.junit.assert.fail(assert.java:93) [junit4]    >  at org.junit.assert.failnotequals(assert.java:647) [junit4]    >  at org.junit.assert.assertequals(assert.java:128) [junit4]    >  at org.junit.assert.assertequals(assert.java:147) [junit4]    >  at org.apache.lucene.codecs.lucene3x.testterminfosreaderindex.testseekenum(testterminfosreaderindex.java:137) [junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) [junit4]    >  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) [junit4]    >  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) [junit4]    >  at java.lang.reflect.method.invoke(method.java:597) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1766) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$1000(randomizedrunner.java:141) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:728) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:789) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:803) [junit4]    >  at org.apache.lucene.util.lucenetestcase$subclasssetupteardownrule$1.evaluate(lucenetestcase.java:744) [junit4]    >  at org.apache.lucene.util.lucenetestcase$internalsetupteardownrule$1.evaluate(lucenetestcase.java:636) [junit4]    >  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:22) [junit4]    >  at org.apache.lucene.util.lucenetestcase$testresultinterceptorrule$1.evaluate(lucenetestcase.java:550) [junit4]    >  at org.apache.lucene.util.lucenetestcase$rememberthreadrule$1.evaluate(lucenetestcase.java:600) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:735) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$600(randomizedrunner.java:141) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:586) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:605) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:641) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:652) [junit4]    >  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:22) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:533) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:141) [junit4]    >  at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:479) [junit4]    >  [junit4]   2> note: reproduce with: ant test -dtests.filter=*.testterminfosreaderindex -dtests.filter.method=testseekenum -drt.seed=c7597dfbbe0b3d7d -dargs=\"-dfile.encoding=utf-8\" [junit4]   2> [junit4]    > (@afterclass output) [junit4]   2> note: test params are: codec=appending, sim=defaultsimilarity, locale=en, timezone=atlantic/stanley [junit4]   2> note: all tests run in this jvm: [junit4]   2> [testlock, testfileswitchdirectory, testwildcardrandom, testversioncomparator, testtermdocperf, testbitvector, testparalleltermenum, testsimplesearchequivalence, testnumericrangequery64, testsort, testiscurrent, testtoken, testintblockcodec, testdocumentswriterdeletequeue, testpagedbytes, testthreadedforcemerge, testomittf, testsegmenttermenum, testindexwriterconfig, testcheckindex, testtermvectorswriter, testnumerictokenstream, testsearchafter, testregexpquery, inbeforeclass, inafterclass, intestmethod, nonstringproperties, testindexwritermergepolicy, testvirtualmethod, testfieldcache, testsurrogates, testsegmenttermdocs, testmultivaluednumericrangequery, testbasicoperations, testcodecs, testdatesort, testpositivescoresonlycollector, testbooleanquery, testindexinput, testminimize, testnumericrangequery32, testboolean2, testsloppyphrasequery, testnodeletionpolicy, testfieldcachetermsfilter, testrandomstoredfields, testdocboost, testtransactionrollback, testunicodeutil, testindexwriterlockrelease, testutf32toutf8, testfixedbitset, testdoublebarrellrucache, testtimelimitingcollector, testspanfirstquery, testdirectory, testspansadvanced2, testconcurrentmergescheduler, testindexwriterexceptions, testdocvalues, testcustomnorms, testfieldvaluefilter, testtermvectors, testterminfosreaderindex] [junit4]   2> note: linux 2.6.32-38-server amd64/sun microsystems inc. 1.6.0_20 (64-bit)/cpus=4,threads=1,free=100102360,total=243859456 [junit4]   2> ",
        "label": 33
    },
    {
        "text": "isolatin1accentfilter a bit slow the isolatin1accentfilter is a bit slow giving 300+ ms responses when used in a highligher for output responses. patch to follow",
        "label": 33
    },
    {
        "text": "fastvectorhighlighter  add a fragmentbuilder to return entire field contents in highlightrer, there is a nullfragmenter. there is a requirement its counterpart in fastvectorhighlighter.",
        "label": 26
    },
    {
        "text": "standarddirectoryreader  initialcapacity  tweaks proposed patch against trunk to follow",
        "label": 9
    },
    {
        "text": " intersects  spatial query returns polygons it shouldn't full description, including sample schema and data, can be found at http://lucene.472066.n3.nabble.com/quot-intersects-quot-spatial-query-returns-polygons-it-shouldn-t-td4008646.html",
        "label": 10
    },
    {
        "text": "simplespanfragmenter fails to start a new fragment simplespanfragmenter fails to identify a new fragment when there is more than one stop word after a span is detected. this problem can be observed when the query contains a phrasequery. the problem is that the span extends toward the end of the tokengroup. this is because waitforprops = positionspans.get.end + 1; and position += posincatt.getpositionincrement(); this generates a value of position greater than the value of waitforprops and (waitforpos == position) never matches. simplespanfragmenter.java   public boolean isnewfragment() {     position += posincatt.getpositionincrement();     if (waitforpos == position) {       waitforpos = -1;     } else if (waitforpos != -1) {       return false;     }     weightedspanterm wspanterm = queryscorer.getweightedspanterm(termatt.term());     if (wspanterm != null) {       list<positionspan> positionspans = wspanterm.getpositionspans();       for (int i = 0; i < positionspans.size(); i++) {         if (positionspans.get(i).start == position) {           waitforpos = positionspans.get(i).end + 1;           break;         }       }     }    ... an example is provided in the test case for the following document and the query \"all tokens\" followed by the words of a. document \"attribute instances are reused for all tokens of a document. thus, a tokenstream/-filter needs to update the appropriate attribute(s) in incrementtoken(). the consumer, commonly the lucene indexer, consumes the data in the attributes and then calls incrementtoken() again until it retuns false, which indicates that the end of the stream was reached. this means that in each call of incrementtoken() a tokenstream/-filter can safely overwrite the data in the attribute instances.\" highlightertest.java  public void testsimplespanfragmenter() throws exception {     ...     dosearching(\"\\\"all tokens\\\"\");     maxnumfragmentsrequired = 2;          scorer = new queryscorer(query, field_name);     highlighter = new highlighter(this, scorer);     for (int i = 0; i < hits.totalhits; i++) {       string text = searcher.doc(hits.scoredocs[i].doc).get(field_name);       tokenstream tokenstream = analyzer.tokenstream(field_name, new stringreader(text));       highlighter.settextfragmenter(new simplespanfragmenter(scorer, 20));       string result = highlighter.getbestfragments(tokenstream, text,           maxnumfragmentsrequired, \"...\");       system.out.println(\"\\t\" + result);     }   } result are reused for <b>all</b> <b>tokens</b> of a document. thus, a tokenstream/-filter needs to update the appropriate attribute(s) in incrementtoken(). the consumer, commonly the lucene indexer, consumes the data in the attributes and then calls incrementtoken() again until it retuns false, which indicates that the end of the stream was reached. this means that in each call of incrementtoken() a tokenstream/-filter can safely overwrite the data in the attribute instances. expected result for <b>all</b> <b>tokens</b> of a document",
        "label": 10
    },
    {
        "text": "move valuesource and functionvalues under core  spinoff from lucene-5298: valuesource and functionvalues are abstract apis which exist under the queries/ module. that causes any module which wants to depend on these apis (but not necessarily on any of their actual implementations!), to depend on the queries/ module. if we move these apis under core/, we can eliminate these dependencies and add some mock impls for testing purposes. quoting robert from lucene-5298: we should eliminate the suggest/ dependencies on expressions and queries, the expressions/ on queries, the grouping/ dependency on queries, the spatial/ dependency on queries, its a mess. to add to that list, facet/ should not depend on queries too.",
        "label": 2
    },
    {
        "text": "knearestneighborclassifier not taking in consideration class ranking currently the knn classifier assign the score for a classificationresult, based only on the frequency of the class in the top k results. this is conceptually a simplification. actually the ranking must take a part. if not this can happen : top 4 1) class1 2) class1 3) class2 4) class2 as a result of this top 4 , both the classes will have the same score. but the expected result is that class1 has a better score, as the mlt score the documents accordingly.",
        "label": 50
    },
    {
        "text": "wildcardquery may has memory leak data 800g, records 15*10000*10000. one search thread. content:??? content:* content:*1 content:*2 content:*3 jvm heap=96g, but the jvm memusage over 130g? run more wildcard, use memory more.... does luence search/index use a lot of directmemory or native memory? i use -xx:maxdirectmemorysize=4g, it does nothing better. thanks.",
        "label": 53
    },
    {
        "text": "httpreplicator does not properly handle server failures when replicationclient.updatenow() using an httpreplicator encounters a server error (like status code 500), it throws a runtime exception instead of an ioexception. furthermore, it does not close the httpclient it used, which leads to an error if a basicclientconnectionmanager is used",
        "label": 43
    },
    {
        "text": "add test to check maven artifacts and their poms as release manager it is hard to find out if the maven artifacts work correct. it would be good to have an ant task that executes maven with a .pom file that requires all contrib/core artifacts (or one for each contrib) that \"downloads\" the artifacts from the local dist/maven folder and builds that test project. this would require maven to execute the build script. also it should pass the ${version} ant property to this pom.xml",
        "label": 47
    },
    {
        "text": "disjunctionsumscorer should not call  score on sub scorers until consumer calls  score spinoff from java-user thread \"question about scorer.freq()\" from koji... booleanscorer2 uses disjunctionsumscorer to score only-should-clause boolean queries. but, this scorer does too much work for collectors that never call .score, because it scores while it's matching. it should only call .score on the subs when the caller calls its .score. this also has the side effect of messing up advanced collectors that gather the freq() of the subs (using lucene-2590).",
        "label": 33
    },
    {
        "text": "improve how indexwriter flushes deletes against existing segments indexwriter buffers up all deletes (by term and query) and only applies them if 1) commit or nrt getreader() is called, or 2) a merge is about to kickoff. we do this because, for a large index, it's very costly to open a segmentreader for every segment in the index. so we defer as long as we can. we do it just before merge so that the merge can eliminate the deleted docs. but, most merges are small, yet in a big index we apply deletes to all of the segments, which is really very wasteful. instead, we should only apply the buffered deletes to the segments that are about to be merged, and keep the buffer around for the remaining segments. i think it's not so hard to do; we'd have to have generations of pending deletions, because the newly merged segment doesn't need the same buffered deletions applied again. so every time a merge kicks off, we pinch off the current set of buffered deletions, open a new set (the next generation), and record which segment was created as of which generation. this should be a very sizable gain for large indices that mix deletes, though, less so in flex since opening the terms index is much faster.",
        "label": 33
    },
    {
        "text": "change config files using lucene current to use latest there are a lot of config files still referring to lucene_current. separate issue from lucene-5900 to keep the changes there visible, without drowning them in an enormous diff.",
        "label": 41
    },
    {
        "text": "empty kuromoji user dictionary   npe kuromoji user dictionary takes reader and allows for comments and other lines to be ignored. but if its \"empty\" in the sense of no actual entries, the returned fst will be null, and it will throw a confusing npe. japanesetokenizer and japaneseanalyzer apis already treat null userdictionary as having none at all, so i think the best fix is to fix the userdictionary api from userdictionary(reader) to userdictionary.open(reader) or similar, and return null if the fst is empty.",
        "label": 8
    },
    {
        "text": "charreader should delegate reset mark marksupported the final class charreader should delegate reset/mark/marksupported to its wrapped reader. otherwise clients will get \"reset() not supported\" exception.",
        "label": 53
    },
    {
        "text": "lucene60pointswriter has a buggy close method this will leak a file handle on e.g. disk full or other exceptions   @override   public void close() throws ioexception {     if (closed == false) {       codecutil.writefooter(dataout); // something bad happens       dataout.close(); i'm not a fan of these complex close methods, should we add a finish() to the codec api or similar?",
        "label": 33
    },
    {
        "text": "add link to irc channel  lucene on the website we should add a link to #lucene irc channel on chat.freenode.org.",
        "label": 33
    },
    {
        "text": "testindexwriterforcemerge still unreliable in nightly discovered by ryan beasting (trunk): ant test -dtestcase=testindexwriterforcemerge -dtests.method=testforcemergetempspaceusage -dtests.seed=dc9adb74850a581b -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.locale=sr__#latn -dtests.timezone=indian/chagos -dtests.asserts=true -dtests.file.encoding=us-ascii   [junit4]   2> note: reproduce with: ant test  -dtestcase=testindexwriterforcemerge -dtests.method=testforcemergetempspaceusage -dtests.seed=dc9adb74850a581b -dtests.multiplier=2 -dtests.nightly=true -dtests.slow=true -dtests.locale=sr__#latn -dtests.timezone=indian/chagos -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 1.20s | testindexwriterforcemerge.testforcemergetempspaceusage <<<    [junit4]    > throwable #1: java.lang.assertionerror: forcemerge used too much temporary space: starting usage was 291570 bytes; final usage was 262469 bytes; max temp usage was 1079501 but should have been 874710 (= 3x starting usage), before=    [junit4]    > _u.scf              146329    [junit4]    > _u.si               635    [junit4]    >  |- (inside compound file) _u.fld              2214    [junit4]    >  |- (inside compound file) _u.inf              392    [junit4]    >  |- (inside compound file) _u.len              2381    [junit4]    >  |- (inside compound file) _u.pst              36758    [junit4]    >  |- (inside compound file) _u.vec              104144    [junit4]    > _s.pst              1338    [junit4]    > _s.inf              392    [junit4]    > _s.fld              94    [junit4]    > _s.len              221    [junit4]    > _s.vec              3744    [junit4]    > _s.si               624    [junit4]    > _t.fld              94    [junit4]    > _t.len              221    [junit4]    > _t.pst              1338    [junit4]    > _t.inf              392    [junit4]    > _t.vec              3744    [junit4]    > _t.si               624    [junit4]    > _v.fld              94    [junit4]    > _v.pst              1338    [junit4]    > _v.inf              392    [junit4]    > _v.vec              3744    [junit4]    > _v.si               624    [junit4]    > _v.len              221    [junit4]    > _w.len              221    [junit4]    > _w.pst              1338    [junit4]    > _w.inf              392    [junit4]    > _w.fld              94    [junit4]    > _w.si               624    [junit4]    > _w.vec              3744    [junit4]    > _x.vec              3744    [junit4]    > _x.inf              392    [junit4]    > _x.pst              1338    [junit4]    > _x.fld              94    [junit4]    > _x.si               624    [junit4]    > _x.len              221    [junit4]    > _y.fld              94    [junit4]    > _y.pst              1338    [junit4]    > _y.inf              392    [junit4]    > _y.si               624    [junit4]    > _y.vec              3744    [junit4]    > _y.len              221    [junit4]    > _z.fld              94    [junit4]    > _z.pst              1338    [junit4]    > _z.inf              392    [junit4]    > _z.len              221    [junit4]    > _z.vec              3744    [junit4]    > _z.si               624    [junit4]    > _10.si              630    [junit4]    > _10.fld             94    [junit4]    > _10.pst             1338    [junit4]    > _10.inf             392    [junit4]    > _10.vec             3744    [junit4]    > _10.len             221    [junit4]    > _11.len             221    [junit4]    > _11.si              630    [junit4]    > _11.vec             3744    [junit4]    > _11.pst             1338    [junit4]    > _11.inf             392    [junit4]    > _11.fld             94    [junit4]    > _12.vec             3744    [junit4]    > _12.si              630    [junit4]    > _12.len             221    [junit4]    > _12.fld             94    [junit4]    > _12.pst             1338    [junit4]    > _12.inf             392    [junit4]    > _13.fld             94    [junit4]    > _13.vec             3744    [junit4]    > _13.si              630    [junit4]    > _13.pst             1338    [junit4]    > _13.inf             392    [junit4]    > _13.len             221    [junit4]    > _14.fld             94    [junit4]    > _14.pst             1338    [junit4]    > _14.inf             392    [junit4]    > _14.si              630    [junit4]    > _14.vec             3744    [junit4]    > _14.len             221    [junit4]    > _15.len             221    [junit4]    > _15.vec             3744    [junit4]    > _15.si              630    [junit4]    > _15.pst             1338    [junit4]    > _15.inf             392    [junit4]    > _15.fld             94    [junit4]    > _16.vec             3744    [junit4]    > _16.len             221    [junit4]    > _16.fld             94    [junit4]    > _16.si              630    [junit4]    > _16.pst             1338    [junit4]    > _16.inf             392    [junit4]    > _17.vec             3744    [junit4]    > _17.pst             1338    [junit4]    > _17.inf             392    [junit4]    > _17.len             221    [junit4]    > _17.si              630    [junit4]    > _17.fld             94    [junit4]    > _18.pst             1338    [junit4]    > _18.inf             392    [junit4]    > _18.len             221    [junit4]    > _18.vec             3744    [junit4]    > _18.si              630    [junit4]    > _18.fld             94    [junit4]    > _19.fld             94    [junit4]    > _19.si              630    [junit4]    > _19.len             221    [junit4]    > _19.vec             3744    [junit4]    > _19.pst             1338    [junit4]    > _19.inf             392    [junit4]    > _1a.fld             94    [junit4]    > _1a.pst             1338    [junit4]    > _1a.inf             392    [junit4]    > _1a.len             221    [junit4]    > _1a.vec             3744    [junit4]    > _1a.si              630    [junit4]    > _1b.fld             94    [junit4]    > _1b.si              630    [junit4]    > _1b.pst             1338    [junit4]    > _1b.inf             392    [junit4]    > _1b.vec             3744    [junit4]    > _1b.len             221    [junit4]    > _1c.vec             3744    [junit4]    > _1c.pst             1338    [junit4]    > _1c.inf             392    [junit4]    > _1c.fld             94    [junit4]    > _1c.len             221    [junit4]    > _1c.si              630    [junit4]    > _1d.pst             1338    [junit4]    > _1d.inf             392    [junit4]    > _1d.fld             94    [junit4]    > _1d.len             221    [junit4]    > _1d.vec             3744    [junit4]    > _1d.si              630    [junit4]    > _1e.fld             94    [junit4]    > _1e.pst             1338    [junit4]    > _1e.inf             392    [junit4]    > _1e.len             221    [junit4]    > _1e.vec             3744    [junit4]    > _1e.si              630    [junit4]    > _1f.fld             40    [junit4]    > _1f.pst             195    [junit4]    > _1f.inf             392    [junit4]    > _1f.len             149    [junit4]    > _1f.vec             405    [junit4]    > _1f.si              629    [junit4]    > after=    [junit4]    > _1j.si              635    [junit4]    > _1j.scf             261701    [junit4]    >  |- (inside compound file) _1j.fld             3982    [junit4]    >  |- (inside compound file) _1j.inf             392    [junit4]    >  |- (inside compound file) _1j.len             4149    [junit4]    >  |- (inside compound file) _1j.pst             66151    [junit4]    >  |- (inside compound file) _1j.vec             186577    [junit4]    >  at __randomizedtesting.seedinfo.seed([dc9adb74850a581b:c6581887eb1abbdc]:0)    [junit4]    >  at org.apache.lucene.index.testindexwriterforcemerge.testforcemergetempspaceusage(testindexwriterforcemerge.java:199)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=simpletext, sim=randomsimilarityprovider(querynorm=false,coord=no): {id=ib spl-d2, content=dfr i(f)b1}, locale=sr__#latn, timezone=indian/chagos    [junit4]   2> note: linux 3.13.0-45-generic amd64/oracle corporation 1.8.0_40-ea (64-bit)/cpus=8,threads=1,free=208092000,total=253231104    [junit4]   2> note: all tests run in this jvm: [testindexwriterforcemerge]    [junit4] completed in 1.54s, 1 test, 1 failure <<< failures!",
        "label": 33
    },
    {
        "text": "nrt support for file systems that do no have delete on last close or cannot delete while referenced semantics  see solr-5693 and our hdfs support - for something like hdfs to work with nrt, we need an ability for near realtime readers to hold references to their files to prevent deletes.",
        "label": 29
    },
    {
        "text": "storing shapes shouldn't be strategy dependent the logic for storing shape representations seems to be different for each strategy. the prefixtreestrategy impls store the shape in wkt, which is nice if you're using wkt but not much help if you're not. bboxstrategy doesn't actually store the shape itself, but a representation of the bounding box. twodoubles seems to follow the prefixtreestrategy approach, which is surprising since it only indexes points and they could be stored without using wkt. i think we need to consider what storing a shape means. if we want to store the shape itself, then that logic should be standardised and done outside of the strategys since it is not really related to them. if we want to store the terms being used by the strategys to make shapes queryable, then we need to change the logic in the strategys to actually do this.",
        "label": 10
    },
    {
        "text": "intermittent failures of testtimelimitedcollector testtimeoutmultithreaded in nightly tests occasionly testtimelimitedcollector.testtimeoutmultithreaded fails. e.g. with this output:    [junit] ------------- standard error -----------------    [junit] exception in thread \"thread-97\" junit.framework.assertionfailederror: no hits found!    [junit]     at junit.framework.assert.fail(assert.java:47)    [junit]     at junit.framework.assert.asserttrue(assert.java:20)    [junit]     at org.apache.lucene.search.testtimelimitedcollector.dotesttimeout(testtimelimitedcollector.java:152)    [junit]     at org.apache.lucene.search.testtimelimitedcollector.access$100(testtimelimitedcollector.java:38)    [junit]     at org.apache.lucene.search.testtimelimitedcollector$1.run(testtimelimitedcollector.java:231)    [junit] exception in thread \"thread-85\" junit.framework.assertionfailederror: no hits found!    [junit]     at junit.framework.assert.fail(assert.java:47)    [junit]     at junit.framework.assert.asserttrue(assert.java:20)    [junit]     at org.apache.lucene.search.testtimelimitedcollector.dotesttimeout(testtimelimitedcollector.java:152)    [junit]     at org.apache.lucene.search.testtimelimitedcollector.access$100(testtimelimitedcollector.java:38)    [junit]     at org.apache.lucene.search.testtimelimitedcollector$1.run(testtimelimitedcollector.java:231)    [junit] ------------- ---------------- ---------------    [junit] testcase: testtimeoutmultithreaded(org.apache.lucene.search.testtimelimitedcollector):      failed    [junit] some threads failed! expected:<50> but was:<48>    [junit] junit.framework.assertionfailederror: some threads failed! expected:<50> but was:<48>    [junit]     at org.apache.lucene.search.testtimelimitedcollector.dotestmultithreads(testtimelimitedcollector.java:255)    [junit]     at org.apache.lucene.search.testtimelimitedcollector.testtimeoutmultithreaded(testtimelimitedcollector.java:220)    [junit] problem either in test or in timelimitedcollector.",
        "label": 12
    },
    {
        "text": "testdocumentswriterstallcontrol hang  reproducible  on trunk (probably affects 4.0 too, but trunk is where i hit it): ant test -dtestcase=testdocumentswriterstallcontrol -dtests.seed=9d5404ff4a909330",
        "label": 46
    },
    {
        "text": "cutover remaining usage of pre flex apis a number of places still use the pre-flex apis. this is actually healthy, since it gives us ongoing testing of the back compat emulation layer. but we should at some point cut them all over to flex. latest we can do this is 4.0, but i'm not sure we should do them all for 3.1... still marking this as 3.1 to \"remind us\"",
        "label": 33
    },
    {
        "text": "lucene test  testcambridgema  fails when jvm bit does not use memory compression when i ran the lucene (4.2.1/4.3) test suite with ibm java i get the following error: [junit4:junit4] suite: org.apache.lucene.search.postingshighlight.testpostingshighlighter [junit4:junit4] 2> note: reproduce with: ant test -dtestcase=testpostingshighlighter -dtests.method=testcambridgema -dtests.seed=571e16aeaf72c9f9 -dtests.s low=true -dtests.locale=mt_mt -dtests.timezone=pacific/kiritimati -dtests.file.encoding=utf-8 [junit4:junit4] error 0.71s j2 | testpostingshighlighter.testcambridgema <<< [junit4:junit4] > throwable #1: java.lang.arrayindexoutofboundsexception: array index out of range: 37 [junit4:junit4] > at __randomizedtesting.seedinfo.seed([571e16aeaf72c9f9:d60b7505c1dc91f8]:0) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.passage.addmatch(passage.java:53) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.postingshighlighter.highlightdoc(postingshighlighter.java:547) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.postingshighlighter.highlightfield(postingshighlighter.java:425) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.postingshighlighter.highlightfields(postingshighlighter.java:364) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.postingshighlighter.highlightfields(postingshighlighter.java:268) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.postingshighlighter.highlight(postingshighlighter.java:198) [junit4:junit4] > at org.apache.lucene.search.postingshighlight.testpostingshighlighter.testcambridgema(testpostingshighlighter.java:373) [junit4:junit4] > at java.lang.thread.run(thread.java:738) [junit4:junit4] 2> note: test params are: codec=fastdecompressioncompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=fa st_decompression, chunksize=386), termvectorsformat=compressingtermvectorsformat(compressionmode=fast_decompression, chunksize=386)), sim=randomsimilarityprov ider(querynorm=false,coord=yes): {body=dfr i(n)z(0.3), title=dfr i(f)z(0.3), id=dfr i(n)2} , locale=mt_mt, timezone=pacific/kiritimati [junit4:junit4] 2> note: linux 2.6.32-279.el6.x86_64 amd64/ibm corporation 1.6.0 (64-bit)/cpus=4,threads=1,free=10783032,total=24030208 [junit4:junit4] 2> note: all tests run in this jvm: [fieldquerytest, fieldphraselisttest, simplefraglistbuildertest, fieldtermstacktest, offsetlimittokenfil tertest, tokensourcestest, testpostingshighlighter] [junit4:junit4] completed on j2 in 2.46s, 23 tests, 1 error <<< failures! this error is not seen with oracle java. a google search showed that this error has already occurred in community builds and the solution proposed was disable the ibm java in the community tests. i took a look in the code and found that the root of the problem is due to the assignment of the variable \"referencesize\" in ramusageestimator.java: // get object reference size by getting scale factor of object[] arrays: try { final method arrayindexscalem = unsafeclass.getmethod(\"arrayindexscale\", class.class); referencesize = ((number) arrayindexscalem.invoke(theunsafe, object[].class)).intvalue(); supportedfeatures.add(jvmfeature.object_reference_size); } catch (exception e) { // ignore. } the java object reference size for arrays have 8 bytes in 64-bit machines (oracle or ibm) and can be reduced to 4 bytes (like 32-bit jvms) using compressed references and compressed ordinary object pointers (oops). this options seems to be enabled by default in oracle java when the heap size is under 32gb, but is not in ibm java. as workaround, when testing with ibm jvm i can pass the options \"-xcompressedrefs\" or \"-xx:+usecompressedoops\" to junit. similarly, you can reproduce the error if you pass the option \"-xx:-usecompressedoops\" when testing with oracle java. the bug is in oversize method of arrayutil.java. it does nothing when the object reference size (bytesperelement) is 8.",
        "label": 53
    },
    {
        "text": "deprecate chineseanalyzer the chineseanalyzer, chinesetokenizer, and chinesefilter (not the smart one, or cjk) indexes chinese text as individual characters and removes english stopwords, etc. in my opinion we should simply deprecate all of this in favor of standardanalyzer, standardtokenizer, and stopfilter, which does the same thing.",
        "label": 46
    },
    {
        "text": "corruptindexexception on indexing after a failure occurs after segments file creation but before any bytes are written fsdirectory.createoutput(..) uses a randomaccessfile to do its work. on my system the default fsdirectory.open(..) creates an niofsdirectory. if createoutput is called on a segments_* file and a crash occurs between randomaccessfile creation (file system shows a segments_* file exists but has zero bytes) but before any bytes are written to the file, subsequent indexwriters cannot proceed. the difficulty is that it does not know how to clear the empty segments_* file. none of the file deletions will happen on such a segment file because the opening bytes cannot not be read to determine format and version. an initial proposed patch file is attached below.",
        "label": 33
    },
    {
        "text": "short circuit fuzzyquery rewrite when input token length is small compared to minsimilarity i found this (unreplied to) email floating around in my lucene folder from during the holidays... from: timo nentwig to: java-dev subject: fuzzy makes no sense for short tokens date: mon, 31 dec 2007 16:01:11 +0100 message-id: <200712311601.12255.lucene@nitwit.de> hi! it generally makes no sense to search fuzzy for short tokens because changing even only a single character of course already results in a high edit distance. so it actually only makes sense in this case:            if( token.length() > 1f / (1f - minsimilarity) ) e.g. changing one character in a 3-letter token (foo) results in an edit distance of 0.6. and if minsimilarity (which is by default: 0.5 :-) is higher we can save all the expensive rewrite() logic. i don't know much about fuzzyqueries, but this reasoning seems sound ... fuzzyquery.rewrite should be able to completely skip all termenumeration in the event that the input token is shorter then some simple math on the minsimilarity. (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at levenstein distances ... tests needed)",
        "label": 29
    },
    {
        "text": " patch  javadoc and comment updates for booleanclause  javadoc and comment updates for booleanclause, one minor code simplification.",
        "label": 55
    },
    {
        "text": "add reopen indexcommit  methods to indexreader add reopen(indexcommit) methods to indexreader to be able to reopen an index on any previously saved commit points with all advantages of lucene-1483. similar to open(indexcommit) & company available in 2.4.0.",
        "label": 33
    },
    {
        "text": "make sparse doc values and segments merging more efficient doc values were optimized recently to efficiently store sparse data. unfortunately there is still big problem with doc values merges for sparse fields. when we imagine 1 billion documents index it seems it doesn't matter if all documents have value for this field or there is only 1 document with value. segment merge time is the same for both cases. in most cases this is not a problem but there are several cases in which one can expect having many fields with sparse doc values. i can describe an example. during performance tests of a system with large number of sparse fields i realized that doc values merges are a bottleneck. i had hundreds of different numeric fields. each document contained only small subset of all fields. average document contains 5-7 different numeric values. as you can see data was very sparse in these fields. it turned out that ingestion process was cpu-bound. most of cpu time was spent in docvalues related methods (singletonsortednumericdocvalues#setdocument, docvaluesconsumer$10$1#next, docvaluesconsumer#issinglevalued, docvaluesconsumer$4$1#setnext, ...) - mostly during merging segments. adrien grand suggested to reduce the number of sparse fields and replace them with smaller number of denser fields. this helped a lot but complicated fields naming. i am not much familiar with doc values source code but i have small suggestion how to improve doc values merges for sparse fields. i realized that doc values producers and consumers use iterators. let's take an example of numeric doc values. would it be possible to replace iterator which \"travels\" through all documents with iterator over collection of non empty values? of course this would require storing object (instead of numeric) which contains value and document id. such an iterator could significantly improve merge time of sparse doc values fields. imho this won't cause big overhead for dense structures but it can be game changer for sparse structures. this is what happens in numericdocvalueswriter on flush     dvconsumer.addnumericfield(fieldinfo,                                new iterable<number>() {                                  @override                                  public iterator<number> iterator() {                                    return new numericiterator(maxdoc, values, docswithfield);                                  }                                }); before this happens during addvalue, this loop is executed to fill holes.     // fill in any holes:     for (int i = (int)pending.size(); i < docid; ++i) {       pending.add(missing);     } it turns out that variable called pending is used only internally in numericdocvalueswriter. i know pending is packedlongvalues and it wouldn't be good to change it with different class (some kind of list) because this may break dv performance for dense fields. i hope someone can suggest interesting solutions for this problem . it would be great if discussion about sparse doc values merge performance can start here.",
        "label": 33
    },
    {
        "text": "fix ant beast to not overwrite junit xml results for each beast iters iteration  we should write the xml output files to different i subdirectories or something so that all the results are available after the run.",
        "label": 29
    },
    {
        "text": "review and potentially remove unused unsupported contribs some of our contribs appear to be lacking for development/support or are missing tests. we should review whether they are even pertinent these days and potentially deprecate and remove them. one of the things we did in mahout when bringing in colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added. those that didn't get tests added over about a 6 mos. period of time were removed. i would suggest taking a hard look at: ant db lucli swing (spatial should be gutted to some extent and moved to modules)",
        "label": 40
    },
    {
        "text": "indexwriterconfig clone should clone the mergescheduler concurrentmergescheduler has a list<mergethread> member to track the running merging threads, so indexwriterconfig.clone should clone the merge scheduler so that both indexwriterconfig instances are independant.",
        "label": 1
    },
    {
        "text": "allows ioexception in docsenum freq  currently, docsenum#freq() does not allow ioexception. this is problematic if somebody wants to implement a codec that allows lazy loading of freq. frequency will be read and decoded only when #freq() will be called, therefore calling indexinput's read methods that can throw ioexception. the current workaround is to catch the ioexception in freq() and ignore it (which is not very nice and not a good solution).",
        "label": 46
    },
    {
        "text": "provide consistent iw behavior for illegal meta data changes currently iw fails late and inconsistent if field metadata like an already defined docvalues type or \"un\"-omitting norms. we can approach this similar to how we handle consistent field number and: throw exception if indexoptions conflict (e.g. omittf=true versus false) instead of silently dropping positions on merge same with omitnorms same with norms types and docvalues types still keeping field numbers consistent this way we could eliminate all these traps and just give an exception instead.",
        "label": 46
    },
    {
        "text": "twodoublesstrategy is broken for circles twodoublesstrategy supports finding documents that are within a circle, yet it is impossible to provide one due to the following code found at the start of twodoublesstrategy.makequery(): shape shape = args.getshape();     if (!(shape instanceof rectangle)) {       throw new invalidshapeexception(\"a rectangle is the only supported shape (so far), not \"+shape.getclass());//todo     }     rectangle bbox = (rectangle) shape; i think instead the code which handles circles should ask for the bounding box of the shape and uses that instead.",
        "label": 7
    },
    {
        "text": "improve the use of isdeleted in the indexing code a spin off from here: http://www.nabble.com/some-thoughts-around-the-use-of-reader.isdeleted-and-hasdeletions-td23931216.html. two changes: optimize segmentmerger work when a reader has no deletions. indexreader.document() will no longer check if the document is deleted. will post a patch shortly",
        "label": 33
    },
    {
        "text": "sandbox remaining contrib queries in lucene-3271, i moved the 'good' queries from the queries contrib to new destinations (primarily the queries module). the remnants now need to find their home. as suggested in lucene-3271, these classes are not bad per se, just odd. so lets create a sandbox contrib that they and other 'odd' contrib classes can go to. we can then decide their fate at another time.",
        "label": 7
    },
    {
        "text": "maven build broken   bootstrap fails building lucene/solr as described in dev-tools/maven/readme.maven fails, because a contributed lib is missing. [error] failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.3.1:install-file (install-langdetect) on project lucene-solr-grandparent:  error installing artifact 'org.apache.solr:solr-langdetect:jar':  failed to install artifact org.apache.solr:solr-langdetect:jar:4.0-snapshot:  /home/truebner/dev/workspace/apache-lucene/solr/contrib/langid/lib/langdetect-r111.jar (no such file or directory) -> [help 1]",
        "label": 47
    },
    {
        "text": "fix for deprecations in contrib surround fix for deprecations in contrib/surround.",
        "label": 55
    },
    {
        "text": " patch  clear threadlocal instances in close  as already found out in lucene-436, there seems to be a garbage collection problem with threadlocals at certain constellations, resulting in an outofmemoryerror. the resolution there was to remove the reference to the threadlocal value when calling the close() method of the affected classes (see fieldsreader and terminfosreader). for java < 5.0, this can effectively be done by calling threadlocal.set(null); for java >= 5.0, we would call threadlocal.remove() analogously, this should be done in any class which creates threadlocal values right now, two classes of the core api make use of threadlocals, but do not properly remove their references to the threadlocal value 1. org.apache.lucene.index.segmentreader 2. org.apache.lucene.analysis.analyzer for segmentreader, i have attached a simple patch. for analyzer, there currently is no patch because analyzer does not provide a close() method (future to-do?)",
        "label": 33
    },
    {
        "text": "writelinedoctask should keep docs w  just title and no body writelinedoctask throws away a document if it does not have a body element. however, if the document has a title, then it should be kept. some documents, such as emails, may not have a body which is legitimate. i'll post a patch + a test case.",
        "label": 29
    },
    {
        "text": "numeric docvalues updates in lucene-4258 we started to work on incremental field updates, however the amount of changes are immense and hard to follow/consume. the reason is that we targeted postings, stored fields, dv etc., all from the get go. i'd like to start afresh here, with numeric-dv-field updates only. there are a couple of reasons to that: numericdv fields should be easier to update, if e.g. we write all the values of all the documents in a segment for the updated field (similar to how livedocs work, and previously norms). it's a fairly contained issue, attempting to handle just one data type to update, yet requires many changes to core code which will also be useful for updating other data types. it has value in and on itself, and we don't need to allow updating all the data types in lucene at once ... we can do that gradually. i have some working patch already which i'll upload next, explaining the changes.",
        "label": 43
    },
    {
        "text": "release forbiddenapi checker on google code currently there is source code in lucene/tools/src (e.g. forbidden apis checker ant task). it would be convenient if you could download this thing in your ant build from ivy (especially if maybe it included our definitions .txt files as resources). in general checking for locale/charset violations in this way is a pretty general useful thing for a server-side app. can we either release lucene-tools.jar as an artifact, or maybe alternatively move this somewhere else as a standalone project and suck it in ourselves?",
        "label": 53
    },
    {
        "text": "elision in frenchanalyzer it seems org.apache.lucene.analysis.fr.frenchanalyzer.default_articles is missing \"d\" and \"c\", but also \"jusqu\", \"quoiqu\", \"lorsqu\", and \"puisqu\".",
        "label": 47
    },
    {
        "text": "ngramtokenfilter increments positions for each gram ngramtokenfilter increments positions for each gram rather for the actual token which can lead to rather funny problems especially with highlighting. if this filter should be used for highlighting is a different story but today this seems to be a common practice in many situations to highlight sub-term matches. i have a test for highlighting that uses ngram failing with a stringioob since tokens are sorted by position which causes offsets to be mixed up due to ngram token filter.",
        "label": 1
    },
    {
        "text": "the demo application does not work as of the demo application does not work. queryparser needs a version argument. while i am here, remove @author too",
        "label": 40
    },
    {
        "text": "segment level bloom filters an addition to each segment which stores a bloom filter for selected fields in order to give fast-fail to term searches, helping avoid wasted disk access. best suited for low-frequency fields e.g. primary keys on big indexes with many segments but also speeds up general searching in my tests. overview slideshow here: http://www.slideshare.net/markharwood/lucene-bloomfilteredsegments benchmarks based on wikipedia content here: http://goo.gl/x7qqu patch based on 3.6 codebase attached. there are no 3.6 api changes currently - to play just add a field with \"_blm\" on the end of the name to invoke special indexing/querying capability. clearly a new field or schema declaration would need adding to apis to configure the service properly. also, a patch for lucene4.0 codebase introducing a new postingsformat",
        "label": 28
    },
    {
        "text": "omit positions but keep termfreq it would be useful to have an option to discard positional information but still keep the term frequency - currently setomittermfreqandpositions discards both. even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring.",
        "label": 40
    },
    {
        "text": "no need for out of order payload checks in spanpayloadcheckquery since lucene-6537, all composite spans implementations collect their payloads in-order, so we don't need special logic for that case anymore.",
        "label": 2
    },
    {
        "text": "similarity can only be set per index  but i may want to adjust scoring behaviour at a field level similarity can only be set per index, but i may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods. currently it is only passed to some such as lengthnorm() but not others such as tf()",
        "label": 40
    },
    {
        "text": "string intern  faster alternative by using our own interned string pool on top of default, string.intern() can be greatly optimized. on my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new string(interned)' for java 5 and 4 speedup is lower, but still considerable.",
        "label": 55
    },
    {
        "text": " gsoc  implementing state of the art ranking for lucene lucene employs the vector space model (vsm) to rank documents, which compares unfavorably to state of the art algorithms, such as bm25. moreover, the architecture is tailored specically to vsm, which makes the addition of new ranking functions a non- trivial task. this project aims to bring state of the art ranking methods to lucene and to implement a query architecture with pluggable ranking functions. the wiki page for the project can be found at http://wiki.apache.org/lucene-java/summerofcode2011projectranking.",
        "label": 40
    },
    {
        "text": "refrencemanager accquire can result in infinite loop if manager resource is abused outside of the manager i think i found a bug that can cause the referencemanager to stick in an infinite loop if the managed reference is decremented outside of the manager without a corresponding increment. i think this is pretty bad since the debugging of this is a mess and we should rather throw ise instead.",
        "label": 46
    },
    {
        "text": "complexphrasequeryparser throws parseexception for fielded queries queries using queryparser's non-default field e.g. author:\"j* smith\" are not supported by complexphrasequeryparser. for example following code snippet complexphrasequeryparser qp = new complexphrasequeryparser(test_version_current, \"defaultfield\", new mockanalyzer(new random()));       qp.parse(\"author:\\\"fred* smith\\\"\") ; yields caused by: org.apache.lucene.queryparser.classic.parseexception: cannot have clause for field \"defaultfield\" nested in phrase  for field \"author\"  at org.apache.lucene.queryparser.complexphrase.complexphrasequeryparser.checkphraseclauseisforsamefield(complexphrasequeryparser.java:147)  at org.apache.lucene.queryparser.complexphrase.complexphrasequeryparser.newtermquery(complexphrasequeryparser.java:135)  ... 49 more",
        "label": 13
    },
    {
        "text": "fastvectorhighlighter   expose fieldfraglist fraginfo for user customizable fragmentsbuilder needed to build a custom highlightable snippet - snippet should start with the sentence containing the first match, then continue for 250 characters. so created a custom fragmentsbuilder extending simplefragmentsbuilder and overriding the createfragments(indexreader reader, int docid, string fieldname, fieldfraglist fieldfraglist) method - unit test containing the code is attached to the jira. to get this to work, needed to expose (make public) the fieldfraglist.fraginfo member variable. this is currently package private, so only fragmentsbuilder implementations within the lucene-highlighter o.a.l.s.vectorhighlight package (such as simplefragmentsbuilder) can access it. since i am just using the lucene-highlighter.jar as an external dependency to my application, the simplest way to access fieldfraglist.fraginfo in my class was to make it public.",
        "label": 26
    },
    {
        "text": "improve ratelimiters initialization semantics i was working on solr-6485 when i realized that the first time pause is called even if we write a lot of bytes pause doesn't work correctly because in simpleratelimiter.pause() lastns is 0 and startns is always more than targetns. if we remove the following line from testratelimiter.testpause() then the test fails - limiter.pause(2);//init should we do one of the following ? 1. initialize lastns in the ctor lastns = system.nanotime(); 2. add a method saying start() which does the same",
        "label": 40
    },
    {
        "text": "sloppyphrasescorer should use conjunctiondisi currently, this guy has his own little built-in algorithm, which doesn't seem optimal to me. it might be better if it reused conjunctiondisi like exactphrasescorer does.",
        "label": 1
    },
    {
        "text": "error on distance query where miles   if miles is under 1.0 distance query will break. to reproduce modify the file http://svn.apache.org/viewvc/lucene/java/trunk/contrib/spatial/src/test/org/apache/lucene/spatial/tier/testcartesian.java?revision=794721 and set the line: final double miles = 6.0; to final double miles = 0.5;",
        "label": 7
    },
    {
        "text": "analyzingsuggester may fail to return correct topn suggestions i hit this when working on lucene-4480. because analyzingsuggester may prune some of the topn paths found by fst's util.topnsearcher, this means the queue size limit of topn makes the overall search inadmissible, ie it may incorrectly prune paths that would have lead to a competitive path. however, such pruning is rare: it happens only for graph token streams, and even then only when competitive analyzed forms share the same surface forms. the simplest way to fix this is to make the queue unbounded but this is likely a sizable performance hit ... i haven't tested yet. it's even possible the way the dups happen (always at the \"end\" of the suggestion, because we tack on 0 byte followed by ord dedup byte) prevent this bug from even occurring and so this could all be a false alarm! i have to try to make a test case showing it ... a cop-out solution would be to expose a separate queuesize or queuemultiplier (over the topn) so that if users are affected by this they could crank up the queue size or multiplier.",
        "label": 33
    },
    {
        "text": "analyzinginfixsuggester should not immediately open an indexwriter over an already built index in tests i'm working on under solr-6246, i can see that attempts to open an analyzinginfixsuggester over a previously-built index fail, because the old suggester's indexwriter lock is never released. the issue is that an indexwriter is created in order to create a searchermanager over the previously built index. but this is not necessary: searchermanager has a ctor that takes a directory instead of an indexwriter.",
        "label": 47
    },
    {
        "text": "add javadoc notes about icucollationkeyfilter's advantages over collationkeyfilter contrib/collation's icucollationkeyfilter, which uses icu4j collation, is faster than collationkeyfilter, the jvm-provided java.text.collator implementation in the same package. the javadocs of these classes should be modified to add a note to this effect. my curiosity was piqued by robert muir's comment on lucene-1581, in which he states that icucollationkeyfilter is up to 30x faster than collationkeyfilter. i timed the operation of these two classes, with sun jvm versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using collators at the default strength, on a windows vista 64-bit machine. i used an analysis pipeline consisting of whitespacetokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, i also timed whitespacetokenizer operating alone for each combination. the rightmost column represents the performance advantage of the icu4j implemtation (icu) over the java.text.collator implementation (jvm), after discounting the whitespacetokenizer time (wst): (jvm-icu) / (icu-wst). the best times out of 5 runs for each combination, in milliseconds, are as follows: sun jvm language java.text icu4j whitespacetokenizer icu4j improvement 1.4.2_17 (32 bit) english 522 212 13 156% 1.4.2_17 (32 bit) french 716 243 14 207% 1.4.2_17 (32 bit) german 669 264 16 163% 1.4.2_17 (32 bit) ukranian 931 474 25 102% 1.5.0_15 (32 bit) english 604 176 16 268% 1.5.0_15 (32 bit) french 817 209 17 317% 1.5.0_15 (32 bit) german 799 225 20 280% 1.5.0_15 (32 bit) ukranian 1029 436 26 145% 1.5.0_15 (64 bit) english 431 89 10 433% 1.5.0_15 (64 bit) french 562 112 11 446% 1.5.0_15 (64 bit) german 567 116 13 438% 1.5.0_15 (64 bit) ukranian 734 281 21 174% 1.6.0_13 (64 bit) english 162 81 9 113% 1.6.0_13 (64 bit) french 192 92 10 122% 1.6.0_13 (64 bit) german 204 99 14 124% 1.6.0_13 (64 bit) ukranian 273 202 21 39%",
        "label": 46
    },
    {
        "text": "benchmark deletes alg fails benchmark deletes.alg fails because the index reader defaults to open readonly.",
        "label": 33
    },
    {
        "text": "docs out of order hello, i can not find out, why (and what) it is happening all the time. i got an exception: java.lang.illegalstateexception: docs out of order at org.apache.lucene.index.segmentmerger.appendpostings(segmentmerger.java:219) at org.apache.lucene.index.segmentmerger.mergeterminfo(segmentmerger.java:191) at org.apache.lucene.index.segmentmerger.mergeterminfos(segmentmerger.java:172) at org.apache.lucene.index.segmentmerger.mergeterms(segmentmerger.java:135) at org.apache.lucene.index.segmentmerger.merge(segmentmerger.java:88) at org.apache.lucene.index.indexwriter.mergesegments(indexwriter.java:341) at org.apache.lucene.index.indexwriter.optimize(indexwriter.java:250) at optimize.main(optimize.java:29) it happens either in 1.2 and 1.3rc1 (anyway what happened to it? i can not find it neither in download nor in version list in this form). everything seems ok. i can search through index, but i can not optimize it. even worse after this exception every time i add new documents and close indexwriter new segments is created! i think it has all documents added before, because of its size. my index is quite big: 500.000 docs, about 5gb of index directory. it is repeatable. i drop index, reindex everything. afterwards i add a few docs, try to optimize and receive above exception. my documents' structure is: static document indexit(string id_strony, reader reader, string data_wydania, string id_wydania, string id_gazety, string data_wstawienia) { document doc = new document(); doc.add(field.keyword(\"id\", id_strony )); doc.add(field.keyword(\"data_wydania\", data_wydania)); doc.add(field.keyword(\"id_wydania\", id_wydania)); doc.add(field.text(\"id_gazety\", id_gazety)); doc.add(field.keyword(\"data_wstawienia\", data_wstawienia)); doc.add(field.text(\"tresc\", reader)); return doc; } sincerely, legez",
        "label": 33
    },
    {
        "text": "indexwriter addindexesnooptimize ignores the compound file setting of the destination index indexwriter.addindexesnooptimize(directory[]) ignores the compound file setting of the destination index. it is using the compound file flags of segments in the source indexes. this sometimes causes undesired increase of the number of files in the destination index when non-compound file indexes are added until merge kicks in.",
        "label": 33
    },
    {
        "text": "nightly build archives do not contain java source code  under the lucene news section of the overview page, this item's link: 26 january 2006 - nightly builds available http://cvs.apache.org/dist/lucene/java/nightly/ goes to a directory with several 1.9m files, none of which have the src/java tree in them.",
        "label": 15
    },
    {
        "text": "escaped wildcard character in wildcard term not handled correctly if an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal. e.g., t??t is converted by the queryparser to t??t - the escape character is discarded.",
        "label": 40
    },
    {
        "text": "long text matching email local part rule in uax29urlemailtokenizer causes extremely slow tokenization this is a pretty nasty bug, and causes the cluster to stop accepting updates. i'm not sure how to consistently reproduce it but i have done so numerous times. switching to a whitespace tokenizer improved indexing speed, and i never got the issue again. i'm running a 4.6 snapshot - i had issues with deadlocks with numerous versions of solr, and have finally narrowed down the problem to this code, which affects many/all versions of solr. when the thread hits this issue it uses 100% cpu, restarting the node which has the error allows indexing to continue until hit again. here is thread dump: http-bio-8080-exec-45 (201) org.apache.lucene.analysis.standard.uax29urlemailtokenizerimpl.getnexttoken(uax29urlemailtokenizerimpl.java:4343) org.apache.lucene.analysis.standard.uax29urlemailtokenizer.incrementtoken(uax29urlemailtokenizer.java:147) org.apache.lucene.analysis.util.filteringtokenfilter.incrementtoken(filteringtokenfilter.java:82) org.apache.lucene.analysis.core.lowercasefilter.incrementtoken(lowercasefilter.java:54) org.apache.lucene.index.docinverterperfield.processfields(docinverterperfield.java:174) org.apache.lucene.index.docfieldprocessor.processdocument(docfieldprocessor.java:248) org.apache.lucene.index.documentswriterperthread.updatedocument(documentswriterperthread.java:253) org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:453) org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1517) org.apache.solr.update.directupdatehandler2.adddoc(directupdatehandler2.java:217) org.apache.solr.update.processor.runupdateprocessor.processadd(runupdateprocessorfactory.java:69) org.apache.solr.update.processor.updaterequestprocessor.processadd(updaterequestprocessor.java:51) org.apache.solr.update.processor.distributedupdateprocessor.dolocaladd(distributedupdateprocessor.java:583) org.apache.solr.update.processor.distributedupdateprocessor.versionadd(distributedupdateprocessor.java:719) org.apache.solr.update.processor.distributedupdateprocessor.processadd(distributedupdateprocessor.java:449) org.apache.solr.handler.loader.javabinloader$1.update(javabinloader.java:89) org.apache.solr.client.solrj.request.javabinupdaterequestcodec$1.readoutermostdociterator(javabinupdaterequestcodec.java:151) org.apache.solr.client.solrj.request.javabinupdaterequestcodec$1.readiterator(javabinupdaterequestcodec.java:131) org.apache.solr.common.util.javabincodec.readval(javabincodec.java:221) org.apache.solr.client.solrj.request.javabinupdaterequestcodec$1.readnamedlist(javabinupdaterequestcodec.java:116) org.apache.solr.common.util.javabincodec.readval(javabincodec.java:186) org.apache.solr.common.util.javabincodec.unmarshal(javabincodec.java:112) org.apache.solr.client.solrj.request.javabinupdaterequestcodec.unmarshal(javabinupdaterequestcodec.java:158) org.apache.solr.handler.loader.javabinloader.parseandloaddocs(javabinloader.java:99) org.apache.solr.handler.loader.javabinloader.load(javabinloader.java:58) org.apache.solr.handler.updaterequesthandler$1.load(updaterequesthandler.java:92) org.apache.solr.handler.contentstreamhandlerbase.handlerequestbody(contentstreamhandlerbase.java:74) org.apache.solr.handler.requesthandlerbase.handlerequest(requesthandlerbase.java:135) org.apache.solr.core.solrcore.execute(solrcore.java:1859) org.apache.solr.servlet.solrdispatchfilter.execute(solrdispatchfilter.java:703) org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:406) org.apache.solr.servlet.solrdispatchfilter.dofilter(solrdispatchfilter.java:195) org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:243) org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:210) org.apache.catalina.core.standardwrappervalve.invoke(standardwrappervalve.java:222) org.apache.catalina.core.standardcontextvalve.invoke(standardcontextvalve.java:123) org.apache.catalina.core.standardhostvalve.invoke(standardhostvalve.java:171) org.apache.catalina.valves.errorreportvalve.invoke(errorreportvalve.java:99) org.apache.catalina.valves.accesslogvalve.invoke(accesslogvalve.java:953) org.apache.catalina.core.standardenginevalve.invoke(standardenginevalve.java:118) org.apache.catalina.connector.coyoteadapter.service(coyoteadapter.java:408) org.apache.coyote.http11.abstracthttp11processor.process(abstracthttp11processor.java:1023) org.apache.coyote.abstractprotocol$abstractconnectionhandler.process(abstractprotocol.java:589) org.apache.tomcat.util.net.jioendpoint$socketprocessor.run(jioendpoint.java:312) java.util.concurrent.threadpoolexecutor.runworker(unknown source) java.util.concurrent.threadpoolexecutor$worker.run(unknown source) java.lang.thread.run(unknown source)",
        "label": 47
    },
    {
        "text": "improve termquery  pk lookup  performance for things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1) we do wasted seeks. while lucene-2694 tries to solve some of this issue with termstate, i'm concerned we could every backport that to 3.1 for example. this is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve lucene-2694, but i don't think we should leave things as they are in 3.x",
        "label": 33
    },
    {
        "text": "add a latent fieldselectorresult i propose adding latent fieldselectorresult this would be similar to lazy_load except that it would never cache the stored value this will be useful for very large fields that should always go direct to disk (because they will take so much memory) when caching documents returned from a searcher, the large field may be initially requested as lazy_load, however once someone reads this field, it will then get locked into memory. if this document (and others like it) are cached, it can start to use a very large amount of memory for these fields contract for fieldselectorresult.latent should be that it will always be pulled direct from the indexinput and never be persisted in memory as part of a fieldable i could prepare a patch if desired",
        "label": 15
    },
    {
        "text": "termspans skipto  doesn't always move forwards in termspans (or the anonymous spans class returned by spanstermquery, depending on the version), the skipto() method is improperly implemented if the target doc is less than or equal to the current doc: public boolean skipto(int target) throws ioexception { // are we already at the correct position? if (doc >= target) { return true; } ... this violates the correct behavior (as described in the spans interface documentation), that skipto() should always move forwards, in other words the correct implementation would be: if (doc >= target) { return next(); } this bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipto() to the same document, then tries to load the \"next\" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error).",
        "label": 32
    },
    {
        "text": " patch  performance improvement to disjunctionsumscorer a recent profile of the new booleanscorer2 showed that quite a bit of cpu time is spent in the advanceaftercurrent method of disjunctionscorer, and in the priorityqueue of scorers that is used there. this patch reduces the internal overhead of disjunctionscorer to about 70% of the current one (ie. 30% saving in cpu time). it also reduces the number of calls to the subscorers, but that was not measured. to get this, it was necessary to specialize the priorityqueue for a scorer and to add move some code fragments from disjunctionscorer to this specialized queue.",
        "label": 55
    },
    {
        "text": "automaton query filter  scalable regex  attached is a patch for an automatonquery/filter (name can change if its not suitable). whereas the out-of-box contrib regexquery is nice, i have some very large indexes (100m+ unique tokens) where queries are quite slow, 2 minutes, etc. additionally all of the existing regexquery implementations in lucene are really slow if there is no constant prefix. this implementation does not depend upon constant prefix, and runs the same query in 640ms. some use cases i envision: 1. lexicography/etc on large text corpora 2. looking for things such as urls where the prefix is not constant (http:// or ftp://) the filter uses the brics package (http://www.brics.dk/automaton/) to convert regular expressions into a dfa. then, the filter \"enumerates\" terms in a special way, by using the underlying state machine. here is my short description from the comments: the algorithm here is pretty basic. enumerate terms but instead of a binary accept/reject do: 1. look at the portion that is ok (did not enter a reject state in the dfa) 2. generate the next possible string and seek to that. the query simply wraps the filter with constantscorequery. i did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is bsd-licensed.",
        "label": 40
    },
    {
        "text": "add equals  and hashcode  to explanation i don't think there's any reason not to add these?",
        "label": 2
    },
    {
        "text": "move reusableanalyzerbase into core in lucene-2309 it was suggested that we should make analyzer reusability compulsory. reusableanalyzerbase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it).",
        "label": 7
    },
    {
        "text": "refactoring of filteredtermsenum and multitermquery filteredtermsenum is confusing as it is initially positioned to the first term. it should instead work like an uninitialized termsenum for a field before the first call to next() or seek(). filteredtermsenums cannot implement seek() as eg. nrq or automaton are not able to support this. seeking is also not needed for mtq at all, so seek can just throw uoe. this issue changes some of the internal behaviour of mtq and filteredtermsenum to allow also seeking in nrq and automaton (see comments below).",
        "label": 53
    },
    {
        "text": "make precommit work for svn or git checkouts ",
        "label": 53
    },
    {
        "text": "bug in the org apache lucene analysis br braziliananalyzer one weird bug with this field is that instead of \"false\", you have to search for \"falsee\" to get the correct results. the same behavior happen with other fields that stored in the index and not analyzed. example of create fields to indexing: field field = new field(\"situacaodocumento\", \"ativo\", field.store.yes, field.index.not_analyzed); or field field = new field(\"copia\", \"false\", field.store.yes, field.index.not_analyzed); example search i need to do, but nothing get correct result: indexsearcher searcher = ...; topscoredoccollector collector = ....; query query = multifieldqueryparser.parse(version, \"copia:false\", \"copia\", flags, getanalyzer()); searcher.search(query, collector); scoredoc[] hits = collector.topdocs().scoredocs; if (hits.length > 0) { return searcher.doc(0); } return null; example search i do to work: indexsearcher searcher = ...; topscoredoccollector collector = ....; query query = multifieldqueryparser.parse(version, \"copia:falsee\", \"copia\", flags, getanalyzer()); searcher.search(query, collector); scoredoc[] hits = collector.topdocs().scoredocs; if (hits.length > 0) { return searcher.doc(0); } return null; i tested on the luke (lucene index toolbox) and he prove the bug.",
        "label": 29
    },
    {
        "text": "testnrtmanager test failure reproduces for me",
        "label": 33
    },
    {
        "text": "testtermsenum testintersectrandom fail     [junit] testsuite: org.apache.lucene.index.testtermsenum     [junit] testcase: testintersectrandom(org.apache.lucene.index.testtermsenum): failed     [junit] (null)     [junit] junit.framework.assertionfailederror     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1530)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1432)     [junit]  at org.apache.lucene.index.codecs.blocktreetermsreader$fieldreader$intersectenum.getstate(blocktreetermsreader.java:894)     [junit]  at org.apache.lucene.index.codecs.blocktreetermsreader$fieldreader$intersectenum.seektostartterm(blocktreetermsreader.java:969)     [junit]  at org.apache.lucene.index.codecs.blocktreetermsreader$fieldreader$intersectenum.<init>(blocktreetermsreader.java:786)     [junit]  at org.apache.lucene.index.codecs.blocktreetermsreader$fieldreader.intersect(blocktreetermsreader.java:483)     [junit]  at org.apache.lucene.index.testtermsenum.testintersectrandom(testtermsenum.java:293)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1530)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1432)     [junit]      [junit]      [junit] tests run: 6, failures: 1, errors: 0, time elapsed: 14.762 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testtermsenum -dtestmethod=testintersectrandom -dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -dtests.multiplier=3",
        "label": 33
    },
    {
        "text": "simpletextcodec needs simpletext docvalues impl currently simpletextcodec uses binary docvalues we should move that to a simple text impl.",
        "label": 46
    },
    {
        "text": "intermittent failure in testfieldcachetermsfilter testmissingterms running tests in while(1) i hit this: note: reproduce with: ant test -dtestcase=testfieldcachetermsfilter -dtestmethod=testmissingterms -dtests.seed=-1046382732738729184:5855929314778232889 1) testmissingterms(org.apache.lucene.search.testfieldcachetermsfilter) java.lang.assertionerror: must match 1 expected:<1> but was:<0>  at org.junit.assert.fail(assert.java:91)  at org.junit.assert.failnotequals(assert.java:645)  at org.junit.assert.assertequals(assert.java:126)  at org.junit.assert.assertequals(assert.java:470)  at org.apache.lucene.search.testfieldcachetermsfilter.testmissingterms(testfieldcachetermsfilter.java:63)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)  at org.junit.rules.testwatchman$1.evaluate(testwatchman.java:48)  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1214)  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1146)  at org.junit.runners.parentrunner$3.run(parentrunner.java:193)  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)  at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)  at org.junit.runners.parentrunner.access$000(parentrunner.java:42)  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)  at org.junit.runners.parentrunner.run(parentrunner.java:236)  at org.junit.runners.suite.runchild(suite.java:128)  at org.junit.runners.suite.runchild(suite.java:24)  at org.junit.runners.parentrunner$3.run(parentrunner.java:193)  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)  at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)  at org.junit.runners.parentrunner.access$000(parentrunner.java:42)  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)  at org.junit.runners.parentrunner.run(parentrunner.java:236)  at org.junit.runner.junitcore.run(junitcore.java:157)  at org.junit.runner.junitcore.run(junitcore.java:136)  at org.junit.runner.junitcore.run(junitcore.java:117)  at org.junit.runner.junitcore.runmain(junitcore.java:98)  at org.junit.runner.junitcore.runmainandexit(junitcore.java:53)  at org.junit.runner.junitcore.main(junitcore.java:45) unfortunately the seed doesn't [consistently] repro for me...",
        "label": 33
    },
    {
        "text": "indexing performance tests with realtime branch we should run indexing performance tests with the dwpt changes and compare to trunk. we need to test both single-threaded and multi-threaded performance. note: flush by ram isn't implemented just yet, so either we wait with the tests or flush by doc count.",
        "label": 46
    },
    {
        "text": "termquery's constructors should clone the incoming term this is a follow-up of lucene-6435: the bug stems from the fact that you can build term queries out of shared bytesref objects (such as the ones returned by termsenum.next), which is a bit trappy. if termquery's constructors would clone the incoming term, we wouldn't have this trap.",
        "label": 50
    },
    {
        "text": "absorb niofsdirectory into fsdirectory i think whether one uses java.io.* vs java.nio.* or eventually java.nio2.*, or some other means, is an under-the-hood implementation detail of fsdirectory and doesn't merit a whole separate class. i think fsdirectory should be the core class one uses when one's index is in the filesystem. so, i'd like to deprecate niofsdirectory, absorbing it into fsdirectory, and add a setting \"usenio\" to fsdirectory. it should default to \"true\" for non-windows oss, because it gives far better concurrent performance on all platforms but windows (due to known sun jre issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).",
        "label": 53
    },
    {
        "text": "issue with holes in polygons hi, it seems the behavior of adding holes to polygons is not behaving well. it seems it is all related to the following method: /** check if a point is within the provided holes. *@param point point to check. *@return true if the point is within any of the holes. */ protected boolean iswithinholes(final geopoint point) { if (holes != null) { for (final geopolygon hole : holes) { if (!hole.iswithin(point)) { return true; } } } return false; } the negation in the if clause is wrong. this affects two classes, geoconvexpolygon and geoconcavepolygon. i have ready a patch if you think it should be corrected. thanks, i.",
        "label": 25
    },
    {
        "text": "remove scoringwrapperspans in lucene-6919 (lucene 5.5), scoringwrapperspans was modified in such a way that made the existence of this class pointless, and possibly broke anyone who was using it as it's simscorer argument isn't used anymore. we should now delete it. spanweight has getsimscorer() so people can customize the simscorer that way. another small change i observe to improve is have spanweight.buildsimweight's last line use the existing similarity that has already been populated on the field?",
        "label": 10
    },
    {
        "text": "add oal util version ctor to queryparser this is a followup of lucene-1987: if somebody uses standardanalyzer with version.lucene_current and then uses queryparser, phrase queries will not work, because the stopfilter enables position increments for stop words, but queryparser ignores them per default. the user has to explicitely enable them. this issue would add a ctor taking the version constant and automatically enable this setting. the same applies to the contrib queryparser. eventually also stopanalyzer should add this version ctor. to be able to remove the default ctor for 3.0 (to remove a possible trap for users of queryparser), it must be deprecated and the new one also added to 2.9.1.",
        "label": 33
    },
    {
        "text": "the dest file might be deleted later by fsdirectory rename when renaming a file, `fsdirectory#rename` tries to delete the dest file if it's in the pending deletes list. if that delete fails, it adds the dest to the pending deletes list again. this causes the dest file to be deleted later by `deletependingfiles`. this was introduced by lucene-8275.",
        "label": 46
    },
    {
        "text": "fix ant beast to fail and succeed based on whether beasting actually fails or succeeds  ",
        "label": 29
    },
    {
        "text": "add locallucene local lucene (geo-search) has been donated to the lucene project, per https://issues.apache.org/jira/browse/incubator-77. this issue is to handle the lucene portion of integration. see http://lucene.markmail.org/message/orzro22sqdj3wows?q=locallucene",
        "label": 42
    },
    {
        "text": "bugs in bytearraydatainput bytearraydatainput has a byte[] ctor, but it doesn't actually work (some things like readvint will work, others will fail due to asserts). the problem is it doesnt set things like limit in the ctor... i think the ctor should call reset() most code using this passes null to the ctor to initialize it, then uses reset(), instead they could just call bytearrayinput(bytesref.empty_bytes) if they want to do that. finally, reset()'s limit looks like it should be offset + len",
        "label": 33
    },
    {
        "text": "charfilters not being invoked in solr on solr trunk, all charfilters have been non-functional since lucene-3396 was committed in r1175297 on 25 sept 2011, until yonik's fix today in r1235810; solr 3.x was not affected - charfilters have been working there all along.",
        "label": 55
    },
    {
        "text": "stemmeroverridefilter should not copy the stem override dictionary in it's ctor  currently the dictionary is cloned each time the token filter is created which is a serious bottleneck if you use this filter with large dictionaries and can also lead to ooms if lots of those filters sit in threadlocals and new threads are added etc. i think cloning the map should be done in the analyzer (which all of our analyzers do btw. but this is the only tf that does that) no need to really copy that map.",
        "label": 46
    },
    {
        "text": "indexwriter deletedocuments bug indexwriter.deletedocuments() fails random testing",
        "label": 33
    },
    {
        "text": "unifiedhighlighter should allow extension for custom query types in our use case, we have custom query types (both spanquery and non-spanquery) which are not provided by lucene. unifiedhighlighter needs extension points to handle some custom query types in order for highlighting to be accurate. this issue represents adding two extension point methods to support custom query types.",
        "label": 10
    },
    {
        "text": "docvaluesfield should not overload setint setfloat etc see my description on lucene-3687. in general we should avoid this for primitive types and give them each unique names. so i think instead of setint(byte), setint(short), setint(int), setint(long), setfloat(float) and setfloat(double), we should have setbyte(byte), setshort(short), setint(int), setlong(long), setfloat(float) and setdouble(double).",
        "label": 33
    },
    {
        "text": "pass potent sr to irwarmer warm  and also call warm  for new segments currently warm() receives a segmentreader without terms index and docstores. it would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. if the warmer is undefined on iw, we probably leave things as they are. it is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in nrt context, and every subreader coming from getreader is guaranteed to be warmed up -> you don't have to introduce even more mess in your code by rechecking it.",
        "label": 33
    },
    {
        "text": "add mergepolicy to indexwriterconfig now that indexwriterconfig is in place, i'd like to move mergepolicy to it as well. the change is not straightforward and so i've kept it for a separate issue. mergepolicy requires in its ctor an indexwriter, however none can be passed to it before an indexwriter actually exists. and today iw may create an mp just for it to be overridden by the application one line afterwards. i don't want to make iw member of mp non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. so the proposed changes are: add a setonce object (to o.a.l.util), or immutable, which can only be set once (hence its name). it'll have the signature setonce<t> w/ synchronized set<t> and t get(). t will be declared volatile, so that get() won't be synchronized. mp will define a protected final setonce<indexwriter> writer instead of the current writer. note: this is a bw break. any suggestions are welcomed. mp will offer a public default ctor, together with a set(indexwriter). indexwriter will set itself on mp using set(this). note that if set will be called more than once, it will throw an exception (alreadysetexception - or does someone have a better suggestion, preferably an already existing java exception?). that's the core idea. i'd like to post a patch soon, so i'd appreciate your review and proposals.",
        "label": 33
    },
    {
        "text": "add a sugar api for traversing categories  mike mccandless said in lucene-java-user mailing list. \"maybe we could add some simple sugar apis? eg something like collection<categorypath> getchildren(int parentord)? (or maybe it returns iterator<categorypath>?)\" what about collection<integer> getchildren(int parentord)? integer would be more versatile and can easily be converted to categorypath with taxonomyreader.getpath.",
        "label": 43
    },
    {
        "text": "rename optimize to a less cool sounding name i think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources. maybe rename to collapsesegments or something?",
        "label": 33
    },
    {
        "text": "fix computation of mergingbytes in tieredmergepolicy it looks like lucene-7976 changed mergingbytes to be computed as the sum of the sizes of eligible segments, rather than the sum of the sizes of segments that are currently merging, which feels wrong.",
        "label": 13
    },
    {
        "text": "split docmaker into contentsource and docmaker this issue proposes some refactoring to the benchmark package. today, docmaker has two roles: collecting documents from a collection and preparing a document object. these two should actually be split up to contentsource and docmaker, which will use a contentsource instance. contentsource will implement all the methods of docmaker, like getnextdocdata, raw size in bytes tracking etc. this can actually fit well w/ 1591, by having a basic contentsource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc. docmaker will implement the makedocument methods, reusing docstate etc. the idea is that collecting the enwiki documents, for example, should be the same whether i create documents using docstate, add payloads or index additional metadata. same goes for trec and reuters collections, as well as linedocmaker. in fact, if one inspects enwikidocmaker and linedocmaker closely, they are 99% the same and 99% different. most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using docstate). that led to a somehwat bizzare extension of linedocmaker by enwikidocmaker (just the reuse of docstate). also, other docmakers do not use that docstate today, something they could have gotten for free with this refactoring proposed. so by having a enwikicontentsource, reuterscontentsource and others (trec, line, simple), i can write several docmakers, such as docstatemaker, configurabledocmaker (one which accpets all kinds of config options) and custom docmakers (payload, facets, sorting), passing to them a contentsource instance and reuse the same docmaking algorithm with many content sources, as well as the same contentsource algorithm with many docmaker implementations. this will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a document object. i've already done so in my code environment (i extend the benchmark package for my application's purposes) and i like the flexibility i have. i think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well.",
        "label": 33
    },
    {
        "text": "remove memory codecs from the codebase memory codecs (memorypostings*, memorydocvalues*) are part of random selection of codecs for tests and cause occasional ooms when a test with huge data is selected. we don't use those memory codecs anywhere outside of tests, it has been suggested to just remove them to avoid maintenance costs and ooms in tests. [1] [1] https://apache.markmail.org/thread/mj53os2ekyldsoy3",
        "label": 11
    },
    {
        "text": "segmentcorereader's  owner  reference back to the first segmentreader causes leaks spinoff from lucene-5248, where shai discovered this ... segmentcorereaders has a segmentreader owner member, that points to the first segmentreader that was opened. when that sr is reopened to sr2, e.g. because new deletes or ndv updates happened, the same scr is shared. but, even if you close sr1, any thing it points to cannot be gcd because scr is pointing to it. i think the big things are livedocs and the ndv update maps; shai is going to fix the latter in lucene-5248, so this issue should fix livedocs. the simplest fix is to make livedocs not final and null it out in doclose ... but that's sort of fragile (what if we add other members in the future and forget to null them on close?). i think it'd be better to eliminate the owner reference; it's only used so we can evict fieldcache entry once the core is closed. maybe we can just store the corecachekey instead?",
        "label": 33
    },
    {
        "text": "demo html parser doesn't work for international documents javacc assumes ascii so it won't work with, say, japanese documents. ideally it would read the charset from the html markup, but that can by tricky. for now assuming unicode would do the trick: add the following line marked with a + to htmlparser.jj: options { static = false; optimize_token_manager = true; //debug_lookahead = true; //debug_token_manager = true; + unicode_input = true; }",
        "label": 40
    },
    {
        "text": "adding emptydocidset iterator adding convenience classes for emptydocidset and emptydocidsetiterator",
        "label": 33
    },
    {
        "text": "improve segmentreader getxxxdocvalues today we do two hash lookups, where in most cases a single one is enough. e.g. sr.getnumericdocvalues initializes the fieldinfo (first lookup in fieldinfos), however if that field was already initialized, we can simply check dvfields.get(). this can be improved in all getxxxdocvalues as well as getdocswithfield.",
        "label": 40
    },
    {
        "text": "implement explain in toparentblockjoinquery blockjoinweight at the moment, toparentblockjoinquery$blockjoinweight.explain throws an unsupportedoperationexception. it would be useful if it could instead return the score of parent document, even if the explanation on how that score was calculated is missing.",
        "label": 31
    },
    {
        "text": "java lang nosuchfielderror  org apache lucene util version lucene i have a run time error when running the below code snippet on websphere application server 8.5: analyzer analyzer = new standardanalyzer(version.lucene_47); indexwriterconfig iw = new indexwriterconfig(version.lucene_47,analyzer); directory index = new ramdirectory(); indexwriter writer = new indexwriter(index,iw); the error is: java.lang.nosuchfielderror: org/apache/lucene/util/version.lucene_47",
        "label": 53
    },
    {
        "text": "nightly builds have wrong version   need to simplify jenkins config tweaks needed after a release right now, if you look at the configuration for these two apache jenkins jobs... https://builds.apache.org/job/lucene-artifacts-4.x/ https://builds.apache.org/job/solr-artifacts-4.x/ ..you can see that even though they are building off of the 4.x branch, and even though the 4.x branch says the next version is 4.4, the artifacts from these jobs are labeled as if they will be 4.1 releases.",
        "label": 53
    },
    {
        "text": "spatial extras bboxstrategy and  confusingly  pointvectorstrategy use legacy numeric encoding we need to deprecate these since they work on the old encoding and provide points based alternatives.",
        "label": 36
    },
    {
        "text": "teststressindexing2 testmulticonfig failure trunk: r1134311 reproducible     [junit] testsuite: org.apache.lucene.index.teststressindexing2     [junit] tests run: 1, failures: 2, errors: 0, time elapsed: 0.882 sec     [junit]      [junit] ------------- standard error -----------------     [junit] java.lang.assertionerror: ram was 460908 expected: 408216 flush mem: 395100 active: 65808     [junit]     at org.apache.lucene.index.documentswriterflushcontrol.assertmemory(documentswriterflushcontrol.java:102)     [junit]     at org.apache.lucene.index.documentswriterflushcontrol.doafterdocument(documentswriterflushcontrol.java:164)     [junit]     at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:380)     [junit]     at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1473)     [junit]     at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1445)     [junit]     at org.apache.lucene.index.teststressindexing2$indexingthread.indexdoc(teststressindexing2.java:723)     [junit]     at org.apache.lucene.index.teststressindexing2$indexingthread.run(teststressindexing2.java:757)     [junit] note: reproduce with: ant test -dtestcase=teststressindexing2 -dtestmethod=testmulticonfig -dtests.seed=2571834029692482827:-8116419692655152763     [junit] note: reproduce with: ant test -dtestcase=teststressindexing2 -dtestmethod=testmulticonfig -dtests.seed=2571834029692482827:-8116419692655152763     [junit] the following exceptions were thrown by threads:     [junit] *** thread: thread-0 ***     [junit] junit.framework.assertionfailederror: java.lang.assertionerror: ram was 460908 expected: 408216 flush mem: 395100 active: 65808     [junit]     at junit.framework.assert.fail(assert.java:47)     [junit]     at org.apache.lucene.index.teststressindexing2$indexingthread.run(teststressindexing2.java:762)     [junit] note: test params are: codec=randomcodecprovider: {f33=standard, f57=mockfixedintblock(blocksize=649), f11=standard, f41=mockrandom, f40=standard, f62=mockrandom, f75=standard, f73=mocksep, f29=mockfixedintblock(blocksize=649), f83=mockrandom, f66=mocksep, f49=mockvariableintblock(baseblocksize=9), f72=pulsing(freqcutoff=7), f54=standard, id=mockfixedintblock(blocksize=649), f80=mockrandom, f94=mocksep, f93=pulsing(freqcutoff=7), f95=standard}, locale=en_sg, timezone=pacific/palau     [junit] note: all tests run in this jvm:     [junit] [teststressindexing2]     [junit] note: linux 2.6.39-gentoo amd64/sun microsystems inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=133324528,total=158400512     [junit] ------------- ---------------- ---------------     [junit] testcase: testmulticonfig(org.apache.lucene.index.teststressindexing2):     failed     [junit] r1.numdocs()=17 vs r2.numdocs()=16     [junit] junit.framework.assertionfailederror: r1.numdocs()=17 vs r2.numdocs()=16     [junit]     at org.apache.lucene.index.teststressindexing2.verifyequals(teststressindexing2.java:308)     [junit]     at org.apache.lucene.index.teststressindexing2.verifyequals(teststressindexing2.java:278)     [junit]     at org.apache.lucene.index.teststressindexing2.testmulticonfig(teststressindexing2.java:124)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1403)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1321)     [junit]      [junit]      [junit] testcase: testmulticonfig(org.apache.lucene.index.teststressindexing2):     failed     [junit] some threads threw uncaught exceptions!     [junit] junit.framework.assertionfailederror: some threads threw uncaught exceptions!     [junit]     at org.apache.lucene.util.lucenetestcase.teardown(lucenetestcase.java:603)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1403)     [junit]     at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:1321)     [junit]      [junit]      [junit] test org.apache.lucene.index.teststressindexing2 failed",
        "label": 46
    },
    {
        "text": "extend expression grammar to allow advanced  variables  we currently allow dots in \"variable\" names in expressions, so that we can fake out object access. we should extend this to allow array access as well (both integer and string keys). this would allow faking out full object nesting through bindings.",
        "label": 41
    },
    {
        "text": "matchalldocsquery  multisearcher and a custom hitcollector throwing exception i have encountered an issue with lucene1.9.1. it involves matchalldocsquery, multisearcher and a custom hitcollector. the following code throws java.lang.unsupportedoperationexception. if i remove the matchalldocsquery condition (comment whole //1 block), or if i dont use the custom hitcollector (ms.search(mbq); instead of ms.search(mbq, allcoll) the exception goes away. by stepping into the source i can see it seems due to matchalldocsquery no implementing extractterms().... searcher searcher = new indexsearcher(\"c:\\\\projects\\\\mig\\\\runtime\\\\index\\\\01aug16 \"); searchable[] indexes = new indexsearcher[1]; indexes[0] = searcher; multisearcher ms = new multisearcher(indexes); allcollector allcoll = new allcollector(ms); booleanquery mbq = new booleanquery(); mbq.add(new termquery(new term(\"body\", \"value1\")), booleanclause.occur.must_not); // 1 matchalldocsquery alld = new matchalldocsquery(); mbq.add(alld, booleanclause.occur.must); // system.out.println(\"query: \" + mbq.tostring()); // 2 ms.search(mbq, allcoll); //ms.search(mbq);",
        "label": 55
    },
    {
        "text": "buildandpushrelease py should run precommit  or the equivalent  buildandpushrelease.py runs ant clean test before building a release, but does not run precommit. as a result, it's possible to build releases with source code that fails precommit.",
        "label": 47
    },
    {
        "text": "nullpointerexception from segmentinfos findsegmentsfile run  if fsdirectory list  returns null found this bug while running unit tests to verify an upgrade of our system from 1.4.3 to 2.1.0. this bug did not occur during 1.4.3, it is new to 2.x (i'm pretty sure it's 2.1-only) if the index directory gets deleted out from under lucene after the fsdirectory has been created, then attempts to open an indexwriter or indexreader will result in an npe. lucene should be throwing an ioexception in this case. repro: 1) create an fsdirectory pointing somewhere in the filesystem (e.g. /foo/index/1) 2) rm -rf the parent dir (rm -rf /foo/index) 3) try to open an indexreader result: nullpointerexception on line \"for(int i=0;i<files.length;i++) { \" \u2013 'files' is null. expect: ioexception .... this is happening because of a missing null check in segmentinfos$findsegmentsfile.run(): if (0 == method) { if (directory != null) { files = directory.list(); } else { files = filedirectory.list(); } gen = getcurrentsegmentgeneration(files); if (gen == -1) { string s = \"\"; for(int i=0;i<files.length;i++) { s += \" \" + files[i]; } throw new filenotfoundexception(\"no segments* file found: files:\" + s); } } the fsdirectory constructor will make sure the index dir exists, but if it is for some reason deleted out from underneath lucene after the fsdirectory is instantiated, then java.io.file.list() will return null. probably better to fix fsdirectory.list() to just check for null and return a 0-length array: (in org/apache/lucene/store/fsdirectory.java) 314c314,317 < return directory.list(indexfilenamefilter.getfilter()); \u2014 > string[] toret = directory.list(indexfilenamefilter.getfilter()); > if (toret == null) > return new string[]{}; > return toret;",
        "label": 33
    },
    {
        "text": "booleanquery explain with boost booleanweight.explain() uses the returned score of subweights to determine if a clause matched. if any required clause has boost==0, the returned score will be zero and the explain for the entire booleanweight will be simply explanation(0.0f, \"match required\"). i'm not sure what the correct fix is here. i don't think it can be done based on score alone, since that isn't how scorers work. perhaps we need a new method \"boolean explain.matched()\" that returns true on a match, regardless of what the score may be? related to the problem above, even if no boosts are zero, it it sometimes nice to know why a particular query failed to match. it would mean a longer explanation, but maybe we should include non matching explains too?",
        "label": 18
    },
    {
        "text": "testconstantscorerangequery does not compile with ecj testconstantscorerangequery has an assertequals(string, float, float) but most of the calls to assertequals are (string, int, int). ecj complains with the following error: the method assertequals(string, float, float) is ambiguous for the type testconstantscorerangequery the simple solution is to supply an assertequals(string, int, int) which calls assert.assertequals(string, int, int) patch to follow.",
        "label": 55
    },
    {
        "text": "rename keywordmarkertokenfilter i would like to rename keywordmarkertokenfilter to keywordmarkerfilter. we havent released it yet, so its a good time to keep the name brief and consistent.",
        "label": 40
    },
    {
        "text": "move jdk collation to core  icu collation to icu contrib as mentioned on the list, i propose we move the jdk-based collationkeyfilter/collationkeyanalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting) these are not much code (the heavy duty stuff is already in core, indexablebinarystring). and i would also like to move the icucollationkeyfilter/icucollationkeyanalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu. this way, we can start looking at integrating other functionality from icu into a fully-fleshed out icu contrib.",
        "label": 40
    },
    {
        "text": "fuzzy searches do not get a boost of as stated in  query syntax  doc according to the website's \"query syntax\" page, fuzzy searches are given a boost of 0.2. i've found this not to be the case, and have seen situations where exact matches have lower relevance scores than fuzzy matches. rather than getting a boost of 0.2, it appears that all variations on the term are first found in the model, where dist* > 0.5. dist = levenshteindistance / length of min(termlength, variantlength) this then leads to a boolean or search of all the variant terms, each of whose boost is set to (dist - 0.5)*2 for that variant. the upshot of all of this is that there are many cases where a fuzzy match will get a higher relevance score than an exact match. see this email for a test case to reproduce this anomalous behaviour. http://www.mail-archive.com/lucene-dev@jakarta.apache.org/msg02819.html here is a candidate patch to address the issue - lucene-1.2\\src\\java\\org\\apache\\lucene\\search\\fuzzytermenum.java sun jun 09 13:47:54 2002 lucene-1.2-modified\\src\\java\\org\\apache\\lucene\\search\\fuzzytermenum.java fri mar 14 11:37:20 2003 *************** 99,105 **** } final protected float difference() { ! return (float)((distance - fuzzy_threshold) * scale_factor); } final public boolean endenum() { --- 99,109 ---- } final protected float difference() { ! if (distance == 1.0) { ! return 1.0f; ! } ! else ! return (float)((distance - fuzzy_threshold) * scale_factor); } final public boolean endenum() { *************** 111,117 **** ******************************/ public static final double fuzzy_threshold = 0.5; ! public static final double scale_factor = 1.0f / (1.0f - fuzzy_threshold); /** finds and returns the smallest of three integers \u2014 115,121 ---- ******************************/ public static final double fuzzy_threshold = 0.5; ! public static final double scale_factor = 0.2f * (1.0f / (1.0f - fuzzy_threshold)); /** finds and returns the smallest of three integers",
        "label": 40
    },
    {
        "text": "fastvectorhighlighter fail to highlight taking in input some proximity query  there are 2 related bug with proximity query 1) in a phrase there are n repeated terms the fvh module fails to highlight that. see testrepeatedtermswithslop 2) if you search the terms reversed the fvh module fails to highlight that. see testreversedtermswithslop",
        "label": 26
    },
    {
        "text": "added new token api impl for asciifoldingfilter i added an implementation of incrementtoken to asciifoldingfilter.java and extended the existing testcase for it. i will attach the patch shortly. beside this improvement i would like to start up a small discussion about this filter. asciifoldingfitler is meant to be a replacement for isolatin1accentfilter which is quite nice as it covers a superset of the latter. i have used this filter quite often but never on a as it is basis. in the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for german umlaut it does not return the expected result. a german umlaut like '\u00e4' does not translate to a but rather to 'ae'. i would like to change this but i'n not 100% sure if that is expected by all users of that filter. another way of doing it would be to make it configurable with a flag. this would not affect performance as we only check if such a umlaut char is found. further it would be really helpful if that filter could \"inject\" the original/unmodified token with the same position increment into the token stream on demand. i think its a valid use-case to index the modified and unmodified token. for instance, the german word \"s\u00fcd\" would be folded to \"sud\". in a query q:(s\u00fcd) the filter would also fold to sud and therefore find sud which has a totally different meaning. folding works quite well but for special cases would could add those options to make users life easier. the latter could be done in a subclass while the umlaut problem should be fixed in the base class. simon",
        "label": 53
    },
    {
        "text": "out of date code examples the following api documents have code examples: http://lucene.apache.org/core/4_1_0/facet/org/apache/lucene/facet/index/ordinalmappingatomicreader.html http://lucene.apache.org/core/4_1_0/facet/org/apache/lucene/facet/index/ordinalmappingatomicreader.html \"// merge the old taxonomy with the new one. ordinalmap map = directorytaxonomywriter.addtaxonomies();\" the two code examples call the directorytaxonomywriter.addtaxonomies method. lucene 3.5 has that method, according to its document: http://lucene.apache.org/core/old_versioned_docs/versions/3_5_0/api/all/org/apache/lucene/facet/taxonomy/directory/directorytaxonomywriter.html however, lucene 4.1 does not have such a method, according to its document\uff1a http://lucene.apache.org/core/4_1_0/facet/org/apache/lucene/facet/taxonomy/directory/directorytaxonomywriter.html please update the code examples to reflect the latest implementation.",
        "label": 43
    },
    {
        "text": "testsimpletextpointsformat testwithexceptions  failure reproducing branch_6x seed from https://jenkins.thetaphi.de/job/lucene-solr-6.x-macosx/690/:    [junit4] suite: org.apache.lucene.codecs.simpletext.testsimpletextpointsformat    [junit4] ignor/a 0.02s j0 | testsimpletextpointsformat.testrandombinarybig    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly())    [junit4]   2> note: reproduce with: ant test  -dtestcase=testsimpletextpointsformat -dtests.method=testwithexceptions -dtests.seed=cce1e867577cfff6 -dtests.slow=true -dtests.locale=uk-ua -dtests.timezone=asia/qatar -dtests.asserts=true -dtests.file.encoding=iso-8859-1    [junit4] error   0.93s j0 | testsimpletextpointsformat.testwithexceptions <<<    [junit4]    > throwable #1: java.lang.illegalstateexception: this writer hit an unrecoverable error; cannot complete forcemerge    [junit4]    >  at __randomizedtesting.seedinfo.seed([cce1e867577cfff6:6eb2741bd8f2b00c]:0)    [junit4]    >  at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1931)    [junit4]    >  at org.apache.lucene.index.indexwriter.forcemerge(indexwriter.java:1881)    [junit4]    >  at org.apache.lucene.index.randomindexwriter.forcemerge(randomindexwriter.java:429)    [junit4]    >  at org.apache.lucene.index.basepointsformattestcase.verify(basepointsformattestcase.java:701)    [junit4]    >  at org.apache.lucene.index.basepointsformattestcase.testwithexceptions(basepointsformattestcase.java:224)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]    > caused by: org.apache.lucene.index.corruptindexexception: problem reading index from mockdirectorywrapper(niofsdirectory@/users/jenkins/workspace/lucene-solr-6.x-macosx/lucene/build/codecs/test/j0/temp/lucene.codecs.simpletext.testsimpletextpointsformat_cce1e867577cfff6-001/tempdir-001 lockfactory=org.apache.lucene.store.nativefslockfactory@4d6de658) (resource=mockdirectorywrapper(niofsdirectory@/users/jenkins/workspace/lucene-solr-6.x-macosx/lucene/build/codecs/test/j0/temp/lucene.codecs.simpletext.testsimpletextpointsformat_cce1e867577cfff6-001/tempdir-001 lockfactory=org.apache.lucene.store.nativefslockfactory@4d6de658))    [junit4]    >  at org.apache.lucene.index.segmentcorereaders.<init>(segmentcorereaders.java:140)    [junit4]    >  at org.apache.lucene.index.segmentreader.<init>(segmentreader.java:74)    [junit4]    >  at org.apache.lucene.index.readersandupdates.getreader(readersandupdates.java:145)    [junit4]    >  at org.apache.lucene.index.readersandupdates.getreaderformerge(readersandupdates.java:617)    [junit4]    >  at org.apache.lucene.index.indexwriter.mergemiddle(indexwriter.java:4293)    [junit4]    >  at org.apache.lucene.index.indexwriter.merge(indexwriter.java:3940)    [junit4]    >  at org.apache.lucene.index.concurrentmergescheduler.domerge(concurrentmergescheduler.java:588)    [junit4]    >  at org.apache.lucene.index.concurrentmergescheduler$mergethread.run(concurrentmergescheduler.java:626)    [junit4]    > caused by: java.io.filenotfoundexception: a random ioexception (_0.inf)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.maybethrowioexceptiononopen(mockdirectorywrapper.java:575)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:744)    [junit4]    >  at org.apache.lucene.store.directory.openchecksuminput(directory.java:137)    [junit4]    >  at org.apache.lucene.store.mockdirectorywrapper.openchecksuminput(mockdirectorywrapper.java:1072)    [junit4]    >  at org.apache.lucene.codecs.simpletext.simpletextfieldinfosformat.read(simpletextfieldinfosformat.java:73)    [junit4]    >  at org.apache.lucene.index.segmentcorereaders.<init>(segmentcorereaders.java:107)    [junit4]    >  ... 7 more    [junit4] ignor/a 0.01s j0 | testsimpletextpointsformat.testmergestability    [junit4]    > assumption #1: merge is not stable    [junit4]   2> note: leaving temporary files on disk at: /users/jenkins/workspace/lucene-solr-6.x-macosx/lucene/build/codecs/test/j0/temp/lucene.codecs.simpletext.testsimpletextpointsformat_cce1e867577cfff6-001    [junit4]   2> note: test params are: codec=asserting(lucene62): {}, docvalues:{}, maxpointsinleafnode=1626, maxmbsortinheap=5.745076713359786, sim=randomsimilarity(querynorm=true,coord=crazy): {}, locale=uk-ua, timezone=asia/qatar    [junit4]   2> note: mac os x 10.11.6 x86_64/oracle corporation 1.8.0_121 (64-bit)/cpus=3,threads=1,free=153242552,total=251133952    [junit4]   2> note: all tests run in this jvm: [testsimpletextfieldinfoformat, testdirectpostingsformat, testsimpletexttermvectorsformat, testfstordpostingsformat, testsimpletextpointsformat]    [junit4] completed [16/20 (1!)] on j0 in 22.49s, 18 tests, 1 error, 2 skipped <<< failures!",
        "label": 33
    },
    {
        "text": "logmergepolicy should use the number of deleted docs when deciding which segments to merge i found that indexwriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. and the existence of such segments affected the query performance significantly. i created an index with 1 million docs, then went over all docs and updated a few thousand at a time. i ran optimize(20) occasionally. what saw were large segments with most of docs deleted. although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created. this is because logmergepolicy.findmergeforoptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. so, a simple fix is to use the delete count to calibrate the segment size. i can create a patch for this.",
        "label": 33
    },
    {
        "text": "tiered flushing of dwpts by ram with low high water marks now that we have documentswriterperthreads we need to track total consumed ram across all dwpts. a flushing strategy idea that was discussed in lucene-2324 was to use a tiered approach: flush the first dwpt at a low water mark (e.g. at 90% of allowed ram) flush all dwpts at a high water mark (e.g. at 110%) use linear steps in between high and low watermark: e.g. when 5 dwpts are used, flush at 90%, 95%, 100%, 105% and 110%. should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120mb, high water mark at 140mb)? or shall we keep for simplicity the single setrambuffersizemb() config method and use something like 90% and 110% for the water marks?",
        "label": 46
    },
    {
        "text": "lruquerycache can leak locks if a querycache is shared between two searchers, one of which has an indexreader with no cachehelper, then cachingwrapperweight can leak locks in scorersupplier() and bulkscorer(). this can cause the indexreader that does have a cachehelper to hang on close.",
        "label": 2
    },
    {
        "text": "add getters for the properties of several query implementations hi! at hibernate search, we are currently working on an elasticsearch backend (aside from the existing lucene backend). as part of this effort, to provide a smooth migration path, we need to be able to rewrite the lucene queries as elasticsearch queries. we know it will be neither perfect or comprehensive but we want it to be the best possible experience. it works well in many cases but several implementations of query don't have the necessary getters to be able to extract the information from the query. the attached patch add getters to several implementations of query. it would be nice if it could be applied. any chance it could be applied to the next point release too? (probably not but i'd better ask).",
        "label": 2
    },
    {
        "text": "document not guaranteed to be found after write and commit after same email on developer list: \"i developed a stress test to assert that a new document containing a specific term \"x\" is always found after a commit on the indexwriter. this works most of the time, but it fails under load in rare occasions. i'm testing with 40 threads, both with a serialmergescheduler and a concurrentmergescheduler, all sharing a common indexwriter. attached testcase is using a ramdirectory only, but i verified a fsdirectory behaves in the same way so i don't believe it's the directory implementation or the mergescheduler. this test is slow, so i don't consider it a functional or unit test. it might give false positives: it doesn't always fail, sorry i couldn't find out how to make it more likely to happen, besides scheduling it to run for a longer time.\" i tested this to affect versions 2.4.1 and 2.9.1;",
        "label": 33
    },
    {
        "text": "multifunction anyexists   creating functionvalues  objects for every document in the class org.apache.lucene.queries.function.valuesource.multifunction there is the following method signature (line 52) public static boolean allexists(int doc, functionvalues... values) this method is called from the class org.apache.lucene.queries.function.valuesource.dualfloatfunction (line 68) public boolean exists(int doc) { return multifunction.allexists(doc, avals, bvals); } because multifunction.allexists uses java varargs syntax (\"...\") a new functionvalues[] object will be created every time this call takes place. the problem is that the call takes place in a document level function, which means that it will create new objects in the heap for every document in the query results. for example if you use the following boost function (where ds and dc1 are both triedatefield) bf=min(ms(ds,dc1),604800000) you will get extra objects created for each document in the result set, which has a big impact on performance and memory usage if you are searching a large result set.",
        "label": 18
    },
    {
        "text": "random shape generator timeout hi karl wright, there was a timeout in the tests. it can be reproduced with this command: ant test -dtestcase=randomgeoshaperelationshiptest -dtests.method=testrandomdisjoint -dtests.seed=6401169bbc3b714f -dtests.multiplier=3 -dtests.slow=true -dtests.locale=ro -dtests.timezone=iran -dtests.asserts=true i attach the fix for the random geoshape generator. thanks!",
        "label": 25
    },
    {
        "text": "indexwriter addindexes copies raw files but acquires no locks i see stuff like: \"merge problem with lucene 3 and 4 indices\" (from solr users list), and cannot even think how to respond to these users because so many things can go wrong with indexwriter.addindexes(directory) it currently has in its javadocs: note: the index in each directory must not be changed (opened by a writer) while this method is running. this method does not acquire a write lock in each input directory, so it is up to the caller to enforce this. this method should be acquiring locks: its copying raw files. otherwise we should remove it. if someone doesnt like that, or is mad because its 10ns slower, they can use nolockfactory.",
        "label": 33
    },
    {
        "text": "analyzingqueryparser can't work with leading wildcards  the getwildcardquery mehtod in analyzingqueryparser.java need the following changes to accept leading wildcards: protected query getwildcardquery(string field, string termstr) throws parseexception { string usetermstr = termstr; string leadingwildcard = null; if (\"*\".equals(field)) { if (\"*\".equals(usetermstr)) return new matchalldocsquery(); } boolean hasleadingwildcard = (usetermstr.startswith(\"*\") || usetermstr.startswith(\"?\")) ? true : false; if (!getallowleadingwildcard() && hasleadingwildcard) throw new parseexception(\"'*' or '?' not allowed as first character in wildcardquery\"); if (getlowercaseexpandedterms()) { usetermstr = usetermstr.tolowercase(); } if (hasleadingwildcard) { leadingwildcard = usetermstr.substring(0, 1); usetermstr = usetermstr.substring(1); } list tlist = new arraylist(); list wlist = new arraylist(); /* somewhat a hack: find/store wildcard chars in order to put them back after analyzing */ boolean iswithintoken = (!usetermstr.startswith(\"?\") && !usetermstr.startswith(\"*\")); iswithintoken = true; stringbuffer tmpbuffer = new stringbuffer(); char[] chars = usetermstr.tochararray(); for (int i = 0; i < usetermstr.length(); i++) { if (chars[i] == '?' || chars[i] == '*') unknown macro: { if (iswithintoken) { tlist.add(tmpbuffer.tostring()); tmpbuffer.setlength(0); } iswithintoken = false; } else unknown macro: { if (!iswithintoken) { wlist.add(tmpbuffer.tostring()); tmpbuffer.setlength(0); } iswithintoken = true; } tmpbuffer.append(chars[i]); } if (iswithintoken) { tlist.add(tmpbuffer.tostring()); } else { wlist.add(tmpbuffer.tostring()); } // get analyzer from superclass and tokenize the term tokenstream source = getanalyzer().tokenstream(field, new stringreader(usetermstr)); org.apache.lucene.analysis.token t; int counttokens = 0; while (true) { try { t = source.next(); } catch (ioexception e) { t = null; } if (t == null) { break; } if (!\"\".equals(t.termtext())) { try { tlist.set(counttokens++, t.termtext()); } catch (indexoutofboundsexception ioobe) { counttokens = -1; } } } try { source.close(); } catch (ioexception e) { // ignore } if (counttokens != tlist.size()) { /* * this means that the analyzer used either added or consumed * (common for a stemmer) tokens, and we can't build a wildcardquery */ throw new parseexception(\"cannot build wildcardquery with analyzer \" + getanalyzer().getclass() + \" - tokens added or lost\"); } if (tlist.size() == 0) { return null; } else if (tlist.size() == 1) { if (wlist.size() == 1) { /* if wlist contains one wildcard, it must be at the end, because: 1) wildcards at 1st position of a term by queryparser where truncated 2) if wildcard was not in end, there would be two or more tokens */ stringbuffer sb = new stringbuffer(); if (hasleadingwildcard) { // adding leadingwildcard sb.append(leadingwildcard); } sb.append((string) tlist.get(0)); sb.append(wlist.get(0).tostring()); return super.getwildcardquery(field, sb.tostring()); } else if (wlist.size() == 0 && hasleadingwildcard) { /* * if wlist contains no wildcard, it must be at 1st position */ stringbuffer sb = new stringbuffer(); if (hasleadingwildcard) { // adding leadingwildcard sb.append(leadingwildcard); } sb.append((string) tlist.get(0)); sb.append(wlist.get(0).tostring()); return super.getwildcardquery(field, sb.tostring()); } else { /* * we should never get here! if so, this method was called with * a termstr containing no wildcard ... */ throw new illegalargumentexception(\"getwildcardquery called without wildcard\"); } } else { /* the term was tokenized, let's rebuild to one token with wildcards put back in postion */ stringbuffer sb = new stringbuffer(); if (hasleadingwildcard) { // adding leadingwildcard sb.append(leadingwildcard); } for (int i = 0; i < tlist.size(); i++) unknown macro: { sb.append((string) tlist.get(i)); if (wlist != null && wlist.size() > i) { sb.append((string) wlist.get(i)); } } return super.getwildcardquery(field, sb.tostring()); } }",
        "label": 47
    },
    {
        "text": " new directorytaxonomyreader directory  throws  indexnotfoundexception  no segments  file found  on a new taxonomy directory i made a taxonomy directory with categorydir=fsdirectory.open(new file(\"category\")); taxowriter=new directorytaxonomywriter(categorydir, openmode.create_or_append); right after creating directorytaxonomywriter, i created a directorytaxonomyreader with taxoreader=new directorytaxonomyreader(categorydir); which throws indexnotfoundexception. it used to work fine with lucene 4.1. if i invoke new directorytaxonomyreader(directorytaxonomywriter) on a new taxonomy directory, no exception is thrown. below is the exception stack trace. org.apache.lucene.index.indexnotfoundexception: no segments* file found in org.apache.lucene.store.mmapdirectory@/home/elisa/repos/mine/zeroirclog/irclog-category lockfactory=org.apache.lucene.store.nativefslockfactory@373983f: files: [write.lock, _0.si, _0.fnm, _0.fdt, _0_lucene41_0.tim, _0_lucene41_0.pos, _0.fdx, _0_lucene41_0.doc, _0_lucene41_0.tip] at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:741) ~[lucene-core-4.2.0.jar:4.2.0 1453694 - rmuir - 2013-03-06 22:25:29] at org.apache.lucene.index.standarddirectoryreader.open(standarddirectoryreader.java:52) ~[lucene-core-4.2.0.jar:4.2.0 1453694 - rmuir - 2013-03-06 22:25:29] at org.apache.lucene.index.directoryreader.open(directoryreader.java:65) ~[lucene-core-4.2.0.jar:4.2.0 1453694 - rmuir - 2013-03-06 22:25:29] at org.apache.lucene.facet.taxonomy.directory.directorytaxonomyreader.openindexreader(directorytaxonomyreader.java:218) ~[lucene-facet-4.2.0.jar:4.2.0 1453694 - rmuir - 2013-03-06 22:26:53] at org.apache.lucene.facet.taxonomy.directory.directorytaxonomyreader.<init>(directorytaxonomyreader.java:99) ~[lucene-facet-4.2.0.jar:4.2.0 1453694 - rmuir - 2013-03-06 22:26:53] at org.zeroirclog.luceneloggerworker.<init>(luceneloggerworker.java:141) ~[na:na]",
        "label": 43
    },
    {
        "text": "testaddtaxonomy testconcurrency failure on the 3.x branch:     [junit] testsuite: org.apache.lucene.facet.taxonomy.directory.testaddtaxonomy     [junit] testcase: testconcurrency(org.apache.lucene.facet.taxonomy.directory.testaddtaxonomy): caused an error     [junit] index: 1, size: 2     [junit] java.lang.indexoutofboundsexception: index: 1, size: 2     [junit]  at java.util.arraylist.rangecheck(arraylist.java:604)     [junit]  at java.util.arraylist.get(arraylist.java:382)     [junit]  at org.apache.lucene.facet.taxonomy.writercache.cl2o.charblockarray.charat(charblockarray.java:148)     [junit]  at org.apache.lucene.facet.taxonomy.categorypath.equalstoserialized(categorypath.java:888)     [junit]  at org.apache.lucene.facet.taxonomy.writercache.cl2o.compactlabeltoordinal.getordinal(compactlabeltoordinal.java:323)     [junit]  at org.apache.lucene.facet.taxonomy.writercache.cl2o.compactlabeltoordinal.getordinal(compactlabeltoordinal.java:163)     [junit]  at org.apache.lucene.facet.taxonomy.writercache.cl2o.cl2otaxonomywritercache.get(cl2otaxonomywritercache.java:49)     [junit]  at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.findcategory(directorytaxonomywriter.java:386)     [junit]  at org.apache.lucene.facet.taxonomy.directory.directorytaxonomywriter.addtaxonomy(directorytaxonomywriter.java:833)     [junit]  at org.apache.lucene.facet.taxonomy.directory.testaddtaxonomy.testconcurrency(testaddtaxonomy.java:206)     [junit]  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     [junit]  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)     [junit]  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)     [junit]  at java.lang.reflect.method.invoke(method.java:601)     [junit]  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45)     [junit]  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)     [junit]  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42)     [junit]  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)     [junit]  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:30)     [junit]  at org.apache.lucene.util.lucenetestcase$subclasssetupteardownrule$1.evaluate(lucenetestcase.java:630)     [junit]  at org.apache.lucene.util.lucenetestcase$internalsetupteardownrule$1.evaluate(lucenetestcase.java:536)     [junit]  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:67)     [junit]  at org.apache.lucene.util.lucenetestcase$testresultinterceptorrule$1.evaluate(lucenetestcase.java:457)     [junit]  at org.apache.lucene.util.uncaughtexceptionsrule$1.evaluate(uncaughtexceptionsrule.java:74)     [junit]  at org.apache.lucene.util.lucenetestcase$savethreadandtestnamerule$1.evaluate(lucenetestcase.java:508)     [junit]  at org.junit.rules.runrules.evaluate(runrules.java:18)     [junit]  at org.junit.runners.parentrunner.runleaf(parentrunner.java:263)     [junit]  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:68)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:146)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:50)     [junit]  at org.junit.runners.parentrunner$3.run(parentrunner.java:231)     [junit]  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:60)     [junit]  at org.junit.runners.parentrunner.runchildren(parentrunner.java:229)     [junit]  at org.junit.runners.parentrunner.access$000(parentrunner.java:50)     [junit]  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:222)     [junit]  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)     [junit]  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:30)     [junit]  at org.apache.lucene.util.uncaughtexceptionsrule$1.evaluate(uncaughtexceptionsrule.java:74)     [junit]  at org.apache.lucene.util.storeclassnamerule$1.evaluate(storeclassnamerule.java:36)     [junit]  at org.apache.lucene.util.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:67)     [junit]  at org.junit.rules.runrules.evaluate(runrules.java:18)     [junit]  at org.junit.runners.parentrunner.run(parentrunner.java:300)     [junit]  at junit.framework.junit4testadapter.run(junit4testadapter.java:39)     [junit]  at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:518)     [junit]  at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:1052)     [junit]  at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:879)     [junit]      [junit]      [junit] tests run: 5, failures: 0, errors: 1, time elapsed: 0.342 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: ignoring nightly-only test method 'testbig'     [junit] warning: test method: 'testconcurrency' left thread running: thread[thread-1,5,main]     [junit] resource leak: test method: 'testconcurrency' left 1 thread(s) running     [junit] note: reproduce with: ant test -dtestcase=testaddtaxonomy -dtestmethod=testconcurrency -dtests.seed=ad99a0aac3d5bf3:4af538a36d0c94b4:-7b609955992bc1c9 -dargs=\"-dfile.encoding=utf-8\"     [junit] note: reproduce with: ant test -dtestcase=testaddtaxonomy -dtestmethod=testconcurrency -dtests.seed=ad99a0aac3d5bf3:4af538a36d0c94b4:-7b609955992bc1c9 -dargs=\"-dfile.encoding=utf-8\"     [junit] note: test params are: locale=mt_mt, timezone=etc/gmt+6     [junit] note: all tests run in this jvm:     [junit] [twoenhancementstest, testadaptiveexample, categorylistpayloadstreamtest, categoryattributeimpltest, perdimensionindexingparamstest, adaptiveaccumulatortest, testfacetarrays, testfacetsaccumulatorwithcomplement, facetsearchparamstest, samplingaccumulatortest, testtaxonomycombined, testaddtaxonomy]     [junit] note: linux 3.2.0-24-generic amd64/oracle corporation 1.7.0_01 (64-bit)/cpus=8,threads=1,free=171017984,total=269352960     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.facet.taxonomy.directory.testaddtaxonomy failed",
        "label": 43
    },
    {
        "text": "two phase intersection currently some scorers have to do a lot of per-document work to determine if a document is a match. the simplest example is a phrase scorer, but there are others (spans, sloppy phrase, geospatial, etc). imagine a conjunction with two must clauses, one that is a term that matches all odd documents, another that is a phrase matching all even documents. today this conjunction will be very expensive, because the zig-zag intersection is reading a ton of useless positions. the same problem happens with filteredquery and anything else that acts like a conjunction.",
        "label": 10
    },
    {
        "text": "postingsenum impls should respect documented behavior when no positions exist postingsenum.nextpositions says that if no positions exist, no_more_positions will be returned on the first call (actually it refers to docsenum.no_more_docs, which should be changed since docsenum doesn't exist on trunk). at least one impl (blockdocsenum) does assert false inside nextposition(). this means if you have assertions turned on (e.g. in a test) you get an assertion here, when the behavior should return no_more_positions. i'm still going through all the implementations, but i also see multitermhighlighting which returns integer.max_value. i think we should clean up all these implementations which have no positions (including maybe the fake scorers that are copied around in a lot of places?) to return no_more_positions.",
        "label": 41
    },
    {
        "text": "supporting deletedocuments in indexwriter  code and performance results provided  today, applications have to open/close an indexwriter and open/close an indexreader directly or indirectly (via indexmodifier) in order to handle a mix of inserts and deletes. this performs well when inserts and deletes come in fairly large batches. however, the performance can degrade dramatically when inserts and deletes are interleaved in small batches. this is because the ramdirectory is flushed to disk whenever an indexwriter is closed, causing a lot of small segments to be created on disk, which eventually need to be merged. we would like to propose a small api change to eliminate this problem. we are aware that this kind change has come up in discusions before. see http://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049 . the difference this time is that we have implemented the change and tested its performance, as described below. api changes ----------- we propose adding a \"deletedocuments(term term)\" method to indexwriter. using this method, inserts and deletes can be interleaved using the same indexwriter. note that, with this change it would be very easy to add another method to indexwriter for updating documents, allowing applications to avoid a separate delete and insert to update a document. also note that this change can co-exist with the existing apis for deleting documents using an indexreader. but if our proposal is accepted, we think those apis should probably be deprecated. coding changes -------------- coding changes are localized to indexwriter. internally, the new deletedocuments() method works by buffering the terms to be deleted. deletes are deferred until the ramdirectory is flushed to disk, either because it becomes full or because the indexwriter is closed. using java synchronization, care is taken to ensure that an interleaved sequence of inserts and deletes for the same document are properly serialized. we have attached a modified version of indexwriter in release 1.9.1 with these changes. only a few hundred lines of coding changes are needed. all changes are commented by \"change\". we have also attached a modified version of an example from chapter 2.2 of lucene in action. performance results ------------------- to test the performance our proposed changes, we ran some experiments using the trec wt 10g dataset. the experiments were run on a dual 2.4 ghz intel xeon server running linux. the disk storage was configured as raid0 array with 5 drives. before indexes were built, the input documents were parsed to remove the html from them (i.e., only the text was indexed). this was done to minimize the impact of parsing on performance. a simple whitespaceanalyzer was used during index build. we experimented with three workloads: insert only. 1.6m documents were inserted and the final index size was 2.3gb. insert/delete (big batches). the same documents were inserted, but 25% were deleted. 1000 documents were deleted for every 4000 inserted. insert/delete (small batches). in this case, 5 documents were deleted for every 20 inserted. current current new workload indexwriter indexmodifier indexwriter ----------------------------------------------------------------------- insert only 116 min 119 min 116 min insert/delete (big batches) \u2013 135 min 125 min insert/delete (small batches) \u2013 338 min 134 min as the experiments show, with the proposed changes, the performance improved by 60% when inserts and deletes were interleaved in small batches. regards, ning ning li search technologies ibm almaden research center 650 harry road san jose, ca 95120",
        "label": 33
    },
    {
        "text": "inflater end  method not always called in fieldsreader we've just found an insidious memory leak in our own application as we did not always call deflater.end() and inflater.end(). as documented here; http://bugs.sun.com/view_bug.do?bug_id=4797189 the non-heap memory that the native zlib code uses is not freed in a timely manner. fieldswriter appears safe as no exception can be thrown between the deflater's creation and end() as it uses a bytearrayoutputstream fieldsreader, however, is not safe. in the event of a dataformatexception the call to end() will not occur.",
        "label": 33
    },
    {
        "text": "add deprecated 'transition' api for document field i think for 4.0 we should have a deprecated transition api for field so you can do new field(..., field.store.xxx, field.index.yyy) like before. these combinations would just be some predefined fieldtypes that are used behind the scenes if you use these deprecated ctors sure it wouldn't be 'totally' backwards binary compat for field.java, but why must it be all or nothing? i think this would eliminate a big hurdle for people that want to check out 4.x",
        "label": 33
    },
    {
        "text": "date field problems using extractingrequesthandler and java  b71  tracking bug to note that the (tika based) extractingrequesthandler will not work properly with jdk9 starting with build71. this first manifested itself with failures like this from the tests...    [junit4]   2> note: reproduce with: ant test  -dtestcase=extractingrequesthandlertest -dtests.method=testarabicpdf -dtests.seed=232d0a5404c2aded -dtests.multiplier=3 -dtests.slow=true -dtests.locale=en_jm -dtests.timezone=etc/gmt-7 -dtests.asserts=true -dtests.file.encoding=utf-8    [junit4] error   0.58s | extractingrequesthandlertest.testarabicpdf <<<    [junit4]    > throwable #1: org.apache.solr.common.solrexception: invalid date string:'tue mar 09 13:44:49 gmt+07:00 2010' workarround noted by uwe... the test passes on jdk 9 b71 with: -dargs=\"-djava.locale.providers=jre,spi\" this reenabled the old locale data. i will add this to the build parameters of policeman jenkins to stop this from failing. to me it looks like the locale data somehow is not able to correctly parse weekdays and/or timezones. i will check this out tomorrow and report a bug to the openjdk people. there is something fishy with cldr locale data. there are already some bugs open, so work is not yet finished (e.g. sometimes it uses wrong timezone shortcuts,...)",
        "label": 53
    },
    {
        "text": "o a l analysis de germanstemmer crashes on some inputs see the tests from lucene-2560. germananalyzer no longer uses this stemmer by default, but we should fix it.",
        "label": 40
    },
    {
        "text": "smartchineseanalyzer javadoc improvement chinese -> english, and corrections to match reality (removes several javadoc warnings)",
        "label": 46
    },
    {
        "text": "decide if we should remove lines numbers from latest changes as lucene dev has grown, a new issue has arisen - many times, new changes invalidate old changes. a proper changes file should just list the changes from the last version, not document the dev life of the issues. keeping changes in proper order now requires a lot of renumbering sometimes. the numbers have no real meaning and could be added to more rich versions (such as the html version) automatically if desired. i think an * makes a good replacement myself. the issues already have ids that are stable, rather than the current, decorational numbers which are subject to change over a dev cycle. i think we should replace the numbers with an asterix for the 2.9 section and going forward (ie 4. becomes *). if we don't get consensus very quickly, this issue won't block.",
        "label": 29
    },
    {
        "text": "spatialoprecursiveprefixtreetest is failing this has been failing lately on trunk (e.g. on rev 1486339): ant test  -dtestcase=spatialoprecursiveprefixtreetest -dtestmethod=testcontains -dtests.seed=456022665217dadf:2c2a2816bd2ba1c5 -dtests.slow=true -dtests.locale=nl_be -dtests.timezone=poland -dtests.file.encoding=iso-8859-1 not sure what's up ...",
        "label": 10
    },
    {
        "text": "indexwriter commits update documents without corresponding delete while backporting the testcase from lucene-3348 i ran into this thread hazard in the 3.x branch. we actually fixed this issue in lucene-3348 for lucene 4.0 but since dwpt has a slightly different behavior when committing segments i create a new issue to track this down in 3.x. when we prepare a commit we sync on iw flush the dw and apply all deletes then release the lock, maybemerge and start the commit (iw#startcommit(userdata)). yet, a new segment could be flushed via getreader and sneak into the segementinfos which are cloned in iw#startcommit instead of in preparecommit right after the flush.",
        "label": 46
    },
    {
        "text": "don't allocate copy bytes all the time in binary dv producers our binary doc values producers keep on creating new byte[] arrays and copying bytes when a value is requested, which likely doesn't help performance. this has been done because of the way fieldcache consumers used the api, but we should try to fix it in 5.0.",
        "label": 1
    },
    {
        "text": "japanesecharfilter can't be reusable com.hulu.lucene.analysis.japanesecharfilter is used in jpromajianalyzer but incrementtoken() can't be reusable. ------------------------------------------------- if (!isend) { //can't be false when analyzer.tokenstream(...) is called if (input.incrementtoken()) { string reading = readingattr.getreading(); if (reading == null) { reading = new string(termattr.buffer()); } if(reading != null) { reading = japanesestringtools.getromanization(reading.trim()); reading = stringtools.removeaccentcharacter(reading); for (int i = 0; i < reading.length(); i++) { chars.add(reading.charat(i)); } } isend = false; } else { isend = true; } } if (chars.size() > 0 || isend == false) { if (chars.size() > 0) { char ch = chars.poll(); termattr.setempty().append(ch); } return true; } return false; -------------------------------------------------- so i can't the call analyzer.tokenstream for the same field twice.",
        "label": 53
    },
    {
        "text": "token type and flags values get lost when using shinglematrixfilter while using the new shinglematrixfilter i noticed that a token's type and flags get lost while using this filter. shinglefilter does respect these values like the other filters i know.",
        "label": 53
    },
    {
        "text": "contrib module uptodate assume name matches path and jar with adding a new 'queries' module, i am trying to change the project name of contrib/queries to queries-contrib. however currently the contrib-uptodate assumes that the name property is used in the path and in the jar name. by using the name in the path, i must set the value to 'queries' (since the path is contrib/queries). however because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected. consequently i think we need to separate the path name from the jar name properties. for simplicity i think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property.",
        "label": 7
    },
    {
        "text": "sortedsetdocvalues caching   state i just spent some time digging into a bug which was due to the fact that sorted_set doc values are stateful (setdocument/nextord) and are cached per thread. so if you try to get two instances from the same field in the same thread, you will actually get the same instance and won't be able to iterate over ords of two documents in parallel. this is not necessarily a bug, this behavior can be documented, but i think it would be nice if the api could prevent from such mistakes by storing the state in a separate object or cloning the sortedsetdocvalues object in segmentcorereaders.getsortedsetdocvalues? what do you think?",
        "label": 1
    },
    {
        "text": "fst should offer lookup by output api when output strictly increases spinoff from \"fst and fieldcache\" java-dev thread http://lucene.markmail.org/thread/swoawlv3fq4dntvl fst is able to associate arbitrary outputs with the sorted input keys, but in the special (and, common) case where the function is strictly monotonic (each output only \"increases\" vs prior outputs), such as mapping to term ords or mapping to file offsets in the terms dict, we should offer a lookup-by-output api that efficiently walks the fst and locates input key (exact or floor or ceil) matching that output.",
        "label": 33
    },
    {
        "text": "queries with too many asterisks causing  cpu usage if a search query has many adjacent asterisks (e.g. fo**************obar), i can get my webapp caught in a loop that does not seem to end in a reasonable amount of time and may in fact be infinite. for just a few asterisks the query eventually does return some results, but as i add more it takes a longer and longer amount of time. after about six or seven asterisks the query never seems to finish. even if i abort the search, the thread handling the troublesome query continues running in the background and pinning a cpu. i found the problem in src/java/org/apache/lucene/search/wildcardtermenum.java on lucene 3.0.1 and it looks like 3.0.2 ought to be affected as well. i'm not sure about trunk, though. i have a patch that fixes the problem for me in 3.0.1.",
        "label": 40
    },
    {
        "text": "upgrade antlr to version i would like to upgrade antlr from 3.5 to 4.5. this version adds several features that will improve the existing grammars. the main improvement would be the allowance of left-hand recursion in grammar rules which will reduce the number of rules significantly for expressions. this change will require some code refactoring to the existing expressions work.",
        "label": 53
    },
    {
        "text": "wordnet parsing bug a user reported that wordnet parses the prolog file incorrectly. also need to check the wordnet parser in the memory contrib for this problem. if this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had. for example, looking up the synsets for the word \"king\", we get: java synlookup wnindex king baron magnate mogul power queen rex scrofula struma tycoon here, \"scrofula\" and \"struma\" are extraneous. this happens because, the line parser code in syns2index.java interpretes the two consecutive single quotes in entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as termination of the string and separates into \"king\". this entry concerns synset of words \"scrofula\" and \"struma\", and thus they get inserted in the synset of \"king\". *there 1382 such entries, in wn_s.pl* and more in other wordnet prolog data-base files, where such use of two consecutive single quotes appears. we have resolved this by adding a statement in the line parsing portion of syns2index.java, as follows:            // parse line            line = line.substring(2);           * line = line.replaceall(\"\\'\\'\", \"`\"); // added statement*            int comma = line.indexof(',');            string num = line.substring(0, comma);  ... ... etc. in short we replace \"''\" by \"`\" (a back-quote). then on recreating the index, we get: java synlookup zwnindex king baron magnate mogul power queen rex tycoon",
        "label": 15
    },
    {
        "text": "test failure  geo3drpttest geo3drpttest.testoperations fails with seed 39bcae475bcfb043 note: reproduce with: ant test  -dtestcase=geo3drpttest -dtests.method=testoperations -dtests.seed=39bcae475bcfb043 -dtests.locale=it-it -dtests.timezone=america/boise -dtests.asserts=true -dtests.file.encoding=utf-8 java.lang.assertionerror: [intersects] qidx:3 shouldn't match i#5:geo3d:geoexactcircle: {planetmodel=planetmodel.wgs84, center=[lat=-1.0394053553992673, lon=-1.9037325881389144([x=-0.16538181742539926, y=-0.4782462267840722, z=-0.8609141805702146])], radius=1.1546166170607672(66.15465911325472), accuracy=4.231100485201301e-4} q:geo3d:geoexactcircle: {planetmodel=planetmodel.wgs84, center=[lat=-1.3165961602008989, lon=-1.887137823746273([x=-0.07807211790901268, y=-0.23850901911945152, z=-0.9659034153262631])], radius=1.432516663588956(82.07715890580914), accuracy=3.172052880854355e-11}  at __randomizedtesting.seedinfo.seed([39bcae475bcfb043:40c6f2143e9fe395]:0)  at org.junit.assert.fail(assert.java:93)  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.fail(randomspatialopstrategytestcase.java:126)  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperation(randomspatialopstrategytestcase.java:115)  at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperationrandomshapes(randomspatialopstrategytestcase.java:55)  at org.apache.lucene.spatial.spatial4j.geo3drpttest.testoperations(geo3drpttest.java:117)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:498)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1737)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:934)  at com.carrotsearch.randomizedtesting.randomizedrunner$9.evaluate(randomizedrunner.java:970)  at com.carrotsearch.randomizedtesting.randomizedrunner$10.evaluate(randomizedrunner.java:984) cc karl wright ignacio vera",
        "label": 25
    },
    {
        "text": "more options for stored fields compression since we added codec-level compression in lucene 4.1 i think i got about the same amount of users complaining that compression was too aggressive and that compression was too light. i think it is due to the fact that we have users that are doing very different things with lucene. for example if you have a small index that fits in the filesystem cache (or is close to), then you might never pay for actual disk seeks and in such a case the fact that the current stored fields format needs to over-decompress data can sensibly slow search down on cheap queries. on the other hand, it is more and more common to use lucene for things like log analytics, and in that case you have huge amounts of data for which you don't care much about stored fields performance. however it is very frustrating to notice that the data that you store takes several times less space when you gzip it compared to your index although lucene claims to compress stored fields. for that reason, i think it would be nice to have some kind of options that would allow to trade speed for compression in the default codec.",
        "label": 40
    },
    {
        "text": "boostingtermquery's boostingspanscorer class should be protected instead of package access currently, boostingtermscorer, an inner class of boostingtermquery is not accessible from outside the search.payloads making it difficult to write an extension of boostingtermquery. the other inner classes are protected already, as they should be.",
        "label": 15
    },
    {
        "text": "drop deprecations from trunk subj. also, to each remaining deprecation add release version when it first appeared. patch incoming.",
        "label": 40
    },
    {
        "text": "incorrect shinglefilter behavior when outputunigrams   false shinglefilter isn't working as expected when outputunigrams == false. in particular, it is outputting unigrams at least some of the time when outputunigrams==false. i'll attach a patch to shinglefiltertest.java that adds some test cases that demonstrate the problem. i haven't checked this, but i hypothesize that the behavior for outputunigrams == false got changed when the class was upgraded to the new tokenstream api?",
        "label": 53
    },
    {
        "text": "exception consistency in o a l store just some minor improvements: always use eofexception when its eof always include the inputstream too so we know filename etc use filenotfoundexception consistently in cfs when a sub-file is not found",
        "label": 40
    },
    {
        "text": " patch  add stopfilter ignorecase option wanted to have the ability to ignore case in the stop filter. in some cases, i don't want to have to lower case before passing through the stop filter, b/c i may need case preserved for other analysis further down the stream, yet i don't need the stopwords and i don't want to have to apply stopword filters twice.",
        "label": 55
    },
    {
        "text": "do not expose full fledged scorers in leafcollector setscorer currently leafcollector.setscorer takes a scorer, which i don't like because several methods should never be called in the context of a collector (like nextdoc or advance). i think it's even more trappy for methods that might seem to work in some particular cases but will not work in the general case, like getchildren which will not work if you have a specialized bulkscorer or iterating over positions which will not work if you are in a multicollector and another leaf collector consumes positions too. so i think we should restrict what can be seen from a collector to avoid such traps.",
        "label": 2
    },
    {
        "text": "add a typetokenfilter it would be convenient to have a typetokenfilter that filters tokens by its type, either with an exclude or include list. this might be a stupid thing to provide for people who use lucene directly, but it would be very useful to later expose it to solr and other lucene-backed search solutions.",
        "label": 53
    },
    {
        "text": "jenkins trunk tests  nightly only  fail quite often with oom in automaton fst tests the nightly job lucene-trunk quite often fails with oom (in several methods, not always in the same test): example from last night (this time a huge automaton): [junit] java.lang.outofmemoryerror: java heap space [junit] dumping heap to /home/hudson/hudson-slave/workspace/lucene-trunk/heapdumps/java_pid38855.hprof ... [junit] heap dump file created [86965954 bytes in 1.186 secs] [junit] testsuite: org.apache.lucene.index.testtermsenum [junit] testcase: testintersectrandom(org.apache.lucene.index.testtermsenum): caused an error [junit] java heap space [junit] java.lang.outofmemoryerror: java heap space [junit]  at org.apache.lucene.util.automaton.runautomaton.<init>(runautomaton.java:128) [junit]  at org.apache.lucene.util.automaton.byterunautomaton.<init>(byterunautomaton.java:28) [junit]  at org.apache.lucene.util.automaton.compiledautomaton.<init>(compiledautomaton.java:134) [junit]  at org.apache.lucene.index.testtermsenum.testintersectrandom(testtermsenum.java:266) [junit]  at org.apache.lucene.util.lucenetestcase$2$1.evaluate(lucenetestcase.java:611) [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:148) [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:50) [junit]  [junit]  [junit] tests run: 6, failures: 0, errors: 1, time elapsed: 11.699 sec other traces: [junit] testsuite: org.apache.lucene.util.fst.testfsts [junit] testcase: testrealterms(org.apache.lucene.util.fst.testfsts): caused an error [junit] java heap space [junit] java.lang.outofmemoryerror: java heap space [junit]  at org.apache.lucene.util.arrayutil.grow(arrayutil.java:338) [junit]  at org.apache.lucene.util.fst.fst$byteswriter.writebytes(fst.java:927) [junit]  at org.apache.lucene.util.fst.bytesequenceoutputs.write(bytesequenceoutputs.java:113) [junit]  at org.apache.lucene.util.fst.bytesequenceoutputs.write(bytesequenceoutputs.java:32) [junit]  at org.apache.lucene.util.fst.fst.addnode(fst.java:451) [junit]  at org.apache.lucene.util.fst.nodehash.add(nodehash.java:122) [junit]  at org.apache.lucene.util.fst.builder.compilenode(builder.java:180) [junit]  at org.apache.lucene.util.fst.builder.finish(builder.java:495) [junit]  at org.apache.lucene.index.codecs.memory.memorycodec$termswriter.finish(memorycodec.java:232) [junit]  at org.apache.lucene.index.freqproxtermswriterperfield.flush(freqproxtermswriterperfield.java:414) [junit]  at org.apache.lucene.index.freqproxtermswriter.flush(freqproxtermswriter.java:92) [junit]  at org.apache.lucene.index.termshash.flush(termshash.java:117) [junit]  at org.apache.lucene.index.docinverter.flush(docinverter.java:80) [junit]  at org.apache.lucene.index.docfieldprocessor.flush(docfieldprocessor.java:78) [junit]  at org.apache.lucene.index.documentswriterperthread.flush(documentswriterperthread.java:472) [junit]  at org.apache.lucene.index.documentswriter.doflush(documentswriter.java:420) [junit]  at org.apache.lucene.index.documentswriter.flushallthreads(documentswriter.java:568) [junit]  at org.apache.lucene.index.indexwriter.getreader(indexwriter.java:366) [junit]  at org.apache.lucene.index.indexreader.open(indexreader.java:317) [junit]  at org.apache.lucene.util.fst.testfsts.testrealterms(testfsts.java:1034) [junit]  at org.apache.lucene.util.lucenetestcase$2$1.evaluate(lucenetestcase.java:611) or: [junit] testsuite: org.apache.lucene.util.automaton.testcompiledautomaton [junit] testcase: testrandom(org.apache.lucene.util.automaton.testcompiledautomaton): caused an error [junit] java heap space [junit] java.lang.outofmemoryerror: java heap space [junit]  at org.apache.lucene.util.automaton.runautomaton.<init>(runautomaton.java:128) [junit]  at org.apache.lucene.util.automaton.byterunautomaton.<init>(byterunautomaton.java:28) [junit]  at org.apache.lucene.util.automaton.compiledautomaton.<init>(compiledautomaton.java:134) [junit]  at org.apache.lucene.util.automaton.testcompiledautomaton.build(testcompiledautomaton.java:39) [junit]  at org.apache.lucene.util.automaton.testcompiledautomaton.testterms(testcompiledautomaton.java:55) [junit]  at org.apache.lucene.util.automaton.testcompiledautomaton.testrandom(testcompiledautomaton.java:101) [junit]  at org.apache.lucene.util.lucenetestcase$2$1.evaluate(lucenetestcase.java:611) [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:148) [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:50) almost every nightly test fails, history: https://builds.apache.org/job/lucene-trunk we should maybe raise the max heap space or reduce doc counts/...",
        "label": 53
    },
    {
        "text": "patch creation tool  rename and improve diffsources py  the script diffsources.py is used for creating patches for feature branches. i think the name createpatch.py would be more apt. it also only works with certain file types and is one of the only python scripts written for python 2. i'd like to rename this script, upgrade it to python 3, and fix it to work with all files that git/svn would not ignore.",
        "label": 41
    },
    {
        "text": "utf8taxonomywritercache inconsistency i\u2019m facing a problem with taxonomy writer cache inconsistency. at some point in time utf8taxonomywritercache starts to return wrong ord for some facet labels. as result wrong ord are written in doc facet fields, and wrong counts are returned (undercount) during search. this bug is manifested on different servers with different index contents (we have several separate indexes with unique data). unfortunately i can\u2019t reproduce this behaviour in tests.  i've dumped \"broken\" utf8taxonomywritercache instance and created app to load it and to compare with real taxonomy. dumps and app are in attachment. to run demo extract archives content and exec: mvn compile mvn exec:java -dexec.mainclass=\"me.torobaev.lucene.taxonomy.cache.taxonomycachecheck\" -dtaxonomydir=../taxonomy/ -dcachedump=../taxonomy-cache.json as you can see, labels [frametype, 7] and [modification_id, 682] have same ord in cache.",
        "label": 11
    },
    {
        "text": "some contrib packages are missing a package html dunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - i don't think this is hugely important, but i think it is important - you should be able to easily and quickly read a quick overview for each contrib i think. so far i have identified collation and spatial.",
        "label": 40
    },
    {
        "text": "missing character in defaultsimilarity's javadoc the part which describes precision loss of norm values is missing a character; the encoded input value 0.89 in the example will actually be decoded to 0.875, not 0.75.",
        "label": 33
    },
    {
        "text": "readerandupdates should create a proper iocontext when writing dv updates today we pass iocontext.default. if dv updates are used in conjunction w/ nrtcachingdirectory, it means the latter will attempt to write the entire dv field in its ramdirectory, which could lead to oom. would be good if we can build our own flushinfo, estimating the number of bytes we're about to write. i didn't see off hand a quick way to guesstimate that - i thought to use the current dv's sizeinbytes as an approximation, but i don't see a way to get it, not a direct way at least. maybe we can use the size of the in-memory updates to guesstimate that amount? something like sizeofinmemupdates * (maxdoc/numupdateddocs)? is it a too wild guess?",
        "label": 43
    },
    {
        "text": "phrasequery incorrectly advertises it supports terms at the same position the following in phrasequery has been here since sept 15th 2004 (by \"goller\"):     /**      * adds a term to the end of the query phrase.      * the relative position of the term within the phrase is specified explicitly.      * this allows e.g. phrases with more than one term at the same position      * or phrases with gaps (e.g. in connection with stopwords).      *       */     public builder add(term term, int position) { of course this isn't true; it's why we have multiphrasequery. yet we even allow you to have consecutive terms with the same positions. we shouldn't allow that; we should throw an exception. for my own sanity, i modified a simple multiphrasequery test to use phrasequery instead and of course it didn't work.",
        "label": 10
    },
    {
        "text": "useless getattribute  in defaultindexingchain causes performance drop some background: we have spotted a significant drop in indexing throughput on machines with a large number of cores (see https://github.com/elastic/elasticsearch/issues/26339 for details). we finally managed to isolate the behavior in a jmh \"microbenchmark\". here is the output of running that benchmark with 32 threads with jmh's perfasm profiler (also oracle development studio finds the same hotspot): ....[hottest region 1].............................................................................. c2, level 4, org.apache.lucene.index.defaultindexingchain$perfield::invert, version 1231 (792 bytes)                           0x00007f52d56b026a: xor    %r10d,%r10d        ;*invokevirtual isassignablefrom                                                                         ; - org.apache.lucene.util.attributesource::addattribute@28 (line 207)                                                                         ; - org.apache.lucene.document.field$binarytokenstream::&lt;init&gt;@8 (line 512)                                                                         ; - org.apache.lucene.document.field::tokenstream@82 (line 491)                                                                         ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@99 (line 729)                           0x00007f52d56b026d: mov    $0x8,%r11d         ;*invokeinterface iterator                                                                         ; - org.apache.lucene.util.attributesource::getcurrentstate@46 (line 254)                                                                         ; - org.apache.lucene.util.attributesource::clearattributes@1 (line 269)                                                                         ; - org.apache.lucene.document.field$binarytokenstream::incrementtoken@10 (line 532)                                                                         ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@153 (line 736)   0.00%    0.02%       \u2197  0x00007f52d56b0273: test   %r10,%r10                   \u256d    \u2502  0x00007f52d56b0276: je     0x00007f52d56b0292  ;*getfield fieldsdata                   \u2502    \u2502                                                ; - org.apache.lucene.document.field::binaryvalue@1 (line 441)                   \u2502    \u2502                                                ; - org.apache.lucene.document.field::tokenstream@65 (line 487)                   \u2502    \u2502                                                ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@99 (line 729)   0.00%    0.00%  \u2502    \u2502  0x00007f52d56b0278: mov    (%r11),%rsi        ;*getfield next                   \u2502    \u2502                                                ; - java.util.hashmap::getnode@137 (line 580)                   \u2502    \u2502                                                ; - java.util.linkedhashmap::get@6 (line 440)                   \u2502    \u2502                                                ; - org.apache.lucene.util.attributesource::getattribute@6 (line 245)                   \u2502    \u2502                                                ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@143 (line 734)   0.09%    0.51%  \u2502    \u2502  0x00007f52d56b027b: mov    0x18(%rsi),%r8  23.70%    3.54%  \u2502    \u2502  0x00007f52d56b027f: mov    $0x7f4926a81d88,%rcx  ;   {metadata(&apos;org/apache/lucene/analysis/tokenattributes/chartermattribute&apos;)}   0.00%    0.00%  \u2502    \u2502  0x00007f52d56b0289: cmp    %rcx,%r8                   \u2502    \u2502  0x00007f52d56b028c: jne    0x00007f52d56b0949  ;*instanceof                   \u2502    \u2502                                                ; - org.apache.lucene.document.field::binaryvalue@4 (line 441)                   \u2502    \u2502                                                ; - org.apache.lucene.document.field::tokenstream@65 (line 487)                   \u2502    \u2502                                                ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@99 (line 729)   0.01%    0.00%  \u2198    \u2502  0x00007f52d56b0292: test   %r10,%r10                        \u2502  0x00007f52d56b0295: je     0x00007f52d56b1e0d  ;*invokevirtual addattributeimpl                        \u2502                                                ; - org.apache.lucene.util.attributesource::addattribute@80 (line 213)                        \u2502                                                ; - org.apache.lucene.document.field$stringtokenstream::&lt;init&gt;@8 (line 550)                        \u2502                                                ; - org.apache.lucene.document.field::tokenstream@47 (line 483)                        \u2502                                                ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@99 (line 729)   0.01%    0.00%       \u2502  0x00007f52d56b029b: mov    (%r11),%rsi        ;*invokevirtual hashcode                        \u2502                                                ; - java.util.hashmap::hash@9 (line 338)                        \u2502                                                ; - java.util.linkedhashmap::get@2 (line 440)                        \u2502                                                ; - org.apache.lucene.util.attributesource::getattribute@6 (line 245)                        \u2502                                                ; - org.apache.lucene.index.defaultindexingchain$perfield::invert@143 (line 734) you can see that a significant amount of time is spent in mov $0x7f4926a81d88,%rcx. it turns out that this maps to the following line in java (defaultindexingchain$perfield::invert): chartermattribute termatt = tokenstream.getattribute(chartermattribute.class); which is - luckily - unused (and removed by the attached patch). we have verified the impact of the change with an elasticsearch macrobenchmark which indexes 165,346,692 documents (~ 74gb) with 24 clients concurrently. the reported numbers are median indexing throughput during that benchmark against a single elasticsearch node: without the patch: 125418 documents/s with the patch: 221237 documents/s details about the benchmark setup etc. are mentioned in the elasticsearch issue #26339 if you're interested. unfortunately, it is beyond me why writing this register to main memory takes such a long time and why c2 did not eliminate this line as dead code to begin with.",
        "label": 53
    },
    {
        "text": "add fuzziness support to simplequeryparser it would be nice to add fuzzy query support to the simplequeryparser so that: foo~2 generates a fuzzyquery with an max edit distance of 2 and: \"foo bar\"~2 generates a phrasequery with a slop of 2.",
        "label": 40
    },
    {
        "text": "openreaderpassed not populated in checkindex status segmentinfostatus when using checkindex programatically, the openreaderpassed flag on the segmentinfostatus is never populated (so it always comes back false) looking at the code, its clear that openreaderpassed is defined, but never used furthermore, it appears that not all information that is propagated to the \"infostream\" is available via segmentiinfostatus all of the following information should be able to be gather from public properties on the segmentinfostatus: test: open reader.........ok test: fields, norms.......ok [2 fields] test: terms, freq, prox...ok [101 terms; 133 terms/docs pairs; 133 tokens] test: stored fields.......ok [100 total field count; avg 1 fields per doc] test: term vectors........ok [0 total vector count; avg 0 term/freq vector fields per doc]",
        "label": 33
    },
    {
        "text": "contrib memory  patternanalyzertest is a very  very  very  bad unit test while working on something else i was started getting consistent illegalstateexceptions from patternanalyzertest \u2013 but only when running the test from the top level. digging into the test, i've found numerous things that are very scary... instead of using assertions to test that tokens streams match, it throws an illegalstateexceptions when they don't, and then logs a bunch of info about the token streams to system.out \u2013 having assertion messages that tell you exactly what doens't match would make a lot more sense. it builds up a list of files to analyze using patsh thta it evaluates relative to the current working directory \u2013 which means you get different files depending on wether you run the tests fro mthe contrib level, or from the top level build file the list of files it looks for include: \"../../.txt\", \"../../.html\", \"../../*.xml\" ... so not only do you get different results when you run the tests in the contrib vs at the top level, but different people runing the tests via the top level build file will get different results depending on what types of text, html, and xml files they happen to have two directories above where they checked out lucene. the test comments indicates that it's purpose is to show that patternanalyzer produces the same tokens as other analyzers - but points out this will fail for whitespaceanalyzer because of the 255 character token limit whitespacetokenizer imposes \u2013 the test then proceeds to compare paternanalyzer to whitespacetokenizer, garunteeing a test failure for anyone who happens to have a text file containing more then 255 characters of non-whitespace in a row somewhere in \"../../\" (in my case: my bookmarks.html file, and the hex encoded favicon.gif images)",
        "label": 40
    },
    {
        "text": "deprecate directory touchfile lucene doesn't use this method, and, findbugs reports that fsdirectory's impl shouldn't swallow the returned result from file.setlastmodified.",
        "label": 33
    },
    {
        "text": "add opennlp analysis capabilities as a module now that opennlp is an asf project and has a nice license, it would be nice to have a submodule (under analysis) that exposed capabilities for it. drew farris, tom morton and i have code that does: sentence detection as a tokenizer (could also be a tokenfilter, although it would have to change slightly to buffer tokens) namedentity recognition as a tokenfilter we are also planning a tokenizer/tokenfilter that can put parts of speech as either payloads (partofspeechattribute?) on a token or at the same position. i'd propose it go under: modules/analysis/opennlp",
        "label": 47
    },
    {
        "text": "instantiatedindexreader throws nullpointerexception in norms  when used with a multireader when using instantiatedindexreader under a multireader where the other reader contains documents, a nullpointerexception is thrown here; public void norms(string field, byte[] bytes, int offset) throws ioexception { byte[] norms = getindex().getnormsbyfieldnameanddocumentnumber().get(field); system.arraycopy(norms, 0, bytes, offset, norms.length); } the 'norms' variable is null. performing the copy only when norms is not null does work, though i'm sure it's not the right fix. java.lang.nullpointerexception at org.apache.lucene.store.instantiated.instantiatedindexreader.norms(instantiatedindexreader.java:297) at org.apache.lucene.index.multireader.norms(multireader.java:273) at org.apache.lucene.search.termquery$termweight.scorer(termquery.java:70) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:131) at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:112) at org.apache.lucene.search.searcher.search(searcher.java:136) at org.apache.lucene.search.searcher.search(searcher.java:146) at org.apache.lucene.store.instantiated.testwithmultireader.test(testwithmultireader.java:41) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at junit.framework.testcase.runtest(testcase.java:164) at junit.framework.testcase.runbare(testcase.java:130) at junit.framework.testresult$1.protect(testresult.java:106) at junit.framework.testresult.runprotected(testresult.java:124) at junit.framework.testresult.run(testresult.java:109) at junit.framework.testcase.run(testcase.java:120) at junit.framework.testsuite.runtest(testsuite.java:230) at junit.framework.testsuite.run(testsuite.java:225) at org.eclipse.jdt.internal.junit.runner.junit3.junit3testreference.run(junit3testreference.java:130) at org.eclipse.jdt.internal.junit.runner.testexecution.run(testexecution.java:38) at org.eclipse.jdt.internal.junit.runner.remotetestrunner.runtests(remotetestrunner.java:460) at org.eclipse.jdt.internal.junit.runner.remotetestrunner.runtests(remotetestrunner.java:673) at org.eclipse.jdt.internal.junit.runner.remotetestrunner.run(remotetestrunner.java:386) at org.eclipse.jdt.internal.junit.runner.remotetestrunner.main(remotetestrunner.java:196)",
        "label": 24
    },
    {
        "text": "fix indexwriter working together with emptytokenizer and emptytokenstream  without chartermattribute  fix basetokenstreamtestcase testrandomchains can fail because emptytokenizer doesn't have a chartermattribute and doesn't compute the end offset (if the offset attribute was added by a filter).",
        "label": 53
    },
    {
        "text": "indexwriter addindexes indexreader  readers  doesn't correctly handle exception success flag  after this bit of code in addindexes(indexreader[] readers) try { flush(true, false, true); optimize(); // start with zero or 1 seg success = true; } finally { // take care to release the write lock if we hit an // exception before starting the transaction if (!success) releasewrite(); } the success flag should be reset to \"false\" because it's used again in another try/catch/finally block. testindexwriter.testaddindexondiskfull() sometimes will hit this bug; but it's infrequent.",
        "label": 33
    },
    {
        "text": "make lucene completely write once today, lucene is mostly write-once, but not always, and these are just very exceptional cases. this is an invitation for exceptional bugs: (and we have occasional test failures when doing \"no-wait close\" because of this). i would prefer it if we didn't try to delete files before we open them for write, and if we opened them with the create_new option by default to throw an exception, if the file already exists. the trickier parts of the change are going to be indexfiledeleter and exceptions on merge / cfs construction logic. overall for indexfiledeleter i think the least invasive option might be to only delete files older than the current commit point? this will ensure that inflategens() always avoids trying to overwrite any files that were from an aborted segment. for cfs construction/exceptions on merge, we really need to remove the custom \"sniping\" of index files there and let only indexfiledeleter delete files. my previous failed approach involved always consistently using trackingdirectorywrapper, but it failed, and only in backwards compatibility tests, because of lucene-6146 (but i could never figure that out). i am hoping this time i will be successful longer term we should think about more simplifications, progress has been made on lucene-5987, but i think overall we still try to be a superhero for exceptions on merge?",
        "label": 33
    },
    {
        "text": "nrtmanager shouldn't expose its private searchermanager spinoff from lucene-3769. to actually obtain an indexsearcher from nrtmanager, it's a 2-step process now. you must .getsearchermanager(), then .acquire() from the returned searchermanager. this is very trappy... because if the app incorrectly calls maybereopen on that private searchermanager (instead of nrtmanager.maybereopen) then it can unexpectedly cause threads to block forever, waiting for the necessary gen to become visible. this will be hard to debug... i don't like creating trappy apis. hopefully once lucene-3761 is in, we can fix nrtmanager to no longer expose its private sm, instead subclassing referencemanaager. or alternatively, or in addition, maybe we factor out a new interface (searcherprovider or something...) that only has acquire and release methods, and both nrtmanager and referencemanager/sm impl that, and we keep nrtmanager's sm private.",
        "label": 33
    },
    {
        "text": "testearlyterminatingsortingcollector testterminatedearly failure the seed below (from http://jenkins.sarowe.net/job/lucene-solr-tests-5.3-java8/16/ ) reproduces 100% (on os x) only on lucene_solr_5_3 for me, with both java7 and java8. passes on both branch_5x and trunk:   [junit4] suite: org.apache.lucene.search.testearlyterminatingsortingcollector   [junit4]   2> note: reproduce with: ant test  -dtestcase=testearlyterminatingsortingcollector -dtests.method=testterminatedearly -dtests.seed=e7dd88a4ffa03df6 -dtests.slow=true -dtests.locale=es_ve -dtests.timezone=bet -dtests.asserts=true -dtests.file.encoding=utf-8   [junit4] failure 0.75s j8 | testearlyterminatingsortingcollector.testterminatedearly <<<   [junit4]    > throwable #1: java.lang.assertionerror: should have terminated early (searcher.reader=exitabledirectoryreader(uninvertingdirectoryreader(uninverting(_11(5.3.0):c1))))   [junit4]    >  at __randomizedtesting.seedinfo.seed([e7dd88a4ffa03df6:c64632165e9a7371]:0)   [junit4]    >  at org.apache.lucene.search.testearlyterminatingsortingcollector.testterminatedearly(testearlyterminatingsortingcollector.java:298)   [junit4]    >  at java.lang.thread.run(thread.java:745)   [junit4]   2> note: test params are: codec=asserting(lucene53): {s=blocktreeords(blocksize=128)}, docvalues:{ndv2=docvaluesformat(name=direct), ndv1=docvaluesformat(name=asserting)}, sim=defaultsimilarity, locale=es_ve, timezone=bet   [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_45 (64-bit)/cpus=16,threads=1,free=432339984,total=514850816   [junit4]   2> note: all tests run in this jvm: [testearlyterminatingsortingcollector]   [junit4] completed [22/25] on j8 in 9.77s, 4 tests, 1 failure <<< failures!",
        "label": 9
    },
    {
        "text": "refactor searchable to not have rmi remote dependency per http://lucene.markmail.org/message/fu34tuomnqejchfj?q=remotesearchable we should refactor searchable slightly so that it doesn't extend the java.rmi.remote marker interface. i believe the same could be achieved by just marking the remotesearchable and refactoring the rmi implementation out of core and into a contrib. if we do this, we should deprecate/denote it for 2.9 and then move it for 3.0",
        "label": 33
    },
    {
        "text": "instantiatedindexreader norms called from multireader bug small bug in instantiatedindexreader.norms(string field, byte[] bytes, int offset) where the offset is not applied properly in the system.arraycopy",
        "label": 24
    },
    {
        "text": "could not limit lucene's memory consumption we are running jira 7.6.1 with lucene 3.3 on sles 12 sp1 we configured 16gb jira heap on 64gb server however, each time, when we run background re-index, the memory will be used out by lucene and we could not only limit its memory consumption. this definitely will cause overall performance issue on a system with heavy load. we have around 500 concurrent users, 400k issues. could you please help to advice if there were workaround  or fix for this? thanks.   btw: i did check a lot and found a blog introducing the new behavior of lucene 3.3 http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html  ",
        "label": 53
    },
    {
        "text": "improvements for testing packed ints i noticed while working on lucene-5266 there is no guarantee every bpv is tested, since the acceptable overhead is always random. it would be good to first ensure all bpvs work, and then have random acceptable overhead. there was also a simple issue with the order of expected/actual for the direct packed reader loop.",
        "label": 41
    },
    {
        "text": "disable transitive dependencies in maven config our ivy configuration does this: each dependency is specified and so we know what will happen. unfortunately the maven setup is not configured the same way. instead the maven setup is configured to download the internet: and it excludes certain things specifically. this is really hard to configure and maintain: we added a 'validate-maven-dependencies' that tries to fail on any extra jars, but all it really does is run a license check after maven \"runs\". it wouldnt find unnecessary dependencies being dragged in if something else in lucene was using them and thus they had a license file. since maven supports wildcard exclusions: mng-3832, we can disable this transitive shit completely. we should do this, so its configuration is the exact parallel of ivy.",
        "label": 47
    },
    {
        "text": "better rat reporting the \"ant rat-sources\" target currently only analyzes src/java ... we can do better then this.",
        "label": 40
    },
    {
        "text": "integrate indexreader with indexwriter the current problem is an indexreader and indexwriter cannot be open at the same time and perform updates as they both require a write lock to the index. while methods such as iw.deletedocuments enables deleting from iw, methods such as ir.deletedocument(int doc) and norms updating are not available from iw. this limits the capabilities of performing updates to the index dynamically or in realtime without closing the iw and opening an ir, deleting or updating norms, flushing, then opening the iw again, a process which can be detrimental to realtime updates. this patch will expose an indexwriter.getreader method that returns the currently flushed state of the index as a class that implements indexreader. the new ir implementation will differ from existing ir implementations such as multisegmentreader in that flushing will synchronize updates with iw in part by sharing the write lock. all methods of ir will be usable including reopen and clone.",
        "label": 33
    },
    {
        "text": "broke backwards compatibility for beta   release indexes as reported by ian on the user list: its trying to treat them as 3.x",
        "label": 53
    },
    {
        "text": "the method equals in constantscorerangequery does not compare collator property fully  this bug is similar to lucene if (this.fieldname != other.fieldname // interned comparison this.includelower != other.includelower this.includeupper != other.includeupper (this.collator != null && ! this.collator.equals(other.collator)) ) { return false; } if this.collator == null and other.collator is not null. the equals method should return false; but in lucene 2.4. the method return true. constantscorerangequery equals method does not compare collator property fully , this bug is similar to lucene-1587 http://issues.apache.org/jira/browse/lucene-1587",
        "label": 53
    },
    {
        "text": "contrib benchmark files need eol style set the following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted. ./build.xml: ./changes.txt: ./conf/sample.alg: ./conf/standard.alg: ./conf/sloppy-phrase.alg: ./conf/deletes.alg: ./conf/micro-standard.alg: ./conf/compound-penalty.alg:",
        "label": 12
    },
    {
        "text": "charsref subsequence broken looks like charsref.subsequence() is currently broken it is implemented as:   @override   public charsequence subsequence(int start, int end) {     // note: must do a real check here to meet the specs of charsequence     if (start < 0 || end > length || start > end) {       throw new indexoutofboundsexception();     }     return new charsref(chars, offset + start, offset + end);   } since charsref constructor is (char[] chars, int offset, int length), should be:   @override   public charsequence subsequence(int start, int end) {     // note: must do a real check here to meet the specs of charsequence     if (start < 0 || end > length || start > end) {       throw new indexoutofboundsexception();     }     return new charsref(chars, offset + start, end - start);   }",
        "label": 40
    },
    {
        "text": "cartesianpolyfilterbuilder doesn't properly account for which tiers actually exist in the index in the cartesianshapefilterbuilder, there is logic that determines the \"best fit\" tier to create the filter against. however, it does not account for which fields actually exist in the index when doing so. for instance, if you index tiers 1 through 10, but then choose a very small radius to restrict the space to, it will likely choose a tier like 15 or 16, which of course does not exist.",
        "label": 15
    },
    {
        "text": "cleanup storedfieldsprocessor   termvectorsconsumer while i was looking into the latest failure here i cleaned up storedfieldsprocessor & termvectorsconsumer a bit since they still seem to have some leftovers from ancient times where we reused the indexing chains across flushes",
        "label": 46
    },
    {
        "text": "allow customization of column stride field and norms via indexing chain we are building an in-memory indexing format and managing our own segments. we are doing this by implementing a custom indexingchain. we would like to support column-stride-fields and norms without having to wire in a codec (since we are managing our postings differently) suggested change is consistent with the api support for passing in a custom inverteddocconsumer.",
        "label": 46
    },
    {
        "text": "new wrapper classes for geo3d hi, after the latest developments in the geo3d library, in particular: https://issues.apache.org/jira/browse/lucene-7906 : spatial relationships between geoshapes https://issues.apache.org/jira/browse/lucene-7936: serialization of geoshapes. i propose a new set of wrapper classes which can be for example linked to solr as they implement their own spatialcontextfactory. it provides the capability of indexing shapes with spherical geometry. thanks!",
        "label": 10
    },
    {
        "text": "docvaluesproducer rambytesused throws concurrentmodificationexception this came up in an elasticsearch issue that if you pull #rambytesused() while docvalues are loaded in a seperate thread you see a concurrentmodificationexception here is an example: caused by: java.util.concurrentmodificationexception         at java.util.hashmap$hashiterator.nextentry(hashmap.java:926)         at java.util.hashmap$valueiterator.next(hashmap.java:954)         at org.apache.lucene.codecs.lucene45.lucene45docvaluesproducer.rambytesused(lucene45docvaluesproducer.java:291)         at org.apache.lucene.codecs.perfield.perfielddocvaluesformat$fieldsreader.rambytesused(perfielddocvaluesformat.java:308)         at org.apache.lucene.index.segmentdocvalues.rambytesused(segmentdocvalues.java:103)         at org.apache.lucene.index.segmentreader.rambytesused(segmentreader.java:555)",
        "label": 43
    },
    {
        "text": "geoexactcircle improvement hi karl wright, current implementation of geoexactcircle seems to work well for planet models with low flattening (~|0.025|). when flattening increase shapes start becoming invalid because of the cutting angle of the circle plane which results on the center of the circle ending up on the wrong side of the plane. i propose a new version of geoexactcircle that tries to overcome this problem by creating a new plane for a circle sector in such cases. the new plane is built built for each sector when needed by using two points from the circle edge and the center of the world. the plane is such that it is built as close as possible to the circle plane of the sector. points from the circle plane must not be within the new plane and the center of the circle must be within the plane. this approach seems to work well up to planets with flattening up to around ~|0.1|. i think after that the cutting angles of circle planes can be so thin that the apporach is not valid. therefore i propose to add this new approach and limit the creation of such circles to planet models with flattening lower than |0.1|. probably a limitation that does not affect most of the realistic cases. in addition this new version forces a minimum of 4 sectors in a circle. the issue on lucene-8071 came up again for circles of any radius so we should enforce it for all circles. thanks!",
        "label": 25
    },
    {
        "text": "add n gram string matching for spell checking n-gram version of edit distance based on paper by grzegorz kondrak, \"n-gram similarity and distance\". proceedings of the twelfth international conference on string processing and information retrieval (spire 2005), pp. 115-126, buenos aires, argentina, november 2005. http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf",
        "label": 15
    },
    {
        "text": "left nav of docs index html in dist artifacts links to hudson for javadocs when building the zip or tgz release artifacts, the docs/index.html file contained in that release (the starter point for people to read documentation) links \"api docs\" to http://lucene.zones.apache.org:8080/hudson/job/lucene-nightly/javadoc/ instead of to ./api/index.html (the local copy of the javadocs) this relates to the initial migration to hudson for the nightly builds and a plan to copy the javadocs back to lucene.apache.org that wasn't considered urgent since it was just for transient nightly docs, but a side affect is that the release documentation also links to hudson. even if we don't modify the nightly build process before the 2.2 release, we should update the link in the left nav in the 2.2 release branch before building the final release.",
        "label": 15
    },
    {
        "text": "improve load time of  tii files we have a large 50 gig index which is optimized as one segment, with a 66 meg .tii file. this index has no norms, and no field cache. it takes about 5 seconds to load this index, profiling reveals that 60% of the time is spent in growablewriter.set(index, value), and most of time in set(...) is spent resizing packedints.mutatable current. in the constructor for terminfosreaderindex, you initialize the writer with the line, growablewriter indextoterms = new growablewriter(4, indexsize, false); for our index using four as the bit estimate results in 27 resizes. the last value in indextoterms is going to be ~ tiifilelength, and if instead you use, int bitestimate = (int) math.ceil(math.log10(tiifilelength) / math.log10(2)); growablewriter indextoterms = new growablewriter(bitestimate, indexsize, false); load time improves to ~ 2 seconds.",
        "label": 33
    },
    {
        "text": "wrong implementation of docidsetiterator advance implementations of docidsetiterator behave differently when advanced is called. taking the following test for openbitset, docidbitset and sortedvintlist only sortedvintlist passes the test: org.apache.lucene.search.testdocidset.java ...  public void testadvancewithopenbitset() throws ioexception {   docidset idset = new openbitset( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10   assertadvance( idset );  }  public void testadvancedocidbitset() throws ioexception {   bitset bitset = new bitset();   bitset.set( 0 );   bitset.set( 5 );   bitset.set( 6 );   bitset.set( 10 );   docidset idset = new docidbitset(bitset);   assertadvance( idset );  }  public void testadvancewithsortedvintlist() throws ioexception {   docidset idset = new sortedvintlist( 0, 5, 6, 10 );   assertadvance( idset );  }   private void assertadvance(docidset idset) throws ioexception {   docidsetiterator iter = idset.iterator();   int docid = iter.nextdoc();   assertequals( \"first doc id should be 0\", 0, docid );   docid = iter.nextdoc();   assertequals( \"second doc id should be 5\", 5, docid );   docid = iter.advance( 5 );   assertequals( \"advancing iterator should return the next doc id\", 6, docid );  } the javadoc for advance says: advances to the first beyond the current whose document number is greater than or equal to target. this seems to indicate that sortedvintlist behaves correctly, whereas the other two don't. just looking at the docidbitset implementation advance is implemented as: bitset.nextsetbit(target); where the docs of nextsetbit say: returns the index of the first bit that is set to true that occurs on or after the specified starting index",
        "label": 43
    },
    {
        "text": " patch  decouple locking implementation from directory implementation this is a spinoff of http://issues.apache.org/jira/browse/lucene-305. i've opened this new issue to capture that it's wider scope than lucene-305. this is a patch originally created by jeff patterson (see above link) and then modified as described here: http://issues.apache.org/jira/browse/lucene-305#action_12418493 with some small additional changes: for each fsdirectory.getdirectory(), i made a corresponding version that also accepts a lockfactory instance. so, you can construct an fsdirectory with your own lockfactory. cascaded defaulting for fsdirectory's lockfactory implementation: if you pass in a lockfactory instance, it's used; else if setdisablelocks was called, we use nolockfactory; else, if the system property \"org.apache.lucene.store.fsdirectorylockfactoryclass\" is defined, we use that; finally, we'll use the original locking implementation (simplefslockfactory). the gist is that all locking code has been moved out of *directory and into subclasses of a new abstract lockfactory class. you can now set the lockfactory of a directory to change how it does locking. for example, you can create an fsdirectory but set its locking to singleinstancelockfactory (if you know all writing/reading will take place a single jvm). the changes pass all unit tests (on ubuntu linux sun java 1.5 and windows xp sun java 1.4), and i added another testcase to test the lockfactory code. note that lockfactory defaults are not changed: fsdirectory defaults to simplefslockfactory and ramdirectory defaults to singleinstancelockfactory. next step (separate issue) is to create a lockfactory that uses the os native locks (through java.nio).",
        "label": 55
    },
    {
        "text": "geo3drpttest failure  geoexactcircle and geocomplexpolygon to reproduce: ant test -dtestcase=geo3drpttest -dtests.method=testoperations -dtests.seed=35f19948c8d0b296 -dtests.multiplier=3 -dtests.slow=true   [junit4] failure 1.60s | geo3drpttest.testoperations {seed=[35f19948c8d0b296:4c8bc51bad80e140]} <<<    [junit4]    > throwable #1: java.lang.assertionerror: [intersects] qidx:46 shouldn't match i#1:geo3d:geoexactcircle: {planetmodel=planetmodel.wgs84, center=[lat=-0.9559804772783842, lon=2.3746400238746745([x=-0.41485128662362136, y=0.3998224895518887, z=-0.8159609316679225])], radius=0.2927546208674785(16.773604208659055), accuracy=1.9835991319951308e-4} q:geo3d:geocomplexpolygon: {planetmodel=planetmodel.wgs84, number of shapes=1, address=48a56db9, testpoint=[lat=-0.6911309532123969, lon=-1.825801910033474([x=-0.19431732451263006, y=-0.7454226529152223, z=-0.6372503253316519])], testpointinset=true, shapes={ {[lat=-1.387818122744865, lon=-2.7792398669224005([x=-0.16978152317325065, y=-0.06436270935780461, z=-0.9812145381630729])], [lat=0.3097381417459196, lon=-1.2840888403614732([x=0.2695553433310933, y=-0.9142719953894461, z=0.30505479536297536])], [lat=-0.0480217407750678, lon=-1.8503085663094863([x=-0.2758749814522231, y=-0.9611487663811242, z=-0.04805662135798377])], [lat=-0.562588503679275, lon=0.1031827231215822([x=0.841513436598657, y=0.08713911491133425, z=-0.533463142870229])], [lat=-1.2344672208544873, lon=-0.4462630141292253([x=0.29714578386731183, y=-0.14217069789173978, z=-0.9422037262649413])], [lat=0.3278373341365327, lon=-1.4874282088599309([x=0.07889725918747778, y=-0.9441785582346046, z=0.3222439993016786])], [lat=1.084105292601162, lon=2.757011421605791([x=-0.4328875155449605, y=0.17520454460023893, z=0.8825538642625743])], [lat=0.9435002493553667, lon=2.474535178256974([x=-0.4606403525400187, y=0.3627431535977759, z=0.8087390201902855])], [lat=-0.6491383005683652, lon=-0.6236527284532923([x=0.6465724515973865, y=-0.46516869386856063, z=-0.6044327179281425])], [lat=-0.615469772740116, lon=2.908550337184802([x=-0.7944279172337232, y=0.1885612520247134, z=-0.5773400032472633])], [lat=-0.26863911348502323, lon=1.2430264151885233([x=0.31065925040467024, y=0.9136095339661758, z=-0.2656535169115309])], [lat=1.340203739358425, lon=0.8281168704009712([x=0.15424430350138682, y=0.16801937134287298, z=0.9715225323505815])], [lat=1.129241312019259, lon=2.627460773729766([x=-0.3714930761512795, y=0.20981779772740047, z=0.9026170614434411])]}}    [junit4]    >        at __randomizedtesting.seedinfo.seed([35f19948c8d0b296:4c8bc51bad80e140]:0)    [junit4]    >        at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.fail(randomspatialopstrategytestcase.java:126)    [junit4]    >        at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperation(randomspatialopstrategytestcase.java:115)    [junit4]    >        at org.apache.lucene.spatial.prefix.randomspatialopstrategytestcase.testoperationrandomshapes(randomspatialopstrategytestcase.java:55)    [junit4]    >        at org.apache.lucene.spatial.spatial4j.geo3drpttest.testoperations(geo3drpttest.java:117)    [junit4]    >        at java.lang.thread.run(thread.java:745) cc karl wright ignacio vera",
        "label": 25
    },
    {
        "text": "passing a null fieldname to memoryfields terms in memoryindex throws a npe i found this when querying a memoryindex using a regexpquery wrapped by a spanmultitermquerywrapper. if the regexp doesn't match anything in the index, it gets rewritten to an empty spanorquery with a null field value, which then triggers the npe.",
        "label": 33
    },
    {
        "text": "intermittent failure in testindexwriter testcommitthreadsafety mark's while(1) hudson box found this failure (and i can repro it too): error message mockramdirectory: cannot close: there are still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1} stacktrace java.lang.runtimeexception: mockramdirectory: cannot close: there are still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}        at org.apache.lucene.store.mockramdirectory.close(mockramdirectory.java:282)        at org.apache.lucene.index.testindexwriter.testcommitthreadsafety(testindexwriter.java:4616)        at org.apache.lucene.util.lucenetestcase.runbare(lucenetestcase.java:328) standard output note: random codec of testcase 'testcommitthreadsafety' was: sep standard error the following exceptions were thrown by threads: *** thread: thread-1784 *** java.lang.runtimeexception: junit.framework.assertionfailederror: null        at org.apache.lucene.index.testindexwriter$9.run(testindexwriter.java:4606) caused by: junit.framework.assertionfailederror: null        at junit.framework.assert.fail(assert.java:47)        at junit.framework.assert.asserttrue(assert.java:20)        at junit.framework.assert.asserttrue(assert.java:27)        at org.apache.lucene.index.testindexwriter$9.run(testindexwriter.java:4597)",
        "label": 33
    },
    {
        "text": "indexreader setnorms is no op if one of the field instances omits norms if i add two documents to an index w/ same field, and one of them omit norms, then indexreader.setnorms is no-op. i'll attach a patch w/ test case",
        "label": 43
    },
    {
        "text": "new flexible query parser from \"new flexible query parser\" thread by micheal busch in my team at ibm we have used a different query parser than lucene's in our products for quite a while. recently we spent a significant amount of time in refactoring the code and designing a very generic architecture, so that this query parser can be easily used for different products with varying query syntaxes. this work was originally driven by andreas neumann (who, however, left our team); most of the code was written by luis alves, who has been a bit active in lucene in the past, and adriano campos, who joined our team at ibm half a year ago. adriano is apache committer and pmc member on the tuscany project and getting familiar with lucene now too. we think this code is much more flexible and extensible than the current lucene query parser, and would therefore like to contribute it to lucene. i'd like to give a very brief architecture overview here, adriano and luis can then answer more detailed questions as they're much more familiar with the code than i am. the goal was it to separate syntax and semantics of a query. e.g. 'a and b', '+a +b', 'and(a,b)' could be different syntaxes for the same query. we distinguish the semantics of the different query components, e.g. whether and how to tokenize/lemmatize/normalize the different terms or which query objects to create for the terms. we wanted to be able to write a parser with a new syntax, while reusing the underlying semantics, as quickly as possible. in fact, adriano is currently working on a 100% lucene-syntax compatible implementation to make it easy for people who are using lucene's query parser to switch. the query parser has three layers and its core is what we call the querynodetree. it is a tree that initially represents the syntax of the original query, e.g. for 'a and b': and / \\ a b the three layers are: 1. queryparser 2. querynodeprocessor 3. querybuilder 1. the upper layer is the parsing layer which simply transforms the query text string into a querynodetree. currently our implementations of this layer use javacc. 2. the query node processors do most of the work. it is in fact a configurable chain of processors. each processors can walk the tree and modify nodes or even the tree's structure. that makes it possible to e.g. do query optimization before the query is executed or to tokenize terms. 3. the third layer is also a configurable chain of builders, which transform the querynodetree into lucene query objects. furthermore the query parser uses flexible configuration objects, which are based on attributesource/attribute. it also uses message classes that allow to attach resource bundles. this makes it possible to translate messages, which is an important feature of a query parser. this design allows us to develop different query syntaxes very quickly. adriano wrote the lucene-compatible syntax in a matter of hours, and the underlying processors and builders in a few days. we now have a 100% compatible lucene query parser, which means the syntax is identical and all query parser test cases pass on the new one too using a wrapper. recent posts show that there is demand for query syntax improvements, e.g improved range query syntax or operator precedence. there are already different qp implementations in lucene+contrib, however i think we did not keep them all up to date and in sync. this is not too surprising, because usually when fixes and changes are made to the main query parser, people don't make the corresponding changes in the contrib parsers. (i'm guilty here too) with this new architecture it will be much easier to maintain different query syntaxes, as the actual code for the first layer is not very much. all syntaxes would benefit from patches and improvements we make to the underlying layers, which will make supporting different syntaxes much more manageable.",
        "label": 32
    },
    {
        "text": "knearestneighbordocumentclassifier knnsearch applies previous boosted field's factor to subsequent unboosted fields when reading code noticed that in knearestneighborclassifier a neutral boost factor is restored but in knearestneighbordocumentclassifier this currently does not happen. this seems unintended.",
        "label": 9
    },
    {
        "text": "stop creating huge arrays to represent the absense of field norms creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. it would be much more efficient to use null to represent a missing norms table.",
        "label": 33
    },
    {
        "text": "checkjavadoclinks py doesn't allow links to new ref guide in javadocs in solr-11135 i'm fixing a number of urls in source that point to the old solr reference guide location (https://cwiki.apache.org/confluence/display/solr/...). the new base url for the ref guide is https://lucene.apache.org/solr/guide... which is the same as the javadocs. several of these references are in java classes, but changing those to the new urls causes precommit to fail because checkjavadoclinks.py doesn't allow links in javadocs to contain urls starting with lucene.apache.org unless they are explicitly allowed. fixing this may not be as simple as just allowing any url starting with https://lucene.apache.org/solr/guide.... for javadocs we want to only use non-versioned urls, but someone could accidentally insert a versioned url (say, for 7.0) that would be invalid in later releases. since javadocs & ref guide are on the same server, perhaps some sort of relative link is preferable, but i honestly don't know enough about url construction in javadocs to know what sorts of options are available.",
        "label": 47
    },
    {
        "text": "geopolygon test failure     [junit4]   2> note: reproduce with: ant test  -dtestcase=randomgeopolygontest -dtests.method=testcomparesmallpolygons -dtests.seed=7ba5f34669e15f97 -dtests.slow=true -dtests.badapples=true -dtests.locale=es-cu -dtests.timezone=america/la_paz -dtests.asserts=true -dtests.file.encoding=utf8    [junit4] failure 0.01s | randomgeopolygontest.testcomparesmallpolygons {seed=[7ba5f34669e15f97:f73170ec892310d]} <<<    [junit4]    > throwable #1: java.lang.assertionerror: test point1 not correctly in/out of set according to test point2    [junit4]    > at __randomizedtesting.seedinfo.seed([7ba5f34669e15f97:f73170ec892310d]:0)    [junit4]    > at org.apache.lucene.spatial3d.geom.geocomplexpolygon.<init>(geocomplexpolygon.java:239)    [junit4]    > at org.apache.lucene.spatial3d.geom.geopolygonfactory$bestshape.creategeocomplexpolygon(geopolygonfactory.java:465)    [junit4]    > at org.apache.lucene.spatial3d.geom.geopolygonfactory.makelargegeopolygon(geopolygonfactory.java:389)    [junit4]    > at org.apache.lucene.spatial3d.geom.geopolygonfactory.makegeopolygon(geopolygonfactory.java:226)    [junit4]    > at org.apache.lucene.spatial3d.geom.geopolygonfactory.makegeopolygon(geopolygonfactory.java:142)    [junit4]    > at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparepolygons(randomgeopolygontest.java:157)    [junit4]    > at org.apache.lucene.spatial3d.geom.randomgeopolygontest.testcomparesmallpolygons(randomgeopolygontest.java:109)    [junit4]    > at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)    [junit4]    > at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)    [junit4]    > at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)    [junit4]    > at java.base/java.lang.reflect.method.invoke(method.java:564)    [junit4]    > at java.base/java.lang.thread.run(thread.java:844)    [junit4] ok      0.34s | randomgeopolygontest.testcomparesmallpolygons {seed=[7ba5f34669e15f97:3ddff260bf592657]}    [junit4] ok      0.35s | randomgeopolygontest.testcomparesmallpolygons {seed=[7ba5f34669e15f97:ea54a4dadb0e1105]}    [junit4]   2> note: test params are: codec=highcompressioncompressingstoredfields(storedfieldsformat=compressingstoredfieldsformat(compressionmode=high_compression, chunksize=1818, maxdocsperchunk=791, blocksize=936), termvectorsformat=compressingtermvectorsformat(compressionmode=high_compression, chunksize=1818, blocksize=936)), sim=randomsimilarity(querynorm=false): {}, locale=es-cu, timezone=america/la_paz",
        "label": 25
    },
    {
        "text": "benchmarks enhancements  precision recall  trec  wikipedia  would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala trec. i don't know what the copyright issues are for the trec queries/data (i think the queries are available, but not sure about the data), so not sure if the is even feasible, but i could imagine we could at least incorporate support for it for those who have access to the data. it has been a long time since i have participated in trec, so perhaps someone more familiar w/ the latest can fill in the blanks here. another option is to ask for volunteers to create queries and make judgments for the reuters data, but that is a bit more complex and probably not necessary. even so, an apache licensed set of benchmarks may be useful for the community as a whole. hmmm.... wikipedia might be another option instead of reuters to setup as a download for benchmarking, as it is quite large and i believe the licensing terms are quite amenable. having a larger collection would be good for stressing lucene more and would give many users a demonstration of how lucene handles large collections. at any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.",
        "label": 12
    },
    {
        "text": "use java markdown provided by ivy to transform build txt  migrate txt  to simple  better readable  html and place under documentation  the migrate.txt and other files are very simple formatted and can be converted using markdown. we can use e.g. pegdown (a java markdown converter) to make html out of them and place those in the html documentation. theoretically, also changes.txt might be processed by markdown, we have to try out. pegdown is extensible, so it could handle lucene-xxx jira links correctly.",
        "label": 53
    },
    {
        "text": "decrease i o pressure when merging high dimensional points related with lucene-8619, after indexing 60 million shapes(~1.65 billion triangles) using latlonshape, the index directory grew to a size of 265 gb when performing merging of different segments. after the processes were over the index size was 57 gb. as an example imagine we are merging several segments to a new segment of size 10gb (4 dimensions). the bkd tree merging logic will create the following files: 1) level 0: 4 copies of the data, each one sorted by one dimensions : 40gb 2) level 1: 6 copies of half of the data, left and right : 30gb 3) level 2: 6 copies of one quarter of the data, left and right : 15 gb 4) level 3: 6 more copies halving the previous level, left and right : 7.5 gb 5) level 4: 6 more copies halving the previous level, left and right : 3.75 gb   and so on... so it requires around 100gb to merge that segment.  in this issue is proposed to delay the creation of sorted copies to when they are needed. it reduces the total size required to half of what it is needed now.       ",
        "label": 19
    },
    {
        "text": "npe when calling iscurrent  on a parallellreader as demonstrated by the test case below, if you call iscurrent() on a parallelreader it causes an npe. fix appears to be to add an iscurrent() to parallelreader which calls it on the underlying indexes but i'm not sure what other problems may be lurking here. do methods such as getversion(), lastmodified(), isoptimized() also have to be rewritten or is this a use case where parallelreader will never mimic indexreader perfectly? at the very least this behavior should be documented so others know what to expect. [junit] testcase: testiscurrent(org.apache.lucene.index.testparallelreader): caused an error [junit] null [junit] java.lang.nullpointerexception [junit] at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:502) [junit] at org.apache.lucene.index.segmentinfos.readcurrentversion(segmentinfos.java:336) [junit] at org.apache.lucene.index.indexreader.iscurrent(indexreader.java:316) [junit] at org.apache.lucene.index.testparallelreader.testiscurrent(testparallelreader.java:146) index: src/test/org/apache/lucene/index/testparallelreader.java =================================================================== \u2014 src/test/org/apache/lucene/index/testparallelreader.java (revision 518122) +++ src/test/org/apache/lucene/index/testparallelreader.java (working copy) @@ -135,6 +135,15 @@ assertequals(docparallel.get(\"f4\"), docsingle.get(\"f4\")); } } + + public void testiscurrent() throws ioexception { + directory dir1 = getdir1(); + directory dir2 = getdir2(); + parallelreader pr = new parallelreader(); + pr.add(indexreader.open(dir1)); + pr.add(indexreader.open(dir2)); + asserttrue(pr.iscurrent()); + } // fiels 1-4 indexed together: private searcher single() throws ioexception {",
        "label": 32
    },
    {
        "text": "remove int floatarrayallocator from facet module  spinoff from lucene-4600. it makes me nervous to have allocation tied to our public apis ... and the ability for int/floatarrayallocator to hold onto n arrays indefinitely makes me even more nervous. i think we should just trust java/gc to do their job here and free the storage as soon as faceting is done.",
        "label": 43
    },
    {
        "text": "add wgs84 capability to geo3d support wgs84 compatibility has been requested for geo3d. this involves working with an ellipsoid rather than a unit sphere. the general formula for an ellipsoid is: x^2/a^2 + y^2/b^2 + z^2/c^2 = 1",
        "label": 10
    },
    {
        "text": "bad java practices which affect performance  result of code inspection  intellij idea found the following issues in the lucense source code and tests: 1) explicit for loops where calls to system.arraycopy() should have been 2) calls to boolean constructor (in stead of the appropriate static method/field) 3) instantiation of unnecessary integer instances for tostring, instead of calling the static one 4) string concatenation using + inside a call to stringbuffer.append(), in stead of chaining the append calls all minor issues. patch is forthcoming.",
        "label": 33
    },
    {
        "text": "allow the complexphrasequeryparser to search order or un order proximity queries  the complexphrasequeryparser use spannearquery, but always set the \"inorder\" value hardcoded to \"true\". this could be configurable.",
        "label": 13
    },
    {
        "text": "nightly build failed javadoc tasked failed due to new project structure in contrib/gdata-server added correct package structure to java/trunk/build.xml javadoc creation successful. patch added as attachment. regards simon",
        "label": 15
    },
    {
        "text": "gdata   server wrong commit does not build the last gdata - server commit does not build due to a wrong commit. yonik did not commit all the files in the diff file. there are several sources and packages missing. the diff - file with the date of 26.06.06 should be applied. --> http://issues.apache.org/jira/browse/lucene-598 26.06.06.diff (644 kb) could any of the lucene committers apply this patch. yonik is on the way to dublin. thanks simon",
        "label": 18
    },
    {
        "text": "upgrade icu libraries to modules/analysis uses 4.4 solr/contrib/extraction uses 4.2.1 i think we should keep them the same version, for consistency, and go to 4.4.2 since it has bugfixes.",
        "label": 40
    },
    {
        "text": "index splitter if an index has multiple segments, this tool allows splitting those segments into separate directories.",
        "label": 33
    },
    {
        "text": " patch  better support gcj compilation there are two methods in indexreader.java called 'delete'. that is a reserved keyword in c++ and these methods cause trouble for gcj which implements a clever workaround in renaming them delete$ but the os x dynamic linker doesn't pick-up on it. the attached patch renames delete(int) to deletedocument(int) and delete(term) to deletedocuments(term) and deprecates the delete methods (as requested by doug cutting).",
        "label": 14
    },
    {
        "text": "add bumpversion script to increment version after release branch creation thanks to lucene-5898 there are many less places to increment version. however, i still think this script can be useful in automating the entire process (minus the commit). this would: add new sections to lucene/changes.txt and solr/changes.txt add new version constant change latest value change version.base in lucene/version.properties change version used in solr example configs create a bwc index and test if necessary",
        "label": 41
    },
    {
        "text": "clean up obselete information on the website when searching for information on 'lucene indexing speed' i get back some really out of date stuff: 1. on the features page it proudly proclaims 20mb/minute, on some really old hardware. i think we should change this to 95gb/hour: http://blog.mikemccandless.com/2010/09/lucenes-indexing-is-fast.html 2. there are ancient benchmarks results from versioned data we link to the website. we list versioned websites for ancient versions going back to 1.4.3. also i noticed when just casually googling for api documentation i tend to get results going to these ancient versions. i think we should remove stuff for all versions prior to 2.9",
        "label": 40
    },
    {
        "text": "unclear documentation of storedfieldvisitor binaryvalue when reading the binary value of a stored field, a storedfieldsreader calls storedfieldvisitor.binaryvalue(arr, offset, length). documentation currently doesn't state whether the byte[] can be reused outside of the scope of storedfieldvisitor.binaryvalue but documentstoredfieldvisitor assumes (as of r1389812) that it can. so documentstoredfieldvisitor would break with a custom storedfieldsformat that would call storedfieldvisitor.binaryvalue with a slice of a reusable buffer.",
        "label": 40
    },
    {
        "text": "fix javadocs after deprecation removal there are a lot of @links in javadocs to methods/classes that no longer exist. javadoc target prints tons of warnings. we should fix that.",
        "label": 53
    },
    {
        "text": "allow aggregating facets by any valuesource facets can be aggregated today by counting them, aggregating by summing their association values, or summing the scores of the documents. applications can write their own facetsaggregator to compute the value of the facet in other ways. following the new expressions module, i think it will be interesting to allow aggregating facets by arbitrary expressions, e.g. _score * sqrt(price) where 'price' is an ndv field. i'd like to explore allowing any valuesource to be passed in and write a valuesourcefacetsaggregator.",
        "label": 43
    },
    {
        "text": "move solr's functionquery impls to queries module now that we have the main interfaces in the queries module, we can move the actual impls over. impls that won't be moved are: function/distance/* (to be moved to a spatial module) function/filefloatsource.java (depends on solr's schema, data directories and exposes a requesthandler)",
        "label": 7
    },
    {
        "text": "chararrayset cannot be made generic  because it violates the set char  interface i tried to make chararrayset using generics (extends abstractset<char[]>) but this is not possible, as it e.g. returns sometimes string instances in the iterator instead of []. also its addall method accepts both string and char[]. i think this class is a complete mis-design and violates almost everything (sorry). what to do? make it set<?> or just place a big @suppresswarnings(\"unchecked\"> in front of it? because of this problem also a lot of set declarations inside stopanalyzer cannot be made generic as you never know whats inside.",
        "label": 53
    },
    {
        "text": "flexible queryparser fails with local different from en us i get the following error during the mentioned testcases on my computer, if i use the locale de_de (windows 32):     [junit] testsuite: org.apache.lucene.queryparser.standard.testqphelper     [junit] tests run: 29, failures: 1, errors: 0, time elapsed: 1,156 sec     [junit]     [junit] ------------- standard output ---------------     [junit] result: (fieldx:xxxxx fieldy:xxxxxxxx)^2.0     [junit] ------------- ---------------- ---------------     [junit] testcase: testlocaldateformat(org.apache.lucene.queryparser.standard.testqphelper): failed     [junit] expected:<1> but was:<0>     [junit] junit.framework.assertionfailederror: expected:<1> but was:<0>     [junit]     at org.apache.lucene.queryparser.standard.testqphelper.asserthits(testqphelper.java:1148)     [junit]     at org.apache.lucene.queryparser.standard.testqphelper.testlocaldateformat(testqphelper.java:1005)     [junit]     at org.apache.lucene.util.lucenetestcase.runtest(lucenetestcase.java:201)     [junit]     [junit]     [junit] test org.apache.lucene.queryparser.standard.testqphelper failed     [junit] testsuite: org.apache.lucene.queryparser.standard.testqueryparserwrapper     [junit] tests run: 27, failures: 1, errors: 0, time elapsed: 1,219 sec     [junit]     [junit] ------------- standard output ---------------     [junit] result: (fieldx:xxxxx fieldy:xxxxxxxx)^2.0     [junit] ------------- ---------------- ---------------     [junit] testcase: testlocaldateformat(org.apache.lucene.queryparser.standard.testqueryparserwrapper):       failed     [junit] expected:<1> but was:<0>     [junit] junit.framework.assertionfailederror: expected:<1> but was:<0>     [junit]     at org.apache.lucene.queryparser.standard.testqueryparserwrapper.asserthits(testqueryparserwrapper.java:1120)     [junit]     at org.apache.lucene.queryparser.standard.testqueryparserwrapper.testlocaldateformat(testqueryparserwrapper.java:985)     [junit]     at org.apache.lucene.util.lucenetestcase.runtest(lucenetestcase.java:201)     [junit]     [junit]     [junit] test org.apache.lucene.queryparser.standard.testqueryparserwrapper failed with en_us as locale it works.",
        "label": 32
    },
    {
        "text": "norms reading fails with filenotfound in exceptional case if we can't get to the bottom of this, we can always add the fileexists check back...     [junit] testsuite: org.apache.lucene.index.testindexwriterexceptions     [junit] testcase: testrandomexceptionsthreads(org.apache.lucene.index.testindexwriterexceptions): caused an error     [junit] no sub-file with id _nrm.cfs found (filename=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])     [junit] java.io.filenotfoundexception: no sub-file with id _nrm.cfs found (filename=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])     [junit]  at org.apache.lucene.store.compoundfiledirectory.createslicer(compoundfiledirectory.java:313)     [junit]  at org.apache.lucene.store.compoundfiledirectory.<init>(compoundfiledirectory.java:65)     [junit]  at org.apache.lucene.codecs.lucene40.lucene40docvaluesproducer.<init>(lucene40docvaluesproducer.java:48)     [junit]  at org.apache.lucene.codecs.lucene40.lucene40normsformat$lucene40normsdocvaluesproducer.<init>(lucene40normsformat.java:70)     [junit]  at org.apache.lucene.codecs.lucene40.lucene40normsformat.docsproducer(lucene40normsformat.java:49)     [junit]  at org.apache.lucene.codecs.lucene40.lucene40normsformat.docsproducer(lucene40normsformat.java:62)     [junit]  at org.apache.lucene.index.segmentcorereaders.<init>(segmentcorereaders.java:122)     [junit]  at org.apache.lucene.index.segmentreader.<init>(segmentreader.java:54)     [junit]  at org.apache.lucene.index.directoryreader$1.dobody(directoryreader.java:65)     [junit]  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:660)     [junit]  at org.apache.lucene.index.directoryreader.open(directoryreader.java:55)     [junit]  at org.apache.lucene.index.indexreader.open(indexreader.java:242)     [junit]  at org.apache.lucene.index.testindexwriterexceptions.testrandomexceptionsthreads(testindexwriterexceptions.java:304)     [junit]  at org.apache.lucene.util.lucenetestcase$3$1.evaluate(lucenetestcase.java:530)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:165)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:57)     [junit]      [junit]      [junit] tests run: 22, failures: 0, errors: 1, time elapsed: 3.439 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testindexwriterexceptions -dtestmethod=testrandomexceptionsthreads -dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -dargs=\"-dfile.encoding=utf-8\"     [junit] note: test params are: codec=lucene40: {field=mockvariableintblock(baseblocksize=31), id=postingsformat(name=nestedpulsing), content=pulsing40(freqcutoff=2 minblocksize=58 maxblocksize=186), contents=mockvariableintblock(baseblocksize=31), content1=mockvariableintblock(baseblocksize=31), content2=postingsformat(name=mocksep), content4=pulsing40(freqcutoff=2 minblocksize=58 maxblocksize=186), content5=mockfixedintblock(blocksize=964), content6=postingsformat(name=memory), content7=postingsformat(name=mockrandom), crash=postingsformat(name=nestedpulsing), subid=postingsformat(name=nestedpulsing)}, sim=randomsimilarityprovider(querynorm=false,coord=true): {other=dfr gb3(800.0), contents=ib spl-l3(800.0), content=dfr gl3(800.0), id=dfr i(f)l1, field=ib ll-dz(0.3), content1=dfr i(ne)bz(0.3), content2=dfr i(n)3(800.0), content3=dfr gz(0.3), content4=dfr i(ne)b2, content5=ib ll-l3(800.0), content6=ib spl-d2, crash=dfr i(f)3(800.0), content7=dfr i(f)b3(800.0), subid=ib ll-l1}, locale=de_ch, timezone=canada/saskatchewan     [junit] note: all tests run in this jvm:     [junit] [testassertions, testnumerictokenstream, testsimpleattributeimpl, testimpersonation, testpulsingreuse, testdocument, testaddindexes, testatomicupdate, testbyteslices, testcheckindex, testconcurrentmergescheduler, testconsistentfieldnumbers, testcrashcausescorruptindex, testdoccount, testdocumentwriter, testflex, testforcemergeforever, testindexinput, testindexreader, testindexwriterconfig, testindexwriterexceptions]     [junit] note: linux 3.0.0-14-generic amd64/sun microsystems inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640",
        "label": 46
    },
    {
        "text": "windowsdirectory we can use windows' overlapped io to do pread() and avoid the performance problems of simplefs/niofsdir.",
        "label": 40
    },
    {
        "text": "testgeo3dpoint testgeo3drelations  failure  invalid bounds for shape georectangle my jenkins found a reproducing master seed: checking out revision 53a34b312e78ce6f56c0bb41304ac834b28b9534 (refs/remotes/origin/master) [...]  [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint    [junit4]   1>     doc=556 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=0.7599270561688233, lon=-3.141592653589793([x=-0.7245396312428835, y=-8.873051402570008e-17, z=0.6885391639622669])]    [junit4]   1>       quantized=[x=-0.7245396311865936, y=-2.3309121299774915e-10, z=0.6885391641833415]    [junit4]   1>     doc=562 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=0.671324362197981, lon=-3.141592653589793([x=-0.7828546079153551, y=-9.587203897812958e-17, z=0.621909142012704])]    [junit4]   1>       quantized=[x=-0.782854608095381, y=-2.3309121299774915e-10, z=0.6219091421482176]    [junit4]   1>     doc=671 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=0.7277792617505155, lon=-3.141592653589793([x=-0.7463767772766416, y=-9.140479312497561e-17, z=0.6649666093161187])]    [junit4]   1>       quantized=[x=-0.7463767772806459, y=-2.3309121299774915e-10, z=0.6649666092550081]    [junit4]   1>     doc=902 is contained by shape but is outside the returned xyzbounds    [junit4]   1>       unquantized=[lat=0.7408865305170907, lon=-3.141592653589793([x=-0.737564708579924, y=-9.032562595264542e-17, z=0.6746626165197899])]    [junit4]   1>       quantized=[x=-0.7375647084975573, y=-2.3309121299774915e-10, z=0.6746626163258577]    [junit4]   1>   shape=georectangle: {planetmodel=planetmodel.wgs84, toplat=0.7988584710911523(45.77121882179671), bottomlat=0.25383311815493353(14.543566370923246), leftlon=-1.2236144735575564e-12(-7.010794508597004e-11), rightlon=7.356011300929654e-49(4.214684015938074e-47)}    [junit4]   1>   bounds=xyzbounds: [xmin=-0.6971004676427383 xmax=0.9688341314344869 ymin=-1.0011854794644763e-9 ymax=1.0e-9 zmin=0.25134364633812856 zmax=0.7161240534205101]    [junit4]   2> note: download the large jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testgeo3drelations -dtests.seed=843049e58dfc1576 -dtests.slow=true -dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -dtests.locale=ja-jp-u-ca-japanese-x-lvariant-jp -dtests.timezone=america/indiana/vevay -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 0.57s j8 | testgeo3dpoint.testgeo3drelations <<<",
        "label": 25
    },
    {
        "text": "compile error with fst package example code i run the fst construction example guided package.html with lucene 4.10, and found a compile error. http://lucene.apache.org/core/4_10_2/core/index.html?org/apache/lucene/util/fst/package-summary.html javac claimed as below. \"fsttest\" is my test class, just copied from javadoc's example. $ javac -cp /opt/lucene-4.10.2/core/lucene-core-4.10.2.jar fsttest.java  fsttest.java:28: error: method tointsref in class util cannot be applied to given types;       builder.add(util.tointsref(scratchbytes, scratchints), outputvalues[i]);                       ^   required: bytesref,intsrefbuilder   found: bytesref,intsref   reason: actual argument intsref cannot be converted to intsrefbuilder by method invocation conversion note: fsttest.java uses or overrides a deprecated api. note: recompile with -xlint:deprecation for details. 1 error i modified scratchints variable type from intsref to intsrefbuilder, it worked fine. (i checked o.a.l.u.fst.testfsts.java testcase and my modification seems to be correct.) util.tointsref() method takes intsrefbuilder as 2nd argument instead of intsref since 4.10, so javadocs also should be fixed.",
        "label": 26
    },
    {
        "text": "ramdirectory not serializable the current implementation of ramdirectory throws a notserializableexception when trying to serialize, due to the inner class keyset of hashmap not being serializable (god knows why) java.io.notserializableexception: java.util.hashmap$keyset at java.io.objectoutputstream.writeobject0(objectoutputstream.java:1081) caused by line 43: private set filenames = filemap.keyset(); edit: while we're at it: same goes for inner class values java.io.notserializableexception: java.util.hashmap$values at java.io.objectoutputstream.writeobject0(objectoutputstream.java:1081) collection files = filemap.values();",
        "label": 33
    },
    {
        "text": "improved kuromoji search mode segmentation decompounding kuromoji has a segmentation mode for search that uses a heuristic to promote additional segmentation of long candidate tokens to get a decompounding effect. this heuristic has been improved. patch is coming up.",
        "label": 40
    },
    {
        "text": "analysis package level javadocs analysis package level javadocs need improving. an overview of what an analyzer does, and maybe some sample code showing how to write you own analyzer, tokenizer and tokenfilter would be really helpful. bonus would be some discussion on best practices for achieving performance during analysis.",
        "label": 12
    },
    {
        "text": "add init method to closeablethreadlocal java threadlocal has an init method that allows subclasses to easily instantiate an initial value.",
        "label": 33
    },
    {
        "text": "uax29urlemailtokenizer should not tokenize no scheme domain only urls that are followed by an alphanumeric character the uax29urlemailtokenizer tokenises index2.php as: <url> index2.ph <alphanum> p while it does not do the same for index.php screenshot from analyser: http://postimg.org/image/aj6c98n3b/",
        "label": 47
    },
    {
        "text": "add crosses query support to rangefield rangefield currently supports intersects, within, and contains query behavior. this feature adds support for an explicit crosses query. unlike intersect and overlap queries the crosses query finds any indexed ranges whose interior (within range) intersect the interior and exterior (outside range) of the query range.",
        "label": 36
    },
    {
        "text": "port index sorter to trunk apis lucene-2482 added an indexsorter to 3.x, but we need to port this functionality to 4.0 apis.",
        "label": 43
    },
    {
        "text": "intellij classpath doesn't include phonetic and morfologik resources  directories the .iml files generated by 'ant idea' for the phonetic and morfologik modules don't include src/resources as source paths. this means that the filterfactory classes defined in these modules are not loaded by the lucene spiloader, and consequently the example solr instance used by various tests does not start.",
        "label": 47
    },
    {
        "text": "regular expression syntax with multifieldqueryparser causes assert npe using regex syntax causes multifieldqueryparser.parse() to throw an assertionerror (if asserts are on) or causes subsequent searches using the returned query instance to throw nullpointerexception (if asserts are off). simon willnauer's comment on the java-user alias: \"this is in-fact a bug in the multifieldqueryparser [...] multifieldqueryparser should override getregexpquery but it doesn't\"",
        "label": 46
    },
    {
        "text": "bytesref copy short missed the length setting when storing a short type integer to bytesref, bytesref missed the length setting. then it will cause the storage size is zero if no continuous options on this bytesref",
        "label": 40
    },
    {
        "text": "add shinglefilter option to output unigrams if no shingles can be generated currently if shinglefilter.outputunigrams==false and the underlying token stream is only one token long, then shinglefilter.next() won't return any tokens. this patch provides a new option, outputunigramifnongrams; if this option is set and the underlying stream is only one token long, then shinglefilter will return that token, regardless of the setting of outputunigrams. my use case here is speeding up phrase queries. the technique is as follows: first, doing index-time analysis using shinglefilter (using outputunigrams==true), thereby expanding things as follows: \"please divide this sentence into shingles\" -> \"please\", \"please divide\" \"divide\", \"divide this\" \"this\", \"this sentence\" \"sentence\", \"sentence into\" \"into\", \"into shingles\" \"shingles\" second, do query-time analysis using shinglefilter (using outputunigrams==false and outputunigramifnongrams==true). if the user enters a phrase query, it will get tokenized in the following manner: \"please divide this sentence into shingles\" -> \"please divide\" \"divide this\" \"this sentence\" \"sentence into\" \"into shingles\" by doing phrase queries with bigrams like this, i can gain a very considerable speedup. without the outputunigramifnongrams option, then a single word query would tokenize like this: \"please\" -> [no tokens] but thanks to outputunigramifnongrams, single words will now tokenize like this: \"please\" -> \"please\" **** the patch also adds a little to the pre-outputunigramifnongrams option tests. **** i'm not sure if the patch in this state is useful to anyone else, but i thought i should throw it up here and try to find out.",
        "label": 47
    },
    {
        "text": "request to change visibility of classes in geo3d i am creating my own spatial context by wrapping the objects in lucene geo3d library and implement my own query which mixes the recursive and the serialized strategy to add precision to searches. i had the following issue specially with polygons: the factory for creating polygons is slow and when serializing shapes, i already know if i am dealing with convex or concave polygons (in my case they are always concave). so when de-serializing a polygon i do not want to go through the factory to decide the type of polygon to create. wouldn't make sense to add the possibility to create a type of polygon directly? you cannot create the polygons directly as they are protected in the package. my suggestion request would be either to add methods in the polygon factory to create a type of polygons with no checking (e.g. makeconcavepolygon(\u2026)) or change the visibility of the classes so that they can be instantiated directly.",
        "label": 25
    },
    {
        "text": "checks for coplanarity when creating polygons shows numerical issues coplanarity checks in geopolygonfactory shows numerical errors when the distance between two points is very small compared to the distance of the other two points. the situation is as follows: having three points a, b, & c and the distance between a & b is very small compared to the distance between a & c, then the plane ac contains all points (co-planar) but the plane defined by ab does not contain c because of numerical issues. this situation makes some polygons fail to build.",
        "label": 25
    },
    {
        "text": "getdocvalues should provide a multireader docvalues abstraction when scoring a valuesourcequery, the scoring code calls valuesource.getvalues(reader) on each leaf level subreader \u2013 so docvalue instances are backed by the individual fieldcache entries of the subreaders \u2013 but if client code were to inadvertently called getvalues() on a multireader (or directoryreader) they would wind up using the \"outer\" fieldcache. since getvalues(indexreader) returns docvalues, we have an advantage here that we don't have with fieldcache api (which is required to provide direct array access). getvalues(indexreader) could be implimented so that if some a caller inadvertently passes in a reader with non-null subreaders, getvalues could generate a docvalues instance for each of the subreaders, and then wrap them in a composite \"multidocvalues\".",
        "label": 33
    },
    {
        "text": "windows tests  x  hanging for hrs in stall control again here the stack dump, from build (extracted with jstack on windows): http://jenkins.sd-datasolutions.de/job/lucene-solr-4.x-windows-java6-64/118/ jvm is 1.6.0_32, 64bit, server, 2 cpus, windows 7 64 2012-06-19 17:21:42 full thread dump java hotspot(tm) 64-bit server vm (20.7-b02 mixed mode): \"thread 0\" prio=6 tid=0x00000000070dc000 nid=0xcec waiting on condition [0x000000000e18f000]    java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method)  - parking to wait for  <0x00000000f684e8d8> (a org.apache.lucene.index.documentswriterstallcontrol$sync)  at java.util.concurrent.locks.locksupport.park(locksupport.java:156)  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:811)  at java.util.concurrent.locks.abstractqueuedsynchronizer.doacquiresharedinterruptibly(abstractqueuedsynchronizer.java:969)  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquiresharedinterruptibly(abstractqueuedsynchronizer.java:1281)  at org.apache.lucene.index.documentswriterstallcontrol.waitifstalled(documentswriterstallcontrol.java:120)  at org.apache.lucene.index.documentswriterflushcontrol.waitifstalled(documentswriterflushcontrol.java:618)  at org.apache.lucene.index.documentswriter.preupdate(documentswriter.java:301)  at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:361)  at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:1327)  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1078)  at org.apache.lucene.index.indexwriter.adddocument(indexwriter.java:1059)  at org.apache.lucene.index.testnrtreaderwiththreads$runthread.run(testnrtreaderwiththreads.java:94)    locked ownable synchronizers:  - none \"test-testscope-org.apache.lucene.index.testnrtreaderwiththreads.testindexing-seed#[641a6b3e2297f46e]\" prio=6 tid=0x00000000070d9800 nid=0x448 in object.wait() [0x00000000058de000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000f685b540> (a org.apache.lucene.index.testnrtreaderwiththreads$runthread)  at java.lang.thread.join(thread.java:1186)  - locked <0x00000000f685b540> (a org.apache.lucene.index.testnrtreaderwiththreads$runthread)  at java.lang.thread.join(thread.java:1239)  at org.apache.lucene.index.testnrtreaderwiththreads.testindexing(testnrtreaderwiththreads.java:61)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1969)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$1100(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:814)  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:875)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:889)  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)  at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)  at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:821)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:669)  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:695)  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:734)  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:745)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)  at org.apache.lucene.util.testruleicuhack$1.evaluate(testruleicuhack.java:51)  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)  at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)  at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:56)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551)    locked ownable synchronizers:  - none \"low memory detector\" daemon prio=6 tid=0x000000000492f000 nid=0xfe4 runnable [0x0000000000000000]    java.lang.thread.state: runnable    locked ownable synchronizers:  - none \"c2 compilerthread1\" daemon prio=10 tid=0x000000000492c000 nid=0x6b0 waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable    locked ownable synchronizers:  - none \"c2 compilerthread0\" daemon prio=10 tid=0x00000000002ca800 nid=0xf1c waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable    locked ownable synchronizers:  - none \"attach listener\" daemon prio=10 tid=0x00000000002c9000 nid=0xcc8 waiting on condition [0x0000000000000000]    java.lang.thread.state: runnable    locked ownable synchronizers:  - none \"signal dispatcher\" daemon prio=10 tid=0x0000000004920800 nid=0x91c runnable [0x0000000000000000]    java.lang.thread.state: runnable    locked ownable synchronizers:  - none \"finalizer\" daemon prio=8 tid=0x00000000002b5000 nid=0xf4c in object.wait() [0x00000000048df000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000e01873c8> (a java.lang.ref.referencequeue$lock)  at java.lang.ref.referencequeue.remove(referencequeue.java:118)  - locked <0x00000000e01873c8> (a java.lang.ref.referencequeue$lock)  at java.lang.ref.referencequeue.remove(referencequeue.java:134)  at java.lang.ref.finalizer$finalizerthread.run(finalizer.java:159)    locked ownable synchronizers:  - none \"reference handler\" daemon prio=10 tid=0x00000000002ac000 nid=0xe10 in object.wait() [0x00000000047df000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000e0156290> (a java.lang.ref.reference$lock)  at java.lang.object.wait(object.java:485)  at java.lang.ref.reference$referencehandler.run(reference.java:116)  - locked <0x00000000e0156290> (a java.lang.ref.reference$lock)    locked ownable synchronizers:  - none \"main\" prio=6 tid=0x00000000003ae000 nid=0x558 in object.wait() [0x0000000000e0f000]    java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  - waiting on <0x00000000f67e96b8> (a com.carrotsearch.randomizedtesting.randomizedrunner$2)  at java.lang.thread.join(thread.java:1186)  - locked <0x00000000f67e96b8> (a com.carrotsearch.randomizedtesting.randomizedrunner$2)  at java.lang.thread.join(thread.java:1239)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:561)  at com.carrotsearch.randomizedtesting.randomizedrunner.run(randomizedrunner.java:521)  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.execute(slavemain.java:145)  at com.carrotsearch.ant.tasks.junit4.slave.slavemain.main(slavemain.java:238)  at com.carrotsearch.ant.tasks.junit4.slave.slavemainsafe.main(slavemainsafe.java:12)    locked ownable synchronizers:  - none \"vm thread\" prio=10 tid=0x00000000002a3800 nid=0x614 runnable  \"gc task thread#0 (parallelgc)\" prio=6 tid=0x0000000000203000 nid=0x7dc runnable  \"gc task thread#1 (parallelgc)\" prio=6 tid=0x0000000000205000 nid=0xc20 runnable  \"vm periodic task thread\" prio=10 tid=0x0000000004938000 nid=0x720 waiting on condition  jni global references: 1586",
        "label": 46
    },
    {
        "text": "weightedspantermextractor should not rewrite multitermquery all the time currently weightedspantermextractor will rewrite multitermquery regardless of the field being requested for highlighting. in some case like solr-2216, it can be costly and cause toomanyclauses exception for no reason.",
        "label": 10
    },
    {
        "text": "finding newest segment in empty index while extending the index writer, i discovered that its newestsegment method does not check to see if there are any segments before accessing the segment infos vector. specifically, if you call the indexwriter#newestsegment method on a brand-new index which is essentially empty, then it throws an java.lang.arrayindexoutofboundsexception exception. the proposed fix is to return null if no segments exist, as shown below: \u2014 lucene/src/java/org/apache/lucene/index/indexwriter.java (revision 930788) +++ lucene/src/java/org/apache/lucene/index/indexwriter.java (working copy) @@ -4587,7 +4587,7 @@ // utility routines for tests segmentinfo newestsegment() { - return segmentinfos.info(segmentinfos.size()-1); + return segmentinfos.size() > 0 ? segmentinfos.info(segmentinfos.size()-1) : null; }",
        "label": 33
    },
    {
        "text": "allowing the benchmarking algorithm to choose postingsformat the algorithm file for benchmarking should allow postingsformat to be configurable.",
        "label": 10
    },
    {
        "text": "make queryautostopwordanalyzer immutable and reusable currently queryautostopwordanalyzer allows its list of stop words to be changed after instantiation through its addstopwords() methods. this stops the analyzer from being reusable since it must instantiate its stopfilters every time. having these methods means that although the analyzer can be instantiated once and reused between indexreaders, the actual analysis stack is not reusable (which is probably the more expensive part). so lets change the analyzer so that its stop words are set at instantiation time, facilitating reuse.",
        "label": 7
    },
    {
        "text": "fieldcache rewrite method for multitermqueries for some multitermqueries, like rangequery we have a fieldcacherangefilter etc (in this case its particularly optimized). but in the general case, since lucene-2784 we can now have a rewrite method to rewrite any multitermquery using the fieldcache, because multitermquery's getenum no longer takes indexreader but terms, and all the filteredtermsenums are now just real termsenum decorators. in cases like low frequency queries this is actually slower (i think this has been shown for numeric ranges before too), but for the really high-frequency cases like especially ugly wildcards, regexes, fuzzies, etc, this can be several times faster using the fieldcache instead, since all the terms are in ram and automaton can blast through them quicker.",
        "label": 40
    },
    {
        "text": "contrib  thaianalyzer to enable thai full text search in lucene thai text don't have space between words. usually, a dictionary-based algorithm is used to break string into words. for lucene to be usable for thai, an analyzer that know how to break thai words is needed. i've implemented such analyzer, thaianalyzer, using icu4j dictionarybasedbreakiterator for word breaking. i'll upload the code later. i'm normally a c++ programmer and very new to java. please review the code for any problem. one possible problem is that it requires icu4j. i don't know whether this is ok.",
        "label": 18
    },
    {
        "text": "nrtcachingdirectory should implement accountable ",
        "label": 46
    },
    {
        "text": "log state in indexreplicatorhandler exception handler replicator indexreplicationhandler method cleanupoldindexfiles() does not log state in case of an exception, but there is a comment inside handler informing that the state should be logged.",
        "label": 43
    },
    {
        "text": "memoryindex addfield violates tokenstream contract running the example from the javadoc page generates a illegalstateexception. http://lucene.apache.org/core/4_7_0/memory/org/apache/lucene/index/memory/memoryindex.html java.lang.runtimeexception: java.lang.illegalstateexception: tokenstream contract violation: reset()/close() call missing, reset() called multiple times, or subclass does not call super.reset(). please see javadocs of tokenstream class for more information about the correct consuming workflow.  at org.apache.lucene.index.memory.memoryindex.addfield(memoryindex.java:463)  at org.apache.lucene.index.memory.memoryindex.addfield(memoryindex.java:298)  at be.curtaincall.provisioning.searchtest.testsearch(searchtest.java:32)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47)  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12)  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44)  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17)  at org.junit.runners.parentrunner.runleaf(parentrunner.java:271)  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70)  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50)  at org.junit.runners.parentrunner$3.run(parentrunner.java:238)  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63)  at org.junit.runners.parentrunner.runchildren(parentrunner.java:236)  at org.junit.runners.parentrunner.access$000(parentrunner.java:53)  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229)  at org.junit.runners.parentrunner.run(parentrunner.java:309)  at org.junit.runner.junitcore.run(junitcore.java:160)  at com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs(junit4ideatestrunner.java:77)  at com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart(junitstarter.java:195)  at com.intellij.rt.execution.junit.junitstarter.main(junitstarter.java:63)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at com.intellij.rt.execution.application.appmain.main(appmain.java:120) caused by: java.lang.illegalstateexception: tokenstream contract violation: reset()/close() call missing, reset() called multiple times, or subclass does not call super.reset(). please see javadocs of tokenstream class for more information about the correct consuming workflow.  at org.apache.lucene.analysis.tokenizer$1.read(tokenizer.java:110)  at org.apache.lucene.analysis.util.characterutils.readfully(characterutils.java:213)  at org.apache.lucene.analysis.util.characterutils$java5characterutils.fill(characterutils.java:255)  at org.apache.lucene.analysis.util.characterutils.fill(characterutils.java:203)  at org.apache.lucene.analysis.util.chartokenizer.incrementtoken(chartokenizer.java:135)  at org.apache.lucene.index.memory.memoryindex.addfield(memoryindex.java:429)  ... 28 more also tested in 3.7.0, but version not yet created in jira.",
        "label": 53
    },
    {
        "text": "improved large result handling per http://search.lucidimagination.com/search/document/350c54fc90d257ed/lots_of_results#fbb84bd297d15dd5, it would be nice to offer some other collectors that are better at handling really large number of results. this could be implemented in a variety of ways via collectors. for instance, we could have a raw collector that does no sorting and just returns the scoredocs, or we could do as mike suggests and have collectors that have heuristics about memory tradeoffs and only heapify when appropriate.",
        "label": 15
    },
    {
        "text": "issues with prefix tree's distance error percentage see attached patch for a failing test basically, it's a simple point and radius scenario that works great as long as args.setdistprecision(0.0); is called. once the default precision is used (2.5%), it doesn't work as expected. the distance between the 2 points in the patch is 35.75 km. taking into account the 2.5% error the effective radius without false negatives/positives should be around 34.8 km. this test fails with a radius of 33 km.",
        "label": 10
    },
    {
        "text": "pass iw to mergepolicy related to lucene-5708 we keep state in the mp holding on to the iw which prevents sharing the mp across index writers. aside of this we should really not keep state in the mp it should really only select merges without being bound to the index writer.",
        "label": 46
    },
    {
        "text": "move hasvectors    hasprox  responsibility out of segmentinfo to fieldinfos spin-off from lucene-2881 which had this change already but due to some random failures related to this change i remove this part of the patch to make it more isolated and easier to test.",
        "label": 46
    },
    {
        "text": "geo3d  more accurate way of computing circle planes the geo3d code that computes circle planes in geopath and geocircle is less accurate the smaller the circle. there's a better way of computing this.",
        "label": 10
    },
    {
        "text": "testsimpleexplanations failure ant test -dtestcase=testsimpleexplanations -dtestmethod=testdmq8 -dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71 fails w/ this on current trunk... looks like silly floating point precision issue:     [junit] testsuite: org.apache.lucene.search.testsimpleexplanations     [junit]   1.4508595 = (match) sum of:     [junit]     1.4508595 = (match) weight(field:yy in 2) [defaultsimilarity], result of:     [junit]       1.4508595 = score(doc=2,freq=1.0 = termfreq=1     [junit] ), product of:     [junit]         1.287682 = queryweight, product of:     [junit]           1.287682 = idf(docfreq=2, maxdocs=4)     [junit]           1.0 = querynorm     [junit]         1.1267219 = fieldweight in 2, product of:     [junit]           1.0 = tf(freq=1.0), with freq of:     [junit]             1.0 = termfreq=1     [junit]           1.287682 = idf(docfreq=2, maxdocs=4)     [junit]           0.875 = fieldnorm(doc=2)     [junit]   145085.95 = (match) weight(field:xx^100000.0 in 2) [defaultsimilarity], result of:     [junit]     145085.95 = score(doc=2,freq=1.0 = termfreq=1     [junit] ), product of:     [junit]       128768.2 = queryweight, product of:     [junit]         100000.0 = boost     [junit]         1.287682 = idf(docfreq=2, maxdocs=4)     [junit]         1.0 = querynorm     [junit]       1.1267219 = fieldweight in 2, product of:     [junit]         1.0 = tf(freq=1.0), with freq of:     [junit]           1.0 = termfreq=1     [junit]         1.287682 = idf(docfreq=2, maxdocs=4)     [junit]         0.875 = fieldnorm(doc=2)     [junit]  expected:<145086.66> but was:<145086.69>)     [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 0.544 sec     [junit]      [junit] ------------- standard error -----------------     [junit] note: reproduce with: ant test -dtestcase=testsimpleexplanations -dtestmethod=testdmq8 -dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02     [junit] note: test params are: codec=preflex, sim=randomsimilarityprovider(querynorm=false,coord=false): {field=defaultsimilarity, alt=dfr i(ne)lz(0.3), key=ib ll-d2}, locale=en_in, timezone=pacific/samoa     [junit] note: all tests run in this jvm:     [junit] [testsimpleexplanations]     [junit] note: linux 2.6.33.6-147.fc13.x86_64 amd64/sun microsystems inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864     [junit] ------------- ---------------- ---------------     [junit] testcase: testdmq8(org.apache.lucene.search.testsimpleexplanations): failed     [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationscore=145086.69 explanation: 145086.69 = (match) max plus 0.5 times others of:     [junit]   1.4508595 = (match) sum of:     [junit]     1.4508595 = (match) weight(field:yy in 2) [defaultsimilarity], result of:     [junit]       1.4508595 = score(doc=2,freq=1.0 = termfreq=1     [junit] ), product of:     [junit]         1.287682 = queryweight, product of:     [junit]           1.287682 = idf(docfreq=2, maxdocs=4)     [junit]           1.0 = querynorm     [junit]         1.1267219 = fieldweight in 2, product of:     [junit]           1.0 = tf(freq=1.0), with freq of:     [junit]             1.0 = termfreq=1     [junit]           1.287682 = idf(docfreq=2, maxdocs=4)     [junit]           0.875 = fieldnorm(doc=2)     [junit]   145085.95 = (match) weight(field:xx^100000.0 in 2) [defaultsimilarity], result of:     [junit]     145085.95 = score(doc=2,freq=1.0 = termfreq=1     [junit] ), product of:     [junit]       128768.2 = queryweight, product of:     [junit]         100000.0 = boost     [junit]         1.287682 = idf(docfreq=2, maxdocs=4)     [junit]         1.0 = querynorm     [junit]       1.1267219 = fieldweight in 2, product of:     [junit]         1.0 = tf(freq=1.0), with freq of:     [junit]           1.0 = termfreq=1     [junit]         1.287682 = idf(docfreq=2, maxdocs=4)     [junit]         0.875 = fieldnorm(doc=2)     [junit]  expected:<145086.66> but was:<145086.69>     [junit] junit.framework.assertionfailederror: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationscore=145086.69 explanation: 145086.69 = (match) max plus 0.5 times others of:     [junit]   1.4508595 = (match) sum of:     [junit]     1.4508595 = (match) weight(field:yy in 2) [defaultsimilarity], result of:     [junit]       1.4508595 = score(doc=2,freq=1.0 = termfreq=1     [junit] ), product of:     [junit]         1.287682 = queryweight, product of:     [junit]           1.287682 = idf(docfreq=2, maxdocs=4)     [junit]           1.0 = querynorm     [junit]         1.1267219 = fieldweight in 2, product of:     [junit]           1.0 = tf(freq=1.0), with freq of:     [junit]             1.0 = termfreq=1     [junit]           1.287682 = idf(docfreq=2, maxdocs=4)     [junit]           0.875 = fieldnorm(doc=2)     [junit]   145085.95 = (match) weight(field:xx^100000.0 in 2) [defaultsimilarity], result of:     [junit]     145085.95 = score(doc=2,freq=1.0 = termfreq=1     [junit] ), product of:     [junit]       128768.2 = queryweight, product of:     [junit]         100000.0 = boost     [junit]         1.287682 = idf(docfreq=2, maxdocs=4)     [junit]         1.0 = querynorm     [junit]       1.1267219 = fieldweight in 2, product of:     [junit]         1.0 = tf(freq=1.0), with freq of:     [junit]           1.0 = termfreq=1     [junit]         1.287682 = idf(docfreq=2, maxdocs=4)     [junit]         0.875 = fieldnorm(doc=2)     [junit]  expected:<145086.66> but was:<145086.69>     [junit]  at org.apache.lucene.search.checkhits.verifyexplanation(checkhits.java:324)     [junit]  at org.apache.lucene.search.checkhits$explanationasserter.collect(checkhits.java:494)     [junit]  at org.apache.lucene.search.scorer.score(scorer.java:60)     [junit]  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:580)     [junit]  at org.apache.lucene.search.indexsearcher.search(indexsearcher.java:363)     [junit]  at org.apache.lucene.search.checkhits.checkexplanations(checkhits.java:302)     [junit]  at org.apache.lucene.search.queryutils.checkexplanations(queryutils.java:92)     [junit]  at org.apache.lucene.search.queryutils.check(queryutils.java:126)     [junit]  at org.apache.lucene.search.queryutils.check(queryutils.java:122)     [junit]  at org.apache.lucene.search.queryutils.check(queryutils.java:106)     [junit]  at org.apache.lucene.search.checkhits.checkhitcollector(checkhits.java:89)     [junit]  at org.apache.lucene.search.testexplanations.qtest(testexplanations.java:99)     [junit]  at org.apache.lucene.search.testsimpleexplanations.testdmq8(testsimpleexplanations.java:224)     [junit]  at org.apache.lucene.util.lucenetestcase$2$1.evaluate(lucenetestcase.java:611)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:148)     [junit]  at org.apache.lucene.util.lucenetestcaserunner.runchild(lucenetestcaserunner.java:50)     [junit]      [junit]      [junit] test org.apache.lucene.search.testsimpleexplanations failed",
        "label": 40
    },
    {
        "text": "termautomatonquery hashcode calculates automaton todot hash this is going to be excruciatingly slow? we could at least cache the hash code once computed...",
        "label": 11
    },
    {
        "text": "make worddelimiterfilter's instantiation more readable currently worddelimiterfilter's constructor is: public worddelimiterfilter(tokenstream in,                               byte[] chartypetable,                               int generatewordparts,                               int generatenumberparts,                               int catenatewords,                               int catenatenumbers,                               int catenateall,                               int splitoncasechange,                               int preserveoriginal,                               int splitonnumerics,                               int stemenglishpossessive,                               chararrayset protwords) { which means its instantiation is an unreadable combination of 1s and 0s. we should improve this by either using a builder, 'int flags' or an enumset.",
        "label": 7
    },
    {
        "text": "random failure testsizeboundedoptimize testfirstsegmenttoolarge i am seeing this on trunk [junit] testsuite: org.apache.lucene.index.testsizeboundedoptimize     [junit] testcase: testfirstsegmenttoolarge(org.apache.lucene.index.testsizeboundedoptimize): failed     [junit] expected:<2> but was:<1>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<1>     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:882)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:848)     [junit]  at org.apache.lucene.index.testsizeboundedoptimize.testfirstsegmenttoolarge(testsizeboundedoptimize.java:160)     [junit]      [junit]      [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 0.658 sec     [junit]      [junit] ------------- standard output ---------------     [junit] note: reproduce with: ant test -dtestcase=testsizeboundedoptimize -dtestmethod=testfirstsegmenttoolarge -dtests.seed=7354441978302993522:-457602792543755447 -dtests.multiplier=3     [junit] note: test params are: codec=standard, locale=sv_se, timezone=mexico/bajanorte     [junit] ------------- ---------------- ---------------     [junit] ------------- standard error -----------------     [junit] note: all tests run in this jvm:     [junit] [testsizeboundedoptimize]     [junit] ------------- ---------------- ---------------     [junit] test org.apache.lucene.index.testsizeboundedoptimize failed when running with this seed ant test -dtestcase=testsizeboundedoptimize -dtestmethod=testfirstsegmenttoolarge -dtests.seed=7354441978302993522:-457602792543755447 -dtests.multiplier=3",
        "label": 43
    },
    {
        "text": "contrib xml query parser  numericrangequery and  filter support create a filterbuilder for numericrangefilter so that it may be used with the xml query parser.",
        "label": 28
    },
    {
        "text": "unsupportedoperationexception wrt spannearquery with gap  needed for synonym query expansion  i am trying to support synonyms on the query side by doing query expansion. for example, the query \"open webpage\" can be expanded if the following things are synonyms: \"open\" | \"go to\" this becomes the following: (i'm using both the stop word filter and the stemming filter) spannear(          [                  spanor([title:open, title:go]),                  title:webpag          ],          0,          true ) notice that \"go to\" became just \"go\", because apparently \"to\" is removed by the stop word filter. interestingly, if you turn \"go to webpage\" into a phrase, you get \"go ? webpage\", but if you turn \"go to\" into a phrase, you just get \"go\", because apparently a trailing stop word in a phrasequery gets dropped. (there would actually be no way to represent the gap currently because it represents gaps implicitly via the position of the phrase tokens, and if there is no second token, there's no way to implicitly indicate that there is a gap there) the above query then fails to match \"go to webpage\", because \"go to webpage\" in the index tokenizes as \"go _ webpage\", and the query, because it lost its gap, tried to only match \"go webpage\". to try and work around that, i represent \"go to\" not as a phrase, but as a spannearquery, like this: spannear(          [                  spanor(                          [                                  title:open,                                  spannear([title:go, spangap(:1)], 0, true),                          ]                  ),                  title:webpag          ],          0,          true ) however, when i run that query, i get the following: a java exception occurred: java.lang.unsupportedoperationexception      at  org.apache.lucene.search.spans.spannearquery$gapspans.positionscost(spannearquery.java:398)      at  org.apache.lucene.search.spans.conjunctionspans.astwophaseiterator(conjunctionspans.java:96)      at  org.apache.lucene.search.spans.nearspansordered.astwophaseiterator(nearspansordered.java:45)      at  org.apache.lucene.search.spans.scoringwrapperspans.astwophaseiterator(scoringwrapperspans.java:88)      at  org.apache.lucene.search.conjunctiondisi.addspans(conjunctiondisi.java:104)      at  org.apache.lucene.search.conjunctiondisi.intersectspans(conjunctiondisi.java:82)      at  org.apache.lucene.search.spans.conjunctionspans.<init>(conjunctionspans.java:41)      at  org.apache.lucene.search.spans.nearspansordered.<init>(nearspansordered.java:54)      at  org.apache.lucene.search.spans.spannearquery$spannearweight.getspans(spannearquery.java:232)      at  org.apache.lucene.search.spans.spanweight.scorer(spanweight.java:134)      at org.apache.lucene.search.spans.spanweight.scorer(spanweight.java:38)      at org.apache.lucene.search.weight.bulkscorer(weight.java:135) ... and when i look up that gapspans class in spannearquery.java, i see: @override public float positionscost() {    throw new unsupportedoperationexception(); } i asked this question on the mailing list on may 14 and was directed to submit a bug here. this issue is of relatively high priority for us, since this represents the most promising technique we have for supporting synonyms on top of lucene. (since the synonymfilter suffers serious issues wrt multi-word synonyms)",
        "label": 2
    },
    {
        "text": "numericutils getminlong and numericutils getmaxlong have undefined behavior when no docs have value   throw npe tracked down a possible cause of solr-7866 to situations where a (numeric) field doesn't have any values in an index and you try to get the min/max. javadocs for numericutils.getminlong and numericutils.getmaxlong don't actually say what that method will do in this case, throw npe when it happens",
        "label": 49
    },
    {
        "text": "enable setting the terms index divisor used by indexwriter whenever it opens internal readers opening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to iwc to do so. apps with very large numbers of unique terms must set the terms index divisor to control ram usage. (note: flex's ram terms dict index ram usage is more efficient, so this will help such apps). but, when iw resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that. though one workaround is to call getreader(terminfosindexdivisor) which will pool the reader with the right divisor.",
        "label": 33
    },
    {
        "text": "checkjavadocs py mis chunks javadocs html and then wrongly reports imbalanced tags spin-off from solr-9107, where hoss man wrote: but as things stand with this patch, precommit currently complains about malformed javadocs...      [echo] checking for malformed docs...      [exec]       [exec] /home/hossman/lucene/dev/solr/build/docs/solr-test-framework/org/apache/solr/util/randomizessl.html      [exec]   broken details html: field detail: reason: saw closing \"</ul>\" without opening <ul...>      [exec]   broken details html: field detail: ssl: saw closing \"</ul>\" without opening <ul...>      [exec]   broken details html: field detail: clientauth: saw closing \"</ul>\" without opening <ul...> ...but i can't really understand why. the <ul> tags look balanced to me, and tidy -output /dev/null .../randomizessl.html concurs that \"no warnings or errors were found.\" i thought maybe the problem was related to some of the @see tags in the docs for these attributes, but even if i completley remove the javadocs the same validation errors occur. when i modify checkjavadocs.py to print out the offending chunk of html, here's what i see for the first of the above: solr/build/docs/solr-test-framework/org/apache/solr/util/randomizessl.html   broken details html: field detail: reason: saw closing \"</ul>\" without opening <ul...> in: ----- <ul><pre>public abstract&nbsp;<a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/string.html?is-external=true\" title=\"class or interface in java.lang\">string</a>&nbsp;reason</pre> <div class=\"block\">comment to inlcude when logging details of ssl randomization</div> <dl> <dt>default:</dt> <dd>\"\"</dd> </dl> </li> </ul> </li> </ul> <ul class=\"blocklist\"> <li class=\"blocklist\"><a name=\"ssl--\"> <!--   --> </a> <ul class=\"blocklist\"> <li class=\"blocklist\"> </ul> so the chunking that's happening here isn't aligning with the detail html for methods, fields etc. - it doesn't start early enough and ends too late. furthormore, i can see that the chunking procedure ignores the final item in an html file (the stuff after the last <h4>) - if i insert trash after the final <h4>, but within the javadocs for the corresponding final detail item in the html file, the current implementation ignores the problem.",
        "label": 47
    },
    {
        "text": "incorrect usage of attributesource addattribute getattribute leads to failures when onlyusenewapi true when seting \"use only new api\" for tokenstream, i received the following exception:    [junit] caused by: java.lang.illegalargumentexception: this attributesource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.termattribute'.     [junit]  at org.apache.lucene.util.attributesource.getattribute(attributesource.java:249)     [junit]  at org.apache.lucene.index.termshashperfield.start(termshashperfield.java:252)     [junit]  at org.apache.lucene.index.docinverterperfield.processfields(docinverterperfield.java:145)     [junit]  at org.apache.lucene.index.docfieldprocessorperthread.processdocument(docfieldprocessorperthread.java:244)     [junit]  at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:772)     [junit]  at org.apache.lucene.index.documentswriter.updatedocument(documentswriter.java:755)     [junit]  at org.apache.lucene.index.indexwriter.updatedocument(indexwriter.java:2613) however, i can't actually see the culprit that caused this exception suggest that the illegalargumentexception include \"getclass().getname()\" in order to be able to identify which tokenstream implementation actually caused this",
        "label": 53
    },
    {
        "text": "termvectoroffsetinfo has incomplete javadocs org.apache.lucene.index.termvectoroffsetinfo has no javadocs at all.",
        "label": 15
    },
    {
        "text": "lucene42docvaluesproducer rambytesused is over estimated lucene42docvaluesproducer.rambytesused uses ramusageestimator.sizeof(this) to return an estimation of the memory usage. one of the issues (there might be other ones) is that this class has a reference to an indexinput that might link to other data-structures that we wouldn't want to take into account. for example, index inputs of a ramdirectory all point to the directory itself, so lucene42docvaluesproducer.rambytesused would return the amount of memory used by the whole directory.",
        "label": 1
    },
    {
        "text": "use nativefslockfactory as default for new api  direct ctors   fsdir open  a user requested we add a note in indexwriter alerting the availability of nativefslockfactory (allowing you to avoid retaining locks on abnormal jvm exit). seems reasonable to me - we want users to be able to easily stumble upon this class. the below code looks like a good spot to add a note - could also improve whats there a bit - opening an indexwriter does not necessarily create a lock file - that would depend on the lockfactory used.   <p>opening an <code>indexwriter</code> creates a lock file for the directory in use. trying to open   another <code>indexwriter</code> on the same directory will lead to a   {@link lockobtainfailedexception}. the {@link lockobtainfailedexception}   is also thrown if an indexreader on the same directory is used to delete documents   from the index.</p> anyone remember why nativefslockfactory is not the default over simplefslockfactory?",
        "label": 53
    },
    {
        "text": "eclipse classpath inconsistent with changes   project fails to build the dot.classpath file is referencing files that no longer exist or were moved. this causes a project error b/c there are references in the build path that no longer exist.",
        "label": 40
    },
    {
        "text": "alternate lucene query highlighter i created a lucene query highlighter (borrowing some code from the one in the sandbox) that my company is using. it better handles phrase queries, doesn't break html entities, and has the ability to either highlight terms in an entire document or to highlight fragments from the document. i would like to make it available to anyone who wants it.",
        "label": 29
    },
    {
        "text": "fix remaining lucene solr javadocs issue java 8 has a new feature (enabled by default): http://openjdk.java.net/jeps/172 it fails the build on: incorrect links (@see, @link,...) incorrect html entities invalid html in general thanks to our linter written in htmltidy and python, most of the bugs are already solved in our source code, but the oracle linter finds some more problems, our linter does not: missing escapes < invalid entities unfortunately the versions of jdk8 released up to today have a bug, making optional closing tags (which are valid html4), like </p>, mandatory. this will be fixed in b78. currently there is another bug in the oracle javadocs tool (it fails to copy doc-files folders), but this is under investigation at the moment. we should clean up our javadocs, so the pass the new jdk8 javadocs tool with build 78+. maybe we can put our own linter out of service, once we rely on java 8",
        "label": 53
    },
    {
        "text": "testgrouping testrandom  failures my jenkins found a reproducing branch_6x seed:    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgrouping -dtests.method=testrandom -dtests.seed=6f0b519bbdc786b3 -dtests.slow=true -dtests.locale=en-ca -dtests.timezone=europe/zagreb -dtests.asserts=true -dtests.file.encoding=us-ascii    [junit4] failure 2.50s j0 | testgrouping.testrandom <<<    [junit4]    > throwable #1: java.lang.assertionerror: expected:<10> but was:<29>    [junit4]    >  at __randomizedtesting.seedinfo.seed([6f0b519bbdc786b3:1d4774940ca730c0]:0)    [junit4]    >  at org.apache.lucene.search.grouping.testgrouping.assertequals(testgrouping.java:1288)    [junit4]    >  at org.apache.lucene.search.grouping.testgrouping.testrandom(testgrouping.java:1130)    [junit4]    >  at java.lang.thread.run(thread.java:745)    [junit4]   2> note: test params are: codec=asserting(lucene62): {groupend=lucene50(blocksize=128), sort1=postingsformat(name=memory dopackfst= true), sort2=lucene50(blocksize=128), content=postingsformat(name=memory dopackfst= false), group=postingsformat(name=memory dopackfst= true)}, docvalues:{groupend=docvaluesformat(name=direct), author=docvaluesformat(name=memory), sort1=docvaluesformat(name=memory), id=docvaluesformat(name=memory), sort2=docvaluesformat(name=direct), content=docvaluesformat(name=lucene54), group=docvaluesformat(name=memory)}, maxpointsinleafnode=454, maxmbsortinheap=6.048552291438714, sim=randomsimilarity(querynorm=false,coord=no): {content=dfi(saturated)}, locale=en-ca, timezone=europe/zagreb    [junit4]   2> note: linux 4.1.0-custom2-amd64 amd64/oracle corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=485264792,total=514850816 git bisect says the first bad commit is this one: http://git-wip-us.apache.org/repos/asf/lucene-solr/commit/6d952076, but that just removed booleansimilarity from the randomsimilarity's candidates list, which in this case likely has the side effect of picking a different similarity with this seed. so the problem is almost certainly older than this. policeman jenkins reported a master seed four weeks ago https://jenkins.thetaphi.de/job/lucene-solr-master-linux/18890/ for a failure that's still reproducing for me on linux w/ java8:   [junit4]   2> note: reproduce with: ant test  -dtestcase=testgrouping -dtests.method=testrandom -dtests.seed=381a6a2bb3b21736 -dtests.multiplier=3 -dtests.slow=true -dtests.locale=nyn -dtests.timezone=antarctica/mcmurdo -dtests.asserts=true -dtests.file.encoding=us-ascii   [junit4] failure 8.72s j0 | testgrouping.testrandom <<<   [junit4]    > throwable #1: java.lang.assertionerror: expected:<2526> but was:<1818>   [junit4]    >  at __randomizedtesting.seedinfo.seed([381a6a2bb3b21736:4a564f2402d2a145]:0)   [junit4]    >  at org.apache.lucene.search.grouping.testgrouping.assertequals(testgrouping.java:1299)   [junit4]    >  at org.apache.lucene.search.grouping.testgrouping.testrandom(testgrouping.java:1141)   [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method)   [junit4]    >  at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)   [junit4]    >  at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)   [junit4]    >  at java.base/java.lang.reflect.method.invoke(method.java:543)   [junit4]    >  at java.base/java.lang.thread.run(thread.java:844)   [junit4]   2> note: test params are: codec=asserting(lucene70): {groupend=postingsformat(name=direct), sort1=postingsformat(name=lucenevargapfixedinterval), sort2=postingsformat(name=direct), content=postingsformat(name=lucenevargapdocfreqinterval), group=postingsformat(name=lucenevargapfixedinterval)}, docvalues:{groupend=docvaluesformat(name=lucene70), author=docvaluesformat(name=memory), sort1=docvaluesformat(name=memory), id=docvaluesformat(name=memory), sort2=docvaluesformat(name=lucene70), content=docvaluesformat(name=direct), group=docvaluesformat(name=memory)}, maxpointsinleafnode=1292, maxmbsortinheap=6.668104559353747, sim=randomsimilarity(querynorm=true): {content=dfi(standardized)}, locale=nyn, timezone=antarctica/mcmurdo   [junit4]   2> note: linux 4.4.0-53-generic i386/oracle corporation 9-ea (32-bit)/cpus=12,threads=1,free=23290544,total=79691776 git bisect points to the exact same first bad commit for this seed too (see above).",
        "label": 2
    },
    {
        "text": "ngramtokenfilter optimization in query phase i found that ngramtokenfilter-ed token stream could be optimized in query. a standard 1,2 ngramtokenfilter will generate a token stream from \"abcde\" as follows: a ab b bc c cd d de e when we index \"abcde\", we'll use all of the tokens. but when we query, we only need: ab cd de",
        "label": 38
    },
    {
        "text": "npe in analyzingsuggester setting maxgraphexpansions > 0 with a lot of expansions (e.g. due to synonyms). set<intsref> paths = tofinitestrings(surfaceform, ts2a); paths may be null, so maxanalyzedpathsforoneinput = math.max(maxanalyzedpathsforoneinput, paths.size()) may end with npe",
        "label": 33
    },
    {
        "text": "lucene index files can not be reproduced faithfully  due to timestamps embedded  eclipse 3.7 uses lucene 2.9.1 for indexing online help content. a pre-generated help index can be shipped together with online content. as per [https://bugs.eclipse.org/bugs/show_bug.cgi?id=364979] it turns out that the help index can not be faithfully reproduced during a build, because there are timestamps embedded in the index files, and the \"namecounter\" field in segments_2 contains different contents on every build. not being able to faithfully reproduce the index from identical source bits undermines trust in the index (and software delivery) being correct. i'm wondering whether this is a known issue and/or has been addressed in a newer lucene version already ?",
        "label": 33
    },
    {
        "text": "compressingstoredfieldswriter ignores the segment suffix if writing aborted if the writing is aborted, compressingstoredfieldswriter does not remove partially-written files as the segment suffix is not taken into consideration.",
        "label": 1
    },
    {
        "text": "geo3dshapespheremodelrectrelationtest testgeobboxrect fails http://build-eu-00.elastic.co/job/lucene_linux_java8_64_test_only/50825/ java.lang.assertionerror: did not find enough contains/within/intersection/disjoint/bounds cases in a reasonable number of random attempts. cwidbd: 3368(23),19(23),9123(23),1251(23),9559(23)  laps exceeded 23320  at __randomizedtesting.seedinfo.seed([e317e8bc5ac1274b:c7b240847ef25915]:0)  at org.junit.assert.fail(assert.java:93)  at org.apache.lucene.spatial.spatial4j.rectintersectiontesthelper.testrelatewithrectangle(rectintersectiontesthelper.java:96)  at org.apache.lucene.spatial.spatial4j.geo3dshaperectrelationtestcase.testgeobboxrect(geo3dshaperectrelationtestcase.java:147)",
        "label": 10
    },
    {
        "text": "add n best output capability to japanesetokenizer japanese morphological analyzers often generate mis-segmented tokens. n-best output reduces the impact of mis-segmentation on search result. n-best output is more meaningful than character n-gram, and it increases hit count too. if you use n-best output, you can get decompounded tokens (ex: \"\u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\" => {\"\u30b7\u30cb\u30a2\", \"\u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\", \"\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\", \"\u30a8\u30f3\u30b8\u30cb\u30a2\"} ) and overwrapped tokens (ex: \"\u6570\u5b66\u90e8\u9577\u8c37\u5ddd\" => {\"\u6570\u5b66\", \"\u90e8\", \"\u90e8\u9577\", \"\u9577\u8c37\u5ddd\", \"\u8c37\u5ddd\"} ), depending on the dictionary and n-best parameter settings.",
        "label": 8
    },
    {
        "text": "add axiomatic similarity add axiomatic similarity approaches to the similarity family. more details can be found at http://dl.acm.org/citation.cfm?id=1076116 and https://www.eecis.udel.edu/~hfang/pubs/sigir05-axiom.pdf there are in total six similarity models. all of them are based on bm25, pivoted document length normalization or language model with dirichlet prior. we think it is worthy to add the models as part of lucene.",
        "label": 50
    },
    {
        "text": "tie mockgraphtokenfilter into all analyzers tests mike made a mockgraphtokenfilter on lucene-3848. many filters currently arent tested with anything but a simple tokenstream. we should test them with this, too, it might find bugs (zero-length terms, stacked terms/synonyms, etc)",
        "label": 33
    },
    {
        "text": "use fst to hold term in stemmeroverridefilter follow-up from lucene-4857",
        "label": 46
    },
    {
        "text": "customscorequery calls weight  where it should call createweight  thanks to uwe for helping me track down this bug after i pulled my hair out for hours on lucene-3174.",
        "label": 53
    },
    {
        "text": "several gdata build targets don't work from contrib gdata the contrib/gdata/build.xml file is a little ... odd, and many of the targets do't work at all when called from that directory (only when using build-contrib from the top level) this problem predates lucene-885 ... chrish@asimov:~/svn/lucene-bugs/contrib/gdata-server$ svnversion 542768 chrish@asimov:~/svn/lucene-bugs/contrib/gdata-server$ ant test buildfile: build.xml test: [echo] building gdata-core... javacc-uptodate-check: javacc-notice: common.init: build-lucene: init: compile-core: [echo] use gdata - compile-core task [javac] compiling 5 source files to /home/chrish/svn/lucene-bugs/build/contrib/gdata-server/core/classes/java warning: reference build.path has not been set at runtime, but was found during build file parsing, attempting to resolve. future versions of ant may support referencing ids defined in non-executed targets. warning: reference common.build.path has not been set at runtime, but was found during build file parsing, attempting to resolve. future versions of ant may support referencing ids defined in non-executed targets. build failed /home/chrish/svn/lucene-bugs/contrib/gdata-server/build.xml:87: the following error occurred while executing this line: /home/chrish/svn/lucene-bugs/contrib/gdata-server/src/core/build.xml:49: the following error occurred while executing this line: /home/chrish/svn/lucene-bugs/common-build.xml:298: /home/chrish/svn/lucene-bugs/contrib/gdata-server/src/core/ext-libs not found. total time: 1 second",
        "label": 38
    },
    {
        "text": " patch  lockfactory implementation based on os native locks  java nio  the current default locking for fsdirectory is simplefslockfactory. it uses java.io.file.createnewfile for its locking, which has this spooky warning in sun's javadocs: note: this method should not be used for file-locking, as the resulting protocol cannot be made to work reliably. the filelock facility should be used instead. so, this patch provides a lockfactory implementation based on filelock (using java.nio.*). all unit tests pass with this patch, on os x (10.4.8), linux (ubuntu 6.06), and windows xp sp2. another benefit of native locks is the os automatically frees them if the jvm exits before lucene can free its locks. many people seem to hit this (old lock files still on disk) now. i've created this new class: org.apache.lucene.store.nativefslockfactory and added a couple test cases to the existing testlockfactory. i've left simplefslockfactory as the default locking for fsdirectory for now. i think we should get some usage / experience with nativefslockfactory and then later on make it the default locking implementation? i also tested changing fsdirectory's default locking to nativefslockfactory and all unit tests still pass (on the above platforms). one important note about locking over nfs: some nfs servers and/or clients do not support it, or, it's a configuration option or mode that must be explicitly enabled. when it's misconfigured it's able to take a long time (35 seconds in my case) before throwing an exception. to handle this, i acquire & release a random test lock on creating the nativefslockfactory to verify locking is configured properly. a few other small changes in the patch: added a \"failure reason\" to lock.java so that in obtain(lockwaittimeout), if there is a persistent ioexception in trying to obtain the lock, this can be messaged & included in the \"lock obtain timed out\" that's raised. corrected javadoc in simplefslockfactory: it previously said the wrong system property for overriding lock class via system properties fixed unhandled ioexception when opening an indexwriter for create, if the locks dir does not exist (just added lockdir.exists() check in clearalllocks method of simplefslockfactory & nativefslockfactory. fixed a few small unrelated issues with testlockfactory, and also fixed tests to accept nativefslockfactory as the default locking implementation for fsdirectory. fixed a typo in javadoc in fieldsreader.java added some more javadoc for the lockfactory.setlockprefix",
        "label": 33
    },
    {
        "text": "tokensources gettokenstream  does not assign positionincrement tokensources.storedtokenstream does not assign positionincrement information. this means that all tokens in the stream are considered adjacent. this has implications for the phrase highlighting in queryscorer when using non-contiguous tokens. for example: consider a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped) when retrieved from the index using tokensources.gettokenstream(tpv,false), the token stream will be - the fox jump jumped now try a search and highlight for the phrase query \"fox jumped\". the search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between \"fox\" and \"jumped\". if we use the original (from the analyzer) token stream then the highlighter works. also, consider the converse - the fox did not jump \"not\" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4) when retrieved from the index using tokensources.gettokenstream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3). so the phrase query \"did jump\" will cause the \"did\" and \"jump\" terms in the text \"did not jump\" to be highlighted. if we use the original (from the analyzer) token stream then the highlighter works correctly.",
        "label": 29
    },
    {
        "text": "consolidate customscorequery  valuesourcequery and boostedquery lucene's customscorequery and solr's boostedquery do essentially the same thing: they boost the scores of documents by the value from a valuesource. boostedquery does this in a direct fashion, by accepting a valuesource. customscorequery on the other hand, accepts a series of valuesourcequerys. valuesourcequery seems to do exactly the same thing as functionquery. with lucene's valuesource being deprecated / removed, we need to resolve these dependencies and simplify the code. therefore i recommend we do the following things: move customscorequery (and customscoreprovider) to the new queries module and change it over to use functionquerys instead of valuesourcequerys. deprecate solr's boostedquery in favour of the new customscorequery. csq provides a lot of support for customizing the scoring process. move and consolidate all tests of csq and boostedquery, to the queries module and have them test csq instead.",
        "label": 7
    },
    {
        "text": "smoketestrelease checkmaven should not require a release branch currently this throws an exception, but i think it should unpack any pom templates it needs from the artifacts themselves. its nice to be able to generate and test rc's without having an official branch yet. i am currently doing this just to move us closer to release (just trying to find bugs etc). also anyone should be able to just toss up an rc at any time. its better to test the artifacts themselves rather than anything in svn. at the least the -src jars have an svn export so this should work... if it doesn't, then there is a packaging bug.",
        "label": 47
    },
    {
        "text": "make default dimension config in facetconfig adjustable now it is hardcoded to default_dim_config. this may be useful for most standard approaches. however, i use lots of facets. these facets can be multivalued, i do not know that on beforehand. so what i would like to do is to change the default config to mulitvalued = true. currently i have a working, but rather ugly workaround that subclasses facetconfig, like this: customfacetconfig.java public class customfacetsconfig extends facetsconfig {  public final static dimconfig default_d2a_dim_config = new dimconfig();  static  {   default_d2a_dim_config.multivalued = true;  }  @override  public synchronized dimconfig getdimconfig( string dimname )  {   dimconfig ft = super.getdimconfig( dimname );   if ( default_dim_config.equals( ft ) )   {    return default_d2a_dim_config;   }   return ft;  } } i created a patch to illustrate what i would like to change. by making a protected method it is easier to create a custom subclass of facetconfig. also, maybe there are better way to accomplish my goal (easy default to multivalue?)",
        "label": 33
    },
    {
        "text": "shinglefilter benchmark spawned from lucene-2218: a benchmark for shinglefilter, along with a new task to instantiate (non-default-constructor) shingleanalyzerwrapper: newshingleanalyzertask. the included shingle.alg runs shingleanalyzerwrapper, wrapping the default standardanalyzer, with 4 different configurations over 10,000 reuters documents each. to allow shinglefilter timings to be isolated from the rest of the pipeline, standardanalyzer is also run over the same set of reuters documents. this set of 5 runs is then run 5 times. the patch includes two perl scripts, the first to output jira table formatted timing information, with the minimum elapsed time for each of the 4 shingleanalyzerwrapper runs and the standardanalyzer run, and the second to compare two runs' jira output, producing another jira table showing % improvement.",
        "label": 40
    },
    {
        "text": "docmakers setup for the  docs dir  property fails when passing an absolute path  setconfig in trecdocmaker assumes docs.dir is a relative path. therefore it create new file(workdir, docs.dir). however, if docs.dir is an absolute path, this works incorrectly and results in no txt files in datadir exception.",
        "label": 12
    },
    {
        "text": "correct copy paste victim comment correct the doc-comment of fieldsproducer (being a copy-paste victim of fieldsconsumer). \"consumes\" replaced with \"produces\". one word change to avoid confusion: safe to commit.",
        "label": 46
    },
    {
        "text": "bump language level in intellij configuration for trunk bump language level in intellij configuration for trunk from 1.7 to 1.8, since we are using 1.8 features already..",
        "label": 45
    },
    {
        "text": "back compat tests  ant test tag  should test jar drop in ability we now test back-compat with \"ant test-tag\", which is very useful for catching breaks in back compat before committing. however, that currently checks out \"src/test\" sources and then compiles them against the trunk jar, and runs the tests. whereas our back compat policy: http://wiki.apache.org/lucene-java/backwardscompatibility states that no recompilation is required on upgrading to a new jar. ie you should be able to drop in the new jar in place of your old one and things should work fine. so... we should fix \"ant test-tag\" to: do full checkout of core sources & tests from the back-compat-tag compile the jar from the back-compat sources compile the tests against that back-compat jar swap in the trunk jar run the tests",
        "label": 32
    },
    {
        "text": "similaritydelegator is missing a delegating scorepayload  method the handy similaritydelegator method is missing a scoredelegator() delegating method. the fix is trivial, add the code below at the end of the class: public float scorepayload(string fieldname, byte [] payload, int offset, int length) { return delegee.scorepayload(fieldname, payload, offset, length); }",
        "label": 15
    },
    {
        "text": "cjktokenizer convert halfwidth and fullwidth forms wrong cjktokenizer have these lines.. if (ub == character.unicodeblock.halfwidth_and_fullwidth_forms) { /** convert halfwidth_and_fullwidth_forms to basic_latin */ int i = (int) c; i = i - 65248; c = (char) i; } this is wrong. some character in the block (e.g. u+ff68) have no basic_latin counterparts. only 65281-65374 can be converted this way. the fix is if (ub == character.unicodeblock.halfwidth_and_fullwidth_forms && i <= 65474 && i> 65281) { /** convert halfwidth_and_fullwidth_forms to basic_latin */ int i = (int) c; i = i - 65248; c = (char) i; }",
        "label": 33
    },
    {
        "text": "indexreader   docommit   typo nit about v3 in trunk trunk is already in 3.0.1+ . but the documentation says - \"in 3.0, this will become ... \". since it is already in 3.0, it might as well be removed.",
        "label": 33
    },
    {
        "text": "fix javadoc comments in search package with lucene-1856 i removed some old, deprecated search methods, and with the hitcollector work new ones were introduced in 2.9. we need to fix the javadocs now in the search package, especially in classes like searcher, searchable, multisearcher, parallelmultisearcher, package.html, etc. we should also make the javadocs more intuitive for the user: currently we call all search methods in searchable \"low-level\" and most of them \"expert\". i think it's currently very confusing for the user to understand which methods to use.",
        "label": 53
    },
    {
        "text": "move queryparsers from contrib queryparser to queryparser module each of the queryparsers will be ported across. those which use the flexible parsing framework will be placed under the package flexible. the standardqueryparser will be renamed to flexiblequeryparser and surround.queryparser will be renamed to surroundqueryparser.",
        "label": 7
    },
    {
        "text": "remove write access from segmentreader and possibly move to separate class or indexwriter buffereddeletes  after lucene-3606 is finished, there are some todos: segmentreader still contains (package-private) all delete logic including crazy copyonwrite for validdocs bits. it would be good, if segmentreader itsself could be read-only like all other indexreaders. there are two possibilities to do this: the simple one: subclass segmentreader and make a rwsegmentreader that is only used by indexwriter/buffereddeletes/... directoryreader will only use the read-only segmentreader. this would move all todos to a separate class. it's reopen/clone method would always create a ro-segmentreader (for nrt). remove all write and commit stuff from segmentreader completely and move it to indexwriter's readerpool (it must be in readerpool as deletions need a not-changing view on an index snapshot). unfortunately the code is so complicated and i have no real experience in those internals of indexwriter so i did not want to do it with lucene-3606, i just separated the code in segmentreader and marked with todo. maybe mike mccandless can help",
        "label": 33
    },
    {
        "text": "nullpointer exception in memoryindex memoryindexreader nullpointer exceptions when searching on an index with a query that has a field that's not in the index. the nullpointer is thrown at line 1141: public docvalues normvalues(string field) { if (fieldinfos.get(field).omitsnorms()) <---- //if fieldinfos doesn't contain the field then a nullpointer is thrown. return null;",
        "label": 46
    },
    {
        "text": "enable defaultsimilarity setdiscountoverlaps by default i think we should enable setdiscountoverlaps in defaultsimilarity by default. if you are using synonyms or commongrams or a number of other 0-posinc-term-injecting methods, these currently screw up your length normalization. these terms have a position increment of zero, so they shouldnt count towards the length of the document. i've done relevance tests with persian showing the difference is significant, and i think its a big trap to anyone using synonyms, etc: your relevance can actually get worse if you don't flip this boolean flag.",
        "label": 40
    },
    {
        "text": "add highlighter test for regexquery ",
        "label": 46
    },
    {
        "text": "a few new benchmark tasks some tasks that would be helpful to see added. might want some expansion, but here are some basic ones i have been using: commitindextask reopenreadertask searchwithsorttask i do the sort in a similar way that the highlighting was done, but another method may be better. just would be great to have sorting. also, since there is no great field for sorting (reuters date always appears to be the same) i changed the id field from doc+id to just id. again maybe not the best solution, but here i am to get the ball rolling",
        "label": 15
    },
    {
        "text": "geo3d quantization test failure for max min encoding values here is a reproducible error: 08:45:21    [junit4] suite: org.apache.lucene.spatial3d.testgeo3dpoint 08:45:21    [junit4] ignor/a 0.01s j1 | testgeo3dpoint.testrandombig 08:45:21    [junit4]    > assumption #1: 'nightly' test group is disabled (@nightly()) 08:45:21    [junit4]   2> note: reproduce with: ant test  -dtestcase=testgeo3dpoint -dtests.method=testquantization -dtests.seed=4cb20cf248f6211 -dtests.slow=true -dtests.badapples=true -dtests.locale=ga-ie -dtests.timezone=america/bogota -dtests.asserts=true -dtests.file.encoding=us-ascii 08:45:21    [junit4] error   0.20s j1 | testgeo3dpoint.testquantization <<< 08:45:21    [junit4]    > throwable #1: java.lang.illegalargumentexception: value=-1.0011188543037526 is out-of-bounds (less than than wgs84's -planetmax=-1.0011188539924791) 08:45:21    [junit4]    >  at __randomizedtesting.seedinfo.seed([4cb20cf248f6211:32220fd9326e7f33]:0) 08:45:21    [junit4]    >  at org.apache.lucene.spatial3d.geo3dutil.encodevalue(geo3dutil.java:56) 08:45:21    [junit4]    >  at org.apache.lucene.spatial3d.testgeo3dpoint.testquantization(testgeo3dpoint.java:1228) 08:45:21    [junit4]    >  at java.lang.thread.run(thread.java:748) 08:45:21    [junit4]   2> note: test params are: codec=asserting(lucene70): {id=postingsformat(name=lucenevargapdocfreqinterval)}, docvalues:{id=docvaluesformat(name=asserting), point=docvaluesformat(name=lucene70)}, maxpointsinleafnode=659, maxmbsortinheap=6.225981846119071, sim=randomsimilarity(querynorm=false): {}, locale=ga-ie, timezone=america/bogota 08:45:21    [junit4]   2> note: linux 2.6.32-754.6.3.el6.x86_64 amd64/oracle corporation 1.8.0_181 (64-bit)/cpus=16,threads=1,free=466116320,total=536346624 08:45:21    [junit4]   2> note: all tests run in this jvm: [geopointtest, randomgeopolygontest, testgeo3dpoint] 08:45:21    [junit4] completed [18/18 (1!)] on j1 in 19.83s, 14 tests, 1 error, 1 skipped <<< failures!   it seems this test will fail if encoding = geo3dutil.min_encoded_value or encoding = geo3dutil.max_encoded_value. it is related with https://issues.apache.org/jira/browse/lucene-7327      ",
        "label": 19
    },
    {
        "text": "attempting to add documents past limit can corrupt index the indexwriter check for too many documents does not always work, resulting in going over the limit. once this happens, lucene refuses to open the index and throws a corruptindexexception: too many documents. this appears to affect all versions of lucene/solr (the check was first implemented in lucene-5843 in v4.9.1/4.10 and we've seen this manifest in 4.10)",
        "label": 46
    },
    {
        "text": "remove deprecated scorer explain int  method this is the only remaining deprecation in core, but is not so easy to handle, because lot's of code in core still uses the explain() method in scorer. so e.g. in phrasequery, the explain method has to be moved from scorer to the weight.",
        "label": 53
    },
    {
        "text": "improve readability of standardtermsdictwriter one variable is named indexwriter, but it is a termsindexwriter. also some layout.",
        "label": 33
    },
    {
        "text": "perfieldcodecwrapper causes crashes if not all per field codes have been used if a perfieldcodecwrapper is used an segmentmerger tries to merge two segments where one segment only has a subset of the field perfieldcodecwrapper defines segmentmerger tries to open non-existing files since codec#files(directory, segmentinfo, set<string>) blindly copies the expected files into the given set. this also hits exceptions in checkindex and addindexes(). the reason for this is that perfieldcodecwrapper simply iterates over the codecs it knows and adds all files without checking if they are present in the given directory. we need to have some mechnanism that check if the \"required\" files for a codec are present and only add the files to the set if that field is really there.",
        "label": 46
    },
    {
        "text": "few issues with cachingcollector cachingcollector (introduced in lucene-1421) has few issues: since the wrapped collector may support out-of-order collection, the document ids cached may be out-of-order (depends on the query) and thus replay(collector) will forward document ids out-of-order to a collector that may not support it. it does not clear cachedscores + cachedsegs upon exceeding ram limits i think that instead of comparing curscores to null, in order to determine if scores are requested, we should have a specific boolean - for clarity this check \"if (base + nextlength > maxdocstocache)\" (line 168) can be relaxed? e.g., what if nextlength is, say, 512k, and i cannot satisfy the maxdocstocache constraint, but if it was 10k i would? wouldn't we still want to try and cache them? also: the todo in line 64 (having collector specify needsscores()) \u2013 why do we need that if cachingcollector ctor already takes a boolean \"cachescores\"? i think it's better defined explicitly than implicitly? let's introduce a factory method for creating a specialized version if scoring is requested / not (i.e., impl the todo in line 189) i think it's a useful collector, which stands on its own and not specific to grouping. can we move it to core? how about using openbitset instead of int[] for doc ids? if the number of hits is big, we'd gain some ram back, and be able to cache more entries note: openbitset can only be used for in-order collection only. so we can use that if the wrapped collector does not support out-of-order do you think we can modify this collector to not necessarily wrap another collector? we have such collector which stores (in-memory) all matching doc ids + scores (if required). those are later fed into several processes that operate on them (e.g. fetch more info from the index etc.). i am thinking, we can make cachingcollector optionally wrap another collector and then someone can reuse it by setting ram limit to unlimited (we should have a constant for that) in order to simply collect all matching docs + scores. i think a set of dedicated unit tests for this class alone would be good. that's it so far. perhaps, if we do all of the above, more things will pop up.",
        "label": 43
    },
    {
        "text": "indexwriter allows to add same field with different docvlaues type indexwriter checks if the dv types are consitent in multiple places but if due to some problems in elasticsearch users where able to add the same field with different dv types causing merges to fail. yet i was able to reduce this to a lucene testcase but i was puzzled since it always failed. yet, i had to run it without assertions and that cause the bug to happen. i can add field foo with binary and sorted_set causing a merge to fail. here is a gist https://gist.github.com/s1monw/8707f924b76ba40ee5f3 / https://github.com/elasticsearch/elasticsearch/issues/8009 while this is certainly a problem in elasticsearch lucene also allows to corrupt an index due to user error which i think should be prevented. note: this only fails if you run without assertions which i think lucene should do in ci once in a while too.",
        "label": 33
    },
    {
        "text": "jenkins failure  control claims no stalled threads but waiter seems to be blocked test failure on jenkins: java.lang.assertionerror: control claims no stalled threads but waiter seems to be blocked  at __randomizedtesting.seedinfo.seed([a49457cd5688cbd8:9ee847ff6e680269]:0)  at org.junit.assert.fail(assert.java:93)  at org.junit.assert.asserttrue(assert.java:43)  at org.apache.lucene.index.testdocumentswriterstallcontrol.testaccquirereleaserace(testdocumentswriterstallcontrol.java:163)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:601)  at com.carrotsearch.randomizedtesting.randomizedrunner.invoke(randomizedrunner.java:1969)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$1100(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$6.evaluate(randomizedrunner.java:814)  at com.carrotsearch.randomizedtesting.randomizedrunner$7.evaluate(randomizedrunner.java:875)  at com.carrotsearch.randomizedtesting.randomizedrunner$8.evaluate(randomizedrunner.java:889)  at org.apache.lucene.util.testrulesetupteardownchained$1.evaluate(testrulesetupteardownchained.java:50)  at org.apache.lucene.util.testrulefieldcachesanity$1.evaluate(testrulefieldcachesanity.java:32)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)  at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)  at org.apache.lucene.util.testrulethreadandtestname$1.evaluate(testrulethreadandtestname.java:48)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsingletest(randomizedrunner.java:821)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$700(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$3$1.run(randomizedrunner.java:669)  at com.carrotsearch.randomizedtesting.randomizedrunner$3.evaluate(randomizedrunner.java:695)  at com.carrotsearch.randomizedtesting.randomizedrunner$4.evaluate(randomizedrunner.java:734)  at com.carrotsearch.randomizedtesting.randomizedrunner$5.evaluate(randomizedrunner.java:745)  at org.apache.lucene.util.abstractbeforeafterrule$1.evaluate(abstractbeforeafterrule.java:45)  at org.apache.lucene.util.testrulereportuncaughtexceptions$1.evaluate(testrulereportuncaughtexceptions.java:68)  at org.apache.lucene.util.testrulestoreclassname$1.evaluate(testrulestoreclassname.java:38)  at org.apache.lucene.util.testruleicuhack$1.evaluate(testruleicuhack.java:51)  at com.carrotsearch.randomizedtesting.rules.systempropertiesinvariantrule$1.evaluate(systempropertiesinvariantrule.java:55)  at org.apache.lucene.util.testrulenoinstancehooksoverrides$1.evaluate(testrulenoinstancehooksoverrides.java:53)  at org.apache.lucene.util.testrulenostatichooksshadowing$1.evaluate(testrulenostatichooksshadowing.java:52)  at org.apache.lucene.util.testruleassertionsrequired$1.evaluate(testruleassertionsrequired.java:36)  at org.apache.lucene.util.testrulemarkfailure$1.evaluate(testrulemarkfailure.java:48)  at org.apache.lucene.util.testruleignoretestsuites$1.evaluate(testruleignoretestsuites.java:56)  at com.carrotsearch.randomizedtesting.randomizedrunner.runsuite(randomizedrunner.java:605)  at com.carrotsearch.randomizedtesting.randomizedrunner.access$400(randomizedrunner.java:132)  at com.carrotsearch.randomizedtesting.randomizedrunner$2.run(randomizedrunner.java:551)",
        "label": 46
    },
    {
        "text": "offsets in memoryindex broken when adding field with more than once when fields with the same name are added more than once to memoryindex, the offsets of the previous additions of the field do not seem to be taken into account. as a result, memoryindex cannot be used for example with the vector highlighter.",
        "label": 46
    },
    {
        "text": "make recursion cap in hunspellstemmer configurable currently there is private static final int recursion_cap = 2; in the code of the class hunspellstemmer. it makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex. it costs 36ms to stem long sentence in latvian for recursion_cap=2 and 5 ms for recursion_cap=1). it would be nice to be able to tune this number as needed. afaik this number (2) was chosen arbitrary. (it's a first issue in my life, so please forgive me any mistakes done).",
        "label": 47
    },
    {
        "text": "ramusageestimator num bytes array header and other constants are incorrect ramusageestimator.num_bytes_array_header is computed like that: num_bytes_object_header + num_bytes_int + num_bytes_object_ref. the num_bytes_object_ref part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml a single-dimension array is a single object. as expected, the array has the usual object header. however, this object head is 12 bytes to accommodate a four-byte array length. then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. the memory usage for one element is 4 bytes for an object reference ... while on it, i wrote a sizeof(string) impl, and i wonder how do people feel about including such helper methods in rue, as static, stateless, methods? it's not perfect, there's some room for improvement i'm sure, here it is:  /**   * computes the approximate size of a string object. note that if this object   * is also referenced by another object, you should add   * {@link ramusageestimator#num_bytes_object_ref} to the result of this   * method.   */  public static int sizeof(string str) {   return 2 * str.length() + 6 // chars + additional safeness for arrays alignment     + 3 * ramusageestimator.num_bytes_int // string maintains 3 integers     + ramusageestimator.num_bytes_array_header // char[] array     + ramusageestimator.num_bytes_object_header; // string object  } if people are not against it, i'd like to also add sizeof(int[] / byte[] / long[] / double[] ... and string[]).",
        "label": 53
    },
    {
        "text": "initiatedindex  cce on casting numericfield to field an unchecked cast to list<field> throws a classcastexception when applied to, for example, a numericfield. appearently, this has been fixed trunk, but for a 2.9.1 release, this could be helpful. the patch can be applied against the 2.9.0 tag.",
        "label": 53
    },
    {
        "text": "enable multitermquery's constant score mode to also use booleanquery under the hood when multitermquery is used (via one of its subclasses, eg wildcardquery, prefixquery, fuzzyquery, etc.), you can ask it to use \"constant score mode\", which pre-builds a filter and then wraps that filter as a constantscorequery. if you don't set that, it instead builds a [potentially massive] booleanquery with one should clause per term. there are some limitations of this approach: the scores returned by the booleanquery are often quite meaningless to the app, so, one should be able to use a booleanquery yet get constant scores back. (though i vaguely remember at least one example someone raised where the scores were useful...). the resulting booleanquery can easily have too many clauses, throwing an extremely confusing exception to newish users. it'd be better to have the freedom to pick \"build filter up front\" vs \"build massive booleanquery\", when constant scoring is enabled, because they have different performance tradeoffs. in constant score mode, an openbitset is always used, yet for sparse bit sets this does not give good performance. i think we could address these issues by giving booleanquery a constant score mode, then empower multitermquery (when in constant score mode) to pick & choose whether to use booleanquery vs up-front filter, and finally empower multitermquery to pick the best (sparse vs dense) bit set impl.",
        "label": 33
    },
    {
        "text": "typo in some of the  alg files  forcmerge instead of forcemerge some of the alg files have a typo and have forcmerge when they should have forcemerge. this causes following exception to display when those are used:      [java] java.lang.exception: error: cannot understand algorithm!      [java]  at org.apache.lucene.benchmark.bytask.benchmark.<init>(benchmark.java:63)      [java]  at org.apache.lucene.benchmark.bytask.benchmark.exec(benchmark.java:109)      [java]  at org.apache.lucene.benchmark.bytask.benchmark.main(benchmark.java:84)      [java] caused by: java.lang.classnotfoundexception: forcmerge not found in packages [org.apache.lucene.benchmark.bytask.tasks]      [java]  at org.apache.lucene.benchmark.bytask.utils.algorithm.taskclass(algorithm.java:283)      [java]  at org.apache.lucene.benchmark.bytask.utils.algorithm.<init>(algorithm.java:73)      [java]  at org.apache.lucene.benchmark.bytask.benchmark.<init>(benchmark.java:61)      [java]  ... 2 more caused by: java.lang.classnotfoundexception: forcmerge not found in packages [org.apache.lucene.benchmark.bytask.tasks]",
        "label": 40
    },
    {
        "text": "spatial prefix tree based on s2 geometry hi david smiley, i have been working on a prefix tree based on goggle s2 geometry (https://s2geometry.io/) to be used mainly with geo3d shapes with very promising results, in particular for complex shapes (e.g polygons). using this pixelization scheme reduces the size of the index, improves the performance of the queries and reduces the loading time for non-point shapes. if you are ok with this contribution and before providing any code i would like to understand what is the correct/prefered approach: 1) add new depency to the s2 library (https://mvnrepository.com/artifact/io.sgr/s2-geometry-library-java). it has apache 2.0 license so it should be ok. 2) create a utility class with all methods necessary to navigate the s2 tree and create shapes from s2 cells (basically port what we need from the library into lucene). what do you think?",
        "label": 19
    },
    {
        "text": "fix typo for japanesenumberfilterfactory usage javadocs for japanesenumberfilterfactory have a typo -  https://lucene.apache.org/core/7_5_0/analyzers-kuromoji/org/apache/lucene/analysis/ja/japanesenumberfilterfactory.html instead of  <filter class=\"solr.japanesenumberfilter\"/> we should have  <filter class=\"solr.japanesenumberfilterfactory\"/>  ",
        "label": 2
    },
    {
        "text": "querynodeimpl throws concurrentmodificationexception on add list querynode  on adding a list of children to a querynodeimplemention a concurrentmodificationexception is thrown. this is due to the fact that querynodeimpl instead of iteration over the supplied list, iterates over its internal clauses list. patch: index: querynodeimpl.java =================================================================== \u2014 querynodeimpl.java (revision 911642) +++ querynodeimpl.java (working copy) @@ -74,7 +74,7 @@ .getlocalizedmessage(queryparsermessages.node_action_not_supported)); } for (querynode child : getchildren()) { + for (querynode child : children) { add(child); }",
        "label": 40
    },
    {
        "text": "remove booleanfilter like termfilter, we should remove this filter and recommend on using booleanquery instead. one reason why this is a bit more tricky than termfilter is that booleanfilter creates doc id sets that support random-access while a booleanquery would not provide random-access support.",
        "label": 1
    },
    {
        "text": "cleanup some unused and unnecessary code several classes in trunk have some unused and unnecessary code. this includes unused fields, unused automatic variables, unused imports and unnecessary assignments. attached it a patch to clean these up.",
        "label": 33
    },
    {
        "text": "fuzzysuggester has to operate fsts of unicode letters  not utf  to work correctly for byte  like english  and multi byte  non latin  letters there is a limitation in the current fuzzysuggester implementation: it computes edits in utf-8 space instead of unicode character (code point) space. this should be fixable: we'd need to fix tokenstreamtoautomaton to work in unicode character space, then fix fuzzysuggester to do the same steps that fuzzyquery does: do the levn expansion in unicode character space, then convert that automaton to utf-8, then intersect with the suggest fst. see the discussion here: http://lucene.472066.n3.nabble.com/minfuzzylength-in-fuzzysuggester-behaves-differently-for-english-and-russian-td4067018.html#none",
        "label": 33
    },
    {
        "text": "make segments nn file codec independent i propose to change the format of segmentinfos file (segments_nn) to use plain text instead of the current binary format. segmentinfos file represents a commit point, and it also declares what codecs were used for writing each of the segments that the commit point consists of. however, this is a chicken and egg situation - in theory the format of this file is customizable via codec.getsegmentinfosformat, but in practice we have to first discover what is the codec implementation that wrote this file - so the segmentcorereaders assumes a certain fixed binary layout of a preamble of this file that contains the codec name... and then the file is read again, only this time using the right codec. this is ugly. instead i propose to use a simple plain text format, either line oriented properties or json, in such a way that newer versions could easily extend it, and which wouldn't require any special codec to read and parse. consequently we could remove segmentinfosformat altogether, and instead add segmentinfoformat (notice the singular) to codec to read single per-segment segmentinfo-s in a codec-specific way. e.g. for lucene40 codec we could either add another file or we could extend the .fnm file (fieldinfos) to contain also this information. then the plain text segmentinfos would contain just the following information: list of global files for this commit point (if any) list of segments for this commit point, and their corresponding codec class names user data map",
        "label": 40
    },
    {
        "text": "point2d defines equals by comparing double types with   ideally, this should allow for a margin of error right?",
        "label": 7
    },
    {
        "text": "default field in query syntax documentation has confusing error the explanation of default search fields uses two different queries that are supposed to be semantically the same, but the query text changes between the two examples.",
        "label": 1
    },
    {
        "text": "don't download extract files when doing the build when you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus. this is only needed for one test, and these 20,000 files drive ides and such crazy. instead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc) for the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.",
        "label": 40
    },
    {
        "text": "add truncatetokenfilter i am using this filter as a stemmer for turkish language. in many academic research (classification, retrieval) it is used and called as fixed prefix stemmer or simple truncation method or f5 in short. among f3 to f7, f5 stemmer (length=5) is found to work well for turkish language in information retrieval on turkish texts. it is the same work where most of stopwords_tr.txt are acquired. elasticsearch has truncate filter but it does not respect keyword attribute. and it has a use case similar to truncatefieldupdateprocessorfactory main advantage of f5 stemming is : it does not effected by the meaning loss caused by ascii folding. it is a diacritics-insensitive stemmer and works well with ascii folding. effects of diacritics on turkish information retrieval here is the full field type i use for \"diacritics-insensitive search\" for turkish  <fieldtype name=\"text_tr_ascii_f5\" class=\"solr.textfield\" positionincrementgap=\"100\">    <analyzer>      <tokenizer class=\"solr.standardtokenizerfactory\"/>      <filter class=\"solr.apostrophefilterfactory\"/>      <filter class=\"solr.turkishlowercasefilterfactory\"/>      <filter class=\"solr.asciifoldingfilterfactory\"/>      <filter class=\"solr.keywordrepeatfilterfactory\"/>      <filter class=\"solr.truncatetokenfilterfactory\" prefixlength=\"5\"/>      <filter class=\"solr.removeduplicatestokenfilterfactory\"/>    </analyzer> i would like to get community opinions : 1) any interest in this? 2) keyword attribute should be respected? 3) package name analysis.misc versus analyis.tr 4) name of the class truncatetokenfilter versus fixedprefixstemfilter",
        "label": 40
    },
    {
        "text": "add icu based tokenizer for unicode text segmentation i pulled out the last part of lucene-1488, the tokenizer itself and cleaned it up some. the idea is simple: first step is to divide text into writing system boundaries (scripts) you supply an icutokenizerconfig (or just use the default) which lets you tailor segmentation on a per-writing system basis. this tailoring can be any breakiterator, so rule-based or dictionary-based or your own. the default implementation (if you do not customize) is just to do uax#29, but with tailorings for stuff with no clear word division: thai (uses dictionary-based word breaking) khmer, myanmar, lao (uses custom rules for syllabification) additionally as more of an example i have a tailoring for hebrew that treats the punctuation special. (people have asked before for ways to make standardanalyzer treat dashes differently, etc)",
        "label": 40
    },
    {
        "text": "typo with querynodeoperation logicaland intellij idea finds a bug/typo in querynodeoperation.logicaland -> a boolean condition is always false which means that a and b fails when a is not an andquerynode and b is",
        "label": 53
    },
    {
        "text": "enwikiconentsource does not work with parallel tasks ",
        "label": 33
    },
    {
        "text": "filter to process output of icutokenizer and create overlapping bigrams for cjk the icutokenizer produces unigrams for cjk. we would like to use the icutokenizer but have overlapping bigrams created for cjk as in the cjk analyzer. this filter would take the output of the icutokenizer, read the scriptattribute and for selected scripts (han, kana), would produce overlapping bigrams.",
        "label": 40
    },
    {
        "text": "random shape generator generates invalid polygons with holes hi karl wright, this ticket provides a fix to randomgeo3dshapegenerator so when a polygon with holes is created, it make sure that the hole inside the polygon.",
        "label": 25
    },
    {
        "text": "extend codec to handle also stored fields and term vectors currently codec api handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere. i propose to extend the codec api to handle this data as well.",
        "label": 40
    },
    {
        "text": "code coverage reports hi all, we should be able to measure the code coverage of our unit testcases. i believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. furthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. it would be nice if we could add a page to the lucene website showing the report, generated by the nightly build. maybe you could add that to your preview page (lucene-707), grant? i attach a patch here that uses the tool emma to generate the code coverage reports. emma is a very nice open-source tool released under the cpl (same license as junit). the patch adds three targets to common-build.xml: emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath emma-instrument: instruments the compiled code generate-emma-report: generates an html code coverage report the following steps are neccessary in order to generate a code coverage report: add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/) execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes) execute ant target 'test' to run the unit tests execute ant target 'generate-emma-report' to view the emma report open build/test/emma/index.html",
        "label": 15
    },
    {
        "text": "nuke manual maintained forrest javadocs menu currently site.xml has every module and a link to its javadocs, its a sub-menu of the javadocs menu in forrest. this causes a few problems: it has to be manually maintained (i noticed in some releases like 3.5, several contribs were totally missing) since it contains sub-menus, there is no room to fit in a description of what these modules do. on the other hand, we actually have an index page, auto-generated from the build.xml's that links to each module's javadocs. it creates links based on the project name and description in the ant files. so i think we should just link to that instead.",
        "label": 53
    },
    {
        "text": "add  override annotations during removal of deprecated apis, mostly the problem was, to not only remove the method in the (abstract) base class (e.g. scorer.explain()), but also remove it in sub classes that override it. you can easily forget that (especially, if the method was not marked deprecated in the subclass). by adding @override annotations everywhere in lucene, such removals are simple, because the compiler throws out an error message in all subclasses which then no longer override the method. also it helps preventing the well-known traps like overriding hashcode() instead of hashcode(). the patch was generated automatically, and is rather large. should i apply it, or would it break too many patches (but i think, trunk has changed so much, that this is only a minimum of additional work to merge)?",
        "label": 53
    },
    {
        "text": "nativefslockfactory makelock islocked  does not work indexwriter.islocked() or indexreader.islocked() do not work with nativefslockfactory. the problem is, that the method nativefslock.islocked() just checks if the same lock instance was locked before (lock != null). if the lockfactory created a new lock instance, this always returns false, even if its locked.",
        "label": 53
    },
    {
        "text": "remove unnecessary null check in findsegmentsfile   cleanup findsegmentsfile accesses the member \"directory\" in line 579 while performing a null check in 592. the null check is unnecessary as if directory is null line 579 would throw a npe. i removed the null check and made the member \"directory\" final. in addition i added a null check in the constructor as if the value is null we should catch it asap.",
        "label": 53
    },
    {
        "text": "port to generics   test cases in contrib lucene-1257 in lucene 3.0 addressed porting to generics across public api-s . lucene-2065 addressed across src/test . this would be a placeholder jira for any remaining pending generic conversions across the code base. please keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket.",
        "label": 33
    },
    {
        "text": "reopen support for segmentreader reopen for segmentreader can be supported simply as the following: @override public synchronized indexreader reopen() throws corruptindexexception, ioexception { return reopensegment(this.si,false,readonly); } @override public synchronized indexreader reopen(boolean openreadonly) throws corruptindexexception, ioexception { return reopensegment(this.si,false,openreadonly); }",
        "label": 33
    },
    {
        "text": "add terms doccount spinoff from lucene-3290, where yonik mentioned: is there currently a way to get the number of documents that have a value in the field? then one could compute the average length of a (sparse) field via sumtotaltermfreq(field)/docswithfield(field) docswithfield(field) would be useful in other contexts that want to know how sparse a field is (automatically selecting faceting algorithms, etc). i think this is a useful stat to add, in case you have sparse fields for heuristics or scoring.",
        "label": 40
    },
    {
        "text": "frenchlightstemmer performs abusive compression of  arbitrary  repeated characters in long tokens frenchlightstemmer performs aggressive deletions on repeated character sequences, even on numbers. this might be unexpected during full text search.",
        "label": 47
    },
    {
        "text": "abstract spatial distance filtering process and supported field formats currently the second stage of the filtering process in the spatial contrib involves calculating the exact distance for the remaining documents, and filtering out those that fall out of the search radius. currently this is done through the 2 impls of distancefilter, latlngdistancefilter and geohashdistancefilter. the main difference between these 2 impls is the format of data they support, the former supporting lat/lngs being stored in 2 distinct fields, while the latter supports geohashed lat/lngs through the geohashutils. this difference should be abstracted out so that the distance filtering process is data format agnostic. the second issue is that the distance filtering algorithm can be considerably optimized by using multiple-threads. therefore it makes sense to have an abstraction of distancefilter which has different implementations, one being a multi-threaded implementation and the other being a blank implementation that can be used when no distance filtering is to occur.",
        "label": 7
    },
    {
        "text": "revise niofsdirectory and its usage due to nio limitations on thread interrupt i created this issue as a spin off from http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201001.mbox/%3cf18c9dde1001280051w4af2bc50u1cfd55f85e50914f@mail.gmail.com%3e we should decide what to do with niofsdirectory, if we want to keep it as the default on none-windows platforms and how we want to document this.",
        "label": 46
    },
    {
        "text": "add insertwithoverflow to priorityqueue this feature proposes to add an insertwithoverflow to priorityqueue so that callers can reuse the objects that are being dropped off the queue. also, it changes heap to protected for easier extensibility of pq",
        "label": 33
    },
    {
        "text": "fvh throws away some boosts the fvh's fieldquery throws away some boosts when flattening queries, including disjunctionmaxquery and booleanquery queries. fragments generated against queries containing boosted boolean queries don't end up sorted correctly.",
        "label": 1
    },
    {
        "text": "norm codec strategy in similarity the static span and resolution of the 8 bit norms codec might not fit with all applications. my use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?",
        "label": 33
    },
    {
        "text": "javadoc of tokenstream end  somehow confusing the javadocs of tokenstream.end() are somehow confusing, because they also refer to the old tokenstream api (\"after next() returned null\"). but one who implements his tokenstream with the old api cannot make use of the end() feature, as he would not use attributes and so cannot update the end offsets (he could, but then he should rewrite the whole tokenstream). to be conform to the old api, there must be an end(token) method, which we will not add. i would drop the old api from this docs.",
        "label": 53
    },
    {
        "text": "indexreader and friends should check ref count when incrementing indexreader and segmentcorereaders blindly increments it's refcount which could already be counted down to 0 which might allow an indexreader to \"rise from the dead\" and use an already closed scr instance. even if that is caught we should try best effort to raise ace asap.",
        "label": 46
    },
    {
        "text": "wikipediatokenizer needs a way of not tokenizing certain parts of the text it would be nice if the wikipediatokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them) thus, if we came across [[category:my big idea]] there would be a way of outputting, as a single token \"my big idea\". optionally, it would be good to output both \"my big idea\" and the individual tokens as well. i am not sure of how to do this in jflex, so any insight would be appreciated.",
        "label": 15
    },
    {
        "text": "while indexing turkish web pages   parse aborted  lexical error  occurs when i try to index turkish page if there is a turkish specific character in the html specific tag html parser gives \"parse aborted: lexical error.on ... line\" error. for this case \"<img src=\"../images/head.jpg\" width=570 height=47 border=0 alt=\"\u015f\">\" exception address \"\u015f\" character (which has 351 ascii value) as an error. or \u0131 character in title tag. <a title=\"(\u0131\u0131\u0131)\"> turkish character in the content do not create any problem.",
        "label": 40
    },
    {
        "text": "rename lucene solr dev jar files to  snapshot jar currently the lucene dev jar files end with '-dev.jar' this is all fine, but it makes people using maven jump through a few hoops to get the -snapshot naming convention required by maven. if we want to publish snapshot builds with hudson, we would need to either write some crazy scripts or run the build twice. i suggest we switch to -snapshot.jar. hopefully for the 3.x branch and for the /trunk (4.x) branch",
        "label": 42
    },
    {
        "text": " patch  lock framework   allows custom lock mechanism proposal: pluggable lock framework for lucene date: nov 2004 developer: jeff patterson (jeffatwebdoyen.com - http://www.webdoyen.com) ------ abstract: a framework to allow lucene users to override the default filesystem locking mechanism with a custom lock mechanism. a lucene user may develop a new class that extends org.apache.lucene.store.lock and implement bodies for the following methods: public boolean obtain() - to obtain custom lock public boolean islocked() - to detect custom lock public void release() - to release custom lock note: when implementing these methods, the developer should make sure to use the this.getlockname() method on the lock to identify which lock is being manipulated (see modified files below for more). after developed, the new class must be added to the classpath (along with any other supporting classes/libraries needed by the new class), and the lucene framework must be alerted of the new class by way of the \"org.apache.lucene.lockclass\" -d system property. example: java -dorg.apache.lucene.lockclass=foo.mycustomlocker lucenetest ------ modified files: the following files were modified to support this framework (diff files at end): org.apache.lucene.store.lock the member \"lockname\" and an accompanying protected getter and setter were added to this class to support naming the lock. this is transparent to the default lock mechanism and is only useful when writing a custom lock. org.apache.lucene.store.fsdirectory instead of instantiating a default lock, this class now checks to see if an overridden lock mechanism is provided, and if so asks the lockfactory (see below) to provide an overridden lock class. new files: the following files were added to support this framework: org.apache.lucene.store.lockfactory this class is used to reflect and instantiate by name the custom lock implementation. error handing should be modified in this class, but that would have required a more extensive code overhaul. the javadocs for the lockfactory contain a skeleton java file for a custom lock implementation. ------",
        "label": 55
    },
    {
        "text": "sorting does not work correcly on  string date  i am doing a simple query with a sort on a date field already put on a string format (yyyymmddhhmmss). i am presenting 10 results per pages. the results i have is not correctly sort. on a page \"4 october\" is before \"5 october\", on another page i got the same kind of error. these are logs of the sorting i get: 20051006102501 date: 06 oct. 2005 20051004130501 date: 04 oct. 2005 <-- problem! 20051005120200 date: 05 oct. 2005 20050928094805 date: 28 sept. 2005 20050928114901 date: 28 sept. 2005 20050928114901 date: 28 sept. 2005 20050928101901 date: 28 sept. 2005 20050928142601 date: 28 sept. 2005 20050928142601 date: 28 sept. 2005 20050929094100 date: 29 sept. 2005 <-- problem! 20050928132931 date: 28 sept. 2005 20050929090002 date: 29 sept. 2005 <-- problem! 20050927203403 date: 27 sept. 2005 20050927203402 date: 27 sept. 2005 20050927131901 date: 27 sept. 2005 20050927131901 date: 27 sept. 2005 20050927132501 date: 27 sept. 2005 20050927132500 date: 27 sept. 2005 20050927133101 date: 27 sept. 2005 20050927133100 date: 27 sept. 2005 20050926115001 date: 26 sept. 2005 20050927112301 date: 27 sept. 2005 20050926091200 date: 26 sept. 2005 <-- problem! 20050927085701 date: 27 sept. 2005 20050923111558 date: 23 sept. 2005 20050923103700 date: 23 sept. 2005 20050923144300 date: 23 sept. 2005 20050922102200 date: 22 sept. 2005 20050922082701 date: 22 sept. 2005 20050923085601 date: 23 sept. 2005 <-- problem! 20050815102204 date: 15 august 2005 20050815102201 date: 15 august 2005 20050816102200 date: 16 august 2005 <-- problem! 20050815110400 date: 15 august 2005 20050816124500 date: 16 august 2005 <-- problem! 20050812163334 date: 12 august 2005 20050812163331 date: 12 august 2005 20050812161840 date: 12 august 2005 20050812161935 date: 12 ao\u00fbt 2005 20050812113442 date: 12 ao\u00fbt 2005",
        "label": 55
    },
    {
        "text": "bufferedchecksumindexinput is not cloneable bufferedchecksumindexinput implements cloneable, yet its close method would return a shallow copy that still wraps the same indexinput and checksum. this is trappy, because reading on the clone would also read on the original instance and update the checksum. since checksum are not cloneable, i think checksumindexinput.clone should just throw an uoe.",
        "label": 1
    },
    {
        "text": "lucenetestcase afterclass does not print enough information if a temp test dir fails to delete i've hit an exception from ltc.afterclass when _testutil.rmdir failed (on write.lock, as if some test did not release resources). however, i had no idea which test caused that (i.e. opened the temp directory and did not release resources). i think we should do the following: track in ltc a map from dirname -> stacktraceelement in afterclass if _testutil.rmdir fails, print the ste of that particular dir, so we know where was this directory created from make tempdirs private and create accessor method, so that we control the inserts to this map (today the set is updated by ltc, _testutils and testbackwards !)",
        "label": 43
    },
    {
        "text": "datetools stringtodate  can cause lock contention under load load testing our application (the jira issue tracker) has shown that threads spend a lot of time blocked in datetools.stringtodate(). the stringtodate() method uses a singleton simpledateformat object to parse the dates. each call to simpledateformat.parse() is synchronized because simpledateformat is not thread safe.",
        "label": 53
    },
    {
        "text": "improvements to contrib benchmark for trec collections the benchmarking utilities for trec test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older trec collections. i have been doing some benchmarking work with lucene and have had to modify the package to support: older trec document formats, which the current parser fails on due to missing document headers. variations in query format - newlines after <title> tag causing the query parser to get confused. ability to detect and read in uncompressed text collections storage of document numbers by default without storing full text. i can submit a patch if there is interest, although i will probably want to write unit tests for the new functionality first.",
        "label": 12
    },
    {
        "text": "comparison operators   and   support as rangequery syntax in queryparser to offer better interoperability with other search engines and to provide an easier and more straight forward syntax, the operators >, >=, <, <= and = should be available to express an open range query. they should at least work for numeric queries. '=' can be made a synonym for ':'.",
        "label": 0
    },
    {
        "text": "add open ended range query syntax to queryparser the queryparser fails to generate open ended range queries. parsing e.g. \"date:[1990 to *]\" gives zero results, but constantrangequery(\"date\",\"1990\",null,true,true) does produce the expected results. \"date:[* to 1990]\" gives the same results as constantrangequery(\"date\",null,\"1990\",true,true).",
        "label": 33
    },
    {
        "text": "intermitted failure on docvalues branch i lately ran into two random failures on the csf branch that seem not to be related to docvalues but i can't reproduce them neither on docvalues branch nor on trunk. jerror message indexfiledeleter doesn't know about file _1e.tvx stacktrace junit.framework.assertionfailederror: indexfiledeleter doesn't know about file _1e.tvx  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:979)  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:917)  at org.apache.lucene.index.indexwriter.filesexist(indexwriter.java:3633)  at org.apache.lucene.index.indexwriter.startcommit(indexwriter.java:3699)  at org.apache.lucene.index.indexwriter.preparecommit(indexwriter.java:2407)  at org.apache.lucene.index.indexwriter.commitinternal(indexwriter.java:2478)  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2460)  at org.apache.lucene.index.indexwriter.commit(indexwriter.java:2444)  at org.apache.lucene.index.testindexwriterexceptions.testrandomexceptionsthreads(testindexwriterexceptions.java:213) standard output note: reproduce with: ant test -dtestcase=testindexwriterexceptions -dtestmethod=testrandomexceptionsthreads -dtests.seed=-6528669668419768890:4860241142852689334 -dtests.codec=randomperfield -dtests.multiplier=3 note: test params are: codec=preflex, locale=sv, timezone=atlantic/south_georgia standard error note: all tests run in this jvm: [testdemo, testtoken, testbinarydocument, testcodecs, testdirectoryreader, testindexinput, testindexwriterexceptions] and [junit] testsuite: org.apache.lucene.index.testindexreaderreopen     [junit] testcase: testthreadsafety(org.apache.lucene.index.testindexreaderreopen): caused an error     [junit] mockdirectorywrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}     [junit] java.lang.runtimeexception: mockdirectorywrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}     [junit]  at org.apache.lucene.store.mockdirectorywrapper.close(mockdirectorywrapper.java:387)     [junit]  at org.apache.lucene.index.testindexreaderreopen.testthreadsafety(testindexreaderreopen.java:859)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:979)     [junit]  at org.apache.lucene.util.lucenetestcase$lucenetestcaserunner.runchild(lucenetestcase.java:917)     [junit] caused by: java.lang.runtimeexception: unclosed indexinput     [junit]  at org.apache.lucene.store.mockdirectorywrapper.openinput(mockdirectorywrapper.java:342)     [junit]  at org.apache.lucene.store.directory.openinput(directory.java:122)     [junit]  at org.apache.lucene.index.codecs.standard.standardpostingsreader.<init>(standardpostingsreader.java:49)     [junit]  at org.apache.lucene.index.codecs.standard.standardcodec.fieldsproducer(standardcodec.java:87)     [junit]  at org.apache.lucene.index.perfieldcodecwrapper$fieldsreader.<init>(perfieldcodecwrapper.java:119)     [junit]  at org.apache.lucene.index.perfieldcodecwrapper.fieldsproducer(perfieldcodecwrapper.java:211)     [junit]  at org.apache.lucene.index.segmentreader$corereaders.<init>(segmentreader.java:137)     [junit]  at org.apache.lucene.index.segmentreader.get(segmentreader.java:532)     [junit]  at org.apache.lucene.index.segmentreader.get(segmentreader.java:509)     [junit]  at org.apache.lucene.index.directoryreader.<init>(directoryreader.java:238)     [junit]  at org.apache.lucene.index.directoryreader.doreopen(directoryreader.java:500)     [junit]  at org.apache.lucene.index.directoryreader.access$000(directoryreader.java:48)     [junit]  at org.apache.lucene.index.directoryreader$2.dobody(directoryreader.java:493)     [junit]  at org.apache.lucene.index.segmentinfos$findsegmentsfile.run(segmentinfos.java:623)     [junit]  at org.apache.lucene.index.directoryreader.doreopennowriter(directoryreader.java:488)     [junit]  at org.apache.lucene.index.directoryreader.doreopen(directoryreader.java:446)     [junit]  at org.apache.lucene.index.directoryreader.reopen(directoryreader.java:406)     [junit]  at org.apache.lucene.index.testindexreaderreopen$9.run(testindexreaderreopen.java:770)     [junit]  at org.apache.lucene.index.testindexreaderreopen$readerthread.run(testindexreaderreopen.java:897)     [junit]      [junit]      [junit] tests run: 17, failures: 0, errors: 1, time elapsed: 13.766 sec     [junit]      [junit] ------------- standard output ---------------     [junit] note: reproduce with: ant test -dtestcase=testindexreaderreopen -dtestmethod=testthreadsafety -dtests.seed=-5455993123574190959:-1935535300313439968 -dtests.codec=randomperfield -dtests.multiplier=3     [junit] note: test params are: codec=randomcodecprovider: {field5=mockvariableintblock(baseblocksize=29), id=standard, fielda=standard, field4=mockfixedintblock(blocksize=924), field3=standard, field2=simpletext, id2=standard, field6=mocksep, field1=pulsing(freqcutoff=8)}, locale=zh_cn, timezone=asia/hovd i haven't seen those before - let me know if you have!",
        "label": 46
    },
    {
        "text": "testindexwriterreader hang hung in jenkins. seed is d344294f98d3f637 (and the usual nightly flags, -dtests.nightly=true, -dtests.multiplier=3, -dtests.linedocsfile=huge) didn't try to reproduce yet.",
        "label": 46
    },
    {
        "text": "get rid of nonmatchingscorer from booleanscorer2 over in lucene-1614 mike has made a comment about removing nonmatchinscorer from bs2, and return null in booleanweight.scorer(). i've checked and this can be easily done, so i'm going to post a patch shortly. for reference: https://issues.apache.org/jira/browse/lucene-1614?focusedcommentid=12715064&page=com.atlassian.jira.plugin.system.issuetabpanels%3acomment-tabpanel#action_12715064. i've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9.",
        "label": 33
    },
    {
        "text": "benchmark tests always fail on windows because directory cannot be removed this seems to be a bug recently introduced. i have no idea what's wrong. attached is a log file, reproduces everytime.",
        "label": 43
    },
    {
        "text": "separate the classifiers to online and caching where possible the lucene classifier implementations are now near onlines if they get a near realtime reader. it is good for the users whoes have a continously changing dataset, but slow for not changing datasets. the idea is: what if we implement a cache and speed up the results where it is possible.",
        "label": 50
    }
]