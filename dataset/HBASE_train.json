[
    {
        "text": "also catch remoteexception in syncreplicationtestbase verifyreplicationrequestrejection  see the last few comments in hbase-22303. we do not have short circuit connection for async client so we need to deal with remoteexception. ",
        "label": 149
    },
    {
        "text": "incorrect error logging when a replication peer is removed  when a replication peer is removed (and all goes well), the following error is still logged: [error][14:14:21,504][ventthread] org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager - the queue we wanted to close is missing peer-state this is due to a watch being set on the peer-state node under the replication peer node in zookeeper, and the replicationsource#peerswatcher doesn't correctly discern between nodes when it gets nodedeleted messages. ",
        "label": 178
    },
    {
        "text": "use netty  pull in netty 4 and sort out the consequences. ",
        "label": 340
    },
    {
        "text": "hstorekey needs a reworking  hbase-832 had to make an ugly hack to address fact that rows that included the delimiter \u2013 or a byte < delimiter \u2013 messed up sorting in .meta. fix the ugly hack, and while we're at it, add a byte for flags to the hsk so can mark if key is a meta key, a delete key (cell/family/row), etc. ",
        "label": 314
    },
    {
        "text": "bulk load handling from secondary region replicas  we should be replaying the bulk load events from the primary region replica in the secondary region replica so that the bulk loaded files will be made visible in the secondaries. this will depend on hbase-11567 and hbase-11568 ",
        "label": 233
    },
    {
        "text": " hbase  when a scanner lease times out  throw a more  user friendly  exception  currently, if a client spends too much time between next() requests on a scanner (or between scanner creation and the first call to next), the region server will time out the scanner and close. the next call to next() will receive an unknownscannerexception because all knowledge of the scanner was wiped out on the server when the lease expired. the the client side scanner should keep a timer so it compute the elapsed time between next() calls so that if it receives an unknownscannerexception, it can determine if the scanner lease timed out and if so, throw a more \"user friendly\" exception such as leasetimedoutexception ",
        "label": 241
    },
    {
        "text": "null check in tablesnapshotinputformat tablesnapshotregionrecordreader initialize  is redundant  here is related code:     public void initialize(inputsplit split, taskattemptcontext context) throws ioexception,         interruptedexception {       configuration conf = context.getconfiguration(); ...       if (context != null) {         this.context = context;         getcounter = tablerecordreaderimpl.retrievegetcounterwithstringsparams(context);       } context was dereferenced first, leaving the null check ineffective. ",
        "label": 441
    },
    {
        "text": "cell iteration is broken  cell implements iterable<cell> but its iteration is broken since it will always go one past the edge and throw an arrayindexoutofboundsexception ",
        "label": 144
    },
    {
        "text": "bring back the seek read functionality to hbase  it was observed that the seek + read gives better performance than pread when performing relatively large scans(scan which cannot be cached). hence bringing it back to hbase. ",
        "label": 154
    },
    {
        "text": "cache index blocks and bloom blocks on write if cachecompactedblocksonwrite is enabled  the existing behaviour even even cacheonwrite is enabled is that we don cache the index or bloom blocks. now with hbase-23066 in place we also write blocks on compaction. so it may be better to cache the index/bloom blocks also if cacheonwrite is enabled?  fyi chenxu ",
        "label": 473
    },
    {
        "text": "zookeeperwrapper constants cleanup  a lot of the zookeeper constants in hconstants are only used in one place. we should remove them from there and directly put them in. ",
        "label": 342
    },
    {
        "text": "all branch unit tests pass  ",
        "label": 314
    },
    {
        "text": "written data can not be read out because memstore timerangetracker might be updated concurrently  in our test environment, we find written data can't be read out occasionally. after debugging, we find that maximumtimestamp/minimumtimestamp of memstore#timerangetracker might decrease/increase when memstore#timerangetracker is updated concurrently, which might make the memstore/storefile to be filtered incorrectly when reading data out. let's see how the concurrent updating of timerangetracker#maximumtimestamp cause this problem.   imagining there are two threads t1 and t2 putting two keyvalues kv1 and kv2. kv1 and kv2 belong to the same store(so belong to the same region), but contain different rowkeys. consequently, kv1 and kv2 could be updated concurrently. when we see the implementation of hregionserver#multi, kv1 and kv2 will be add to memstore by hregion#applyfamilymaptomemstore in hregion#dominibatchmutation. then, memstore#internaladd will be invoked and memstore#timerangetracker will be updated by timerangetracker#includetimestamp as follows:   private void includetimestamp(final long timestamp) {      ...     else if (maximumtimestamp < timestamp) {       maximumtimestamp = timestamp;     }     return;   } imagining the current maximumtimestamp of timerangetracker is t0 before includetimestamp(...) invoked, kv1.timestamp=t1, kv2.timestamp=t2, t1 and t2 are both set by user(then, user knows the timestamps of kv1 and kv2), and t1 > t2 > t0. t1 and t2 will be executed concurrently, therefore, the two threads might both find the current maximumtimestamp is less than the timestamp of its kv. after that, t1 and t2 will both set maximumtimestamp to timestamp of its kv. if t1 set maximumtimestamp before t2 doing that, the maximumtimestamp will be set to t2. then, before any new update with bigger timestamp has been applied to the memstore, if we try to read out kv1 by htable#get and set the timestamp of 'get' to t1, the storescanner will decide whether the memstorescanner(imagining kv1 has not been flushed) should be selected as candidate scanner by memstorescanner#shouldusescanner. then, the memstore won't be selected in memstorescanner#shouldusescanner because maximumtimestamp of the memstore has been set to t2 (t2 < t1). consequently, the written kv1 can't be read out and kv1 is lost from user's perspective.  if the above analysis is right, after maximumtimestamp of memstore#timerangetracker has been set to t2, user will experience data lass in the following situations:  1. before any new write with kv.timestamp > t1 has been add to the memstore, read request of kv1 with timestamp=t1 can not read kv1 out.  2. before any new write with kv.timestamp > t1 has been add to the memstore, if a flush happened, the data of memstore will be flushed to storefile with storefile#maximumtimestamp set to t2. after that, any read request with timestamp=t1 can not read kv1 before next compaction(actually, kv1.timestamp might not be included in timerange of the storefile even after compaction).  the second situation is much more serious because the incorrect timerange of memstore has been persisted to the file.   similarly, the concurrent update of timerangetracker#minimumtimestamp may also cause this problem.  as a simple way to fix the problem, we could add synchronized to timerangetracker#includetimestamp so that this method won't be invoked concurrently. ",
        "label": 238
    },
    {
        "text": "the context classloader isn't set while calling into a coprocessor  whenever one of the methods of a coprocessor is invoked, the context classloader isn't set to be the coprocessorclassloader. it's only set properly when calling the coprocessor's start method. this means that if the coprocessor code attempts to load classes using the context classloader, it will fail to find the classes it's looking for. ",
        "label": 38
    },
    {
        "text": "update javadoc on website  javadoc for 2.0 is still on 2.0.5. since the 2.0 branch is eol'd the up-to-date javadoc should be hosted here: https://hbase.apache.org/2.0/apidocs/index.html ",
        "label": 352
    },
    {
        "text": "checking restoredir in restoresnapshothelper  the restoredir shouldn't be a sub directory of rootdir. the code check it with a prefix check \"restoredir.touri().getpath().startswith(rootdir.touri().getpath())\". but it goes error in some reasonable cases. eg: rootdir = hdfs://user/hbase restoredir = hdfs://user/hbase_restore. so i think it's more reasonable to chang the code to \"restoredir.touri().getpath().startswith(rootdir.touri().getpath() + \"/\" )\". ",
        "label": 470
    },
    {
        "text": "change scanquerymatcher to use cells instead of keyvalue   ",
        "label": 544
    },
    {
        "text": "support for binary data  keys values  in ui and on shell  a few fellas are using binary keys. the ui and shell show rubbish when data is binary. if binary keys, admin facility in shell and ui are unavailable. ",
        "label": 285
    },
    {
        "text": "enable table throws npe and leaves trash in zk in competition with delete table  2012-08-15 19:23:36,178 debug org.apache.hadoop.hbase.client.clientscanner: creating scanner over .meta. starting at key 'test,,'  2012-08-15 19:23:36,178 debug org.apache.hadoop.hbase.client.clientscanner: advancing internal scanner to startkey at 'test,,'  2012-08-15 19:24:09,180 debug org.apache.hadoop.hbase.client.clientscanner: creating scanner over .meta. starting at key ''  2012-08-15 19:24:09,180 debug org.apache.hadoop.hbase.client.clientscanner: advancing internal scanner to startkey at ''  2012-08-15 19:24:09,183 debug org.apache.hadoop.hbase.client.clientscanner: finished with scanning at {name => '.meta.,,1', startkey => '', endkey => '', encoded => 1028785192,}  2012-08-15 19:24:09,183 debug org.apache.hadoop.hbase.master.catalogjanitor: scanned 2 catalog row(s) and gc'd 0 unreferenced parent region(s)  2012-08-15 19:25:12,260 debug org.apache.hadoop.hbase.master.handler.deletetablehandler: deleting region test,,1345029764571.d1e24b251ca6286c840a9a5f571b7db1. from meta and fs  2012-08-15 19:25:12,263 info org.apache.hadoop.hbase.catalog.metaeditor: deleted region test,,1345029764571.d1e24b251ca6286c840a9a5f571b7db1. from meta  2012-08-15 19:25:12,265 info org.apache.hadoop.hbase.master.handler.enabletablehandler: attemping to enable the table test  2012-08-15 19:25:12,265 warn org.apache.hadoop.hbase.zookeeper.zktable: moving table test state to enabling but was not first in disabled state: null  2012-08-15 19:25:12,267 debug org.apache.hadoop.hbase.client.clientscanner: creating scanner over .meta. starting at key 'test,,'  2012-08-15 19:25:12,267 debug org.apache.hadoop.hbase.client.clientscanner: advancing internal scanner to startkey at 'test,,'  2012-08-15 19:25:12,270 debug org.apache.hadoop.hbase.client.clientscanner: finished with scanning at {name => '.meta.,,1', startkey => '', endkey => '', encoded => 1028785192,} 2012-08-15 19:25:12,270 error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event c_m_enable_table  java.lang.nullpointerexception  at org.apache.hadoop.hbase.master.handler.enabletablehandler.handleenabletable(enabletablehandler.java:116)  at org.apache.hadoop.hbase.master.handler.enabletablehandler.process(enabletablehandler.java:97)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:169)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) table is disabled now, then we enable and delete the table at the same time. since the thread num of master_table_operations is 1 by default. the two operations are serial in master.before deletetable deletes all the regions in meta, createtablehandler ships the check of tableexists,then it will block until deletetable finishs, then createtablehandler will set zk enabling, and find no data in meta:  regionsinmeta = metareader.gettableregions(this.ct, tablename, true);  int countofregionsintable = regionsinmeta.size(); npe will be throwed here. and we could not create the same table anymore. ",
        "label": 518
    },
    {
        "text": "add hbase spark to hbase assembly  hbase-spark currently is missing from hbase assembly.  we should add it. ",
        "label": 234
    },
    {
        "text": "better zk error when failed connect  from a discussion with ken weiner up on hbase-user: index: src/java/org/apache/hadoop/hbase/zookeeper/zookeeperwrapper.java  ===================================================================  \u2014 src/java/org/apache/hadoop/hbase/zookeeper/zookeeperwrapper.java (revision 797665)  +++ src/java/org/apache/hadoop/hbase/zookeeper/zookeeperwrapper.java (working copy)  @@ -348,9 +348,11 @@  } catch (keeperexception.nonodeexception e) { return ensureparentexists(znode) && ensureexists(znode); } catch (keeperexception e) { - log.warn(\"failed to create \" + znode + \":\", e); + log.warn(\"failed to create \" + znode + + \" -- check quorum servers, currenty=\" + this.quorumservers, e); } catch (interruptedexception e) { - log.warn(\"failed to create \" + znode + \":\", e); + log.warn(\"failed to create \" + znode + + \" -- check quorum servers, currenty=\" + this.quorumservers, e); } return false;  } looks like this in logs: 2009-07-24 14:48:40,923 warn org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: failed to create /hbase \u2013 check quorum servers, currenty=localhost:2181 org.apache.zookeeper.keeperexception$connectionlossexception: keepererrorcode = connectionloss for /hbase ",
        "label": 314
    },
    {
        "text": "clean up using directives in cc files   there's a ton of files that just barf out all of folly, wangle, and hbase into the global namespace. we should use the using directive better than that when possible. ",
        "label": 401
    },
    {
        "text": "shell tools   close region does not work for regions that did not deploy properly on startup  2008-12-28 16:14:32,292 info org.apache.hadoop.hbase.master.hmaster: marking result_domain,,1230404458436 as closed on 10.30.94.48:60020; cleaning server + startcode; master will tell regionserver to close region on next heartbeat  2008-12-28 16:14:32,293 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 4 on 60000, call modifytable([b@64c6e290, 4, [lorg.apache.hadoop.io.writable;@3d448fa0) from 10.30.94.51:55082: error: java.io.ioexception: java.lang.illegalstateexception: cannot transition to closing from any other state. region: result_domain,,1230404458436  java.io.ioexception: java.lang.illegalstateexception: cannot transition to closing from any other state. region: result_domain,,1230404458436  at org.apache.hadoop.hbase.master.regionmanager.setclosing(regionmanager.java:797)  at org.apache.hadoop.hbase.master.hmaster.modifytable(hmaster.java:836)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:894) clearing 'info:server' and 'info:serverstartcode' for the affected region has no effect either. the region in question did not properly deploy: 2008-12-28 15:58:44,857 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: result_domain,,1230404458436: java.io.ioexception: could not obtain block: blk_-1589286342406311350_4245713 file=/data/hbase/result_domain/43721464/errorresult/mapfiles/284087383342325737/index  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.choosedatanode(dfsclient.java:1472)  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.blockseekto(dfsclient.java:1322)  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.read(dfsclient.java:1427)  at java.io.datainputstream.readfully(datainputstream.java:178)  at java.io.datainputstream.readfully(datainputstream.java:152)  at org.apache.hadoop.io.sequencefile$reader.init(sequencefile.java:1453)  at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1431)  at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1420)  at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1415)  at org.apache.hadoop.io.mapfile$reader.open(mapfile.java:292)  at org.apache.hadoop.hbase.io.hbasemapfile$hbasereader.<init>(hbasemapfile.java:81)  at org.apache.hadoop.hbase.io.bloomfiltermapfile$reader.<init>(bloomfiltermapfile.java:66)  at org.apache.hadoop.hbase.regionserver.hstorefile.getreader(hstorefile.java:443)  at org.apache.hadoop.hbase.regionserver.hstore.setupreaders(hstore.java:254)  at org.apache.hadoop.hbase.regionserver.hstore.<init>(hstore.java:239)  at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:1624)  at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:270)  at org.apache.hadoop.hbase.regionserver.hregionserver.instantiateregion(hregionserver.java:1360)  at org.apache.hadoop.hbase.regionserver.hregionserver.openregion(hregionserver.java:1331)  at org.apache.hadoop.hbase.regionserver.hregionserver$worker.run(hregionserver.java:1247)  at java.lang.thread.run(thread.java:619)  from 10.30.94.34:60020 i am trying to force a redeployment in an attempt to manually recover the region. ",
        "label": 314
    },
    {
        "text": "backport hbase 'testwallockup is flakey' to branch  i'm hitting this one regularly and there's a fix for branch-1.2. let's bring it back. ",
        "label": 314
    },
    {
        "text": "log ioexception when unable to determine the size of committed file  during troubleshooting of slow response, i saw the following in region server log: 2017-10-26 14:03:53,080 warn org.apache.hadoop.hbase.regionserver.hregion: failed to find the size of hfile hdfs://beta/hbase/data/default/beta_b_history/e514111fae9d7ffc38ed48ad72fa197f/d/04d7c9fce73d4197be114448b1eb295a_seqid_3766_ here is related code:             } catch (ioexception e) {               log.warn(\"failed to find the size of hfile \" + commitedstorefile); the exception should also be logged to facilitate debugging. ",
        "label": 188
    },
    {
        "text": "port hbase tier based compaction from fb to trunk  with changes   see hbase-6371 for details. ",
        "label": 450
    },
    {
        "text": "add support for specifying filters in scan  in the implementation of stateless scanner from hbase-9343, the support for specifying filters is missing. this jira aims to implement support for filter specification. ",
        "label": 472
    },
    {
        "text": "separate out jars related to jdk into a folder in  lib  update: separate out the the jars related to jdk 11 and add control their addition to the classpath using an environment variable or auto-detection of the jdk version installed. old: this is in continuation with hbase-22249. when compiled with jdk 8 and run on jdk 11, the master branch throws the following exception during an attempt to start the hbase rest server: exception in thread \"main\" java.lang.noclassdeffounderror: javax/annotation/priority at org.glassfish.jersey.model.internal.componentbag.modelfor(componentbag.java:483) at org.glassfish.jersey.model.internal.componentbag.access$100(componentbag.java:89) at org.glassfish.jersey.model.internal.componentbag$5.call(componentbag.java:408) at org.glassfish.jersey.model.internal.componentbag$5.call(componentbag.java:398) at org.glassfish.jersey.internal.errors.process(errors.java:315) at org.glassfish.jersey.internal.errors.process(errors.java:297) at org.glassfish.jersey.internal.errors.process(errors.java:228) at org.glassfish.jersey.model.internal.componentbag.registermodel(componentbag.java:398) at org.glassfish.jersey.model.internal.componentbag.register(componentbag.java:235) at org.glassfish.jersey.model.internal.commonconfig.register(commonconfig.java:420) at org.glassfish.jersey.server.resourceconfig.register(resourceconfig.java:425) at org.apache.hadoop.hbase.rest.restserver.run(restserver.java:245) at org.apache.hadoop.hbase.rest.restserver.main(restserver.java:421) ",
        "label": 391
    },
    {
        "text": "create integration perf tests for stripe compactions  while writing tests i seem to be finding more errors with edge cases unrelated to what test actually tests compared to what is expected.  integration test will be needed... probably good for perf/compare too. ",
        "label": 406
    },
    {
        "text": "ssh and disabletablehandler happening together does not clear the znode of the region and rit map   a possible exception: if the related regionserver was just killed(but hmaster has not perceived that), then we will get a local exception \"connection reset by peer\". if this region belongs to a disabling table. what will happen? servershutdownhandler will remove this region from am#regions. so this region is still existing in rit. timeoutmonitor will take care of it after it got timeout. then invoke unassign again. since this region has been removed from am#regions, it will return directly due to the below code:     synchronized (this.regions) {       // check if this region is currently assigned       if (!regions.containskey(region)) {         log.debug(\"attempted to unassign region \" +           region.getregionnameasstring() + \" but it is not \" +           \"currently assigned anywhere\");         return;       }     } then it leads to an end-less loop. ",
        "label": 543
    },
    {
        "text": "offline snapshots in hbase  continuation of hbase-50 for the current trunk. since the implementation has drastically changed, opening as a new ticket. ",
        "label": 248
    },
    {
        "text": "htabledescriptor changes for region replicas  region replication should be configurable per table. thus we can add an attribute to indicate the desired region replication count to htabledescriptor. ",
        "label": 139
    },
    {
        "text": "introduce more examples to show how to intercept normal region operations  ",
        "label": 252
    },
    {
        "text": "expand unit testing to cover mockito and mrunit and give more examples  the section at http://hbase.apache.org/book.html#mockito only has a todo where examples should go. ",
        "label": 330
    },
    {
        "text": "thrift getrow does not support specifying columns  thrift interface has a getrow function but it does not support asking for specific columns. ",
        "label": 144
    },
    {
        "text": "release hbase alpha  theme  scrubbed api   from the dev mail thread: \"moving 2.0 forward\", the theme is solidifying api. i listed a bunch of api jiras to address. mike drob nicely tagged them all w/ the 2.0.0-alpha-3 fix version. this issue is for pushing out alpha-3. ",
        "label": 314
    },
    {
        "text": "  hregion computehdfsblocksdistribution does not account for links and reference files   ",
        "label": 286
    },
    {
        "text": "intermittent test failure in testhstore testrundoublememstorecompactors  here was one of the test failures: https://builds.apache.org/job/precommit-hbase-build/9812/testreport/junit/org.apache.hadoop.hbase.regionserver/testhstore/testrundoublememstorecompactors/ [error] org.apache.hadoop.hbase.regionserver.testhstore.testrundoublememstorecompactors(org.apache.hadoop.hbase.regionserver.testhstore) [error]   run 1: testhstore.testrundoublememstorecompactors:1500 expected:<2> but was:<3> [error]   run 2: testhstore.testrundoublememstorecompactors:1481 expected:<1> but was:<4> [error]   run 3: testhstore.testrundoublememstorecompactors:1481 expected:<1> but was:<5> from the counts for second and third runs, we know that runner_count was not cleared in between the reruns, leading to failure at the 1st assertion. ",
        "label": 441
    },
    {
        "text": "create test for cell level acls involving user group  currently we have testcellacls and testcellaclwithmultipleversions which exercise cell level acls for users. however, test for cell level acls involving user group is missing. this issue is to add such test(s) ",
        "label": 53
    },
    {
        "text": "if split happens while regionserver is going down  we can stick open   j-d found this one testing. he found that if a split comes in during shutdown of a regionserver, then the regionserver can stick open... and won't go down. we fixed a similar problem in the past where if balancer cut in during shutdown and assigned a regionserver an region during shutdown, we'd open it and it'd cause us again to stick open. we fixed that by introducing the 'closing' state. fix for the issue j-d found is to do closing check when onlining daughters. ",
        "label": 314
    },
    {
        "text": "drop root and instead store meta location s  directly in zookeeper  rather than storing the root region location in zookeeper, going to root, and reading the meta location, we should just store the meta location directly in zookeeper. the purpose of the root region from the bigtable paper was to support multiple meta regions. currently, we explicitly only support a single meta region, so the translation from our current code of a single root location to a single meta location will be very simple. long-term, it seems reasonable that we could store several meta region locations in zk. there's been some discussion in hbase-1755 about actually moving meta into zk, but i think this jira is a good step towards taking some of the complexity out of how we have to deal with catalog tables everywhere. as-is, a new client already requires zk to get the root location, so this would not change those requirements in any way. the primary motivation for this is to simplify things like catalogtracker. the way we can handle root in that class is really simple but the tracking of meta is difficulty and a bit hacky. this hack on tracking of the meta location is what caused one of the bugs over in hbase-3159. ",
        "label": 229
    },
    {
        "text": "table 'does not exist' when it does  this one i've seen a few times. in hql, i do show tables and it shows my table. i then try to do a select against the table and hql reports table does not exist. digging, whats happening is that the getclosest facility is failing to find the first table region in the .meta. table. i hacked up a region reading tool \u2013 attached (for 0.1 branch) \u2013 and tried it against but a copy and the actual instance of the region and it could do the getclosest fine. i'm pretty sure i restarted the hrs and when it came up again, the master had given it again the .meta. and again was failing to find the first region in the table (looked around in server logs and it seemed 'healthy'). ",
        "label": 86
    },
    {
        "text": "open daughters immediately on parent's regionserver  let the regionserver just open regions immediately on the parent's regionserver. this way splits will be faster still. if cluster is out of balance, the balancer will step in and correct (new balancer will do it in a 'smart' way). other advantage is that no need of master involvement redeploying the regions. ",
        "label": 314
    },
    {
        "text": "add fifo compaction section to hbase book  hbase-14468 introduced new compaction policy. book needs to be updated. ",
        "label": 527
    },
    {
        "text": "use special pause for callqueuetoobigexception  ",
        "label": 149
    },
    {
        "text": "make logger instance modifiers consistent  we have some instances of logger that are missing one of being private, static, and final. ex from healthchecker.java, missing final     private static log log = logfactory.getlog(healthchecker.class); clean up where possible by making private static final if we can't, add a non-javadoc note about why one way to look for problematic instances is to grep for initial assignment for the commonly used log member, e.g. missing final: grep -r \"log =\" * | grep -v \"final\" missing static: grep -r \"log =\" * | grep -v \"static\" missing private: grep -r \"log =\" * | grep -v \"private\" ",
        "label": 460
    },
    {
        "text": " fb  add an option to blacklist a region server  add an api to blacklist a region server. the master should not assign any new regions to the blacklisted region server. this will help in rolling restart of hbase, where one can blacklist a regionserver, drain all the regions(which should be fast), restart the regionserver, assign the regions backs and then remove the region server from the blacklist ",
        "label": 378
    },
    {
        "text": "testmasterobserver testregiontransitionoperations occasionally fails  looks this logs:  https://builds.apache.org/job/hbase-trunk-security/ws/trunk/target/surefire-reports/ it seems that we should wait region is added to online region set. i made a patch, please review. ",
        "label": 529
    },
    {
        "text": "when a new master comes up  regionservers should continue with their region assignments from the last master  after hbase-1205, we can now handle a master going down and coming up somewhere else. when this happens, the new master will scan everything and reassign all the regions, which is not ideal. instead of doing that, we should keep the region assignments from the last master. ",
        "label": 229
    },
    {
        "text": "document master failover on the web site  we need to document how master failover works from a user perspective and how zk plays its role. we should also send a warning when hbase.master is configured to say it's obsolete and refer to that documentation. getting started should also point to that doc. ",
        "label": 229
    },
    {
        "text": "hrs unable to contact master  hrs unable to contact master for initialization after expiration from zk. master thinks hrs is still up whereas hrs went down and now cannot restart. the rs logs have a flurry of the following warning messages: 2009-07-08 12:53:19,547 warn org.apache.hadoop.hbase.regionserver.hregionserver: unable to get master for initialization more logs from the rs and the master attached. ",
        "label": 342
    },
    {
        "text": "multiget doesn't fully work  ",
        "label": 406
    },
    {
        "text": "keycomparator comparewithoutrow can be wrong when families have the same prefix  as reported by desert rose on irc and on the ml, result has a weird behavior when some families share the same prefix. he posted a link to his code to show how it fails, http://pastebin.com/7tba1xgh basically keycomparator.comparewithoutrow doesn't differentiate families and qualifiers so \"f:a\" is said to be bigger than \"f1:\", which is false. then what happens is that the kvs are returned in the right order from the rs but then doing result.binarysearch it uses keycomparator.comparewithoutrow which has a different sorting so the end result is undetermined. i added some debug and i can see that the data is returned in the right order but arrays.binarysearch returned the wrong kv, which is then verified agains the passed family and qualifier which fails so null is returned. i don't know how frequent it is for users to have families with the same prefix, but those that do have that and that use those families at the same time will have big correctness issues. this is why i mark this as a blocker. ",
        "label": 240
    },
    {
        "text": "deadlock in hmaster on masterandzklock in hconnectionmanager  on one of our clusters we got a deadlock in hmaster.  in a nutshell deadlock caused by using one hconnectionmanager for serving client-like calls and calls from hmaster rpc handlers. hbaseadmin uses hconnectionmanager which takes a lock masterandzklock.  on the other side of this game sits tablesnamespacemanager (tnm). this class uses hconnectionmanager too (in my case for getting list of available namespaces).   problem is that hmaster class uses tnm for serving rpc requests.  if we look at tnm more closely, we can see, that this class is totally synchronised. thats gives us a problem. webinterface calls request via hconnectionmanager and locks masterandzklock.  connection is blocking, so rpcclient will spin, awaiting for reply (while holding lock).  that how it looks like in thread dump:    java.lang.thread.state: timed_waiting (on object monitor) at java.lang.object.wait(native method) - waiting on <0x00000000c8905430> (a org.apache.hadoop.hbase.ipc.rpcclient$call) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1435) - locked <0x00000000c8905430> (a org.apache.hadoop.hbase.ipc.rpcclient$call) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1653) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1711) at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$blockingstub.ismasterrunning(masterprotos.java:40216) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$masterservicestate.ismasterrunning(hconnectionmanager.java:1467) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.iskeepalivemasterconnectedandrunning(hconnectionmanager.java:2093) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getkeepalivemasterservice(hconnectionmanager.java:1819) - locked <0x00000000d15dc668> (a java.lang.object) at org.apache.hadoop.hbase.client.hbaseadmin$mastercallable.prepare(hbaseadmin.java:3187) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:119) - locked <0x00000000cd0c1238> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:96) - locked <0x00000000cd0c1238> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:3214) at org.apache.hadoop.hbase.client.hbaseadmin.listtabledescriptorsbynamespace(hbaseadmin.java:2265) some other client call any hmaster rpc, and it calls tablesnamespacemanager methods, which in turn will block on hconnectionmanager global lock masterandzklock.  that how it looks like:   java.lang.thread.state: blocked (on object monitor) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getkeepalivezookeeperwatcher(hconnectionmanager.java:1699) - waiting to lock <0x00000000d15dc668> (a java.lang.object) at org.apache.hadoop.hbase.client.zookeeperregistry.istableonlinestate(zookeeperregistry.java:100) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.istabledisabled(hconnectionmanager.java:874) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.relocateregion(hconnectionmanager.java:1027) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getregionlocation(hconnectionmanager.java:852) at org.apache.hadoop.hbase.client.regionservercallable.prepare(regionservercallable.java:72) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:119) - locked <0x00000000cd0ef108> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.htable.getroworbefore(htable.java:705) at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:144) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.prefetchregioncache(hconnectionmanager.java:1102) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregioninmeta(hconnectionmanager.java:1162) - locked <0x00000000d1b49fd8> (a java.lang.object) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:1054) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:1011) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getregionlocation(hconnectionmanager.java:852) at org.apache.hadoop.hbase.client.regionservercallable.prepare(regionservercallable.java:72) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:119) - locked <0x00000000cd0ef248> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.htable.get(htable.java:756) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:134) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:118) - locked <0x00000000d189da20> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3113) at org.apache.hadoop.hbase.master.hmaster.listtabledescriptorsbynamespace(hmaster.java:3133) at org.apache.hadoop.hbase.master.hmaster.listtabledescriptorsbynamespace(hmaster.java:3034) at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$2.callblockingmethod(masterprotos.java:38261) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2175) at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1879) and finally original handler, which should serve request from webgui can be blocked on tnm methods effectively forming dead lock. ",
        "label": 441
    },
    {
        "text": "allow columnprefixfilter to support multiple prefixes  when having a lot of columns grouped by name i've found that it would be very useful to be able to scan them using multiple prefixes, allowing to fetch specific groups in one scan, without fetching the entire row. this is impossible to achieve using a filterlist, so i've added such support to the existing colmnprefixfilter while keeping backward compatibility.  the attached patch is based on 0.90.4, i noticed that the 0.92 branch has a new method to support instantiating filters using thrift. i'm not sure how the serialization works there so i didn't implement that, but the rest of my code should work in 0.92 as well. ",
        "label": 157
    },
    {
        "text": "the forkjoinpool in cleanerchore will spawn thousands of threads in our cluster with thousands table  the thousands of spawned threads make the safepoint cost 80+s in our master jvm processs. 2019-08-15,19:35:35,861 info [main-sendthread(zjy-hadoop-prc-zk02.bj:11000)] org.apache.zookeeper.clientcnxn: client session timed out, have not heard from server in 82260ms for sessionid 0x1691332e2d3aae5, closing socket connection and at tempting reconnect the stdout from jvm (can see from here there're 9126 threads & sync cost 80+s) vmop                    [threads: total initially_running wait_to_block]    [time: spin block sync cleanup vmop] page_trap_count 32358.859: forceasyncsafepoint              [    9126         67            474    ]      [     1    28 86596    87   101    ]  0 also we got the jstack: $ cat 31162.stack.1  | grep 'forkjoinpool-1-worker' | wc -l 8648 it's a dangerous bug, make it as blocker. ",
        "label": 514
    },
    {
        "text": "client doesn't recover from a stalled region server  got this testing the 0.95.2 rc. i killed -stop a region server and let it stay like that while running pe. the clients didn't find the new region locations and in the jstack were stuck doing rpc. eventually i killed -cont and the client printed these: exception in thread \"testclient-6\" java.lang.runtimeexception: org.apache.hadoop.hbase.client.retriesexhaustedwithdetailsexception: failed 128 actions: ioexception: 90 times, sockettimeoutexception: 38 times, ",
        "label": 340
    },
    {
        "text": "rs going down with npe when splitting a region with compaction disabled in branch  trying to backport hbase-22096 to brach-1, i faced the issue where a rs goes down with npe when splitting a region with compaction disabled. the steps to reproduce this issue are as follows: compaction_switch false create \"test\", \"cf\" (0...2000).each{|i| put \"test\", \"row#{i}\", \"cf:col\", \"val#{i}\"} split \"test\" looking at the regionserver log, i saw the following log: 2019-12-03 22:25:38,611 info  [rs:0;10.0.1.11:53504-splits-0] regionserver.splitrequest: running rollback/cleanup of failed split of test,,1575379535506.50e322ec68162025e17cddffdc2fb17e.; null java.lang.nullpointerexception at org.apache.hadoop.hbase.regionserver.hstore.cancelrequestedcompaction(hstore.java:1834) at org.apache.hadoop.hbase.regionserver.compactsplitthread$rejection.rejectedexecution(compactsplitthread.java:656) at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:830) at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1379) at org.apache.hadoop.hbase.regionserver.compactsplitthread.requestcompactioninternal(compactsplitthread.java:401) at org.apache.hadoop.hbase.regionserver.compactsplitthread.requestsystemcompaction(compactsplitthread.java:348) at org.apache.hadoop.hbase.regionserver.hregionserver.postopendeploytasks(hregionserver.java:2111) at org.apache.hadoop.hbase.regionserver.hregionserver.postopendeploytasks(hregionserver.java:2097) at org.apache.hadoop.hbase.regionserver.splittransactionimpl.opendaughters(splittransactionimpl.java:478) at org.apache.hadoop.hbase.regionserver.splittransactionimpl.stepsafterponr(splittransactionimpl.java:549) at org.apache.hadoop.hbase.regionserver.splittransactionimpl.execute(splittransactionimpl.java:532) at org.apache.hadoop.hbase.regionserver.splitrequest.dosplitting(splitrequest.java:82) at org.apache.hadoop.hbase.regionserver.splitrequest.run(splitrequest.java:153) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at java.lang.thread.run(thread.java:748) 2019-12-03 22:25:38,613 fatal [rs:0;10.0.1.11:53504-splits-0] regionserver.hregionserver: aborting region server 10.0.1.11,53504,1575379011279: abort; we got an error after point-of-no-return ",
        "label": 455
    },
    {
        "text": "hbaseobjectwritable code is a byte  we will eventually run out of codes  there are about 90 classes/codes in hbaseobjectwritable currently and byte.max_value is 127. in addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94. ",
        "label": 544
    },
    {
        "text": "hcm istableenabled doesn't really tell if it is  or not  in current trunk, if i load a table of 8m rows and then try and delete it, the disable returns saying the table was successfully deleted but when i then try to drop the table, it says table not disabled. i run the disable/drop cycle a few more times and still fails. eventually, if i wait long enough, it succeeds. maybe the table drop should just block if table is seen to have disabled regions in it. as is, its a little disorientating the way it works. could lead admins to distrust status messages emitted. ",
        "label": 229
    },
    {
        "text": "fuzzyrowfilter omits certain rows when multiple fuzzy keys exist  https://issues.apache.org/jira/browse/hbase-13761 introduced a rowtracker in fuzzyrowfilter to avoid performing getnextforfuzzyrule() for each fuzzy key on each getnextcellhint() by maintaining a list of possible row matches for each fuzzy key. the implementation assumes that the prepared rows will be matched one by one, so it removes the first row in the list as soon as it is used. however, this approach may lead to omitting rows in some cases: consider a case where we have two fuzzy keys:  1?1  2?2 and the data is like:  000  111  112  121  122  211  212 when the first row 000 fails to match, rowtracker will update possible row matches with cell 000 and fuzzy keys 1?1,2?2. this will populate rowtracker with 101 and 202. then 101 is popped out of rowtracker, hint the scanner to go to row 101. the scanner will get 111 and find it is a match, and continued to find that 112 is not a match, getnextcellhint will be called again. then comes the bug: row 101 has been removed out of rowtracker, so rowtracker will jump to 202. as you see row 121 will be omitted, but it is actually a match for fuzzy key 1?1. i will illustrate the bug by adding a new test case in testfuzzyrowfilterendtoend. also i will provide the bug fix in my patch. the idea of the new solution is to maintain a priority queue for all the possible match rows for each fuzzy key, and whenever getnextcellhint is called, the elements in the queue that are smaller than the parameter currentcell will be updated(and re-insert into the queue). the head of queue will always be the \"next cell hint\". ",
        "label": 202
    },
    {
        "text": "purge use of jmock and remove as dependency  jmock is a dependency used by one test only, testbulkload. it looks like you can do anything in mockito that can be done in jmock. lets purge it. ",
        "label": 177
    },
    {
        "text": "javadoc warnings from 'javadoc' target  some javadoc warnings:  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/indexconfiguration.java:149: warning - @return tag has no arguments.  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/indexconfiguration.java:149: warning - tag @see: reference not found: use #isanalyze(string) for replacement.  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/mapreduce/indexconfiguration.java:159: warning - tag @see: reference not found: use #setanalyze(string, boolean) for replacement.  [javadoc] ../workspace/hbase/src/java/org/apache/hadoop/hbase/regionserver/wal/hlog.java:104: warning - tag @link: can't find getreader(org.apache.hadoop.fs.filesystem,  [javadoc] org.apache.hadoop.fs.path, org.apache.hadoop.hbase.hbaseconfiguration) in org.apache.hadoop.hbase.regionserver.wal.hlog ",
        "label": 266
    },
    {
        "text": "log splitting doesn't prevent rs creating new hlog file  here is something weird happened to my cluster. when the master recovered, it thought rs was dead by mistake and ssh processed it. four hlog files were split and 4 regions were reassigned. later on when the rs tried to report status, it got youaredeadexception from master. master tried to recover it again. now ssh split one more hlog file, assigned 0 more extra region. now the question is how come is the extra log file? i think it caused a data loss (one row) in my case. how can we prevent it? i can add a checking in the master to make sure a rs is really dead before marking it dead during recovery since it may be slow in reporting to the new master. but this is not a proper fix. a proper fix should be in the rs side, not creating the extra hlog file. is this doable? here is the related master log. the rs log does not have much thing interested. 2013-10-20 17:40:33,025 info  [master:e1119:36000] master.masterfilesystem: log folder hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312 doesn't belong to a known region server, splitting 2013-10-20 17:40:33,432 info  [master_server_operations-e1119:36000-0] handler.servershutdownhandler: splitting logs for e1521.halxg.cloudera.com,36020,1382314725312 before assignment. 2013-10-20 17:40:33,461 debug [master_server_operations-e1119:36000-0] master.masterfilesystem: renamed region directory: hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting 2013-10-20 17:40:33,462 info  [master_server_operations-e1119:36000-0] master.splitlogmanager: dead splitlog workers [e1521.halxg.cloudera.com,36020,1382314725312] 2013-10-20 17:40:33,467 info  [master_server_operations-e1119:36000-0] master.splitlogmanager: started splitting 3 logs in [hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting] 2013-10-20 17:40:36,157 debug [main-eventthread] wal.hlogsplitter: archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315954499 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldwals/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315954499 2013-10-20 17:40:37,357 debug [main-eventthread] wal.hlogsplitter: archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315934942 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldwals/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315934942 2013-10-20 17:40:38,889 debug [main-eventthread] wal.hlogsplitter: archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315971393 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldwals/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315971393 2013-10-20 17:40:38,906 info  [master_server_operations-e1119:36000-0] master.splitlogmanager: finished splitting (more than or equal to) 255322969 bytes in 3 log files in [hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting] in 5439ms 2013-10-20 17:40:38,907 info  [master_server_operations-e1119:36000-0] handler.servershutdownhandler: reassigning 4 region(s) that e1521.halxg.cloudera.com,36020,1382314725312 was carrying (and 0 regions(s) that were opening on this server) 2013-10-20 17:40:39,269 debug [master_server_operations-e1119:36000-0] master.deadserver: finished processing e1521.halxg.cloudera.com,36020,1382314725312 2013-10-20 17:40:39,269 info  [master_server_operations-e1119:36000-0] handler.servershutdownhandler: finished processing of shutdown of e1521.halxg.cloudera.com,36020,1382314725312 2013-10-20 17:40:46,764 debug [fiforpcscheduler.handler1-thread-11] master.servermanager: server report rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server 2013-10-20 17:40:46,871 error [fiforpcscheduler.handler1-thread-13] master.hmaster: region server e1521.halxg.cloudera.com,36020,1382314725312 reported a fatal error: aborting region server e1521.halxg.cloudera.com,36020,1382314725312: org.apache.hadoop.hbase.youaredeadexception: server report rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server org.apache.hadoop.hbase.youaredeadexception: org.apache.hadoop.hbase.youaredeadexception: server report rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception(org.apache.hadoop.hbase.youaredeadexception): org.apache.hadoop.hbase.youaredeadexception: server report rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server 2013-10-20 17:40:46,872 error [fiforpcscheduler.handler1-thread-14] master.hmaster: region server e1521.halxg.cloudera.com,36020,1382314725312 reported a fatal error: aborting region server e1521.halxg.cloudera.com,36020,1382314725312: ioe in log roller java.io.filenotfoundexception: file does not exist: hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382315971393 2013-10-20 17:40:48,075 error [fiforpcscheduler.handler1-thread-15] master.hmaster: region server e1521.halxg.cloudera.com,36020,1382314725312 reported a fatal error: aborting region server e1521.halxg.cloudera.com,36020,1382314725312: regionserver:36020-0x1419fea35edda89 regionserver:36020-0x1419fea35edda89 received expired from zookeeper, aborting 2013-10-20 17:53:15,272 info  [master:e1119:36000] master.masterfilesystem: log folder hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312 doesn't belong to a known region server, splitting 2013-10-20 17:53:15,806 info  [master_server_operations-e1119:36000-0] handler.servershutdownhandler: splitting logs for e1521.halxg.cloudera.com,36020,1382314725312 before assignment. 2013-10-20 17:53:15,825 debug [master_server_operations-e1119:36000-0] master.masterfilesystem: renamed region directory: hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting 2013-10-20 17:53:15,825 info  [master_server_operations-e1119:36000-0] master.splitlogmanager: dead splitlog workers [e1521.halxg.cloudera.com,36020,1382314725312] 2013-10-20 17:53:15,831 info  [master_server_operations-e1119:36000-0] master.splitlogmanager: started splitting 1 logs in [hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting] 2013-10-20 17:53:15,998 debug [main-eventthread] wal.hlogsplitter: archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382316046805 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldwals/e1521.halxg.cloudera.com%2c36020%2c1382314725312.1382316046805 2013-10-20 17:53:16,014 info  [master_server_operations-e1119:36000-0] master.splitlogmanager: finished splitting (more than or equal to) 15 bytes in 1 log files in [hdfs://e1119.halxg.cloudera.com:35802/hbase/wals/e1521.halxg.cloudera.com,36020,1382314725312-splitting] in 183ms 2013-10-20 17:53:16,015 info  [master_server_operations-e1119:36000-0] handler.servershutdownhandler: reassigning 0 region(s) that e1521.halxg.cloudera.com,36020,1382314725312 was carrying (and 0 regions(s) that were opening on this server) ",
        "label": 242
    },
    {
        "text": "the javadoc of setenabledtable maybe not describe accurately  /** sets the enabled state in the cache and deletes the zookeeper node. fails silently if the node is not in enabled in zookeeper @param tablename @throws keeperexception  */  public void setenabledtable(final string tablename) throws keeperexception { settablestate(tablename, tablestate.enabled); } when setenabledtable occours ,it will update the cache and the zookeeper node,rather than to delete the zk node. ",
        "label": 131
    },
    {
        "text": "testfromclientside testcheckanddeletewithcompareop and testnullqualifier are flakey  these two tests fail frequently locally; rare does this suite pass. the failures are either of these two tests. unfortunately, running the test standalone does not bring on the issue; need to run the whole suite. in both cases, we have a delete followed by a put and then a checkand* -type operation which does a get expecting to find the just put put but it fails on occasion. looks to be an mvcc issues or put going in at same timestamp as the delete. its hard to debug given any added logging seems to make it all pass again. seems this too is new in beta-1. running tests against alpha-4 seem to pass. doing a compare.... ",
        "label": 149
    },
    {
        "text": "hbtop  a top-like monitor could be useful for testing, debugging, operations of clusters of moderate size, and possibly for diagnosing issues in large clusters. consider a curses interface like the one presented by atop (http://www.atoptool.nl/images/screenshots/genericw.png) - with aggregate metrics collected over a monitoring interval in the upper portion of the pane, and a listing of discrete measurements sorted and filtered by various criteria in the bottom part of the pane. one might imagine a cluster overview with cluster aggregate metrics above and a list of regionservers sorted by utilization below; and a regionserver view with process metrics above and a list of metrics by operation type below, or a list of client connections, or a list of threads, sorted by utilization, throughput, or latency. generically 'htop' is taken but would be distinctive in the hbase context, a utility org.apache.hadoop.hbase.htop no need necessarily for a curses interface. could be an external monitor with a web front end as has been discussed before. i do like the idea of a process that runs in a terminal because i interact with dev and test hbase clusters exclusively by ssh. update:  the tool name is changed from htop to hbtop. ",
        "label": 455
    },
    {
        "text": "unhelpful npe in hbck  repair when adopting orphans if no tableinfo is found   13/01/09 17:34:54 debug util.hbasefsck: attempting to adopt orphan: { meta => null, hdfs => hdfs://c1514.hal.cloudera.com:56020/hbase-cdh4.2/pe-2-table/28fbe62eee2ffd8ea2611335ed78f8ce, deployed =>  } exception in thread \"main\" java.lang.nullpointerexception         at org.apache.hadoop.hbase.util.hbasefsck$tableinfo.access$000(hbasefsck.java:1871)         at org.apache.hadoop.hbase.util.hbasefsck.adopthdfsorphan(hbasefsck.java:482)         at org.apache.hadoop.hbase.util.hbasefsck.adopthdfsorphans(hbasefsck.java:455)         at org.apache.hadoop.hbase.util.hbasefsck.restorehdfsintegrity(hbasefsck.java:576)         at org.apache.hadoop.hbase.util.hbasefsck.offlinehdfsintegrityrepair(hbasefsck.java:353)         at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:431)         at org.apache.hadoop.hbase.util.hbasefsck.exec(hbasefsck.java:3614)         at org.apache.hadoop.hbase.util.hbasefsck.run(hbasefsck.java:3436)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:84)         at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:3430) ",
        "label": 406
    },
    {
        "text": "master am regionstates changes to create and assign region replicas  as per design in the parent jira, this jira will capture the changes in the master side (especially am / regionstates) for creating tables with region replicas, and making sure the regions are assigned on create and failover. ",
        "label": 139
    },
    {
        "text": "precommit should not run all java goals when given a docs only patch  right now docs-only patches (those that only impact the top level src/main/site, src/main/asciidoc, or src/main/xslt) run through all of the java related precommit checks, including test4tests and the full unit test suite. since we know these paths don't require those checks, we should update our personality to skip them. (or fix our project structure to match \"the maven way\".) ",
        "label": 402
    },
    {
        "text": "close region bugs  lars found and fixed some issues with close_region. see http://search-hadoop.com/m/l8dd1v5yts1/issues+with+close_region&subj=issues+with+close_region ",
        "label": 314
    },
    {
        "text": "when an exception has to escape servercallable due to exhausted retries  show all the exceptions that lead to this situation  every so often, we find ourselves trying to debug a problem that happens in htable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. oftentimes the last exception that comes out is something like wrongregionexception, which should just never be the case. as a way to improve our debugging capabilities, when we decide to throw an exception out of servercallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. this will help us understand the sequence of events that led to us running out of retries. ",
        "label": 86
    },
    {
        "text": "add support for small reverse scan  hbase-4811 adds feature of reverse scan. this jira adds the support for small reverse scan.  this is activated when both 'reversed' and 'small' attributes are true in scan object ",
        "label": 340
    },
    {
        "text": "surefire fails  when writing xml report stdout stderr   no such file or directory  17:22:33 [error] failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (secondparttestsexecution) on project hbase-server: executionexception: java.lang.runtimeexception: java.lang.runtimeexception: org.apache.maven.surefire.report.reporterexception: when writing xml report stdout/stderr: /tmp/stderr1114622923250399196deferred (no such file or directory) -> [help 1] it happens frequently on my jenkins...i update the surefire to 2.20.1, and then the failure doesn't happen again. see surefire-1239. ",
        "label": 98
    },
    {
        "text": "droppedsnapshotexception but regionserver doesn't restart  regionserver was carrying root and having trouble writing hdfs. regionserver judged that a flush failed and reported a droppedsnapshotexception. usually, the filesystem check would fail and set all the abort flags but it in this case filesystem somehow returned healthy and the flags were not set. the code path shutdown the rpc only and exited then we exited the flusher. all the rest of the regionserver stayed up and kept reporting the master. the master thought it alive and kept trying to scan the unreachable root. cluster was hosed until manual intervention 20 minutes later. ",
        "label": 314
    },
    {
        "text": "weird behavior of wildcardcolumntracker checkcolumn  looks like recursive loop  i got a weird error twice on a mr job which eventually though completed ... 09/08/19 11:28:31 info mapreduce.tableinputformatbase: split: 2591->foo.bar.net:1fff99f02088fe,1fffdcbdb0476b 09/08/19 11:28:31 info mapreduce.tableinputformatbase: split: 2592->foo.bar.net:1fffdcbdb0476b, 09/08/19 11:28:31 info mapred.jobclient: running job: job_200908120615_0015 09/08/19 11:28:32 info mapred.jobclient:  map 0% reduce 0% 09/08/19 11:35:43 info mapred.jobclient:  map 1% reduce 0% 09/08/19 11:39:53 info mapred.jobclient:  map 2% reduce 0% 09/08/19 11:42:58 info mapred.jobclient:  map 3% reduce 0% 09/08/19 11:47:02 info mapred.jobclient:  map 4% reduce 0% 09/08/19 11:50:41 info mapred.jobclient:  map 5% reduce 0% 09/08/19 11:54:25 info mapred.jobclient:  map 6% reduce 0% 09/08/19 11:58:31 info mapred.jobclient:  map 7% reduce 0% 09/08/19 12:02:36 info mapred.jobclient:  map 8% reduce 0% 09/08/19 12:06:12 info mapred.jobclient:  map 9% reduce 0% 09/08/19 12:10:01 info mapred.jobclient:  map 10% reduce 0% 09/08/19 12:13:40 info mapred.jobclient:  map 11% reduce 0% 09/08/19 12:17:04 info mapred.jobclient:  map 12% reduce 0% 09/08/19 12:21:07 info mapred.jobclient:  map 13% reduce 0% 09/08/19 12:24:46 info mapred.jobclient:  map 14% reduce 0% 09/08/19 12:29:27 info mapred.jobclient:  map 15% reduce 0% 09/08/19 12:33:42 info mapred.jobclient:  map 16% reduce 0% 09/08/19 12:38:04 info mapred.jobclient:  map 17% reduce 0% 09/08/19 12:44:16 info mapred.jobclient:  map 18% reduce 0% 09/08/19 12:50:20 info mapred.jobclient:  map 19% reduce 0% 09/08/19 12:55:44 info mapred.jobclient:  map 20% reduce 0% 09/08/19 13:01:11 info mapred.jobclient:  map 21% reduce 0% 09/08/19 13:06:21 info mapred.jobclient:  map 22% reduce 0% 09/08/19 13:11:24 info mapred.jobclient:  map 23% reduce 0% 09/08/19 13:15:39 info mapred.jobclient:  map 24% reduce 0% 09/08/19 13:20:33 info mapred.jobclient:  map 25% reduce 0% 09/08/19 13:25:58 info mapred.jobclient:  map 26% reduce 0% 09/08/19 13:29:52 info mapred.jobclient:  map 27% reduce 0% 09/08/19 13:34:44 info mapred.jobclient:  map 28% reduce 0% 09/08/19 13:38:19 info mapred.jobclient:  map 29% reduce 0% 09/08/19 13:41:53 info mapred.jobclient:  map 30% reduce 0% 09/08/19 13:45:09 info mapred.jobclient:  map 31% reduce 0% 09/08/19 13:49:06 info mapred.jobclient:  map 32% reduce 0% 09/08/19 13:52:47 info mapred.jobclient:  map 33% reduce 0% 09/08/19 13:56:37 info mapred.jobclient:  map 34% reduce 0% 09/08/19 13:59:48 info mapred.jobclient:  map 35% reduce 0% 09/08/19 14:04:14 info mapred.jobclient:  map 36% reduce 0% 09/08/19 14:09:32 info mapred.jobclient:  map 37% reduce 0% 09/08/19 14:14:00 info mapred.jobclient:  map 38% reduce 0% 09/08/19 14:17:42 info mapred.jobclient:  map 39% reduce 0% 09/08/19 14:21:50 info mapred.jobclient:  map 40% reduce 0% 09/08/19 14:25:17 info mapred.jobclient:  map 41% reduce 0% 09/08/19 14:28:57 info mapred.jobclient:  map 42% reduce 0% 09/08/19 14:33:03 info mapred.jobclient:  map 43% reduce 0% 09/08/19 14:36:51 info mapred.jobclient:  map 44% reduce 0% 09/08/19 14:40:49 info mapred.jobclient:  map 45% reduce 0% 09/08/19 14:44:44 info mapred.jobclient:  map 46% reduce 0% 09/08/19 14:48:37 info mapred.jobclient:  map 47% reduce 0% 09/08/19 14:52:15 info mapred.jobclient:  map 48% reduce 0% 09/08/19 14:55:57 info mapred.jobclient:  map 49% reduce 0% 09/08/19 14:59:21 info mapred.jobclient:  map 50% reduce 0% 09/08/19 15:02:58 info mapred.jobclient:  map 51% reduce 0% 09/08/19 15:07:23 info mapred.jobclient:  map 52% reduce 0% 09/08/19 15:10:19 info mapred.jobclient:  map 53% reduce 0% 09/08/19 15:13:19 info mapred.jobclient:  map 54% reduce 0% 09/08/19 15:16:38 info mapred.jobclient:  map 55% reduce 0% 09/08/19 15:19:36 info mapred.jobclient:  map 56% reduce 0% 09/08/19 15:22:41 info mapred.jobclient:  map 57% reduce 0% 09/08/19 15:25:35 info mapred.jobclient:  map 58% reduce 0% 09/08/19 15:30:07 info mapred.jobclient:  map 59% reduce 0% 09/08/19 15:37:41 info mapred.jobclient:  map 60% reduce 0% 09/08/19 15:42:04 warn zookeeper.clientcnxn: exception closing session 0x422d8cc8cbb310e to sun.nio.ch.selectionkeyimpl@4737371 java.io.ioexception: timed out         at org.apache.zookeeper.clientcnxn$sendthread.run(clientcnxn.java:858) 09/08/19 15:42:05 info zookeeper.clientcnxn: attempting connection to server tr-bt-dal-03/10.12.205.194:2181 09/08/19 15:42:05 info zookeeper.clientcnxn: priming connection to java.nio.channels.socketchannel[connected local=/10.16.182.238:41125 remote=tr-bt-dal-03/10.12.205.194:2181] 09/08/19 15:42:05 info zookeeper.clientcnxn: server connection successful 09/08/19 15:43:08 info mapred.jobclient:  map 61% reduce 0% 09/08/19 15:49:24 info mapred.jobclient:  map 62% reduce 0% 09/08/19 15:54:04 info mapred.jobclient:  map 63% reduce 0% 09/08/19 15:57:01 info mapred.jobclient:  map 64% reduce 0% 09/08/19 16:00:42 info mapred.jobclient:  map 65% reduce 0% 09/08/19 16:03:20 info mapred.jobclient:  map 66% reduce 0% 09/08/19 16:06:53 info mapred.jobclient:  map 67% reduce 0% 09/08/19 16:09:20 info mapred.jobclient:  map 68% reduce 0% 09/08/19 16:12:02 info mapred.jobclient:  map 69% reduce 0% 09/08/19 16:14:35 info mapred.jobclient:  map 70% reduce 0% 09/08/19 16:17:26 info mapred.jobclient:  map 71% reduce 0% 09/08/19 16:20:19 info mapred.jobclient:  map 72% reduce 0% 09/08/19 16:23:14 info mapred.jobclient:  map 73% reduce 0% 09/08/19 16:25:56 info mapred.jobclient:  map 74% reduce 0% 09/08/19 16:30:16 info mapred.jobclient:  map 75% reduce 0% 09/08/19 16:34:14 info mapred.jobclient:  map 76% reduce 0% 09/08/19 16:37:45 info mapred.jobclient:  map 77% reduce 0% 09/08/19 16:41:38 info mapred.jobclient:  map 78% reduce 0% 09/08/19 16:44:33 info mapred.jobclient:  map 79% reduce 0% 09/08/19 16:47:54 info mapred.jobclient:  map 80% reduce 0% 09/08/19 16:52:20 info mapred.jobclient:  map 81% reduce 0% 09/08/19 16:56:44 info mapred.jobclient:  map 82% reduce 0% 09/08/19 17:00:50 info mapred.jobclient:  map 83% reduce 0% 09/08/19 17:02:57 info mapred.jobclient: task id : attempt_200908120615_0015_m_002231_0, status : failed org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row '7c78ca95-4d82-420f-ab2f-495fe139bad0', but failed after 10 attempts. exceptions: java.io.ioexception: java.io.ioexception: java.lang.stackoverflowerror         at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:847)         at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:837)         at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1770)         at sun.reflect.generatedmethodaccessor39.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:650)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:913) caused by: java.lang.stackoverflowerror         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:136)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracke 09/08/19 17:04:01 info mapred.jobclient:  map 84% reduce 0% 09/08/19 17:07:16 info mapred.jobclient:  map 85% reduce 0% 09/08/19 17:10:38 info mapred.jobclient:  map 86% reduce 0% 09/08/19 17:13:44 info mapred.jobclient:  map 87% reduce 0% 09/08/19 17:16:12 info mapred.jobclient:  map 88% reduce 0% 09/08/19 17:19:29 info mapred.jobclient:  map 89% reduce 0% 09/08/19 17:23:00 info mapred.jobclient:  map 90% reduce 0% 09/08/19 17:29:25 info mapred.jobclient: task id : attempt_200908120615_0015_m_002231_1, status : failed org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row '7c78ca95-4d82-420f-ab2f-495fe139bad0', but failed after 10 attempts. exceptions: java.io.ioexception: java.io.ioexception: java.lang.stackoverflowerror         at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:847)         at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:837)         at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1770)         at sun.reflect.generatedmethodaccessor39.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:650)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:913) caused by: java.lang.stackoverflowerror         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:136)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracke 09/08/19 17:31:22 info mapred.jobclient:  map 91% reduce 0% 09/08/19 17:43:27 info mapred.jobclient:  map 92% reduce 0% 09/08/19 17:57:27 info mapred.jobclient:  map 93% reduce 0% 09/08/19 18:09:43 info mapred.jobclient:  map 94% reduce 0% 09/08/19 18:19:16 info mapred.jobclient:  map 95% reduce 0% 09/08/19 18:21:49 info mapred.jobclient: task id : attempt_200908120615_0015_m_002231_2, status : failed org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server 10.16.182.240:60020 for region segmentdata,79d34f17-9b48-4835-bf03-971c9e21ff2f,1250687021375, row '7c78ca95-4d82-420f-ab2f-495fe139bad0', but failed after 10 attempts. exceptions: java.io.ioexception: java.io.ioexception: java.lang.stackoverflowerror         at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:847)         at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:837)         at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1770)         at sun.reflect.generatedmethodaccessor39.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:650)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:913) caused by: java.lang.stackoverflowerror         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:136)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracker.checkcolumn(wildcardcolumntracker.java:158)         at org.apache.hadoop.hbase.regionserver.wildcardcolumntracke 09/08/19 18:31:22 info mapred.jobclient:  map 96% reduce 0% 09/08/19 18:42:54 info mapred.jobclient:  map 97% reduce 0% 09/08/19 18:43:02 info mapred.jobclient: job complete: job_200908120615_0015 09/08/19 18:43:02 info mapred.jobclient: counters: 11 09/08/19 18:43:02 info mapred.jobclient:   job counters 09/08/19 18:43:02 info mapred.jobclient:     rack-local map tasks=30 09/08/19 18:43:02 info mapred.jobclient:     launched map tasks=2530 09/08/19 18:43:02 info mapred.jobclient:     data-local map tasks=2500 09/08/19 18:43:02 info mapred.jobclient:     failed map tasks=1 09/08/19 18:43:02 info mapred.jobclient:   net.foo.hadoop.hbase.mapred.setcurrentdata$setcurrentmapper$counters 09/08/19 18:43:02 info mapred.jobclient:     rows=22692113 09/08/19 18:43:02 info mapred.jobclient:     network=101481740 09/08/19 18:43:02 info mapred.jobclient:     assoc=231471790 09/08/19 18:43:02 info mapred.jobclient:     curr_network=23944682 09/08/19 18:43:02 info mapred.jobclient:     curr_provider=1801150 09/08/19 18:43:02 info mapred.jobclient:   map-reduce framework 09/08/19 18:43:02 info mapred.jobclient:     map input records=22692113 09/08/19 18:43:02 info mapred.jobclient:     spilled records=0 ",
        "label": 229
    },
    {
        "text": "print a message when an invalid hbase rootdir is passed  as seen on the mailing list: http://comments.gmane.org/gmane.comp.java.hadoop.hbase.user/24124 if hbase.rootdir doesn't specify a folder on hdfs we crash while opening a path to .oldlogs: 2012-02-02 23:07:26,292 fatal org.apache.hadoop.hbase.master.hmaster: unhandled exception. starting shutdown. java.lang.illegalargumentexception: java.net.urisyntaxexception: relative path in absolute uri: hdfs://sv4r11s38:9100.oldlogs         at org.apache.hadoop.fs.path.initialize(path.java:148)         at org.apache.hadoop.fs.path.<init>(path.java:71)         at org.apache.hadoop.fs.path.<init>(path.java:50)         at org.apache.hadoop.hbase.master.masterfilesystem.<init>(masterfilesystem.java:112)         at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:448)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:326)         at java.lang.thread.run(thread.java:662) caused by: java.net.urisyntaxexception: relative path in absolute uri: hdfs://sv4r11s38:9100.oldlogs         at java.net.uri.checkpath(uri.java:1787)         at java.net.uri.<init>(uri.java:735)         at org.apache.hadoop.fs.path.initialize(path.java:145)         ... 6 more it could also crash anywhere else, this just happens to be the first place we use hbase.rootdir. we need to verify that it's an actual folder. ",
        "label": 242
    },
    {
        "text": "improve test coverage in pkg org apache hadoop hbase mapred  ",
        "label": 40
    },
    {
        "text": "site target fails  mvn -dskiptests -dhadoop.profile=2.0 clean install site assembly:assembly [...] recoverable error org.xml.sax.saxparseexception: include operation failed, reverting to fallback. resource error reading file as xml (href='../../target/site/hbase-default.xml'). reason: /usr/src/hadoop/hbase/target/site/hbase-default.xml (no such file or directory) error on line 672 column 52 of file:///usr/src/hadoop/hbase/src/docbkx/configuration.xml:   error reported by xml parser: an 'include' failed, and no 'fallback' element was found. [info]                                                                          [info] ------------------------------------------------------------------------ [info] skipping hbase [info] this project has been banned from the build due to previous failures. [info] ------------------------------------------------------------------------ [info] ------------------------------------------------------------------------ [info] reactor summary: [info]  [info] hbase ............................................. failure [5:34.980s] [info] hbase - common .................................... skipped [info] hbase - protocol .................................. skipped [info] hbase - client .................................... skipped [info] hbase - prefix tree ............................... skipped [info] hbase - hadoop compatibility ...................... skipped [info] hbase - hadoop two compatibility .................. skipped [info] hbase - server .................................... skipped [info] hbase - integration tests ......................... skipped [info] hbase - examples .................................. skipped [info] ------------------------------------------------------------------------ [info] build failure [info] ------------------------------------------------------------------------ [info] total time: 5:36.029s [info] finished at: thu mar 07 21:59:14 cst 2013 [info] final memory: 29m/297m [info] ------------------------------------------------------------------------ [error] failed to execute goal com.agilejava.docbkx:docbkx-maven-plugin:2.0.14:generate-html (multipage) on project hbase: failed to transform configuration.xml. org.xml.sax.saxparseexception: an 'include' failed, and no 'fallback' element was found. -> [help 1] ",
        "label": 38
    },
    {
        "text": "note how dfs support append has to be enabled in clusters  ",
        "label": 314
    },
    {
        "text": "hfile main fixes  hfile.main has a bunch of nice new facility added by lars. this issue is small fixes. ",
        "label": 314
    },
    {
        "text": "in the hbase shell  an error is thrown that states replication related znodes already exist  on a replication-enabled cluster, querying the list_peers produces the error lines shown below. it doesn't appear that anything is broken in terms of functionality. stack trace: hbase(main):001:0> list_peers  12/09/29 14:41:03 error zookeeper.recoverablezookeeper: node /hbase/replication/peers already exists and this is not a retry  12/09/29 14:41:03 error zookeeper.recoverablezookeeper: node /hbase/replication/rs already exists and this is not a retry  peer id cluster key  0 row(s) in 0.4650 seconds ",
        "label": 186
    },
    {
        "text": "create a jenkins file for yetus to processing github pr  i think we can just copy the jenkinsfile from the hadoop project. ",
        "label": 149
    },
    {
        "text": "rest mangles table names  from sishen: hi, guys. i think i have met the problem of table query. when playing with the rest interface, the result is all the table name are un-readable format and i can't get the table metadata. after some debug, i found that if use table.getname().tostring() to construct the table name string. however, getname() return a byte array. so tostring() method jsut return the string representation of the array object.  the correct way should be new string(table.getname()). i have tested it and all work well now. below are patches: index: org/apache/hadoop/hbase/rest/metahandler.java =================================================================== --- org/apache/hadoop/hbase/rest/metahandler.java    (revision 678344) +++ org/apache/hadoop/hbase/rest/metahandler.java    (working copy) @@ -85,7 +85,7 @@            xmloutputter outputter = getxmloutputter(response.getwriter());            outputter.starttag(\"tables\");            for (int i = 0; i < tables.length; i++) { -            doelement(outputter, \"table\", tables[i].getname().tostring()); +            doelement(outputter, \"table\", new string(tables[i].getname()));            }            outputter.endtag();            outputter.enddocument(); @@ -96,7 +96,7 @@              contenttype.plain.tostring());            printwriter out = response.getwriter();            for (int i = 0; i < tables.length; i++) { -            out.println(tables[i].getname().tostring()); +            out.println(new string(tables[i].getname()));            }            out.close();          break; index: org/apache/hadoop/hbase/rest/tablehandler.java =================================================================== --- org/apache/hadoop/hbase/rest/tablehandler.java    (revision 678344) +++ org/apache/hadoop/hbase/rest/tablehandler.java    (working copy) @@ -391,7 +391,7 @@      htabledescriptor [] tables = this.admin.listtables();      htabledescriptor descriptor = null;      for (int i = 0; i < tables.length; i++) { -      if (tables[i].getname().tostring().equals(tablename)) { +      if (new string(tables[i].getname()).equals(tablename)) {          descriptor = tables[i];          break;        } @@ -406,7 +406,7 @@          setresponseheader(response, 200, contenttype.xml.tostring());          xmloutputter outputter = getxmloutputter(response.getwriter());          outputter.starttag(\"table\"); -        doelement(outputter, \"name\", descriptor.getname().tostring()); +        doelement(outputter, \"name\", new string(descriptor.getname()));          outputter.starttag(\"columnfamilies\");          for (hcolumndescriptor e: descriptor.getfamilies()) {            outputter.starttag(\"columnfamily\"); best regards. sishen index: org/apache/hadoop/hbase/rest/metahandler.java =================================================================== --- org/apache/hadoop/hbase/rest/metahandler.java (revision 678344) +++ org/apache/hadoop/hbase/rest/metahandler.java (working copy) @@ -85,7 +85,7 @@            xmloutputter outputter = getxmloutputter(response.getwriter());            outputter.starttag(\"tables\");            for (int i = 0; i < tables.length; i++) { -            doelement(outputter, \"table\", tables[i].getname().tostring()); +            doelement(outputter, \"table\", new string(tables[i].getname()));            }            outputter.endtag();            outputter.enddocument(); @@ -96,7 +96,7 @@              contenttype.plain.tostring());            printwriter out = response.getwriter();            for (int i = 0; i < tables.length; i++) { -            out.println(tables[i].getname().tostring()); +            out.println(new string(tables[i].getname()));            }            out.close();          break; index: org/apache/hadoop/hbase/rest/tablehandler.java =================================================================== --- org/apache/hadoop/hbase/rest/tablehandler.java (revision 678344) +++ org/apache/hadoop/hbase/rest/tablehandler.java (working copy) @@ -391,7 +391,7 @@      htabledescriptor [] tables = this.admin.listtables();      htabledescriptor descriptor = null;      for (int i = 0; i < tables.length; i++) { -      if (tables[i].getname().tostring().equals(tablename)) { +      if (new string(tables[i].getname()).equals(tablename)) {          descriptor = tables[i];          break;        } @@ -406,7 +406,7 @@          setresponseheader(response, 200, contenttype.xml.tostring());          xmloutputter outputter = getxmloutputter(response.getwriter());          outputter.starttag(\"table\"); -        doelement(outputter, \"name\", descriptor.getname().tostring()); +        doelement(outputter, \"name\", new string(descriptor.getname()));          outputter.starttag(\"columnfamilies\");          for (hcolumndescriptor e: descriptor.getfamilies()) {            outputter.starttag(\"columnfamily\"); ",
        "label": 314
    },
    {
        "text": "re enable hbase checksums by default  double checksumming issues in hbase level checksums(hbase-5074) has been fixed in hbase-6868. however, that patch also disables hbase checksums by default. i think we should re-enable it by default, and document the interaction with shortcircuit reads. ",
        "label": 229
    },
    {
        "text": "expose multi region merge in shell and admin api  hbase-22777 adds being able to merge more than two regions at once. it is only available internally currently for use by hbck2 doing fixup of overlaps in hbase:meta. this issue is about exposing it via the admin interface and in turn, via the shell. probably best if old two region merge method is deprecated. ",
        "label": 391
    },
    {
        "text": "workaround  purge user api building from branch so can make a beta  entertaining idea of purging user api doc generation from branch-2 because am unable to figure why it broke (see parent issue). i don't want to hold up beta-1 for this. filing placeholder issue against which i can hang testing this alternative. ",
        "label": 314
    },
    {
        "text": "scanning api must be reworked to allow for fully functional filters client side  right now, a client replays part of the filter locally by calling filterrowkey() and filterallremaining() to determine whether it should continue to the next region. a number of new filters rely on filterkeyvalue() and other calls to alter state. it's also a false assumption that all rows/keys affecting a filter returning true for far will be seen client-side (what about those that failed the filter). this issue is about dealing with filters properly from the client-side. ",
        "label": 314
    },
    {
        "text": "compoundconfiguration should implement iterable  here is from hadoop configuration class: public class configuration implements iterable<map.entry<string,string>>, there're 3 addxx() methods for compoundconfiguration:   public compoundconfiguration add(final configuration conf) {   public compoundconfiguration addwritablemap(       final map<immutablebyteswritable, immutablebyteswritable> map) {   public compoundconfiguration addstringmap(final map<string, string> map) { parameters to these methods all support iteration. we can enhance immutableconfigmap with the following new method:   public abstract java.util.iterator iterator(); then the following method of compoundconfiguration can be implemented:   public iterator<map.entry<string, string>> iterator() { this enhancement would be useful in scenario where a mutable configuration is required. ",
        "label": 441
    },
    {
        "text": " book  document how to count rows  i just saw that there's no documentation on how to count rows. basically there's: the \"count\" command in the shell, it's quite fast when configured with the right cache. the \"rowcounter\" mr job, it's super fast but you need to have a mapreduce setup in place in order to use it. there's also a way to estimate the row count by using the hfile tool and doing a bit of bash magic. ",
        "label": 115
    },
    {
        "text": "rename accesscontrollists to permissionstorage  accesscontrollists is a utility class which deal with get/put/delete operations with hbase acl table. the name of the class is confusing, so shall we rename it to permissionstorage? ",
        "label": 500
    },
    {
        "text": "reevaluate default values of configurations  remove cruft and mythologies. make descriptions more digestible. change defaults given experience. ",
        "label": 314
    },
    {
        "text": "hbase server leaks jdk tools dependency to mapreduce consumers  hbase-13963 / hbase-14844 take care of removing leaks of our dependency on jdk-tools. until we move the mapreduce support classes out of hbase-server (hbase-11843), we need to also avoid leaking the dependency from that module. ",
        "label": 402
    },
    {
        "text": "put up 8rc0  ",
        "label": 149
    },
    {
        "text": " amv2  reenable tests temporarily disabled  we disabled tests that didn't make sense or relied on behavior not supported by amv2. revisit and reenable after amv2 gets committed. here is the set (from https://docs.google.com/document/d/1evka7fhdeoj1-9o8yzcotaqbv0u0bblblcczvsin69g/edit#heading=h.rsj53tx4vlwj) testallfavorednodesdead and testallfavorednodesdeadmasterrestarted and testmisplacedregions in testfavoredstochasticloadbalancer \u2026 not sure what this about.  testregionnormalizationmergeoncluster in testsimpleregionnormalizeroncluster disabled for now till we fix up merge.  testmergewithreplicas in testregionmergetransactiononcluster because don't know how it is supposed to work.  admin#close does not update master. causes testhbasefsckwithfewermetareplicaznodes in testmetawithreplicas to fail (master gets report about server closing when it didn\u2019t run the close \u2013 gets freaked out).  disabled/ignore testrsgroupsofflinemode#testoffline; need to dig in on what offline is.  disabled/ignore testrsgroups.  all tests that have to do w/ fsck:testhbasefscktwors, testofflinemetarebuildbase testhbasefsckreplicas, testofflinemetarebuildoverlap, testchangingreplicacount in testmetawithreplicas (internally it is doing fscks which are killing rs)...  fsck test testhbasefsckwithexcessmetareplicas in testmetawithreplicas.  so is testhbasefsckwithfewermetareplicas in same class.  testhbasefsckoners is fsck. disabled.  testofflinemetarebuildhole is about rebuilding hole with fsck.  master carries meta:  testregionrebalancing is disabled because doesn't consider the fact that master carries system tables only (fix of average in regionstates brought out the issue).  disabled testmetaaddresschange in testmetawithreplicas because presumes can move meta... you can't  testasynctablegetmultithreaded wants to move hbase:meta...balancer does npes. amv2 won't let you move hbase:meta off master.  disabled parts of...testcreatetablewithmultiplereplicas in testmasteroperationsforregionreplicas there is an issue w/ assigning more replicas if number of replicas is changed on us. see '/* disabled!!!!! for now!!!!'.  disabled testcorruptedregionstorefile. depends on a half-implemented reopen of a region when a store file goes missing; todo.  testretainassignmentonrestart in testrestartcluster does not work. amv2 does retain semantic differently. fix. todo.  testmasterfailover needs to be rewritten for amv2. it uses tricks not ordained when up on amv2. the test is also hobbled by fact that we religiously enforce that only master can carry meta, something we are lose about in old am.  fix ignores in testservercrashprocedure. master is different now.  offlining is done differently now: because of this disabled testofflineregion in testasyncregionadminapi  skipping delete of table after test in testaccesscontroller3 because of access issues w/ amv2. amv1 seems to crash servers on exit too for same lack of auth perms but amv2 gets hung up. todo. see cleanup method.  testhcm#testmulti and testhcm  fix testmastermetrics. stuff is different now around startup which messes up this test. disabled two of three tests.  i tried to fix testmasterbalancethrottling but it looks like simpleloadbalancer is borked whether amv2 or not.  disabled testpickers in testfavoredstochasticbalancerpickers. it hangs. ",
        "label": 314
    },
    {
        "text": "npe in hlog append   droppedsnapshotexception causes hosed regionserver  might be related to hbase-644? in my regionserver log i see this: 2008-09-17 21:36:06,335 info org.apache.hadoop.ipc.server: ipc server handler 3 on 60020, call batchupdate([b@ff0be7b, row => sn5mit-wiyuoud4gqyj0r-==, {column => anchor:anchor_text, value => '...'} ) f  rom 208.76.44.97:41671: error: java.io.ioexception: java.lang.nullpointerexception  java.io.ioexception: java.lang.nullpointerexception  at org.apache.hadoop.io.sequencefile$writer.checkandwritesync(sequencefile.java:970)  at org.apache.hadoop.io.sequencefile$writer.append(sequencefile.java:1012)  at org.apache.hadoop.io.sequencefile$writer.append(sequencefile.java:979)  at org.apache.hadoop.hbase.regionserver.hlog.append(hlog.java:395)  at org.apache.hadoop.hbase.regionserver.hregion.update(hregion.java:1631)  at org.apache.hadoop.hbase.regionserver.hregion.batchupdate(hregion.java:1417)  at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1145)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:473)  at org.apache.hadoop.ipc.server$handler.run(server.java:888)  2008-09-17 21:36:06,336 warn org.apache.hadoop.ipc.server: ipc server responder, call batchupdate([b@ff0be7b, row => sn5mit-wiyuoud4gqyj0r-==, {column => anchor:anchor_text, value => '...'} ) from x.x.44.97:41671: output error  2008-09-17 21:36:06,336 info org.apache.hadoop.ipc.server: ipc server handler 3 on 60020 caught: java.nio.channels.closedchannelexception  at sun.nio.ch.socketchannelimpl.ensurewriteopen(unknown source)  at sun.nio.ch.socketchannelimpl.write(unknown source)  at org.apache.hadoop.ipc.server$responder.processresponse(server.java:587)  at org.apache.hadoop.ipc.server$responder.dorespond(server.java:651)  at org.apache.hadoop.ipc.server$handler.run(server.java:909) 2008-09-17 21:36:06,336 info org.apache.hadoop.ipc.server: ipc server handler 3 on 60020: exiting  2008-09-17 21:37:06,312 fatal org.apache.hadoop.hbase.regionserver.flusher: replay of hlog required. forcing server restart  org.apache.hadoop.hbase.droppedsnapshotexception: region: enwiki3,4m7afqmi8ijhepbxtuzp3k==,1221687099180  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1079)  at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:977)  at org.apache.hadoop.hbase.regionserver.flusher.flushregion(flusher.java:174)  at org.apache.hadoop.hbase.regionserver.flusher.flushsomeregions(flusher.java:268)  at org.apache.hadoop.hbase.regionserver.flusher.reclaimmemcachememory(flusher.java:253)  at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1144)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:473)  at org.apache.hadoop.ipc.server$handler.run(server.java:888)  caused by: java.io.ioexception: call failed on local exception  at org.apache.hadoop.ipc.client.call(client.java:718)  at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:216)  at org.apache.hadoop.dfs.$proxy1.getfileinfo(unknown source)  at sun.reflect.generatedmethodaccessor2.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:82)  at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:59)  at org.apache.hadoop.dfs.$proxy1.getfileinfo(unknown source)  at org.apache.hadoop.dfs.dfsclient.getfileinfo(dfsclient.java:566)  at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:390)  at org.apache.hadoop.fs.filesystem.exists(filesystem.java:667)  at org.apache.hadoop.hbase.regionserver.hstorefile.<init>(hstorefile.java:149)  at org.apache.hadoop.hbase.regionserver.hstore.internalflushcache(hstore.java:591)  at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:569)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1066)  ... 10 more  caused by: java.nio.channels.closedbyinterruptexception  at java.nio.channels.spi.abstractinterruptiblechannel.end(unknown source)  at sun.nio.ch.socketchannelimpl.write(unknown source)  at org.apache.hadoop.net.socketoutputstream$writer.performio(socketoutputstream.java:55)  at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:140)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:146)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:107)  at java.io.bufferedoutputstream.flushbuffer(unknown source)  at java.io.bufferedoutputstream.flush(unknown source)  at java.io.dataoutputstream.flush(unknown source)  at org.apache.hadoop.ipc.client$connection.sendparam(client.java:478)  at org.apache.hadoop.ipc.client.call(client.java:705)  ... 25 more  2008-09-17 21:37:06,339 info org.apache.hadoop.ipc.client: retrying connect to server: coral-dfs.cluster.powerset.com/208.76.44.135:10000. already tried 0 time(s).  2008-09-17 21:37:06,346 info org.apache.hadoop.ipc.server: ipc server handler 2 on 60020, call batchupdate([b@52fac63b, row => 6pceotjhe-vpigbs5dzgmv==, {column => [..], value => '...'} ) from x.x.44.96:44885: error: java.io.ioexception: cannot append; log is closed  java.io.ioexception: cannot append; log is closed  at org.apache.hadoop.hbase.regionserver.hlog.append(hlog.java:376)  at org.apache.hadoop.hbase.regionserver.hregion.update(hregion.java:1631)  at org.apache.hadoop.hbase.regionserver.hregion.batchupdate(hregion.java:1417)  at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1145)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:473)  at org.apache.hadoop.ipc.server$handler.run(server.java:888)  2008-09-17 21:37:06,346 warn org.apache.hadoop.ipc.server: ipc server responder, call batchupdate([b@52fac63b, row => 6pceotjhe-vpigbs5dzgmv==, {column => [...], value => '...'} ) from 208.76.44.96:44885: output error  2008-09-17 21:37:06,346 info org.apache.hadoop.ipc.server: ipc server handler 2 on 60020 caught: java.nio.channels.closedchannelexception  at sun.nio.ch.socketchannelimpl.ensurewriteopen(unknown source)  at sun.nio.ch.socketchannelimpl.write(unknown source)  at org.apache.hadoop.ipc.server$responder.processresponse(server.java:587)  at org.apache.hadoop.ipc.server$responder.dorespond(server.java:651)  at org.apache.hadoop.ipc.server$handler.run(server.java:909) 2008-09-17 21:37:06,346 info org.apache.hadoop.ipc.server: ipc server handler 2 on 60020: exiting  2008-09-17 21:37:54,491 info org.apache.hadoop.hbase.regionserver.hregionserver: scanner 3236781325465377548 lease expired  2008-09-17 21:37:54,492 debug org.apache.hadoop.hbase.regionserver.hregion: no more active scanners for region enwiki,mhw740ladnxzm68dn7lxs-==,1220059653599  2008-09-17 21:37:54,492 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region enwiki,mhw740ladnxzm68dn7lxs-==,1220059653599  2008-09-17 21:37:54,492 debug org.apache.hadoop.hbase.regionserver.hregion: no more row locks outstanding on region enwiki,mhw740ladnxzm68dn7lxs-==,1220059653599  2008-09-17 21:37:54,492 debug org.apache.hadoop.hbase.regionserver.hstore: closed 370784783/anchor  2008-09-17 21:37:54,493 debug org.apache.hadoop.hbase.regionserver.hstore: closed 370784783/misc  2008-09-17 21:37:54,493 debug org.apache.hadoop.hbase.regionserver.hstore: closed 370784783/alternate_title  2008-09-17 21:37:54,493 debug org.apache.hadoop.hbase.regionserver.hstore: closed 370784783/page  2008-09-17 21:37:54,493 debug org.apache.hadoop.hbase.regionserver.hstore: closed 370784783/alternate_url  2008-09-17 21:37:54,493 info org.apache.hadoop.hbase.regionserver.hregion: closed enwiki,mhw740ladnxzm68dn7lxs-==,1220059653599  2008-09-17 21:37:54,493 info org.apache.hadoop.hbase.regionserver.hregionserver: worker thread exiting  2008-09-17 21:38:07,581 fatal org.apache.hadoop.hbase.regionserver.hregionserver: shutting down hregionserver: file system not available  java.io.ioexception: file system is not available  at org.apache.hadoop.hbase.util.fsutils.checkfilesystemavailable(fsutils.java:82)  at org.apache.hadoop.hbase.regionserver.hregionserver.checkfilesystem(hregionserver.java:1484)  at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1150)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:473)  at org.apache.hadoop.ipc.server$handler.run(server.java:888)  caused by: java.io.ioexception: call failed on local exception  at org.apache.hadoop.ipc.client.call(client.java:718)  at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:216)  at org.apache.hadoop.dfs.$proxy1.getfileinfo(unknown source)  at sun.reflect.generatedmethodaccessor2.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:82)  at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:59)  at org.apache.hadoop.dfs.$proxy1.getfileinfo(unknown source)  at org.apache.hadoop.dfs.dfsclient.getfileinfo(dfsclient.java:566)  at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:390)  at org.apache.hadoop.fs.filesystem.exists(filesystem.java:667)  at org.apache.hadoop.hbase.util.fsutils.checkfilesystemavailable(fsutils.java:69)  ... 7 more  : the result is that every call to the regionserver hangs because the regionserver can't touch the dfs, because the dfsclient is closed. the regionserver won't restart either. ",
        "label": 241
    },
    {
        "text": "document the new 'normalization' feature in refguid  a nice new feature is coming in to 1.2.0, normalization. a small bit of doc on it in refguide would help. should define what normalization is. should say a sentence or two on how it works and when. throw in the output of shell commands. a paragraph or so. i can help. marking critical against 1.2.0. not a blocker. ",
        "label": 383
    },
    {
        "text": "new connector for avro rpc access to hbase cluster  build a new connector contrib architecturally equivalent to the thrift connector, but using avro serialization and associated transport and rpc server work. support aaa (audit, authentication, authorization). ",
        "label": 231
    },
    {
        "text": " hbase thirdparty  produce src jars tgz  in ides when want to dig in on protobuf or guava, need src jars. currently we don't generate them. generate them in next release. ",
        "label": 149
    },
    {
        "text": "put all test service principals into the superusers list  testvisibilitylabelswithacl is not setting up the secure minicluster correctly. you can see the issue here: 2013-12-20 18:25:44,979 warn  [htable-pool42-t1] client.asyncprocess(664): #57, table=hbase:meta, attempt=1/350 failed 1 ops, last exception: org.apache.hadoop.hbase.security.accessdeniedexception: org.apache.hadoop.hbase.security.accessdeniedexception: insufficient permissions (user=apurtell.hfs.1, scope=hbase:meta, family=info:server|info:serverstartcode|info:seqnumduringopen, action=write) at org.apache.hadoop.hbase.security.access.accesscontroller.requirecoveringpermission(accesscontroller.java:567) at org.apache.hadoop.hbase.security.access.accesscontroller.preput(accesscontroller.java:1139) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.preput(regioncoprocessorhost.java:964) at org.apache.hadoop.hbase.regionserver.hregion.dopremutationhook(hregion.java:2131) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2106) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2062) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2066) at org.apache.hadoop.hbase.regionserver.hregionserver.dobatchop(hregionserver.java:4206) at org.apache.hadoop.hbase.regionserver.hregionserver.dononatomicregionmutation(hregionserver.java:3448) at org.apache.hadoop.hbase.regionserver.hregionserver.multi(hregionserver.java:3352) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:28460) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2008) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:92) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:160) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:38) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:110) at java.lang.thread.run(thread.java:744)  on acer,47702,1387592735459, tracking started fri dec 20 18:25:44 pst 2013 - failed, not retrying anymore 2013-12-20 18:25:44,980 debug [postopendeploytasks:bdd47ebb21d762590edf81127c8cd30f] client.htable(949): hbase:meta: one or more of the operations have failed - waiting for all operation in progress to finish (successfully or not) ",
        "label": 38
    },
    {
        "text": "enabling a table after master switch does not allow table scan  throwing notservingregionexception  following is the scenario: start rs and active and standby masters  create table and insert data.  disable the table.  stop the active master and switch to the standby master.  now enable the table.  do a scan on the enabled table.  notservingregionexception is thrown. but the same works well when we dont switch the master. ",
        "label": 544
    },
    {
        "text": "fix testregionservercoprocessorendpoint  in asyncadmin we will unwrap the remoteexception so the assertion should be changed. ",
        "label": 149
    },
    {
        "text": "make  coprocessorhost classes private  move out configuration name constants (into coprocessor class?) and made host classes private. ",
        "label": 48
    },
    {
        "text": "open region failed open doesn't clear the taskmonitor status  keeps showing the old status  taskmonitor status will not be cleared in case regions failed_open. this will keeps showing old status. this will miss leads the user. ",
        "label": 543
    },
    {
        "text": "space quota  compaction is not working for super user in case of no writes compactions  space quota: compaction is not working for super user. compaction command is issued successfully at client but actually compaction is not happening. in debug log below message is printed: as an active space quota violation policy disallows compaction.  reference:   https://lists.apache.org/thread.html/d09aa7abaacf1f0be9d59fa9260515ddc0c17ac0aba9cc0f2ac569bf@%3cuser.hbase.apache.org%3e actually in requestcompactioninternal method of  compactsplit class ,there is no check for super user and compcations are disallowed   regionserverspacequotamanager spacequotamanager =         this.server.getregionserverspacequotamanager();     if (spacequotamanager != null &&         spacequotamanager.arecompactionsdisabled(region.gettabledescriptor().gettablename())) {       string reason = \"ignoring compaction request for \" + region +           \" as an active space quota violation \" + \" policy disallows compactions.\";       tracker.notexecuted(store, reason);       completetracker.completed(store);       log.debug(reason);       return;     }   ",
        "label": 391
    },
    {
        "text": " replication  change replication rpc to use cell blocks  currently, the replication rpc that ships edits simply dumps the byte value of wal edit key/value pairs into a protobuf message. modify the replication rpc mechanism to use cell blocks so it can leverage encoding and compression. ",
        "label": 314
    },
    {
        "text": " replication  race in replicationsourcemanager init can initiate a failover even if the node is alive  yet another bug found during the leap second madness, it's possible to miss the registration of new region servers so that in replicationsourcemanager.init we start the failover of a live and replicating region server. i don't think there's data loss but the rs that's being failed over will die on: 2012-07-01 06:25:15,604 fatal org.apache.hadoop.hbase.regionserver.hregionserver: aborting region server sv4r23s48,10304,1341112194623: writing replication status org.apache.zookeeper.keeperexception$nonodeexception: keepererrorcode = nonode for /hbase/replication/rs/sv4r23s48,10304,1341112194623/4/sv4r23s48%2c10304%2c1341112194623.1341112195369         at org.apache.zookeeper.keeperexception.create(keeperexception.java:111)         at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)         at org.apache.zookeeper.zookeeper.setdata(zookeeper.java:1246)         at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.setdata(recoverablezookeeper.java:372)         at org.apache.hadoop.hbase.zookeeper.zkutil.setdata(zkutil.java:655)         at org.apache.hadoop.hbase.zookeeper.zkutil.setdata(zkutil.java:697)         at org.apache.hadoop.hbase.replication.replicationzookeeper.writereplicationstatus(replicationzookeeper.java:470)         at org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager.logpositionandcleanoldlogs(replicationsourcemanager.java:154)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.shipedits(replicationsource.java:607)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:368) it seems to me that just refreshing otherregionservers after getting the list of currentreplicators would be enough to fix this. ",
        "label": 229
    },
    {
        "text": "large response handling  some fixups and cleanups  this may not be common for many use cases, but it might be good to put a couple of safety nets as well as logging to protect against large responses.  aravind and i were trying to track down why jvm memory usage was oscillating so much when dealing with very large buffers rather than oom'ing or hitting some index out of bound type exception, and this is what we found. java.io.bytearrayoutputstream graduates its internal buffers by doubling them. also, it is supposed to be able to handle \"int\" sized buffers (2g). the code which handles \"write\" (in jdk 1.6) is along the lines of:    public synchronized void write(byte b[], int off, int len) { if ((off < 0) || (off > b.length) || (len < 0) ||             ((off + len) > b.length) || ((off + len) < 0)) {     throw new indexoutofboundsexception(); } else if (len == 0) {     return; }         int newcount = count + len;         if (newcount > buf.length) {             buf = arrays.copyof(buf, math.max(buf.length << 1, newcount));         }         system.arraycopy(b, off, buf, count, len);         count = newcount;     } the \"buf.length << 1\" will start producing -ve values when buf.length reaches 1g, and \"newcount\" will instead dictate the size of the buffer allocated. at this point, all attempts to write to the buffer will grow linearly, and the buffer will be resized by only the required amount on each write. effectively, each write will allocate a new 1g buffer + reqd size buffer, copy the contents, and so on. this will put the process in heavy gc mode (with jvm heap oscillating by several gbs rapidly), and render it practically unusable. (ii) when serializing a result, the writearray method doesn't assert that the resultant size does not overflow an \"int\".     int buflen = 0;     for(result result : results) {       buflen += bytes.sizeof_int;       if(result == null || result.isempty()) {         continue;       }       for(keyvalue key : result.raw()) {         buflen += key.getlength() + bytes.sizeof_int;       }     } we should do the math in \"long\" and assert on buflen values > integer.max_value. (iii) in hbaseserver.java on rpc responses, we could add some logging on responses above a certain thresholds. (iv) increase buffer size threshold for buffers that are reused by rpc handlers. and make this configurable. currently, any response buffer about 16k is not reused on next response. (hbaseserver.java). ",
        "label": 547
    },
    {
        "text": "improve shell's cli help  in the hirb.rb source we have # so they don't go through to irb.  output shell 'usage' if user types '--help' cmdline_help = <<here # here document output as shell usage hbase shell command-line options:  format        formatter for outputting results: console | html. default: console  -d | --debug  set debug log levels. here found = [] format = 'console' script2run = nil log_level = org.apache.log4j.level::error for arg in argv  if arg =~ /^--format=(.+)/i    format = $1    if format =~ /^html$/i      raise nomethoderror.new(\"not yet implemented\")    elsif format =~ /^console$/i      # this is default    else      raise argumenterror.new(\"unsupported format \" + arg)    end    found.push(arg)  elsif arg == '-h' || arg == '--help'    puts cmdline_help    exit  elsif arg == '-d' || arg == '--debug'    log_level = org.apache.log4j.level::debug    $fullbacktrace = true    puts \"setting debug log level...\"  else    # presume it a script. save it off for running later below    # after we've set up some environment.    script2run = arg    found.push(arg)    # presume that any other args are meant for the script.    break  end end we should enhance the help printed when using h/-help to look like this? cmdline_help = <<here # here document output as shell usage hbase shell command-line options:  --format={console|html}        formatter for outputting results. default: console  -d | --debug  set debug log levels.  -h | --help   this help.  <script-filename> [<script-options>] here ",
        "label": 194
    },
    {
        "text": "per cf compaction via the shell  ",
        "label": 333
    },
    {
        "text": "coprocessor service methods in htableinterface should be annotated public  the htableinterface.coprocessorservice(...) and htableinterface.batchcoprocessorservice(...) methods were made private in hbase-9529, when the coprocessor apis were seen as unstable and evolving. however, these methods represent a standard way for clients to use custom apis exposed via coprocessors. in that sense, they are targeted at general hbase users (who may run but not develop coprocessors), as opposed to coprocessor developers who want to extend hbase. the coprocessor endpoint api has also remained much more stable than the coprocessor observer interfaces, which tend to change along with hbase internals. so there should not be much difficulty in supporting these methods as part of the public api. i think we should drop the @interfaceaudience.private annotation on these methods and support them as part of the public htableinterface. ",
        "label": 180
    },
    {
        "text": "eofexception opening region  hbase redux   hbase-550 dealt w/ eofexceptions out of write-ahead-logs. on our cluster, i'm seeing eofes out of mapfiles. in at least one case its because index is empty. /hbase/aa0-005-2.u.powerset.com/enwiki_080312/342351854/page/mapfiles/3485795994529467150/data  <r 3>   31268   2008-05-27 03:40        rw-r--r--       hadoop  supergroup /hbase/aa0-005-2.u.powerset.com/enwiki_080312/342351854/page/mapfiles/3485795994529467150/index <r 3>   246     2008-05-27 03:40        rw-r--r--       hadoop  supergroup /hbase/aa0-005-2.u.powerset.com/enwiki_080312/342351854/page/mapfiles/4707873234615510003/data  <r 3>   259979236       2008-05-26 22:23        rw-r--r--       hadoop  supergroup /hbase/aa0-005-2.u.powerset.com/enwiki_080312/342351854/page/mapfiles/4707873234615510003/index <r 3>   36689   2008-05-26 22:23        rw-r--r--       hadoop  supergroup /hbase/aa0-005-2.u.powerset.com/enwiki_080312/342351854/page/mapfiles/7019459271582770846/data  <r 3>   31268   2008-05-26 22:54        rw-r--r--       hadoop  supergroup /hbase/aa0-005-2.u.powerset.com/enwiki_080312/342351854/page/mapfiles/7019459271582770846/index <r 3>   0       2008-05-26 22:54        rw-r--r--       hadoop  supergroup here's the exception: 2008-05-27 18:05:08,289 debug org.apache.hadoop.hbase.hstore: loaded /hbase/xx.xx.xx.xx/enwiki_080312_meta/2102573201/alternate_title/info/8844820574762678369, isreference=false 2008-05-27 18:05:08,302 debug org.apache.hadoop.hbase.hstore: loaded 4 file(s) in hstore 2102573201/alternate_title, max sequence id 1081299433 2008-05-27 18:05:08,315 error org.apache.hadoop.hbase.hregionserver: error opening region enwiki_080312_meta,r-agpvbjj3r82tp0_eg2a-==,1207951923868 java.io.eofexception         at java.io.datainputstream.readfully(unknown source)         at java.io.datainputstream.readfully(unknown source)         at org.apache.hadoop.io.sequencefile$reader.init(sequencefile.java:1434)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1411)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1400)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1395)         at org.apache.hadoop.io.mapfile$reader.<init>(mapfile.java:254)         at org.apache.hadoop.io.mapfile$reader.<init>(mapfile.java:242)         at org.apache.hadoop.hbase.hstorefile$hbasemapfile$hbasereader.<init>(hstorefile.java:554)         at org.apache.hadoop.hbase.hstorefile$bloomfiltermapfile$reader.<init>(hstorefile.java:609)         at org.apache.hadoop.hbase.hstorefile.getreader(hstorefile.java:382)         at org.apache.hadoop.hbase.hstore.<init>(hstore.java:849)         at org.apache.hadoop.hbase.hregion.<init>(hregion.java:431)         at org.apache.hadoop.hbase.hregionserver.openregion(hregionserver.java:1258)         at org.apache.hadoop.hbase.hregionserver$worker.run(hregionserver.java:1204)         at java.lang.thread.run(unknown source) ",
        "label": 314
    },
    {
        "text": "add to hbase branch   if it works  make this a blocker for 0.1.1 release. ",
        "label": 241
    },
    {
        "text": "remove 'core' maven module  move core up a level  ",
        "label": 314
    },
    {
        "text": "move rpcserver connection to a separated file  ",
        "label": 149
    },
    {
        "text": "bin  scripts   not to include lib test jar  in trunk - the bin scripts ideally should not care about lib/test/*/.jar in cp, since they should be restricted to the scope of junit tests only. ",
        "label": 266
    },
    {
        "text": "calling gettimestamp  on a kv in cp preput  causes kv not to be flushed  there is an issue when you call gettimestamp() on any kv handed into a coprocessor's preput(). it initializes the internal \"timestampcache\" variable. when you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updatelateststamp() call). the timerangetracker then calls gettimestamp() later on to see if it has to include the kv, but instead of getting the proper time it sees the cached timestamp from the preput() call. ",
        "label": 285
    },
    {
        "text": "replace jackson with relocated gson everywhere but hbase rest  hbase-22728 moved out jackson transitive dependencies. mostly good, but moving jackson2 to provided in hbase-server broke few things testing-util needs a transitive jackson 2 in order to start the minicluster, currently fails with cnfe for com.fasterxml.jackson.databind.objectmapper when trying to initialize the master. shaded-testing-util needs a relocated jackson 2 for the same reason it's not used for any of the mapreduce stuff in hbase-server, so hbase-shaded-server for that purpose should be fine. but it is used by walprettyprinter and some folks might expect that to work from that artifact since it is present. ",
        "label": 473
    },
    {
        "text": "classloader that loads from hdfs  useful adding filters to classpath without having to restart services  ",
        "label": 242
    },
    {
        "text": "fix rest cluster constructor with string list  the hbase rest cluster which takes a list of hostname colon port numbers is not setting the internal list of nodes correctly. existing method: public cluster(list<string> nodes) {  nodes.addall(nodes)  } corrected method: public cluster(list<string> nodes) {  this.nodes.addall(nodes)  } ",
        "label": 38
    },
    {
        "text": "improve asyncwal by using independent thread for netty  io in fanoutoneblockasyncdfsoutput  the logic now is that the netty #io thread and asyncwal's thread are the same one.  improvement proposal:  1, split into two.  2, all multiwal share the netty #io thread pool. ",
        "label": 149
    },
    {
        "text": "hbase does not read hadoop xml for dfs configuration after moving out hadoop contrib  when hbase was in hadoop/contrib, the hbase script set both hadoop_conf_dir  and hbase_conf_dir to classpath, so that dfs's configuration can be loaded  correctly. however, when moved out hadoop/contrib, it only sets hbase_conf_dir. i can think of several possible solutions: 1) set hadoop_conf_dir in hbase-env.sh, then add hadoop_conf_dir to classpath as before  2) instruct user to create links for hadoop-*.xml if they want to customize some dfs settings.  3) if only a small set of dfs confs are related to dfs's client, maybe they can be set via hbase-site.xml, then hbase sets these for us when create a filesystem obj. please see the thread \"# of dfs replications when using hbase\" on hbase-user@. ",
        "label": 314
    },
    {
        "text": "coprocessors observers are not called during a recovery with the new log replay algorithm  see the patch to reproduce the issue: if we activate log replay we don't have the events on wal restore. pinging jeffrey zhong, we discussed this offline. ",
        "label": 233
    },
    {
        "text": "connect cellchunkmap to be used for flattening in compactingmemstore  the cellchunkmap helps to create a new type of immutablesegment, where the index (cellset's delegatee) is going to be cellchunkmap. no big cells or upserted cells are going to be supported here. ",
        "label": 35
    },
    {
        "text": "sanity date and time check when a region server joins the cluster  introduce a sanity check when a rs joins the cluster to make sure its clock isn't too far out of skew with the rest of the cluster. if the rs's time is too far out of skew then the master would prevent it from joining and rs would die and log the error. having a rs with even small differences in time can cause huge problems due to how bhase stores values with timestamps. according to j-d in servermanager we are already doing:     hserverinfo info = new hserverinfo(serverinfo);     checkisdead(info.getservername(), \"startup\");     checkalreadysamehostport(info);     recordnewserver(info, false, null); and that the new check would fit in nicely there. jg suggests we add a \"clockoutofsync-like exception\" ",
        "label": 232
    },
    {
        "text": "gracefully rolling restart region servers in rolling restart sh  the rolling-restart.sh has a --rs-only option which simply restarts all region servers in the cluster.  consider improve it to gracefully restart region servers to avoid the offline time of the regions deployed on that server, and keep the region distributions same as what it was before the restarting. ",
        "label": 501
    },
    {
        "text": "coprocessors  support aggregate functions  chatting with jgray and holstad at the kitchen table about counts, sums, and other aggregating facility, facility generally where you want to calculate some meta info on your table, it seems like it wouldn't be too hard making a filter type that could run a function server-side and return the result only of the aggregation or whatever. for example, say you just want to count rows, currently you scan, server returns all data to client and count is done by client counting up row keys. a bunch of time and resources have been wasted returning data that we're not interested in. with this new filter type, the counting would be done server-side and then it would make up a new result that was the count only (kinda like mysql when you ask it to count, it returns a 'table' with a count column whose value is count of rows). we could have it so the count was just done per region and return that. or we could maybe make a small change in scanner too so that it aggregated the per-region counts. ",
        "label": 199
    },
    {
        "text": "requests count per hregion  path-1 add another mertic for hregion to count request made to region. path-2 add another command to hbase shell to grab all regions, sort by requests from path-1 and move in round-robin algorithm to servers ",
        "label": 441
    },
    {
        "text": "start stop of large cluster untenable  starting and stopping a loaded large cluster is way too flakey and takes too long. this is 0.19.x but same issues apply to trunk i'd say. at pset with our > 100 nodes carrying 6k regions: + shutdown takes way too long.... maybe ten minutes or so. we compact regions inline with shutdown. we should just go down. it doesn't seem like all regionservers go down everytime either.  + startup is a mess with our assigning out regions an rebalancing at same time. by time that the compactions on open run, it can be near an hour before whole thing settles down and becomes useable ",
        "label": 314
    },
    {
        "text": "hbase shell non interactive broken  here is error for command line: $ echo \"list\" | ./hbase shell -n 2017-07-17 08:01:09,442 warn  [main] util.nativecodeloader: unable to load native-hadoop library for your platform... using builtin-java classes where applicable error nomethoderror: undefined method `encoding' for #<io:<stdin>> did you mean?  set_encoding ",
        "label": 320
    },
    {
        "text": "handle splitting merging of regions that have region replication greater than one  ",
        "label": 139
    },
    {
        "text": "fix overcommit to  hbase hbase   last night i made two commits. the first was an overcommit (my tree was dirty). this issue is about fixup. ",
        "label": 314
    },
    {
        "text": "remove the redundant comma from retriesexhaustedwithdetailsexception getdesc  the describe from `retriesexhaustedwithdetailsexception#getdesc` is `  org.apache.hadoop.hbase.client.retriesexhaustedwithdetailsexception: failed 3 actions: failedserverexception: 3 times, `, there is a not need ', ' in the tail. ",
        "label": 68
    },
    {
        "text": "hconstants max row length is incorrectly 64k  should be 32k  the max length of a row has to fit into a signed short. that is 32k. the constant is incorrect, but in the depths of keyvalue it does the right thing:  if (rlength > short.max_value) { throw new illegalargumentexception(\"row > \" + short.max_value); } so your put wont fail and instead it will fail in the keyvalue constructor. so far only that 1 line in put uses this constant, but we should have a correct value here. ",
        "label": 30
    },
    {
        "text": "periodic dependency bump for sep  we should do a pass to see if there are any dependencies we can bump. (also follow-on we should automate this check) ",
        "label": 314
    },
    {
        "text": "improve compatibility warning about hbase with hadoop x  hmaster is not able to start because of the following error  please find the following error   ----------------------------  2012-03-30 11:12:19,487 fatal org.apache.hadoop.hbase.master.hmaster: unhandled exception. starting shutdown.  java.lang.noclassdeffounderror: org/apache/hadoop/hdfs/protocol/fsconstants$safemodeaction  at org.apache.hadoop.hbase.util.fsutils.waitonsafemode(fsutils.java:524)  at org.apache.hadoop.hbase.master.masterfilesystem.checkrootdir(masterfilesystem.java:324)  at org.apache.hadoop.hbase.master.masterfilesystem.createinitialfilesystemlayout(masterfilesystem.java:127)  at org.apache.hadoop.hbase.master.masterfilesystem.<init>(masterfilesystem.java:112)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:496)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:363)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.classnotfoundexception: org.apache.hadoop.hdfs.protocol.fsconstants$safemodeaction  at java.net.urlclassloader$1.run(urlclassloader.java:202)  at java.security.accesscontroller.doprivileged(native method)  at java.net.urlclassloader.findclass(urlclassloader.java:190)  at java.lang.classloader.loadclass(classloader.java:306)  at sun.misc.launcher$appclassloader.loadclass(launcher.java:301)  at java.lang.classloader.loadclass(classloader.java:247)  ... 7 more  there is a change in the fsconstants ",
        "label": 248
    },
    {
        "text": "add apache license header  executing mvn apache-rat:check fails with [error] failed to execute goal org.apache.rat:apache-rat-plugin:0.6:check (default-cli) on project hbase: too many unapproved licenses: 84 -> [help 1]  org.apache.maven.lifecycle.lifecycleexecutionexception: failed to execute goal org.apache.rat:apache-rat-plugin:0.6:check (default-cli) on project hbase: too many unapproved licenses: 84 there are about 70 + files which are missing the apache license headers and rest of them should be added to the exclude list. ",
        "label": 314
    },
    {
        "text": "allow hbase shell command to operate on group of tables  suppose i have the following 20 tables in hbase:  errors-1276213362068-1  errors-1276213362068-2  errors-1276213362068-3  ...  errors-1276213362068-20 hbase shell commands, such as disable and drop, should be able to operate on group of tables which are identified by prefix or suffix. ",
        "label": 375
    },
    {
        "text": "add exists list  in htableinterface to allow multiple parallel exists at one time  we need to have a boolean[] exists(list<get> gets) throws ioexception method implemented in htableinterface. ",
        "label": 230
    },
    {
        "text": "upgrade instructions from to  i cloned this from the 0.96 upgrade docs task. it was suggested that we need upgrade instructions from 0.94 to 0.98. i will need source material to even prioritize this. assuming this is minor. ",
        "label": 330
    },
    {
        "text": "add variable size chunks to the mslab  add possibility to create a variable size chunks of memory, so any cell (of any size) can reside on a chunk. ",
        "label": 35
    },
    {
        "text": "secondary indexes  i'm working on a secondary index impl. the basic idea is to maintain a separate table per index. ",
        "label": 38
    },
    {
        "text": "scanner responses from rs should include metrics on rows kvs filtered  currently it's difficult to know, when issuing a filter, what percentage of rows were skipped by that filter. we should expose some basic counters back to the client scanner object. for example: number of rows filtered by row key alone (filterrowkey()) number of times each filter response was returned by filterkeyvalue() - corresponding to filter.returncode what would be slickest is if this could actually return a tree of counters for cases where filterlist or other combining filters are used. but a top-level is a good start. ",
        "label": 249
    },
    {
        "text": "recoverfilelease does not check return value of recoverlease  i think this is a problem, so i'm opening a ticket so an hbase person takes a look. apache accumulo has moved its write-ahead log to hdfs. i modeled the lease recovery for accumulo after hbase's lease recovery. during testing, we experienced data loss. i found it is necessary to wait until recoverlease returns true to know that the file has been truly closed. in fshdfsutils, the return result of recoverlease is not checked. in the unit tests created to check lease recovery in hbase-2645, the return result of recoverlease is always checked. i think fshdfsutils should be modified to check the return result, and wait until it returns true. ",
        "label": 441
    },
    {
        "text": "refactor rpcserver  break internal class out, remove redundent code, etc. ",
        "label": 149
    },
    {
        "text": "release  let's roll 1.6.0 to get hbase-23174 out on recent branch-1 ",
        "label": 38
    },
    {
        "text": "put regionserverdynamicstatistics under regionserver in mbean hierarchy rather than have it as a peer   ",
        "label": 314
    },
    {
        "text": "remove remaining get code from store java etc  there is still remaining get code due to hbase-2248, remove it! ",
        "label": 547
    },
    {
        "text": "refactor hlog into an interface   what the summary says. create hlog interface. make current implementation use it. ",
        "label": 172
    },
    {
        "text": "error while syncing  dfsoutputstream is closed  in a billion-row load on ~25 servers, i see \"error while syncing\" reasonable often with the error \"dfsoutputstream is closed\" around a roll. we have some race where a roll at the same time as heavy inserts causes a problem. ",
        "label": 286
    },
    {
        "text": "record the class name of writer in wal header so that only proper reader can open the wal file  reported by kiran in this thread: \"hbase file encryption, inconsistencies observed and data loss\" after step 4 ( i.e disabling of wal encryption, removing secureprotobufreader/writer and restart), read of encrypted wal fails mainly due to eof exception at basedecoder. this is not considered as error and these wal are being moved to /oldwals. following is observed in log files: 2014-07-30 19:44:29,254 info  [rs_log_replay_ops-host-16:15264-1] wal.hlogsplitter: splitting hlog: hdfs://host-16:18020/hbase/wals/host-16,15264,1406725441997-splitting/host-16%2c15264%2c1406725441997.1406725444017, length=172 2014-07-30 19:44:29,254 info  [rs_log_replay_ops-host-16:15264-1] wal.hlogsplitter: distributedlogreplay = false 2014-07-30 19:44:29,313 info  [rs_log_replay_ops-host-16:15264-1] util.fshdfsutils: recovering lease on dfs file hdfs://host-16:18020/hbase/wals/host-16,15264,1406725441997-splitting/host-16%2c15264%2c1406725441997.1406725444017 2014-07-30 19:44:29,315 info  [rs_log_replay_ops-host-16:15264-1] util.fshdfsutils: recoverlease=true, attempt=0 on file=hdfs://host-16:18020/hbase/wals/host-16,15264,1406725441997-splitting/host-16%2c15264%2c1406725441997.1406725444017 after 1ms 2014-07-30 19:44:29,429 debug [rs_log_replay_ops-host-16:15264-1-writer-0] wal.hlogsplitter: writer thread thread[rs_log_replay_ops-host-16:15264-1-writer-0,5,main]: starting 2014-07-30 19:44:29,429 debug [rs_log_replay_ops-host-16:15264-1-writer-1] wal.hlogsplitter: writer thread thread[rs_log_replay_ops-host-16:15264-1-writer-1,5,main]: starting 2014-07-30 19:44:29,430 debug [rs_log_replay_ops-host-16:15264-1-writer-2] wal.hlogsplitter: writer thread thread[rs_log_replay_ops-host-16:15264-1-writer-2,5,main]: starting 2014-07-30 19:44:29,591 error [rs_log_replay_ops-host-16:15264-1] codec.basedecoder: partial cell read caused by eof: java.io.ioexception: premature eof from inputstream 2014-07-30 19:44:29,592 info  [rs_log_replay_ops-host-16:15264-1] wal.hlogsplitter: finishing writing output logs and closing down. 2014-07-30 19:44:29,592 info  [rs_log_replay_ops-host-16:15264-1] wal.hlogsplitter: waiting for split writer threads to finish 2014-07-30 19:44:29,592 info  [rs_log_replay_ops-host-16:15264-1] wal.hlogsplitter: split writers finished 2014-07-30 19:44:29,592 info  [rs_log_replay_ops-host-16:15264-1] wal.hlogsplitter: processed 0 edits across 0 regions; log file=hdfs://host-16:18020/hbase/wals/host-16,15264,1406725441997-splitting/host-16%2c15264%2c1406725441997.1406725444017 is corrupted = false progress failed = false to fix this, we need to propagate eof exception to hlogsplitter. any suggestions on the fix? -------- (end of quote from kiran) in basedecoder#rethroweofexception() :     if (!iseof) throw ioex;     log.error(\"partial cell read caused by eof: \" + ioex);     eofexception eofex = new eofexception(\"partial cell read\");     eofex.initcause(ioex);     throw eofex; throwing eofexception would not propagate the \"partial cell read\" condition to hlogsplitter which doesn't treat eofexception as an error. i think ioexception should be thrown above - hlogsplitter#getnextlogline() would translate the ioex to corruptedlogfileexception. ",
        "label": 441
    },
    {
        "text": "loadincrementalhfiles ignores additional configurations  method run ignores configuration, which was passed in as constructor argument: loadincrementalhfiles hfilesmergetask = new loadincrementalhfiles(conf); toolrunner.run(hfilesmergetask, args);  this happens because htable creation (new htable(tablename); in loadincrementalhfiles.run() method) skips existing configuration and tries to create a new one for htable. if there is no hbase-site.xml in classpath, previously loaded properties (via -conf <configuration file>) will be missed. quick fix: --- loadincrementalhfiles.java 2011-07-18 08:20:38.000000000 +0400 +++ loadincrementalhfiles.java 2011-10-19 18:08:31.228972054 +0400 @@ -447,14 +446,20 @@      if (!tableexists) this.createtable(tablename,dirpath);            path hfofdir = new path(dirpath); -    htable table = new htable(tablename); +    htable table; +    configuration configuration = getconf(); +    if (configuration != null) { +      table = new htable(configuration, tablename); +    } else { +      table = new htable(tablename); +    }            dobulkload(hfofdir, table);      return 0;    } ",
        "label": 26
    },
    {
        "text": " visibility controller  replicate the visibility of cells as strings  this issue is aimed at persisting the visibility labels as strings in the wal rather than label ordinals. this would help in replicating the label ordinals to the replication cluster as strings directly and also that after hbase-11553 would help because the replication cluster could have an implementation as string based visibility labels. ",
        "label": 544
    },
    {
        "text": "make hbase examples module  there are some examples under /examples/, which are not compiled as a part of the build. we can move them to an hbase-examples module. ",
        "label": 406
    },
    {
        "text": "a bug in testzkbasedopencloseregion  the following two test cases was executed orderly: 1.testzkbasedopencloseregion#testcloseregion()  2.testzkbasedopencloseregion#testrsalreadyprocessingregion() i found the problem while the two test cases used the same region with little possibility. the following describtion called the region as regiona (1). region-a was closed in the test of \"testcloseregion\". and it was trying to be opening at the end of \"testcloseregion\".    2011-06-20 08:14:24,967 debug [main-eventthread] master.assignmentmanager(374): handling transition=rs_zk_region_opening, server=linux1.site,41784,1308528851644, region=5251635727486eb97dfbe6f953c587c3    2011-06-20 08:14:24,967 debug [rs_open_region-linux1.site,41784,1308528851644-2] regionserver.hregion(311): instantiated testzkbasedopencloseregion,ccc,1308528860373.5251635727486eb97dfbe6f953c587c3.    2011-06-20 08:14:24,986 info  [thread-397] master.testzkbasedopencloseregion(213): done with testcloseregion (2). the region next test case used was just the same region which was opening:    2011-06-20 08:14:25,012 info  [main] master.testzkbasedopencloseregion(139): .meta.,,1.1028785192    2011-06-20 08:14:25,013 info  [main] master.testzkbasedopencloseregion(139): testzkbasedopencloseregion,ccc,1308528860373.5251635727486eb97dfbe6f953c587c3. (3) in test case 2, the code of \"while (!reopeneventprocessed.get())\" got an un-expect opened event from the prev opening.     eventhandlerlistener openlistener =     new reopeneventlistener(hri.getregionnameasstring(),           reopeneventprocessed, eventtype.rs_zk_region_opened);     cluster.getmaster().executorservice.       registerlistener(eventtype.rs_zk_region_opened, openlistener);     // now ask the master to move the region to hr1, will fail     test_util.gethbaseadmin().move(hri.getencodednameasbytes(),         bytes.tobytes(hr1.getservername()));          while (!reopeneventprocessed.get()) {       threads.sleep(100);     }    2011-06-20 08:14:25,032 debug [master_open_region-linux1.site:34061-4] handler.openedregionhandler(108): opened region testzkbasedopencloseregion,ccc,1308528860373.5251635727486eb97dfbe6f953c587c3. on linux1.site,41784,1308528851644    2011-06-20 08:14:25,033 info  [master_open_region-linux1.site:34061-4] master.testzkbasedopencloseregion$reopeneventlistener(170): afterprocess(org.apache.hadoop.hbase.master.handler.openedregionhandler@711185e7)    2011-06-20 08:14:25,033 info  [master_open_region-linux1.site:34061-4] master.testzkbasedopencloseregion$reopeneventlistener(172): finished processing rs_zk_region_opened so it think the region was opened, but really not. (4) so the test failed.    java.lang.reflect.undeclaredthrowableexception    at $proxy24.move(unknown source)    at org.apache.hadoop.hbase.client.hbaseadmin.move(hbaseadmin.java:978)    at org.apache.hadoop.hbase.master.testzkbasedopencloseregion.testrsalreadyprocessingregion(testzkbasedopencloseregion.java:291)     caused by: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hbase.unknownregionexception: 5251635727486eb97dfbe6f953c587c3    at org.apache.hadoop.hbase.master.hmaster.move(hmaster.java:725) exchange the position of the two test cases will solve the problem. ",
        "label": 240
    },
    {
        "text": "aggregateclient validateparameters can throw npe  when using methods such as max(...), min(...) in aggregationclient\uff0c we will pass scan as parameter. these methods will throw nullpointerexception if users invoke scan.setstartrow(null) or scan.setstoprow(null) before passing the scan as parameter. the nullpointerexception is thrown by validateparameters(scan scan) which will be invoked before sending requests to server. the implementation of validateparameters is :   private void validateparameters(scan scan) throws ioexception {     if (scan == null         || (bytes.equals(scan.getstartrow(), scan.getstoprow()) && !bytes             .equals(scan.getstartrow(), hconstants.empty_start_row))         || ((bytes.compareto(scan.getstartrow(), scan.getstoprow()) > 0) &&          !bytes.equals(scan.getstoprow(), hconstants.empty_end_row))) {       throw new ioexception(           \"agg client exception: startrow should be smaller than stoprow\");     } else if (scan.getfamilymap().size() != 1) {       throw new ioexception(\"there must be only one family.\");     }   } \u201cbytes.equals(scan.getstartrow(), hconstants.empty_start_row)\u201d will throw nullpointerexception if the startrow of scan is set to null. ",
        "label": 238
    },
    {
        "text": "split parents are reassigned on restart and on disable enable  j-d found this nice bug testing. ",
        "label": 314
    },
    {
        "text": "avoid using ' tmp' directory in testbulkload  ",
        "label": 198
    },
    {
        "text": "robust start and stop for cluster  regionserver  when a new region server is started, it should have a period during which it will retry to connect to the master, rather than just immediately shutting down when it can't contact the master. also, when hbase and hdfs are started at around the same time, hbase shouldn't immediately exit. instead, wait a little while for everything to come online. ",
        "label": 241
    },
    {
        "text": "master dying while going to close a region can leave it in transition forever  i saw this in the aftermath of hbase-4729 on a 0.92 refreshed yesterday, when the master died it had just created the rit znode for a region but didn't tell the rs to close it yet. when the master restarted it saw the znode and started printing this: 2011-11-03 00:02:49,130 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: testtable,0007560564,1320253568406.f76899564cabe7e9857c3aeb526ec9dc. state=closing, ts=1320253605285, server=sv4r11s38,62003,1320195046948  2011-11-03 00:02:49,130 info org.apache.hadoop.hbase.master.assignmentmanager: region has been closing for too long, this should eventually complete or the server will expire, doing nothing it's never going to happen, and it's blocking balancing. i'm marking this as minor since i believe this situation is pretty rare unless you hit other bugs while trying out stuff to root bugs out. ",
        "label": 529
    },
    {
        "text": "add back support for running chaosmonkey as standalone tool  according to the ref guide, it was once possible to run chaosmonkey as a standalone tool against a deployed cluster. after 0.94, this is no longer possible. ",
        "label": 504
    },
    {
        "text": "filter evaluates kvs outside requested columns  1- fill row with some columns  2- get row with some columns less than universe - use filter to print kvs  3- filter prints not requested columns filter (allwaysnextcolfilter) always return returncode.include_and_next_col and prints kv's qualifier suffix_0 = 0  suffix_1 = 1  suffix_4 = 4  suffix_6 = 6 p= persisted  r= requested  e= evaluated  x= returned 5580   5581   5584   5586   5590   5591   5594   5596   5600   5601   5604   5606  ...    p   p       p   p       p   p    ...    r   r   r     r   r   r          ...    e   e       e   e       e      ...    x   x       x   x            extracolumntest.java     @test     public void testfilter() throws exception {         configuration config = hbaseconfiguration.create();         config.set(\"hbase.zookeeper.quorum\", \"myzk\");         htable htable = new htable(config, \"testtable\");         byte[] cf = bytes.tobytes(\"cf\");         byte[] row = bytes.tobytes(\"row\");         byte[] col1 = new qualifierconverter().objecttobytearray(new qualifier((short) 558, (byte) suffix_1));         byte[] col2 = new qualifierconverter().objecttobytearray(new qualifier((short) 559, (byte) suffix_1));         byte[] col3 = new qualifierconverter().objecttobytearray(new qualifier((short) 560, (byte) suffix_1));         byte[] col4 = new qualifierconverter().objecttobytearray(new qualifier((short) 561, (byte) suffix_1));         byte[] col5 = new qualifierconverter().objecttobytearray(new qualifier((short) 562, (byte) suffix_1));         byte[] col6 = new qualifierconverter().objecttobytearray(new qualifier((short) 563, (byte) suffix_1));         byte[] col1g = new qualifierconverter().objecttobytearray(new qualifier((short) 558, (byte) suffix_6));         byte[] col2g = new qualifierconverter().objecttobytearray(new qualifier((short) 559, (byte) suffix_6));         byte[] col1v = new qualifierconverter().objecttobytearray(new qualifier((short) 558, (byte) suffix_4));         byte[] col2v = new qualifierconverter().objecttobytearray(new qualifier((short) 559, (byte) suffix_4));         byte[] col3v = new qualifierconverter().objecttobytearray(new qualifier((short) 560, (byte) suffix_4));         byte[] col4v = new qualifierconverter().objecttobytearray(new qualifier((short) 561, (byte) suffix_4));         byte[] col5v = new qualifierconverter().objecttobytearray(new qualifier((short) 562, (byte) suffix_4));         byte[] col6v = new qualifierconverter().objecttobytearray(new qualifier((short) 563, (byte) suffix_4));         // =========== insertion =============//         put put = new put(row);         put.add(cf, col1, bytes.tobytes((short) 1));         put.add(cf, col2, bytes.tobytes((short) 1));         put.add(cf, col3, bytes.tobytes((short) 3));         put.add(cf, col4, bytes.tobytes((short) 3));         put.add(cf, col5, bytes.tobytes((short) 3));         put.add(cf, col6, bytes.tobytes((short) 3));         htable.put(put);         put = new put(row);         put.add(cf, col1v, bytes.tobytes((short) 10));         put.add(cf, col2v, bytes.tobytes((short) 10));         put.add(cf, col3v, bytes.tobytes((short) 10));         put.add(cf, col4v, bytes.tobytes((short) 10));         put.add(cf, col5v, bytes.tobytes((short) 10));         put.add(cf, col6v, bytes.tobytes((short) 10));         htable.put(put);         htable.flushcommits();         //==============reading=================//         filter allwaysnextcolfilter = new allwaysnextcolfilter();         get get = new get(row);         get.addcolumn(cf, col1); //5581         get.addcolumn(cf, col1v); //5584         get.addcolumn(cf, col1g); //5586         get.addcolumn(cf, col2); //5591         get.addcolumn(cf, col2v); //5594                 get.addcolumn(cf, col2g); //5596                  get.setfilter(allwaysnextcolfilter);         get.setmaxversions(1);         system.out.println(get);         scan scan = new scan(get);         resultscanner scanner = htable.getscanner(scan);         iterator<result> iterator = scanner.iterator();         system.out.println(\"scan\");         while (iterator.hasnext()) {             result next = iterator.next();             for (keyvalue kv : next.list()) {                 system.out.println(new qualifierconverter().bytearraytoobject(kv.getqualifier()));             }         }     } } requested 5581 5584 5586 5591 5594 5596  not requested: 5561 sysout filter \\x00\\x00\\x1a\\xbe\\x00\\x05^:\\x00\\x00\\xa0x\\x00\\x00=\\x1a/h0:\\x02.\\x01/1373577819267/put/vlen=2/ts=2 qualifier{date=558, type=suffix_1} \\x00\\x00\\x1a\\xbe\\x00\\x05^:\\x00\\x00\\xa0x\\x00\\x00=\\x1a/h0:\\x02.\\x02/1373577819272/put/vlen=2/ts=3 qualifier{date=558, type=suffix_4} \\x00\\x00\\x1a\\xbe\\x00\\x05^:\\x00\\x00\\xa0x\\x00\\x00=\\x1a/h0:\\x02/\\x01/1373577819267/put/vlen=2/ts=2 ualifier{date=559, type=suffix_1} \\x00\\x00\\x1a\\xbe\\x00\\x05^:\\x00\\x00\\xa0x\\x00\\x00=\\x1a/h0:\\x02/\\x02/1373577819272/put/vlen=2/ts=3 qualifier{date=559, type=suffix_4}   \\x00\\x00\\x1a\\xbe\\x00\\x05^:\\x00\\x00\\xa0x\\x00\\x00=\\x1a/h0:\\x020\\x01/1373577819267/put/vlen=2/ts=2 qualifier{date=560, type=suffix_1} (date 5601 not requested but evaluated) sysout extracolumntest {\"timerange\":[0,9223372036854775807],\"totalcolumns\":6,\"cacheblocks\":true,\"families\":{\"h0\":[\"\\\\x02.\\\\x01\",\"\\\\x02.\\\\x02\",\"\\\\x02.\\\\x06\",\"\\\\x02/\\\\x01\"]},\"maxversions\":1,\"filter\":\"allwaysnextcolfilter\",\"row\":\"\\\\x00\\\\x00\\\\x1a\\\\xbe\\\\x00\\\\x05^:\\\\x00\\\\x00\\\\xa0x\\\\x00\\\\x00=\\\\x1a\"} scan qualifier{date=558, type=suffix_1} qualifier{date=558, type=suffix_4} qualifier{date=559, type=suffix_1} qualifier{date=559, type=suffix_4} ",
        "label": 464
    },
    {
        "text": "blockcachekey tostring performance    @override   public string tostring() {     return string.format(\"%s_%d\", hfilename, offset);   } i found through bench-marking that the following code is 10x faster.   @override   public string tostring() {     return hfilename.concat(\"_\").concat(long.tostring(offset));   } normally it wouldn't matter for a tostring() method, but this is comes into play because memcachedblockcache uses it. memcachedblockcache.java   @override   public void cacheblock(blockcachekey cachekey, cacheable buf) {     if (buf instanceof hfileblock) {       client.add(cachekey.tostring(), max_size, (hfileblock) buf, tc);     } else {       if (log.isdebugenabled()) {         log.debug(\"memcachedblockcache can not cache cacheable's of type \"             + buf.getclass().tostring());       }     }   } ",
        "label": 130
    },
    {
        "text": "make sure we have all the hadoop fixes in our our copy of its rpc  our rpc was different from hadoops because we fixed some bugs \u2013 i.e. that buffers in rpc would keep the shape of the largest ever allocation and not snap back to original size, that rpc would never timeout \u2013 and we used send codes instead of method names. the latter has been removed recently from hbase rpc and the above two fixes have been committed to hadoop (iiuc). so, this issue about reviewing our rpc to see that we have all good fixes that are currently up in hadoop and to look at perhaps using hadoop rpc again, directly, because the reason to have our own has perhaps dissipated. ",
        "label": 453
    },
    {
        "text": "trim down supplemental models file for unnecessary entries  with the more permissive \"apache license\" check in hbase-18033, we can remove many entries from the supplemental-models.xml file. this issue is to track that work separately. ",
        "label": 320
    },
    {
        "text": "nullpointerexception when launching the balancer due to unknown region location  i don't reproduce this all the time, but i had it on a fairly clean env.  it occurs every 5 minutes (i.e. the balancer period). impact is severe: the balancer does not run. when it starts to occurs, it occurs all the time. i haven't tried to restart the master, but i think it should be enough.  now, looking at the code, the npe is strange. 2013-04-18 08:09:52,079 error [box,60000,1366281581983-balancerchore] org.apache.hadoop.hbase.master.balancer.balancerchore: caught exception java.lang.nullpointerexception at org.apache.hadoop.hbase.master.balancer.baseloadbalancer$cluster.<init>(baseloadbalancer.java:145) at org.apache.hadoop.hbase.master.balancer.stochasticloadbalancer.balancecluster(stochasticloadbalancer.java:194) at org.apache.hadoop.hbase.master.hmaster.balance(hmaster.java:1295) at org.apache.hadoop.hbase.master.balancer.balancerchore.chore(balancerchore.java:48) at org.apache.hadoop.hbase.chore.run(chore.java:81) at java.lang.thread.run(thread.java:662) 2013-04-18 08:09:52,103 debug [box,60000,1366281581983-catalogjanitor] org.apache.hadoop.hbase.client.clientscanner: creating scanner over .meta. starting at key ''           if (regionfinder != null) {             //region location             list<servername> loc = regionfinder.gettopblocklocations(region);             regionlocations[regionindex] = new int[loc.size()];             for (int i=0; i < loc.size(); i++) {               regionlocations[regionindex][i] = serverstoindex.get(loc.get(i));  // <========= npe here             }           } pinging enis soztutar, just in case. ",
        "label": 441
    },
    {
        "text": "cluster uuid is not properly parsable after rewriting to pb   i am facing this problem while testing hbase-8348(migration).  ->i have started two clusters with 0.94 version to replicate data from one cluster to other. when we have started master,writing cluster id without pb.   -> killed both the clusters when replication in progress  -> running migration script to migrate hdfs and zk data to 0.96.0(no changes in cluster id.)  -> starting 0.96.0 processes.   then not able to start region server. java.io.ioexception: region server startup failed at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:2629) at org.apache.hadoop.hbase.regionserver.hregionserver.handlereportfordutyresponse(hregionserver.java:1202) at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:802) at java.lang.thread.run(thread.java:662) caused by: java.lang.numberformatexception: for input string: \" $411f611e at java.lang.numberformatexception.forinputstring(numberformatexception.java:48) at java.lang.long.parselong(long.java:410) at java.lang.long.valueof(long.java:498) at java.lang.long.decode(long.java:641) at java.util.uuid.fromstring(uuid.java:208) at org.apache.hadoop.hbase.zookeeper.zkclusterid.getuuidforcluster(zkclusterid.java:90) at org.apache.hadoop.hbase.replication.regionserver.replication.initialize(replication.java:127) at org.apache.hadoop.hbase.regionserver.hregionserver.newreplicationinstance(hregionserver.java:2248) at org.apache.hadoop.hbase.regionserver.hregionserver.createnewreplicationinstance(hregionserver.java:2219) at org.apache.hadoop.hbase.regionserver.hregionserver.setupwalandreplication(hregionserver.java:1423) at org.apache.hadoop.hbase.regionserver.hregionserver.handlereportfordutyresponse(hregionserver.java:1190) ... 2 more       try {         clusterid = clusterid.parsefrom(content);       } catch (deserializationexception e) {         throw new ioexception(\"content=\" + bytes.tostring(content), e);       }       // if not pb'd, make it so.       if (!protobufutil.ispbmagicprefix(content)) rewriteaspb(fs, rootdir, idpath, clusterid); ",
        "label": 543
    },
    {
        "text": "'major' compactions and upper bound on files we compact at any one time  from billy in hbase-64, which we closed because it got pulled all over the place: currently we do compaction on a region when the hbase.hstore.compactionthreshold is reached - default 3 i thank we should configure a max number of mapfiles to compact at one time simulator to doing a minor compaction in bigtable. this keep compaction's form getting tied up in one region to long letting other regions get way to many memcache flushes making compaction take longer and longer for each region if we did that when a regions updates start to slack off the max number will eventuly include all mapfiles causeing a major compaction on that region. unlike big table this would leave the master out of the process and letting the region server handle the major compaction when it has time. when doing a minor compaction on a few files i thank we should compact the newest mapfiles first leave the larger/older ones for when we have low updates to a region. ",
        "label": 73
    },
    {
        "text": "bulk assignment on startup runs serially through the cluster servers assigning in bulk to one at a time  multi-thread the bulk startup assignment of regions. ",
        "label": 314
    },
    {
        "text": "data loss after snapshot restore into cloned table  take snapshot s1 of table t1 which has some data  drop t1  clone snapshot s1 to t1  disable t1  restore s1 to t1  enable t1 at this moment, scan 't1' returns nothing. ",
        "label": 309
    },
    {
        "text": "improve master split logging  i'm looking at a log around a regionserver crash on top of hdfs errors and am having trouble deciphering whats going on. would help if hlog.split said who was running the split \u2013 the meta scanner or the shutdown processing \u2013 and would help if messages like \"2008-05-18 20:22:28,183 debug org.apache.hadoop.hbase.hlog: applied 30001 total edits\" said where (or from which file) the edits had been applied. it seems that there are two ongoing splits happening, two different tables. ",
        "label": 314
    },
    {
        "text": "organize performanceevaluation usage output  performanceevaluation has enjoyed a good bit of attention recently. all the new features are muddled together. it would be nice to organize the output of the options list according to some scheme. i was thinking you're group entries by when they're used. for example general options nomapred rows onecon ... table creation/write tests compress flushcommits valuezipf ... read tests filterall multiget replicas ... ",
        "label": 330
    },
    {
        "text": "normalizer switch in configuration is not used  the newly added global switch to enable the new normalizer functionality is never used apparently, meaning it is always on. the hbase-default.xml has this:   <property>     <name>hbase.normalizer.enabled</name>     <value>false</value>     <description>if set to true, master will try to keep region size       within each table approximately the same.</description>   </property> but only a test class uses it to set the switch to \"true\". we should implement a proper if statement that checks this value and properly disables the feature cluster wide if not wanted. ",
        "label": 517
    },
    {
        "text": "regionmover's region server hostname option is no longer case insensitive  with the move from the ruby-based to java-based regionmover implementation, it appears that the case-insensitivity \"feature\" was dropped. if the user provides a rs hostname in the wrong case, the class will fail to perform its actions. dns hostnames are case insensitive, so this would be nice to restore. ",
        "label": 407
    },
    {
        "text": "generate changes md and releasenotes md for  ",
        "label": 149
    },
    {
        "text": "m r tool to replay wal files  just an idea i had. might be useful for restore of a backup using the hlogs.  this could an m/r (with a mapper per hlog file). the tool would get a timerange and a (set of) table(s). we'd pick the right hlogs based on time before the m/r job is started and then have a mapper per hlog file.  the mapper would then go through the hlog, filter all waledits that didn't fit into the time range or are not any of the tables and then uses hfileoutputformat to generate hfiles.  would need to indicate the splits we want, probably from a live table. ",
        "label": 286
    },
    {
        "text": "table name isn't checked in istableenabled istabledisabled  currently when we enable or disable a table in the shell, since we don't verify the table name with htabledescriptor.islegaltablename in istableenabled and istabledisabled, we get the following exception: error: java.lang.illegalargumentexception: invalid path string \"/jdcryans180/table/testtable \" caused by invalid charater @28 this is coming out of zookeeper. instead we should check the table name ourselves in order to give a better feedback. ",
        "label": 441
    },
    {
        "text": "ability to batch edits destined to different regions  the old (pre-pb) \"multi\" and \"multiput\" rpcs allowed one to batch edits destined to different regions. seems like we've lost this ability after the switch to protobufs. the multirequest only contains one regionspecifier, and a list of multiaction. the multiaction message is contains either a single mutationproto or a get (but not both \u2013 so its name is misleading as there is nothing \"multi\" about it). also it seems redundant with multigetrequest, i'm not sure what's the point of supporting get in multiaction. i propose that we change multirequest to be a just a list of multiaction, and multiaction will contain the regionspecifier, the bool atomic and a list of mutationproto. this would be a non-backward compatible protobuf change. if we want we can support mixing edits and reads, in which case we'd also add a list of get in multiaction, and we'd have support having both that list and the list of mutationproto set at the same time. but this is a bonus and can be done later (in a backward compatible manner, hence no need to rush on this one). ",
        "label": 314
    },
    {
        "text": "wrong request sec in the gui reporting wrong  i am seeing lower number of request in the masters gui then i have seen in 0.18.0 while scanning.  i thank part of it is we moved to report per sec request not per 3 secs so the request should be 1/3 of the old numbers i was getting. hbase.client.scanner.caching is not the reason the request are under reported.  i set hbase.client.scanner.caching = 1 and still get about 2k request a sec in the gui  but when the job is done i take records / job time and get 36,324/ records /sec. so  there must be some caching out side of the hbase.client.scanner.caching making the  request per sec lower then it should be. i know it running faster then reported just thought  it might give some new users the wrong impression that request/sec = read/write /sec. ",
        "label": 229
    },
    {
        "text": "region mover script can get stuck in infinite loop if unknownregionexception  13/05/02 23:56:55 info region_mover: moving 1 region(s) from procyon-8.nova.cloudera.com,60020,1367530634629 during this cycle 13/05/02 23:56:55 info region_mover: moving region 588ccd887c699c7c23188a79a3d1807f (0 of 1) to server=procyon-5.nova.cloudera.com,60020,1367542172830 13/05/02 23:56:55 info region_mover: exception moving 588ccd887c699c7c23188a79a3d1807f; split/moved? continuing: java.lang.reflect.undeclaredthrowableexception: org.apache.hadoop.hbase.unknownregionexception: 588ccd887c699c7c23188a79a3d1807f at org.apache.hadoop.hbase.master.hmaster.move(hmaster.java:1015) at sun.reflect.generatedmethodaccessor22.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1345) this was triggered due illegalstateexception while trying to open an unexisting region: 2013-05-02 16:23:06,558 info org.apache.hadoop.hbase.regionserver.hregionserver: received request to open region: testtable,,1367530404417.588ccd887c699c7c23188a79a3d1 807f. 2013-05-02 16:23:06,583 info org.apache.hadoop.hbase.util.fsutils: hdfs://nameservice1/hbase/testtable doesn't exist 2013-05-02 16:23:06,586 info org.apache.hadoop.hbase.util.fsutils: hdfs://nameservice1/hbase/testtable doesn't exist 2013-05-02 16:23:06,586 warn org.apache.hadoop.hbase.util.fstabledescriptors: the following folder is in hbase's root directory and doesn't contain a table descriptor,  do consider deleting it: testtable 2013-05-02 16:23:06,666 error org.apache.hadoop.hbase.regionserver.handler.openregionhandler: failed open of region=testtable,,1367530404417.588ccd887c699c7c23188a79a3 d1807f. java.lang.illegalstateexception: could not instantiate a region instance. at org.apache.hadoop.hbase.regionserver.hregion.newhregion(hregion.java:3123) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:3254) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.openregion(openregionhandler.java:331) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:107) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:169) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.lang.reflect.invocationtargetexception at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27) at java.lang.reflect.constructor.newinstance(constructor.java:513) at org.apache.hadoop.hbase.regionserver.hregion.newhregion(hregion.java:3120) ... 7 more caused by: java.lang.nullpointerexception at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.loadtablecoprocessors(regioncoprocessorhost.java:133) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.<init>(regioncoprocessorhost.java:125) at org.apache.hadoop.hbase.regionserver.hregion.<init>(hregion.java:376) ... 12 more ",
        "label": 163
    },
    {
        "text": "testdrainingserver expects round robin region assignment but misses a config parameter  from https://builds.apache.org/view/g-l/view/hbase/job/hbase-trunk/2595/testreport/org.apache.hadoop.hbase/testdrainingserver/org_apache_hadoop_hbase_testdrainingserver/, we can see that some region server didn't have any regions assigned.     // assert that every regionserver has some regions on it. it turns out that bulkenabler has an internal knob whose value is false:       boolean roundrobinassignment = this.server.getconfiguration().getboolean(           \"hbase.master.enabletable.roundrobin\", false); testdrainingserver should have set this config parameter before calling admin.enabletable(tablename) ",
        "label": 441
    },
    {
        "text": "remove admin deletesnapshot byte   because update to new version, we need to remove the deprecated function admin.deletesnapshot(byte[])  there are some testing also use this function, so we need to change it to admin.deletesnapshot(string[]) ",
        "label": 277
    },
    {
        "text": "make sure we always return the scanner id with scanresponse  some external tooling (like opentsdb) relies on the scanner id to tie asynchronous responses back to their requests. (see comments on hbase-17489) ",
        "label": 149
    },
    {
        "text": "htable   put put    put list put  code duplication  do exactly the same thing except for aggregation. internal method doput() added to which the actual implementation is delegated to. ",
        "label": 266
    },
    {
        "text": "automagically tweak global memstore and block cache sizes based on workload  hypertable does a neat thing where it changes the size given to the cellcache (our memstores) and block cache based on the workload. if you need an image, scroll down at the bottom of this link: http://www.hypertable.com/documentation/architecture/ that'd be one less thing to configure. ",
        "label": 46
    },
    {
        "text": "make tablesnapshotscanner audience private  from hbase-10462 api cleanup. tablesnapshotscanner should be audience private. it should not be public. see hbase-10462 for enis soztutar rationale. if private, then there needs to be a way to construct it. it is awkward since we normally construct 'table' by getting one from a connection. enis suggests: maybe we can do a snapshotconnection which returns a connectiontable, which returns the scanner. since connection is the factory to everything, i think this approach is the most clean. sounds good. ",
        "label": 98
    },
    {
        "text": "add support for doing get scans against a particular replica id  we have support for doing an rpc against the primary region in get/scans and also new functionality for doing failover get/scan rpcs trying primary first then doing backup rpcs to secondaries when the first one times out. we can add functionality for doing get/scan against a particular replica_id so that we can use it in tests, or by clients in some specific situations. we can think about adding support for querying the replica_id from get/scan responses coming from failover rpcs so that the client can then issue a follow up request against that particular replica_id. ",
        "label": 233
    },
    {
        "text": " backport  hbase to branch  ",
        "label": 59
    },
    {
        "text": "don't retry increments on an invalid cell  this says it all: error: org.apache.hadoop.hbase.client.retriesexhaustedexception: failed after attempts=7, exceptions: thu jun 28 18:34:44 utc 2012, org.apache.hadoop.hbase.client.htable$8@4eabaf8c, java.io.ioexception: java.io.ioexception: attempted to increment field that isn't 64 bits wide hregion should be modified here to send a donotretryioexception: if (wronglength) {   throw new donotretryioexception(     \"attempted to increment field that isn't 64 bits wide\"); } ",
        "label": 286
    },
    {
        "text": "remove the  cache hit for row  message  i don't see any reason why we should keep this message: client.hconnectionmanager$hconnectionimplementation: cache hit for row <0003706355> in tablename testtable: location server sv2borg176:60020, location region name testtable,0003706355,1302908760305.7ac9fae7facc8a69330340f1c3977339. it happens only when we are looking for a region in the hcm cache and the row key we use is the start key of that region. i don't just find it useless, also it litters our thrift logs. ",
        "label": 229
    },
    {
        "text": "should sanity check table configuration when clone snapshot to a new table  hbase-12570 imporved table configuration sanity checking. but it only worked for create table or alter table. should check table configuration too when clone snapshot to a new table. ",
        "label": 187
    },
    {
        "text": "optimise the storage that we use for storing mvcc information   ",
        "label": 34
    },
    {
        "text": " hbase operator tools  add checkstyle plugin and configs from hbase  hbase-operator-tools does not include checkstyle plugin in pom. in particular, does not include the checkstyle config that is in hbase so when yetus does a check of operator-tools in a pr, checkstyle finds a massive amount of violations as the default checkstyle config is strict (e.g. 80 character line lengths rather than 100 as we have in hbase). ",
        "label": 226
    },
    {
        "text": "hbaseadmin may leak zookeeper connections  when master crashs, hbaseadmin will leaks zookeeper connections  i think we should close the zk connetion when throw masternotrunningexception  public hbaseadmin(configuration c)  throws masternotrunningexception, zookeeperconnectionexception {  this.conf = hbaseconfiguration.create(c);  this.connection = hconnectionmanager.getconnection(this.conf);  this.pause = this.conf.getlong(\"hbase.client.pause\", 1000);  this.numretries = this.conf.getint(\"hbase.client.retries.number\", 10);  this.retrylongermultiplier = this.conf.getint(\"hbase.client.retries.longer.multiplier\", 10);  //we should add this code and close the zk connection  try { this.connection.getmaster(); } catch(masternotrunningexception e) { hconnectionmanager.deleteconnection(conf, false); throw e; } } ",
        "label": 557
    },
    {
        "text": "make zookeeperlistener abstract  org.apache.hadoop.hbase.zookeeper.zookeeperlistener seems to have all unimplemented methods. this should be made an abstract class. ",
        "label": 11
    },
    {
        "text": "htable flushcommits clears write buffer in finally clause  metthod flushcommits clears the write buffer in a finally clause. when using the write buffer, if the call to processbatchofrows done in flushcommits throws an exception, the write buffer will be cleared thus potentially leading to loss of data on the client side. ",
        "label": 38
    },
    {
        "text": "hbase broke build on hudson   jim, you want to take a look at it? 841 changed interfaces. changed interfaces can make for odd issues like the hangs exhibited up on hudson (stuff is failing for me on my laptop since about the commit 841... timeouts. i don't have same issue on branch). ",
        "label": 241
    },
    {
        "text": "hbck dies on npe when a region folder exists but the table does not  this is what i'm getting for leftover data that has no .regioninfo first: 12/07/17 23:13:37 warn util.hbasefsck: failed to read .regioninfo file for region null  java.io.filenotfoundexception: file does not exist: /hbase/stumble_info_urlid_user/bd5f6cfed674389b4d7b8c1be227cb46/.regioninfo  at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.openinfo(dfsclient.java:1822)  at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.<init>(dfsclient.java:1813)  at org.apache.hadoop.hdfs.dfsclient.open(dfsclient.java:544)  at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:187)  at org.apache.hadoop.fs.filesystem.open(filesystem.java:456)  at org.apache.hadoop.hbase.util.hbasefsck.loadhdfsregioninfo(hbasefsck.java:611)  at org.apache.hadoop.hbase.util.hbasefsck.access$2200(hbasefsck.java:140)  at org.apache.hadoop.hbase.util.hbasefsck$workitemhdfsregioninfo.call(hbasefsck.java:2882)  at org.apache.hadoop.hbase.util.hbasefsck$workitemhdfsregioninfo.call(hbasefsck.java:2866)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:98)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:206)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) then it hangs on: 12/07/17 23:13:39 info util.hbasefsck: attempting to handle orphan hdfs dir: hdfs://sfor3s24:10101/hbase/stumble_info_urlid_user/bd5f6cfed674389b4d7b8c1be227cb46  12/07/17 23:13:39 info util.hbasefsck: checking orphan for table null  exception in thread \"main\" java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.hbasefsck$tableinfo.access$100(hbasefsck.java:1634)  at org.apache.hadoop.hbase.util.hbasefsck.adopthdfsorphan(hbasefsck.java:435)  at org.apache.hadoop.hbase.util.hbasefsck.adopthdfsorphans(hbasefsck.java:408)  at org.apache.hadoop.hbase.util.hbasefsck.restorehdfsintegrity(hbasefsck.java:529)  at org.apache.hadoop.hbase.util.hbasefsck.offlinehdfsintegrityrepair(hbasefsck.java:313)  at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:386)  at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:3227) the npe is sent by: preconditions.checknotnull(\"table \" + tablename + \"' not present!\", tableinfo); i wonder why the condition checking was added if we don't handle it... in any case hbck dies but it hangs because there are some non-daemon hanging around. ",
        "label": 248
    },
    {
        "text": "'mvn site' is broken due to org apache jasper jspc not found  from https://builds.apache.org/job/precommit-hbase-build/8642/artifact/trunk/patchprocess/patchsiteoutput.txt : [warning] the pom for org.apache.hbase:hbase-server:jar:0.99.0-20140127.165302-1 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details [warning] the pom for org.apache.hbase:hbase-server:jar:tests:0.99.0-20140127.165302-1 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details ... [error] failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:site (default-site) on project hbase: failed to get report for org.apache.maven.plugins:maven-javadoc-plugin: failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (generate) on project hbase-thrift: an ant buildexception has occured: taskdef class org.apache.jasper.jspc cannot be found ",
        "label": 38
    },
    {
        "text": "book  update our hadoop vendor section  ",
        "label": 314
    },
    {
        "text": "a few hbase procedure classes missing  interfaceaudience annotation  noopprocedurestore.java  procedurestorebase.java  proceduremetrics.java  lockstatus.java  lockandqueue.java  procedurestateserializer.java ",
        "label": 187
    },
    {
        "text": "port hbase  reduce unnecessary getfilestatus hdfs calls in ttl hfile and hlog cleanners to  for each in file in archive dir, the timetolivehfilecleaner need call getfilestatus to get the modify time of file. actually the cleanerchore have had the file status when listing its parent dir. when we set the ttl to 7 days in our cluster for data security, the number of files left in archive dir is up to 65 thousands. in each clean period, timetolivehfilecleaner will generate ten thousand getfilestatus call in a short time, which is very heavy for hdfs namenode. fix is to change the path param to filestatus in isfiledeletable method and reduce unnecessary getfilestatus hdfs calls in ttl cleaners. this issue is to backport this improvement to 0.94 ",
        "label": 441
    },
    {
        "text": "shaded jar modules create spurious source and test jars with incorrect license notice info  the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than meta-inf) currently we create the test jars are missing license source jars have license/notice files that claim all the bundled works in the normal jar. hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/ hbase-shaded-server-1.1.2-sources.jar/ hbase-shaded-server-1.1.2-sources.jar//meta-inf hbase-shaded-server-1.1.2-sources.jar//meta-inf/license hbase-shaded-server-1.1.2-sources.jar//meta-inf/manifest.mf hbase-shaded-server-1.1.2-sources.jar//meta-inf/notice hbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/ hbase-shaded-client-1.1.2-sources.jar/ hbase-shaded-client-1.1.2-sources.jar//meta-inf hbase-shaded-client-1.1.2-sources.jar//meta-inf/license hbase-shaded-client-1.1.2-sources.jar//meta-inf/manifest.mf hbase-shaded-client-1.1.2-sources.jar//meta-inf/notice hbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/ hbase-shaded-client-1.1.2-tests.jar/ hbase-shaded-client-1.1.2-tests.jar//meta-inf hbase-shaded-client-1.1.2-tests.jar//meta-inf/notice hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/ hbase-shaded-server-1.1.2-tests.jar/ hbase-shaded-server-1.1.2-tests.jar//meta-inf hbase-shaded-server-1.1.2-tests.jar//meta-inf/notice ",
        "label": 402
    },
    {
        "text": "tableinputformat is ignoring input scan's stop row setting  in the getsplits() function of tableinputformatbase, there is a line to compare the region's stop key and the given scan's stop key. if the region's stop key is smaller, then use the region's stop key instead of the one given by user. but the last region will not have a stop key, so comparing a null key to the given stop key is always smaller, and the user's stop key is simply ignored. ",
        "label": 400
    },
    {
        "text": "oome on master splitting logs  stuck  won't go down  this is holding it up. \"hmaster\" prio=10 tid=0x000000004048c000 nid=0x6ab0 in object.wait() [0x0000000040d6c000..0x0000000040d6cd00]    java.lang.thread.state: waiting (on object monitor)         at java.lang.object.wait(native method)         at java.lang.thread.join(unknown source)         - locked <0x00007fc6d28be720> (a org.apache.hadoop.hbase.master.rootscanner)         at java.lang.thread.join(unknown source)         at org.apache.hadoop.hbase.master.regionmanager.stop(regionmanager.java:611)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:405) was splitting 11 logs. ",
        "label": 314
    },
    {
        "text": "hadoop metrics properties can include configuration of the  rest  context for ganglia  it appears from hadoop-metrics.properties that configuration for rest context is missing. it would be good if we add the rest context and commented out them, if anyone is using rest-server and if they want to monitor using ganglia context then they can uncomment the rest context and use them for rest-server monitoring using ganglia. # configuration of the \"rest\" context for ganglia #rest.class=org.apache.hadoop.metrics.ganglia.gangliacontext #rest.period=10 #rest.servers=ganglia-metad-hostname:port working on the patch, will submit it. ",
        "label": 333
    },
    {
        "text": "remove no longer used options  from lars george list up on hbase-dev: hi, i went through the config values as per the defaults xml file (still going through it again now based on what is actually in the code, i.e. those not in defaults). here is what i found: hbase.master.balancer.period - only used in hbase-default.xml? hbase.regions.percheckin, hbase.regions.slop - some tests still have it but not used anywhere else zookeeper.pause, zookeeper.retries - never used? only in hbase-defaults.xml and then there are differences between hardcoded and xml based defaults: hbase.client.pause - xml: 1000, hardcoded: 2000 (hbaseclient) and 30 * 1000 (hbaseadmin) hbase.client.retries.number - xml: 10, hardcoded 5 (hbaseadmin) and 2 (hmaster) hbase.hstore.blockingstorefiles - xml: 7, hardcoded: -1 hbase.hstore.compactionthreshold - xml: 3, hardcoded: 2 hbase.regionserver.global.memstore.lowerlimit - xml: 0.35, hardcoded: 0.25 hbase.regionserver.handler.count - xml: 25, hardcoded: 10 hbase.regionserver.msginterval - xml: 3000, hardcoded: 1000 hbase.rest.port - xml: 8080, hardcoded: 9090 hfile.block.cache.size - xml: 0.2, hardcoded: 0.0 finally, some keys are already in hconstants, some are in local classes and others used as literals. there is an issue open to fix this though. just saying. thoughts? ",
        "label": 314
    },
    {
        "text": "master cannot go from to due to zookeeper data  after running a 94 cluster and then updating it to 96, master cannot start due to the following: 2013-02-04 19:59:15,494 fatal org.apache.hadoop.hbase.master.hmaster: unhandled exception. starting shutdown.  org.apache.zookeeper.keeperexception$datainconsistencyexception: keepererrorcode = datainconsistency  at org.apache.hadoop.hbase.zookeeper.zkutil.convert(zkutil.java:1705)  at org.apache.hadoop.hbase.zookeeper.zktablereadonly.gettablestate(zktablereadonly.java:158)  at org.apache.hadoop.hbase.zookeeper.zktable.populatetablestates(zktable.java:82)  at org.apache.hadoop.hbase.zookeeper.zktable.<init>(zktable.java:69)  at org.apache.hadoop.hbase.master.assignmentmanager.<init>(assignmentmanager.java:199)  at org.apache.hadoop.hbase.master.hmaster.initializezkbasedsystemtrackers(hmaster.java:550)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:671)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:476)  at java.lang.thread.run(thread.java:662)  caused by: org.apache.hadoop.hbase.deserializationexception: missing pb magic pbuf prefix  at org.apache.hadoop.hbase.protobuf.protobufutil.expectpbmagicprefix(protobufutil.java:196)  at org.apache.hadoop.hbase.zookeeper.zktablereadonly.gettablestate(zktablereadonly.java:148)  ... 7 more it should probably nuke the data and start as with blanks. ",
        "label": 543
    },
    {
        "text": "transition offline snapshots to foreignexceptions  this will eliminate the old errorhandling code, and update existing code to use the foreignexception mechanisms. i'd like this to be done before attempt merging to trunk. ",
        "label": 248
    },
    {
        "text": "non environment variable solution for  illegalaccesserror  class com google protobuf zerocopyliteralbytestring cannot access its superclass com google protobuf literalbytestring   i am running into the problem described in https://issues.apache.org/jira/browse/hbase-10304, while trying to use a newer version within cascading.hbase (https://github.com/cascading/cascading.hbase). one of the features of cascading.hbase is that you can use it from lingual (http://www.cascading.org/projects/lingual/), our sql layer for hadoop. lingual has a notion of providers, which are fat jars that we pull down dynamically at runtime. those jars give users the ability to talk to any system or format from sql. they are added to the classpath programmatically before we submit jobs to a hadoop cluster. since lingual does not know upfront , which providers are going to be used in a given run, the hadoop_classpath trick proposed in the jira above is really clunky and breaks the ease of use we had before. no other provider requires this right now. it would be great to have a programmatical way to fix this, when using fat jars. ",
        "label": 314
    },
    {
        "text": "document how to run integration tests  hbase-6203 has attached the old it doc with some mods. when we figure how its are to be run, update it and apply the documentation under this issue. making a blocker against 0.96. ",
        "label": 155
    },
    {
        "text": "user permission command failed to show global permission  when use user_permission command to show global permissions, the following exception occurred: hbase(main):001:0> user_permission user namespace,table,family,qualifier:permission error: failed to coerce org.apache.hadoop.hbase.security.access.globalpermission to org.apache.hadoop.hbase.security.access.tablepermission for usage try 'help \"user_permission\"' took 1.1249 seconds hbase(main):002:0> ",
        "label": 500
    },
    {
        "text": "remove duplicated code from put  delete  get  scan  multiput  this came from discussion with stack w.r.t. hbase-2195. there is currently a lot of duplicated code especially between put and delete, and also between all operations.  for example all of put/delete/get/scan have attributes with exactly the same code in all classes.  put and delete also have the familymap, row, rowlock, timestamp, etc. one way to do this is to introduce \"operationwithattributes\" which extends operation, and have put/delete/get/scan extend that rather than operation.  in addition put and delete could extends from mutation (which itself would extend operationwithattributes). if a static inheritance hierarchy is not desired here, we can use delegation. ",
        "label": 286
    },
    {
        "text": "remove duplicate code gettabledescriptor in htable  as todo in comment said,   htable.gettabledescriptor is same as hadmin.gettabledescriptor. remove the duplicate code. ",
        "label": 198
    },
    {
        "text": "change ownership of the directory to bulk load  to bulk load something you need to change the ownership of the data directory to allow the hbase user to read and move the files, also in the split case you must use the hbase user to run the loadincrementalhfiles tool, since internally some directories \"_tmp\" are created to add the split reference files. in a secure cluster, the securebulkloadendpoint will take care of this problem by doing a chmod 777 on the directory to bulk load. note that a chown is not possible since you must be a super user to change the ownership, a change group may be possible but the user must be in the hbase group... and anyway it will require a chmod to allow the group to perform the move. caused by: org.apache.hadoop.security.accesscontrolexception: permission denied: user=hbase, access=write, inode=\"/test/cf\":th30z:supergroup:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.fspermissionchecker.check(fspermissionchecker.java:205) caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception(java.io.ioexception): java.io.ioexception: exception in rename at org.apache.hadoop.hbase.regionserver.hregionfilesystem.rename(hregionfilesystem.java:928) at org.apache.hadoop.hbase.regionserver.hregionfilesystem.commitstorefile(hregionfilesystem.java:340) at org.apache.hadoop.hbase.regionserver.hregionfilesystem.bulkloadstorefile(hregionfilesystem.java:414) ",
        "label": 309
    },
    {
        "text": "pluggable compaction algorithms  it would be good to create a set of common compaction algorithms so that we can tune this on a per-cf basis. in order to accomplish this, we need to refactor the current algorithm for plugability. ",
        "label": 12
    },
    {
        "text": "up versions on historian and keep history of deleted regions for a while rather than delete immediately  since removal of master logging of all rows of catalog tables, there is a hole when it comes to debugging what happened in hbase at some time in the past. the region historian is the place we keep up region history only we've been deleting old history when region is deleted. instead, lets not delete region history but keep it around a while, say a week. also, keep all versions \u2013 currently its set to 1. to do this, may need to also so hbase-998. ",
        "label": 314
    },
    {
        "text": "add apache releases to pom  list of   repositories  hadoop 0.20.2 available in the releases now at - https://repository.apache.org/content/repositories/releases/ . would be useful to add the same to the repository url-s. ",
        "label": 266
    },
    {
        "text": "row locks are acquired repeatedly in hregion dominibatchmutation for duplicate rows   if we already have the lock in the dominibatchmutation we don't need to re-acquire it. the solution would be to keep a cache of the rowkeys already locked for a minibatchmutation and if we already have the   rowkey in the cache, we don't repeatedly try and acquire the lock. a fix to this problem would be to keep a set of rows we already locked and not try to acquire the lock for these rows. we have tested this fix in our production environment and has improved replication performance quite a bit. we saw a replication batch go from 3+ minutes to less than 10 seconds for batches with duplicate row keys. static final int acquire_lock_count = 0;   @test   public void testredundantrowkeys() throws exception {     final int batchsize = 100000;          string tablename = getclass().getsimplename();     configuration conf = hbaseconfiguration.create();     conf.setclass(hconstants.region_impl, mockhregion.class, heapsize.class);     mockhregion region = (mockhregion) testhregion.inithregion(bytes.tobytes(tablename), tablename, conf, bytes.tobytes(\"a\"));     list<pair<mutation, integer>> somebatch = lists.newarraylist();     int i = 0;     while (i < batchsize) {       if (i % 2 == 0) {         somebatch.add(new pair<mutation, integer>(new put(bytes.tobytes(0)), null));       } else {         somebatch.add(new pair<mutation, integer>(new put(bytes.tobytes(1)), null));       }       i++;     }     long starttime = system.currenttimemillis();     region.batchmutate(somebatch.toarray(new pair[0]));     long endtime = system.currenttimemillis();     long duration = endtime - starttime;     system.out.println(\"duration: \" + duration + \" ms\");     assertequals(2, acquire_lock_count);   }   @override   public integer getlock(integer lockid, byte[] row, boolean waitforlock) throws ioexception {     acquire_lock_count++;     return super.getlock(lockid, row, waitforlock);   } ",
        "label": 522
    },
    {
        "text": "testfromclientside testcacheonwriteevictonclose is flaky  occasionally, this test fails: expected:<2049> but was:<2069> stacktrace java.lang.assertionerror: expected:<2049> but was:<2069> at org.junit.assert.fail(assert.java:93) at org.junit.assert.failnotequals(assert.java:647) at org.junit.assert.assertequals(assert.java:128) at org.junit.assert.assertequals(assert.java:472) at org.junit.assert.assertequals(assert.java:456) at org.apache.hadoop.hbase.client.testfromclientside.testcacheonwriteevictonclose(testfromclientside.java:4248) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) it could be because there is other thread still accessing the cache. ",
        "label": 242
    },
    {
        "text": "enable table replication command should only create specified table for a peer cluster  for a peer only user specified tables should be created but enable_table_replication command is not honouring that. eg:  like peer1 : t1:cf1, t2  create 't3', 'd'  enable_table_replication 't3' > should not create t3 in peer1 ",
        "label": 53
    },
    {
        "text": "ensure there is always at least one server x line in zookeeper conf zoo cfg  this is needed so that we can get rid of the \"hbase.master\" property from the configuration xml. from hbase-1145: there is a bit of an issue regarding getting rid of the \"hbase.master\" property in hbase-site.xml. currently, anybody in hbase needing zookeeper parses the server.x lines out of conf/zoo.cfg to know how to connect to zookeeper. if there are no server.x lines, it means we are running a single instance zookeeper on the master server. to find that server, we use the \"hbase.master\" property in hbase-site.xml. after discussing it with andrew purtell and michael stack, we came up with the following: we have three cases:  1) \"local\" mode. all servers/clients are on localhost.  2) distributed operation with single instance zookeeper running on master host.  3) distributed operation with distributed zookeeper running. to fix these cases (1 and 2) and allow us to remove the \"hbase.master\" property from hbase-site.xml, we need to make sure there is always a server.x line with the master's address in the zoo.cfg. even when you're running a single instance zookeeper on the master server, there should be a single server.x line with the master's address. to avoid adding a lot of configuration headache on the user, we want to have the default conf/zoo.cfg have a sensible default for the single server master hostname case. we suggest shipping a new zookeeper jar with zookeeper-279 patched in, and using the variable substitution to inject the master hostname into zoo.cfg. for example, our new zoo.cfg would look something like: # the number of milliseconds of each tick ticktime=2000 # the number of ticks that the initial  # synchronization phase can take initlimit=10 # the number of ticks that can pass between  # sending a request and getting an acknowledgement synclimit=5 # the directory where the snapshot is stored. datadir=${hbase.tmp.dir}/zookeeper # the port at which the clients will connect clientport=2181  server.0=${master.address.hostname}:2888:3888 the \"master.address.hostname\" is a new property being added to the hbase-site.xml during this period while we transition users to zookeeper. if the property is not set, we will use gethostname() to fill in the variable and produce copious warning messages that it may not work and the user should either fill in that variable in hbase-site.xml, or edit zoo.cfg by hand. ",
        "label": 342
    },
    {
        "text": "confirm can upgrade to from by just stopping and restarting  over in hbase-6294, larsh says you have to currently clear zk to get a 0.96 to start over data written by a 0.94. need to fix it so don't have to do this \u2013 that zk state left over gets auto-migrated. ",
        "label": 314
    },
    {
        "text": "avoid synchronization in hregionscannerimpl isfilterdone  a while ago i introduced hregoinscannerimpl.nextraw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines). looking at the code again i see that isfilterdone() is called from nextraw() and isfilterdone() is synchronized.  the caller of nextraw is required to ensure single threaded access to nextraw() anyway, we can call an unsynchronized internal version of isfilterdone(). ",
        "label": 286
    },
    {
        "text": "update hbase metrics framework to metrics2 framework  metrics framework has been marked deprecated in hadoop 0.20.203+ and 0.22+, and it might get removed in future hadoop release. hence, hbase needs to revise the dependency of metricscontext to use metrics2 framework. ",
        "label": 154
    },
    {
        "text": "deletion of a cell deletes the previous versions too  when i tried to delete a cell using it's timestamp in the hbase shell, the previous versions of the same cell also got deleted. but when i tried the same using the java api, then the previous versions are not deleted and i can retrive the previous values. https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/client/delete.java see this file to fix the issue. this method (public delete addcolumn(final byte [] family, final byte [] qualifier, final long timestamp)) only deletes the current version of the cell. the previous versions are not deleted. ",
        "label": 106
    },
    {
        "text": "fix npe in hfileoutputformat2  when i dig in hbase-14659, i run testwritingpedata. there are a lot of npe thrown and testcase run a long time. the reason is that, in hfileoutputformat2             hregionlocation loc = null;             string tablename = conf.get(output_table_name_conf_key);             try (connection connection = connectionfactory.createconnection(conf);                    regionlocator locator =                      connection.getregionlocator(tablename.valueof(tablename))) {               loc = locator.getregionlocation(rowkey);             } catch (throwable e) {               log.warn(\"there's something wrong when locating rowkey: \" +                 bytes.tostring(rowkey), e);               loc = null;             } because we did not set output_table_name_conf_key, so tablename is null, so npe thrown. and connection will be created which regionlocator use to find region location. because zk is not start in this testcase, so it will retry many times. but all this actions are not required, we can skip create connection by check whether tablename is null ",
        "label": 198
    },
    {
        "text": "testassignmentmanager testsockettimeout fails in master branch  toward the end of the test output, i saw: 2017-08-05 03:30:16,591 info  [time-limited test] assignment.testassignmentmanager(446): executionexception java.util.concurrent.executionexception: org.apache.hadoop.hbase.master.procedure.servercrashexception: servercrashprocedure pid=3, server=localhost,103,1   at org.apache.hadoop.hbase.master.procedure.proceduresyncwait$procedurefuture.get(proceduresyncwait.java:104)   at org.apache.hadoop.hbase.master.procedure.proceduresyncwait$procedurefuture.get(proceduresyncwait.java:62)   at org.apache.hadoop.hbase.master.assignment.testassignmentmanager.waitonfuture(testassignmentmanager.java:444)   at org.apache.hadoop.hbase.master.assignment.testassignmentmanager.testsockettimeout(testassignmentmanager.java:255)   at sun.reflect.nativemethodaccessorimpl.invoke0(native method)   at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)   at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)   at java.lang.reflect.method.invoke(method.java:498)   at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:50)   at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12)   at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:47)   at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17)   at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:26)   at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:27)   at org.junit.rules.testwatcher$1.evaluate(testwatcher.java:55)   at org.junit.internal.runners.statements.failontimeout$callablestatement.call(failontimeout.java:298)   at org.junit.internal.runners.statements.failontimeout$callablestatement.call(failontimeout.java:292)   at java.util.concurrent.futuretask.run(futuretask.java:266)   at java.lang.thread.run(thread.java:745) caused by: org.apache.hadoop.hbase.master.procedure.servercrashexception: servercrashprocedure pid=3, server=localhost,103,1   at org.apache.hadoop.hbase.master.assignment.unassignprocedure.updatetransition(unassignprocedure.java:169)   at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:274)   at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:57)   at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:847)   at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1440)   at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1209)   at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$800(procedureexecutor.java:79)   at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1719) this test failure seems to happen after hbase-18491 was checked in. looking at the change in unassignprocedure, it seems we should handle the two conditions differently:      if (servercrashed.get() || !isserveronline(env, regionnode)) { with attached patch, testassignmentmanager#testsockettimeout and testservercrashprocedure#testrecoveryanddoubleexecutiononrswithmeta pass. ",
        "label": 459
    },
    {
        "text": "enhance resourcechecker to log stack trace for potentially hanging threads  currently resourcechecker logs a line similar to the following if it detects potential thread leak: 2012-11-02 10:18:59,299 info  [main] hbase.resourcechecker(157): after master.cleaner.testhfilecleaner#testttlcleaner: 44 threads (was 43), 145 file descriptors (was 145). 0 connections,  -thread leak?- we should enhance the log to include stack trace of the potentially hanging thread(s) this work was motivated when i investigated test failure in hbase-6796 ",
        "label": 441
    },
    {
        "text": "add more useful statistics in the hfile tool  the hfile tool has been very useful to us recently to get a better idea of the size of our rows. however, it happened frequently that we wished for more statistics to have a more complete picture of the distribution of the row sizes. scott kuehn requested that feature often enough in private that i decided to give it a go. here's the patch that adds more nice little stats via yammer's histograms. it was easy enough since com.yammer.metrics is already in hbase's dependencies. example of the new output from org.apache.hadoop.hbase.io.hfile.hfile -s -f ...: stats: key length:                min = 24.00                max = 24.00               mean = 24.00             stddev = 0.00             median = 24.00               75% <= 24.00               95% <= 24.00               98% <= 24.00               99% <= 24.00             99.9% <= 24.00 row size (bytes):                min = 33.00                max = 33.00               mean = 33.00             stddev = 0.00             median = 33.00               75% <= 33.00               95% <= 33.00               98% <= 33.00               99% <= 33.00             99.9% <= 33.00 row size (columns):                min = 1.00                max = 1.00               mean = 1.00             stddev = 0.00             median = 1.00               75% <= 1.00               95% <= 1.00               98% <= 1.00               99% <= 1.00             99.9% <= 1.00 val length:                min = 1.00                max = 1.00               mean = 1.00             stddev = 0.00             median = 1.00               75% <= 1.00               95% <= 1.00               98% <= 1.00               99% <= 1.00             99.9% <= 1.00 key of biggest row: \\x00 ",
        "label": 24
    },
    {
        "text": "open close region request may be executed twice when master restart  we found this problem when run itbll for our internal branch which based branch-2.2. 1. master a schedule a trsp which will reopen region1. and this trsp firstly schdule a sub remote procedure: closeregionprocedure and send the close region request to rs. 2. master a shutdown and master b is the new active master. and restore this trsp and the remote procedure closeregionprocedure. 3. rs reported to the new master b and the closeregionprocedure finished. then the trsp schdule a new openregionprocedure and send open region request to rs. 4. but meanwhile master b send the close region request to rs again. 5. the open region request finished firstly and report to master succeed. the master thought the region was opened on rs. but the rs excuted the close region request again and closed the region1. 6. the master thought the region opened but the rs closed the region. then the new trsp will stuck forever. ",
        "label": 187
    },
    {
        "text": "user permission does not list namespace permissions  the user_permission command does not list namespace permissions: for example: if i create a new namespace or use an existing namespace and grant a user privileges to that namespace, the command user_permission does not list it. the permission is visible in the acl table. example:  hbase(main):005:0> create_namespace 'ns3'  0 row(s) in 0.1640 seconds  hbase(main):007:0> grant 'test_user','rwxac','@ns3'  0 row(s) in 0.5680 seconds  hbase(main):008:0> user_permission '.*'  user namespace,table,family,qualifier:permission   sh82993 finance,finance:emp,,: [permission: actions=read,write,exec,create,admin]   @hbaseglobaldba hbase,hbase:acl,,: [permission: actions=exec,create,admin]   @hbaseglobaloper hbase,hbase:acl,,: [permission: actions=exec,admin]   hdfs hbase,hbase:acl,,: [permission: actions=read,write,create,admin,exec]   sh82993 ns1,ns1:tbl1,,: [permission: actions=read,write,exec,create,admin]   ns1admin ns1,ns1:tbl2,,: [permission: actions=exec,create,admin]   @hbaseappltest_ns1funct ns1,ns1:tbl2,,: [permission: actions=read,write,exec]   ns1funct ns1,ns1:tbl2,,: [permission: actions=read,write,exec,create,admin]   hbase ns2,ns2:tbl1,,: [permission: actions=read,write,exec,create,admin]   9 row(s) in 1.8090 seconds as you can see user test_user does not appear in the output, but we can see the permission in the acl table. hbase(main):001:0> scan 'hbase:acl'  row column+cell   @finance column=l:sh82993, timestamp=1444405519510, value=rwxca   @gcbcppdn column=l:hdfs, timestamp=1446141119602, value=rwcxa   @hbase column=l:hdfs, timestamp=1446141485136, value=rwcax   @ns1 column=l:@hbaseappltest_ns1admin, timestamp=1447437007467, value=rwxca   @ns1 column=l:@hbaseappltest_ns1funct, timestamp=1447427366835, value=rwx   @ns2 column=l:@hbaseappltest_ns2admin, timestamp=1446674470456, value=xca   @ns2 column=l:test_user, timestamp=1447692840030, value=rwac   @ns3 column=l:test_user, timestamp=1447692860434, value=rwxac   finance:emp column=l:sh82993, timestamp=1444407723316, value=rwxca   hbase:acl column=l:@hbaseglobaldba, timestamp=1446590375370, value=xca   hbase:acl column=l:@hbaseglobaloper, timestamp=1446590387965, value=xa   hbase:acl column=l:hdfs, timestamp=1446141737213, value=rwcax   ns1:tbl1 column=l:sh82993, timestamp=1446674153058, value=rwxca   ns1:tbl2 column=l:@hbaseappltest_ns1funct, timestamp=1447183824580, value=rwx   ns1:tbl2 column=l:ns1admin, timestamp=1447183766370, value=xca   ns1:tbl2 column=l:ns1funct, timestamp=1447184077545, value=rwxca   ns2:tbl1 column=l:hbase, timestamp=1447182228314, value=rwxca   11 row(s) in 0.4990 seconds it would be nice to be able to see namespace permissions via the user_permission '.*' command as scanning the acl table is not the recommended way to view object permissions. especially if one is looking to access base via a shell and collect acl information. steven ",
        "label": 490
    },
    {
        "text": "procedure stuck due to region happen to recorded on two servers   master log: $ grep \"cf9a4ec6cd890aa6806fb313d71e2ebd\" hbase-hbaseadmin-master-100.107.176.225.log.1 2019-12-17 11:24:03,534 debug [keepalivepeworker-20] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=34 size=1662) to run queue because: the exclusive lock is not held by anyone when adding pid=193706, ppid=187614, state=runnable:region_state_transition_get_assign_candidate; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign 2019-12-17 11:24:22,851 info  [keepalivepeworker-17] procedure.masterprocedurescheduler: took xlock for pid=193706, ppid=187614, state=runnable:region_state_transition_get_assign_candidate; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign 2019-12-17 11:24:22,852 info  [keepalivepeworker-17] assignment.transitregionstateprocedure: starting pid=193706, ppid=187614, state=runnable:region_state_transition_get_assign_candidate, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; rit=open, location=null; forcenewplan=true, retain=false 2019-12-17 11:24:22,852 debug [keepalivepeworker-17] procedure2.rootprocedurestate: add procedure pid=193706, ppid=187614, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 51669th rollback step 2019-12-17 11:24:22,858 debug [master/100.107.176.225:60000] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=349 size=1666) to run queue because: pid=193706, ppid=187614, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign has lock 2019-12-17 11:24:22,912 info  [peworker-9] assignment.transitregionstateprocedure: starting pid=193706, ppid=187614, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; openregion rit=open, location=100.107.176.215,60020,1576552834619; loc=100.107.176.215,60020,1576552834619 2019-12-17 11:24:22,912 info  [peworker-9] assignment.regionstatestore: pid=193706 updating hbase:meta row=cf9a4ec6cd890aa6806fb313d71e2ebd, regionstate=opening, regionlocation=100.107.176.215,60020,1576552834619 2019-12-17 11:24:22,912 debug [peworker-9] procedure2.rootprocedurestate: add procedure pid=193706, ppid=187614, state=waiting:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 52115th rollback step 2019-12-17 11:24:22,918 warn  [peworker-8] assignment.regionremoteprocedurebase: can not add remote operation pid=243482, ppid=193706, state=runnable, locked=true; org.apache.hadoop.hbase.master.assignment.openregionprocedure for region {encoded => cf9a4ec6cd890aa6806fb313d71e2ebd, name => 'table1w_7,user68694,1576484498244.cf9a4ec6cd890aa6806fb313d71e2ebd.', startkey => 'user68694', endkey => 'user68703'} to server 100.107.176.215,60020,1576552834619, this usually because the server is alread dead, give up and mark the procedure as complete, the parent procedure will take care of this. 2019-12-17 11:24:22,921 debug [peworker-8] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=331 size=1664) to run queue because: pid=193706, ppid=187614, state=runnable:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign has lock 2019-12-17 11:24:22,921 info  [peworker-8] procedure2.procedureexecutor: finished subprocedure pid=243482, resume processing parent pid=193706, ppid=187614, state=runnable:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign 2019-12-17 11:24:22,921 info  [peworker-9] assignment.transitregionstateprocedure: retry=1 of max=2147483647; pid=193706, ppid=187614, state=runnable:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; rit=opening, location=100.107.176.215,60020,1576552834619 2019-12-17 11:24:22,921 debug [peworker-9] procedure2.rootprocedurestate: add procedure pid=193706, ppid=187614, state=runnable:region_state_transition_get_assign_candidate, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 52157th rollback step 2019-12-17 11:24:22,922 info  [peworker-9] assignment.transitregionstateprocedure: starting pid=193706, ppid=187614, state=runnable:region_state_transition_get_assign_candidate, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; rit=opening, location=null; forcenewplan=true, retain=false 2019-12-17 11:24:22,922 debug [peworker-9] procedure2.rootprocedurestate: add procedure pid=193706, ppid=187614, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 52164th rollback step 2019-12-17 11:24:22,930 debug [master/100.107.176.225:60000] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=331 size=1661) to run queue because: pid=193706, ppid=187614, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign has lock 2019-12-17 11:24:22,943 info  [peworker-14] assignment.transitregionstateprocedure: starting pid=193706, ppid=187614, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; openregion rit=opening, location=100.107.164.90,60020,1576553001648; loc=100.107.164.90,60020,1576553001648 2019-12-17 11:24:22,943 info  [peworker-14] assignment.regionstatestore: pid=193706 updating hbase:meta row=cf9a4ec6cd890aa6806fb313d71e2ebd, regionstate=opening, regionlocation=100.107.164.90,60020,1576553001648 2019-12-17 11:24:22,944 debug [peworker-14] procedure2.rootprocedurestate: add procedure pid=193706, ppid=187614, state=waiting:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 52284th rollback step 2019-12-17 11:24:23,000 info  [peworker-11] assignment.regionstatestore: pid=193706 updating hbase:meta row=cf9a4ec6cd890aa6806fb313d71e2ebd, regionstate=open, openseqnum=17, regionlocation=100.107.164.90,60020,1576553001648 2019-12-17 11:24:23,003 debug [peworker-11] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=336 size=1635) to run queue because: pid=193706, ppid=187614, state=runnable:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign has lock 2019-12-17 11:24:23,003 info  [peworker-11] procedure2.procedureexecutor: finished subprocedure pid=243523, resume processing parent pid=193706, ppid=187614, state=runnable:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign 2019-12-17 11:24:23,003 debug [keepalivepeworker-17] procedure2.rootprocedurestate: add procedure pid=193706, ppid=187614, state=success, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 52624th rollback step 2019-12-17 11:24:23,005 info  [keepalivepeworker-17] procedure2.procedureexecutor: finished pid=193706, ppid=187614, state=success; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign in 19.7230sec 2019-12-17 11:24:28,902 debug [peworker-7] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=27 size=1856) to run queue because: the exclusive lock is not held by anyone when adding pid=258071, ppid=243483, state=runnable:region_state_transition_ge 2019-12-17 11:24:22,914 debug [regionservertracker-0] procedure2.procedureexecutor: stored pid=243483, state=runnable:server_crash_start; servercrashprocedure server=100.107.176.215,60020,1576552834619, splitwal=true, meta=false 2019-12-17 11:24:39,485 info  [peworker-10] procedure.masterprocedurescheduler: took xlock for pid=258071, ppid=243483, state=runnable:region_state_transition_get_assign_candidate; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign 2019-12-17 11:24:39,486 info  [peworker-10] assignment.transitregionstateprocedure: starting pid=258071, ppid=243483, state=runnable:region_state_transition_get_assign_candidate, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; rit=open, location=null; forcenewplan=true, retain=false 2019-12-17 11:24:39,486 debug [peworker-10] procedure2.rootprocedurestate: add procedure pid=258071, ppid=243483, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 68029th rollback step 2019-12-17 11:24:39,571 debug [master/100.107.176.225:60000] procedure.masterprocedurescheduler: add tablequeue(table1w_7, xlock=false sharedlock=47 size=1847) to run queue because: pid=258071, ppid=243483, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign has lock 2019-12-17 11:24:39,625 info  [peworker-16] assignment.transitregionstateprocedure: starting pid=258071, ppid=243483, state=runnable:region_state_transition_open, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign; openregion rit=open, location=100.107.164.90,60020,1576553001648; loc=100.107.164.90,60020,1576553001648 2019-12-17 11:24:39,625 info  [peworker-16] assignment.regionstatestore: pid=258071 updating hbase:meta row=cf9a4ec6cd890aa6806fb313d71e2ebd, regionstate=opening, regionlocation=100.107.164.90,60020,1576553001648 2019-12-17 11:24:39,626 debug [peworker-16] procedure2.rootprocedurestate: add procedure pid=258071, ppid=243483, state=waiting:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd, assign as the 68773th rollback step 2019-12-17 11:26:23,217 warn  [procexectimeout] assignment.assignmentmanager: stuck region-in-transition rit=opening, location=100.107.164.90,60020,1576553001648, table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd 2019-12-17 11:27:23,217 warn  [procexectimeout] assignment.assignmentmanager: stuck region-in-transition rit=opening, location=100.107.164.90,60020,1576553001648, table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd 2019-12-17 11:28:06,532 info  [master/100.107.176.225:60000.chore.1] master.hmaster: not running balancer because 1 region(s) in transition: [rit=opening, location=100.107.164.90,60020,1576553001648, table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd] 2019-12-17 11:28:23,218 warn  [procexectimeout] assignment.assignmentmanager: stuck region-in-transition rit=opening, location=100.107.164.90,60020,1576553001648, table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd 2019-12-17 11:29:23,218 warn  [procexectimeout] assignment.assignmentmanager: stuck region-in-transition rit=opening, location=100.107.164.90,60020,1576553001648, table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd 2019-12-17 11:30:23,219 warn  [procexectimeout] assignment.assignmentmanager: stuck region-in-transition rit=opening, location=100.107.164.90,60020,1576553001648, table=table1w_7, region=cf9a4ec6cd890aa6806fb313d71e2ebd regionserver log: $ grep \"cf9a4ec6cd890aa6806fb313d71e2ebd\" hbase-hbaseadmin-regionserver-100.107.164.90.log 2019-12-17 11:24:22,972 info  [rs_open_region-regionserver/100.107.164.90:60020-1] handler.assignregionhandler: open table1w_7,user68694,1576484498244.cf9a4ec6cd890aa6806fb313d71e2ebd. 2019-12-17 11:24:22,972 info  [rs_open_region-regionserver/100.107.164.90:60020-1] regionserver.hregion: filter rows disabled with empty filtering pattern for region=table1w_7,user68694,1576484498244.cf9a4ec6cd890aa6806fb313d71e2ebd.. 2019-12-17 11:24:22,983 info  [storeopener-cf9a4ec6cd890aa6806fb313d71e2ebd-1] hfile.cacheconfig: created cacheconfig: cachedataonread=true, cachedataonwrite=false, cacheindexesonwrite=false, cachebloomsonwrite=false, cacheevictonclose=true, cachedatacompressed=false, prefetchonopen=false for family {name => 'cf', versions => '1', evict_blocks_on_close => 'false', new_version_behavior => 'false', keep_deleted_cells => 'false', cache_data_on_write => 'false', data_block_encoding => 'diff', ttl => 'forever', min_versions => '0', replication_scope => '0', bloomfilter => 'row', cache_index_on_write => 'false', in_memory => 'false', cache_blooms_on_write => 'false', prefetch_blocks_on_open => 'false', compression => 'snappy', blockcache => 'true', blocksize => '65536'} with blockcache=org.apache.hadoop.hbase.io.hfile.combinedblockcache@1376a714 2019-12-17 11:24:22,983 info  [storeopener-cf9a4ec6cd890aa6806fb313d71e2ebd-1] compactions.compactionconfiguration: size [128 mb, 2 gb, 2 gb); files [7, 25); ratio 1.200000; off-peak ratio 5.000000; throttle point 4294967296; major period 0, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.exploringcompactionpolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.exponentialcompactionwindowfactory 2019-12-17 11:24:22,984 info  [storeopener-cf9a4ec6cd890aa6806fb313d71e2ebd-1] regionserver.hstore: store=cf,  memstore type=defaultmemstore, storagepolicy=hot, verifybulkloads=false, parallelputcountprintthreshold=50, encoding=diff, compression=snappy 2019-12-17 11:24:22,987 info  [rs_open_region-regionserver/100.107.164.90:60020-1] regionserver.hregion: opened cf9a4ec6cd890aa6806fb313d71e2ebd; next sequenceid=17 2019-12-17 11:24:22,987 info  [rs_open_region-regionserver/100.107.164.90:60020-1] regionserver.hregionserver: post open deploy tasks for table1w_7,user68694,1576484498244.cf9a4ec6cd890aa6806fb313d71e2ebd., openprocid=243523, mastersystemtime=1576553062965 2019-12-17 11:24:22,990 info  [rs_open_region-regionserver/100.107.164.90:60020-1] handler.assignregionhandler: opened table1w_7,user68694,1576484498244.cf9a4ec6cd890aa6806fb313d71e2ebd. 2019-12-17 11:24:39,657 warn  [rs_open_region-regionserver/100.107.164.90:60020-1] handler.assignregionhandler: received open for the region:table1w_7,user68694,1576484498244.cf9a4ec6cd890aa6806fb313d71e2ebd., which is already online 2019-12-17 11:23:49,598 debug [regionservertracker-0] procedure2.procedureexecutor: stored pid=187614, state=runnable:server_crash_start; servercrashprocedure server=100.107.165.41,60020,1576552792328, splitwal=true, meta=true process servercrashprocedure(100.107.165.41,60020,1576552792328) assign region cf9a4ec6cd890aa6806fb313d71e2ebd,  region assign to 100.107.176.215,60020,1576552834619, but failed, so retry and assign to 100.107.164.90,60020,1576553001648 and open on 100.107.164.90,60020,1576553001648 success.  later process servercrashprocedure(100.107.176.215,60020,1576552834619) and find region cf9a4ec6cd890aa6806fb313d71e2ebd on this server also, than try assign this region, and assign the region to 100.107.164.90,60020,1576553001648 also, but 100.107.164.90,60020,1576553001648 find it has open this region already, than the assign stuck. ",
        "label": 149
    },
    {
        "text": "make hadoop2 the default and deprecate hadoop1  see \"hadoop version trunk dependency?\" on the dev mailing ilst. consensus seems to be forming to do the subject line (recheck the mail thread before going ahead). ",
        "label": 314
    },
    {
        "text": "wrong methods' names in clusterloadstate class  in clusterloadstate class, there are two methods using the wrong names: getmaxload() and getminload(). however, the implementation of the two methods are right, so just exchange them. ",
        "label": 466
    },
    {
        "text": "gets from multiactions are not counted in metrics for gets   rsrpcservices#get updates the get metrics. however multiactions do not. ",
        "label": 198
    },
    {
        "text": "review compaction heuristic and move compaction code out so standalone and independently testable  anything that improves our i/o profile makes hbase run smoother. over in hbase-2457, good work has been done already describing the tension between minimizing compactions versus minimizing count of store files. this issue is about following on from what has been done in 2457 but also, breaking the hard-to-read compaction code out of store.java out to a standalone class that can be the easier tested (and easily analyzed for its performance characteristics). if possible, in the refactor, we'd allow specification of alternate merge sort implementations. ",
        "label": 247
    },
    {
        "text": "clientscanner might not close the hconnection created in construction  clientscanner will create hconnection in its construction:     public clientscanner(final configuration conf, final scan scan,         final byte[] tablename) throws ioexception {       this(conf, scan, tablename, hconnectionmanager.getconnection(conf));     } however, this connection won't be closed in clientscanner.close(). is it better to deprecate this construction? ",
        "label": 238
    },
    {
        "text": "two scanner objects are open for each hbase map task but only one scanner object is closed  map reduce framework calls createrecordreader of the tableinputformat/multitableinputformat to get the record reader instance. in this method, we are initializing the tablerecordreaderimpl (restart method). this initializes the scanner object. after this, map reduce framework calls initialize on the recordreader. in our case, this calls restart of the tablerecordreaderimpl again. here, it doesn't close the first scanner. at the end of the task, only the second scanner object is closed. because of this, the smallest read point of hregion is affected. we don't need to initialize the recordreader in the createrecordreader method and we need to close the scanner object in the restart method. (incase if the restart method is called because of exceptions in the nextkeyvalue method) ",
        "label": 464
    },
    {
        "text": "wrong return values of comparators for columnvaluefilter  the return values of the compareto() method have to be changed against each other. the method has to return 0 if the regex matches and 1 if it does not. ",
        "label": 110
    },
    {
        "text": "can't pass script to hbase shell  shell documentation says: ~ stack$ ${hbase_home}/bin/hbase shell path_to_script your script can lean on the methods provided by the hbase shell. this doesn't actually work. ",
        "label": 218
    },
    {
        "text": "update pom xml to use another protoc as external protobuf  currently, there is no protoc 2.5.0 for release [1]. so we can make a new one for arm specific. for make sure that could work on arm. we will introduce a new arm artifact for protoc, group_id is org.openlabtesting.protobuf .. this is just used for protobuf-maven-plugin to compile .proto files. as the 3.x version of protoc support arm already. so this won't affect the internal protoc usage, which is 3.5.1-1 now.   [1]https://github.com/protocolbuffers/protobuf/issues/3844#issuecomment-343355946 ",
        "label": 560
    },
    {
        "text": "further optimize byte comparison methods  guava uses some clever tricks with sun.misc.unsafe to compare byte arrays about 100% faster than the naive byte-by-byte implementation: http://guava-libraries.googlecode.com/svn/trunk/guava/src/com/google/common/primitives/unsignedbytes.java we should borrow this [apache 2 licensed] code. ",
        "label": 441
    },
    {
        "text": "testhregiononcluster testdatacorrectnessreplayingrecoverededits fails  https://builds.apache.org/job/hbase-trunk/4269/testreport/org.apache.hadoop.hbase.regionserver/testhregiononcluster/testdatacorrectnessreplayingrecoverededits/ java.lang.exception: test timed out after 180000 milliseconds at java.lang.thread.sleep(native method) at org.apache.hadoop.hbase.regionserver.testhregiononcluster.testdatacorrectnessreplayingrecoverededits(testhregiononcluster.java:97) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:74) test looked to be making progress though perhaps a stall here: 2013-07-18 19:30:56,111 info  [am.zk.worker-pool-2-thread-14] master.assignmentmanager$4(1223): the master has opened testdatacorrectnessreplayingrecoverededits,,1374175853834.44b3c494ea2682d3113ffe6658bf4910. that was online on janus.apache.org,41592,1374175849520 2013-07-18 19:33:37,407 info  [thread-0] hbase.hbasetestingutility(914): shutting down minicluster may have just been taking time loading data. let me up the timeout and add debug because hard tracking where we are in this issue going by logs. ",
        "label": 314
    },
    {
        "text": "replace json licensed json dependencies  hbase depends on agile-json, which includes the following phrase in its license: \"the software shall be used for good, not evil.\" debian-legal advised me, that for this phrase the software is considered non-free. as a consequence, hbase could not be included in the official debian distribution but only in the contrib repository. i discussed this issue also with crockford, but he made it clear that he has no intention to change his license. http://www.json.org lists several java json libraries. maybe you could take another one with a more sane license text? yes, this issue is highly controversial and you'll most likely not spend the hours of work only because crockford and debian are both bullheads. just keep this issue open as crockford-remembrance issue. may it help to fight the evil! ",
        "label": 314
    },
    {
        "text": "handle master failover for regions in transition  to this point in hbase-2692 tasks we have moved everything for regions in transition into zk, but we have not fully handled the master failover case. this is to deal with that and to write tests for it. ",
        "label": 247
    },
    {
        "text": "reuse compression streams in hfileblock writer  we need to to reuse compression streams in hfileblock.writer instead of allocating them every time. the motivation is that when using java's built-in implementation of gzip, we allocate a new gzipoutputstream object and an associated native data structure every time we create a compression stream. the native data structure is only deallocated in the finalizer. this is one suspected cause of recent testhfileblock failures on hadoop qa: https://builds.apache.org/job/hbase-trunk/2658/testreport/org.apache.hadoop.hbase.io.hfile/testhfileblock/testpreviousoffset_1_/. ",
        "label": 324
    },
    {
        "text": "incremental load support for multiple table hfileoutputformat  introduction mapreduce currently supports the ability to write hbase records in bulk to hfiles for a single table. the file(s) can then be uploaded to the relevant regionservers information with reasonable latency. this feature is useful to make a large set of data available for queries at the same time as well as provides a way to efficiently process very large input into hbase without affecting query latencies. there is, however, no support to write variations of the same record key to hfiles belonging to multiple hbase tables from within the same mapreduce job. goal the goal of this jira is to extend hfileoutputformat2 to support writing to hfiles for different tables within the same mapreduce job while single-table hfile features backwards-compatible. for our use case, we needed to write a record key to a smaller hbase table for quicker access, and the same record key with a date appended to a larger table for longer term storage with chronological access. each of these tables would have different ttl and other settings to support their respective access patterns. we also needed to be able to bulk write records to multiple tables with different subsets of very large input as efficiently as possible. rather than run the mapreduce job multiple times (one for each table or record structure), it would be useful to be able to parse the input a single time and write to multiple tables simultaneously. additionally, we'd like to maintain backwards compatibility with the existing heavily-used hfileoutputformat2 interface to allow benefits such as locality sensitivity (that was introduced long after we implemented support for multiple tables) to support both single table and multi table hfile writes. proposal backwards compatibility for existing single table support in hfileoutputformat2 will be maintained and in this case, mappers will need to emit the table rowkey as before. however, a new class - multihfileoutputformat - will provide a helper function to generate a rowkey for mappers that prefixes the desired tablename to the existing rowkey as well as provides configureincrementalload support for multiple tables. hfileoutputformat2 will be updated in the following way: configureincrementalload will now accept multiple table descriptor and region locator pairs, analogous to the single pair currently accepted by hfileoutputformat2. compression, block size, bloom type and datablock settings per column family that are set in the configuration object are now indexed and retrieved by tablename and column family getregionstartkeys will now support multiple regionlocators and calculate split points and therefore partitions collectively for all tables. similarly, now the eventual number of reducers will be equal to the total number of partitions across all tables. the recordwriter class will be able to process rowkeys either with or without the tablename prepended depending on how configureincrementalload was configured with multihfileoutputformat or hfileoutputformat2. the use of multihfileoutputformat will write the output into hfiles which will match the output format of hfileoutputformat2. however, while the default use case will keep the existing directory structure with column family name as the directory and hfiles within that directory, in the case of multihfileoutputformat, it will output hfiles in the output directory with the following relative paths:      --table1         --family1           --hfiles       --table2         --family1         --family2           --hfiles this aims to be a comprehensive solution to the original tickets - hbase-3727 and hbase-16261. thanks to clay b. for his support. this is a contribution from bloomberg developers. the patch will be attached shortly. ",
        "label": 135
    },
    {
        "text": "maintain information on the time a rs went dead  just something that'd be generally helpful, is to maintain deadserver info with the last timestamp when it was determined as dead. makes it easier to hunt the logs, and i don't think its much too expensive to maintain (one additional update per dead determination). ",
        "label": 548
    },
    {
        "text": "splitting logs  we'll make an output file though the region no longer exists  the \"human unit tester\" (kannan) last night wondered what happens splitting logs and we come across an edit whose region has since been removed. taking a look, it looks like we'll create the output file and write the edits for the no-longer-extant region anyways. this will leave litter in the filesystem \u2013 region split files that will never be used nor removed. this issue is about verifying that indeed this is whats happening (we do sequencefile.createwriter with the overwrite flag set to true which tracing seems to mean create all intermediary directories \u2013 to be verified) and if it indeed is happening, fixing split so unless the region dir exists, don't write out edits.. just drop them. ",
        "label": 229
    },
    {
        "text": "auto close 'unknown' regions reported as open on regionservers  in old days, if a regionserver reported a variance that didn't agree w/ master view of the cluster, we'd kill the regionserver. lately, in tests that overrun a cluster, after a sustained high-load, master can start failing its updates against meta (callqueuetoobigexception <= more on this later). it then can lose proper accounting of all region members. one variant has a regionserver reporting its list of open regions to the master and the master doesn't 'know' of a particular region or the master may know the region but expects it open on another regionserver. here is an example of how it looks each time rs reports:  2019-12-03 07:07:00,757 warn org.apache.hadoop.hbase.master.assignment.assignmentmanager: no t1,08f5c285,1573094375485.ee78a0c951c1c902d8f3f3912394a0e5. regionstatenode but reported online at server.example.org,16020,1575354666245 (inserverregionlist=false).  2019-12-03 07:07:03,793 warn org.apache.hadoop.hbase.master.assignment.assignmentmanager: no t1,08f5c285,1573094375485.ee78a0c951c1c902d8f3f3912394a0e5. regionstatenode but reported online at server.example.org,16020,1575354666245 (inserverregionlist=false). will also show as an 'inconsistency' in the 'hbck' tab on the master ui. ",
        "label": 314
    },
    {
        "text": "expose the ability to set custom http request headers for the rest client used by remotehtable  my corporate security office (iso) requires that all http traffic get routed through a web access management layer (http://en.wikipedia.org/wiki/web_access_management) our hadoop cluster has been segmented by a virtual network with all access to hbase from outside clients being managed through hbase stargate rest server. the corporate wam system requires that all http clients authenticate with it first before making any http request to any http service in the corporate network. after the http client authenticates with the wam system the wam system returns the client a set of values that must be inserted into a http cookie and request header of all future http requests to other http clients. this would mean that all requests through the remotehtable interface would require that this cookie and request header be set as part of the http request. org.apache.hadoop.hbase.rest.client.client looks like the appropriate place that this functionality would need to be plugged into. ",
        "label": 242
    },
    {
        "text": "reduce annoying catch clauses of unsupportedencodingexception that is never thrown because of utf  there are some codes that catch unsupportedencodingexception, and log or ignore it because java always supports utf-8 (see the javadoc of charset). the catch clauses are annoying, and they should be replaced by methods of bytes. ",
        "label": 200
    },
    {
        "text": "javadoc for result getrow is confusing to new users   org.apache.hadoop.hbase.client.result getrow is confusing to new users. the documentation could be read to mean the raw data of the row. in addition it is written with improper grammar. ",
        "label": 146
    },
    {
        "text": "remove distributed mode from minizookeeper  minizookeeper currently has a standalone and a distributed mode. for hbase testing we only need (and only use) the standalone mode. we should remove all of the distributed logic. ",
        "label": 342
    },
    {
        "text": "importtsv does not check for table existence  the usage statement for the \"importtsv\" command to hbase claims this: \"note: if you do not use this option, then the target table must already exist in hbase\" (in reference to the \"importtsv.bulk.output\" command-line option) the truth is, the table must exist no matter what, importtsv cannot and will not create it for you. this is the case because the createsubmittablejob method of importtsv does not even attempt to check if the table exists already, much less create it: (from org.apache.hadoop.hbase.mapreduce.importtsv.java) 305 htable table = new htable(conf, tablename); the htable method signature in use there assumes the table exists and runs a meta scan on it: (from org.apache.hadoop.hbase.client.htable.java) 142 * creates an object to access a hbase table.  ...  151 public htable(configuration conf, final string tablename) what we should do inside of createsubmittablejob is something similar to what the \"completebulkloads\" command would do: (taken from org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles.java) 690 boolean tableexists = this.doestableexist(tablename);  691 if (!tableexists) this.createtable(tablename,dirpath); currently the docs are misleading, the table in fact must exist prior to running importtsv. we should check if it exists rather than assume it's already there and throw the below exception: 12/03/14 17:15:42 warn client.hconnectionmanager$hconnectionimplementation: encountered problems when prefetch meta table:   org.apache.hadoop.hbase.tablenotfoundexception: cannot find row in .meta. for table: mytable2, row=mytable2,,99999999999999  at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:150)  ... ",
        "label": 199
    },
    {
        "text": "baseregionobserver pre compact flush store scanneropen returns null  as pointed out in https://github.com/forcedotcom/phoenix/pull/131, baseregionobserver#precompactscanneropen returns null by default, which hoses any coprocessors down the line, making override of this method mandatory. the fix is trivial, patch coming momentarily. update:  this same behavior is present in the flush and store versions of the same method - this should all be moved to the proposed 'return passed scanner' default behavior. ",
        "label": 236
    },
    {
        "text": "add thread local behavior to htable pool  it is a well-documented fact that the hbase table client (viz., htable) is not thread-safe. hence, the recommendation has been to use a htablepool or a threadlocal to manage access to tables. the downside of the latter is that it (a) requires the user to reinvent the wheel in terms of mapping table names to tables and (b) forces the user to maintain the thread-local objects. ideally, it would be nice if we could make the htablepool handle thread-local objects as well. that way, it not only becomes the \"one stop shop\" for all client-side tables, but also insulates the user from the threadlocal object.  here, we propose a way to generalize the htablepool so that the underlying pool type is either \"reusable\" or \"thread-local\". to make this possible, we introdudce the concept of a sharedmap, which essentially, maps a key to a collection of values, the elements of which are managed by a pool. in effect, that collection acts as a shared pool of resources, access to which is closely controlled as dictated by the particular semantics of the pool.  furthermore, to simplify the construction of htablepools, we added a couple of parameters (viz. \"hbase.client.htable.pool.type\" and \"hbase.client.hbase.pool.size\") to control the default behavior of a htablepool.  in case the size of the pool is set to a non-zero positive number, that is used to cap the number of resources that a pool may contain for any given key. a size of integer#max_value is interpreted to mean an unbounded pool.  currently, the sharedmap supports the following types of pools: a threadlocalpool, which represents a pool that builds on the threadlocal class. it essentially binds the resource to the thread from which it is accessed. a reusablepool, which represents a pool that builds on the linkedlist class. it essentially allows resources to be checked out, at which point it is (temporarily) removed from the pool. when the resource is no longer required, it should be returned to the pool in order to be reused. a roundrobinpool, which represents a pool that stores its resources in an arraylist. it load-balances access to its resources by returning a different resource every time a given key is looked up. ",
        "label": 265
    },
    {
        "text": "registering a coprocessor at htabledescriptor should be less strict  registering a copressor in the following way will fail as the \"coprocessor$1\" keyword is case sensitive (instead coprocessor$1 works fine). removing this restriction would improve usability. htabledescriptor desc = new htabledescriptor(tname);  desc.setvalue(\"coprocessor$1\",  path.tostring() + \":\" + full_class_name +  \":\" + coprocessor.priority.user); ",
        "label": 327
    },
    {
        "text": "add spm for hbase to ref guide  ref guide should point users to spm for hbase in monitoring section(s). ",
        "label": 330
    },
    {
        "text": "backport hbase 'recoverfilelease does not check return value of recoverlease' to  hbase-7878 fixed an issue which may lead to data loss. we should backport to 0.94 branch ",
        "label": 290
    },
    {
        "text": "fix tablenotfound when grant revoke if accesscontroller is not loaded  when doing grant, revoke..., a tablenotfoundexception will occur if accesscontroller if is not configured. ",
        "label": 500
    },
    {
        "text": "fix broken link in hfile javadoc  there's a bad link in the hfile javadoc; should point to hbase-61, but instead point to hbase-3315. also cleaned up the reference to tfile. ",
        "label": 231
    },
    {
        "text": "have master show its address using hostname rather than ip  here is the patch: diff --git a/src/main/java/org/apache/hadoop/hbase/hserveraddress.java b/src/main/java/org/apache/hadoop/hbase/hserveraddress.java index 763eca2..3859968 100644 --- a/src/main/java/org/apache/hadoop/hbase/hserveraddress.java +++ b/src/main/java/org/apache/hadoop/hbase/hserveraddress.java @@ -46,7 +46,7 @@ public class hserveraddress implements writablecomparable<hserveraddress> {     */    public hserveraddress(inetsocketaddress address) {      this.address = address; -    this.stringvalue = address.getaddress().gethostaddress() + \":\" + +    this.stringvalue = address.getaddress().gethostname() + \":\" +        address.getport();      checkbindaddresscanberesolved();    } this change will effect what is shown in log when master starts up printing its hostname instead of address and it will also show over in nn logs \u2013 using hostname rather than ip will match what the rs is showing in nn log. ",
        "label": 314
    },
    {
        "text": "intermittent testregionservercoprocessorexceptionwithabort testexceptionfromcoprocessorduringput failure  see https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.92/83/testreport/junit/org.apache.hadoop.hbase.coprocessor/testregionservercoprocessorexceptionwithabort/testexceptionfromcoprocessorduringput/ somehow getrsforfirstregionintable() wasn't able to retrieve the region server. one fix for this issue is to spin up minicluster with 1 region server so that we don't need to search for the region server where first region is hosted. ",
        "label": 441
    },
    {
        "text": "add ut and docs for datablockencodingtool  there is no example, documents, or tests for datablockencodingtool. we should have it friendly if any use case exists. otherwise, we should just get rid of it because datablockencodingtool presumes that the implementation of cell returned from datablockencoder is keyvalue. the presume may obstruct the cleanup of keyvalue references in the code base of read/write path. ",
        "label": 281
    },
    {
        "text": "tests in hbase spark module fail with unsatisfiedlinkerror  the following can be observed in recent trunk builds: [31m  java.io.ioexception: shutting down[0m [31m  at org.apache.hadoop.hbase.minihbasecluster.init(minihbasecluster.java:232)[0m [31m  at org.apache.hadoop.hbase.minihbasecluster.<init>(minihbasecluster.java:94)[0m [31m  at org.apache.hadoop.hbase.hbasetestingutility.startminihbasecluster(hbasetestingutility.java:1111)[0m [31m  at org.apache.hadoop.hbase.hbasetestingutility.startminicluster(hbasetestingutility.java:1065)[0m [31m  at org.apache.hadoop.hbase.hbasetestingutility.startminicluster(hbasetestingutility.java:936)[0m [31m  at org.apache.hadoop.hbase.hbasetestingutility.startminicluster(hbasetestingutility.java:930)[0m [31m  at org.apache.hadoop.hbase.hbasetestingutility.startminicluster(hbasetestingutility.java:859)[0m [31m  at org.apache.hadoop.hbase.spark.hbasedstreamfunctionssuite.beforeall(hbasedstreamfunctionssuite.scala:41)[0m [31m  at org.scalatest.beforeandafterall$class.beforeall(beforeandafterall.scala:187)[0m [31m  at org.apache.hadoop.hbase.spark.hbasedstreamfunctionssuite.beforeall(hbasedstreamfunctionssuite.scala:30)[0m [31m  ...[0m [31m  cause: java.lang.runtimeexception: failed construction of master: class org.apache.hadoop.hbase.master.hmasterorg.apache.hadoop.hbase.shaded.io.netty.channel.epoll.      nativestaticallyreferencedjnimethods.epollin()i[0m [31m  at org.apache.hadoop.hbase.util.jvmclusterutil.createmasterthread(jvmclusterutil.java:145)[0m [31m  at org.apache.hadoop.hbase.localhbasecluster.addmaster(localhbasecluster.java:217)[0m [31m  at org.apache.hadoop.hbase.localhbasecluster.<init>(localhbasecluster.java:152)[0m [31m  at org.apache.hadoop.hbase.minihbasecluster.init(minihbasecluster.java:214)[0m [31m  at org.apache.hadoop.hbase.minihbasecluster.<init>(minihbasecluster.java:94)[0m [31m  at org.apache.hadoop.hbase.hbasetestingutility.startminihbasecluster(hbasetestingutility.java:1111)[0m this is due to scalatest-maven-plugin missing systempropertyvariables for shaded netty. ",
        "label": 314
    },
    {
        "text": "document jdk versions supported by each release  we can make use of a jdk version x hbase version matrix to explain which jdk version is supported and required. 0.94, 0.96, and 0.98 releases all support jdk6 and jdk7. for 1.0, there is a discussion thread to decide whether to drop jdk6 support. there has been some work to support jdk8. we can also document that. ",
        "label": 330
    },
    {
        "text": "merge tool should work on online cluster  taking down the entire cluster to merge 2 regions is a pain, i dont see why the table or regions specifically couldnt be taken offline, then merged then brought back up. this might need a new api to the regionservers so they can take direction from not just the master. ",
        "label": 314
    },
    {
        "text": "tableoutputformat ignores failure to create table instance  if tableoutputformat in the new api fails to create a table, it simply logs this at error level and then continues on its way. then, the first write() to the table will throw a npe since table hasn't been set. instead, it should probably rethrow the exception as a runtimeexception in setconf, or do what the old-api tof does and not create the htable instance until getrecordwriter, where it can throw an ioe. ",
        "label": 82
    },
    {
        "text": "regionserver needs to recover if datanode goes down  if i take down a datanode, the regionserver will repeatedly return this error: java.io.ioexception: stream closed.  at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.isclosed(dfsclient.java:1875)  at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.writechunk(dfsclient.java:2096)  at org.apache.hadoop.fs.fsoutputsummer.writechecksumchunk(fsoutputsummer.java:141)  at org.apache.hadoop.fs.fsoutputsummer.flushbuffer(fsoutputsummer.java:124)  at org.apache.hadoop.fs.fsoutputsummer.write1(fsoutputsummer.java:112)  at org.apache.hadoop.fs.fsoutputsummer.write(fsoutputsummer.java:86)  at org.apache.hadoop.fs.fsdataoutputstream$positioncache.write(fsdataoutputstream.java:41)  at java.io.dataoutputstream.write(unknown source)  at org.apache.hadoop.io.sequencefile$writer.append(sequencefile.java:977)  at org.apache.hadoop.hbase.hlog.append(hlog.java:377)  at org.apache.hadoop.hbase.hregion.update(hregion.java:1455)  at org.apache.hadoop.hbase.hregion.batchupdate(hregion.java:1259)  at org.apache.hadoop.hbase.hregionserver.batchupdate(hregionserver.java:1433)  at sun.reflect.generatedmethodaccessor9.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:910) it appears that hbase/dfsclient does not attempt to reopen the stream. ",
        "label": 86
    },
    {
        "text": "expose individual cache stats in a combinedcache through jmx  with offheap cache being used to store data blocks and on-heap for index and bloom filters, exposing the stats from the individual caches through jmx will help understand the cache usage trend. currently the combined cache stats is available through jmx which may not provide insight into the individual cache usage. ",
        "label": 72
    },
    {
        "text": "testclientscannerrpctimeout failing in branch   branch  testclientscannerrpctimeout is failing in branch-1 / branch-1.4, but it's a setup problem involving createtableprocedure. fails for me the first time in both linux and macos dev environments. bisect says this is the cause, been there for a while commit af359d03b5e2cc798cee8ba52d2a9fcbb1022104 author: stephen yuan jiang <syuanjiangdev@gmail.com> date:   tue jul 18 06:58:29 2017 -0700     hbase-16488 starting namespace and quota services in master startup asynchronizely (stephen yuan jiang) and sure enough, if i revert hbase-16488 from branch-1 / branch-1.4 then the test passes again. tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 7.211 sec <<< failure! - in org.apache.hadoop.hbase.client.testclientscannerrpctimeout testscannernextrpctimesout(org.apache.hadoop.hbase.client.testclientscannerrpctimeout)  time elapsed: 6.248 sec  <<< error! org.apache.hadoop.hbase.tableexistsexception: testscannernextrpctimesout         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)         at java.lang.reflect.constructor.newinstance(constructor.java:526)         at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:106)         at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:95)         at org.apache.hadoop.hbase.util.foreignexceptionutil.toioexception(foreignexceptionutil.java:45)         at org.apache.hadoop.hbase.client.hbaseadmin$procedurefuture.convertresult(hbaseadmin.java:4774)         at org.apache.hadoop.hbase.client.hbaseadmin$procedurefuture.waitprocedureresult(hbaseadmin.java:4732)         at org.apache.hadoop.hbase.client.hbaseadmin$procedurefuture.get(hbaseadmin.java:4665)         at org.apache.hadoop.hbase.client.hbaseadmin.createtable(hbaseadmin.java:679)         at org.apache.hadoop.hbase.hbasetestingutility.createtable(hbasetestingutility.java:1500)         at org.apache.hadoop.hbase.hbasetestingutility.createtable(hbasetestingutility.java:1547)         at org.apache.hadoop.hbase.hbasetestingutility.createtable(hbasetestingutility.java:1438)         at org.apache.hadoop.hbase.hbasetestingutility.createtable(hbasetestingutility.java:1414)         at org.apache.hadoop.hbase.hbasetestingutility.createtable(hbasetestingutility.java:1370)         at org.apache.hadoop.hbase.client.testclientscannerrpctimeout.testscannernextrpctimesout(testclientscannerrpctimeout.java:87) caused by: org.apache.hadoop.ipc.remoteexception: testscannernextrpctimesout         at org.apache.hadoop.hbase.master.procedure.createtableprocedure.preparecreate(createtableprocedure.java:286)         at org.apache.hadoop.hbase.master.procedure.createtableprocedure.executefromstate(createtableprocedure.java:107)         at org.apache.hadoop.hbase.master.procedure.createtableprocedure.executefromstate(createtableprocedure.java:59)         at org.apache.hadoop.hbase.procedure2.statemachineprocedure.execute(statemachineprocedure.java:139)         at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:506)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1152)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.execloop(procedureexecutor.java:940)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.execloop(procedureexecutor.java:893)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$400(procedureexecutor.java:76)         at org.apache.hadoop.hbase.procedure2.procedureexecutor$2.run(procedureexecutor.java:478) ",
        "label": 38
    },
    {
        "text": "jython docs out of date  the documentation describing how to launch jython + hbase is out of date. - https://hbase.apache.org/book.html#jython first, we would set the classpath differently: hbase_classpath=/home/hbase/jython.jar bin/hbase org.python.util.jython then, the actual code example is out of date too: >>> desc = htabledescriptor(tablename) >>> desc.addfamily(hcolumndescriptor(\"content:\")) traceback (most recent call last):   file \"<stdin>\", line 1, in <module> at org.apache.hadoop.hbase.hcolumndescriptor.islegalfamilyname(hcolumndescriptor.java:566) at org.apache.hadoop.hbase.hcolumndescriptor.<init>(hcolumndescriptor.java:470) at org.apache.hadoop.hbase.hcolumndescriptor.<init>(hcolumndescriptor.java:425) at org.apache.hadoop.hbase.hcolumndescriptor.<init>(hcolumndescriptor.java:390) at org.apache.hadoop.hbase.hcolumndescriptor.<init>(hcolumndescriptor.java:338) at org.apache.hadoop.hbase.hcolumndescriptor.<init>(hcolumndescriptor.java:327) at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:526) at org.python.core.pyreflectedconstructor.constructproxy(pyreflectedconstructor.java:211) we should make sure that the examples we claim are runnable actually are. ",
        "label": 51
    },
    {
        "text": "add abliity to add and remove  table  indexes on existing tables  a first cut at adding/removing indexes to existing tables. tested with small tables, and will not scale very good as is (should be a map/reduce). factored out index maintenance stuff into indexmaintenanceutils ",
        "label": 110
    },
    {
        "text": "user who created table cannot scan the same table due to insufficient permissions  user hrt_qa has been given 'c' permission. create 'te', {name => 'f1', versions => 5} ... hbase(main):003:0> list table hbase:acl hbase:namespace te 6 row(s) in 0.0570 seconds hbase(main):004:0> scan 'te' row                                      column+cell 2013-08-21 02:21:00,921 debug [main] token.authenticationtokenselector: no matching token found 2013-08-21 02:21:00,921 debug [main] security.hbasesaslrpcclient: creating sasl gssapi client. server's kerberos principal name is hbase/hor16n13.gq1.ygridcore.net@horton.ygridcore.net 2013-08-21 02:21:00,923 debug [main] security.hbasesaslrpcclient: have sent token of size 582 from initsaslcontext. 2013-08-21 02:21:00,926 debug [main] security.hbasesaslrpcclient: will read input token of size 0 for processing by initsaslcontext 2013-08-21 02:21:00,926 debug [main] security.hbasesaslrpcclient: will send token of size 0 from initsaslcontext. 2013-08-21 02:21:00,926 debug [main] security.hbasesaslrpcclient: will read input token of size 53 for processing by initsaslcontext 2013-08-21 02:21:00,927 debug [main] security.hbasesaslrpcclient: will send token of size 53 from initsaslcontext. 2013-08-21 02:21:00,927 debug [main] security.hbasesaslrpcclient: sasl client context established. negotiated qop: auth 2013-08-21 02:21:00,935 warn  [main] client.rpcretryingcaller: call exception, tries=0, retries=7, retrytime=-14ms org.apache.hadoop.hbase.security.accessdeniedexception: org.apache.hadoop.hbase.security.accessdeniedexception: insufficient permissions for user 'hrt_qa' for scanner open on table te at org.apache.hadoop.hbase.security.access.accesscontroller.prescanneropen(accesscontroller.java:1116) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.prescanneropen(regioncoprocessorhost.java:1294) at org.apache.hadoop.hbase.regionserver.hregionserver.scan(hregionserver.java:3007) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26847) ... caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception(org.apache.hadoop.hbase.security.accessdeniedexception): org.apache.hadoop.hbase.security.accessdeniedexception: insufficient permissions for user 'hrt_qa' for scanner open on table te at org.apache.hadoop.hbase.security.access.accesscontroller.prescanneropen(accesscontroller.java:1116) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.prescanneropen(regioncoprocessorhost.java:1294) at org.apache.hadoop.hbase.regionserver.hregionserver.scan(hregionserver.java:3007) here was related entries in hbase:acl table: hbase(main):001:0> scan 'hbase:acl' row                                      column+cell  hbase:acl                               column=l:hrt_qa, timestamp=1377045996685, value=c  te                                      column=l:hrt_qa, timestamp=1377051648649, value=rwxca ",
        "label": 441
    },
    {
        "text": "root region appeared in two regionserver's onlineregions at the same time  this could be happen under the following steps with little probability:  (i suppose the cluster nodes names are rs1/rs2/hm, and there's more than 10,000 regions in the cluster) 1.root region was opened in rs1.  2.due to some reason(maybe the hdfs process was got abnormal),rs1 aborted.  3.servershutdownhandler process start.  4.hmaster was restarted, during the finishinitialization's handling, root region was unsetted, and assigned to rs2.   5.root region was opened successfully in rs2.  6.but after while, root region was unsetted again by rs1's servershutdownhandler. then it was reassigned. before that, the rs1 was restarted. so there's two possibilities:  case a:  root region was assigned to rs1.   it seemed nothing would be affected. but the root region was still online in rs2.  case b:  root region was assigned to rs2.   the root region couldn't be opened until it would be reassigned to other regionserver, because it was showed online in this regionserver. this could be proved from the logs: 1. root region was opened with two times:  2011-05-17 10:32:59,188 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: opened region root,,0.70236052 on 162-2-77-0,20020,1305598359031  2011-05-17 10:33:01,536 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: opened region root,,0.70236052 on 162-2-16-6,20020,1305597548212 2.regionserver 162-2-16-6 was aborted, so it was reassigned to 162-2-77-0, but already online on this server:  10:49:30,920 info org.apache.hadoop.hbase.regionserver.hregionserver: received request to open region: root,,0.70236052 10:49:30,920 debug org.apache.hadoop.hbase.regionserver.handler.openregionhandler: processing open of root,,0.70236052 10:49:30,920 warn org.apache.hadoop.hbase.regionserver.handler.openregionhandler: attempted open of root,,0.70236052 but already online on this server this could be cause a long break of root region offline, though it happened under a special scenario. and i have checked the code, it seems a tiny bug here. there's 2 references about assignroot(): 1.  hmaster# assignrootandmeta:  if (!catalogtracker.verifyrootregionlocation(timeout)) { this.assignmentmanager.assignroot(); this.catalogtracker.waitforroot(); assigned++; } 2.  servershutdownhandler# process:  if (iscarryingroot()) { // root   try { this.services.getassignmentmanager().assignroot(); } catch (keeperexception e) { this.server.abort(\"in server shutdown processing, assigning root\", e); throw new ioexception(\"aborting\", e); } } i think each time call the method of assignroot(), we should verify root region's location first. because before the assigning, the root region could have been assigned by another place. ",
        "label": 240
    },
    {
        "text": "hbase book link to  beginner  issues includes resolved issues  the link at http://hbase.apache.org/book.html#getting.involved for beginner issues is https://issues.apache.org/jira/issues/?jql=project%20%3d%20hbase%20and%20labels%20in%20(beginner) but this includes resolved issues as well, which is not useful to folks looking for new issues to cut their teeth on. ",
        "label": 352
    },
    {
        "text": "split zktable into interface and implementation  in hbase-11071 we are trying to split admin handlers away from zk. however, a zktable instance is being used in multiple places, hence it would be beneficial to hide its implementation behind a well defined interface. ",
        "label": 323
    },
    {
        "text": "don't build javadocs for hbase protocol module  i'm not sure i have all the affected versions, but it seems that something is amiss in making our javadocs:  mvn -papache-release -prelease -dskiptests clean package ... snip ... [info] ------------------------------------------------------------------------ [info] reactor summary: [info]  [info] apache hbase ....................................... success [ 11.149 s] [info] apache hbase - checkstyle .......................... success [  1.249 s] [info] apache hbase - resource bundle ..................... success [  0.539 s] [info] apache hbase - annotations ......................... success [  4.438 s] [info] apache hbase - protocol ............................ success [10:15 min] [info] apache hbase - common .............................. success [ 48.465 s] [info] apache hbase - procedure ........................... success [ 14.375 s] [info] apache hbase - client .............................. success [ 45.187 s] [info] apache hbase - hadoop compatibility ................ success [  6.998 s] [info] apache hbase - hadoop two compatibility ............ success [ 14.891 s] [info] apache hbase - prefix tree ......................... success [ 14.214 s] [info] apache hbase - server .............................. success [02:01 min] [info] apache hbase - testing util ........................ success [ 12.779 s] [info] apache hbase - thrift .............................. success [01:15 min] [info] apache hbase - shell ............................... success [  6.649 s] [info] apache hbase - integration tests ................... success [  6.429 s] [info] apache hbase - examples ............................ success [ 13.200 s] [info] apache hbase - rest ................................ success [ 27.831 s] [info] apache hbase - assembly ............................ success [ 19.400 s] [info] apache hbase - shaded .............................. success [  0.419 s] [info] apache hbase - shaded - client ..................... success [ 23.707 s] [info] apache hbase - shaded - server ..................... success [ 43.654 s] [info] apache hbase - spark ............................... success [02:22 min] [info] ------------------------------------------------------------------------ [info] build success [info] ------------------------------------------------------------------------ [info] total time: 21:13 min [info] finished at: 2015-08-19t15:48:00-05:00 [info] final memory: 181m/1513m [info] ------------------------------------------------------------------------ ",
        "label": 402
    },
    {
        "text": "optimize codepath for minor compactions  there are some additional optimizations in the specialized storescanner and also in hfile for minor compactions. for example, there is some kv disassembling and sanity checking in hfile even though in a minor compaction these checks have already been run during the flush. another area to discuss is whether we should actually process deletes during minor compactions. it's not especially expensive (scandeletetracker is quite simple) but it requires looking at both the row and the qualifier value of every single kv. removing this would drop our axiom that \"deletes only apply to later storefiles\", which is used during get processing to have more efficient delete handling. ",
        "label": 34
    },
    {
        "text": " c  update wangle dependency to pick up the new release with apache license v2  good news that facebook decided to re-license the wangle library to use apache license v2. this unblocks the eventual merge of branch hbase-14850 since there will be no remaining licensing issues with dependencies. however, we need to update the wangle dependency version to include the commit https://github.com/facebook/wangle/commit/5f686dee5753890f37326fcf38c396b7ba8981ee. next weekly release tag for wangle should be sufficient. see  https://github.com/facebook/wangle/issues/105 ",
        "label": 155
    },
    {
        "text": "add pylintrc file to hbase  yetus runs all commits with python files through a linter. i think that the hbase community should add a pylintrc file to actively choose the project's python style instead of just relying on yetus defaults. as an argument for this, the yetus project itself doesn't even use the default python linter for its own commits. ",
        "label": 20
    },
    {
        "text": "rpc related metrics are missing in since recent changes  since the recent change to the metrics setup it seems that the rpc related stats have been completely dropped. see attached files for details. ",
        "label": 180
    },
    {
        "text": "client scanner spams the logs if there are lots of scanners   the log in client scanner should probably be on the trace level otherwise you end up with this: 2013-07-05 12:41:12,501 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,502 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,503 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,506 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,507 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,508 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,509 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,509 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= 2013-07-05 12:41:12,510 debug [pool-48-thread-3] client.clientscanner(97): scan table=integrationtestmttr, startrow= ",
        "label": 154
    },
    {
        "text": "improve hbck tool to fix  meta  hole issue   hbase hbck tool can check the meta hole, but it can not fix this problem by --fix.  i plan to improve the tool. ",
        "label": 248
    },
    {
        "text": "hbase vote download step requires url to end with ' '  the hbase-vote script's download step requires the sourcedir url be terminated with a path separator or else the retrieval will escape the candidate's directory and mirror way too much. ",
        "label": 435
    },
    {
        "text": "add restoresnapshot and clonesnapshot with acl methods in asyncadmin  ",
        "label": 149
    },
    {
        "text": "fsdatainputstreamwrapper unbuffer can not invoke the classes that not implements canunbuffer but its parents class implements canunbuffer  fsdatainputstreamwrapper unbuffer can not invoke the classes that not implements canunbuffer but its parents class implements canunbuffer for example: there are 1 interface i1 and one class implements i1 named pc1 and the class c1 extends from pc1 if we want to invoke the c1 unbuffer() method the fsdatainputstreamwrapper unbuffer  can not do that.        ",
        "label": 513
    },
    {
        "text": "master does not load balance regions well on startup  we get poor distribution of regions when we start up hbase. we have a total of 13 nodes and 898 regions, which should yield an average of 69 regions per node. instead, one node has 173 regions and one node has 16 regions. address start code load  10.100.11.62:60020 1199406218912 requests: 0 regions: 63  10.100.11.59:60020 1199406219179 requests: 0 regions: 55  10.100.11.60:60020 1199406219062 requests: 0 regions: 90  10.100.11.61:60020 1199406219132 requests: 1 regions: 54  10.100.11.64:60020 1199406218817 requests: 0 regions: 173  10.100.11.31:60020 1199406219039 requests: 1 regions: 16  10.100.11.58:60020 1199406218895 requests: 0 regions: 89  10.100.11.56:60020 1199406219037 requests: 0 regions: 76  10.100.11.65:60020 1199406219135 requests: 0 regions: 56  10.100.11.57:60020 1199406219183 requests: 1 regions: 56  10.100.11.33:60020 1199406219174 requests: 1 regions: 56  10.100.11.32:60020 1199406218944 requests: 0 regions: 66  10.100.11.63:60020 1199406219182 requests: 0 regions: 48  total: servers: 13 requests: 4 regions: 898 ",
        "label": 86
    },
    {
        "text": "move protobuf stuff in hbase rsgroup to hbase protocol shaded  ",
        "label": 149
    },
    {
        "text": "port hbase  concurrentmodificationexception in bucketallocator  to branch  hbase-10205 solves the following problem:  \"  the bucketcache writerthread calls bucketcache.freespace() upon draining the ram queue containing entries to be cached. freespace() in turn calls bucketsizeinfo.statistics() through bucketallocator.getindexstatistics(), which iterates over 'bucketlist'. at the same time another writerthread might call bucketallocator.allocateblock(), which may call bucketsizeinfo.allocateblock(), add a bucket to 'bucketlist' and consequently cause a concurrentmodificationexception. calls to bucketallocator.allocateblock() are synchronized, but calls to bucketallocator.getindexstatistics() are not, which allows this race to occur.  \" however, for some unknown reason, hbase-10205 was only committed to master (2.0 and beyond) and 0.98 branches only. to preserve continuity we should commit it to branch-1. ",
        "label": 426
    },
    {
        "text": "loadincrementalhfiles throws too generic of an exception  in 0.90 the loadincrementalhfiles constructor did not throw an exception, now it throws exception. the constructor should ether not throw an exception or throw zookeeperconnectionexception, and masternotrunningexception since those come from the hbaseadmin call. https://github.com/apache/hbase/blob/trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/loadincrementalhfiles.java#l105 ",
        "label": 53
    },
    {
        "text": "cleanup client connection layers  this issue originated from a discussion over in hbase-7442. we currently have a broken abstraction with hbaseclient, where it is bound to a single configuration instance at time of construction, but then reused for all connections to all clusters. this is combined with multiple, overlapping layers of connection caching. going through this code, it seems like we have a lot of mismatch between the higher layers and the lower layers, with too much abstraction in between. at the lower layers, most of the clientcache stuff seems completely unused. we currently effectively have an hbaseclient singleton (for secureclient as well in 0.92/0.94) in the client code, as i don't see anything that calls the constructor or rpcengine.getproxy() versions with a non-default socket factory. so a lot of the code around this seems like built up waste. the fact that a single configuration is fixed in the hbaseclient seems like a broken abstraction as it currently stands. in addition to cluster id, other configuration parameters (max retries, retry sleep) are fixed at time of construction. the more i look at the code, the more it looks like the clientcache and sharing the hbaseclient instance is an unnecessary complication. why cache the hbaseclient instances at all? in hconnectionmanager, we already have a mapping from configuration to hconnection. it seems to me like each hconnection(implementation) instance should have it's own hbaseclient instance, doing away with the clientcache mapping. this would keep each hbaseclient associated with a single cluster/configuration and fix the current breakage from reusing the same hbaseclient against different clusters. we need a refactoring of some of the interactions of hconnection(implementation), hbaserpc/rpcengine, and hbaseclient. off hand, we might want to expose a separate rpcengine.getclient() method that returns a new rpcclient interface (implemented by hbaseclient) and move the rpcengine.getproxy()/stopproxy() implementations into the client. so all proxy invocations can go through the same client, without requiring the static client cache. i haven't fully thought this through, so i could be missing other important aspects. but that approach at least seems like a step in the right direction for fixing the client abstractions. ",
        "label": 180
    },
    {
        "text": "fix the bin rename table rb script  make it work again  bin/rename_table.rb script rotted. bring it back up to date (someone asked nicely for it up on irc). ",
        "label": 314
    },
    {
        "text": "testtableinputformatscan on hadoop fails because yarn kills our applications  snippet from http://54.241.6.143/job/hbase-0.94-hadoop-2/org.apache.hbase$hbase/72/testreport/junit/org.apache.hadoop.hbase.mapreduce/testtableinputformatscan1/testscanemptytoempty/ 2013-04-16 04:12:06,097 warn  [container monitor] monitor.containersmonitorimpl$monitoringthread(436): container [pid=30006,containerid=container_1366085506903_0001_01_000001] is running beyond virtual memory limits. current usage: 160.9mb of 1.5gb physical memory used; 3.4gb of 3.1gb virtual memory used. killing container. dump of the process-tree for container_1366085506903_0001_01_000001 : |- pid ppid pgrpid sessid cmd_name user_mode_time(millis) system_time(millis) vmem_usage(bytes) rssmem_usage(pages) full_cmd_line |- 30006 24535 30006 30006 (bash) 0 3 110800896 303 /bin/bash -c /usr/java/default/bin/java -dlog4j.configuration=container-log4j.properties -dyarn.app.mapreduce.container.log.dir=/home/ec2-user/jenkins/workspace/hbase-0.94-hadoop-2/target/org.apache.hadoop.mapred.minimrcluster/org.apache.hadoop.mapred.minimrcluster-logdir-nm-1_1/application_1366085506903_0001/container_1366085506903_0001_01_000001 -dyarn.app.mapreduce.container.log.filesize=0 -dhadoop.root.logger=info,cla -xmx1024m org.apache.hadoop.mapreduce.v2.app.mrappmaster 1>/home/ec2-user/jenkins/workspace/hbase-0.94-hadoop-2/target/org.apache.hadoop.mapred.minimrcluster/org.apache.hadoop.mapred.minimrcluster-logdir-nm-1_1/application_1366085506903_0001/container_1366085506903_0001_01_000001/stdout 2>/home/ec2-user/jenkins/workspace/hbase-0.94-hadoop-2/target/org.apache.hadoop.mapred.minimrcluster/org.apache.hadoop.mapred.minimrcluster-logdir-nm-1_1/application_1366085506903_0001/container_1366085506903_0001_01_000001/stderr    |- 30024 30006 30006 30006 (java) 714 62 3517714432 40881 /usr/java/default/bin/java -dlog4j.configuration=container-log4j.properties -dyarn.app.mapreduce.container.log.dir=/home/ec2-user/jenkins/workspace/hbase-0.94-hadoop-2/target/org.apache.hadoop.mapred.minimrcluster/org.apache.hadoop.mapred.minimrcluster-logdir-nm-1_1/application_1366085506903_0001/container_1366085506903_0001_01_000001 -dyarn.app.mapreduce.container.log.filesize=0 -dhadoop.root.logger=info,cla -xmx1024m org.apache.hadoop.mapreduce.v2.app.mrappmaster  it started with this build according to my gmail search: http://54.241.6.143/job/hbase-0.94-hadoop-2/70/ related to hbase-8326 somehow, nick dimiduk? ",
        "label": 248
    },
    {
        "text": "seek only to the newly flushed file on scanner reset on flush  ",
        "label": 544
    },
    {
        "text": "make asyncfswal as our default wal  as it should be predicated on passing basic cluster itbll ",
        "label": 149
    },
    {
        "text": "postgresql to hbase table replication example  it is useful to have an easy way to replicate data from postgresql tables to a hbase tables.  i made a simple python tool which does this, called hbrep. hbrep is a tool for replicating data from postgresql tables to hbase tables. dependancies: python 2.4 hbase 0.2.0 skytools 2.1.7 postgresql it has two main functions. bootstrap, which bootstraps all the data from specified columns of a table play, which processes incoming insert, update and delete events and applies them to hbase. example usage:  install triggers:  ./hbrep.py hbrep.ini install schema1.table1 schema2.table2  now that future updates are queuing, bootstrap the tables.  ./hbrep.py hbrep.ini bootstrap schema1.table1 schema2.table2  start pgq ticker (this is part of skytools, it manages event queues and sends the events to registered consumers).  pgqadm.py pgq.ini ticker  play our queue consumer to replicate events  ./hbrep.py hbrep.ini play schema1.table1 schema2.table2 more details in the readme.  feedback and improvements appreciated. ",
        "label": 314
    },
    {
        "text": "resourcechecker refinement  this was based on some discussion from hbase-6234. the resourcechecker was added by n. keywal to help resolve some hadoop qa issues, but has since not be widely utilized. further, with modularization we have had to drop the resourcechecker from the tests that are moved into the hbase-common module because bringing the resourcechecker up to hbase-common would involved bringing all its dependencies (which are quite far reaching). the question then is, what should we do with it? get rid of it? refactor and resuse? ",
        "label": 340
    },
    {
        "text": "fix findbugs and error prone warnings  branch   fix important findbugs and error-prone warnings on branch-2 / master. start with a forward port pass from hbase-19239. assume rejected hunks need a new analysis. do that analysis. ",
        "label": 352
    },
    {
        "text": "multiput makes proper error handling impossible and leads to corrupted data  tl;dr version: i think the multiput rpc needs to be completely rewritten. the current code makes it totally impossible for an hbase client to do proper error handling. when an edit fails, the client has no clue as to what the problem was (certain error cases can be retried, others cannot e.g. when using a non-existent family) and the client doesn't even know which of the edits have been applied successfully. so the client often has to retry edits without knowing whether they've been applied or not, which leads to extra unwanted versions for the keyvalue that were successfully applied (for those who care about versions, this is essentially equivalent to data corruption). in addition, there's no way for a client to properly handle notservingregionexception, the client has to unnecessarily invalidate cached locations of some regions and retry all edits. life of a failed multi-put let's see why step by step what happens when a single edit in a multi-put fails. 1. an hbase user calls any of the put methods on an htable instance. 2. eventually, htable#flushcommits is invoked to actually send the edits to the regionserver(s). 3. this takes us to hconnectionmanager#processbatchofputs where all edits are sorted into one or more multiput. each multiput is aggregating all the edits that are going to a particular regionserver. 4. a thread pool is used to send all the multiput in parallel to their respective regionserver. let's follow what happens for a single multiput. 5. the multiput travels through the ipc code on the client and then through the network and then through the ipc code on the regionserver. 6. we're now in hregionserver#multiput where a new multiputresponse is created. 7. still in hregionserver#multiput. since a multiput is essentially a map from region name to a list of put for that region, there's a for loop that executes each list of put for each region sequentially. let's follow what happens for a single list of put for a particular region. 8. we're now in hregionserver#put(byte[], list<put>). each put is associated with the row lock that was specified by the client (if any). then the pairs of (put, lock id) are handed to the right hregion. 9. now we're in hregion#put(pair<put, integer>[]), which immediately takes us to hregion#dominibatchput. 10. at this point, let's assume that we're doing just 2 edits. so the batchoperationinprogress that dominibatchput contains just 2 put. 11. the while loop in dominibatchput that's going to execute each put starts. 12. the first put fails because an exception is thrown when appending the edit to the wal. its batchop.retcodes is marked as operationstatuscode.failure. 13. because there was an exception, we're back to hregion#put(pair<put, integer>[]) where the while loop will test that batchop.isdone is false and do another iteration. 14. dominibatchput is called again and handles the remaining put. 15. the second put succeeds normally, so its batchop.retcodes is marked as operationstatuscode.success. 16. dominibatchput is done and returns to hregion#put(pair<put, integer>[]), which returns to hregionserver#put(byte[], list<put>). 17. at this point, hregionserver#put(byte[], list<put>) does a for loop and extracts the index of the first put that failed out of the operationstatuscode[]. in our case, it'll return 0 since the first put failed. 18. this index in the list of put of the first that failed (0 in this case) is returned to hregionserver#multiput, which records in the multiputresponse - the client knows that the first put failed but has no idea about the other one. so the client has no reliable way of knowing which put failed (if any) past the first failure. all it knows is that for a particular region, they succeeded up to a particular put, at which point there was a failure, and then the remaining may or may not have succeeded. its best bet is to retry all the put past the index of the first failure for this region. but this has an unintended consequence. the put that were successful during the first run will be re-applied. this will unexpectedly create extra versions. now i realize most people don't really care about versions, so they won't notice. but whoever relies on the versions for whatever reason will rightfully consider this to be data corruption. as it is now, multiput makes proper error handling impossible. since this rpc cannot guarantee any atomicity other than at the individual put level, it should return to the client specific information about which put failed in case of a failure, so that the client can do proper error handling. this requires us to change the multiputresponse so that it can indicate which put specifically failed. we could do this for instance by giving the index of the put along with its operationstatuscode. so in the scenario above, the multiputresponse would essentially return something like: \"for that particular region, put #0 failed, put #1 succeeded\". if we want to save a bit of space, we may want to omit the successes from the response and only mention the failures - so a response that doesn't mention any failure means that everything was successful. not sure whether that's a good idea though. since doing this require an incompatible rpc change, i propose that we take the opportunity to rewrite the multiput rpc too. right now it's very inefficient, it's just a hack on top of put. when multiput is written to the wire, a lot of unnecessary duplicate data is sent out. the timestamp, the row key and the family are sent out to the wire n+1 times, where n is the number of edits for a particular row, instead of just once . alternatively, if we don't want to change the rpcs, we can fix this issue in a backward compatible way by making the while loop in hregion#put(pair<put, integer>[]) stop as soon as a failure is encountered. inability to properly handle notservingregionexception since the code in hregionserver#multiput invokes hregionserver#put(byte[], list<put>) for each region for which there are edits, it's possible that edits for a first region all get successfully applied, then when moving on the 2nd region, a notservingregionexception is thrown, which fails the rpc entirely and leaves the client with absolutely no clue as to which edits were successfully applied or not and which region caused the notservingregionexception. currently the code in hconnectionmanager will simply retry everything when that happens, and it'll invalidate the cached location of all the regions involved in the multi-put (even though it could well be that just a single region has to be invalidated). i'm not sure how to best solve this problem because the hadoop rpc protocol doesn't give us an easy way of passing data around in an exception. the only two things i can think of are both really ugly: 1. the message of the exception could contain the name of the region that caused the exception so that the client can parse the name out of the message and invalidate the right entry in its cache. 2. as a special case, instead of throwing a notservingregionexception, we'd change multiputresponse to also contain a list of regions no longer served by this region server, so the client could invalidate the right entries in its cache and retry just the edits that need to be retried. i think 2. is better but it also requires an incompatible rpc change. repro code simple test case to highlight the bug (against hbase trunk). the hbase client retries hopelessly until it reaches the maximum number of attempts before bailing out. hbase(main):001:0> describe 'mytable' description                                                              enabled                                  {name => 'mytable', families => [{name => 'myfam', bloomfilter => 'none true                                     ', replication_scope => '0', compression => 'none', versions => '1', tt                                          l => '2147483647', blocksize => '65536', in_memory => 'false', blockcac                                          he => 'true'}]}                                                                                                 1 row(s) in 0.7760 seconds import org.apache.hadoop.hbase.hbaseconfiguration; import org.apache.hadoop.hbase.client.htable; import org.apache.hadoop.hbase.client.put; final class put {   public static void main(string[]a) throws exception {     final htable t = new htable(hbaseconfiguration.create(), \"mytable\");     final put p = new put(\"somerow\".getbytes());     p.add(\"badfam\".getbytes(), \"qual2\".getbytes(), \"badvalue\".getbytes());     t.put(p);     t.flushcommits();   } } excerpt of the log produced when running put: hconnectionmanager$tableservers: found root at 127.0.0.1:54754 hconnectionmanager$tableservers: cached location for .meta.,,1.1028785192 is localhost.:54754 hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 1000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 1000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 1000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 2000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 2000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 4000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 4000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 8000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 16000 ms! hconnectionmanager$tableservers: locateregioninmeta(parenttable=.meta., tablename=mytable, row=[115, 111, 109, 101, 114, 111, 119] (\"somerow\"), usecache=true) hconnectionmanager$tableservers: cached location for mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. is localhost.:54754 hconnectionmanager$tableservers: failed past 0 for region: mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db., removing from cache hconnectionmanager$tableservers: removed mytable,,1280754761944.db25b4cc63e97e6a7e1bc28402fe57db. for tablename=mytable from cache because of somerow hconnectionmanager$tableservers: processbatchofputs had some failures, sleeping for 32000 ms! exception in thread \"main\" org.apache.hadoop.hbase.client.retriesexhaustedexception: still had 1 puts left after retrying 10 times. at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.processbatchofputs(hconnectionmanager.java:1534) at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:664) at org.apache.hadoop.hbase.client.htable.doput(htable.java:549) at org.apache.hadoop.hbase.client.htable.put(htable.java:535) at put.main(put.java:10) ",
        "label": 547
    },
    {
        "text": "dapper like tracing  add dapper like tracing to hbase. accumulo added something similar with their cloudtrace package. ",
        "label": 250
    },
    {
        "text": "failed parse of branch element in saveversion sh  a mvn build would fail because of the string it was getting back from svn:       [exec] not part of the command.       execute:java13commandlauncher: executing 'sh' with arguments:        '/home/x/y/hadoop/branches/v.v/hbase/hbase-trunk/src/saveversion.sh'         '0.21.0-snapshot'          '/home/x/y/hadoop/branches/v.v/hbase-trunk/target/generated-sources'         the ' characters around the executable and arguments are          not part of the command.          [exec] sed: -e expression #6, char 48: unterminated `s' command          [exec] result: 1 path amended in the above to protect the innocent. the failure was around parse of branch. branch is not used. ",
        "label": 70
    },
    {
        "text": "hbase spark snappy snappyerror on arm64  when running the hbase-spark unit tests on arm64, the failures are shown as follows: scalatest-maven-plugin:1.0:test (test) @ hbase-spark --- discovery starting. discovery completed in 2 seconds, 837 milliseconds. run starting. expected test count is: 79 hbasedstreamfunctionssuite: formatting using clusterid: testclusterid - bulkput to test hbase client *** failed ***   java.lang.reflect.invocationtargetexception:   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:72)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:65)   at org.apache.spark.broadcast.torrentbroadcast.org$apache$spark$broadcast$torrentbroadcast$$setconf(torrentbroadcast.scala:73)   at org.apache.spark.broadcast.torrentbroadcast.<init>(torrentbroadcast.scala:80)   at org.apache.spark.broadcast.torrentbroadcastfactory.newbroadcast(torrentbroadcastfactory.scala:34)   at org.apache.spark.broadcast.broadcastmanager.newbroadcast(broadcastmanager.scala:63)   ...   cause: java.lang.illegalargumentexception: org.xerial.snappy.snappyerror: [failed_to_load_native_library] no native library is found for os.name=linux and os.arch=aarch64   at org.apache.spark.io.snappycompressioncodec.<init>(compressioncodec.scala:156)   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:72)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:65)   at org.apache.spark.broadcast.torrentbroadcast.org$apache$spark$broadcast$torrentbroadcast$$setconf(torrentbroadcast.scala:73)   at org.apache.spark.broadcast.torrentbroadcast.<init>(torrentbroadcast.scala:80)   at org.apache.spark.broadcast.torrentbroadcastfactory.newbroadcast(torrentbroadcastfactory.scala:34)   ...   cause: org.xerial.snappy.snappyerror: [failed_to_load_native_library] no native library is found for os.name=linux and os.arch=aarch64   at org.xerial.snappy.snappyloader.findnativelibrary(snappyloader.java:331)   at org.xerial.snappy.snappyloader.loadnativelibrary(snappyloader.java:171)   at org.xerial.snappy.snappyloader.load(snappyloader.java:152)   at org.xerial.snappy.snappy.<clinit>(snappy.java:46)   at org.apache.spark.io.snappycompressioncodec.<init>(compressioncodec.scala:154)   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:72)   ... formatting using clusterid: testclusterid partitionfiltersuite: *** run aborted ***   java.lang.reflect.invocationtargetexception:   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:72)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:65)   at org.apache.spark.broadcast.torrentbroadcast.org$apache$spark$broadcast$torrentbroadcast$$setconf(torrentbroadcast.scala:73)   at org.apache.spark.broadcast.torrentbroadcast.<init>(torrentbroadcast.scala:80)   at org.apache.spark.broadcast.torrentbroadcastfactory.newbroadcast(torrentbroadcastfactory.scala:34)   at org.apache.spark.broadcast.broadcastmanager.newbroadcast(broadcastmanager.scala:63)   ...   cause: java.lang.illegalargumentexception: java.lang.noclassdeffounderror: could not initialize class org.xerial.snappy.snappy   at org.apache.spark.io.snappycompressioncodec.<init>(compressioncodec.scala:156)   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:72)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:65)   at org.apache.spark.broadcast.torrentbroadcast.org$apache$spark$broadcast$torrentbroadcast$$setconf(torrentbroadcast.scala:73)   at org.apache.spark.broadcast.torrentbroadcast.<init>(torrentbroadcast.scala:80)   at org.apache.spark.broadcast.torrentbroadcastfactory.newbroadcast(torrentbroadcastfactory.scala:34)   ...   cause: java.lang.noclassdeffounderror: could not initialize class org.xerial.snappy.snappy   at org.apache.spark.io.snappycompressioncodec.<init>(compressioncodec.scala:154)   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:72)   at org.apache.spark.io.compressioncodec$.createcodec(compressioncodec.scala:65)   at org.apache.spark.broadcast.torrentbroadcast.org$apache$spark$broadcast$torrentbroadcast$$setconf(torrentbroadcast.scala:73)   at org.apache.spark.broadcast.torrentbroadcast.<init>(torrentbroadcast.scala:80)   at org.apache.spark.broadcast.torrentbroadcastfactory.newbroadcast(torrentbroadcastfactory.scala:34) root cause:   the spark version in hbase is 1.6.0.  and the snappy version based on hbase-1.6.0 do not support the arm64.  fix it by upgrading snappy-java to 1.1.4 in hbase-spark. ",
        "label": 510
    },
    {
        "text": "add reference documentation on changes made by hbase  replication handling of peer cluster changes   hbase-7634 introduced changes to how the hbase replication handles changes to the composition of a slave cluster. information on this mechanism should be added to the reference documentation for replication. ",
        "label": 178
    },
    {
        "text": "backport the missed doc fix from master to branch  it seems there are many doc fixes aren't in branch-2. file this jira to trace all missed commits. ",
        "label": 99
    },
    {
        "text": "backport hbase to  backup master will never come up if primary master dies during initialization  backport hbase-8519 to 0.94, backup master will never come up if primary master dies during initialization.  the issue described in the hbase-8519 exits in 0.94, we need to backport the hbase-8519 to the 0.94 too. ",
        "label": 234
    },
    {
        "text": "backport hbase to branch  ",
        "label": 352
    },
    {
        "text": "backport hbase  error prone upgrade  to branch  ",
        "label": 149
    },
    {
        "text": "decode memstorets in hfilereaderv2 only when necessary  hfiles v2 store the memstorets of each kv.  in many cases all the kvs in an hfile will have a memstorets of 0 (that is the case when at the time the hfile was written there are no kvs that were created after the oldest still active scanner - which is frequently the case).  in that case we: 1. do not need to decode the memstorets (a vlong), since we know its value is 0 and its length is 1 byte. 2. when we compact hfiles and all of the involved files have only kvs with memstorets = 0 we know ahead of time that all kvs meet this condition and we do not need to store the memstorets in the new hfile. this issue will cover the first part. the performance improvement will be modest as it is fairly cheap to decode vlongs of size 1. ",
        "label": 286
    },
    {
        "text": "refactor recoverlease retries and pauses informed by findings over in hbase  hbase-8359 is an interesting issue that roams near and far. this issue is about making use of the findings handily summarized on the end of hbase-8359 which have it that trunk needs refactor around how it does its recoverlease handling (and that the patch committed against hbase-8359 is not what we want going forward). this issue is about making a patch that adds a lag between recoverlease invocations where the lag is related to dfs timeouts \u2013 the hdfs-side dfs timeout \u2013 and optionally makes use of the isfileclosed api if it is available (a facility that is not yet committed to a branch near you and unlikely to be within your locality with a good while to come). ",
        "label": 314
    },
    {
        "text": "disable writetowal in tests where possible  see discussion in hbase-10665.  we should disable writetoall in all tests except for those that test wal specific stuff, in order to speed up the test suite. ",
        "label": 286
    },
    {
        "text": "quarantine corrupted hfiles with hbck  we've encountered a few upgrades from 0.90 hbases + 20.2/1.x hdfs to 0.92 hbases + hdfs 2.x that get stuck. i haven't been able to duplicate the problem in my dev environment but we suspect this may be related to hdfs-3731. on the hbase side, it seems reasonable to quarantine what are most likely truncated hfiles, so that can could later be recovered. here's an example of the exception we've encountered: 2012-07-18 05:55:01,152 error handler.openregionhandler (openregionhandler.java:openregion(346)) - failed open of region=user_mappings,080112102aa76ef98197605d341b9e6c5824d2bc|1001,1317824890618.eaed0e7abc6d27d28ff0e5a9b49c4c 0d.  java.io.ioexception: java.lang.illegalargumentexception: invalid hfile version: 842220600 (expected to be between 1 and 2)  at org.apache.hadoop.hbase.io.hfile.fixedfiletrailer.readfromstream(fixedfiletrailer.java:306)  at org.apache.hadoop.hbase.io.hfile.hfile.pickreaderversion(hfile.java:371)  at org.apache.hadoop.hbase.io.hfile.hfile.createreader(hfile.java:387)  at org.apache.hadoop.hbase.regionserver.storefile$reader.<init>(storefile.java:1026)  at org.apache.hadoop.hbase.regionserver.storefile.open(storefile.java:485)  at org.apache.hadoop.hbase.regionserver.storefile.createreader(storefile.java:566)  at org.apache.hadoop.hbase.regionserver.store.loadstorefiles(store.java:286)  at org.apache.hadoop.hbase.regionserver.store.<init>(store.java:223)  at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:2534)  at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:454)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:3282)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:3230)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.openregion(openregionhandler.java:331) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:107) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:169)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.lang.illegalargumentexception: invalid hfile version: 842220600 (expected to be between 1 and 2)  at org.apache.hadoop.hbase.io.hfile.hfile.checkformatversion(hfile.java:515)  at org.apache.hadoop.hbase.io.hfile.fixedfiletrailer.readfromstream(fixedfiletrailer.java:303)  ... 17 more specifically \u2013 the fixedfiletrailer are incorrect, and seemingly missing. ",
        "label": 248
    },
    {
        "text": "backport of add increment coalescing in thrift   backport hbase-6043 ",
        "label": 154
    },
    {
        "text": "fix 'should' assertions in testfastfail  over in hbase-14421, testfastfail has been failing assertions that talk of events that 'should' be happening. fix. for now hbase-14421 has disabled the 'should' assertions. they seem fine on apache jenkins build but fail fairly reliably for me on alternate hw. to address, get familiar with the test. change the commented out asserts to be yes/no instead of a 'likely' (on a cursory scan, it is possible that a test run may not involve preemption and it is these runs that are throwing asserts). ",
        "label": 3
    },
    {
        "text": "don't reassign regions if cluster is being shutdown  on stop of a cluster, handling a close message, we'll go ahead and reassign regions as per normal though the cluster up flag is false. this is what cause the testregionrebalancing test to fail up on hudson just now, #1546. ",
        "label": 314
    },
    {
        "text": "add integration test for  the hcat scenario   hbase-8140 needs an integration test. ",
        "label": 339
    },
    {
        "text": "exportsnapshot  import  will fail if copying files to root directory takes longer than cleaner ttl  hbase-17330 removed the checking of the snapshot .tmp directory when determining which files are candidates for deletes. it appears that in the latest branches, this isn't an issue for taking a snapshot as it checks whether a snapshot is in progress via the snapshotmanager. however, when using the exportsnapshot tool to import a snapshot into a cluster, it will first copy the snapshot manifest into /.snapshot/.tmp/<snapshot> [1], copies the files, and then renames the snapshot manifest to the final snapshot directory. if the copyfiles job takes longer than the cleaner ttl, the exportsnapshot job will fail because hfiles will get deleted before the snapshot is committed to the final directory. the exportsnapshot tool already has a functionality to skiptmp and write the manifest directly to the final location. however, this has unintended consequences such as the snapshot appearing to the user before it is usable. so it looks like we will have to bring back the tmp directory check to avoid this situation. [1] https://github.com/apache/hbase/blob/master/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/exportsnapshot.java#l1029 ",
        "label": 188
    },
    {
        "text": " regression  cannot save timestamp in the future  0.20, compared to previous versions, doesn't let you save with a timestamp in the future and will set it to current time without telling you. this is really bad for users upgrading to 0.20 that were using those timestamps. example:  hbase(main):004:0> put 'testtable', 'r1', 'f1:c1', 'val', 5373965335336911168  0 row(s) in 0.0070 seconds  hbase(main):005:0> scan 'testtable'  row column+cell   r1 column=f1:c1, timestamp=1251223892010, value=val   1 row(s) in 0.0380 seconds ",
        "label": 229
    },
    {
        "text": "hfilecontext should adopt builder pattern  hfilecontext is used in so many places for hfile v3, hfilecontext should use the builder pattern. ",
        "label": 544
    },
    {
        "text": "npe in regionreplicareplicationendpoint  2019-04-29 22:29:01,414 error [time-limited test] util.futureutils(72): unexpected error caught when processing completablefuture java.lang.nullpointerexception at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.replicate(regionreplicareplicationendpoint.java:205) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.lambda$2(regionreplicareplicationendpoint.java:247) at org.apache.hadoop.hbase.util.futureutils.lambda$0(futureutils.java:70) at java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) at java.util.concurrent.completablefuture.uniwhencompletestage(completablefuture.java:778) at java.util.concurrent.completablefuture.whencomplete(completablefuture.java:2140) at org.apache.hadoop.hbase.util.futureutils.addlistener(futureutils.java:63) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.replicate(regionreplicareplicationendpoint.java:243) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.replicate(regionreplicareplicationendpoint.java:301) at org.apache.hadoop.hbase.replication.regionserver.testregionreplicareplicationendpoint.testregionreplicareplicationignores(testregionreplicareplicationendpoint.java:449) at org.apache.hadoop.hbase.replication.regionserver.testregionreplicareplicationendpoint.testregionreplicareplicationignoresdisabledtables(testregionreplicareplicationendpoint.java:373) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:50) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:47) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.rules.testwatcher$1.evaluate(testwatcher.java:55) at org.junit.rules.runrules.evaluate(runrules.java:20) at org.junit.runners.parentrunner.runleaf(parentrunner.java:325) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:78) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:57) at org.junit.runners.parentrunner$3.run(parentrunner.java:290) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:71) at org.junit.runners.parentrunner.runchildren(parentrunner.java:288) at org.junit.runners.parentrunner.access$000(parentrunner.java:58) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:268) at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:26) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:27) at org.junit.internal.runners.statements.failontimeout$callablestatement.call(failontimeout.java:298) at org.junit.internal.runners.statements.failontimeout$callablestatement.call(failontimeout.java:292) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.lang.thread.run(thread.java:748) ",
        "label": 149
    },
    {
        "text": " 90rc1  bin hbase script displays  no such file  warning on target cached classpath txt  hbase version: 0.90 release-candidate-1 bin/hbase blindly creates \"target\" directory and fails to generate \"target/cached_classpath.txt\" if it's in the binary (pre-built) distribution. $ bin/hbase shell cat: ... /hbase-0.90.0/bin/../target/cached_classpath.txt: no such file or directory hbase shell; enter 'help<return>' for list of supported commands. bin/hbase 112: add_maven_deps_to_classpath() { 113:   # the maven build dir is called 'target' 114:   target=\"${hbase_home}/target\" 115:   if [ ! -d \"${hbase_home}/target\" ] 116:   then 117:     mkdir \"${target}\" 118:   fi 119:   # need to generate classpath from maven pom. this is costly so generate it 120:   # and cache it. save the file into our target dir so a mvn clean will get 121:   # clean it up and force us create a new one. 122:   f=\"${target}/cached_classpath.txt\" 123:   if [ ! -f \"${f}\" ] 124:   then 125:     ${mvn} -f \"${hbase_home}/pom.xml\" dependency:build-classpath -dmdep.outputfile=\"${f}\" &> /dev/null 126:   fi 127:   classpath=${classpath}:`cat \"${f}\"` maybe we can simply skip this process if \"target\" directory doesn't exist. ",
        "label": 229
    },
    {
        "text": "hbase filesystem does not build against hbase  hbase-filesystem does not build against hbase 1 because hbase 1 does not provide a hbase-zookeeper module, which is a required dependency. this could be moved into a version specific build profile. $ mvn clean install package -dhbase.version=1.4.10 -dhadoop.version=2.9.2   ...  [error] failed to execute goal on project hbase-oss:   could not resolve dependencies for project org.apache.hbase.filesystem:hbase-oss:jar:1.0.0-alpha1:   the following artifacts could not be resolved: org.apache.hbase:hbase-zookeeper:jar:1.4.10,  org.apache.hbase:hbase-zookeeper:jar:tests:1.4.10:   could not find artifact org.apache.hbase:hbase-zookeeper:jar:1.4.10 in central (https://repo.maven.apache.org/maven2) ",
        "label": 38
    },
    {
        "text": "rat check fails if hs err pid26514 log dropped in tests  let test fail because jvm crashed rather than because of rat license complaint ",
        "label": 314
    },
    {
        "text": "split classes for client server module split   prepare classes for the coming hbase-client/hbase-server split. ",
        "label": 154
    },
    {
        "text": "npe in housekeeping  two regionservers hosting thousands of regions. one went down. other was trying to open about 1000 regions and got this: 2009-05-07 10:24:59,995 [regionserver/0:0:0:0:0:0:0:0:60021.worker] info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: region => {name => 'testtable,1094797576,1241684896885', startkey => '1094797576', endkey => '1096819542', encod ed => 1333893125, table => {{name => 'testtable', families => [{name => 'info', compression => 'none', versions => '3', length => '2147483647', ttl => '-1', blocksize => '65536', in_memory => 'false', blockcache => 'false'}]}} 2009-05-07 10:24:59,996 [regionserver/0:0:0:0:0:0:0:0:60021] fatal org.apache.hadoop.hbase.regionserver.hregionserver: unhandled exception. aborting... java.lang.nullpointerexception     at org.apache.hadoop.hbase.regionserver.hregionserver.housekeeping(hregionserver.java:1141)     at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:518)     at java.lang.thread.run(thread.java:619) ",
        "label": 314
    },
    {
        "text": "translate the apache hbase book into japanese  translate the apache hbase book into japanese.   this is my spare-time project and it will take a couple of weeks to translate. ",
        "label": 440
    },
    {
        "text": "provide mutability to compoundconfiguration  in discussion of hbase-8347, it was proposed that compoundconfiguration should support mutability. this can be done by consolidating immutableconfigmap's on first modification to compoundconfiguration. ",
        "label": 248
    },
    {
        "text": "disable reverse dns lookup at hmaster and use the hostname provided by regionserver  description updated: in some unusual network environment, forward dns lookup is supported while reverse dns lookup may not work properly. this jira is to address that hmaster uses the hostname passed from rs instead of doing reverse dns lookup to tells rs which hostname to use during reportforduty() . this has already been implemented by hbase-12954 by adding \"usethishostnameinstead\" field in regionserverstatusprotos. currently \"usethishostnameinstead\" is optional and rs by default only passes port, server start code and server current time info to hmaster during rs reportforduty(). in order to use this field, users currently need to specify \"hbase.regionserver.hostname\" on every regionserver node's hbase-site.xml. this causes some trouble in 1. some deployments managed by some management tools like ambari, which maintains the same copy of hbase-site.xml across all the nodes. 2. hbase-12954 is targeting multihomed hosts, which users want to manually set the hostname value for each node. in the other cases (not multihomed), i just want rs to use the hostname return by the node and set it in usethishostnameinstead and pass to hmaster during reportforduty(). i would like to introduce a setting that if the setting is set to true, \"usethishostnameinstead\" will be set to the hostname rs gets from the node. then hmaster will skip reverse dns lookup because it sees \"usethishostnameinstead\" field is set in the request. \"hbase.regionserver.hostname.reported.to.master\", is it a good name? --------------------  regarding the hostname returned by the rs node, i read the source code again (including hadoop-common dns.java). by default rs gets hostname by calling inetaddress.getlocalhost().getcanonicalhostname(). if users specify \"hbase.regionserver.dns.interface\" or \"hbase.regionserver.dns.nameserver\" or some underlying system configuration changes (eg. modifying /etc/nsswitch.conf), it may first read from dns or other sources instead of first checking /etc/hosts file. ",
        "label": 148
    },
    {
        "text": "implement inexpensive seek operations in hfile  when we early-out of a row because of columns, versions, filters, etc... we seek to the end of that row one key at a time. we should do the seek at the hfile level in cases where we would end up skipping blocks in the process. this will be very common in cases with relatively large rows and regex row filters. if calls that end up doing nothing are constant time, we could also call this to seek to the next column (or even a specific column in explicittracker case). ",
        "label": 357
    },
    {
        "text": "rename rwcc to mvcc  readwriteconcurrencycontrol should be called multiversionconcurrencycontrol. ",
        "label": 34
    },
    {
        "text": "forced splits only act on the first family in a table  while working on a patch for hbase-2375, i came across a few bugs in the existing code related to splits. if a user triggers a manual split, it flips a forcesplit boolean to true and then triggers a compaction (this is very similar to my current implementation for hbase-2375). however, the forcesplit boolean is flipped back to false at the beginning of store.compact(). so the force split only acts on the first family in the table. if that store is not splittable for some reason (it is empty or has only one row), then the entire region will not be split, regardless of what is in other families. even if there is data in the first family, the midkey is determined based solely on that family. if it has two rows and the next family has 1m rows, we pick the split key based on the two rows. ",
        "label": 326
    },
    {
        "text": "testiofencing testfencingaroundcompactionafterwalsync occasionally fails  any one want to take a look at this one? https://builds.apache.org/job/hbase-trunk/4283/testreport/org.apache.hadoop.hbase/testiofencing/testfencingaroundcompactionafterwalsync/ java.lang.assertionerror at org.junit.assert.fail(assert.java:86) at org.junit.assert.asserttrue(assert.java:41) at org.junit.assert.asserttrue(assert.java:52) at org.apache.hadoop.hbase.testiofencing.dotest(testiofencing.java:263) at org.apache.hadoop.hbase.testiofencing.testfencingaroundcompactionafterwalsync(testiofencing.java:217) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.runners.parentrunner.runleaf(parentrunner.java:271) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63) at org.junit.runners.parentrunner.runchildren(parentrunner.java:236) at org.junit.runners.parentrunner.access$000(parentrunner.java:53) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229) at org.junit.runners.parentrunner.run(parentrunner.java:309) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662) ",
        "label": 441
    },
    {
        "text": "there is a space missing from table  foo  is not currently available   go to the following line in loadincrementalhfiles.java throw new tablenotfoundexception(\"table \" + table.getname() + \"is not currently available.\"); and add a space before is and after ' ",
        "label": 87
    },
    {
        "text": "add per kv security details to hbase book  per kv visibility labels  per kv acls ",
        "label": 46
    },
    {
        "text": " hbck tool  usage is wrong for hbck tool for  sidelinecorrupthfiles option  usage is wrong for hbck tool for -sidelinecorrupthfiles option: it is like:  -sidelinecorrupthfiles quarantine corrupted hfiles. implies -checkcorrupthfiles here in \"sidelinecorrupthfiles\" and \"checkcorrupthfiles\" small 'f' is used but actually in code it is like  else if (cmd.equals(\"-checkcorrupthfiles\")) { checkcorrupthfiles = true; } else if (cmd.equals(\"-sidelinecorrupthfiles\")) { sidelinecorrupthfiles = true; } so if we use sidelinecorrupthfiles option for hbck then it will give error unrecognized option:-sidelinecorrupthfiles ",
        "label": 133
    },
    {
        "text": "illegalstateexception  cannot set a region to be closed it it was not already marked as closing  content,ec0ec4d005ee7da14cbd1182339f8cf4,1230416299322 is problem region. below is snippet from master log. looks to me like things are proceeding in order. 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: server xx.xx.xx.51:60020 is overloaded. server load: 74 avg: 48.0, slop: 0.1 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: choosing to reassign 26 regions. mostloadedregions has 10 regions in it. 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,3ea6e20cf48a32fc063912ccae0e960e,1230416307426 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,07730cecbb5b13364cbe17af5778f983,1230448573272 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,e8c750a5983137306e6b3a84eb1fd6e6,1230424193882 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,29be0a07dece4e9bb318c4d253807287,1230356680860 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,23893c82d792ff5c9d3ff99123bc1773,1230390013767 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,e76475012b59500d25000aa7658bc279,1230498762282 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region result_domain,cn.com.sg.www,1230498785950 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,c7b7e7a475765e16d0bbf1e51c95e85d,1230519854914 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,ec0ec4d005ee7da14cbd1182339f8cf4,1230416299322 2008-12-29 12:19:32,074 debug org.apache.hadoop.hbase.master.regionmanager: going to close region content,e16b2ea9f6da8b32fa96ab3490e3d849,1230424086678 2008-12-29 12:19:32,074 info org.apache.hadoop.hbase.master.regionmanager: skipped 0 region(s) that are in transition states 2008-12-29 12:19:32,076 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: pendingopenoperation from xx.xx.xx.51:60020 2008-12-29 12:19:32,076 info org.apache.hadoop.hbase.master.processregionopen$1: content,42b19221f3a60945595b18017d6bacfa,1230357228381 open on xx.xx.xx.51:60020 2008-12-29 12:19:32,076 info org.apache.hadoop.hbase.master.processregionopen$1: updating row content,42b19221f3a60945595b18017d6bacfa,1230357228381 in region .meta.,,1 with startcode 1230571104777 and server xx.xx.xx.51:60020 2008-12-29 12:19:32,077 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: pendingopenoperation from xx.xx.xx.51:60020 2008-12-29 12:19:32,077 info org.apache.hadoop.hbase.master.processregionopen$1: content,3c619bc7a2c824fb3c4f5d747b0f2f12,1230354673241 open on xx.xx.xx.51:60020 2008-12-29 12:19:32,077 info org.apache.hadoop.hbase.master.processregionopen$1: updating row content,3c619bc7a2c824fb3c4f5d747b0f2f12,1230354673241 in region .meta.,,1 with startcode 1230571104777 and server xx.xx.xx.51:60020 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,36946541ed9a62f419cf7238d32a6a38,1230448587552, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,58221f5f3a79b7dd5e23c339c507a7c6,1230487059978, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,94b170cae87992c33c3913d74bb257e4,1230498360162, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,bb34d860d76c1eabbe2eb7d2ce8148d9,1230416266955, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,8ac5fbdd63b313ce252838a189c73438,1230530000480, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,f9f5b2d035349cf4292ed7dd7e9823f0,1230505507313, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,4b30e2a8b7949ed62e097824969f8bca,1230448501152, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,0c4b7c2825f674510a511233e9e0c539,1230530122456, false 2008-12-29 12:19:32,078 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,43f83c42e08bb0f4b1f6d1125f39f018,1230498357831, false 2008-12-29 12:19:32,299 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,43f83c42e08bb0f4b1f6d1125f39f018,1230498357831 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,300 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,bb34d860d76c1eabbe2eb7d2ce8148d9,1230416266955 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,301 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,94b170cae87992c33c3913d74bb257e4,1230498360162 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,301 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,8ac5fbdd63b313ce252838a189c73438,1230530000480 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,302 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,36946541ed9a62f419cf7238d32a6a38,1230448587552 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,302 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,58221f5f3a79b7dd5e23c339c507a7c6,1230487059978 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,303 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,f9f5b2d035349cf4292ed7dd7e9823f0,1230505507313 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,303 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,4b30e2a8b7949ed62e097824969f8bca,1230448501152 to server xx.xx.xx.34:60020 2008-12-29 12:19:32,304 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,0c4b7c2825f674510a511233e9e0c539,1230530122456 to server xx.xx.xx.34:60020 2008-12-29 12:19:35,080 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,3ea6e20cf48a32fc063912ccae0e960e,1230416307426 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,07730cecbb5b13364cbe17af5778f983,1230448573272 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,3ea6e20cf48a32fc063912ccae0e960e,1230416307426, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,e8c750a5983137306e6b3a84eb1fd6e6,1230424193882 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,07730cecbb5b13364cbe17af5778f983,1230448573272, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,29be0a07dece4e9bb318c4d253807287,1230356680860 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,e8c750a5983137306e6b3a84eb1fd6e6,1230424193882, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,23893c82d792ff5c9d3ff99123bc1773,1230390013767 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,29be0a07dece4e9bb318c4d253807287,1230356680860, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,e76475012b59500d25000aa7658bc279,1230498762282 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,23893c82d792ff5c9d3ff99123bc1773,1230390013767, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: result_domain,cn.com.sg.www,1230498785950 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,e76475012b59500d25000aa7658bc279,1230498762282, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,c7b7e7a475765e16d0bbf1e51c95e85d,1230519854914 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of result_domain,cn.com.sg.www,1230498785950, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,ec0ec4d005ee7da14cbd1182339f8cf4,1230416299322 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,c7b7e7a475765e16d0bbf1e51c95e85d,1230519854914, false 2008-12-29 12:19:35,081 info org.apache.hadoop.hbase.master.servermanager: received msg_report_close: content,e16b2ea9f6da8b32fa96ab3490e3d849,1230424086678 from xx.xx.xx.51:60020 2008-12-29 12:19:35,081 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,ec0ec4d005ee7da14cbd1182339f8cf4,1230416299322, false 2008-12-29 12:19:35,082 debug org.apache.hadoop.hbase.master.hmaster: main processing loop: processregionclose of content,e16b2ea9f6da8b32fa96ab3490e3d849,1230424086678, false 2008-12-29 12:19:35,083 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 9 on 60000, call regionserverreport(address: xx.xx.xx.51:60020, startcode: 1230571104777, load: (requests=0, regions=64, usedheap=99, maxheap=1777), [lorg. apache.hadoop.hbase.hmsg;@3e2dce4e, [lorg.apache.hadoop.hbase.hregioninfo;@314382c6) from xx.xx.xx.51:55371: error: java.io.ioexception: java.lang.illegalstateexception: cannot set a region to be closed if it was not already marked as clo sing. region: content,ec0ec4d005ee7da14cbd1182339f8cf4,1230416299322 java.io.ioexception: java.lang.illegalstateexception: cannot set a region to be closed if it was not already marked as closing. region: content,ec0ec4d005ee7da14cbd1182339f8cf4,1230416299322         at org.apache.hadoop.hbase.master.regionmanager$regionstate.setclosed(regionmanager.java:1108)         at org.apache.hadoop.hbase.master.regionmanager.setclosed(regionmanager.java:816)         at org.apache.hadoop.hbase.master.servermanager.processmsgs(servermanager.java:377)         at org.apache.hadoop.hbase.master.servermanager.processregionserverallswell(servermanager.java:327)         at org.apache.hadoop.hbase.master.servermanager.regionserverreport(servermanager.java:240)         at org.apache.hadoop.hbase.master.hmaster.regionserverreport(hmaster.java:570)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:894) ",
        "label": 241
    },
    {
        "text": "log multi  requests for more than threshold number of rows  today, if a user happens to do something like a large multi-put, they can get through request throttling (e.g. it is one request) but still crash a region server with a garbage storm. we have seen regionservers hit this issue and it is silent and deadly. the rs will report nothing more than a mysterious garbage collection and exit out. ideally, we could report a large multi-* request before starting it, in case it happens to be deadly. knowing the client, user and how many rows are affected would be a good start to tracking down painful users. ",
        "label": 129
    },
    {
        "text": "testlogrolling fails in branch killing the test suite up on jenkins  see recent 0.90 builds up on jenkins: https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.90/471/console ",
        "label": 557
    },
    {
        "text": "start of new version master fails if old master's znode is hanging around  i shut down an 0.90 cluster, and had to do so uncleanly. i then started a trunk (0.92) cluster before the old master znode had expired. this cased: java.lang.stringindexoutofboundsexception: string index out of range: -1  at java.lang.string.substring(string.java:1937)  at org.apache.hadoop.hbase.servername.parsehostname(servername.java:81)  at org.apache.hadoop.hbase.servername.<init>(servername.java:63)  at org.apache.hadoop.hbase.master.activemastermanager.blockuntilbecomingactivemaster(activemastermanager.java:148)  at org.apache.hadoop.hbase.master.hmaster.becomeactivemaster(hmaster.java:342)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:297) ",
        "label": 314
    },
    {
        "text": "periodicmemstoreflusher should inspect the queue before adding a delayed flush request  if you run a big job every 4 hours, impacting many tables (they have 150 regions per server), ad the end all the regions might have some data to be flushed, and we want, after one hour, trigger a periodic flush. that's totally fine. now, to avoid a flush storm, when we detect a region to be flushed, we add a \"randomdelay\" to the delayed flush, that way we spread them away. range_of_delay is 5 minutes. so we spread the flush over the next 5 minutes, which is very good. however, because we don't check if there is already a request in the queue, 10 seconds after, we create a new request, with a new randomdelay. if you generate a randomdelay every 10 seconds, at some point, you will end up having a small one, and the flush will be triggered almost immediatly. as a result, instead of spreading all the flush within the next 5 minutes, you end-up getting them all way more quickly. like within the first minute. which not only feed the queue to to many flush requests, but also defeats the purpose of the randomdelay.     @override     protected void chore() {       final stringbuffer whyflush = new stringbuffer();       for (region r : this.server.onlineregions.values()) {         if (r == null) continue;         if (((hregion)r).shouldflush(whyflush)) {           flushrequester requester = server.getflushrequester();           if (requester != null) {             long randomdelay = randomutils.nextint(range_of_delay) + min_delay_time;             log.info(getname() + \" requesting flush of \" +               r.getregioninfo().getregionnameasstring() + \" because \" +               whyflush.tostring() +               \" after random delay \" + randomdelay + \"ms\");             //throttle the flushes by putting a delay. if we don't throttle, and there             //is a balanced write-load on the regions in a table, we might end up             //overwhelming the filesystem with too many flushes at once.             requester.requestdelayedflush(r, randomdelay, false);           }         }       }     } 2017-07-24 18:44:33,338 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 270785ms 2017-07-24 18:44:43,328 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 200143ms 2017-07-24 18:44:53,954 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 191082ms 2017-07-24 18:45:03,528 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 92532ms 2017-07-24 18:45:14,201 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 238780ms 2017-07-24 18:45:24,195 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 35390ms 2017-07-24 18:45:33,362 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 283034ms 2017-07-24 18:45:43,933 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 84328ms 2017-07-24 18:45:53,866 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 72291ms 2017-07-24 18:46:03,329 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 62658ms 2017-07-24 18:46:14,084 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 87491ms 2017-07-24 18:46:23,538 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 290170ms 2017-07-24 18:46:33,353 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 228541ms 2017-07-24 18:46:43,359 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 211814ms 2017-07-24 18:46:54,150 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 155167ms 2017-07-24 18:47:03,546 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 115318ms 2017-07-24 18:47:13,350 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 216662ms 2017-07-24 18:47:23,329 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 96018ms 2017-07-24 18:47:33,891 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 144751ms 2017-07-24 18:47:43,329 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 152461ms 2017-07-24 18:47:53,832 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 97352ms 2017-07-24 18:48:03,348 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 263910ms 2017-07-24 18:48:13,353 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 151793ms 2017-07-24 18:48:23,340 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 61926ms 2017-07-24 18:48:33,724 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 188901ms 2017-07-24 18:48:44,038 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 233148ms 2017-07-24 18:48:53,624 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 6206ms 2017-07-24 18:49:03,378 info org.apache.hadoop.hbase.regionserver.hregionserver: hbasetest2.domainname.com,60020,1500916375517-memstoreflusherchore requesting flush of testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. because f has an old edit so flush to free wals after random delay 240946ms 2017-07-24 18:49:04,125 info org.apache.hadoop.hbase.regionserver.hregion: flushing 1/1 column families, memstore=160 b 2017-07-24 18:49:04,182 info org.apache.hadoop.hbase.regionserver.defaultstoreflusher: flushed, sequenceid=6, memsize=160, hasbloomfilter=true, into tmp file hdfs://hbasetest1.domainname.com:8020/hbase/data/default/testflush/578c27d2eb7ef0ad437bf2ff38c053ae/.tmp/3609340d58ae4ca190e62c3b0fe415f4 2017-07-24 18:49:04,208 info org.apache.hadoop.hbase.regionserver.hstore: added hdfs://hbasetest1.domainname.com:8020/hbase/data/default/testflush/578c27d2eb7ef0ad437bf2ff38c053ae/f/3609340d58ae4ca190e62c3b0fe415f4, entries=1, sequenceid=6, filesize=1015 2017-07-24 18:49:04,212 info org.apache.hadoop.hbase.regionserver.hregion: finished memstore flush of ~160 b/160, currentsize=0 b/0 for region testflush,,1500932649126.578c27d2eb7ef0ad437bf2ff38c053ae. in 87ms, sequenceid=6, compaction requested=false ",
        "label": 494
    },
    {
        "text": "hbadmin is not closed after loadincrementalhfiles completes  i was looking at loadincrementalhfiles since it is used in backup / restore feature. hbadmin is set in loadincrementalhfiles#initialize(). however, it is not closed when run() method completes. ",
        "label": 198
    },
    {
        "text": "deletefamily takes timestamp  should only take row and family  javadoc describes both cases but only implements the timestamp case   the three version of deletefamily in client.htable (text, string, byte[]) have varying descriptions about whether they take timestamps or not. public void deletefamily(org.apache.hadoop.io.text row, org.apache.hadoop.io.text family, long timestamp) throws ioexception  delete all cells for a row with matching column family at all timestamps. public void deletefamily(string row, string family, long timestamp) throws ioexception  delete all cells for a row with matching column family at all timestamps. public void deletefamily(byte[] row, byte[] family, long timestamp) throws ioexception  delete all cells for a row with matching column family with timestamps less than or equal to timestamp. these will become: public void deletefamily(org.apache.hadoop.io.text row, org.apache.hadoop.io.text family) throws ioexception  delete all cells for a row with matching column family at all timestamps. public void deletefamily(string row, string family) throws ioexception  delete all cells for a row with matching column family at all timestamps. public void deletefamily(byte[] row, byte[] family) throws ioexception  delete all cells for a row with matching column family at all timestamps. per jean-daniel's comment, deleteall should then not permit families. i'm unsure whether this is currently allowed or not, but the documentation must be updated either way. will post patch after more thorough testing. ",
        "label": 218
    },
    {
        "text": "deleting a column in mapreduce fails  in latest trunk, deleting a column in batchupdate causes exception because batchupdate's copy constructor (or whatever they are called in java) directly calls batchupdate#put even in delete-s thus causing put to throw illegalargumentexception(\"passed value cannot be null\"). ",
        "label": 144
    },
    {
        "text": "print hdfs pipeline when hlog's sync is slow  sometimes the slow sync of hlog writer is because there is an abnormal datanode in the pipeline. so it will be helpful to print the pipeline of slow sync to diagnose those problems. the ultimate solution is to join the trace of hbase and hdfs. ",
        "label": 411
    },
    {
        "text": "filter on one cf and if a match  then load and return full row  was  improve performance of scans with some kind of filters   when the scan is performed, whole row is loaded into result list, after that filter (if exists) is applied to detect that row is needed. but when scan is performed on several cfs and filter checks only data from the subset of these cfs, data from cfs, not checked by a filter is not needed on a filter stage. only when we decided to include current row. and in such case we can significantly reduce amount of io performed by a scan, by loading only values, actually checked by a filter. for example, we have two cfs: flags and snap. flags is quite small (bunch of megabytes) and is used to filter large entries from snap. snap is very large (10s of gb) and it is quite costly to scan it. if we needed only rows with some flag specified, we use singlecolumnvaluefilter to limit result to only small subset of region. but current implementation is loading both cfs to perform scan, when only small subset is needed. attached patch adds one routine to filter interface to allow filter to specify which cf is needed to it's operation. in hregion, we separate all scanners into two groups: needed for filter and the rest (joined). when new row is considered, only needed data is loaded, filter applied, and only if filter accepts the row, rest of data is loaded. at our data, this speeds up such kind of scans 30-50 times. also, this gives us the way to better normalize the data into separate columns by optimizing the scans performed. ",
        "label": 406
    },
    {
        "text": "strange dependencies for hbase client  here is what we have with hadoop 2.  in our plate we have junit (should be test, it's not because we use it in the integration test runner log4j (we have direct dependencies in regionplacementmaintainer, restservlet, and logmonitoring, not counting the dependencies in the tests) the others are in hadoop. i marked the ones that were strange to me. do we need all of them? mvn dependency:tree -pl hbase-client -dhadoop.profile=2.0  [info] \u2014 maven-dependency-plugin:2.1:tree (default-cli) @ hbase-client \u2014  [info] org.apache.hbase:hbase-client:jar:0.97.0-snapshot  [info] +- org.apache.hbase:hbase-common:jar:0.97.0-snapshot:compile  [info] | - commons-collections:commons-collections:jar:3.2.1:compile  [info] +- org.apache.hbase:hbase-common:test-jar:tests:0.97.0-snapshot:test  [info] +- org.apache.hbase:hbase-protocol:jar:0.97.0-snapshot:compile  [info] +- commons-codec:commons-codec:jar:1.7:compile  [info] +- commons-io:commons-io:jar:2.4:compile  [info] +- commons-lang:commons-lang:jar:2.6:compile  [info] +- commons-logging:commons-logging:jar:1.1.1:compile  [info] +- com.google.guava:guava:jar:12.0.1:compile  [info] | - com.google.code.findbugs:jsr305:jar:1.3.9:compile  [info] +- com.google.protobuf:protobuf-java:jar:2.5.0:compile  [info] +- org.apache.zookeeper:zookeeper:jar:3.4.5:compile  [info] | +- org.slf4j:slf4j-api:jar:1.6.4:compile  [info] | - org.slf4j:slf4j-log4j12:jar:1.6.1:compile  [info] +- org.cloudera.htrace:htrace-core:jar:2.01:compile  [info] | - org.mortbay.jetty:jetty-util:jar:6.1.26:compile <======= why?  [info] +- org.codehaus.jackson:jackson-mapper-asl:jar:1.8.8:compile  [info] | - org.codehaus.jackson:jackson-core-asl:jar:1.8.8:compile  [info] +- io.netty:netty:jar:3.5.9.final:compile  [info] +- log4j:log4j:jar:1.2.17:test (scope not updated to compile)  [info] +- org.apache.hadoop:hadoop-common:jar:2.1.0-beta:compile  [info] | +- commons-cli:commons-cli:jar:1.2:compile  [info] | +- org.apache.commons:commons-math:jar:2.2:compile (version managed from 2.1)  [info] | +- xmlenc:xmlenc:jar:0.52:compile  [info] | +- commons-httpclient:commons-httpclient:jar:3.0.1:compile (version managed from 3.1) <===== decrease the version. dangerous. but why hadoop does this?  [info] | +- commons-net:commons-net:jar:3.1:compile  [info] | +- javax.servlet:servlet-api:jar:2.5:compile <==== why a servlet api in hbase-client or hadoop common?  [info] | +- org.mortbay.jetty:jetty:jar:6.1.26:compile  [info] | +- com.sun.jersey:jersey-core:jar:1.8:compile  [info] | +- com.sun.jersey:jersey-json:jar:1.8:compile  [info] | | +- org.codehaus.jettison:jettison:jar:1.3.1:compile (version managed from 1.1)  [info] | | | - stax:stax-api:jar:1.0.1:compile  [info] | | +- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile  [info] | | | - javax.xml.bind:jaxb-api:jar:2.1:compile (version managed from 2.2.2) <===== decrease the version. dangerous  [info] | | | - javax.activation:activation:jar:1.1:compile  [info] | | +- org.codehaus.jackson:jackson-jaxrs:jar:1.8.8:compile (version managed from 1.7.1)  [info] | | - org.codehaus.jackson:jackson-xc:jar:1.8.8:compile (version managed from 1.7.1)  [info] | +- com.sun.jersey:jersey-server:jar:1.8:compile <========= why a server in a common piece of code? could we exclude it from hbase client?  [info] | | - asm:asm:jar:3.1:compile  *[info] | +- tomcat:jasper-compiler:jar:5.5.23:runtime <=== ??? why *  [info] | +- tomcat:jasper-runtime:jar:5.5.23:runtime <=== ??? why  [info] | +- javax.servlet.jsp:jsp-api:jar:2.1:runtime <==== why? could we exclude it from hbase client?  [info] | +- commons-el:commons-el:jar:1.0:runtime  [info] | +- net.java.dev.jets3t:jets3t:jar:0.6.1:compile  [info] | +- commons-configuration:commons-configuration:jar:1.6:compile  [info] | | +- commons-digester:commons-digester:jar:1.8:compile  [info] | | | - commons-beanutils:commons-beanutils:jar:1.7.0:compile  [info] | | - commons-beanutils:commons-beanutils-core:jar:1.8.0:compile  [info] | +- org.apache.avro:avro:jar:1.5.3:compile  [info] | | +- com.thoughtworks.paranamer:paranamer:jar:2.3:compile  [info] | | - org.xerial.snappy:snappy-java:jar:1.0.3.2:compile  [info] | +- com.jcraft:jsch:jar:0.1.42:compile  [info] | - org.apache.commons:commons-compress:jar:1.4:compile  [info] | - org.tukaani:xz:jar:1.0:compile  [info] +- org.apache.hadoop:hadoop-auth:jar:2.1.0-beta:compile  [info] +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.1.0-beta:compile  [info] | +- org.apache.hadoop:hadoop-yarn-common:jar:2.1.0-beta:compile  [info] | | +- org.apache.hadoop:hadoop-yarn-api:jar:2.1.0-beta:compile  [info] | | +- com.google.inject:guice:jar:3.0:compile  [info] | | | +- javax.inject:javax.inject:jar:1:compile  [info] | | | - aopalliance:aopalliance:jar:1.0:compile  [info] | | +- com.sun.jersey.jersey-test-framework:jersey-test-framework-grizzly2:jar:1.8:compile <======== wow!  [info] | | | +- com.sun.jersey.jersey-test-framework:jersey-test-framework-core:jar:1.8:compile  [info] | | | | +- org.glassfish:javax.servlet:jar:3.0:compile  [info] | | | | - com.sun.jersey:jersey-client:jar:1.8:compile  [info] | | | - com.sun.jersey:jersey-grizzly2:jar:1.8:compile  [info] | | | +- org.glassfish.grizzly:grizzly-http:jar:2.1.1:compile  [info] | | | | - org.glassfish.grizzly:grizzly-framework:jar:2.1.1:compile  [info] | | | | - org.glassfish.gmbal:gmbal-api-only:jar:3.0.0-b023:compile  [info] | | | | - org.glassfish.external:management-api:jar:3.0.0-b012:compile  [info] | | | +- org.glassfish.grizzly:grizzly-http-server:jar:2.1.1:compile  [info] | | | | - org.glassfish.grizzly:grizzly-rcm:jar:2.1.1:compile  [info] | | | - org.glassfish.grizzly:grizzly-http-servlet:jar:2.1.1:compile  [info] | | | - org.glassfish.grizzly:grizzly-framework:jar:tests:2.1.1:compile  [info] | | - com.sun.jersey.contribs:jersey-guice:jar:1.8:compile  [info] | - com.google.inject.extensions:guice-servlet:jar:3.0:compile  [info] +- org.apache.hadoop:hadoop-annotations:jar:2.1.0-beta:compile  [info] | - jdk.tools:jdk.tools:jar:1.6:system  [info] +- com.github.stephenc.findbugs:findbugs-annotations:jar:1.3.9-1:compile  [info] +- junit:junit:jar:4.11:compile <==================== bad  [info] - org.mockito:mockito-all:jar:1.9.0:test ",
        "label": 340
    },
    {
        "text": " hbase  add a read only attribute to columns  one day we'll have support for per-column acls. meantime, it should be possible to mark loaded content read-only so it doesn't get trashed accidentally. ",
        "label": 38
    },
    {
        "text": "master assigns region in the original region server when opening region failed  master assigns region in the original region server when rs_zk_region_failed_open envent was coming.  maybe we should choose other region server. [2012-03-07 10:14:21,750] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:14:31,826] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:14:41,903] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:14:51,975] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:15:02,056] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:15:12,167] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:15:22,231] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:15:32,303] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:15:42,375] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:15:52,447] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:16:02,528] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:16:12,600] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053  [2012-03-07 10:16:22,676] [debug] [main-eventthread] [org.apache.hadoop.hbase.master.assignmentmanager 553] handling transition=rs_zk_region_failed_open, server=158-1-130-11,20020,1331108408232, region=c70e98bdca98a0657a56436741523053 ",
        "label": 56
    },
    {
        "text": "in general  close should not throw exception  in particular, a close of a scanner whose lease has timed out elsewhere will throw an unknownscannerexception rather than just return. makes it so can't easily put the close into a finally on client side. this issue is about reviewing all calls to close to make sure all don't throw exceptions gratutiously. ",
        "label": 86
    },
    {
        "text": "rsgroups are broken under amv2  the following rsgroups tests were disabled by core proc-v2 am in hbase-14614: disabled/ignore testrsgroupsofflinemode#testoffline; need to dig in on what offline is. disabled/ignore testrsgroups. this jira tracks the work to enable them (or remove/modify if not applicable). ",
        "label": 60
    },
    {
        "text": "scan doesn't return rows for user who has authorization by visibility label in secure deployment  in secure deployment of 0.98 tip, i did:  as user hbase: add_labels 'a' create 'tb', 'f1' put 'tb', 'row', 'f1:q', 'v1', {visibility=>'a'} set_auths 'oozie', ['a'] as user oozie: hbase(main):001:0> scan 'tb', { authorizations => ['a']} row                                          column+cell 0 row(s) in 0.1030 seconds here is my config:   <property>     <name>hfile.format.version</name>     <value>3</value>   </property>   <property>    <name>hbase.coprocessor.master.classes</name>    <value>org.apache.hadoop.hbase.security.visibility.visibilitycontroller</value>   </property>   <property>    <name>hbase.coprocessor.region.classes</name>    <value>org.apache.hadoop.hbase.security.visibility.visibilitycontroller</value>   </property>   <property>    <name>hbase.regionserver.scan.visibility.label.generator.class&lt;/name>    <value>org.apache.hadoop.hbase.security.visibility.defaultscanlabelgenerator</value>   </property> ",
        "label": 441
    },
    {
        "text": "hfileoutputformat2 hardcodes default fileoutputcommitter  apache kylin uses hbase's hfileoutputformat2.java to configure the mr job. the original reporting is in kylin-2788[1]. after some investigation, we found this class always uses the default \"fileoutputcommitter\", see [2], regardless of the job's configuration; so it always writing to \"_temporary\" folder. since aws emr configured to use directoutputcommitter for s3, then this problem occurs: hadoop expects to see the file directly under output path, while the recordwriter generates them in \"_temporary\" folder. this caused no data be loaded to htable. seems this problem exists in all versions so far. [1] https://issues.apache.org/jira/browse/kylin-2788  [2] https://github.com/apache/hbase/blob/master/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/mapreduce/hfileoutputformat2.java#l193 ",
        "label": 410
    },
    {
        "text": "authentication for thriftserver clients  the current implementation of hbase client authentication only works with the java api. alternate access gateways, like thrift and rest are left out and will not work. for the thriftserver to be able to fully interoperate with the security implementation: 1. the thriftserver should be able to login from a keytab file with it's own server principal on startup 2. thrift clients should be able to authenticate securely when connecting to the server 3. the thriftserver should be able to act as a proxy for those clients so that the rpcs it issues will be correctly authorized as the original client identities there is already some support for step 3 in usergroupinformation and related classes. for step #2, we really need to look at what thrift itself supports. at a bare minimum, we need to implement step #1. if we do this, even without steps 2 & 3, this would at least allow deployments to use a thriftserver per application user, and have the server login as that user on startup. thrift clients may not be directly authenticated, but authorization checks for hbase could still be handled correctly this way. ",
        "label": 314
    },
    {
        "text": "break hmaster metrics into multiple contexts  some metrics about the hmaster are about rpc call duration and other are about how internal processes are running. we should make the difference clear so that adding more of either kind doesn't confuse the user. ",
        "label": 154
    },
    {
        "text": "fix testlogrolling testlogrollondatanodedeath  it appears several times on the failing list of pre commit result. the test code is a bit confusing to me. need to dig more. ",
        "label": 149
    },
    {
        "text": "npe processing server crash in metareader  getserveruserregions  2010-09-21 18:28:30,856 error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event m_server_shutdown java.lang.nullpointerexception        at org.apache.hadoop.hbase.catalog.metareader.getserveruserregions(metareader.java:413)        at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:106)        at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:150)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:619) problem is that there was no server set in .meta. yet for a particular row but we went ahead tried to use the server as though it non-null. ",
        "label": 314
    },
    {
        "text": "canary sniff  should close the htable instance        table = new htable(admin.getconfiguration(), tabledesc.getname()); htable instance should be closed by the end of the method. ",
        "label": 441
    },
    {
        "text": "bucketcache nullpointerexception in cacheblock  there is no synchronized in the check in cacheblock, and wen see nullpointerexception in production cluster. 2019-05-17 18:17:21,299 error [rpcserver.default.fpbq.fifo.handler=20,queue=7,port=16020] ipc.rpcserver: unexpected throwable object java.lang.nullpointerexception         at org.apache.hadoop.hbase.io.hfile.bucket.bucketcache.returnblock(bucketcache.java:1665)         at org.apache.hadoop.hbase.io.hfile.blockcacheutil.shouldreplaceexistingcacheblock(blockcacheutil.java:250)         at org.apache.hadoop.hbase.io.hfile.bucket.bucketcache.cacheblockwithwait(bucketcache.java:426)         at org.apache.hadoop.hbase.io.hfile.bucket.bucketcache.cacheblock(bucketcache.java:412)         at org.apache.hadoop.hbase.io.hfile.combinedblockcache.cacheblock(combinedblockcache.java:67)         at org.apache.hadoop.hbase.io.hfile.hfilereaderimpl.lambda$readblock$2(hfilereaderimpl.java:1501)         at java.util.optional.ifpresent(optional.java:159)         at org.apache.hadoop.hbase.io.hfile.hfilereaderimpl.readblock(hfilereaderimpl.java:1499)         at org.apache.hadoop.hbase.io.hfile.hfileblockindex$cellbasedkeyblockindexreader.loaddatablockwithscaninfo(hfileblockindex.java:340)         at org.apache.hadoop.hbase.io.hfile.hfileblockindex$blockindexreader.seektodatablock(hfileblockindex.java:577)         at org.apache.hadoop.hbase.io.hfile.hfilereaderimpl$hfilescannerimpl.seekbefore(hfilereaderimpl.java:869)         at org.apache.hadoop.hbase.regionserver.storefilescanner.seektopreviousrow(storefilescanner.java:515)         at org.apache.hadoop.hbase.regionserver.reversedkeyvalueheap.next(reversedkeyvalueheap.java:135)         at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:644)         at org.apache.hadoop.hbase.regionserver.keyvalueheap.next(keyvalueheap.java:153)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.populateresult(hregion.java:6612)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.nextinternal(hregion.java:6776)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.nextraw(hregion.java:6549)         at org.apache.hadoop.hbase.regionserver.rsrpcservices.scan(rsrpcservices.java:3183)         at org.apache.hadoop.hbase.regionserver.rsrpcservices.scan(rsrpcservices.java:3428)         at org.apache.hadoop.hbase.shaded.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:42002)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:413)         at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:132)         at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:324)         at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:304) ",
        "label": 292
    },
    {
        "text": "masterobserver premove  and postmove  should throw ioexception instead of unknownregionexception  we've standardized on ioexception as the main way for coprocessors to communicate errors back out of the observer hooks. all observer hooks throw ioe except for masterobserver.premove() and masterobserver.postmove(), which throw unknownregionexception, since that's what hmasterinterface.move() declares. in hindsight, making these two masterobserver methods inconsistent seems like a mistake. i think we should change masterobserver.premove() and masterobserver.postmove() to throw ioexception for consistency with the other methods. we could deprecate the existing hmasterinterface.move() method to have it switch over to throwing ioexception as well, but this would require creating a version with a new name, which seems unnecessarily ugly. so i'd suggest we just have hmaster.move() handle the ioexception and use it to init an unknownregionexception. wonky as that is, it seems the lesser evil. ",
        "label": 180
    },
    {
        "text": "create a config that forces to cache blocks on compaction  in cases where users care a lot about read performance for tables that are small enough to fit into a cache (or the cache is large enough), prefetchonopen can be enabled to make the entire table available in cache after the initial region opening is completed. any new data can also be guaranteed to be in cache with the cacheblocksonwrite setting. however, the missing piece is when all blocks are evicted after a compaction. we found very poor performance after compactions for tables under heavy read load and a slower backing filesystem (s3). after a compaction the prefetching threads need to compete with threads servicing read requests and get constantly blocked as a result.  this is a proposal to introduce a new cache configuration option that would cache blocks on write during compaction for any column family that has prefetch enabled. this would virtually guarantee all blocks are kept in cache after the initial prefetch on open is completed allowing for guaranteed steady read performance despite a slow backing file system. ",
        "label": 221
    },
    {
        "text": "m r on bulk imported tables  we are bulk importing using loadtable.rb and running m/r jobs using hbase as input. we're taking the following steps:  1a. load hbase with a m/r job using the normal api.   or  1b. load hbase with bulk import. then 2a. using the shell, do a \"count\" over the table.  or  2b. run a m/r job that scans the whole hbase table (and nothing else). of the 4 combos, 3 are fine: 1a+2a, 1a+2b, 1b+2a. we're having trouble with 1b+2b. when we run the m/r job, it doesn't seem to read in any records, but there are no explicit errors in either the hadoop or hbase logs. any ideas on what might be wrong with the bulk import to cause this problem? we confirmed this problem exists in both hbase-0.20.3 and hbase-0.20.4. we have created dummy data (see attached). this is the test case: after loading the data into hdfs. in hbase shell:  create 'tiny', 'values' execute: {hbase-home}/bin/hbase org.jruby.main {hbase-home} /bin/loadtable.rb tiny tinytable then run the simple row counter {hadoop-home} /bin/hadoop jar {hbase-home} /hbase-0.20.x.jar rowcounter tiny values notice that map input records read is always zero. we confirmed that other mapreduce jobs do not execute the map function at all, always returning 0 records. we also ran a major_compaction of all hbase tables (.meta. and .root. as well) but this did not fix the problem. ",
        "label": 314
    },
    {
        "text": "handle empty qualifier better in shell for increments  when trying to increment a counter using the examples, which specify no explicit qualifier you get an error: hbase(main):014:0> incr 'testtable', 'cnt1', 'colfam1', 1 error: org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server 10.0.0.57:51640 for region testtable,,1300267113942.cd2e7925140eb414d519621e384fb654., row 'cnt1', but failed after 7 attempts. exceptions: java.io.ioexception: java.io.ioexception: java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.columncount.<init>(columncount.java:47)         at org.apache.hadoop.hbase.regionserver.explicitcolumntracker.<init>(explicitcolumntracker.java:69)         at org.apache.hadoop.hbase.regionserver.scanquerymatcher.<init>(scanquerymatcher.java:93)         at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:65)         at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:1436)         at org.apache.hadoop.hbase.regionserver.hregion$regionscanner.<init>(hregion.java:2412)         at org.apache.hadoop.hbase.regionserver.hregion.instantiateinternalscanner(hregion.java:1185)         at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1171)         at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1155)         at org.apache.hadoop.hbase.regionserver.hregion.getlastincrement(hregion.java:3087)         at org.apache.hadoop.hbase.regionserver.hregion.incrementcolumnvalue(hregion.java:3312)         at org.apache.hadoop.hbase.regionserver.hregionserver.incrementcolumnvalue(hregionserver.java:2570)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:309)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1060) here is some help for this command: increments a cell 'value' at specified table/row/column coordinates. to increment a cell value in table 't1' at row 'r1' under column 'c1' by 1 (can be omitted) or 10 do:   hbase> incr 't1', 'r1', 'c1'   hbase> incr 't1', 'r1', 'c1', 1   hbase> incr 't1', 'r1', 'c1', 10 handle this more gracefully (printing 5 stacktraces is ugly), improve the help to specify what is needed more clearly. or fix the server side to support this, if this makes sense, and therefore never triggering this issue. adding a qualifier makes it work: hbase(main):015:0> incr 'testtable', 'cnt1', 'colfam1:test', 1 counter value = 1 hbase(main):016:0> incr 'testtable', 'cnt1', 'colfam1:test', 1 counter value = 2 ",
        "label": 312
    },
    {
        "text": "enhance test suite to be able to specify distributed scenarios  we keep finding good cases that are reasonably hard to test, yet the test suite does not encode these.   for example:   hbase-2413 master does not respect generation stamps, may result in meta getting permanently offlined  hbase-2312 possible data loss when rs goes into gc pause while rolling hlog i am sure there are many more such \"scenarios\" we should put into the unit tests. ",
        "label": 314
    },
    {
        "text": "make patch sh should add the branch name when  b is passed   when using something other than master as the base branch we should default to adding the branch name to the patch file name. ",
        "label": 154
    },
    {
        "text": "test zookeeper broken in trunk and branch  broken on hudson too  anyone want to take a look at this? ",
        "label": 314
    },
    {
        "text": "thrift's package descrpition needs to update for start stop procedure  currently, http://hadoop.apache.org/hbase/docs/current/api/org/apache/hadoop/hbase/thrift/package-summary.html says to run thrift, one can use  ./bin/hbase thrift h|-help | [--port=port] start however, we can use hbase-daemon.sh for better control, especially to stop it. ",
        "label": 385
    },
    {
        "text": "hdfsblocksdistribution shouldn't send npes when something goes wrong  i saw a pretty weird failure on a cluster with corrupted files and this particular exception really threw me off: 2013-01-07 09:58:59,054 error org.apache.hadoop.hbase.regionserver.handler.openregionhandler: failed open of region=redacted., starting to roll back the global memstore size. java.io.ioexception: java.io.ioexception: java.lang.nullpointerexception: empty hosts at org.apache.hadoop.hbase.regionserver.hregion.initializeregioninternals(hregion.java:548) at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:461) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:3814) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:3762) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.openregion(openregionhandler.java:332) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:108) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:169) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.io.ioexception: java.lang.nullpointerexception: empty hosts at org.apache.hadoop.hbase.regionserver.store.loadstorefiles(store.java:403) at org.apache.hadoop.hbase.regionserver.store.<init>(store.java:256) at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:2995) at org.apache.hadoop.hbase.regionserver.hregion$1.call(hregion.java:523) at org.apache.hadoop.hbase.regionserver.hregion$1.call(hregion.java:521) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) ... 3 more caused by: java.lang.nullpointerexception: empty hosts at org.apache.hadoop.hbase.hdfsblocksdistribution.addhostsandblockweight(hdfsblocksdistribution.java:123) at org.apache.hadoop.hbase.util.fsutils.computehdfsblocksdistribution(fsutils.java:597) at org.apache.hadoop.hbase.regionserver.storefile.computehdfsblockdistribution(storefile.java:492) at org.apache.hadoop.hbase.regionserver.storefile.open(storefile.java:521) at org.apache.hadoop.hbase.regionserver.storefile.createreader(storefile.java:602) at org.apache.hadoop.hbase.regionserver.store$1.call(store.java:380) at org.apache.hadoop.hbase.regionserver.store$1.call(store.java:375) ... 8 more 2013-01-07 09:58:59,059 info org.apache.hadoop.hbase.regionserver.handler.openregionhandler: opening of region \"redacted\" failed, marking as failed_open in zk this is what the code looks like: if (hosts == null || hosts.length == 0) {  throw new nullpointerexception(\"empty hosts\"); } so hosts can exist but we send an npe anyways? and then this is wrapped in store by: } catch (executionexception e) {   throw new ioexception(e.getcause()); fwiw there's another npe thrown in hdfsblocksdistribution.addhostandblockweight and it looks wrong. we should change the code to just skip computing the locality if it's missing and not throw big ugly exceptions. in this case the region would fail opening later anyways but at least the error message will be clearer. ",
        "label": 154
    },
    {
        "text": "add the option to bind to a specific ip address to the nonblocking thrift servers  this is not possible in thrift 0.2.0 so we'll have to wait until the next version is released (which includes thrift-684). after that is released this is an easy and quick fix. for a few more details see hbase-1373 and hbase-65. ",
        "label": 290
    },
    {
        "text": "testrsgroups is broke  i noticed this. tried bisect'ing but nought definitive at mo. i see testdefaultnamespacecreateandassign failing most of the time. the crazy gettable parse in regioninfo gets a negative offset because the passed in region name is nonsense. ",
        "label": 314
    },
    {
        "text": "hbase rest query params for maxversions and maxvalues are not parsed  i am trying to use maxvalues with a \"globbed\" row resource in stargate. from looking at the source code one has to do something like table/row/column(s)/timestamp(s)/?n=1 (except the ?n=1 piece must be urlencoded) however i cannot get the n=1 piece to work. i get this stacktrace: <p>problem accessing /some_table_name/93%2b002%2b*/cf:tx_cust_name/1,13862892906600/%3fn%3d1. reason:  <pre> string index out of range: 50</pre></p><h3>caused by:</h3><pre>java.lang.stringindexoutofboundsexception: string index out of range: 50  at java.lang.abstractstringbuilder.charat(abstractstringbuilder.java:174)  at java.lang.stringbuilder.charat(stringbuilder.java:55)  at org.apache.hadoop.hbase.rest.rowspec.parsequeryparams(rowspec.java:260)  at org.apache.hadoop.hbase.rest.rowspec.<init>(rowspec.java:59)  at org.apache.hadoop.hbase.rest.rowresource.<init>(rowresource.java:74)  at org.apache.hadoop.hbase.rest.tableresource.getrowresource(tableresource.java:90) the offending line is (260 in rowspec):  c = query.charat; i think this should be  c = query.charat(j); same for line 248 (which handles the maxversions) i have not been able to test this (never tried to build hbase myself). ",
        "label": 230
    },
    {
        "text": "testnamespacesinstancemodel fails on jdk8  noticed this in the build output of hbase-12911. seems this test has been failing for a long time, got all the way back to 6534583, master~44 and it's still failing there. i guess tests usually fail in hbase-server, so we don't often get to hbase-rest module. ",
        "label": 308
    },
    {
        "text": " hbase  provide a hbase checker and repair tool similar to fsck  we need a tool to verify (and repair) hbase much like fsck ",
        "label": 140
    },
    {
        "text": "ensure hbase is supported by thrift  hbase-4658 adds support for \"attributes\" for certain operations. make sure thrift 2 supports them where ever available in the native api. ",
        "label": 193
    },
    {
        "text": "convert test coprocessorprotocol implementations to protocol buffer services  with coprocessor endpoints now exposed as protobuf defined services, we should convert over all of our built-in endpoints to pb services. several coprocessorprotocol implementations are defined for tests: columnaggregationprotocol genericprotocol testservercustomprotocol.pingprotocol these should either be converted to pb services or removed if they duplicate other tests/are no longer necessary. ",
        "label": 314
    },
    {
        "text": "row with 55k deletes timesout scanner lease  made a blocker because it was found by jon gray (smile) so, jon gray has a row with 55k deletes all in the same row. when he tries to scan, his scanner timesout when it gets to this row. the root cause is the mechanism we use to make sure a delete in a new store file overshadows an entry at same address in an old file. we accumulate a list of all deletes encountered. before adding a delete to the list, we check if already a deleted. this check is whats killing us. one issue is that its doing super inefficient check of whether table is root but even fixing this inefficency \u2013 and then removing the check for root since its redundant we're still too slow. chatting with jim k, he suggested that arraylist check is linear. changing the aggregation of deletes to instead use hashset makes all run an order of magnitude faster. also part of this issue, need to figure why on compaction we are not letting go of these deletes. filing this issue against 0.18.1 so it gets into the rc2 (after chatting w/ j-d and jk \u2013 j-d is seeing the issue also). ",
        "label": 314
    },
    {
        "text": "failover handling for secondary region replicas  with the async wal approach (hbase-11568), the edits are not persisted (to wal) in the secondary region replicas. however this means that we have to deal with secondary region replica failures. we can seek to re-replicate the edits from primary to the secondary when the secondary region is opened in another server but this would mean to setup a replication queue again, and holding on to the wals for longer. instead, we can design it so that the edits form the secondaries are not persisted to wal, and if the secondary replica fails over, it will not start serving reads until it has guaranteed that it has all the past data. for guaranteeing that the secondary replica has all the edits before serving reads, we can use flush and region opening markers. whenever a region open event is seen, it writes all the files at the time of opening to wal (hbase-11512). in case of flush, the flushed file is written as well, and the secondary replica can do a ls for the store files and pick up all the files before the seqid of the flushed file. so, in this design, the secodary replica will wait until it sees and replays a flush or region open marker from wal from primary. and then start serving. for speeding up replica opening time, we can trigger a flush to the primary whenever the secondary replica opens as an optimization. ",
        "label": 155
    },
    {
        "text": "hbck should have the ability to repair basic problems  right now, the hbck utility can detect issues with region deployment but can't fix them. it should be able to handle basic things like closing one side of a double assignment, re-adding something to meta, etc. ",
        "label": 314
    },
    {
        "text": "flakey testmastermetrics testclusterrequests on branch  i found it during hbase-15169, let me see what's happening. https://builds.apache.org/job/precommit-hbase-build/586/testreport/org.apache.hadoop.hbase.master/testmastermetrics/testclusterrequests/ ",
        "label": 198
    },
    {
        "text": "names legal in are not in  breaks migration  from the list: 08/07/24 18:34:35 info v5.hlog: new log writer created at /user/sindice/log_1216920866949/hlog.dat.1216920874958 08/07/24 18:34:35 info v5.hlog: removing old log file /user/sindice/log_1216920866949/hlog.dat.0 whose highest sequence/edit id is 496415052 08/07/24 18:34:35 fatal util.migrate: upgrade failed java.lang.illegalargumentexception: illegal character <45>. user-space table names can only contain 'word characters':i.e. [a-za-z_0-9]: page-repository at org.apache.hadoop.hbase.htabledescriptor.islegaltablename(htabledescriptor.java:220) at org.apache.hadoop.hbase.htabledescriptor.<init>(htabledescriptor.java:130) at org.apache.hadoop.hbase.util.migrate.updatehregioninfo(migrate.java:266) at org.apache.hadoop.hbase.util.migrate$1$1.processrow(migrate.java:244) at org.apache.hadoop.hbase.util.migration.v5.metautils.scanmetaregion(metautils.java:264) at org.apache.hadoop.hbase.util.migration.v5.metautils.scanmetaregion(metautils.java:237) at org.apache.hadoop.hbase.util.migrate$1.processrow(migrate.java:241) at org.apache.hadoop.hbase.util.migration.v5.metautils.scanrootregion(metautils.java:211) at org.apache.hadoop.hbase.util.migrate.rewritemetahregioninfo(migrate.java:228) at org.apache.hadoop.hbase.util.migrate.migratetov5(migrate.java:209) at org.apache.hadoop.hbase.util.migrate.run(migrate.java:187) at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65) at org.apache.hadoop.util.toolrunner.run(toolrunner.java:79) at org.apache.hadoop.hbase.util.migrate.main(migrate.java:446) ",
        "label": 241
    },
    {
        "text": "testhbasefsck prevents testsuite from finishing  seen twice in a row in a local build: \"pool-1-thread-1\" prio=10 tid=0x00007fb4b8410000 nid=0x4481 waiting on condition [0x00007fb4b7d48000]    java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.util.testhbasefsck.deletetable(testhbasefsck.java:359)         at org.apache.hadoop.hbase.util.testhbasefsck.testnotinhdfs(testhbasefsck.java:1002)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45)         at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)         at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42)         at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)         at org.junit.rules.testwatcher$1.evaluate(testwatcher.java:47)         at org.junit.rules.testwatcher$1.evaluate(testwatcher.java:47)         at org.junit.rules.runrules.evaluate(runrules.java:18)         at org.junit.runners.parentrunner.runleaf(parentrunner.java:263)         at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:68)         at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:47)         at org.junit.runners.parentrunner$3.run(parentrunner.java:231)         at org.junit.runners.parentrunner$1.schedule(parentrunner.java:60)         at org.junit.runners.parentrunner.runchildren(parentrunner.java:229)         at org.junit.runners.parentrunner.access$000(parentrunner.java:50)         at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:222)         at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)         at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:30)         at org.junit.runners.parentrunner.run(parentrunner.java:300)         at org.junit.runners.suite.runchild(suite.java:128)         at org.junit.runners.suite.runchild(suite.java:24)         at org.junit.runners.parentrunner$3.run(parentrunner.java:231)         at java.util.concurrent.executors$runnableadapter.call(executors.java:441)         at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)         at java.util.concurrent.futuretask.run(futuretask.java:138)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:619) ",
        "label": 286
    },
    {
        "text": "add a getfamily method that returns all the cells in a column family  add a getfamily method that returns all the cells for a column. something like: public cell[] getfamily(byte[] row, byte[] family, long timestamp, int versions) ",
        "label": 144
    },
    {
        "text": "distributed log splitting deletenode races against splitlog retry  recently, during 0.92 rc testing, we found distributed log splitting hangs there forever. please see attached screen shot.  i looked into it and here is what happened i think: 1. one rs died, the servershutdownhandler found it out and started the distributed log splitting;  2. all three tasks failed, so the three tasks were deleted, asynchronously;  3. servershutdownhandler retried the log splitting;  4. during the retrial, it created these three tasks again, and put them in a hashmap (tasks);  5. the asynchronously deletion in step 2 finally happened for one task, in the callback, it removed one  task in the hashmap;  6. one of the newly submitted tasks' zookeeper watcher found out that task is unassigned, and it is not  in the hashmap, so it created a new orphan task.  7. all three tasks failed, but that task created in step 6 is an orphan so the batch.err counter was one short,  so the log splitting hangs there and keeps waiting for the last task to finish which is never going to happen. so i think the problem is step 2. the fix is to make deletion sync, instead of async, so that the retry will have  a clean start. async deletenode will mess up with split log retrial. in extreme situation, if async deletenode doesn't happen  soon enough, some node created during the retrial could be deleted. deletenode should be sync. ",
        "label": 356
    },
    {
        "text": "infoserver and hence hbase master doesn't fully start if you have hadoop patch  this is was due to a minor issue on the hbase side. the hadoop httpserver prior to hadoop-6151 was more tolerant than now. in /org/apache/hadoop/hbase/util/infoserver.java, adddefaultapps() adds a null key to the defaultcontexts map. after hadoop-6151, the httpserver code raises a npe. and hence hbase master doesn't fully start. will submit the patch shortly. ",
        "label": 263
    },
    {
        "text": "testrestartcluster testretainassignmentonrestart is flakey  java.lang.assertionerror: values should be different. actual: 1554802873115 at org.apache.hadoop.hbase.master.testrestartcluster.testretainassignmentonrestart(testrestartcluster.java:292) ",
        "label": 149
    },
    {
        "text": "the pool chunks from chunkcreator are deallocated while in pool because there is no reference to them  because mslab list of chunks was changed to list of chunk ids, the chunks returned back to pool can be deallocated by jvm because there is no reference to them. the solution is to protect pool chunks from gc by the strong map of chunkcreator introduced by hbase-18010. will prepare the patch today. ",
        "label": 35
    },
    {
        "text": "more doc around native lib setup and check and crc  banged my head a while this evening trying to get rid of purejavacrc32c from my perf top... took a while. add a bit of doc around native lib setup so hopefully save others getting same headache. while in here, point at the appy work to make native/hardware checksumming work in hbase. ",
        "label": 314
    },
    {
        "text": " shell  have list and list snapshot return jruby string arrays   it is really convenient to allow comamnds like list and list_snapshots return a jruby array of values in the hbase shell. it allows for nice things like this: # drop all tables starting with foo list(\"foo.*\").map { |t| disable t; drop t } or # clone all tables that start with bar list_snapshots(\"bar.*\").map { |s| clone_snapshot s, s + \"-table\"} ",
        "label": 248
    },
    {
        "text": "eofexception opening hstorefile info file  spin on hbase and   2008-05-28 05:46:06,271 error org.apache.hadoop.hbase.hregionserver: error opening region enwiki_080312_meta,uzbhxtgd-y4ziuvdjixxk-==,1207951941241 java.io.eofexception         at java.io.datainputstream.readbyte(unknown source)         at org.apache.hadoop.hbase.hstorefile.loadinfo(hstorefile.java:298)         at org.apache.hadoop.hbase.hstore.<init>(hstore.java:808)         at org.apache.hadoop.hbase.hregion.<init>(hregion.java:431)         at org.apache.hadoop.hbase.hregionserver.openregion(hregionserver.java:1256)         at org.apache.hadoop.hbase.hregionserver$worker.run(hregionserver.java:1202)         at java.lang.thread.run(unknown source) lets get rid of info files when we move to new mapfile format. ",
        "label": 314
    },
    {
        "text": "memory aware maps with lru eviction for cell cache  caching is key for 0.20. we need a set of memory-aware data structures to manage our caches. i propose two initial classes: lruhashmap and lrublockmap lruhashmap is currently being used over in hbase-80 for the cell cache. erik holstad has done extensive testing and benchmarking and will post results over in this issue. memory-aware fixed size lru eviction lrublockmap can be used for the block caching of the new file format in hbase-61. it should try to use all available memory, but must contend with memcaches so is resizable to deal with heap pressure. adding high priority blocks (evicted last) gives us in-memory functionality as described in bigtable paper. memory-aware fully resizable lru eviction (with some additions) high priority blocks optional: scan resistant algorithm part of this issue is also solving how we will determine the size of cached objects. ",
        "label": 247
    },
    {
        "text": "fix responsetooslow formatting  make sure the responsetooslow, etc., logs region at least and possibly row. as one well-known tall man once said: i really hate this: org.apache.hadoop.hbase.ipc.callerdisconnectedexception: aborting call next(-5472965299350279760, 101), rpc version=1, client version=29, methodsfingerprint=-56040613 from 10.4.21.36:59647 after 192373 ms, since caller disconnected which region was it??? .... 2013-06-09 01:11:03,574 warn org.apache.hadoop.ipc.hbaseserver: (responsetooslow): {\"processingtimems\":1141537,\"call\":\"next(6527877619467313103, 101), rpc version=1, client version=29, methodsfingerprint=-56040613\",\"client\":\"10.4.21.44:41453\",\"starttimems\":1370739122036,\"queuetimems\":0,\"class\":\"hregionserver\",\"responsesize\":293994,\"method\":\"next\"} which region is this? ",
        "label": 229
    },
    {
        "text": "backport hbase to   scanmetrics depends on number of rpc calls to the server   see hbase-8402 ",
        "label": 411
    },
    {
        "text": "add release managers to reference guide  reference guide lists release managers only up to version 1.3. we should have a complete list there.  http://hbase.apache.org/book.html#_release_managers ",
        "label": 352
    },
    {
        "text": "meta query statistics metrics source  implement a meta query statistics metrics source, created whenever a regionserver starts hosting meta, removed when meta hosting moves. provide views on top tables by request counts, top meta rowkeys by request count, top clients making requests by their hostname. can be implemented as a coprocessor.         =============== release note (wip) 1. usage: use this coprocessor by adding below section to hbase-site.xml <property>      <name>hbase.coprocessor.region.classes</name>      <value>org.apache.hadoop.hbase.coprocessor.metatablemetrics</value>  </property> ",
        "label": 494
    },
    {
        "text": "exportsnapshot should provide capability to limit bandwidth consumption  this capability was first brought up in this thread:  http://search-hadoop.com/m/dhed4td8xb1 the rewritten distcp already provides this capability.  see mapreduce-2765 distcp implementation utilizes throttledinputstream which provides bandwidth throttling on a specified inputstream. as a first step, we can add an option to exportsnapshot which expresses bandwidth per map in mb utilize throttledinputstream in exportsnapshot#exportmapper#copyfile(). ",
        "label": 441
    },
    {
        "text": "add peer lock test for shell command list locks  ",
        "label": 187
    },
    {
        "text": "during reads when passed the specified time range  seek to next column  when we are processing the stream of keyvalues in the scanquerymatcher, we will check the timestamp of the current kv against the specific timerange. currently we only check if it is in the range or not, returning skip if outside the range or continuing to other checks if within the range. the check should actually return skip if the stamp is greater than the timerange and next_col if the stamp is less than the timerange (we know we won't take anymore columns from the current column once we are below the timerange). ",
        "label": 357
    },
    {
        "text": "blocksize in testhfileblock is unintentionally small  looking at testhfileblock.writeblocks i see this:       for (int j = 0; j < rand.nextint(500); ++j) {         // this might compress well.         dos.writeshort(i + 1);         dos.writeint(j + 1);       } the result is probably not what the author intended. rand.nextint(500) is evaluated during each iterations and that leads to very small blocks size mostly between ~100 and 300 bytes or so. the author probably intended this:       int size = rand.nextint(500);       for (int j = 0; j < size; ++j) {         // this might compress well.         dos.writeshort(i + 1);         dos.writeint(j + 1);       } this leads to more reasonable block sizes between ~200 and 3000 bytes ",
        "label": 286
    },
    {
        "text": "clean up inconsistencies around deletes  compaction clears all cells behind any found 'delete' cell. this makes for an inconsistency in that before the compaction runs, you can do a get or scan on a timestamp older than the delete cell and get results. confusing. either preserve all cells for all time (or until > max_versions or ttl) or else make it so presence of a delete cell in the future stops client fetching anything older (expensive). ",
        "label": 218
    },
    {
        "text": "audit log messages should contain info about the higher level operation being executed  currently, audit log messages contains the \"action\" for which access was checked; this is one of read, write, create or admin. these give very little information to the person digging into the logs about what was done, though. you can't ask \"who deleted rows from table x?\", because \"delete\" is translated to a \"write\" action. it would be nice if the audit logs contained the higher-level operation, either replacing or in addition to the rwca information. ",
        "label": 309
    },
    {
        "text": "durability setting per table  hbase-7801 introduces the notion of per mutation fine grained durability settings.  this issue is to consider and the discuss the same for the per table settings (i.e. what would be used if the mutation indicates use_default). i propose the following setting per table: skip_wal (i.e. an unlogged table) async_wal (the current deferred log flush) sync_wal (the current default) fsync_wal (for future uses of hdfs' hsync()) ",
        "label": 155
    },
    {
        "text": " amv2  remove dispatchmergingregionsrequest   dispatchmergingregions  they don't align with how we have named the split equivalents; i.e. splitregion (so should be mergeregion...). they probably have these awkward names because the obvious slots are occupied... so this may not be fixable but filing issue anyways. ",
        "label": 499
    },
    {
        "text": "reimplement rsgroup feature and move it into core of hbase  the class rsgroupadminclient is not public  we need to use java api  rsgroupadminclient  to manager rsg  so  rsgroupadminclient should be public   ",
        "label": 149
    },
    {
        "text": "write attachment id of tested patch into jira comment  the optimization proposed in hbase-10044 wouldn't work if qa bot doesn't know the attachment id of the most recently tested patch. the first step is to write attachment id of tested patch into jira comment. for details, see hadoop-10163 ",
        "label": 441
    },
    {
        "text": "stack overflow when calling htable checkandput  when deleting a lot of values  we get a stackoverflow when calling htable.checkandput() from a map-reduce job though the client api after doing a large number of deletes. our mapred job is a periodic job (which extends tablemapper) that merges the versions for a value in a column into a new value/version and then deletes the older versions. this is because we use versions to store data so we can do append-only insertion. our rows can have large/huge (from 1 till > 1m) numbers of columns (aka key-values). the problem seems to be that the org.apache.hadoop.hbase.regionserver.getdeletetracker.isdeleted() method is implemented with recursion but since java has no tail recursion optimization, this fails for cases where the number of deletes that are being tracked is bigger than the stack size. i'm not sure why recursion is used here but it is not safe without tail-call optimization and it should be optimized into a simple loop. i'll attach the stacktrace. ",
        "label": 229
    },
    {
        "text": "our nightly jobs for master and branch are still using hadoop in integration test  we use ls to get the hadoop 2 jars, so maybe the problem is that the 2.7.1 jars are already there for a long time. we need to clean the workspace. ",
        "label": 149
    },
    {
        "text": "change format of enum messages in o a h h executor package  change rs2zk_region_closing to rs_zk_region_closing format. ryan and j-d prefer the latter (it was ryan's idea). ",
        "label": 314
    },
    {
        "text": "release hbase alpha  theme  coprocessor api cleanup   ",
        "label": 314
    },
    {
        "text": "abstract a replication storage interface to extract the zk specific code  for now, we will do sanity checks at the same time when updating replication peer. but this is not a safe way for procedure based replication peer modification. for the old zk watcher way, the only thing is updating the data on zk, so if the data is updated and then we crashes, there is no problem. for the new procedure way, we need to trigger refresh by ourselves after updating zk. if we crashes after the updating and before we record the state change of the procedure, we may fail with illegalargumentexception when we execute the procedure next time since the data on zk has already been updated. so the current way is to do sanity checks in pre_peer_modification state, and in update_storage state we will not do sanity checks any more, just update(overwrite) the peer storage. ",
        "label": 149
    },
    {
        "text": "support for fault tolerant  instant schema updates with out master's intervention  i e with out enable disable and bulk assign unassign  through zk   this jira is a slight variation in approach to what is being done as part of   https://issues.apache.org/jira/browse/hbase-1730 support instant schema updates such as modify table, add column, modify column operations:  1. with out enable/disabling the table.  2. with out bulk unassign/assign of regions. ",
        "label": 428
    },
    {
        "text": "hmsg carries safemode flag  remove  hmsg got a state flag on committ of hbase-1121. this kinda pollutes the simple hmsg type \u2013 a shell for carrying state as byte type \u2013 to instead being a vehicle that carries state in byte and in a boolean that is only used on cluster start then never read again but is passed in all hmsgs anyways. can we get rid of it when we integrate zk? go to zk to figure if master is in safe mode? ",
        "label": 342
    },
    {
        "text": "make corepool as a configurable parameter in htable  make the corepool a configurable parameter in htable. so we can tune this parameter in the config file. while at it, change the core pool name so we can distinguish it from other appserver pools. ",
        "label": 341
    },
    {
        "text": "do init sizing of the stringbuilder making a servername   simple patch from beno\u00eet. \u2014  .../java/org/apache/hadoop/hbase/servername.java | 3 ++-  1 files changed, 2 insertions, 1 deletions diff --git a/src/main/java/org/apache/hadoop/hbase/servername.java b/src/main/java/org/apache/hadoop/hbase/servername.java  index 6b03832..4ddb5b7 100644  \u2014 a/src/main/java/org/apache/hadoop/hbase/servername.java  +++ b/src/main/java/org/apache/hadoop/hbase/servername.java  @@ -128,7 +128,8 @@ public class servername implements comparable<servername> { startcode formatted as <code><hostname> ',' <port> ',' <startcode></code>  */  public static string getservername(string hostname, int port, long startcode) { stringbuilder name = new stringbuilder(hostname);  + final stringbuilder name = new stringbuilder(hostname.length() + 1 + 5 + 1 + 13);  + name.append(hostname);  name.append(servername_separator);  name.append(port);  name.append(servername_separator);  \u2013  1.7.6.434.g1d2b3 ",
        "label": 70
    },
    {
        "text": "better request latency and size histograms  i just discussed this with a colleague.  the get, put, etc, histograms that each region server keeps are somewhat useless (depending on what you want to achieve of course), as they are aggregated and calculated by each region server. it would be better to record the number of requests in certainly latency bands in addition to what we do now.  for example the number of gets that took 0-5ms, 6-10ms, 10-20ms, 20-50ms, 50-100ms, 100-1000ms, > 1000ms, etc. (just as an example, should be configurable). that way we can do further calculations after the fact, and answer questions like: how often did we miss our sla? percentage of requests that missed an sla, etc. comments? ",
        "label": 469
    },
    {
        "text": " amv2  unassignprocedure and crashed regionservers  this has been umesh agashe and my obsession over the last few days, what should an unassignprocedure do when it dispatches a close but the close fails because of connectexception or sockettimeout. + we used to let unassignprocedure continue presuming the region would be closed since the server is dead. but, if the unassign was part of a moveprocedure, the unassign would proceed and the move would then run without first splitting logs. bad.  + so, we made it so unassignprocedure failed; let the upper layers take care of the failure. see hbase-18491 that enabled this behavior. but, we are since figuring that even if the up completes as a failure, since it gives up the region lock on completion, another procedure \u2013 say an assignprocedure \u2013 could cut in before the servercrashprocedure had finished and again there could be dataloss.  + now we are thinking the up should hold on to the region lock until we are signalled by a servercrashprocedure; only then let go of the region. the up has context that is hard to pass another. waiting on a scp has the up living on for what could be a good amount of time. it might be ok if we can suspend the procedure. there is a good sample scenario that came up doing the no-regions-on-master issue, hbase-18511. when meta is not on master, testsplittransactiononcluster is failing. it fails because though the test completes, the tests commonly kill a regionserver. the teardown for the test runs before we've noticed the aborted rs. so, the disable of the table in the teardown prepartory to our deleting the test table as part of clean up, goes to unassign regions but the unassign fails against the aborted server. good stuff. ",
        "label": 314
    },
    {
        "text": "update completebulkload hadoop jar usage in related doc  update the usage of completebulkload from \"hadoop jar hbase-server-version.jar completebulkload\" to \"hadoop jar hbase-mapreduce-version.jar completebulkload\" according to hbase-18698 ",
        "label": 314
    },
    {
        "text": "move user triggered region admin methods  compact split flush  to go directly to rs  currently region admin commands like compact, split, and flush go via hbaseadmin from clients to the master. the master than piggybacks the requests onto the heartbeat responses. this issue is about moving these commands to go directly from client to rs. at the same time, we should be able to make both sync and async versions (currently this is only async and there is no way to make it sync). ",
        "label": 247
    },
    {
        "text": "remove optional parameters in asyncadmin interface  ",
        "label": 187
    },
    {
        "text": "fix flaky testrsgroupskillrs  should wait the scp to finish  in teardownaftermethod, it will movetables and delete rsgroups. it will fail if not wait scp to finish.     ",
        "label": 187
    },
    {
        "text": " visibilitycontroller  apply max versions from schema or request when scanning  if we update the row multiple times with different visibility labels  we are able to get the \"old version\" of the row until is flushed $ sudo -u hbase hbase shell hbase> add_labels 'a' hbase> add_labels 'b' hbase> create 'tb', 'f1' hbase> put 'tb', 'row', 'f1:q', 'v1', {visibility=>'a'} hbase> put 'tb', 'row', 'f1:q', 'v1all' hbase> put 'tb', 'row', 'f1:q', 'v1aorb', {visibility=>'a|b'} hbase> put 'tb', 'row', 'f1:q', 'v1aandb', {visibility=>'a&b'} hbase> scan 'tb' row column=f1:q, timestamp=1395948168154, value=v1aandb 1 row $ sudo -u testuser hbase shell hbase> scan 'tb' row column=f1:q, timestamp=1395948168102, value=v1all 1 row when we flush the memstore we get a single row (the last one inserted)  so the testuser get 0 rows now. $ sudo -u hbase hbase shell hbase> flush 'tb' hbase> scan 'tb' row column=f1:q, timestamp=1395948168154, value=v1aandb 1 row $ sudo -u testuser hbase shell hbase> scan 'tb' 0 row ",
        "label": 46
    },
    {
        "text": "apply acls to new bulk load hooks  hbase-6224 adds coprocessor hooks for bulk loading. this should require table write permission. ",
        "label": 174
    },
    {
        "text": "increment is non idempotent but client retries rpc  the htable.increment() operation is non-idempotent. the client retries the increment rpc a few times (as specified by configuration) before throwing an error to the application. this makes it possible that the same increment call be applied twice at the server. for increment operations, is it better to use hconnectionmanager.getregionserverwithoutretries()? another option would be to enhance the ipc module to make the rpc server correctly identify if the rpc is a retry attempt and handle accordingly. ",
        "label": 406
    },
    {
        "text": "move saslserver creation to hbasesaslrpcserver  ",
        "label": 149
    },
    {
        "text": "fix time b w recoverlease invocations from hbase  the time b/w recover lease attempts is conservative but is still not correct. it does not factor in datanode heartbeat time intervals. ",
        "label": 463
    },
    {
        "text": "periodic flusher only handles hbase meta  not other system tables  in hregion.shouldflush we have     long modifiedflushcheckinterval = flushcheckinterval;     if (getregioninfo().ismetaregion() &&         getregioninfo().getreplicaid() == hregioninfo.default_replica_id) {       modifiedflushcheckinterval = meta_cache_flush_interval;     } that method is called by the periodicmemstoreflusher thread, and prefers the hbase:meta only for faster flushing. it should be doing the same for other system tables. i suggest to use hri.issystemtable(). ",
        "label": 3
    },
    {
        "text": "testhcm failed with hadoop  running org.apache.hadoop.hbase.client.testhcm  tests run: 7, failures: 1, errors: 0, skipped: 0, time elapsed: 11.742 sec <<< failure! failed tests: testregioncaching(org.apache.hadoop.hbase.client.testhcm) ",
        "label": 242
    },
    {
        "text": "rename the keeperexception to replicationexception in replicationqueuesclient for abstracting  there're still some interfaces in replicationqueuesclient which throws a keeperexception. it make nonsense for replicationqueuesclient implemented by hbase table.   list<string> getlistofreplicators() throws keeperexception; file an issue to address this. ",
        "label": 514
    },
    {
        "text": "ant tar build broken since switch to ivy  running ant tar produces a very small tar file because all .jar dependencies are missing. this happens since the switch to ivy. adding common.ivy.lib.dir to the build.xml fixes some of it but some things still don't work:     <mkdir dir=\"${dist.dir}/lib\"/>     <copy todir=\"${dist.dir}/lib\">       <fileset dir=\"${build.lib}\" />       <fileset dir=\"${common.ivy.lib.dir}\"/>     </copy> the jars for the contrib apps still seem to be missing. at the moment this is only stargate but the i've got the same problem for the new thrift contrib. i am afraid i don't know enough about ant or ivy to be of any further assistance. ",
        "label": 266
    },
    {
        "text": "improve unit test coverage of package org apache hadoop hbase master metrics    the patch suggests new test that elevates unit-test coverage of package org.apache.hadoop.hbase.master.metrics .  the patch is applicable to branch 0.94 only. ",
        "label": 217
    },
    {
        "text": "handle the incompatible change about the replication tablecfs' config  about compatibility, there is one incompatible change about the replication tablecfs' config. the old config is a string and it concatenate the list of tables and column families in format \"table1:cf1,cf2;table2:cfa,cfb\" in zookeeper for table-cf to replication peer mapping. when parse the config, it use \":\" to split the string. if table name includes namespace, it will be wrong (see hbase-11386). it is a problem since we support namespace (0.98). so hbase-11393 (and hbase-16653) changed it to a pb object. when rolling update cluster, you need rolling master first. and the master will try to translate the string config to a pb object. but there are two problems.  1. permission problem. the replication client can write the zookeeper directly. so the znode may have different owner. and master may don't have the write permission for the znode. it maybe failed to translate old table-cfs string to new pb object. see hbase-16938  2. we usually keep compatibility between old client and new server. but the old replication client may write a string config to znode directly. then the new server can't parse them. ",
        "label": 187
    },
    {
        "text": "fix new findbugs issues after we upgrade hbase thirdparty dependencies  in hbase-server module, most warnings are pass null for non null field, it should be related to guava. let's fix them. ",
        "label": 149
    },
    {
        "text": "consistent log severity level guards and statements  a log statement should be guarded by its matching severity level. a log statement like  if (log.istraceenabled()) {  log.debug(identifier + \" opening connection to zookeeper ensemble=\" + ensemble); doesn't make much sense because the log message is only printed out when trace-level is enabled. this inconsistency was possibly introduced when developers demoted the original log statement from debug but forgot to change its corresponding log severity level. ",
        "label": 220
    },
    {
        "text": "master coprocessor notification for assignmentmanager balance  is inconsistent  i found this issue when reading user discussion which is quoted below. in hmaster.moveregion(), we have:       this.assignmentmanager.balance(rp);       if (this.cphost != null) {         this.cphost.postmove(hri, rp.getsource(), rp.getdestination());       } meaning, user can register master coprocessor which would receive region movement notification. the assignmentmanager.balance(plan) call in hmaster.balance() doesn't send out such notification.  i think we should enhance the following hook (at line 1335) with list of regions moved so that notification from master is consistent:         this.cphost.postbalance(); here is excerpt for user discussion: sometimes user performs compaction after a region is moved (by balancer). we should provide 'hook' which lets user specify what follow-on actions to take after region movement. see discussion on user mailing list under the thread 'how to know it's time for a major compaction?' for background information: http://search-hadoop.com/m/bdx4s1jmjf92&subj=how+to+know+it+s+time+for+a+major+compaction+ ",
        "label": 441
    },
    {
        "text": "improve the stability of splitting log when do fail over  the way we splitting log now is like the following figure:    the problem is the outputsink will write the recovered edits during splitting log, which means it will create one writerandpath for each region and retain it until the end. if the cluster is small and the number of regions per rs is large, it will create too many hdfs streams at the same time. then it is prone to failure since each datanode need to handle too many streams. thus i come up with a new way to split log.     we try to cache all the recovered edits, but if it exceeds the maxheapusage, we will pick the largest entrybuffer and write it to a file (close the writer after finish). then after we read all entries into memory, we will start a writeandclosethreadpool, it starts a certain number of threads to write all buffers to files. thus it will not create hdfs streams more than hbase.regionserver.hlog.splitlog.writer.threads we set.  the biggest benefit is we can control the number of streams we create during splitting log,   it will not exceeds hbase.regionserver.wal.max.splitters * hbase.regionserver.hlog.splitlog.writer.threads, but before it is hbase.regionserver.wal.max.splitters * the number of region the hlog contains. ",
        "label": 244
    },
    {
        "text": "nightly client integration test fails with hadoop  2019-05-28 15:27:10,107 warn common.storage: storage directory /home/jenkins/jenkins-slave/workspace/hbase_nightly_master/output-integration/hadoop-3/target/test/data/dfs/name-0-1 does not exist 2019-05-28 15:27:10,109 warn namenode.fsnamesystem: encountered exception loading fsimage org.apache.hadoop.hdfs.server.common.inconsistentfsstateexception: directory /home/jenkins/jenkins-slave/workspace/hbase_nightly_master/output-integration/hadoop-3/target/test/data/dfs/name-0-1 is in an inconsistent state: storage directory does not exist or is not accessible. at org.apache.hadoop.hdfs.server.namenode.fsimage.recoverstoragedirs(fsimage.java:376) at org.apache.hadoop.hdfs.server.namenode.fsimage.recovertransitionread(fsimage.java:227) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.loadfsimage(fsnamesystem.java:1074) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.loadfromdisk(fsnamesystem.java:706) at org.apache.hadoop.hdfs.server.namenode.namenode.loadnamesystem(namenode.java:628) at org.apache.hadoop.hdfs.server.namenode.namenode.initialize(namenode.java:690) at org.apache.hadoop.hdfs.server.namenode.namenode.<init>(namenode.java:919) at org.apache.hadoop.hdfs.server.namenode.namenode.<init>(namenode.java:892) at org.apache.hadoop.hdfs.server.namenode.namenode.createnamenode(namenode.java:1622) at org.apache.hadoop.hdfs.minidfscluster.createnamenode(minidfscluster.java:1305) at org.apache.hadoop.hdfs.minidfscluster.configurenameservice(minidfscluster.java:1074) at org.apache.hadoop.hdfs.minidfscluster.createnamenodesandsetconf(minidfscluster.java:949) at org.apache.hadoop.hdfs.minidfscluster.initminidfscluster(minidfscluster.java:881) at org.apache.hadoop.hdfs.minidfscluster.<init>(minidfscluster.java:514) at org.apache.hadoop.hdfs.minidfscluster$builder.build(minidfscluster.java:473) at org.apache.hadoop.mapreduce.minihadoopclustermanager.start(minihadoopclustermanager.java:160) at org.apache.hadoop.mapreduce.minihadoopclustermanager.run(minihadoopclustermanager.java:132) at org.apache.hadoop.mapreduce.minihadoopclustermanager.main(minihadoopclustermanager.java:320) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at org.apache.hadoop.util.programdriver$programdescription.invoke(programdriver.java:71) at org.apache.hadoop.util.programdriver.run(programdriver.java:144) at org.apache.hadoop.test.mapredtestdriver.run(mapredtestdriver.java:139) at org.apache.hadoop.test.mapredtestdriver.main(mapredtestdriver.java:147) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at org.apache.hadoop.util.runjar.run(runjar.java:244) at org.apache.hadoop.util.runjar.main(runjar.java:158) ",
        "label": 187
    },
    {
        "text": "table became unusable when master balanced its region after table was dropped  0.98 was used.  this was sequence of events: create 'tablethree_mod'  snapshot 'tablethree_mod', 'snapshot_tablethree_mod'  disable 'tablethree_mod'  2014-01-15 09:34:51,749 restore_snapshot 'snapshot_tablethree_mod'  2014-01-15 09:35:07,210 enable 'tablethree_mod'  2014-01-15 09:35:46,134 delete_snapshot 'snapshot_tablethree_mod'  2014-01-15 09:41:42,210 disable 'tablethree_mod'  2014-01-15 09:41:43,610 drop 'tablethree_mod'  create 'tablethree_mod' for the last table creation request:   2014-01-15 10:03:52,999|beaver.component.hbase|info| 'create 'tablethree_mod', {name => 'f1', versions => 3} , {name => 'f2', versions => 3} , {name => 'f3', versions => 3} ' 2014-01-15 10:03:52,999|beaver.component.hbase|info| 'exists 'tablethree_mod'' 2014-01-15 10:03:52,999|beaver.component.hbase|info| 'put 'tablethree_mod', '0', 'f1:q1', 'value-0', 10' 2014-01-15 10:03:52,999|beaver.component.hbase|info| 'put 'tablethree_mod', '1', 'f1:q1', 'value-1', 20' 2014-01-15 10:03:53,000|beaver.component.hbase|info| 'put 'tablethree_mod', '2', 'f2:q2', 'value-2', 30' 2014-01-15 10:03:53,000|beaver.component.hbase|info| 'put 'tablethree_mod', '3', 'f3:q3', 'value-3', 40' 2014-01-15 10:03:53,000|beaver.component.hbase|info| 'put 'tablethree_mod', '4', 'f3:q3', 'value-4', 50' 2014-01-15 10:03:53,000|beaver.component.hbase|info|done writing commands to file. will execute them now. 2014-01-15 10:03:53,000|beaver.machine|info|running: /usr/lib/hbase/bin/hbase shell /grid/0/tmp/hwqe/artifacts/tmp-471142 2014-01-15 10:03:55,878|beaver.machine|info|2014-01-15 10:03:55,878 info [main] configuration.deprecation: hadoop.native.lib is deprecated. instead, use io.native.lib.available 2014-01-15 10:03:57,283|beaver.machine|info|2014-01-15 10:03:57,283 warn [main] conf.configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; ignoring. 2014-01-15 10:03:57,669|beaver.machine|info|2014-01-15 10:03:57,669 warn [main] conf.configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; ignoring. 2014-01-15 10:03:57,720|beaver.machine|info|2014-01-15 10:03:57,720 warn [main] conf.configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; ignoring. 2014-01-15 10:03:57,997|beaver.machine|info| 2014-01-15 10:03:57,997|beaver.machine|info|error: table already exists: tablethree_mod! 2014-01-15 10:03:57,997|beaver.machine|info| this was an intermittent issue after using snapshots, a table is not properly dropped / and not able to properly re-create with the same name. and a hregion is empty or null error occurs. (when you try to drop the table it says it does not exist, and when you try to create the table it says that it does already exist). 2014-01-15 10:04:02,462|beaver.machine|info|error: hregioninfo was null or empty in hbase:meta, row=keyvalues= {tablethree_mod,,1389778226606.afc82d1ceabbaca36a504b83b65fc0c9./info:seqnumduringopen/1389778905355/put/vlen=8/mvcc=0, tablethree_mod,,1389778226606.afc82d1ceabbaca36a504b83b65fc0c9./info:server/1389778905355/put/vlen=32/mvcc=0, tablethree_mod,,1389778226606.afc82d1ceabbaca36a504b83b65fc0c9./info:serverstartcode/1389778905355/put/vlen=8/mvcc=0}  thanks to huned who discovered this issue. ",
        "label": 242
    },
    {
        "text": "purge deprecated hbaseclustertestcase  it could gain us a few minutes on overall test run in the cases where we don't spin up a cluster for each test. ",
        "label": 314
    },
    {
        "text": "restrict the universe of labels and authorizations  currently we allow any string as visibility label or request authorization. however as seen on hbase-10878, we accept for authorizations strings that would not work if provided as labels in visibility expressions. we should throw an exception at least in cases where someone tries to define or use a label or authorization including visibility expression operators '&', '|', '!', '(', ')'. ",
        "label": 544
    },
    {
        "text": "classnotfoundexception on trunk for rest  reported by zheng shao on list: java.lang.classnotfoundexception: org.apache.hadoop.hbase.rest.dispatcher         at java.net.urlclassloader$1.run(urlclassloader.java:200)         at java.security.accesscontroller.doprivileged(native method)         at java.net.urlclassloader.findclass(urlclassloader.java:188)         at java.lang.classloader.loadclass(classloader.java:306)         at sun.misc.launcher$appclassloader.loadclass(launcher.java:276)         at java.lang.classloader.loadclass(classloader.java:251) related to         at org.apache.hadoop.http.httpserver.start(httpserver.java:460)         at org.apache.hadoop.hbase.master.hmaster.startservicethreads(hmaster.java:641)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:410) rest was removed completely in 0.21. investigate. ",
        "label": 247
    },
    {
        "text": "fix top level notices txt file  its stale   ",
        "label": 314
    },
    {
        "text": "change client table  client admin  region  store  and hbasetestingutility to not use htabledescriptor or hcolumndescriptor  htabledescriptor is deprecated and scheduled to be removed in 3.0. but client.table and client.admin method gettabledescriptor returns htabledescriptor. ",
        "label": 98
    },
    {
        "text": "regionserver blocked on waiting dfsclient dfsoutputstream waitforackedseqno running   running 0.20.205.1 (i was not at tip of the branch) i ran into the following hung regionserver: \"regionserver7003.logroller\" daemon prio=10 tid=0x00007fd98028f800 nid=0x61af in object.wait() [0x00007fd987bfa000]    java.lang.thread.state: waiting (on object monitor)         at java.lang.object.wait(native method)         at java.lang.object.wait(object.java:485)         at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.waitforackedseqno(dfsclient.java:3606)         - locked <0x00000000f8656788> (a java.util.linkedlist)         at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.flushinternal(dfsclient.java:3595)         at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.closeinternal(dfsclient.java:3687)         - locked <0x00000000f8656458> (a org.apache.hadoop.hdfs.dfsclient$dfsoutputstream)         at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.close(dfsclient.java:3626)         at org.apache.hadoop.fs.fsdataoutputstream$positioncache.close(fsdataoutputstream.java:61)         at org.apache.hadoop.fs.fsdataoutputstream.close(fsdataoutputstream.java:86)         at org.apache.hadoop.io.sequencefile$writer.close(sequencefile.java:966)         - locked <0x00000000f8655998> (a org.apache.hadoop.io.sequencefile$writer)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.close(sequencefilelogwriter.java:214)         at org.apache.hadoop.hbase.regionserver.wal.hlog.cleanupcurrentwriter(hlog.java:791)         at org.apache.hadoop.hbase.regionserver.wal.hlog.rollwriter(hlog.java:578)         - locked <0x00000000c443deb0> (a java.lang.object)         at org.apache.hadoop.hbase.regionserver.logroller.run(logroller.java:94)         at java.lang.thread.run(thread.java:662) other threads are like this (here's a sample): \"regionserver7003.logsyncer\" daemon prio=10 tid=0x00007fd98025e000 nid=0x61ae waiting for monitor entry [0x00007fd987cfb000]    java.lang.thread.state: blocked (on object monitor)         at org.apache.hadoop.hbase.regionserver.wal.hlog.syncer(hlog.java:1074)         - waiting to lock <0x00000000c443deb0> (a java.lang.object)         at org.apache.hadoop.hbase.regionserver.wal.hlog.sync(hlog.java:1195)         at org.apache.hadoop.hbase.regionserver.wal.hlog$logsyncer.run(hlog.java:1057)         at java.lang.thread.run(thread.java:662) .... \"ipc server handler 0 on 7003\" daemon prio=10 tid=0x00007fd98049b800 nid=0x61b8 waiting for monitor entry [0x00007fd9872f1000]    java.lang.thread.state: blocked (on object monitor)         at org.apache.hadoop.hbase.regionserver.wal.hlog.append(hlog.java:1007)         - waiting to lock <0x00000000c443deb0> (a java.lang.object)         at org.apache.hadoop.hbase.regionserver.hregion.dominibatchput(hregion.java:1798)         at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1668)         at org.apache.hadoop.hbase.regionserver.hregionserver.multi(hregionserver.java:2980)         at sun.reflect.generatedmethodaccessor636.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1325) looks like hdfs-1529? (todd?) ",
        "label": 314
    },
    {
        "text": "testbytebufferioengine testbytebufferioengine occasionally fails  from https://builds.apache.org/job/precommit-hbase-build/8189/testreport/junit/org.apache.hadoop.hbase.io.hfile.bucket/testbytebufferioengine/testbytebufferioengine/ : java.lang.assertionerror at org.apache.hadoop.hbase.util.bytebufferarray.multiple(bytebufferarray.java:160) at org.apache.hadoop.hbase.util.bytebufferarray.putmultiple(bytebufferarray.java:123) at org.apache.hadoop.hbase.io.hfile.bucket.bytebufferioengine.write(bytebufferioengine.java:81) at org.apache.hadoop.hbase.io.hfile.bucket.testbytebufferioengine.testbytebufferioengine(testbytebufferioengine.java:61) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.runners.parentrunner.runleaf(parentrunner.java:271) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63) at org.junit.runners.parentrunner.runchildren(parentrunner.java:236) at org.junit.runners.parentrunner.access$000(parentrunner.java:53) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229) at org.junit.runners.parentrunner.run(parentrunner.java:309) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) standard output 2013-12-16 21:12:00,417 info  [pool-1-thread-1] hbase.resourcechecker(147): before: io.hfile.bucket.testbytebufferioengine#testbytebufferioengine thread=36, openfiledescriptor=146, maxfiledescriptor=60000, systemloadaverage=142, processcount=167, availablememorymb=7901, connectioncount=1 2013-12-16 21:12:00,417 info  [pool-1-thread-1] util.bytebufferarray(57): allocating buffers total=32 mb , sizeperbuffer=2 mb, count=16 2013-12-16 21:12:00,460 info  [pool-1-thread-1] hbase.resourcechecker(171): after: io.hfile.bucket.testbytebufferioengine#testbytebufferioengine thread=36 (was 36), openfiledescriptor=148 (was 146) - openfiledescriptor leak? -, maxfiledescriptor=60000 (was 60000), systemloadaverage=142 (was 142), processcount=167 (was 167), availablememorymb=7901 (was 7901), connectioncount=1 (was 1) standard error 2013-12-16 21:12:00,417 info  [pool-1-thread-1] hbase.resourcechecker(147): before: io.hfile.bucket.testbytebufferioengine#testbytebufferioengine thread=36, openfiledescriptor=146, maxfiledescriptor=60000, systemloadaverage=142, processcount=167, availablememorymb=7901, connectioncount=1 2013-12-16 21:12:00,417 info  [pool-1-thread-1] util.bytebufferarray(57): allocating buffers total=32 mb , sizeperbuffer=2 mb, count=16 2013-12-16 21:12:00,460 info  [pool-1-thread-1] hbase.resourcechecker(171): after: io.hfile.bucket.testbytebufferioengine#testbytebufferioengine thread=36 (was 36), openfiledescriptor=148 (was 146) - openfiledescriptor leak? -, maxfiledescriptor=60000 (was 60000), systemloadaverage=142 (was 142), processcount=167 (was 167), availablememorymb=7901 (was 7901), connectioncount=1 (was 1) ",
        "label": 107
    },
    {
        "text": "hbase rest tests fail in hbase alpha2  pointed out by andrew on vote mail for hbase-2.0.0-alpha2 ",
        "label": 314
    },
    {
        "text": "rewrite deadlock prevention for concurrent connection close  the deadlock prevention approach used in hbase-14241 introduces unnecessary logic which is not intuitive. depending on the value for config hbase.ipc.client.specificthreadforwriting , there may or may not be callsender threads running. the attached patch simplifies deadlock prevention by using a set which represents the connections to be closed. outside the synchronized (connections) block, this set is iterated where the connections are closed. ",
        "label": 441
    },
    {
        "text": "remove asyncadmin istableavailable tablename  byte   it has been deprecated so remove it in 3.0.0, to align with the admin interface. ",
        "label": 277
    },
    {
        "text": "enable testmasteroperationsforregionreplicas testincompletemetatablereplicainformation in branch and branch  todo: enable when we have support for alter_table- hbase-10361 the hbase-10361 was resolved 2 years ago, and the test had been enabled in branch-1 and branch-1.4. hence we should also enable it for all active branches. ",
        "label": 509
    },
    {
        "text": "master should wait for dfs to come up when creating hbase version  the master does not wait for dfs to come up in the circumstance where the dfs master is started for the first time after format and no datanodes have been started yet. 2009-11-07 11:47:28,115 info org.apache.hadoop.hbase.master.hmaster: vmname=java hotspot(tm) 64-bit server vm, vmvendor=sun microsystems inc., vmversion=14.2-b01 2009-11-07 11:47:28,116 info org.apache.hadoop.hbase.master.hmaster: vminputarguments=[-xmx1000m, -xx:+heapdumponoutofmemoryerror, -xx:+useconcmarksweepgc, -xx:+cmsincrementalmode, -dhbase.log.dir=/mnt/hbase/logs, -dhbase.log.file=hbase-root-master-ip-10-242-15-159.log, -dhbase.home.dir=/usr/local/hbase-0.20.1/bin/.., -dhbase.id.str=root, -dhbase.root.logger=info,drfa, -djava.library.path=/usr/local/hbase-0.20.1/bin/../lib/native/linux-amd64-64] 2009-11-07 11:47:28,247 info org.apache.hadoop.hbase.master.hmaster: my address is ip-10-242-15-159.ec2.internal:60000 2009-11-07 11:47:28,728 warn org.apache.hadoop.hdfs.dfsclient: datastreamer exception: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: file /hbase/hbase.version could only be replicated to 0 nodes, instead of 1 at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getadditionalblock(fsnamesystem.java:1267) at org.apache.hadoop.hdfs.server.namenode.namenode.addblock(namenode.java:422) [...] 2009-11-07 11:47:28,728 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block null bad datanode[0] nodes == null 2009-11-07 11:47:28,728 warn org.apache.hadoop.hdfs.dfsclient: could not get block locations. source file \"/hbase/hbase.version\" - aborting... 2009-11-07 11:47:28,729 fatal org.apache.hadoop.hbase.master.hmaster: not starting hmaster because: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: file /hbase/hbase.version could only be replicated to 0 nodes, instead of 1 at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getadditionalblock(fsnamesystem.java:1267) at org.apache.hadoop.hdfs.server.namenode.namenode.addblock(namenode.java:422) should probably sleep and retry the write a few times. ",
        "label": 38
    },
    {
        "text": "test up on hudson are leaking zookeeper ensembles  here is from a recent run up on hudson: 2010-10-14 23:31:56,482 info  [main] zookeeper.minizookeepercluster(111): failed binding zk server to client port: 21818 2010-10-14 23:31:56,483 info  [main] zookeeper.minizookeepercluster(111): failed binding zk server to client port: 21819 2010-10-14 23:31:56,483 info  [main] zookeeper.minizookeepercluster(111): failed binding zk server to client port: 21820 2010-10-14 23:31:56,522 info  [main] zookeeper.minizookeepercluster(125): started minizk server on client port: 21821 see how we start trying to bind to 21818 but we don't get a free port till we get to 21821? some test or tests is not cleaning up after itself leaving a running zk cluster or two about. testreplication looks to be suspect. here is its @afterclass method:   /**    * @throws java.lang.exception    */   @afterclass   public static void teardownafterclass() throws exception {     /* reenable     utility2.shutdownminicluster();     utility1.shutdownminicluster();     */   } ",
        "label": 314
    },
    {
        "text": "deletes and puts with the same ts should be resolved according to mvcc seqnum  this came up during hbase-8721. puts with the same ts are resolved by seqnum. it's not clear why deletes with the same ts as a put should always mask the put, rather than also being resolve by seqnum. what do you think? ",
        "label": 314
    },
    {
        "text": "add end to end test of sync flush  add a test to do the following: + start a hbase/hdfs cluster (local node is fine).  + use top-level (htable) level apis to put items.  + try about single column puts, as well as puts which span multiple columns/multiple column families, etc. + then kill one region server. + wait for recovery to happen. + and then check the rows exist. assigning myself. ",
        "label": 314
    },
    {
        "text": "define public api for spark integration module  before we can put the spark integration module into a release, we need to annotate its public api surface. ",
        "label": 234
    },
    {
        "text": "testhtablepool hangs when run as part of runmediumtests profile  in the medium set, we found a test -'testhtablepool.java' hanging and the build fails reporting a timeout. though the test passes when run separately, while running the whole suite using the command \" mvn -u clean package -dhadoop.profile=2.0 -psecurity -dsnappy -p runmediumtests\", this test is observed in a waiting state, tracking for the root region server. it turns out be a problem with the test initialization. the class testhtable\u200bpool contains a class testhtable\u200bpooltype which uses nested static test classes. but the initializati\u200bon is being done for the outer class - testhtable\u200bpool using @beforeclass annotation. surefire reruns the tests written in nested manner (for some reasons), and when this test is instantiated the second time, the method setupbefore\u200bclass() is not being called and hence the initializati\u200bon is not done. guess that surefire tries to run testhtable\u200bpool.\u200btesthtable\u200breusablepool.\u200bclass directly. (tried this by writing a separate program using junitcore.\u200brunclasses(\u200bttesthtable\u200bpool.\u200btesthtable\u200breusablepool.\u200bclass) and observed that the outer class's @beforeclass method is not called.) ",
        "label": 365
    },
    {
        "text": "implement atomic update operations  checkandput  checkanddelete  for rest client server  i have several large application/hbase clusters where an application node will occasionally need to talk to hbase from a different cluster. in order to help ensure some of my consistency guarantees i have a sentinel table that is updated atomically as users interact with the system. this works quite well for the \"regular\" hbase client but the rest client does not implement the checkandput and checkanddelete operations. this exposes the application to some race conditions that have to be worked around. it would be ideal if the same checkandput/checkanddelete operations could be supported by the rest client. ",
        "label": 333
    },
    {
        "text": "htable needs a non cached version of getregionlocation  there is a need for a non caching version of getregionlocation  on the client side. this api is needed to quickly lookup the regionserver  that hosts a particular region without using the heavy weight  getregionsinfo() method. ",
        "label": 359
    },
    {
        "text": "analyse and fix the findbugs reporting by qa and add invalid bugs into findbugs excludefilter file  there are many findbugs errors reporting by hbaseqa. hbase-5597 is going to up the ok count.  this may lead to other issues when we re-factor the code, if we induce new valid ones and remove invalid bugs also can not be reported by qa. so, i would propose to add the exclude filter file for findbugs(for the invalid bugs). if we find any valid ones, we can fix under this jira. ",
        "label": 458
    },
    {
        "text": "the thrift scanneropen functions should support row caching  after noticing very poor scanner performance using the thrift api, i realized that there was no way to set caching on the scanner. this should probably be supported. ",
        "label": 5
    },
    {
        "text": "add a configuration point for maxversion of column family  starting on 0.96.0. hcolumndescriptor.default_versions change to 1 from 3. so a columnfamily will be default have 1 version of data. currently a user can specifiy the maxversion during create table time or alter the columnfam later. this feature will add a configuration point in hbase-sit.xml so that an admin can set the default globally. a small discussion in hbase-10941 lead to this jira ",
        "label": 156
    },
    {
        "text": "add skip support to importtsv  it'd be nice to have support for \"skip\" mappings so that you can omit columns from the tsv during the import. for example -dimporttsv.columns=skip,hbase_row_key,cf1:col1,cf1:col2,skip,skip,cf2:col1... or maybe hbase_skip_column to be less ambiguous. ",
        "label": 194
    },
    {
        "text": "decide between  inmb  and  mb  as suffix for field names in clusterstatus objects  we vacillate between \"inmb\" (e.g. hserverload.getmemstoresizeinmb(), hserverload.getstorefileindexsizeinmb()) and \"mb\" (e.g. hserverload.getusedheapmb()) in the various clusterstatus objects (hserverinfo, hserverload, hserverload.regionload). we should probably pick one and stick to it. ",
        "label": 136
    },
    {
        "text": "hbck cannot detect any ioexception while  tableinfo  file is missing  hbasefsck checks those missing .tableinfo files in loadhdfsregioninfos() function. however, no ioexception will be catched while .tableinfo is missing, since \"fstabledescriptors.gettabledescriptor\" doesn't throw any ioexception. ",
        "label": 239
    },
    {
        "text": "combine mvcc and seqid  hbase-8701 and a lot of recent issues include good discussions about mvcc + seqid semantics. it seems that having mvcc and the seqid complicates the comparator semantics a lot in regards to flush + wal replay + compactions + delete markers and out of order puts. thinking more about it i don't think we need a mvcc write number which is different than the seqid. we can keep the mvcc semantics, read point and smallest read points intact, but combine mvcc write number and seqid. this will allow cleaner semantics + implementation + smaller data files. we can do some brainstorming for 0.98. we still have to verify that this would be semantically correct, it should be so by my current understanding. ",
        "label": 233
    },
    {
        "text": "a delete version could mask other values  a delete version operation mask values that have version = 0. the problem happens at scandeletetracker. ",
        "label": 119
    },
    {
        "text": "split unit tests from testtablename into a separate test only class  the class testtablename in hbase-common is both a unit test for the tablename class, and a junit testrule. this double-duty is strange; the rule functionality is not discoverable at all. split the rule out into a separate class, maybe tablenamerule or some such. ",
        "label": 71
    },
    {
        "text": "  upgrade to jetty latest and jackson latest  worked through some static analysis scans and these two popped up as a result. upgrades are: jetty 9.3.25->9.3.27 jackson 2.9.2->2.9.9 not expecting any pain with these, but we'll find out what qa thinks! ",
        "label": 252
    },
    {
        "text": "region merge request should be audited with request user through proper scope of doas  calls to region observer notifications  hbase-14475 and hbase-14605 narrowed the scope of doas() calls to region observer notifications for region splitting. during review of hbase-14605, andrew brought up the case for region merge. this jira is to implement similar scope narrowing technique for region merging. the majority of the change would be in regionmergetransactionimpl class. ",
        "label": 441
    },
    {
        "text": "implement executor tostring for master handlers at least  on shutdown, if still outstanding executors queued then when executorservice lists what is outstanding, the list will be other than a list of default tostring implementations of servershutdownhandler objects. ",
        "label": 82
    },
    {
        "text": "weird npes compacting  over on apurtell cluster, hbase trunk on hadoop 0.18.0 branch, see below. they are weird because first would seem to be an empty fileystem data member and the second's line number is start of the method and even then, we check for null everywhere first before doing stuff in here (because of erik holstad experience in past). 2009-01-09 16:28:21,262 info org.apache.hadoop.hbase.regionserver.hregion: starting  compaction on region content,c84bbfc94b2143e41ba119d159be2958,1231518442461 2009-01-09 16:28:21,265 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region content,c84bbfc94b2143e41ba119d159be2958,1231518442461 java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:869)         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:709)         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:666)         at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:105) 2009-01-09 16:28:45,896 error org.apache.hadoop.hbase.regionserver.hregionserver: java.lang.nullpointerexception 2009-01-09 16:28:45,896 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 2 on 60020, call next(3999959915225939603, 30) from xx.xx.xx.33:36665: error: java.io.ioexception: java.lang.nullpointerexception java.io.ioexception: java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.hregionserver.checkoome(hregionserver.java:696)         at org.apache.hadoop.hbase.regionserver.hregionserver.cleanup(hregionserver.java:664)         at org.apache.hadoop.hbase.regionserver.hregionserver.cleanup(hregionserver.java:648)         at org.apache.hadoop.hbase.regionserver.hregionserver.next(hregionserver.java:1579)         at sun.reflect.generatedmethodaccessor13.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:894) 2009-01-09 16:28:47,947 error org.apache.hadoop.hbase.regionserver.hregionserver: java.lang.nullpointerexception 2009-01-09 16:28:47,948 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 9 on 60020, call next(3999959915225939603, 30) from xx.xx.xx.33:36665: error: java.io.ioexception: java.lang.nullpointerexception java.io.ioexception: java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.hregionserver.checkoome(hregionserver.java:696)         at org.apache.hadoop.hbase.regionserver.hregionserver.cleanup(hregionserver.java:664)         at org.apache.hadoop.hbase.regionserver.hregionserver.cleanup(hregionserver.java:648)         at org.apache.hadoop.hbase.regionserver.hregionserver.next(hregionserver.java:1579)         at sun.reflect.generatedmethodaccessor13.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:894) ",
        "label": 314
    },
    {
        "text": "jenkins builds timing out  undo setting hbase client retries number to  ",
        "label": 314
    },
    {
        "text": "modify old filter tests to use junit4 no longer use hbasetestcase  ",
        "label": 253
    },
    {
        "text": "regionservers waiting for root while master waiting for regionservers  after a cluster disastrophe due to a disconnected switch, i ended up in a state where the master was up with no region servers (see hbase-3263). when i brought the rs back up, because of the aforementioned bug, the master didn't get itself into a happy state (internal datastructure had some null in it). so i killed the master and started it again. now, the master is in \"waiting for region servers to check in\" mode, and the region servers are in the following stack: locked <0x00002aaab1bda5d0> (a org.apache.hadoop.hbase.zookeeper.rootregiontracker)  at org.apache.hadoop.hbase.catalog.catalogtracker.waitforroot(catalogtracker.java:177)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:537)  at java.lang.thread.run(thread.java:619) i imagine what happened is that the rs got through \"tryreportforduty\" with the old master, but the old master was unable to assign anything due to bad state. so, when it crashed, all the rs were stuck in waitforroot(), and when i brought the new one up, no one was reporting for duty. ",
        "label": 314
    },
    {
        "text": "delete table followed by recreation results in honked table  daniel leffel suspected that delete and then recreate causes issues. i tried it on our little cluster. i'm doing a mr load up into the newly created table and after a few million rows, the mr job just hangs. its looking for a region that doesn't exist: 2008-08-13 03:32:36,840 info org.apache.hadoop.metrics.jvm.jvmmetrics: initializing jvm metrics with processname=map, sessionid= 2008-08-13 03:32:36,940 info org.apache.hadoop.mapred.maptask: numreducetasks: 1 2008-08-13 03:32:37,420 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: found root region => {name => '-root-,,0', startkey => '', endkey => '', encoded => 70236052, table => {{name => '-root-', is_root => 'true', is_meta => 'true', families => [{name => 'info', bloomfilter => 'false', compression => 'none', versions => '1', length => '2147483647', ttl => '-1', in_memory => 'false', blockcache => 'false'}]}} 2008-08-13 03:32:37,541 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: reloading table servers because: hregioninfo was null or empty in .meta. 2008-08-13 03:32:37,541 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: removed .meta.,,1 from cache because of testtable,0008388608,99999999999999 2008-08-13 03:32:37,544 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: found root region => {name => '-root-,,0', startkey => '', endkey => '', encoded => 70236052, table => {{name => '-root-', is_root => 'true', is_meta => 'true', families => [{name => 'info', bloomfilter => 'false', compression => 'none', versions => '1', length => '2147483647', ttl => '-1', in_memory => 'false', blockcache => 'false'}]}} 2008-08-13 03:32:47,605 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: reloading table servers because: hregioninfo was null or empty in .meta. 2008-08-13 03:32:47,606 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: removed .meta.,,1 from cache because of testtable,0008388608,99999999999999 .... my guess is that its a region that was in the tables' previous incarnation with ghosts left over down inside .meta. ",
        "label": 314
    },
    {
        "text": "web ui is extremely slow   freezes up if you have many tables  i have 40 tables and every time the web ui loads (or i refresh it) the browser locks up and it takes 5-10 seconds to render the page. in a dev cluster i had 3 tables and it was fast. i thought there was a jira for this already but i couldn't find it... please correct me if there is. ",
        "label": 247
    },
    {
        "text": "master and regionserver web uis show nonexistent regions after a drop table  i dropped a table with 121 regions, ans read from the web ui. afterwards, most of the regions went away, but 6 remained counted and listed in the web ui. select * from .meta. shows no rows, so in reality the regions are gone. this inconsistency gives a poor sense of the consistency of hbase. ",
        "label": 218
    },
    {
        "text": "rs crash due to dbe reference to an reused bytebuff  after introduce hbase-21879 into our own branch, when enable data block encoding with row_index_v1, regionserver crashed (the crash log has been uploaded).  after reading rowindexencoderv1, find lastcell may refer to an reused bytebuff, because dbe is not a listener of shipper\u3002 ",
        "label": 521
    },
    {
        "text": "hbase removed bulk sync optimization for multi row puts  previously to hbase-2283 we used to call flush/sync once per put(put[]) call (ie: batch of commits). now we do for every row. this makes bulk uploads slower if you are using wal. is there an acceptable solution to achieve both safety and performance by bulk-sync'ing puts? or would this not work in face of atomic guarantees? discuss! ",
        "label": 453
    },
    {
        "text": "work on repository order in hbase pom  we have ibiblio first. we probably should go via some caching servers first. ",
        "label": 284
    },
    {
        "text": "fix lastseqid logic in regionserver  ",
        "label": 34
    },
    {
        "text": "add a new function to thrift to open scanner  get results and close scanner  we found, very often we open a scanner , get x number of rows , then close the scanner immediately. the attached patch , add a new function getscannerresults that does just that. using getscannerresults function reduce number of calls to thrift server. also since the scanner gets closed immediately, it is less prone to cause memory leak. ",
        "label": 193
    },
    {
        "text": "htableinterface should extend java io closeable  ioan eugen stan found this issue. ",
        "label": 314
    },
    {
        "text": "cellscanner advance may overflow stack  on user@hbase, johannes.schaback@visual-meta.com reported: we face a serious issue with our hbase production cluster for two days now. every couple minutes, a random regionserver gets stuck and does not process any requests. in addition this causes the other regionservers to freeze within a minute which brings down the entire cluster. stopping the affected regionserver unblocks the cluster and everything comes back to normal. subsequent troubleshooting reveals that rpc is getting stuck because we are losing rpc handlers. in the .out files we have this: exception in thread \"defaultrpcserver.handler=5,queue=2,port=60020\" java.lang.stackoverflowerror         at org.apache.hadoop.hbase.cellutil$1.advance(cellutil.java:210)         at org.apache.hadoop.hbase.cellutil$1.advance(cellutil.java:210)         at org.apache.hadoop.hbase.cellutil$1.advance(cellutil.java:210)         at org.apache.hadoop.hbase.cellutil$1.advance(cellutil.java:210) [...] exception in thread \"defaultrpcserver.handler=5,queue=2,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=18,queue=0,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=23,queue=2,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=24,queue=0,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=2,queue=2,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=11,queue=2,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=25,queue=1,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=20,queue=2,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=19,queue=1,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=15,queue=0,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=1,queue=1,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=7,queue=1,port=60020\" java.lang.stackoverflowerror exception in thread \"defaultrpcserver.handler=4,queue=1,port=60020\" java.lang.stackoverflowerror\u200b that is the anonymous cellscanner instance we create from cellutil#createcellscanner: \u200b    return new cellscanner() {       private final iterator<? extends cellscannable> iterator = cellscannerables.iterator();       private cellscanner cellscanner = null;       @override       public cell current() {         return this.cellscanner != null? this.cellscanner.current(): null;       }       @override       public boolean advance() throws ioexception {         if (this.cellscanner == null) {           if (!this.iterator.hasnext()) return false;           this.cellscanner = this.iterator.next().cellscanner();         }         if (this.cellscanner.advance()) return true;         this.cellscanner = null; --->        return advance();       }     }; that final return statement is the immediate problem. we should also fix this so the regionserver aborts if it loses a handler to an error. ",
        "label": 314
    },
    {
        "text": "wiki thrift documentation for scan methods  the documentation on the wiki for the scan methods in thrift are wrong. the wiki page is http://wiki.apache.org/hadoop/hbase/thriftapi. the openscanner, getscannerresult and closescanner methods aren't methods in the thrift interface. they should be scanneropen, scannerget, and scannerclose. some of the method paramaters might need to be changed too. ",
        "label": 330
    },
    {
        "text": "deprecated istableavailable with splitkeys  it is deprecated in admin interface and plan to be removed in 3.0.0 release, we should do this for asyncadmin interface. ",
        "label": 257
    },
    {
        "text": "testthriftserver failing in trunk  delete of a table is removing regions from the filesystem while they are trying to close. not sure yet how to fix. ",
        "label": 314
    },
    {
        "text": "close  split  open of regions in regionserver are run by a single thread only   jgray and karthik observed yesterday that a regoin open message arrived at the regionserver but that the regionserver worker thread did not get around to the actually opening until 45 seconds later (region offline for 45 seconds). we only run a single worker thread in a regoinserver processing open, close, and splits. in this case, a long running close (or two) held up the worker thread. we need to run more than a single worker. a pool of workers? should opens be prioritized? ",
        "label": 314
    },
    {
        "text": "testsplitlogmanager testdeadworker may fail because of hard limit on the timeoutmonitor's timeout period  the timeout in testdeadworker is set to 1 sec, it is the same as the timeoutmonitor thread timeout. in some case, this may fail: java.lang.assertionerror at org.junit.assert.fail(assert.java:92) at org.junit.assert.asserttrue(assert.java:43) at org.junit.assert.asserttrue(assert.java:54) at org.apache.hadoop.hbase.master.testsplitlogmanager.waitforcounter(testsplitlogmanager.java:147) at org.apache.hadoop.hbase.master.testsplitlogmanager.waitforcounter(testsplitlogmanager.java:127) at org.apache.hadoop.hbase.master.testsplitlogmanager.testdeadworker(testsplitlogmanager.java:433) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) fix is to increase the timeout for this test. its not needed in trunk as the timeout is 3 seconds. ",
        "label": 199
    },
    {
        "text": "hbaseadmin never recovers from restarted cluster  while testing common scenarios that we might encounter i found that hbaseadmin does not recover from a restarted cluster. it turns out hbaseclient.connection.stop() is send into an endless loop here:     // wait until all connections are closed     while (!connections.isempty()) {       try {         thread.sleep(100);       } catch (interruptedexception ignored) {       }     } the reason is that poolmap.remove(k,v) does not remove empty pools, and hence connections.isempty() is never true if there ever was any connection in there.  my fix is to remove the pool from the poolmap when it is empty. (alternatively one could change poolmap.isempty() to also look inside of all pools and see if their size is 0). when i fixed that i noticed that if the master wasn't running when hbaseadmin is created it also will not recover from that.  even creating a new hbaseadmin from the same configuration will still use the old stale hconnection. in that case a masternotrunningexception is thrown, which is not handled in hbaseadmin's constructor. the hconnection handling in hconnectionmanager is funky. there should never be a closed connection in the hbase_instances.  i might look at that as well but in a separate issue. ",
        "label": 286
    },
    {
        "text": "double play of openedregionhandler for a single region  fails second time through and aborts master  here is master log with annotations: http://people.apache.org/~stack/master.txt region in question is: b8827a67a9d446f345095d25e1f375f7 the running code is doctored in that i've added in a bit of logging \u2013 zk in particular \u2013 and i've also removed what i thought was a provocation of this condition, reassign inside in an assign if server has gone away when we try the open rpc (turns out we have the condition even w/o this code in place). the log starts where the region in question timesout in rit. we assign it to 186. notice how we see 'handling transition' for this region twice. this means two openedregionhandlers will be scheduled \u2013 and so the failure to delete a znode already gone. as best i can tell, the watcher for this region is triggered once only \u2013 which is odd because how then the double scheduling of openedregionhandler but also, why am i not seeing opening, opening, opened and only what i presume is an opened? ",
        "label": 247
    },
    {
        "text": "online schema change doesn't return errors  still after the fun i had over in hbase-4729, i tried to finish altering my table (remove a family) since only half of it was changed so i did this: hbase(main):002:0> alter 'testtable', name => 'allo', method => 'delete'   updating all regions with the new schema...  244/244 regions updated.  done.  0 row(s) in 1.2480 seconds nice it all looks good, but over in the master log: org.apache.hadoop.hbase.invalidfamilyoperationexception: family 'allo' does not exist so cannot be deleted  at org.apache.hadoop.hbase.master.handler.tabledeletefamilyhandler.handletableoperation(tabledeletefamilyhandler.java:56)  at org.apache.hadoop.hbase.master.handler.tableeventhandler.process(tableeventhandler.java:86)  at org.apache.hadoop.hbase.master.hmaster.deletecolumn(hmaster.java:1011)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:348)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1242) maybe we should do checks before launching the async task. marking critical as this is a regression. ",
        "label": 314
    },
    {
        "text": "remove infoserver  use hadoop statushttpserver instead  requires hadoop   replace our infoserver now hadoop http status server has been made subclassable (see hadoop-3824). ",
        "label": 229
    },
    {
        "text": "make hbasefsck less verbose  adding an option to output summaries related to each table into a file in json format. the table information would contain the following details total/online/offline (not deployed)/missing/skipped regions. also each table would contain a list of errors for the regions for that table. ",
        "label": 154
    },
    {
        "text": "rare race condition can take down a regionserver   this happened after > 24 hours of heavy import load on my cluster. luckily the shutdown seemed to be clean: java.lang.illegalaccesserror: call open first  at org.apache.hadoop.hbase.regionserver.storefile.getreader(storefile.java:356)  at org.apache.hadoop.hbase.regionserver.store.getstorefilesindexsize(store.java:1378)  at org.apache.hadoop.hbase.regionserver.hregionserver.dometrics(hregionserver.java:1075)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:454)  at java.lang.thread.run(thread.java:619) ",
        "label": 314
    },
    {
        "text": "log recovery  splitlog deletes old logs prematurely  splitlog()'s purpose is to take a bunch of commit logs of a crashed rs and create per-region logs. splitlog() runs in the master. there are two cases where splitlog() might end up deleting an old log before actually creating (sync/closing) the newly created logs. if the master crashes in between deletion of the old log and creation of the new log, then edits could be lost irrecoverably. more specifically here are the two issues we (nicolas, aravind and i) noticed: issue #1: the old logs are read one at a time. an in memory structure, logentries (a map from region name to edits for the region), is populated. and the old logs are closed. then the in-memory map is written out to per region files. fix: we should move the file deletion to later. issue #2: there is another little case. the per-region log file is written under the region directory (named oldlogfile.log or the constant hregion_oldlogfile_name). before the master creates the file, it checks to see if there is already a file with that name, and if so, it renames it to oldlogfile.log.old, and then creates file oldlogfile.log again, and copies over the contents of oldlogfile.log.old to oldlogfile.log. it then proceeds to delete \"oldlogfile.log.old\", even though it hasn't closed/sync'ed \"oldlogfile.log\" yet. \u2013 i think we should be able to restructure the code such that all deletion of old logs happens after the new logs have been created (i.e. written to & closed). ",
        "label": 341
    },
    {
        "text": "testthriftserver testall failing  http://jenkins-public.iridiant.net/job/hbase-0.96-hadoop2/140/org.apache.hbase$hbase-thrift/testreport/junit/org.apache.hadoop.hbase.thrift/testthriftserver/testall/ java.lang.assertionerror: metrics counters should be equal expected:<2> but was:<4> at org.junit.assert.fail(assert.java:88) at org.junit.assert.failnotequals(assert.java:743) at org.junit.assert.assertequals(assert.java:118) at org.junit.assert.assertequals(assert.java:555) here too: http://jenkins-public.iridiant.net/job/hbase-0.96/134/ http://jenkins-public.iridiant.net/job/hbase-0.96-hadoop2/140/ mind taking a looksee elliott neil clark ",
        "label": 314
    },
    {
        "text": "fix the flaky testrestartcluster  i guess it's related to hbase-21565 or hbase-21588 log can be see here: https://builds.apache.org/job/hbase-flaky-tests/job/master/2902/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.testrestartcluster-output.txt ",
        "label": 149
    },
    {
        "text": "update new shell docs and commands on help menu  from the help screen on the new shell [root@s2 hbase]# hbase shell hbase shell; enter 'help<return>' for list of supported commands. version: 0.2.0-dev, r670701, wed jun 25 02:27:19 cdt 2008 hbase(main):001:0> create 't1' {name => 'f1', versions => 5} syntaxerror: (hbase):2: , unexpected tlcurly the help menu gives the above example on creating table in hbase but it does not work! if we release this for the new shell examples need to be more clear. i have not been able to create a table using the new shell yet, also might be worth adding the old shell back in and remove it after we release 2.0 or at lease until we work out the bugs in new client. if it would not require much updating to keep it current with the new api. ",
        "label": 314
    },
    {
        "text": "testmultiparallel fails intermittently in trunk builds  from trunk build #3598:  testflushcommitsnoabort(org.apache.hadoop.hbase.client.testmultiparallel): count of regions=8 it failed in 3595 as well: java.lang.assertionerror: server count=2, abort=true expected:<1> but was:<2> at org.junit.assert.fail(assert.java:93) at org.junit.assert.failnotequals(assert.java:647) at org.junit.assert.assertequals(assert.java:128) at org.junit.assert.assertequals(assert.java:472) at org.apache.hadoop.hbase.client.testmultiparallel.dotestflushcommits(testmultiparallel.java:267) at org.apache.hadoop.hbase.client.testmultiparallel.testflushcommitswithabort(testmultiparallel.java:226) ",
        "label": 107
    },
    {
        "text": "hbase book  fix definition of max min size to compact  i think we need to change wording/definition of these config parameters in hbase book, they are misleading: hbase.hstore.compaction.min.size  description  a storefile smaller than this size will always be eligible for minor compaction. hfiles this size or larger are evaluated by hbase.hstore.compaction.ratio to determine if they are eligible. because this limit represents the \"automatic include\"limit for all storefiles smaller than this value, this value may need to be reduced in write-heavy environments where many storefiles in the 1-2 mb range are being flushed, because every storefile will be targeted for compaction and the resulting storefiles may still be under the minimum size and require further compaction. if this parameter is lowered, the ratio check is triggered more quickly. this addressed some issues seen in earlier versions of hbase but changing this parameter is no longer necessary in most situations. default: 128 mb expressed in bytes.  default  134217728  hbase.hstore.compaction.max.size  description  a storefile larger than this size will be excluded from compaction. the effect of raising hbase.hstore.compaction.max.size is fewer, larger storefiles that do not get compacted often. if you feel that compaction is happening too often without much benefit, you can try raising this value. default: the value of long.max_value, expressed in bytes.  hbase.hstore.compaction.ratio  description  for minor compaction, this ratio is used to determine whether a given storefile which is larger than hbase.hstore.compaction.min.size is eligible for compaction. its effect is to limit compaction of large storefiles. the value of hbase.hstore.compaction.ratio is expressed as a floating-point decimal. a large ratio, such as 10, will produce a single giant storefile. conversely, a low value, such as .25, will produce behavior similar to the bigtable compaction algorithm, producing four storefiles. a moderate value of between 1.0 and 1.4 is recommended. when tuning this value, you are balancing write costs with read costs. raising the value (to something like 1.4) will have more write costs, because you will compact larger storefiles. however, during reads, hbase will need to seek through fewer storefiles to accomplish the read. consider this approach if you cannot take advantage of bloom filters. otherwise, you can lower this value to something like 1.0 to reduce the background cost of writes, and use bloom filters to control the number of storefiles touched during reads. for most cases, the default value is appropriate. default  1.2f for details, see hbase-14263. ",
        "label": 330
    },
    {
        "text": "sanity check visiblity and audience for hbase client and hbase common apis   this is a task to audit and enumerate places where hbase-common and hbase-client should narrow or widen the exposed user program supported api. ",
        "label": 248
    },
    {
        "text": "coprocessors  lifecycle management  considering extending cps to the master, we have no equivalent to pre/postopen and pre/postclose as on the regionserver. we also should consider how to resolve dependencies and initialization ordering if loading coprocessors that depend on others. osgi (http://en.wikipedia.org/wiki/osgi) has a lifecycle api and is familiar to many java programmers, so we propose to borrow its terminology and state machine. a lifecycle layer manages coprocessors as they are dynamically installed, started, stopped, updated and uninstalled. coprocessors rely on the framework for dependency resolution and class loading. in turn, the framework calls up to lifecycle management methods in the coprocessor as needed. a coprocessor transitions between the below states over its lifetime: state description uninstalled the coprocessor implementation is not installed. this is the default implicit state. installed the coprocessor implementation has been successfully installed starting a coprocessor instance is being started. active the coprocessor instance has been successfully activated and is running. stopping a coprocessor instance is being stopped. see attached state diagram. transitions to stopping will only happen as the region is being closed. if a coprocessor throws an unhandled exception, this will cause the regionserver to close the region, stopping all coprocessor instances on it. transitions from installed->starting and active->stopping would go through upcall methods into the coprocessor via the coprocessorlifecycle interface: public interface coprocessorlifecycle {   void start(coprocessorenvironment env) throws ioexception;    void stop(coprocessorenvironment env) throws ioexception; } ",
        "label": 180
    },
    {
        "text": "hregionlocator getallregionlocations should put the results in cache  getallregionlocations walks meta. well after doing the work, we should put the results into cache. ",
        "label": 198
    },
    {
        "text": "creating an existing table from shell does not throw tableexistsexception  when i try to create a same table from shell i don't get tableexistsexception instead i get error: cannot load java class org.apache.hadoop.hbase.tablenotfoundexception here is some help for this command: creates a table. pass a table name, and a set of column family specifications (at least one), and, optionally, table configuration. column specification can be a simple string (name), or a dictionary (dictionaries are described below in main help output), necessarily including name attribute. examples:   hbase> create 't1', {name => 'f1', versions => 5}   hbase> create 't1', {name => 'f1'}, {name => 'f2'}, {name => 'f3'}   hbase> # the above in shorthand would be the following: ",
        "label": 233
    },
    {
        "text": "testhregion testdelete mixed  failing on hudson  ",
        "label": 247
    },
    {
        "text": "hbaseobjectwritable methods should allow null hbaseconfigurations for when writable is not configurable  hbase-1828 dealt with a broken scanner because we were passing null for hbaseconfiguration. previous patches attempted to make it so hbaseobjectwritable would work with null hbc but it appears there is still a problem here. fix it so we allow null hbc. ",
        "label": 314
    },
    {
        "text": "fix table jsp in regards to splitting a region table with an optional splitkey  after hbase-3328 and hbase-3437 went in there is also the table.jsp that needs updating to support the same features. also, at the same time update the wording, for example this action will force a split of all eligible regions of the table, or, if a key is supplied, only the region containing the given key. an eligible region is one that does not contain any references to other regions. split requests for noneligible regions will be ignored. i think it means it splits either all regions (that are splittable) or a specific one. it says though \"the region containing the given key\", that seems wrong in any event. currently we do a split on the tablename when nothing was specified or else do an internal get(region), which is an exact match on the rows in .meta.. in other words you need to match the region name exactly or else it fails. it reports it has accepted the request but logs internally 2011-01-21 15:37:24,340 info org.apache.hadoop.hbase.client.hbaseadmin: no server in .meta. for csfsef; pair=null error reporting could be better but because of the async nature this is more difficult, yet it would be nice there is some concept of a future to be able to poll the result if needed. finally, when you go back to the previous page after submitting the split the entered values show up in the \"compact\" input fields, at least on my chrome. the inputs in both forms are named the same so it seems to confuse it. this could be improved a lot by making the landing page reload the main one automatically or refresh on reload instead of submitting the request again. ",
        "label": 60
    },
    {
        "text": "ensure hbase is covered by thrift  hbase-7826 is about sorted results, we need to check if thrift 2 handles this as well. ",
        "label": 285
    },
    {
        "text": "implement retry logic around starting exclusive backup operation  specifically, the client does a checkandput to specifics coordinates in the backup table and throws an exception when that fails. remember that backups are client driven (per some design review from a long time ago), so queuing is tough to reason about (we have no \"centralized\" execution system to use). at a glance, it seems pretty straightforward to add some retry/backoff semantics to backupsystemtable#startbackupexclusiveoperation(). while we are in a state in which backup operations cannot be executed in parallel, it would be nice to provide some retry logic + configuration. this would alleviate users from having to build this themselves. ",
        "label": 478
    },
    {
        "text": "integration test mr jobs attempt to load htrace jars from the wrong location  the mapreduce jobs submitted by integrationtestimporttsv want to load the htrace jar from the local maven cache but get confused and use a hdfs uri. tests run: 2, failures: 0, errors: 2, skipped: 0, time elapsed: 8.489 sec <<< failure! testgenerateandload(org.apache.hadoop.hbase.mapreduce.integrationtestimporttsv)  time elapsed: 0.488 sec  <<< error! java.io.filenotfoundexception: file does not exist: hdfs://localhost:37548/home/apurtell/.m2/repository/org/cloudera/htrace/htrace-core/2.04/htrace-core-2.04.jar         at org.apache.hadoop.hdfs.distributedfilesystem$17.docall(distributedfilesystem.java:1110)         at org.apache.hadoop.hdfs.distributedfilesystem$17.docall(distributedfilesystem.java:1102)         at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81)         at org.apache.hadoop.hdfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:1102)         at org.apache.hadoop.mapreduce.filecache.clientdistributedcachemanager.getfilestatus(clientdistributedcachemanager.java:288)         at org.apache.hadoop.mapreduce.filecache.clientdistributedcachemanager.getfilestatus(clientdistributedcachemanager.java:224)         at org.apache.hadoop.mapreduce.filecache.clientdistributedcachemanager.determinetimestamps(clientdistributedcachemanager.java:93)         at org.apache.hadoop.mapreduce.filecache.clientdistributedcachemanager.determinetimestampsandcachevisibilities(clientdistributedcachemanager.java:57)         at org.apache.hadoop.mapreduce.jobsubmitter.copyandconfigurefiles(jobsubmitter.java:264)         at org.apache.hadoop.mapreduce.jobsubmitter.copyandconfigurefiles(jobsubmitter.java:300)         at org.apache.hadoop.mapreduce.jobsubmitter.submitjobinternal(jobsubmitter.java:387)         at org.apache.hadoop.mapreduce.job$10.run(job.java:1268)         at org.apache.hadoop.mapreduce.job$10.run(job.java:1265)         at java.security.accesscontroller.doprivileged(native method)         at javax.security.auth.subject.doas(subject.java:415)         at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1491)         at org.apache.hadoop.mapreduce.job.submit(job.java:1265)         at org.apache.hadoop.mapreduce.job.waitforcompletion(job.java:1286)         at org.apache.hadoop.hbase.mapreduce.importtsv.run(importtsv.java:603)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)         at org.apache.hadoop.hbase.mapreduce.testimporttsv.domrontabletest(testimporttsv.java:270)         at org.apache.hadoop.hbase.mapreduce.testimporttsv.domrontabletest(testimporttsv.java:232)         at org.apache.hadoop.hbase.mapreduce.integrationtestimporttsv.testgenerateandload(integrationtestimporttsv.java:206) ",
        "label": 339
    },
    {
        "text": "add to package documentation the need of zk to be in classpath  add to package documentation the need of zk to be in classpath as of 0.20.0 ",
        "label": 314
    },
    {
        "text": "add count of regions on filesystem to master ui  add percentage online as difference between whats open and whats on filesystem  jim firby idea. shouldn't be hard to add. ",
        "label": 419
    },
    {
        "text": "add a max number of regions per regionserver limit  in a testing environment, a cluster got to a state with more than 1500 regions per region server, and essentially became stuck and unavailable. we could add a limit to the number of regions that a region server can serve to prevent this from happening. this looks like it could be implemented in the core or as a coprocessor. ",
        "label": 248
    },
    {
        "text": "investigate why bucket cache filling up in file mode in an exisiting file is slower  this issue was observed when we recently did some tests with ssd based bucket cache. similar thing was also reported by @stack and daniel pol while doing some of these bucket cache related testing.  when we try to preload a bucket cache (in file mode) with a new file the bucket cache fills up quite faster and there not much 'failedblockadditions'. but when the same bucket cache is filled up with a preexisitng file ( that had already some entries filled up) this time it has more 'failedblockadditions' and the cache does not fill up faster. investigate why this happens. ",
        "label": 544
    },
    {
        "text": "port 'make flush decisions per column family' to trunk  currently the flush decision is made using the aggregate size of all column families. when large and small column families co-exist, this causes many small flushes of the smaller cf. we need to make per-cf flush decisions. ",
        "label": 149
    },
    {
        "text": "hbase importtsv fails when the line contains no data   i've tried to import tsv data by using importtsv tools. but the task failed with the following errors. 10/10/23 02:56:52 info mapred.jobclient: task id : attempt_201010222300_0036_m_000016_2, status : failed  java.lang.illegalargumentexception: no columns to insert  at org.apache.hadoop.hbase.client.htable.validateput(htable.java:682)  at org.apache.hadoop.hbase.client.htable.doput(htable.java:544)  at org.apache.hadoop.hbase.client.htable.put(htable.java:535)  at org.apache.hadoop.hbase.mapreduce.tableoutputformat$tablerecordwriter.write(tableoutputformat.java:104)  at org.apache.hadoop.hbase.mapreduce.tableoutputformat$tablerecordwriter.write(tableoutputformat.java:65)  at org.apache.hadoop.mapred.maptask$newdirectoutputcollector.write(maptask.java:523)  at org.apache.hadoop.mapreduce.taskinputoutputcontext.write(taskinputoutputcontext.java:80)  at org.apache.hadoop.hbase.mapreduce.importtsv$tsvimporter.map(importtsv.java:241)  at org.apache.hadoop.hbase.mapreduce.importtsv$tsvimporter.map(importtsv.java:184)  at org.apache.hadoop.mapreduce.mapper.run(mapper.java:144)  at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:639)  at org.apache.hadoop.mapred.maptask.run(maptask.java:315)  at org.apache.hadoop.mapred.child$4.run(child.java:217)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1063)  at org.apache.hadoop.mapred.child.main(child.java:211) if the line contains invalid data, the parser should throw badtsvlineexception. but unfortunately, the codepath throws illegalargumentexception for the empty line, and that wasn't caught in the map() function. ",
        "label": 270
    },
    {
        "text": "fix notice and license  license.txt contains only apache license v2 but the hbase-operator-tools project uses dependencies with different licenses.  ",
        "label": 352
    },
    {
        "text": "create an package integration project  for integrating hadoop ecosystem more tightly and reduce the cost of hadoop stack development, it would be nice to have a set of installable packages which can setup hadoop stack with least amount effort. the goal of this jira is to create installable rpm and debian packages for hbase from the build system. ",
        "label": 160
    },
    {
        "text": "unable to start hbase 0rc0 out of the box because of zk trying to access  var folders   hbase 0.95.0rc0 is failing out of the box because of some zookeeper exceptions to write in /var/folders/ jmspaggi@virtual:~/hbase-0.95.0-hadoop1$ bin/start-hbase.sh   jmspaggi@virtual:~/hbase-0.95.0-hadoop1$ tail -100f logs/hbase-jmspaggi-master-virtual.log   mardi 2 avril 2013, 07:24:13 (utc-0400) starting master on virtual  core file size (blocks, -c) 0  data seg size (kbytes, -d) unlimited  scheduling priority (-e) 0  file size (blocks, -f) unlimited  pending signals (-i) 31634  max locked memory (kbytes, -l) 64  max memory size (kbytes, -m) unlimited  open files (-n) 1024  pipe size (512 bytes, -p) 8  posix message queues (bytes, -q) 819200  real-time priority (-r) 0  stack size (kbytes, -s) 8192  cpu time (seconds, -t) unlimited  max user processes (-u) 31634  virtual memory (kbytes, -v) unlimited  file locks (-x) unlimited  2013-04-02 07:24:16,093 info org.apache.hadoop.hbase.util.versioninfo: hbase 0.95.0-hadoop1  2013-04-02 07:24:16,132 info org.apache.hadoop.hbase.util.versioninfo: subversion file:///users/stack/checkouts/0.95/hbase-common -r unknown  2013-04-02 07:24:16,132 info org.apache.hadoop.hbase.util.versioninfo: compiled by stack on mon apr 1 15:38:48 pdt 2013  2013-04-02 07:24:17,475 info org.apache.zookeeper.server.zookeeperserver: server environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 gmt  2013-04-02 07:24:17,475 info org.apache.zookeeper.server.zookeeperserver: server environment:host.name=virtual.distparser.com  2013-04-02 07:24:17,586 info org.apache.zookeeper.server.zookeeperserver: server environment:java.version=1.7.0_13  2013-04-02 07:24:17,587 info org.apache.zookeeper.server.zookeeperserver: server environment:java.vendor=oracle corporation  2013-04-02 07:24:17,587 info org.apache.zookeeper.server.zookeeperserver: server environment:java.home=/home/jmspaggi/jdk/jre  2013-04-02 07:24:17,587 info org.apache.zookeeper.server.zookeeperserver: server environment:java.class.path=/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../conf:/home/jmspaggi/jdk//lib/tools.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/..:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/activation-1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/asm-3.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-beanutils-1.7.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-cli-1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-codec-1.7.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-collections-3.2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-configuration-1.6.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-digester-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-el-1.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-httpclient-3.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-io-2.4.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-lang-2.6.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-logging-1.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-math-2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-net-1.4.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/core-3.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/guava-12.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hadoop-core-1.1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-client-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-common-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-common-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-examples-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-hadoop1-compat-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-hadoop-compat-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-it-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-it-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-prefix-tree-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-protocol-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-server-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-server-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/high-scale-lib-1.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/htrace-1.50.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/httpclient-4.1.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/httpcore-4.1.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-core-asl-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-xc-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jamon-runtime-2.3.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jasper-compiler-5.5.23.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jasper-runtime-5.5.23.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jaxb-api-2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-core-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-json-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-server-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jetty-6.1.26.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jetty-util-6.1.26.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jruby-complete-1.6.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsp-2.1-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsr305-1.3.9.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/libthrift-0.9.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/log4j-1.2.17.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/metrics-core-2.1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/netty-3.5.9.final.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/protobuf-java-2.4.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/slf4j-api-1.4.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/slf4j-log4j12-1.4.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/stax-api-1.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/xmlenc-0.52.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/zookeeper-3.4.5.jar:/etc/hadoop:/usr/lib/jvm/java-6-sun/lib/tools.jar:/usr/libexec/../share/hadoop:/usr/libexec/../share/hadoop/hadoop-core-1.1.1.jar:/usr/libexec/../share/hadoop/lib/asm-3.2.jar:/usr/libexec/../share/hadoop/lib/aspectjrt-1.6.11.jar:/usr/libexec/../share/hadoop/lib/aspectjtools-1.6.11.jar:/usr/libexec/../share/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/libexec/../share/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/libexec/../share/hadoop/lib/commons-cli-1.2.jar:/usr/libexec/../share/hadoop/lib/commons-codec-1.4.jar:/usr/libexec/../share/hadoop/lib/commons-collections-3.2.1.jar:/usr/libexec/../share/hadoop/lib/commons-configuration-1.6.jar:/usr/libexec/../share/hadoop/lib/commons-daemon-1.0.1.jar:/usr/libexec/../share/hadoop/lib/commons-digester-1.8.jar:/usr/libexec/../share/hadoop/lib/commons-el-1.0.jar:/usr/libexec/../share/hadoop/lib/commons-httpclient-3.0.1.jar:/usr/libexec/../share/hadoop/lib/commons-io-2.1.jar:/usr/libexec/../share/hadoop/lib/commons-lang-2.4.jar:/usr/libexec/../share/hadoop/lib/commons-logging-1.1.1.jar:/usr/libexec/../share/hadoop/lib/commons-logging-api-1.0.4.jar:/usr/libexec/../share/hadoop/lib/commons-math-2.1.jar:/usr/libexec/../share/hadoop/lib/commons-net-3.1.jar:/usr/libexec/../share/hadoop/lib/core-3.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-capacity-scheduler-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-fairscheduler-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-thriftfs-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hsqldb-1.8.0.10.jar:/usr/libexec/../share/hadoop/lib/jackson-core-asl-1.8.8.jar:/usr/libexec/../share/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/usr/libexec/../share/hadoop/lib/jasper-compiler-5.5.12.jar:/usr/libexec/../share/hadoop/lib/jasper-runtime-5.5.12.jar:/usr/libexec/../share/hadoop/lib/jdeb-0.8.jar:/usr/libexec/../share/hadoop/lib/jersey-core-1.8.jar:/usr/libexec/../share/hadoop/lib/jersey-json-1.8.jar:/usr/libexec/../share/hadoop/lib/jersey-server-1.8.jar:/usr/libexec/../share/hadoop/lib/jets3t-0.6.1.jar:/usr/libexec/../share/hadoop/lib/jetty-6.1.26.jar:/usr/libexec/../share/hadoop/lib/jetty-util-6.1.26.jar:/usr/libexec/../share/hadoop/lib/jsch-0.1.42.jar:/usr/libexec/../share/hadoop/lib/junit-4.5.jar:/usr/libexec/../share/hadoop/lib/kfs-0.2.2.jar:/usr/libexec/../share/hadoop/lib/log4j-1.2.15.jar:/usr/libexec/../share/hadoop/lib/mockito-all-1.8.5.jar:/usr/libexec/../share/hadoop/lib/oro-2.0.8.jar:/usr/libexec/../share/hadoop/lib/servlet-api-2.5-20081211.jar:/usr/libexec/../share/hadoop/lib/slf4j-api-1.4.3.jar:/usr/libexec/../share/hadoop/lib/slf4j-log4j12-1.4.3.jar:/usr/libexec/../share/hadoop/lib/xmlenc-0.52.jar:/usr/libexec/../share/hadoop/lib/jsp-2.1/jsp-2.1.jar:/usr/libexec/../share/hadoop/lib/jsp-2.1/jsp-api-2.1.jar  2013-04-02 07:24:17,588 info org.apache.zookeeper.server.zookeeperserver: server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib  2013-04-02 07:24:17,588 info org.apache.zookeeper.server.zookeeperserver: server environment:java.io.tmpdir=/tmp  2013-04-02 07:24:17,588 info org.apache.zookeeper.server.zookeeperserver: server environment:java.compiler=<na>  2013-04-02 07:24:17,589 info org.apache.zookeeper.server.zookeeperserver: server environment:os.name=linux  2013-04-02 07:24:17,589 info org.apache.zookeeper.server.zookeeperserver: server environment:os.arch=amd64  2013-04-02 07:24:17,589 info org.apache.zookeeper.server.zookeeperserver: server environment:os.version=3.2.0-4-amd64  2013-04-02 07:24:17,589 info org.apache.zookeeper.server.zookeeperserver: server environment:user.name=jmspaggi  2013-04-02 07:24:17,589 info org.apache.zookeeper.server.zookeeperserver: server environment:user.home=/home/jmspaggi  2013-04-02 07:24:17,589 info org.apache.zookeeper.server.zookeeperserver: server environment:user.dir=/home/jmspaggi/hbase-0.95.0-hadoop1  2013-04-02 07:24:17,598 error org.apache.hadoop.hbase.master.hmastercommandline: failed to start master  java.io.ioexception: unable to create data directory /var/folders/bp/2z1cykc92rs6j24251cg__ph0000gp/t/hbase-stack/zookeeper/zookeeper_0/version-2  at org.apache.zookeeper.server.persistence.filetxnsnaplog.<init>(filetxnsnaplog.java:85)  at org.apache.zookeeper.server.zookeeperserver.<init>(zookeeperserver.java:213)  at org.apache.hadoop.hbase.zookeeper.minizookeepercluster.startup(minizookeepercluster.java:161)  at org.apache.hadoop.hbase.zookeeper.minizookeepercluster.startup(minizookeepercluster.java:131)  at org.apache.hadoop.hbase.master.hmastercommandline.startmaster(hmastercommandline.java:137)  at org.apache.hadoop.hbase.master.hmastercommandline.run(hmastercommandline.java:107)  at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65)  at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:78)  at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:2482) ",
        "label": 314
    },
    {
        "text": "hbase shell deleteall to  meta  allows insertion of malformed rowkey  when using the hbase shell to manipulate meta entries, one is allowed to 'delete' malformed rows (entries with less than 2 ascii 44 ',' chars). when this happens hbase servers may go down and the cluster will not be restartable without manual intervention. the delete results in a durable malformed rowkey in .meta.'s memstore, .meta.'s hlog, and eventually .meta.'s hfiles. subsequent scans to meta (such as when a hmaster starts) fail in the scanner because the comparator fails. in the case of an hmaster startup, it causes an abort that kills the hmaster process. 12/04/18 22:07:34 fatal master.hmaster: unhandled exception. starting shutdown. org.apache.hadoop.ipc.remoteexception: java.io.ioexception: java.lang.illegalargumentexception: no 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54 at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:990) at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:979) at org.apache.hadoop.hbase.regionserver.hregionserver.next(hregionserver.java:1894) at org.apache.hadoop.hbase.regionserver.hregionserver.next(hregionserver.java:1834) at sun.reflect.generatedmethodaccessor31.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1039) caused by: java.lang.illegalargumentexception: no 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54 at org.apache.hadoop.hbase.keyvalue.getrequireddelimiterinreverse(keyvalue.java:1300) at org.apache.hadoop.hbase.keyvalue$metakeycomparator.comparerows(keyvalue.java:1846) at org.apache.hadoop.hbase.regionserver.scanquerymatcher.match(scanquerymatcher.java:130) at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:257) at org.apache.hadoop.hbase.regionserver.keyvalueheap.next(keyvalueheap.java:114) at org.apache.hadoop.hbase.regionserver.hregion$regionscanner.nextinternal(hregion.java:2435) at org.apache.hadoop.hbase.regionserver.hregion$regionscanner.next(hregion.java:2391) at org.apache.hadoop.hbase.regionserver.hregion$regionscanner.next(hregion.java:2408) at org.apache.hadoop.hbase.regionserver.hregionserver.next(hregionserver.java:1870) ... 6 more at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:771) at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:257) at $proxy9.next(unknown source) at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:264) at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:237) at org.apache.hadoop.hbase.catalog.metareader.fullscanofresults(metareader.java:220) at org.apache.hadoop.hbase.master.assignmentmanager.rebuilduserregions(assignmentmanager.java:1580) at org.apache.hadoop.hbase.master.assignmentmanager.processfailover(assignmentmanager.java:221) at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:422) at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:295) 12/04/18 22:07:34 info master.hmaster: aborting  ",
        "label": 375
    },
    {
        "text": "cluster won't stop  it seems that clusters on trunk have some trouble stopping. even manually deleting the shutdown file in zk doesn't always help. investigate. ",
        "label": 180
    },
    {
        "text": "hadoop has deprecated filesystem delete path  in favor of filesystem delete path  boolean   hadoop has deprecated filesystem.delete(path) in favor of filesystem.delete(path, boolean). hbase code should be updated to not use deprecated methods. additionally, since the new filesystem.delete api now supports recursive delete, we can change uses of fsutil.fullydelete(path) to use filesystem.delete(path, true) ",
        "label": 241
    },
    {
        "text": " maven  mvn install breaks running tests  it does not include hbase classes  in current trunk, mvn install is broke. test compilation fails: [info] [resources:testresources {execution: default-testresources}] [info] using 'utf-8' encoding to copy filtered resources. [info] copying 4 resources [info] [compiler:testcompile {execution: default-testcompile}] [info] compiling 101 source files to /users/stack/checkouts/trunk/core/target/test-classes [info] ------------------------------------------------------------- [error] compilation error : [info] ------------------------------------------------------------- [error] /users/stack/checkouts/trunk/core/src/test/java/org/apache/hadoop/hbase/io/hfile/testseekto.java:[26,30] [deprecation] org.apache.hadoop.hbase.hbasetestcase in org.apache.hadoop.hbase has been deprecated [error] /users/stack/checkouts/trunk/core/src/test/java/org/apache/hadoop/hbase/io/hfile/testseekto.java:[27,35] cannot find symbol symbol  : class bytes location: package org.apache.hadoop.hbase.util [error] /users/stack/checkouts/trunk/core/src/test/java/org/apache/hadoop/hbase/hbasetestcase.java:[36,37] cannot find symbol symbol  : class delete location: package org.apache.hadoop.hbase.client ... it does not seem to be able to find hbase classes. ",
        "label": 350
    },
    {
        "text": "store loadstorefiles should close opened files if there's an exception  related to hbase-7513. if a rs is able to open a few store files in store.loadstorefiles but one of them fails like in 7513, the opened files won't be closed and file descriptors will remain in a closed_wait state. the situation we encountered is that over the weekend one region was bounced between >100 region servers and eventually they all started dying on \"too many open files\". ",
        "label": 441
    },
    {
        "text": "hregion's append operation may lose data  like the hbase-6195, when flushing the append thread will read out the old value for the larger timestamp in snapshot and smaller timestamp in memstore. we should make the first-in-thread generates the smaller timestamp. ",
        "label": 414
    },
    {
        "text": "hbase thrift web ui not available  in standalone mode hbase thrift web-ui is gives 404 error. $ ./bin/start-hbase.sh running master, logging to /users/peter.somogyi/hbase/bin/../logs/hbase-peter.somogyi-master-psomogyi-mbp.local.out $ ./bin/hbase thrift start ... jul 12, 2017 12:32:03 pm org.apache.jasper.servlet.jspservlet servicejspfile severe: pwc6117: file \"/users/peter.somogyi/hbase/hbase-server/target/hbase-webapps/thrift/thrift.jsp\" not found ",
        "label": 439
    },
    {
        "text": "tableinputformat tablerecordreaderimpl leaks htable  as far as i can tell, tableinputformat creates an instance of htable which is used by tablerecordreaderimpl. however tablerecordreaderimpl.close() only closes the scanner, not the table. in turn the htable's hconnection's reference count is never decreased which leads to leaking hconnections. tableoutputformat might have a similar bug. ",
        "label": 441
    },
    {
        "text": "when we create the  version file  we should create it in a tmp location and then move it into place  todd suggests over in hbase-3258 that writing hbase.version, we should write it off in a /tmp location and then move it into place after writing it to protect against case where file writer crashes between creation and write. ",
        "label": 38
    },
    {
        "text": "check filesystem permissions on startup  we added a new node to a 44 node cluster starting the datanode, mapred and regionserver processes on it. the unix filesystem was configured incorrectly, i.e. /tmp was not writable to processes. all three processes had issues with this. datanode and mapred shutdown on exception.  regionserver did not stop, in fact reported to master that its up without regions. so master assigned regions to it. regionserver would not accept them, resulting in a constant assign, reject, reassign cycle, that put many regions into a state of not being available. there are no logs about this, but we could observer the regioncount fluctuate by hundredths of regions and the application throwing many notservingregion exceptions. in fact to the master process the regionserver looked fine, so it was trying to send regions its way. regionserver rejected them. so the master/balancer was going into a assign/reassign cycle destabilizing the cluster. many puts and gets simply failed with notservingregionexceptions and took a long time to complete. exception from regionserver:  2011-08-06 23:57:13,953 info org.apache.hadoop.hbase.regionserver.hregionserver: got zookeeper event, state: syncconnected, type: nodecreated, path: /hbase/master  2011-08-06 23:57:13,957 info org.apache.hadoop.hbase.regionserver.hregionserver: telling master at 17.1.0.1:60000 that we are up  2011-08-06 23:57:13,957 info org.apache.hadoop.hbase.regionserver.hregionserver: telling master at 17.1.0.1:60000 that we are up  2011-08-07 00:07:39.648::info: logging to stderr via org.mortbay.log.stderrlog  2011-08-07 00:07:39.712::info: jetty-6.1.14  2011-08-07 00:07:39.742::warn: tmpdir  java.io.ioexception: permission denied  at java.io.unixfilesystem.createfileexclusively(native method)  at java.io.file.checkandcreate(file.java:1704)  at java.io.file.createtempfile(file.java:1792)  at java.io.file.createtempfile(file.java:1828)  at org.mortbay.jetty.webapp.webappcontext.gettempdirectory(webappcontext.java:745)  at org.mortbay.jetty.webapp.webappcontext.dostart(webappcontext.java:458)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.mortbay.jetty.handler.handlercollection.dostart(handlercollection.java:152)  at org.mortbay.jetty.handler.contexthandlercollection.dostart(contexthandlercollection.java:156)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.mortbay.jetty.handler.handlerwrapper.dostart(handlerwrapper.java:130)  at org.mortbay.jetty.server.dostart(server.java:222)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.apache.hadoop.http.httpserver.start(httpserver.java:461)  at org.apache.hadoop.hbase.regionserver.hregionserver.startservicethreads(hregionserver.java:1168)  at org.apache.hadoop.hbase.regionserver.hregionserver.init(hregionserver.java:792)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:430)  at java.lang.thread.run(thread.java:619) exception from datanode:  2011-08-06 23:37:20,444 info org.apache.hadoop.http.httpserver: jetty bound to port 50075  2011-08-06 23:37:20,444 info org.mortbay.log: jetty-6.1.14  2011-08-06 23:37:20,469 warn org.mortbay.log: tmpdir  java.io.ioexception: permission denied  at java.io.unixfilesystem.createfileexclusively(native method)  at java.io.file.checkandcreate(file.java:1704)  at java.io.file.createtempfile(file.java:1792)  at java.io.file.createtempfile(file.java:1828)  at org.mortbay.jetty.webapp.webappcontext.gettempdirectory(webappcontext.java:745)  at org.mortbay.jetty.webapp.webappcontext.dostart(webappcontext.java:458)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.mortbay.jetty.handler.handlercollection.dostart(handlercollection.java:152)  at org.mortbay.jetty.handler.contexthandlercollection.dostart(contexthandlercollection.java:156)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.mortbay.jetty.handler.handlerwrapper.dostart(handlerwrapper.java:130)  at org.mortbay.jetty.server.dostart(server.java:222)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.apache.hadoop.http.httpserver.start(httpserver.java:463)  at org.apache.hadoop.hdfs.server.datanode.datanode.startdatanode(datanode.java:384)  at org.apache.hadoop.hdfs.server.datanode.datanode.<init>(datanode.java:225)  at org.apache.hadoop.hdfs.server.datanode.datanode.makeinstance(datanode.java:1309)  at org.apache.hadoop.hdfs.server.datanode.datanode.instantiatedatanode(datanode.java:1264)  at org.apache.hadoop.hdfs.server.datanode.datanode.createdatanode(datanode.java:1272)  at org.apache.hadoop.hdfs.server.datanode.datanode.main(datanode.java:1394)  2011-08-06 23:37:20,471 info org.apache.hadoop.hdfs.server.datanode.datanode: shutdown_msg:   /************************************************************  shutdown_msg: shutting down datanode at hdp1122/17.1.0.22  ************************************************************/ exception from tasktracker:  2011-08-06 23:33:50,380 info org.apache.hadoop.http.httpserver: jetty bound to port 50060  2011-08-06 23:33:50,380 info org.mortbay.log: jetty-6.1.14  2011-08-06 23:33:50,415 warn org.mortbay.log: tmpdir  java.io.ioexception: permission denied  at java.io.unixfilesystem.createfileexclusively(native method)  at java.io.file.checkandcreate(file.java:1704)  at java.io.file.createtempfile(file.java:1792)  at java.io.file.createtempfile(file.java:1828)  at org.mortbay.jetty.webapp.webappcontext.gettempdirectory(webappcontext.java:745)  at org.mortbay.jetty.webapp.webappcontext.dostart(webappcontext.java:458)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.mortbay.jetty.handler.handlercollection.dostart(handlercollection.java:152)  at org.mortbay.jetty.handler.contexthandlercollection.dostart(contexthandlercollection.java:156)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.mortbay.jetty.handler.handlerwrapper.dostart(handlerwrapper.java:130)  at org.mortbay.jetty.server.dostart(server.java:222)  at org.mortbay.component.abstractlifecycle.start(abstractlifecycle.java:50)  at org.apache.hadoop.http.httpserver.start(httpserver.java:463)  at org.apache.hadoop.mapred.tasktracker.<init>(tasktracker.java:935)  at org.apache.hadoop.mapred.tasktracker.main(tasktracker.java:2837)  2011-08-06 23:33:50,416 info org.apache.hadoop.mapred.tasktracker: shutdown_msg:   /************************************************************  shutdown_msg: shutting down tasktracker at hdp1122/17.1.0.22  ************************************************************/ ",
        "label": 544
    },
    {
        "text": "after disabling enabling a table  the regions seems to be assigned to only region servers  after disabling/enabling a small table (20 regions), we see that the master tend to assign the regions to only 1-2 region servers. unfortunately, that table is extensively used in random reads which really kills those rs when they hold those regions. as a fix, we have to restart hbase... ",
        "label": 229
    },
    {
        "text": "backport hbase  collect executor status info periodically and report to metrics system  to branch  ",
        "label": 521
    },
    {
        "text": "regiontoobusyexception should provide region name which was too busy  under this thread: http://search-hadoop.com/m/wsfkp1yjofj, john showed log from loadincrementalhfiles where the following is a snippet: 04:18:07,110  info loadincrementalhfiles:451 - trying to load hfile=hdfs://pc08.pool.ifis.uni-luebeck.de:8020/tmp/bulkloaddirectory/po_s_rowbufferhfile/hexa/_tmp/po_s,9.bottom first=<http://purl.org/dc/elements/1.1/title>,\"emulates drylot births\"^^<http://www.w3.org/2001/xmlschema#string> last=<http://purl.org/dc/e$ org.apache.hadoop.hbase.client.retriesexhaustedexception: failed after attempts=10, exceptions: sun oct 20 04:15:50 cest 2013, org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles$3@4cfdfc98, org.apache.hadoop.hbase.regiontoobusyexception: org.apache.hadoop.hbase.regiontoobusyexception: failed to get a lock in 60000ms         at org.apache.hadoop.hbase.regionserver.hregion.lock(hregion.java:5778)         at org.apache.hadoop.hbase.regionserver.hregion.lock(hregion.java:5764)         at org.apache.hadoop.hbase.regionserver.hregion.startbulkregionoperation(hregion.java:5723)         at org.apache.hadoop.hbase.regionserver.hregion.bulkloadhfiles(hregion.java:3534)         at org.apache.hadoop.hbase.regionserver.hregion.bulkloadhfiles(hregion.java:3517)         at org.apache.hadoop.hbase.regionserver.hregionserver.bulkloadhfiles(hregionserver.java:2793)         at sun.reflect.generatedmethodaccessor14.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) looking at the above, it is not immediately clear which region was busy. region name should be included in the exception so that user can correlate with the region server where the problem occurs. ",
        "label": 191
    },
    {
        "text": "dangerous http methods are been allowed  secure mode   while executing the rest api curl commands in secure cluster , steps to reproduce:  try to change the http methods(get/put/post) to options/trace   execute the commad actual output:  command is getting executed and giving out the 200ok status with the result expected output :  should not accept the other methods and throw 403 error ",
        "label": 486
    },
    {
        "text": "procedure v2   procedure queue pass procedure for better debuggability  changes the various acquire/release methods to take the procedure as argument.  that allows better debuggability.   (the patch it is just a refactor, it does not introduce any new thing) https://reviews.apache.org/r/42271/ ",
        "label": 309
    },
    {
        "text": "move rs to master region open close messaging into zookeeper  as a first step towards hbase-2485, this issue is about changing the message flow of opening and closing of regions without actually changing the implementation of what happens on both the master and regionserver sides. this way we can debug the messaging changes before the introduction of more significant changes to the master architecture and handling of regions in transition. ",
        "label": 268
    },
    {
        "text": "clientsideregionscanner's reaction to scan setbatch is not consistent between hbase versions  in 1.1.3, clientsideregionscanner calls regionscannerimpl#next() with single argument, so it honors scan#setbatch(through defaultscannercontext in regionscannerimpl). // 1.1.3 public class clientsideregionscanner extends abstractclientscanner {   ...   @override   public result next() throws ioexception {     values.clear();     scanner.nextraw(values);     ...  but in 1.0.3 and 0.98.17, clientsideregionscanner calls regionscannerimpl#next() with limit=-1, so it ignores scan#setbatch. // 1.0.3 and 0.98.17 public class clientsideregionscanner extends abstractclientscanner {   ...   @override   public result next() throws ioexception {     values.clear();     scanner.nextraw(values, -1); // pass -1 as limit so that we see the whole row.     ... ",
        "label": 503
    },
    {
        "text": "coprocessors should be loaded in a custom classloader to prevent dependency conflicts with hbase  currently each coprocessor is loaded with a urlclassloader that puts the coprocessor's jar at the beginning of the classpath. the urlclassloader always tries to load classes from the parent classloader first and only attempts to load from its own configured urls if the class was not found by the parent. this class loading behavior can be problematic for coprocessors that have common dependencies with hbase but whose versions are incompatible. for example, i have a coprocessor that depends on a different version of avro than the version used by hbase. the current class loading behavior results in nosuchmethoderrors in my coprocessor because some avro classes have already been loaded by hbase, and the classloader for my coprocessor picks up hbase's loaded classes first. my proposed solution to this problem is to use a custom classloader when instantiating coprocessor instances. this custom classloader would always attempt to load classes from the coprocessor's jar first and would only delegate to the parent classloader if the class were not found in the coprocessor jar. however, certain classes would need to be exempt from this behavior. as an example, if the copcoessor interface were loaded by both the region server's classloader and the coprocessor's custom classloader, then the region server would get a classcastexception when attempting to cast the coprocessor instance to the coprocessor interface. this problem can be avoided by defining a set of class name prefixes that would be exempt from loading by the custom classloader. when loading a class, if the class starts with any of these prefixes (e.g. \"org.apache.hadoop\"), then the classloader would delegate immediately to the parent classloader. i've already implemented a patch to provide this functionality which i'll attach shortly. ",
        "label": 38
    },
    {
        "text": "race between balancer and disable table can lead to inconsistent cluster  appear in 94. trunk is ok for the issue  balancer will collect the regionplans to move(unassign and then assign).  before unassign, disable table appears,   after close the region in rs, master will delete the znode, romove region from rit,  and then clean the region from the online regions. during romoving region from rit and cleaning out the region from the online regions.   balancer begins to unassign, it will get a notservingregionexception and if the table is disabling, it will deal with the state in master and delete the znode . however the table is disabled now, so the rit and znode will remain. timeoutmonitor draws a blank on it. it will hold back enabling the table or balancer unless restart ",
        "label": 518
    },
    {
        "text": "declare perl namespace in hbase thrift  this patch adds a namespace for perl bindings ",
        "label": 88
    },
    {
        "text": "clean up defunct git branches  when the asf ban on branch deletion is lifted, clean up the mistakenly pushed stuff we have sitting in git. this issue should track the discussion of which branches should go. ",
        "label": 402
    },
    {
        "text": "dynamic metrics2 metrics may consume large amount of heap memory  user sunweiei provided the following jmap output in 0.96 deployment:  num     #instances         #bytes  class name ----------------------------------------------    1:      14917882     3396492464  [c    2:       1996994     2118021808  [b    3:      43341650     1733666000  java.util.linkedhashmap$entry    4:      14453983     1156550896  [ljava.util.hashmap$entry;    5:      14446577      924580928  org.apache.hadoop.metrics2.lib.interns$cachewith2keys$2 heap consumption by interns$cachewith2keys$2 could be due to calls to interns.info() in dynamicmetricsregistry.  this scenario would arise when large number of regions are tracked through metrics2.  interns class doesn't provide api to remove entries in its internal map.  meaning, dynamicmetricsregistry#clearmetrics() is unable to reclaim that part of heap. ",
        "label": 441
    },
    {
        "text": " fb  isolate assignment plan on subset of machines  we use assignmentplan to decide on favored nodes for regions. right now we include all machines from the cluster in the assignment plan of a table. we want ability to limit the number of machines when assigning regions. this will help us to isolate the failure domain in case different users are using the cluster (in case someone is misusing the cluster, hopefully just a subset of machines would be affected). ",
        "label": 154
    },
    {
        "text": "npe in hconnectionmanager tableservers processbatchofrows  had dfs errors take down a region server. looks like a full restart may be necessary because errors on the region continue. clients attempting to store to the region get a npe in hconnectionmanager$tableservers.processbatchofrows. caused by: java.lang.nullpointerexception  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.processbatchofrows(hconnectionmanager.java:911)  at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1267)  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1238)  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1218)  at net.iridiant.content.content.store(unknown source)  at net.iridiant.content.content.store(unknown source)  ... 6 more ",
        "label": 38
    },
    {
        "text": "backport regionserver groups  hbase  to branch  based on dev list discussion backporting region server group should not be an issue as it does not: 1. destabilize the code. 2. cause backward incompatibility. ",
        "label": 38
    },
    {
        "text": "add remove peer requests should be routed through master  replicationadmin directly operates over the zookeeper data for replication setup. we should move these operations to be routed through master for two reasons: replication implementation details are exposed to client. we should move most of the replication related classes to hbase-server package. routing the requests through master is the standard practice for all other operations. it allows for decoupling implementation details from the client and code. review board: https://reviews.apache.org/r/54730/ ",
        "label": 187
    },
    {
        "text": "scannext metric is size based while every other per operation metric is time based  we have per-operation metrics for get, mutate, delete, increment, and scannext. the metrics are emitted like:    \"get_num_ops\" : 4837505,     \"get_min\" : 0,     \"get_max\" : 296,     \"get_mean\" : 0.2934618155433431,     \"get_median\" : 0.0,     \"get_75th_percentile\" : 0.0,     \"get_95th_percentile\" : 1.0,     \"get_99th_percentile\" : 1.0, ...     \"scannext_num_ops\" : 194705,     \"scannext_min\" : 0,     \"scannext_max\" : 18441,     \"scannext_mean\" : 7468.274651395701,     \"scannext_median\" : 583.0,     \"scannext_75th_percentile\" : 583.0,     \"scannext_95th_percentile\" : 13481.0,     \"scannext_99th_percentile\" : 13481.0, the problem is that all of get,mutate,delete,increment,append,replay are time based tracking how long the operation ran, while scannext is tracking returned response sizes (returned cell-sizes to be exact). obviously, this is very confusing and you would only know this subtlety if you read the metrics collection code. not sure how useful is the scannext metric as it is today. we can deprecate it, and introduce a time based one to keep track of scan request latencies. ps. shamelessly using the parent jira (since these seem relavant). ",
        "label": 198
    },
    {
        "text": "polish the admin interface  the snapshot related methods are not well declared, we missed several methods which has the restoreacl parameter. and also, the snapshotasync method returns nothing, which is bit strange. and we can use default methods to reduce the code in hbaseadmin and also the new admin implementation in the future. ",
        "label": 149
    },
    {
        "text": "add metrics around heapmemorymanager  it would be good to know how many invocations there have been.  how many decided to expand memstore.  how many decided to expand block cache.  how many decided to do nothing.  etc. when that's done use those metrics to clean up the tests. ",
        "label": 370
    },
    {
        "text": "distributed log splitting coding enhancement to make it easier to understand  no semantics change  in reviewing distributed log splitting feature, we found some cosmetic issues. they make the code hard to understand.  it will be great to fix them. for this issue, there should be no semantic change. ",
        "label": 242
    },
    {
        "text": " fb  tier based compaction  currently, the compaction selection is not very flexible and is not sensitive to the hotness of the data. very old data is likely to be accessed less, and very recent data is likely to be in the block cache. both of these considerations make it inefficient to compact these files as aggressively as other files. in some use-cases, the access-pattern is particularly obvious even though there is no way to control the compaction algorithm in those cases. in the new compaction selection algorithm, we plan to divide the candidate files into different levels according to oldness of the data that is present in those files. for each level, parameters like compaction ratio, minimum number of store-files in each compaction may be different. number of levels, time-ranges, and parameters for each level will be configurable online on a per-column family basis. ",
        "label": 294
    },
    {
        "text": "npes running canary  there's a few. here is one: 19/11/01 23:54:16 error tool.canary: run regionmonitor failed java.lang.nullpointerexception at org.apache.hadoop.hbase.tool.canarytool.sniff(canarytool.java:1582) at org.apache.hadoop.hbase.tool.canarytool.sniff(canarytool.java:1562) at org.apache.hadoop.hbase.tool.canarytool.access$100(canarytool.java:124) at org.apache.hadoop.hbase.tool.canarytool$regionmonitor.run(canarytool.java:1366) at java.base/java.lang.thread.run(thread.java:834) ",
        "label": 314
    },
    {
        "text": "disable block cache on compactions  is there a good reason to believe that caching blocks during compactions is beneficial? currently, if block cache is enabled on a certain family, then every time it's compacted, we load all of its blocks into the (lru) cache, at the expense of the legitimately hot ones. as a matter of fact, this concern was raised earlier in hbase-1597, which rightly points out that, \"we should not bog down the lru with unneccessary blocks\" during compaction. even though that issue has been marked as \"fixed\", it looks like it ought to be reopened. should we err on the side of caution and not cache blocks during compactions period (as illustrated in the attached patch)? or, can we be selectively aggressive about what blocks do get cached during compaction (e.g., only cache those blocks from the recent files)? ",
        "label": 324
    },
    {
        "text": "backport the pre commit changes in hbase to all active branches  ",
        "label": 149
    },
    {
        "text": "handle possible nullpointerexception in fsutils create filesystem fs  path path  fspermission perm  inetsocketaddress  favorednodes   running test suite on hadoop 2.0, i saw the following test failure: testfavorednodes(org.apache.hadoop.hbase.regionserver.testregionfavorednodes)  time elapsed: 0.106 sec  <<< error! org.apache.hadoop.hbase.exceptions.droppedsnapshotexception: region: table,rrr,1369355298031.1fdb1b446b02b497f0869a08adad7745.         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1568)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1429)         at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:1347)         at org.apache.hadoop.hbase.minihbasecluster.flushcache(minihbasecluster.java:531)         at org.apache.hadoop.hbase.hbasetestingutility.flush(hbasetestingutility.java:961)         at org.apache.hadoop.hbase.regionserver.testregionfavorednodes.testfavorednodes(testregionfavorednodes.java:132)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47)         at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12)         at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44)         at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17)         at org.junit.runners.parentrunner.runleaf(parentrunner.java:271)         at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70)         at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50)         at org.junit.runners.parentrunner$3.run(parentrunner.java:238)         at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63)         at org.junit.runners.parentrunner.runchildren(parentrunner.java:236)         at org.junit.runners.parentrunner.access$000(parentrunner.java:53)         at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229)         at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:26)         at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:27)         at org.junit.runners.parentrunner.run(parentrunner.java:309)         at org.junit.runners.suite.runchild(suite.java:127)         at org.junit.runners.suite.runchild(suite.java:26)         at org.junit.runners.parentrunner$3.run(parentrunner.java:238)         at java.util.concurrent.executors$runnableadapter.call(executors.java:439)         at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)         at java.util.concurrent.futuretask.run(futuretask.java:138)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) caused by: java.io.ioexception: java.lang.nullpointerexception         at org.apache.hadoop.hbase.util.fsutils.create(fsutils.java:293)         at org.apache.hadoop.hbase.io.hfile.abstracthfilewriter.createoutputstream(abstracthfilewriter.java:268)         at org.apache.hadoop.hbase.io.hfile.hfile$writerfactory.create(hfile.java:427)         at org.apache.hadoop.hbase.regionserver.storefile$writer.<init>(storefile.java:791)         at org.apache.hadoop.hbase.regionserver.storefile$writer.<init>(storefile.java:733)         at org.apache.hadoop.hbase.regionserver.storefile$writerbuilder.build(storefile.java:671)         at org.apache.hadoop.hbase.regionserver.hstore.createwriterintmp(hstore.java:799)         at org.apache.hadoop.hbase.regionserver.defaultstoreflusher.flushsnapshot(defaultstoreflusher.java:75)         at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:704)         at org.apache.hadoop.hbase.regionserver.hstore$storeflusherimpl.flushcache(hstore.java:1813)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1543)         ... 33 more caused by: java.lang.nullpointerexception         at org.apache.hadoop.hdfs.dfsclient.create(dfsclient.java:1306)         at org.apache.hadoop.hdfs.distributedfilesystem.create(distributedfilesystem.java:283) distributedfilesystem#create() which supports favorednodes parameter threw nullpointerexception.  we should handle the nullpointerexception and fall back to conventional distributedfilesystem#create() ",
        "label": 441
    },
    {
        "text": " performance  make hbase splits run faster  hbase-1506 tried and failed making splits faster in 0.20 context. this issue is about doing it in 0.21 where we'll have to tools to do. ",
        "label": 314
    },
    {
        "text": "autorestart doesn't work if zkcleaner fails  i've seen this several times where a master didn't autorestart because zk cleaner failed. we should still restart the daemon even if it's not possible to clean the zk nodes. ",
        "label": 314
    },
    {
        "text": "split reports incorrect elapsed time  split reports incorrect elapsed time. that is because the start time for the split is never set. (it used to be set in closing()). additionally, since compactsplitthread doesn't do anything in closing or closed anymore, why keep them around? we can just pass null for the regionunavailablelistener and can then remove closing and closed from compactsplitthread. in fact, it turns out that regionunavailablelistener is not used anywhere anymore so it should just be removed altogether. ",
        "label": 241
    },
    {
        "text": "ensure that we flush close regions before the rs stops answering client requests   currently, the rs stops responding as soon as the stop is requested. we then go in and close all regions in a 2-flush mechanism. ensure that rs will first close the regions, and then stop taking client requests. this will reduce the number of errors seen by the client. ",
        "label": 154
    },
    {
        "text": "remove methods that have text as a parameter and were deprecated in  hbase-799 deprecated methods that take text as a parameter in hbase-0.2.1 for this release, those same methods should be removed. ",
        "label": 229
    },
    {
        "text": "bulk incremental load into an existing table  hbase-48 is about bulk load of a new table,maybe it's more practicable to bulk load aganist a existing table. ",
        "label": 453
    },
    {
        "text": "remove unnecessary traversing to the first and last keys in the cellset  the implementation of finding the first and last keys in the cellset is as following:  public cell first() {     return this.delegatee.get(this.delegatee.firstkey());   }   public cell last() {     return this.delegatee.get(this.delegatee.lastkey());   } recall we have cell to cell mapping, therefore the methods bringing the first/last key, which allready return cell. thus no need to waist time on the get() method for the same cell.  fix: return just the first/lastkey(), should be at least twice more effective. ",
        "label": 455
    },
    {
        "text": "region server should deleted restore log after successfull restore  currently we do not remove the restore log \"oldlogfile.log\" after we reopen a region after a crashed region server. suggestion would be to remove after we successfully flush of all the edits to a mapfile so something like:  replay log   memcache flush  deleted log ",
        "label": 241
    },
    {
        "text": "move hstorekey back to o a h h  ",
        "label": 241
    },
    {
        "text": "api polluted with default and protected access data members and methods  if you look at the api in javadoc, its polluted with data members and methods of protected or default access. it makes it difficult for users figuring how the api should be used. at least the user-facing classes \u2013 htable and hbaseadmin \u2013 need cleanup ",
        "label": 314
    },
    {
        "text": "delete table column should delete stored permissions on  acl  table  deleting the table or a column does not cascade to the stored permissions at the acl table. we should also remove those permissions, otherwise, it can be a security leak, where freshly created tables contain permissions from previous same-named tables. we might also want to ensure, upon table creation, that no entries are already stored at the acl table. ",
        "label": 309
    },
    {
        "text": "htable get  uninteruptible  i've got a stuck thread on htable.get() that can't be interrupted, looks like its designed to be interruptible but can't be in interrupted in practice due to while loop. the offending code is in org.apache.hadoop.hbase.ipc.hbaseclient.call() line 981, it catches interruptedexception then goes right back to waiting due to the while loop. it looks like future versions of the client (.95+) are significantly different and might not have this problem... not sure about release schedules etc. or if this version is still getting patched. ",
        "label": 340
    },
    {
        "text": "remove references to writable in the ipc package  i see references to writable in the ipc package, most notably in the invocation class. this class is not being used that much in the core ipc package but used in the coprocessor protocol implementations (there are some coprocessor protocols that are writable based still). this jira is to track removing those references and the invocation class (once hbase-6895 is resolved). ",
        "label": 314
    },
    {
        "text": "request is always zero in webui for region server  request is always zero in webui for region server  >  > metrics request=0.0, regions=36, stores=36, storefiles=148,   > storefileindexsize=29, memstoresize=253, compactionqueuesize=24,   > flushqueuesize=0, usedheap=655, maxheap=8175, blockcachesize=14230920,   > blockcachefree=1700269560, blockcachecount=21,   > blockcachehitcount=2887, blockcachemisscount=204829,   > blockcacheevictedcount=0, blockcachehitratio=1,   > blockcachehitcachingratio=99  >  > requests is not zero in webui for hmaster requests=15000, regions=35,   > usedheap=513, maxheap=8175  >  > is there any different for these metrics?  > how do i use it?  > thanks.  >  > ",
        "label": 529
    },
    {
        "text": "inittablereducerjob  unused method parameter   in method org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.inittablereducerjob(string, class<? extends tablereducer>, job, class) the partitioner parameter was not passed to called org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.inittablereducerjob(string, class<? extends tablereducer>, job, class) method. ",
        "label": 291
    },
    {
        "text": "potential null object deference in assignmentmanager handleregion   here is the related code, starting line 921:           if (regionstate == null               || !regionstate.ispendingopenoropeningonserver(sn)) {             log.warn(\"received opened for \" + prettyprintedregionname               + \" from \" + sn + \" but the region isn't pending_open/opening here: \"               + regionstates.getregionstate(encodedname));             // close it without updating the internal region states,             // so as not to create double assignments in unlucky scenarios             // mentioned in openregionhandler#process             unassign(regionstate.getregion(), null, -1, null, false, sn); if regionstate is null, we should not dereference it. ",
        "label": 242
    },
    {
        "text": "potential npe in hbasefsck checkmetaregion   metahbckinfo is initialized to null and assigned when metaregions is not empty. however, it is dereferenced unconditionally later (line 2710):           // try fix it (treat is a dupe assignment)           hbasefsckrepair.fixmultiassignment(admin, metahbckinfo.metaentry, servers); ",
        "label": 441
    },
    {
        "text": "clean up so core is ready for development on a recent version of c   ",
        "label": 154
    },
    {
        "text": "classes in security subpackages missing  interfaceaudience annotations   i was reading some of the security related code and noticed that many of the security related classes lack @interfaceaudience markings. with the current api i believe all but permission should be private. with the introduction of cell level acl's permission must be public because it is now exposed in the mutation setacl calls[1]. there is an inconsistency with the mutation acl \u2013 the acl setters take permission instances but the getter returns byte[]'s. as a follow on issue we could change the signature of mutation.setacl so we don't have to expose the permission class and convert it to be byte[], or change the getter to return an exposed permission instance. [1] http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/mutation.html#setacl(java.util.map) ",
        "label": 38
    },
    {
        "text": "have catalogjanitor report holes and overlaps  i e  problems it sees when doing its regular scan of hbase meta  catalogjanitor scans hbase:meta on a configurable period. currently it just looks for splits and merges to gc. might as well generate a report on stuff like holds, overlaps, empty regioninfos, and unknown servers while it is at it. ",
        "label": 314
    },
    {
        "text": "hbaseadmin mergeregions should recognize both full region names and encoded region names  hbaseadmin.mergeregions() calls hbaseadmin.getregion() internally. hbaseadmin.getregion() requires the full region name. so metatableaccessor.getregion always returns null and this causes one more meta table scan.   pair<hregioninfo, servername> getregion(final byte[] regionname) throws ioexception {     if (regionname == null) {       throw new illegalargumentexception(\"pass a table name or region name\");     }     pair<hregioninfo, servername> pair =       metatableaccessor.getregion(connection, regionname);     if (pair == null) { i suppose to use full region names instead of encoded region names in hbaseadmin.mergeregions(). ",
        "label": 165
    },
    {
        "text": "document that loadincrementalhfiles will be removed in  here we break the rule, it should be removed in 4.0.0 by default. so we need to document clearly that it will be removed 3.0.0, and also explain the reason. ",
        "label": 149
    },
    {
        "text": "support for dynamic coprocessor endpoints with pb based rpc  ",
        "label": 180
    },
    {
        "text": " site  make it so each page of manual allows users comment like mysql's manual does  i like the way the mysql manuals allow users comment, improve or correct mysql manual pages. we should have same. ",
        "label": 314
    },
    {
        "text": "cleanerchore checkanddeletedirectory not deleting empty directories  cleanerchore checkanddeletedirectory is not deleting empty directories. as a result, some directories are kept in the fs but should have been removed. to reproduce, simply create an empty directory under /hbase/.archive/table_name/. if you place a file into this directory, it's not more empty and therefore it's correctly removed. ",
        "label": 230
    },
    {
        "text": "move table schema out of hregioninfo  every hri carries a htabledescriptor instance. when a region context needs a table descriptor, doesn't have far to go. move the htd out of hri and when wanted, go elsewhere to go get it. in bigtable paper, schema is stored over in chubby. could run a zookeeper instance easy-enough and store it there. would run on master. zookeeper snapshots its in-memory database to local director on disk \u2013 not dfs. if a zookeeper cluster, then that should protect against loss. master could tell regionservers the address of the zookeeper instance to use (as it does other vitals currently). later we could add the indirection so zookeeper is where regionservers register themselves on startup and master could watch here for the coming and going of servers. or, we could store the schema in dfs. good thing would be replication of critical data and an hbasck tool could read the file to learn table schema (would be awkward having to read zookeeper format out on local filesystem). downside would be that any change in schema would require offlining unless we develop a message that the master could send regionservers to notify them of of minor schema changes \u2013 e.g. flip to being memory-based or to being compressed or that two column families are now of a single locality group (zookeeper has the watcher mechanism where regionservers could 'notice' schema changes). ",
        "label": 314
    },
    {
        "text": "build against hadoop broken  i got the following when compiling trunk against hadoop 0.22: [error] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:compile (default-compile) on project hbase: compilation failure: compilation failure: [error] /users/zhihyu/trunk-hbase/src/main/java/org/apache/hadoop/hbase/regionserver/wal/sequencefilelogreader.java:[37,39] cannot find symbol [error] symbol  : class dfsinputstream [error] location: class org.apache.hadoop.hdfs.dfsclient [error]  [error] /users/zhihyu/trunk-hbase/src/main/java/org/apache/hadoop/hbase/regionserver/wal/sequencefilelogreader.java:[109,37] cannot find symbol [error] symbol  : class dfsinputstream [error] location: class org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.walreader.walreaderfsdatainputstream ",
        "label": 314
    },
    {
        "text": "compaction in out of date store causes region split failure  in out product cluster, we encounter a problem that two daughter regions can not been opened for filenotfoundexception. 2014-01-14,20:12:46,927 info org.apache.hadoop.hbase.regionserver.splitrequest: running rollback/cleanup of failed split of user_profile,xxxxxxxxx,1389671863815.99e016485b0bc142d67ae07a884f6966.; failed lg-hadoop-st34.bj,21600,1389060755669-daughteropener=ec8bbda0f132c481b451fa40e7152b98  java.io.ioexception: failed lg-hadoop-st34.bj,21600,1389060755669-daughteropener=ec8bbda0f132c481b451fa40e7152b98  at org.apache.hadoop.hbase.regionserver.splittransaction.opendaughters(splittransaction.java:375)  at org.apache.hadoop.hbase.regionserver.splittransaction.execute(splittransaction.java:467)  at org.apache.hadoop.hbase.regionserver.splitrequest.run(splitrequest.java:69)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.ioexception: java.io.ioexception: java.io.filenotfoundexception: file does not exist: /hbase/lgprc-xiaomi/user_profile/99e016485b0bc142d67ae07a884f6966/a/5e05d706e4a84f34acc2cf00f089a4cf  .... the reason is that a compaction in an out-of-date store deletes the hfiles, which are referenced by the daughter regions after split. this will cause the daughter regions can not be opened forever. the timeline is that assumption: there are two hfiles: a, b in store a in region r  t0: a compaction request of store a(a+b) in region r is sent. t1: first split for region r. but this split is timeout and rollbacked. in the rollback, region reinitializes all store objects , see splittransaction #824. now the store is region r is a'(a+b). t2: run the compaction sent in t0 . (hfile: a + b -> c): a(a+b) -> a(c). hfile a and b are archived. t3: another split for region r. r splits into two region r.0, r.1, which create hfile references for hfile a, b from store a'(a + b) t4: for hfile a, b have been deleted, the opening for region r.0 and r.1 will failed for filenotfoundexception. i have add a test to identity this problem. after search the jira, maybe hbase-8502 is the same problem. dimitri goldin ",
        "label": 411
    },
    {
        "text": "add debug support for github pr pre commit job  and we could also remove the jira_issue_key parameter, as it is useless... ",
        "label": 149
    },
    {
        "text": " dropping fs latency stats since buffer is full  spam  i see tons of this while running tests (note that it's a warn): 2012-04-03 18:54:47,172 warn org.apache.hadoop.hbase.io.hfile.hfile: dropping fs latency stats since buffer is full while the code says this:   // we don't want to fill up the logs with this message, so only log it    // once every 30 seconds at most   // i also want to avoid locks on the 'critical path' (the common case will be   // uncontended) - hence the cas   private static void logdroppedlatencystat() { it doesn't seem like this message is actionnable and even though it's printed \"only\" every 30 seconds it's still very spammy. we should get rid of it or make it more useful (i don't know which). ",
        "label": 409
    },
    {
        "text": "new compactions logic can silently prevent user initiated compactions from occurring  there seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic from store.java in the function  list<storefile> compactselection(list<storefile> candidates) when a user manually triggers a compaction, this follows the same logic as a normal compaction check. when a user manually triggers a major compaction, something similar happens. putting this all together: 1. if a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). if the number of storefiles to compact is > max files, then we downgrade to a minor compaction  2. if we are in a minor compaction, we do the following checks:  a. if the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it  b. otherwise, we check how the size compares to the next largest size. based on hbase.hstore.compaction.ratio.   c. if the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.  in many of the exit strategies, we aren't seeing an error message.  the net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact. i'm trying to go through and see if i'm understanding things correctly, but this seems like the bug to put it another way  2012-05-02 20:09:36,389 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: large compaction requested: regionname=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store  name=c, filecount=15, filesize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; because: recursive enqueue; compaction_queue=(59:0), split_queue=0 when we had a minimum compaction size of 128m, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran  major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell. note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m). i think the bimodal nature of the sizes prevented us from doing a compaction.  i'm not 100% sure where this errored out because when i manually triggered a compaction, i did not see  ' // if we don't have enough files to compact, just wait   if (filestocompact.size() < this.minfilestocompact) {   if (log.isdebugenabled()) { log.debug(\"skipped compaction of \" + this.storenamestr + \". only \" + (end - start) + \" file(s) of size \" + stringutils.humanreadableint(totalsize) + \" have met compaction criteria.\"); }  '   being printed in the logs (and i know debug logging was enabled because i saw this elsewhere). i'd be happy with better error messages when we decide not to compact for user enabled compactions.  i'd also like to see some override that says \"user triggered major compaction always occurs\", but maybe that's a bad idea for other reasons. ",
        "label": 138
    },
    {
        "text": "orderedbytes isencodedvalue does not check for int8 and int16 types  orderedbytes.isencodedvalue does not check for int8 and int16 types. this also means that orderedbytes.length may return an incorrect result, since it calls orderedbytes.isencodedvalue. ",
        "label": 380
    },
    {
        "text": "add test for hbase compaction in out of date store causes region split failure  hbase-10370 fixes the issue where region split fails following compacting out-of-date store the new test failed in this build: https://builds.apache.org/job/hbase-0.98-on-hadoop-1.1/82/testreport/org.apache.hadoop.hbase.regionserver/testsplittransactiononcluster/testsplitfailedcompactionandsplit/ this issue is to make the new test, testsplitfailedcompactionandsplit, robust. ",
        "label": 411
    },
    {
        "text": "hfiledatablockencoderimpl uses wrong header size when reading hfiles with no checksums  when reading a .92 hfile without checksums, encoding it, and storing in the block cache, the hfiledatablockencoderimpl always allocates a dummy header appropriate for checksums even though there are none. this corrupts the byte[]. attaching a patch that allocates a dummy_header_no_checksum in that case which i think is the desired behavior. ",
        "label": 307
    },
    {
        "text": "precommit doesn't warn about unused imports  on hbase-18419 i accidentally included an unused import in my patch, but precommit didn't flag it. use this issue to diagnose and fix. ",
        "label": 98
    },
    {
        "text": "trunk tarball packaging  packaging needs work now we have maven multi-moduled. sounds like folks want a package for hadoop1 and hadoop2. also want source package and maybe even a package that includes test jars so can run integration tests. let this be umbrella issue for package fixes. will hang smaller issues off this one as we figure them. ",
        "label": 314
    },
    {
        "text": "add documentation for walplayer to hbase reference guide   ",
        "label": 286
    },
    {
        "text": "type support in importtsv tool  now the importtsv tool treats all the table column to be of type string. it converts the input data into bytes considering its type to be string. some times user will need a type of say int/float to get added to table by using this tool. ",
        "label": 543
    },
    {
        "text": "add documentation about the new async client  ",
        "label": 149
    },
    {
        "text": "regions are unexpectedly made offline in certain failure conditions  came across this issue (hbase-9338 test):  1. client issues a request to move a region from servera to serverb  2. servera is compacting that region and doesn't close region immediately. in fact, it takes a while to complete the request.  3. the master in the meantime, sends another close request.  4. servera sends it a notservingregionexception  5. master handles the exception, deletes the znode, and invokes regionoffline for the said region.  6. servera fails to operate on zk in the closeregionhandler since the node is deleted. the region is permanently offline. there are potentially other situations where when a regionserver is offline and the client asks for a region move off from that server, the master makes the region offline. ",
        "label": 242
    },
    {
        "text": " shell  truncates output  this is bad because it gives odd impression that an upload was not complete. one line fix in the formatter. ",
        "label": 314
    },
    {
        "text": "testzklessamoncluster testforceassignwhileclosing failed on jenkins  failed on branch-1. example failure: https://builds.apache.org/job/hbase-1.0/75/testreport/org.apache.hadoop.hbase.master/testzklessamoncluster/testforceassignwhileclosing/ ",
        "label": 242
    },
    {
        "text": "compaction needs little better skip algo  looking at this section of one of my compaction's we have 3 files to compact the new algo is working great in my test but i see this below often we are skipping 2 out of the 3 files and compacting 1 file. 1 file is kind of a wast might as well just copy the file my suggestion is if there is only 1 file left after the new algo skips then just go on to the next column and skip the last file also. this will help improve compaction times a little more. 2008-08-10 10:00:45,310 debug org.apache.hadoop.hbase.regionserver.hstore: compaction size of 1339600874/size: 4.6m, skipped 2, 4851776 2008-08-10 10:00:45,438 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 1 files into /hbase/webdata/compaction.dir/1339600874/size/mapfiles/8653208152776334891 2008-08-10 10:00:46,838 debug org.apache.hadoop.hbase.regionserver.hstore: moving /hbase/webdata/compaction.dir/1339600874/size/mapfiles/8653208152776334891 to /hbase/webdata/1339600874/size/mapfiles/7539342470259528578 2008-08-10 10:00:47,166 debug org.apache.hadoop.hbase.regionserver.hstore: completed compaction of 1339600874/size store size is 4.6m ",
        "label": 121
    },
    {
        "text": " ihbase  improve the expression classes by adding the ability to generate filters from them   one of the requirements of ihbase is that the a filter should be provided that at least matches the index expression hint. the ihbase expression classes could easily be used to generate a filter that can be used on the scan... for example: expression expression = expression     .or(         expression.comparison(columnname1, qualifer1, operator1, value1)     )     .or(         expression.and()             .and(expression.comparison(columnname2, qualifer2, operator2, value2))             .and(expression.comparison(columnname3, qualifer3, operator3, value3))     ); filter filter  = expression.tofilter(); ",
        "label": 314
    },
    {
        "text": "canary will always invoke admin balancer  in each sniffing period when writesniffing is enabled  when canary#writesniffing is enabled, canary#checkwritetabledistribution will make sure the regions of write table distributed on all region servers as:       int numberofservers = admin.getclusterstatus().getservers().size();       ......       int numberofcoveredservers = serverset.size();       if (numberofcoveredservers < numberofservers) {         admin.balancer();       } the master will also work as a regionserver, so that clusterstatus#getservers will contain the master. on the other hand, write table of canary will not be assigned to master, making numberofcoveredservers always smaller than numberofservers and admin.balancer always be invoked in each sniffing period. this may cause frequent region moves. a simple fix is excluding master from numberofservers. ",
        "label": 238
    },
    {
        "text": "upgrade jasckson databind to  due to this cve https://nvd.nist.gov/vuln/detail/cve-2019-14379 ",
        "label": 540
    },
    {
        "text": "hbase shell hangs when creating some 'illegal' tables   in hbase shell. these commands hang: create 'hbase.version','foo' create 'splitlog','foo' interestingly create 'hbase.id','foo' create existingtablename, 'foo' create '.meta.','foo' create '-root-','foo' are properly rejected. we should probably either rename to make the files illegal table names (hbase.version to .hbase.version and splitlog to .splitlog) or we could add more special cases. ",
        "label": 290
    },
    {
        "text": "backport hbase to branch  ",
        "label": 53
    },
    {
        "text": "data inserted into tables is not immidiately visible in the scanner with row filter  when data is inserted into region that is on the different machine than the master it is not immidiately available in the scanner that is using rowfilter  here is the output of the test case on 3 nodes (tmptable_0 and tmptable_2 were placed on the region servers that are not running master server).  --------------delaytest---------------  table tmptable_0 created  table tmptable_1 created  table tmptable_2 created  ***********************************  table tmptable_0 test start  inserting some sample data into random row id (aaaaa-2108369209)  inserted  testing if the new row is available through get method  confirming that data:test value = test of row aaaaa-2108369209 is available  testing if the new row is available through scanner with filter  ...................................  the new row has been found within 35 seconds  table tmptable_0 test finish  ***********************************  ***********************************  table tmptable_1 test start  inserting some sample data into random row id (aaaaa-20410017)  inserted  testing if the new row is available through get method  confirming that data:test value = test of row aaaaa-20410017 is available  testing if the new row is available through scanner with filter the new row has been found within 0 seconds  table tmptable_1 test finish  ***********************************  ***********************************  table tmptable_2 test start  inserting some sample data into random row id (aaaaa1756705479)  inserted  testing if the new row is available through get method  confirming that data:test value = test of row aaaaa1756705479 is available  testing if the new row is available through scanner with filter  ....................  the new row has been found within 20 seconds  table tmptable_2 test finish  ***********************************  --------------done--------------- ",
        "label": 314
    },
    {
        "text": "testthriftservercmdline testrunthriftserver  failed  https://builds.apache.org/job/hbase-trunk-on-hadoop-2.0.0/624/testreport/junit/org.apache.hadoop.hbase.thrift/testthriftservercmdline/testrunthriftserver_0_/ it seems stuck here: 2013-07-19 03:52:03,158 info  [thread-131] thrift.testthriftservercmdline(132): starting hbase thrift server with command line: -hsha -port 56708 start 2013-07-19 03:52:03,174 info  [thriftserver-cmdline] thrift.thriftserverrunner$impltype(208): using thrift server type hsha 2013-07-19 03:52:03,205 warn  [thriftserver-cmdline] conf.configuration(817): fs.default.name is deprecated. instead, use fs.defaultfs 2013-07-19 03:52:03,206 warn  [thriftserver-cmdline] conf.configuration(817): mapreduce.job.counters.limit is deprecated. instead, use mapreduce.job.counters.max 2013-07-19 03:52:03,207 warn  [thriftserver-cmdline] conf.configuration(817): io.bytes.per.checksum is deprecated. instead, use dfs.bytes-per-checksum 2013-07-19 03:54:03,156 info  [pool-1-thread-1] hbase.resourcechecker(171): after: thrift.testthriftservercmdline#testrunthriftserver[0] thread=146 (was 155), openfiledescriptor=295 (was 311), maxfiledescriptor=4096 (was 4096), systemloadaverage=293 (was 240) - systemloadaverage leak? -, processcount=145 (was 143) - processcount leak? -, availablememorymb=779 (was 1263), connectioncount=4 (was 4) 2013-07-19 03:54:03,157 debug [pool-1-thread-1] thrift.testthriftservercmdline(107): impltype=-hsha, specifyframed=false, specifybindip=false, specifycompact=true my guess is that we didn't get scheduled because load was almost 300 on this box at the time? let me up the timeout of two minutes. ",
        "label": 314
    },
    {
        "text": "at regionserver start there's a request to roll the wal  see hbase-22301 for logic that requests to roll the wal if regionserver encounters a slow write pipeline. in the logs, during regionserver start, i see that the wal is requested to roll once. it's strange that we roll the wal because it wasn't a slow sync. it appears when this code executes, we haven't initialized the rollonsyncns variable to use for determining whether it's a slow sync. current pipeline also shows empty in the logs. disclaimer: i'm experiencing this after backporting this to 1.3.x and building it there - i haven't attempted in 1.5.x, though i'd expect similar results. regionserver logs follow (notice threshold=0 ms, current pipeline: []): tue aug 20 23:29:50 gmt 2019 starting regionserver ... 2019-08-20 23:29:57,824 info  wal.fshlog - wal configuration: blocksize=256 mb, rollsize=243.20 mb, prefix=[truncated]%2c1566343792434, suffix=, logdir=hdfs://[truncated]/hbase/wals/[truncated],1566343792434, archivedir=hdfs://[truncated]/hbase/oldwals 2019-08-20 23:29:58,104 info  wal.fshlog - slow sync cost: 186 ms, current pipeline: [] 2019-08-20 23:29:58,104 warn  wal.fshlog - requesting log roll because we exceeded slow sync threshold; time=186 ms, threshold=0 ms, current pipeline: [] 2019-08-20 23:29:58,107 debug regionserver.replicationsourcemanager - start tracking logs for wal group [truncated]%2c1566343792434 for peer 1 2019-08-20 23:29:58,107 info  wal.fshlog - new wal /hbase/wals/[truncated],1566343792434/[truncated]%2c1566343792434.1566343797824 2019-08-20 23:29:58,109 debug regionserver.replicationsource - starting up worker for wal group [truncated]%2c1566343792434 ",
        "label": 394
    },
    {
        "text": "the implementation of asynctableregionlocator does not follow the javadoc  the reload parameter is just ignored. need to fix this before 2.0.0 release. ",
        "label": 149
    },
    {
        "text": "fix uncategorized tests  some tests are not categorized. they are not run if they are not categorized. i found the set of six or seven tests by running nkeywal's little ./dev-support/hbasetests.sh tool. this looks useful. ",
        "label": 314
    },
    {
        "text": "mesh replication  a k a  multi master replication   this is to setup nxn replication. see background discussion here:   http://mail-archives.apache.org/mod_mbox/hbase-user/201311.mbox/%3ccaoium-4ummla7uhmp4hhjplwurhdxg1t4tn4awvnzumctxg%2bkq%40mail.gmail.com%3e we can add a new mode in replication to not forward edits from other clusters. not sure what should be done when some clusters are configured with this setting and some aren't. ",
        "label": 213
    },
    {
        "text": "if no master  regionservers should hang out rather than fail on connection and shut themselves down  here is what currently happens if no master: 08/10/14 18:16:52 info metrics.rpcmetrics: initializing rpc metrics with hostname=hregionserver, port=60020 08/10/14 18:16:52 debug regionserver.hregionserver: telling master at xx.xx.xx.xx:60000 that we are up 08/10/14 18:16:53 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 0 time(s). 08/10/14 18:16:54 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 1 time(s). 08/10/14 18:16:55 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 2 time(s). 08/10/14 18:16:56 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 3 time(s). 08/10/14 18:16:57 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 4 time(s). 08/10/14 18:16:58 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 5 time(s). 08/10/14 18:16:59 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 6 time(s). 08/10/14 18:17:00 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 7 time(s). 08/10/14 18:17:01 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 8 time(s). 08/10/14 18:17:02 info ipc.client: retrying connect to server: xx.xx.xx.xx:60000. already tried 9 time(s). 08/10/14 18:17:02 fatal regionserver.hregionserver: unhandled exception. aborting... java.io.ioexception: call failed on local exception         at org.apache.hadoop.ipc.client.call(client.java:718)         at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:245)         at $proxy0.getprotocolversion(unknown source)         at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:388)         at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:364)         at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:412)         at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:328)         at org.apache.hadoop.hbase.regionserver.hregionserver.reportforduty(hregionserver.java:711)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:290)         at java.lang.thread.run(thread.java:674) caused by: java.net.connectexception: connection refused         at sun.nio.ch.socketchannelimpl.checkconnect(native method)         at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:592)         at sun.nio.ch.socketadaptor.connect(socketadaptor.java:118)         at org.apache.hadoop.ipc.client$connection.setupiostreams(client.java:300)         at org.apache.hadoop.ipc.client$connection.access$1700(client.java:177)         at org.apache.hadoop.ipc.client.getconnection(client.java:789)         at org.apache.hadoop.ipc.client.call(client.java:704)         ... 9 more 08/10/14 18:17:02 debug hbase.regionhistorian: offlined 08/10/14 18:17:02 info ipc.server: stopping server on 60020 08/10/14 18:17:02 info regionserver.hregionserver: aborting server at: 0.0.0.0:60020 08/10/14 18:17:02 info regionserver.hregionserver: regionserver/0:0:0:0:0:0:0:0:60020 exiting 08/10/14 18:17:02 info regionserver.hregionserver: starting shutdown thread. 08/10/14 18:17:02 info regionserver.hregionserver: shutdown thread complete making it a blocker. boys at pset need this. ",
        "label": 241
    },
    {
        "text": "documentation review  it would be good to go over the documentation once and update missing parts and clean up for 1.0. ",
        "label": 330
    },
    {
        "text": " hbck  offline rebuild  meta  from fs data only   in a worst case situation, it may be helpful to have an offline .meta. rebuilder that just looks at the file system's .regioninfos and rebuilds meta from scratch. users could move bad regions out until there is a clean rebuild. it would likely fill in region split holes. follow on work could given options to merge or select regions that overlap, or do online rebuilds. ",
        "label": 248
    },
    {
        "text": "userpermission should be annotated as interfaceaudience public  hbase-11318 mark userpermission as interfaceaudience.private.  hbase-11452 instroduce accesscontrolclient#getuserpermissions and return userpermission list but the userpermission class is private. i also encounter the same problem when i want to move getuserpermissions method as a admin api in hbase-21911, otherwise the api of getuserpermissions may be  map<string, list<permission>> getuserpermissions so shall we mark the userpermission as public? discussions are welcomed. ",
        "label": 500
    },
    {
        "text": "alert when heap is over committed  something i just witnessed, the block cache setting was at 70% but the max global memstore size was at the default of 40% meaning that 110% of the heap can potentially be \"assigned\" and then you need more heap to do stuff like flushing and compacting. we should run a configuration check that alerts the user when that happens and maybe even refuse to start. ",
        "label": 428
    },
    {
        "text": "metaserver crash cause all splitting regionserver abort  if metaserver crash now,  all the splitting regionserver will abort theirself.  becasue the code this.journal.add(journalentry.ponr); metaeditor.offlineparentinmeta(server.getcatalogtracker(),             this.parent.getregioninfo(), a.getregioninfo(), b.getregioninfo()); if the journalentry is ponr, split's roll back will abort itselef. it is terrible in huge putting environment when metaserver crash ",
        "label": 107
    },
    {
        "text": "under continuous upload of rows  wrongregionexceptions are thrown that reach the client even after retries  i have installed 0.16.0 rc 1 which i believe contains a fix for similar issue hbase-138, but i still see the same problem. i am using a single node. the client application runs in a single thread, loading data into a single table. i get good throughput of about 200 rows/sec to start with, with occasional significant drops due to notservingregionexception's that are recoverable on client retry (internal to hbase). after 54 minutes, and about 500,000 rows i start to see wrongregionexception's in the client application, i.e. real failures. (note that this compares to 0.15.3 which would being to throw notservingregionexceptions after a few tens of thousands of rows). my data consists of a single table with 5 column families. the data written is as follows:>>  key: a url  family 1: a small string, often emty, 2 longs, 1 int  family 2: a byte averaging averaging between 1k and 10k, a small string  family 3: several columns with different names per row, values of small strings  family 4: most rows have zero columns, some rows have 1 or more columns with a ul value  the urls are typically \"long-ish\" url as seen when crawling a site, not short home page urls i am assuming the data is stored in files of the form <hbaseroot>//<tablename>/<9digitnum>/data/mapfiles/<19digitnum>/data. i have attached a csv file showing the distribution of size of these files. average size is 19mb, but the sizes are not evenly distributed at all here are two sample exceptions thrown, copied from the region server log: 2008-02-08 02:08:22,495 info org.apache.hadoop.ipc.server: ipc server handler 4 on 60020, call batchupdate(pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924,1202401088077, 9223372036854775807, org.apache.hadoop.hbase.io.batchupdate@feb215) from 66.135.42.137:38484: error: org.apache.hadoop.hbase.wrongregionexception: requested row out of range for hregion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924,1202401088077, startkey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', getendkey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', row='http://go2purdue.com/redeemer_university.cfm?pt=2&sp=2&vid=1199243289_3x02x1468757255&rpt=2&kt=4&kp=1 wap2 20080102081237'  org.apache.hadoop.hbase.wrongregionexception: requested row out of range for hregion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924,1202401088077, startkey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', getendkey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', row='http://go2purdue.com/redeemer_university.cfm?pt=2&sp=2&vid=1199243289_3x02x1468757255&rpt=2&kt=4&kp=1 wap2 20080102081237'  at org.apache.hadoop.hbase.hregion.checkrow(hregion.java:1486)  at org.apache.hadoop.hbase.hregion.obtainrowlock(hregion.java:1531)  at org.apache.hadoop.hbase.hregion.batchupdate(hregion.java:1226)  at org.apache.hadoop.hbase.hregionserver.batchupdate(hregionserver.java:1433)  at sun.reflect.generatedmethodaccessor10.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:910)  2008-02-08 02:08:22,696 info org.apache.hadoop.ipc.server: ipc server handler 6 on 60020, call batchupdate(pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924,1202401088077, 9223372036854775807, org.apache.hadoop.hbase.io.batchupdate@15d9be1) from 66.135.42.137:38484: error: org.apache.hadoop.hbase.wrongregionexception: requested row out of range for hregion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924,1202401088077, startkey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', getendkey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', row='http://go2umass.com/travel.cfm?pt=2&sp=2&vid=1199230721_3x04x1485302803&rpt=2&kt=5&kp=8 wap2 20080102081239'  org.apache.hadoop.hbase.wrongregionexception: requested row out of range for hregion pagefetch,http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924,1202401088077, startkey='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', getendkey()='http://galsn1.mobilook.mobiwap.com/bm/listproducts;jsessionid=d2ed1eb898163cdb27135dc2cf6958b3.197b?rsi=78011 wap2 20080102052924', row='http://go2umass.com/travel.cfm?pt=2&sp=2&vid=1199230721_3x04x1485302803&rpt=2&kt=5&kp=8 wap2 20080102081239'  at org.apache.hadoop.hbase.hregion.checkrow(hregion.java:1486)  at org.apache.hadoop.hbase.hregion.obtainrowlock(hregion.java:1531)  at org.apache.hadoop.hbase.hregion.batchupdate(hregion.java:1226)  at org.apache.hadoop.hbase.hregionserver.batchupdate(hregionserver.java:1433)  at sun.reflect.generatedmethodaccessor10.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:910) ",
        "label": 314
    },
    {
        "text": " hbase thirdparty  change the relocation offset of shaded artifacts  on the dev@hbase list, we conclude that we need to change the relocation offset in hbase-thirdparty to avoid shading conflicts with the other hbase shaded components (hbase-shaded-client and hbase-shaded-mapreduce components).  https://lists.apache.org/thread.html/1aa5d1d7f6d176df49e72096926b011cafe1315932515346d06e8342@%3cdev.hbase.apache.org%3e  the suggestion is to use \"o.a.h.hbase.thirdparty\" in hbase-thirdparty to differentiate between \"shaded\" for downstream of us vs \"thirdparty\" for our internal use. ",
        "label": 320
    },
    {
        "text": "code  hrs delete seems to ignore exceptions it shouldnt  the code is:  region.delete(delete, lid, writetowal);  this.hlog.sync(region.getregioninfo().ismetaregion());  } catch (wrongregionexception ex) {  } catch (notservingregionexception ex) { // ignore } catch (throwable t) { throw convertthrowabletoioe(cleanup(t)); } we ignore those 2 exceptions... weird... should not be! ",
        "label": 229
    },
    {
        "text": "shade protobuf  we need to change our protobuf. currently it is pb2.5.0. as is, protobufs expect all buffers to be on-heap byte arrays. it does not have facility for dealing in bytebuffers and off-heap bytebuffers in particular. this fact frustrates the off-heaping-of-the-write-path project as marshalling/unmarshalling of protobufs involves a copy on-heap first. so, we need to patch our protobuf so it supports off-heap bytebuffers. to ensure we pick up the patched protobuf always, we need to relocate/shade our protobuf and adjust all protobuf references accordingly. given as we have protobufs in our public facing api, coprocessor endpoints \u2013 which use protobuf service to describe new api \u2013 a blind relocation/shading of com.google.protobuf.* will break our api for coprocessor endpoints (cpep) in particular. for example, in the table interface, to invoke a method on a registered cpep, we have: <t extends com.google.protobuf.service,r> map<byte[],r> coprocessorservice( class<t> service, byte[] startkey, byte[] endkey,                                             org.apache.hadoop.hbase.client.coprocessor.batch.call<t,r> callable) throws com.google.protobuf.serviceexception, throwable this issue is how we intend to shade protobuf for hbase-2.0.0 while preserving our api as is so cpeps continue to work on the new hbase. ",
        "label": 314
    },
    {
        "text": "backport hbase  add per user metrics  to branch  we will need hbase-15519 in branch-1 for eventual backport of hbase-23065. ",
        "label": 38
    },
    {
        "text": "hbck should be able to fix case where region is missing from meta but on fs  ",
        "label": 248
    },
    {
        "text": "replayed edits from regions that failed to open during recovery aren't removed from the global memstore size  this bug is rather easy to get if the timeoutmonitor is on, else i think it's still possible to hit it if a region fails to open for more obscure reasons like hdfs errors. consider a region that just went through distributed splitting and that's now being opened by a new rs. the first thing it does is to read the recovery files and put the edits in the memstores. if this process takes a long time, the master will move that region away. at that point the edits are still accounted for in the global memstore size but they are dropped when the hregion gets cleaned up. it's completely invisible until the memstoreflusher needs to force flush a region and that none of them have edits: 2012-03-21 00:33:39,303 debug org.apache.hadoop.hbase.regionserver.memstoreflusher: flush thread woke up because memory above low water=5.9g 2012-03-21 00:33:39,303 error org.apache.hadoop.hbase.regionserver.memstoreflusher: cache flusher failed for entry null java.lang.illegalstateexception         at com.google.common.base.preconditions.checkstate(preconditions.java:129)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushoneforglobalpressure(memstoreflusher.java:199)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.run(memstoreflusher.java:223)         at java.lang.thread.run(thread.java:662) the null here is a region. in my case i had so many edits in the memstore during recovery that i'm over the low barrier although in fact i'm at 0. it happened yesterday and it still printing this out. to fix this we need to be able to decrease the global memstore size when the region can't open. ",
        "label": 240
    },
    {
        "text": "log an optional's value null instead of optional value optional empty   after hbase-18878, the audit log contains redundant characters \"optional\" which leading to unreadable 2017-10-19 19:51:08,054 info  [rpcserver.default.fpbq.fifo.handler=49,queue=4,port=8081] master.hmaster: client=optional[username]/optional[/locahost] disable tablename  ",
        "label": 188
    },
    {
        "text": "testrskilledwheninitializing failing on branch  branch   running org.apache.hadoop.hbase.regionserver.testrskilledwheninitializing  tests run: 1, failures: 1, errors: 0, skipped: 0, time elapsed: 12.343 sec <<< failure! - in org.apache.hadoop.hbase.regionserver.testrskilledwheninitiali  zing  testrsterminationafterregisteringtomasterbeforecreatingephemeralnode(org.apache.hadoop.hbase.regionserver.testrskilledwheninitializing) time elapsed: 12.  329 sec <<< failure!  java.lang.assertionerror: null  at org.junit.assert.fail(assert.java:86)  at org.junit.assert.asserttrue(assert.java:41)  at org.junit.assert.asserttrue(assert.java:52)  at org.apache.hadoop.hbase.regionserver.testrskilledwheninitializing.testrsterminationafterregisteringtomasterbeforecreatingephemeralnode(testrski  lledwheninitializing.java:123) tests run: 2, failures: 0, errors: 2, skipped: 0, time elapsed: 192.427 sec <<< failure! - in org.apache.hadoop.hbase.regionserver.testrskilledwheninitial  izing  testrsterminationafterregisteringtomasterbeforecreatingephemeralnode(org.apache.hadoop.hbase.regionserver.testrskilledwheninitializing) time elapsed: 179  .859 sec <<< error!  org.junit.runners.model.testtimedoutexception: test timed out after 180 seconds  at java.lang.thread.sleep(native method)  at org.apache.hadoop.hbase.util.threads.sleep(threads.java:146)  at org.apache.hadoop.hbase.regionserver.testrskilledwheninitializing.testrsterminationafterregisteringtomasterbeforecreatingephemeralnode(testrski  lledwheninitializing.java:113) ",
        "label": 38
    },
    {
        "text": "figure how to deal with eof splitting logs  when splitting the wal and encountering eof, it's not clear what to do. initial discussion of this started in http://review.hbase.org/r/74/ - summarizing here for brevity: we can get an eofexception while splitting the wal in the following cases: the writer died after creating the file but before even writing the header (or crashed halfway through writing the header) the writer died in the middle of flushing some data - sync() guarantees that we can see at least the last edit, but we may see half of an edit that was being written out when the rs crashed (especially for large rows) the data was actually corrupted somehow (eg a length field got changed to be too long and thus points past eof) ideally we would know when we see eof whether it was really the last record, and in that case, simply drop that record (it wasn't synced, so therefore we dont need to split it). some open questions: currently we ignore empty files. is it ok to ignore an empty log file if it's not the last one? similarly, do we ignore an eof mid-record if it's not the last log file? ",
        "label": 341
    },
    {
        "text": "remove error prone from our precommit javac check  as the result is not stable. can add it back as a separated check later. ",
        "label": 149
    },
    {
        "text": "implement getclusterstatus getregionload getcompactionstate getlastmajorcompactiontimestamp methods  ",
        "label": 187
    },
    {
        "text": "endless recursive of deletenode happened in splitlogmanager deleteasynccallback  you can ealily understand the problem from the below logs: [2012-09-01 11:41:02,062] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$createasynccallback 978] create rc =sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=3 [2012-09-01 11:41:02,062] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$createasynccallback 978] create rc =sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=2 [2012-09-01 11:41:02,063] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$createasynccallback 978] create rc =sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=1 [2012-09-01 11:41:02,063] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$createasynccallback 978] create rc =sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=0 [2012-09-01 11:41:02,063] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager 393] failed to create task node/hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 [2012-09-01 11:41:02,063] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager 353] error splitting /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 [2012-09-01 11:41:02,063] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$deleteasynccallback 1052] delete rc=sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=9223372036854775807 [2012-09-01 11:41:02,064] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$deleteasynccallback 1052] delete rc=sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=9223372036854775806 [2012-09-01 11:41:02,064] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$deleteasynccallback 1052] delete rc=sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=9223372036854775805 [2012-09-01 11:41:02,064] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$deleteasynccallback 1052] delete rc=sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=9223372036854775804 [2012-09-01 11:41:02,065] [warn ] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.splitlogmanager$deleteasynccallback 1052] delete rc=sessionexpired for /hbase/splitlog/hdfs%3a%2f%2fxh01%3a9000%2fhbase%2f.logs%2fxh01%2c20020%2c1339552105088-splitting%2fxh01%252c20020%252c1339552105088.1339557014846 remaining retries=9223372036854775803 ................... [2012-09-01 11:41:03,307] [error] [master_server_operations-xh03,20000,1339549619270-1] [org.apache.zookeeper.clientcnxn 623] caught unexpected throwable java.lang.stackoverflowerror ",
        "label": 233
    },
    {
        "text": "the logic used in waiting for region servers during startup is broken  see the tail of hbase-4993, which i'll report here: me: i think a bug was introduced here. here's the new waiting logic in waitforregionservers: the 'hbase.master.wait.on.regionservers.mintostart' is reached and  there have been no new region server in for  'hbase.master.wait.on.regionservers.interval' time and the code that verifies that: !(lastcountchange+interval > now && count >= mintostart) nic: it seems that changing the code to (count < mintostart ||  lastcountchange+interval > now) would make the code works as documented.  if you have 0 region servers that checked in and you are under the interval, you wait: (true or true) = true.  if you have 0 region servers but you are above the interval, you wait: (true or false) = true.  if you have 1 or more region servers that checked in and you are under the interval, you wait: (false or true) = true. ",
        "label": 229
    },
    {
        "text": " ergonomics  hbase client scanner caching is dogged and will try to return batch even if it means oome  running some tests, i set hbase.client.scanner.caching=1000. dataset has large cells. i kept oome'ing. serverside, we should measure how much we've accumulated and return to the client whatever we've gathered once we pass out a certain size threshold rather than keep accumulating till we oome. ",
        "label": 249
    },
    {
        "text": "checkandput doesn't work when value is empty byte   when a value contains an empty byte[] and then a checkandput is performed with an empty byte[] , the operation will fail. for example:  put put = new put(row1);  put.add(fam1, qf1, new byte[0]);  table.put(put); put = new put(row1);  put.add(fam1, qf1, val1);  table.checkandput(row1, fam1, qf1, new byte[0], put); ---> false i think this is related to hbase-3793 and hbase-3468. note that you will also get into this situation when first putting a null value ( put.add(fam1,qf1,null) ), as this value will then be regarded and returned as an empty byte[] upon a get. ",
        "label": 166
    },
    {
        "text": "test patch sh should accept documents by known file extensions  currently only htm[l] files are filtered out when test-patch.sh looks for patch attachment. in the email thread, 'extensions for patches accepted by qa bot', consensus was to accept the following file extensions only:  .patch  .txt  .diff ",
        "label": 441
    },
    {
        "text": "got zookeeper event  state  disconnected on hrs and then npe on reinit  we got disconnect from zk but then when we tried to reinitialize ourselves, got a npe. see below. 2009-06-17 11:58:55,102 [thread-16] info org.apache.hadoop.hbase.regionserver.hregionserver: starting shutdown thread.  2009-06-17 11:58:55,102 [thread-16] info org.apache.hadoop.hbase.regionserver.hregionserver: shutdown thread complete 2009-06-17 11:58:55,102 [main-eventthread] info org.apache.hadoop.hbase.ipc.hbaserpcmetrics: initializing rpc metrics with hostname=hregionserver, port=60021 2009-06-17 11:58:55,103 [main-eventthread] info org.apache.hadoop.hbase.regionserver.memcacheflusher: globalmemcachelimit=556.7m, globalmemcachelimitlowmark=347.9m, maxheap=1.4g 2009-06-17 11:58:55,103 [main-eventthread] info org.apache.hadoop.hbase.regionserver.hregionserver: runs every 10000000ms 2009-06-17 11:58:55,148 [regionserver/0:0:0:0:0:0:0:0:60021] error org.apache.hadoop.hbase.regionserver.hregionserver: failed init java.lang.nullpointerexception     at org.apache.hadoop.hbase.regionserver.hregionserver.init(hregionserver.java:713)     at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:431)     at java.lang.thread.run(thread.java:619) 2009-06-17 11:58:55,153 [regionserver/0:0:0:0:0:0:0:0:60021] fatal org.apache.hadoop.hbase.regionserver.hregionserver: unhandled exception. aborting... java.io.ioexception: region server startup failed     at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:832)     at org.apache.hadoop.hbase.regionserver.hregionserver.init(hregionserver.java:751)     at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:431)     at java.lang.thread.run(thread.java:619) caused by: java.lang.nullpointerexception     at org.apache.hadoop.hbase.regionserver.hregionserver.init(hregionserver.java:713)     ... 2 more    ",
        "label": 342
    },
    {
        "text": "accesscontrolexception should be a not retriable exception  rpc server does not handle the accesscontrolexception thrown by authorizeconnection failure properly and in return sends ioexception to the hbase client.   ultimately the client does retries and gets retriesexhaustedexception but does not getting any link or information or stack trace about accesscontrolexception.  in short summary, upon inspection of rpcserver.java, it seems for the listener, the reader read code as below does not handle accesscontrolexception void doread(\u2026. \u2026.. \u2026..               try {         count = c.readandprocess(); // this readandprocess method throws accesscontrolexception from processonerpc(byte[] buf) which is not handled ?       } catch (interruptedexception ieo) {         throw ieo;       } catch (exception e) {         log.warn(getname() + \": count of bytes read: \" + count, e);         count = -1; //so that the (count < 0) block is executed       } below is the client logs if authorizeconnection throws accesscontrolexception: 2014-07-24 19:40:58,768 info [main] client.hconnectionmanager$hconnectionimplementation: getmaster attempt 7 of 7 failed; no more retrying.  com.google.protobuf.serviceexception: java.io.ioexception: call to host-10-18-40-101/10.18.40.101:60000 failed on local exception: java.io.eofexception  at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1674)  at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1715)  at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$blockingstub.ismasterrunning(masterprotos.java:42561)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$masterservicestubmaker.ismasterrunning(hconnectionmanager.java:1688)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$stubmaker.makestubnoretries(hconnectionmanager.java:1597)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$stubmaker.makestub(hconnectionmanager.java:1623)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$masterservicestubmaker.makestub(hconnectionmanager.java:1677)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getkeepalivemasterservice(hconnectionmanager.java:1885)  [...]  caused by: java.io.ioexception: call to host-10-18-40-101/10.18.40.101:60000 failed on local exception: java.io.eofexception  at org.apache.hadoop.hbase.ipc.rpcclient.wrapexception(rpcclient.java:1485)  at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1457)  at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1657)  ... 254 more  caused by: java.io.eofexception  at java.io.datainputstream.readint(datainputstream.java:375)  at org.apache.hadoop.hbase.ipc.rpcclient$connection.readresponse(rpcclient.java:1072)  at org.apache.hadoop.hbase.ipc.rpcclient$connection.run(rpcclient.java:728) ",
        "label": 361
    },
    {
        "text": "added support for clearing blockcache based on table name  bulk loading the primary hbase cluster triggers a lot of compactions resulting in archival/ creation  of multiple hfiles. this process will cause a lot of items to become stale in replica\u2019s blockcache.  this patch will help users to clear the block cache for a given table by either using shell or api. ",
        "label": 511
    },
    {
        "text": "scan different timerange for each column family  at present the scan api supports only table level time range. we have specific use cases that will benefit from per column family time range. (see background discussion at https://mail-archives.apache.org/mod_mbox/hbase-user/201508.mbox/%3ccaa4mzom00ef5eoxstk0hetxeby8mqss61gbvgttgpaspmhqhaw@mail.gmail.com%3e) there are a couple of choices that would be good to validate. first - how to update the scan api to support family and table level updates. one proposal would be to add scan.settimerange(byte family, long mintime, long maxtime), then store it in a map<byte[], timerange>. when executing the scan, if a family has a specified timerange, then use it, otherwise fall back to using the table level timerange. clients using the new api against old region servers would not get the families correctly filterd. old clients sending scans to new region servers would work correctly. the other question is how to get storefilescanner.shouldusescanner to match up the proper family and time range. it has the scan available but doesn't currently have available which family it is a part of. one option would be to try to pass down the column family in each constructor path. another would be to instead alter shouldusescanner to pass down the specific timerange to use (similar to how it currently passes down the columns to use which also appears to be a workaround for not having the family available). ",
        "label": 522
    },
    {
        "text": "slabstats should be a daemon thread  i had a hanging jvm on shutdown caused by: \"slab statistics #0\" prio=5 tid=7fc0238bc800 nid=0x10dadf000 waiting on condition [10dade000]    java.lang.thread.state: timed_waiting (parking) ",
        "label": 286
    },
    {
        "text": "export does one version only  make it configurable how many it does  i was playing with export to backup all of a table. a small change on the scan saying save all versions was all it took to back up all versions. as is the import did the right thing. wouldn't take much to add versions and/or timerange as little config. on the export mr job. ",
        "label": 314
    },
    {
        "text": "the introspection  etc  of objects in the rpc has to be handled for pb objects  the places where the type of objects are inspected need to be updated to take into consideration pb types. i have noticed objects.describequantity being used, and the private writablerpcengine.server.logresponse method also needs updating (in the pb world, all information about operations/tablenames is contained in one pb argument). ",
        "label": 314
    },
    {
        "text": "revert the package name change for tableexistsexception  i was going through the code changes that were needed for getting an application that was running with hbase-0.92 run with hbase-0.95. tableexistsexception's package has changed - hence, needs a code change in the application. offline discussion with some folks led us to believe that this change can probably be reverted back. ",
        "label": 139
    },
    {
        "text": "snapshot attempt with the name of a previously taken snapshots fails sometimes   in a test rig, we repeatedly snapshot, clone and delete a table with the same using the same set of snapshot names. sometimes, the snapshot request will be rejected until the hmaster is restarted. ",
        "label": 248
    },
    {
        "text": "fix javadoc for put list put  puts  in htableinterface  we say this in the interface:   /**    * puts some data in the table, in batch.    * <p>    * if {@link #isautoflush isautoflush} is false, the update is buffered    * until the internal buffer is full.    * @param puts the list of mutations to apply.  the list gets modified by this    * method (in particular it gets re-ordered, so the order in which the elements    * are inserted in the list gives no guarantee as to the order in which the    * {@link put}s are executed).    * @throws ioexception if a remote or network exception occurs. in that case    * the {@code puts} argument will contain the {@link put} instances that    * have not be successfully applied.    * @since 0.20.0    */   void put(list<put> puts) throws ioexception; this is outdated and needs to be updated to reflect that this is nothing else but a client side iteration over all puts, but using the write buffer to aggregate to one rpc. the list is never modified and after the call contains the same number of elements. ",
        "label": 194
    },
    {
        "text": " snapshot  list and delete snapshot by table  support list and delete snapshots by table names.  user scenario:  a user wants to delete all the snapshots which were taken in january month for a table 't' where snapshot names starts with 'jan'. ",
        "label": 53
    },
    {
        "text": "orphan rpc connection in hbaseclient leaves  null  out member  causing npe in hcm  just like: https://issues.apache.org/jira/browse/hadoop-7428  exceptions except ioexception thrown in setupiostreams would leave the connection half-setup. but the connection would not close utill it become timeout. the orphane connection cause npe when is used in hcm. ",
        "label": 516
    },
    {
        "text": "fix client next int  javadoc  its not clear what signifies scanner end and noobs probably think that batch size is how much we fetch in an rpc (thats different, thats scan#setcaching). ",
        "label": 314
    },
    {
        "text": "handler code for missing 'data' files failing  renaud delbri up on the list and jon gray just now ran into issue where getting length of an hsf inside in the hstore constructor failed with fnfe. test of length is done after supposed tests that file exists so odd. ",
        "label": 314
    },
    {
        "text": "fsutils getrootdir should qualify path  currently you can run into a stackoverflowerror if the hbase root dir is on the non-default filesystem. making fsutils.getrootdir qualify its path solves this issue. ",
        "label": 453
    },
    {
        "text": "optional setbatch for copytable to copy large rows in batches  we've had copytable jobs fail because a small number of rows are wide enough to not fit into memory. if we could specify the batch size for copytable scans that shoud be able to break those large rows up into multiple iterations to save the heap. ",
        "label": 339
    },
    {
        "text": "loadbalancer needsbalance  should check for co located region replicas as well  this is a left over todo from reviews of hbase-10351.   lb.needsbalance() does some basic checking before running the lb.balance() method. we need to check whether there are co-located regions in this method so that the balancer can increase availability. ",
        "label": 139
    },
    {
        "text": "suspect methods on cell to be deprecated  chia-ping tsai suggested on the mailing list that we have some methods on cell which should be deprecated for removal: #gettype() #gettimestamp() #gettag() #getsequenceid() let's make a pass over these (and maybe the rest) to make sure that there aren't others which are either implementation details or methods returning now-private-marked classes. ",
        "label": 544
    },
    {
        "text": "replicationpeer logs at warn level aborting server instead of at fatal  ",
        "label": 290
    },
    {
        "text": "ineffective handling of filenotfoundexception in filelink filelinkinputstream tryopen   found the following in region server log: 2017-07-03 11:22:04,669 warn  [regionserver/a.b.c.d:16020-shortcompactions-1499094046361] retry.retryinvocationhandler: exception while       invoking clientnamenodeprotocoltranslatorpb.getblocklocations over e.f.g.h:8020. not retrying because try once and fail. org.apache.hadoop.ipc.remoteexception(java.io.filenotfoundexception): file does not exist: /hbase/data/default/x/4d61af9d1cbcc5fe2a5cbddbfc92fe7e/ k/47222a9cbd294f499f49de92ecf330ee   at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:71)   at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:61) ...   at org.apache.hadoop.hbase.io.filelink$filelinkinputstream.tryopen(filelink.java:291)   at org.apache.hadoop.hbase.io.filelink$filelinkinputstream.<init>(filelink.java:122)   at org.apache.hadoop.hbase.io.filelink$filelinkinputstream.<init>(filelink.java:113)   at org.apache.hadoop.hbase.io.filelink.open(filelink.java:404)   at org.apache.hadoop.hbase.io.fsdatainputstreamwrapper.<init>(fsdatainputstreamwrapper.java:98)   at org.apache.hadoop.hbase.io.fsdatainputstreamwrapper.<init>(fsdatainputstreamwrapper.java:83) here is related code:     private fsdatainputstream tryopen() throws ioexception {       for (path path: filelink.getlocations()) { ...         } catch (filenotfoundexception e) {           // try another file location         } the intention is to try possible locations for the linked file.  however, remoteexception was the exception encountered. this makes the above catch clause ineffective. ",
        "label": 441
    },
    {
        "text": "flushcache should write to a tmp directory and then move into the store directory  currently it appears that internalflushcache writes directly to the target spot of the flushed data. the finally() block appends the metadata and closes the file as if nothing bad went wrong in case of an exception. this is really bad, since it means that an ioe in the middle of flushing cache could easily write a valid looking file with only half the data, which would then prevent us from recovering those edits during log replay. instead, it should flush to a tmp location and move it into the region dir only after it's successfully written. ",
        "label": 314
    },
    {
        "text": "some doc and cleanup in rpcserver  rpc is a dog to follow. i want to do buffer pooling for reading requests but its tough drawing the diagram of who is doing what when. hbase-8884 seems to have made it more involved still. this issue is about doing a bit of untangling. ",
        "label": 314
    },
    {
        "text": "nosuchcolumnfamilyexception in multi doesn't say which family is bad  it's kind of a dumb one, in hregion.dominibatchput we do: log.warn(\"no such column family in batch put\", nscf); batchop.retcodes[lastindexexclusive] = operationstatuscode.bad_family; so we lose the family here, all we know is there's a bad one, that's what's in hrs.multi: } else if (code == operationstatuscode.bad_family) {   result = new nosuchcolumnfamilyexception(); we can't just throw the exception like that, we need to say which one is bad even if it requires testing all passed multiactions. ",
        "label": 544
    },
    {
        "text": "hbase shell has an issue accepting filter for the 'scan' command   stack had encountered+fixed an issue nearly a couple of years ago related to filter not being accepted by the 'scan' command in the shell. this, however, didn't make it to the trunk i believe. i hit it today while revamping some docs for hbase-3539 the thread where stack had posted a patch: http://mail-archives.apache.org/mod_mbox/hbase-user/200912.mbox/%3c7c962aed0912181049l2f9110c3q43b1e3d897a2768e@mail.gmail.com%3e ",
        "label": 194
    },
    {
        "text": "integrationtestrsgroup is useless now  it extends testrsgroupsbase and wants to run all the uts defined in testrsgroupsbase, but after hbase-21265, all the uts have been moved to sub classes... ",
        "label": 394
    },
    {
        "text": "hbase shell still using deprecated methods removed in hbase  the patch applied in hbase-4622 (subtask of hbase-4436) to remove deprecated methods seems to have missed some usage of those methods by the hbase shell. at least src/main/ruby/hbase/admin.rb is still using some of the removed methods, breaking some shell commands: hbase(main):007:0> alter 'privatetable', { name => 'f1', versions => 2} error: wrong number of arguments (3 for 2) backtrace: /usr/lib/hbase/bin/../bin/../lib/ruby/hbase/admin.rb:344:in `alter'            org/jruby/rubyarray.java:1572:in `each'            /usr/lib/hbase/bin/../bin/../lib/ruby/hbase/admin.rb:317:in `alter'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands/alter.rb:79:in `command'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:68:in `format_simple_command'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands/alter.rb:78:in `command'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:31:in `command_safe'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:74:in `translate_hbase_exceptions'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:31:in `command_safe'            /usr/lib/hbase/bin/../bin/../lib/ruby/shell.rb:110:in `command'            (eval):2:in `alter' this trace translates to the line:   @admin.modifycolumn(table_name, column_name, descriptor) which is calling one of the removed methods. ",
        "label": 180
    },
    {
        "text": "the scanner order for memstore scanners are wrong  this is comments for keyvaluescanner.getscannerorder keyvaluescanner.java   /**    * get the order of this keyvaluescanner. this is only relevant for storefilescanners and    * memstorescanners (other scanners simply return 0). this is required for comparing multiple    * files to find out which one has the latest data. storefilescanners are ordered from 0    * (oldest) to newest in increasing order. memstorescanner gets long.max since it always    * contains freshest data.    */   long getscannerorder(); as now we may have multiple memstore scanners, i think the right way to select scanner order for memstore scanner is to ordered from long.max_value in decreasing order. but in compactingmemstore and defaultmemstore, the scanner order for memstore scanner is also start from 0, which will be messed up with storefilescanners. ",
        "label": 244
    },
    {
        "text": "task tabs on master ui cause page scroll  on the master info page, the clicking the tabs under tasks causes the page to scroll back to the top of the page. tasks show all monitored tasks show non-rpc tasks show all rpc handler tasks show active rpc calls show client operations view as json ^^ any of those the other tab-like links on the page keep the scroll in the same location. ",
        "label": 389
    },
    {
        "text": "control where to put meta region  there is a concern on placing meta regions on the master, as in the comments of hbase-10569. i was thinking we should have a configuration for a load balancer to decide where to put it. adjusting this configuration we can control whether to put the meta on master, or other region server. ",
        "label": 242
    },
    {
        "text": "set version as snapshot in branch  ",
        "label": 149
    },
    {
        "text": "add special snapshotscanner which presents view of all data at some time in the past  in order to support a particular kind of database \"snapshot\" feature which doesn't require copying data, we came up with the idea for a special snapshotscanner that would present a view of your data at some point in the past. the primary use case for this would be to be able to recover particular data/rows (but not all data, like a global rollback) should they have somehow been messed up (application fault, application bug, user error, etc.). ",
        "label": 359
    },
    {
        "text": "in 'hbck report'  distinguish between live  dead  and unknown servers  debugging, when viewing 'hbck report' sections, it helps if we know if referenced server is online, dead, or unknown. add ornamentation so that when we mention a servername in 'hbck report', if live, then show the server as link (to live server), if dead, show it in italics, and if unknown, show it plain text. ",
        "label": 314
    },
    {
        "text": "update htrace to  htrace is not perfectly integrated into hbase, the version 3.2.0 is buggy, the upgrade to 4.x is not trivial and would take time. it might not worth to keep it in this state, so would be better to remove it. of course it doesn't mean tracing would be useless, just that in this form the use of htrace 3.2 might not add any value to the project and fixing it would be far too much effort.  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  based on the decision of the community we keep htrace now and update version ",
        "label": 60
    },
    {
        "text": "make it clear that hbase requires hadoop at least  we will no longer work on older versions  looks like there is pretty much consensus that depending on 1.0.0 in 0.96 should be fine? see http://search-hadoop.com/m/dsbvw14esub2/discuss+0.96&subj=re+discuss+have+hbase+require+at+least+hadoop+1+0+0+in+hbase+0+96+0+ ",
        "label": 314
    },
    {
        "text": "fix environment variables typos in scripts  i noticed in the top of many of the scripts in bin/ that the old environment variables are documented, such as hadoop_ssh_opts options passed to ssh when running remote commands. however, in the script code, hbase_ssh_opts is clearly used. i've attached a trivial script to fix this in many locations. ",
        "label": 13
    },
    {
        "text": "htrace synchronized on getinstance  when doing tests on cached data, one of the bottleneck is the getinstance() on htrace, called in requestcontext#set() --> trace.istracing() when it's fixed, we see threads blocked in sendresponse and in the metrics (with hadoop 1). the difference is not huge (it's in the range 0-5%), but there is no reason to keep this. i'm sending a pull request to htrace. ",
        "label": 154
    },
    {
        "text": "hfiledatablockencoderimpl uses wrong header size when reading hfiles with no checksums    ",
        "label": 406
    },
    {
        "text": "get rid of thrift exception 'notfound'  as per the discussion in hbase-794, the use of exceptions is not good, since some clients may disconnect. return a structure like: { boolean: found, rowresult: result } where 'result' is optional if found is false. ",
        "label": 38
    },
    {
        "text": "filterlist with multiple familyfilters concatenated by or does not work   scan gives back incomplete list if multiple filters are combined with or / must_pass_one.  using 2 familyfilters in a filterlist using must_pass_one operator will give back results for only the first filter. test code   @test   public void testfilterswithor() throws exception {     tablename tn = tablename.valueof(\"mytest\");     table table = utility.createtable(tn, new string[] {\"cf1\", \"cf2\"});     byte[] cf1 = bytes.tobytes(\"cf1\");     byte[] cf2 = bytes.tobytes(\"cf2\");     put put1 = new put(bytes.tobytes(\"0\"));     put1.addcolumn(cf1, bytes.tobytes(\"col_a\"), bytes.tobytes(0));     table.put(put1);     put put2 = new put(bytes.tobytes(\"0\"));     put2.addcolumn(cf2, bytes.tobytes(\"col_b\"), bytes.tobytes(0));     table.put(put2);     familyfilter filtercf1 = new familyfilter(comparefilter.compareop.equal, new binarycomparator(cf1));     familyfilter filtercf2 = new familyfilter(comparefilter.compareop.equal, new binarycomparator(cf2));     filterlist filterlist = new filterlist(filterlist.operator.must_pass_one);     filterlist.addfilter(filtercf1);     filterlist.addfilter(filtercf2);     scan scan = new scan();     scan.setfilter(filterlist);     resultscanner scanner = table.getscanner(scan);     system.out.println(filterlist);     for (result rr = scanner.next(); rr != null; rr = scanner.next()) {       system.out.println(rr);     }   } output filterlist or (2/2): [familyfilter (equal, cf1), familyfilter (equal, cf2)] keyvalues={0/cf1:col_a/1499852754957/put/vlen=4/seqid=0} ",
        "label": 514
    },
    {
        "text": "method expecting hbaseconfiguration throw npe when given configuration  the hbaseadmin constructor takes a configuration object, but if passed a hadoop configuration, will throw a very unhelpful npe. instead, it requires a hbaseconfiguration, and should explicitly say so. ",
        "label": 314
    },
    {
        "text": " hbck2  fix the orphan regions on filesystem  plan to add this feature to hbck2 tool firstly. ",
        "label": 314
    },
    {
        "text": "spew about rebalancing but none done   i'm seeing loads of this in logs: 2009-09-24 21:27:22,130 debug org.apache.hadoop.hbase.master.regionmanager: server xx.xx.xx.100,20020,1253219583523 will be unloaded for balance. server load: 5 avg: 3.78, regions can be moved: 4 its like balancer is coming up w/ wrong answer to question... i don't see subsequent stuff going on... it does it over and over for hours. then a split comes in and its seems to shake things up. i see it do a bunch of assigning. 2009-09-24 21:41:02,784 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_split: locations,,1253657949707: daughters; locations,,1253 828460677, locations,http:\\x2f\\x2fen.wikipedia.org\\x2fwiki\\x2flarry_lucchino,1253828460677 from aa0-009-2.u.powerset.com,20020,1253219584971; 1 of 3 2009-09-24 21:41:02,784 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.xx.6:20020, startcode: 1253219584971, load: (reque sts=5213, regions=3, usedheap=114, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false 2009-09-24 21:41:02,820 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.xx.96:20020, startcode: 1253219584175, load: (requ ests=12, regions=4, usedheap=404, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false ... then back to the 'will be unloaded'... message. a new split comes in and then the assigning gets triggered again... a few regions are opened but not enough. eventually it goes back to 'normal' (average load went to 3.85 from 3.8?) ",
        "label": 314
    },
    {
        "text": "qos for meta table access  i'd like to brainstorm some ideas on how we can prioritize reads and writes to meta above reads and writes to other tables. i've noticed that if the regionserver hosting meta is under heavy load, then lots of other operations take much longer than they should. for example, i'm currently running 120 threads of ycsb across 3 client nodes hitting a 5-node cluster. doing a full scan of meta (only 600 rows) takes upwards of 30 seconds in the shell, since all of the handler threads are tied up and there's a long rpc queue. ",
        "label": 547
    },
    {
        "text": "remove broken java home autodetection in hbase config sh  allen wittenauer mentioned on twitter that the old java_home autodetection script we have in hbase-config.sh is very unlikely to do the right thing now. rip it out. ",
        "label": 163
    },
    {
        "text": "add a getregionlocator method in table asynctable interface  as it is used in shell, for now we just call the getregionlocator method in htable. ",
        "label": 149
    },
    {
        "text": "filters are not working correctly  filters used in scanning the table are not working correctly. for example a table with three rows:  1. rowkey = adminbackslash-nb0, desc:temp = \"temp\"  2. rowkey = adminbackslash-nb1, desc:temp = \"temp\"  3. rowkey = adminkleptoman, desc:temp = \"temp\" if i scan all rows in the table without filter i get all the rows as expected. but applying a simple prefixfilter with parameter \"adminbackslash\" will return only first row. i searched it down to hregion::nextinternal method, which will not output one passed row before denied row(by filter). ",
        "label": 247
    },
    {
        "text": "enable hfilev3 by default  distributed log replay needs this. should be on by default in 1.0/0.99. ",
        "label": 314
    },
    {
        "text": "regions assigned while master is splitting logs of recently crashed server  regionserver tries to execute incomplete log  in master log, i see master trying to process split of a crashed server. its split two of three logs. the server that just crashed comes back on line. balancing cuts in and master starts assigning the new server regions. new regionserver starts opening regions and messing with the log file that master is trying to write. it makes for a mess. here is how the events playout with a focus on a region that gets opened while master is processing split: 2008-12-29 16:16:38,456 info org.apache.hadoop.hbase.master.servermanager: xx.xx.xx.53:60020 lease expired ... 2008-12-29 16:16:39,494 debug org.apache.hadoop.hbase.regionserver.hlog: creating new log file writer for path hdfs://xx.xx.xx.xx:50000/data/hbase/content/1526904420/oldlogfile.log and region content,36946541ed9a62f419cf7238d32a6a38,1230448587552 ... 2008-12-29 16:17:19,686 info org.apache.hadoop.hbase.master.servermanager: received start message from: xx.xx.xx.53:60020 .... 2008-12-29 16:17:22,480 debug org.apache.hadoop.hbase.master.basescanner: current assignment of content,36946541ed9a62f419cf7238d32a6a38,1230448587552 is not valid. serverinfo: address: xx.xx.xx.53:60020, startcode: 1230585439593, load: (requests=3, regions=0, usedheap=28, maxheap=1777), passed startcode: 1230577089035, storedinfo.startcode: 1230585439593 region is not unassigned, assigned or pending ... 2008-12-29 16:17:23,622 info org.apache.hadoop.hbase.master.regionmanager: assigning region content,36946541ed9a62f419cf7238d32a6a38,1230448587552 to server xx.xx.xx.53:60020 ... 2008-12-29 16:17:26,632 info org.apache.hadoop.hbase.master.servermanager: received msg_report_process_open: content,36946541ed9a62f419cf7238d32a6a38,1230448587552 from xx.xx.xx.53:60020 ... 2008-12-29 16:17:29,666 info org.apache.hadoop.hbase.master.servermanager: received msg_report_open: content,36946541ed9a62f419cf7238d32a6a38,1230448587552 from xx.xx.xx.53:60020 .... 2008-12-29 16:17:31,933 debug org.apache.hadoop.hbase.regionserver.hlog: applied 100001 total edits from hdfs://xx.xx.xx.xx:50000/data/hbase/log_xx.xx.xx.53_1230577089035_60020/hlog.dat.1230582612406 2008-12-29 16:17:31,941 debug org.apache.hadoop.hbase.regionserver.hlog: splitting 3 of 3: hdfs://xx.xx.xx.xx:50000/data/hbase/log_xx.xx.xx.53_1230577089035_60020/hlog.dat.1230584516314 .... 2008-12-29 16:17:34,522 info org.apache.hadoop.dfs.dfsclient: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.dfs.leaseexpiredexception: no lease on /data/hbase/content/1526904420/oldlogfile.log file does not exist. [lease.  hold er: dfsclient_-1506530059, pendingcreates: 45]         at org.apache.hadoop.dfs.fsnamesystem.checklease(fsnamesystem.java:1172)         at org.apache.hadoop.dfs.fsnamesystem.getadditionalblock(fsnamesystem.java:1103)         at org.apache.hadoop.dfs.namenode.addblock(namenode.java:330)         at sun.reflect.generatedmethodaccessor15.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.ipc.rpc$server.call(rpc.java:481)         at org.apache.hadoop.ipc.server$handler.run(server.java:888) ... regionserver side i see this when it tries to open above region: 2008-12-29 21:17:28,811 warn org.apache.hadoop.hbase.regionserver.hstore: exception processing reconstruction log hdfs://xx.xx.xx.xx:50000/data/hbase/content/1526904420/oldlogfile.log opening [b@12183272 -- continuing .  probably lack-of-hadoop-1700 causing data loss! java.io.eofexception         at java.io.datainputstream.readfully(datainputstream.java:180)         at org.apache.hadoop.io.dataoutputbuffer$buffer.write(dataoutputbuffer.java:64)         at org.apache.hadoop.io.dataoutputbuffer.write(dataoutputbuffer.java:102)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1933)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1833)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1879)         at org.apache.hadoop.hbase.regionserver.hstore.doreconstructionlog(hstore.java:351)         at org.apache.hadoop.hbase.regionserver.hstore.runreconstructionlog(hstore.java:296)         at org.apache.hadoop.hbase.regionserver.hstore.<init>(hstore.java:236)         at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:1624)         at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:270)         at org.apache.hadoop.hbase.regionserver.hregionserver.instantiateregion(hregionserver.java:1364)         at org.apache.hadoop.hbase.regionserver.hregionserver.openregion(hregionserver.java:1335)         at org.apache.hadoop.hbase.regionserver.hregionserver$worker.run(hregionserver.java:1251)         at java.lang.thread.run(thread.java:619) ... ",
        "label": 314
    },
    {
        "text": "create a better way to chain log cleaners  from stack's review of hbase-2223: why this implementation have to know about other implementations? can't we do a chain of decision classes? any class can say no? as soon as any decision class says no, we exit the chain.... so in this case, first on the chain would be the ttl decision... then would be this one... and third would be the snapshotting decision. you don't have to do the chain as part of this patch but please open an issue to implement. ",
        "label": 288
    },
    {
        "text": "print the stripes' state with file size info  add some hfile size info to the stripestorefilemanager.debugdumpstate(). ",
        "label": 466
    },
    {
        "text": " hbase  method to get a number of timestamped versions of a row all at once  if you have rows that are densely populated (most versions have all the columns) and heavily versioned, and if you are doing things like checking difference between the versions, it would be really handy to be able to get all of the versions and data at once. this method would let you do that. map<long, map<text, byte[]>> getrowtimestampswithcolumns(text row); map<long, map<text, byte[]>> getrowtimestampswithcolumns(text row, text[] columns); map<long, map<text, byte[]>> getrowtimestampswithcolumns(text row, text[] columns, long timestamp); map<long, map<text, byte[]>> getrowtimestampswithcolumns(text row, text[] columns, long timestamp, int numversions); ",
        "label": 144
    },
    {
        "text": "add a log4j category for all edits to meta root  occasionally we run into bugs that have corrected meta and written some bad data to meta/root but it's difficult to understand the order in which things happened. one option is to dump the hlog contents from the servers that hosted meta at that time, but then it's interspersed with all other data. it would be nice to add a log4j logger to which we log all edits being applied to meta and root in textual form at debug level. then it would be easier to do a cluster-wide log grep to see what happened when. ",
        "label": 41
    },
    {
        "text": "negative 'requests per second' counts in ui  ",
        "label": 267
    },
    {
        "text": "regionload needs a tostring implementation  hbase(main):001:0> status 'detailed' [...]         \"-root-,,0\"             org.apache.hadoop.hbase.regionload@20b307b8         \".meta.,,1\"             org.apache.hadoop.hbase.regionload@75d343f3 [...] ",
        "label": 230
    },
    {
        "text": "add a emoji on the vote table for pre commit result on github  [~zghaobac] said that the current vote table on github is not good enough, as the colors are almost the same, it is not easy to find out which line is broken. since github can not change the color of the text, he suggested that we add a column at the left most with some emojis to better notify the developpers. ",
        "label": 149
    },
    {
        "text": "multi cf bulk load is not atomic across column families  currently the bulk load api simply imports one hfile at a time. with multi-column-family support, this is inappropriate, since different cfs show up separately. instead, the ipc endpoint should take a of cf -> hfiles, so we can online them all under a single region-wide lock. ",
        "label": 248
    },
    {
        "text": "pass htable as parameter to methods of aggregationclient  in aggregationclient, methods such as max(...), min(...) pass 'tablename' as a parameter, then a htable will be created in the method, before the method return, the created htable will be closed.  the process above may be heavy because each call must create and close a htable. the situation becomes worse when there is only one thread access hbase using aggregationclient. the underly hconnection of created htable will also be created and then closed each time when we invoke these method because no other htables using the hconnection. this operation is heavy. therefore, can we add another group of methods which pass htable as a parameter to methods defined in aggregationclient ",
        "label": 46
    },
    {
        "text": "hbase should use builder pattern to create log files while using wal on erasure coding  right now an hbase instance using the wal won't function properly in an erasure coded environment. we should change the following line to use the hdfs.distributedfilesystem builder pattern https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/protobuflogwriter.java#l92 ",
        "label": 320
    },
    {
        "text": "provide access to rpcserver instance from regionserverservices  in some cases, regionobserver coprocessors may want to directly access the running rpcserver instance on the region server. for token based authentication, for example, this is needed for a coprocessor to interact with the secretmanager that validates authentication tokens in the secure rpc engine. with the addition of async call handling on the server-side, this becomes additionally important if coprocessors want to send back delayed responses to clients. in this case, the coprocessor would need to be able to call rpcserver.getcurrentcall() to send back the response. so i propose we add access to the rpcserver in regionserverservices:   /**    * returns a reference to the region server's rpc server    */   public rpcserver getrpcserver(); we can simultaneously drop the existing regionserverservices.getrpcmetrics() method, since this could then be accessed via rpcserver.getrpcmetrics(). ",
        "label": 180
    },
    {
        "text": "compression does not work in store java of  hbase-5442 the store.createwriterintmp method missing \"compression\" ",
        "label": 204
    },
    {
        "text": "bump yammer coda dropwizard metrics dependency version  after hbase-12911 lands, let's update our dependency to the latest incarnation of this library. i guess they're now calling it dropwizard metrics. ",
        "label": 323
    },
    {
        "text": "use old code path for small amounts of batch puts  the new multi put is great, but it has lots of threads and other things that seem unnecessary for a single put. so use the old code when the put batch is small. perhaps a count of 8 would be a reasonable heuristic. ",
        "label": 547
    },
    {
        "text": "a hbase shell command to list the tables replicated from current cluster  thanks for the discussion and very good suggestions,i'd reduce the scope of this jira to only display the tables replicated from current cluster. since currently no good(accurate and consistent) way to flag a table on slave cluster, this jira will not cover such scenario. instead, the patch will be flexible enough to adapt such scenario and a follow up jira will be opened to address such situation. the shell command and output will be like. since all replication is 'global', so no need to display the cluster name here. in the future, the command will be extended for other scenarios, such as 1) replicated only to selected peers or 2) indicate table:colfam on slave side hbase shell command:list_replicated_tables hbase(main):001:0> list_replicated_tables table:columnfamily                           replicationtype                                                                             t1_dn:cf1                                   global                                                                                      t2_dn:cf2                                   global                                                                                      usertable:family                            global                                                                                     3 row(s) in 0.4110 seconds hbase(main):003:0> list_replicated_tables \"dn\" table:columnfamily                           replicationtype                                                                             t1_dn:cf1                                   global                                                                                      t2_dn:cf2                                   global                                                                                     2 row(s) in 0.0280 seconds -------------- the original jira description, keep as the history of discussion ---------------  this jira is to provide a hbase shell command which can give user can overview of the tables/columnfamilies currently being replicated. the information will help system administrator for design and planning, and also help application programmer to know which tables/columns should be watchout(for example, not to modify a replicated columnfamily on the slave cluster) currently there is no easy way to tell which table(s)/columnfamily(ies)   replicated from or to a particular cluster. on master cluster, an indirect method can be used by combining two steps: 1) $describe 'usertable' and 2) $list_peers to map the replication_scope to target(aka slave) cluster on slave cluster, this is no existing api/methods to list all the tables replicated to this cluster. here is an example, and prototype for master cluster hbase shell command:list_replicated_tables hbase(main):001:0> list_replicated_tables  table      columnfamily       target_cluster  scores      course            hdtest017.svl.ibm.com:2181:/hbase  t3_dn       cf1               hdtest017.svl.ibm.com:2181:/hbase  usertable   family            hdtest017.svl.ibm.com:2181:/hbase 3 row(s) in 0.3380 seconds -------------- end of original description ",
        "label": 134
    },
    {
        "text": "make our mr jobs implement tool and use toolrunner so can do  d trickery  etc   our tif can take a bunch of config. if our mr jobs \u2013 rowcounter, export, import, etc. \u2013 all implemented tool/toolrunner, then we'd pick up the tool cmdline parse of -d that sets config. small change. lots of utility. ",
        "label": 437
    },
    {
        "text": "root region should not be splitted even with meta row as explicit split key  split operation on root table by specifying explicit split key as .meta.  closing the root region and taking long time to fail the split before rollback.  i think we can skip split for root table as how we are doing for meta region. ",
        "label": 543
    },
    {
        "text": "expose scan cursor for asynchronous scanner  ",
        "label": 149
    },
    {
        "text": "regionsplitter's rollingsplit terminated with   by zero  and the  balancedsplit file was not deleted properly  first create a table > create 't1', 'f1' then split the table with util: > bin/hbase org.apache.hadoop.hbase.util.regionsplitter -r -o 2 t1 uniformsplit finally get the following error : 12/11/08 19:21:12 debug util.regionsplitter: all regions have been successfully split!  12/11/08 19:21:12 debug util.regionsplitter: total time = 30sec  12/11/08 19:21:12 debug util.regionsplitter: splits = 0  exception in thread \"main\" java.lang.arithmeticexception: / by zero  at org.apache.hadoop.hbase.util.regionsplitter.rollingsplit(regionsplitter.java:576)  at org.apache.hadoop.hbase.util.regionsplitter.main(regionsplitter.java:349) ",
        "label": 402
    },
    {
        "text": "typo in pom xml with  exlude  no definition of  test exclude pattern   there is a typo in pom.xml with \"exlude\", and there is no definition of \"test.exclude.pattern\". ",
        "label": 318
    },
    {
        "text": "make bloomfilter true false and self sizing  remove bloomfilter options. only one bloomfilter type makes sense in hbase context. also, make bloomfilter self-sizing; you know size when flushing. putting in 0.2 for now because its api change (for the simpler). we can punt later. ",
        "label": 218
    },
    {
        "text": "hbase backup phase  support physical table layout change  table operation such as add column family, delete column family, truncate , delete table may result in subsequent backup restore failure. ",
        "label": 441
    },
    {
        "text": "check in the generated website so can point apache infrastructure at what to publish as our hbase apache org  january 1st is deadline for changing how we publish our website. we may no longer rsync out to people.apache.org. apache infrastructure supplies two options here: http://www.apache.org/dev/project-site.html we could redo our site in apache cms format. or we could just use svnpubsub and keep on w/ how the site is currently generated and on checkin, have it autopublished. i'll go the latter route unless i hear otherwise. for svnpubsub, we need to point apache infrastructure at a directory that has our checkedin site in it. i was thinking ${hbasedir}/hbase.apache.org let me raise this on the dev list too. ",
        "label": 314
    },
    {
        "text": "fix npe that is showing up since hbase went in  saw this in testdistributedlogsplitting after hbase-14274 was applied. 119113 2015-08-20 15:31:10,704 warn  [hbase-metrics2-1] impl.metricsconfig(124): cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties 119114 2015-08-20 15:31:10,710 error [hbase-metrics2-1] lib.methodmetric$2(118): error invoking method getblockstotal 119115 java.lang.reflect.invocationtargetexception 119116 \u203a   at sun.reflect.generatedmethodaccessor72.invoke(unknown source) 119117 \u203a   at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) 119118 \u203a   at java.lang.reflect.method.invoke(method.java:606) 119119 \u203a   at org.apache.hadoop.metrics2.lib.methodmetric$2.snapshot(methodmetric.java:111) 119120 \u203a   at org.apache.hadoop.metrics2.lib.methodmetric.snapshot(methodmetric.java:144) 119121 \u203a   at org.apache.hadoop.metrics2.lib.metricsregistry.snapshot(metricsregistry.java:387) 119122 \u203a   at org.apache.hadoop.metrics2.lib.metricssourcebuilder$1.getmetrics(metricssourcebuilder.java:79) 119123 \u203a   at org.apache.hadoop.metrics2.impl.metricssourceadapter.getmetrics(metricssourceadapter.java:195) 119124 \u203a   at org.apache.hadoop.metrics2.impl.metricssourceadapter.updatejmxcache(metricssourceadapter.java:172) 119125 \u203a   at org.apache.hadoop.metrics2.impl.metricssourceadapter.getmbeaninfo(metricssourceadapter.java:151) 119126 \u203a   at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getnewmbeanclassname(defaultmbeanserverinterceptor.java:333) 119127 \u203a   at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.registermbean(defaultmbeanserverinterceptor.java:319) 119128 \u203a   at com.sun.jmx.mbeanserver.jmxmbeanserver.registermbean(jmxmbeanserver.java:522) 119129 \u203a   at org.apache.hadoop.metrics2.util.mbeans.register(mbeans.java:57) 119130 \u203a   at org.apache.hadoop.metrics2.impl.metricssourceadapter.startmbeans(metricssourceadapter.java:221) 119131 \u203a   at org.apache.hadoop.metrics2.impl.metricssourceadapter.start(metricssourceadapter.java:96) 119132 \u203a   at org.apache.hadoop.metrics2.impl.metricssystemimpl.registersource(metricssystemimpl.java:245) 119133 \u203a   at org.apache.hadoop.metrics2.impl.metricssystemimpl$1.poststart(metricssystemimpl.java:229) 119134 \u203a   at sun.reflect.generatedmethodaccessor50.invoke(unknown source) 119135 \u203a   at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) 119136 \u203a   at java.lang.reflect.method.invoke(method.java:606) 119137 \u203a   at org.apache.hadoop.metrics2.impl.metricssystemimpl$3.invoke(metricssystemimpl.java:290) 119138 \u203a   at com.sun.proxy.$proxy13.poststart(unknown source) 119139 \u203a   at org.apache.hadoop.metrics2.impl.metricssystemimpl.start(metricssystemimpl.java:185) 119140 \u203a   at org.apache.hadoop.metrics2.impl.jmxcachebuster$jmxcachebusterrunnable.run(jmxcachebuster.java:81) 119141 \u203a   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) 119142 \u203a   at java.util.concurrent.futuretask.run(futuretask.java:262) 119143 \u203a   at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:178) 119144 \u203a   at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:292) 119145 \u203a   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) 119146 \u203a   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) 119147 \u203a   at java.lang.thread.run(thread.java:744) 119148 caused by: java.lang.nullpointerexception 119149 \u203a   at org.apache.hadoop.hdfs.server.blockmanagement.blocksmap.size(blocksmap.java:198) 119150 \u203a   at org.apache.hadoop.hdfs.server.blockmanagement.blockmanager.gettotalblocks(blockmanager.java:3158) 119151 \u203a   at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblockstotal(fsnamesystem.java:5652) 119152 \u203a   ... 32 more ",
        "label": 154
    },
    {
        "text": "ignore  archive directory as a table  from a recent test run: 2012-07-22 02:27:30,699 warn [ipc server handler 0 on 47087] util.fstabledescriptors(168): the following folder is in hbase's root directory and doesn't contain a table descriptor, do consider deleting it: .archive with the addition of hbase-5547, table-level folders are no-longer all table folders. fstabledescriptors needs to then have a 'gold-list' that we can update with directories that aren't tables so we don't have this kind of thing showing up in the logs. currently, we have the following block:  invocations++;  if (htabledescriptor.root_tabledesc.getnameasstring().equals(tablename)) unknown macro: { cachehits++; return htabledescriptor.root_tabledesc; }  if (htabledescriptor.meta_tabledesc.getnameasstring().equals(tablename)) unknown macro: { cachehits++; return htabledescriptor.meta_tabledesc; } to handle special cases, but that's a bit clunky and not clean in terms of table-level directories that need to be ignored. ",
        "label": 236
    },
    {
        "text": "filterlist getnextkeyhint skips rows that should be included in the results  i hit a weird issue/bug and am able to reproduce the error consistently. the problem arises when filterlist has two filters where each implements the getnextkeyhint method. the way the current implementation works is, storescanner will call matcher.getnextkeyhint() whenever it gets a seek_next_using_hint. this in turn will call filter.getnextkeyhint() which at this stage is of type filterlist. the implementation in filterlist iterates through all the filters and keeps the max keyvalue that it sees. all is fine if you wrap filters in filterlist in which only one of them implements getnextkeyhint. but if multiple of them implement then that's where things get weird. for example: create two filters: one is fuzzyrowfilter and second is columnrangefilter. both of them implement getnextkeyhint wrap them in filterlist with must_pass_all fuzzyrowfilter will seek to the correct first row and then pass it to columnrangefilter which will return the seek_next_using_hint code. now in filterlist when getnextkeyhint is called, it calls the one on fuzzyrow first which basically says what the next row should be. while in reality we want the columnrangefilter to give the seek hint. the above behavior skips data that should be returned, which i have verified by using a rowfilter with regexstringcomparator. i updated the filterlist to maintain state on which filter returns the seek_next_using_hint and in getnextkeyhint, i invoke the method on the saved filter and reset that state. i tested it with my current queries and it works fine but i need to run the entire test suite to make sure i have not introduced any regression. in addition to that i need to figure out what should be the behavior when the opeation is must_pass_one, but i doubt it should be any different. is my understanding of it being a bug correct ? or am i trivializing it and ignoring something very important ? if it's tough to wrap your head around the explanation, then i can open a jira and upload a patch against 0.94 head. ",
        "label": 474
    },
    {
        "text": "add extra test cases for assignement on the region server and fix the related issues  we don't have a lot of tests on the region server itself.  here are some.  some of them are failing, feedback welcome.  see as well the attached state diagram for the zk nodes on assignment. ",
        "label": 340
    },
    {
        "text": "writetowal is not serialized for increment operation  class org.apache.hadoop.hbase.client.increment has a member  boolean writetowal;  that is not serialized/deserialized in write/readfields functions. as a result an operation to increment several columns within a single row always writes to wal, even if a client calls  increment.setwritetowal(false); ",
        "label": 38
    },
    {
        "text": "pretty print ttl  i've seen a lot of users getting confused by the ttl configuration and i think that if we just pretty printed it it would solve most of the issues. for example, let's say a user wanted to set a ttl of 90 days. that would be 7776000. but let's say that it was typo'd to 77760000 instead, it gives you 900 days! so when we print the ttl we could do something like \"x days, x hours, x minutes, x seconds (real_ttl_value)\". this would also help people when they use ms instead of seconds as they would see really big values in there. ",
        "label": 163
    },
    {
        "text": "expose rpcscheduling implementations as limitedprivate interfaces  in phoenix-938 we are attempting to resolve cross-rs deadlocks in indexing by adding custom rpc handlers (so regular puts/reads don't interfere with index updates). however, we've run into a couple of snags where the interfaces change, making it a bit more difficult to support interoperability between minor versions as the underlying rpc handling changed (for the better, but still different . this would just mark those interfaces public, evolving, so we still have some flexibility, but don't break existing usage. note, this kind of thing will come up for any client who is doing custom rpc handling - beyond the recently added flexibility - but wants to stay in line with the current hbase implementation (rather than building their own rpc handling mechanisms). ",
        "label": 236
    },
    {
        "text": "findbugs task in build xml  integrate findbugs task in build.xml , so that eventually we are good to go with hudson integration as well. ",
        "label": 266
    },
    {
        "text": "region mover rb can hang if table region it belongs to is deleted   i was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation). here's the start of the relevent stack dump 12/02/08 13:27:13 warn client.hconnectionmanager$hconnectionimplementation: encountered problems when prefetch meta table: org.apache.hadoop.hbase.tablenotfoundexception: cannot find row in .meta. for table: testloadandverify_1328735001040, row=testloadand\\ verify_1328735001040,yc^p\\xd7\\x945\\xd4,99999999999999         at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:136)         at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:95)         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.prefetchregioncache(hconnectionmanager.java:64\\ 9)         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregioninmeta(hconnectionmanager.java:703\\ )         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:594)         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.relocateregion(hconnectionmanager.java:565)         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getregionlocation(hconnectionmanager.java:416)         at org.apache.hadoop.hbase.client.servercallable.instantiateserver(servercallable.java:57)         at org.apache.hadoop.hbase.client.scannercallable.instantiateserver(scannercallable.java:63)         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getregionserverwithretries(hconnectionmanager.\\ java:1018)         at org.apache.hadoop.hbase.client.htable$clientscanner.nextscanner(htable.java:1104)         at org.apache.hadoop.hbase.client.htable$clientscanner.initialize(htable.java:1027)         at org.apache.hadoop.hbase.client.htable.getscanner(htable.java:535)         at sun.reflect.generatedmethodaccessor24.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.jruby.javasupport.javamethod.invokedirectwithexceptionhandling(javamethod.java:525)         at org.jruby.javasupport.javamethod.invokedirect(javamethod.java:380)         at org.jruby.java.invokers.instancemethodinvoker.call(instancemethodinvoker.java:58)         at org.jruby.runtime.callsite.cachingcallsite.call(cachingcallsite.java:137)         at usr.lib.hbase.bin.region_mover.method__7$ruby$issuccessfulscan(/usr/lib/hbase/bin/region_mover.rb:133)         at usr$lib$hbase$bin$region_mover#method__7$ruby$issuccessfulscan.call(usr$lib$hbase$bin$region_mover#method__7$ruby$issucces\\ sfulscan:65535)         at usr$lib$hbase$bin$region_mover#method__7$ruby$issuccessfulscan.call(usr$lib$hbase$bin$region_mover#method__7$ruby$issucces\\ sfulscan:65535)         at org.jruby.runtime.callsite.cachingcallsite.call(cachingcallsite.java:171)         at usr.lib.hbase.bin.region_mover.block_4$ruby$__for__(/usr/lib/hbase/bin/region_mover.rb:326)         at usr$lib$hbase$bin$region_mover#block_4$ruby$__for__.call(usr$lib$hbase$bin$region_mover#block_4$ruby$__for__:65535)         at org.jruby.runtime.compiledblock.yield(compiledblock.java:133)         at org.jruby.runtime.blockbody.call(blockbody.java:73)         at org.jruby.runtime.block.call(block.java:89)         at org.jruby.rubyproc.call(rubyproc.java:268)         at org.jruby.rubyproc.call(rubyproc.java:228)         at org.jruby.rubyproc$i$0$0$call.call(rubyproc$i$0$0$call.gen:65535)         at org.jruby.internal.runtime.methods.dynamicmethod.call(dynamicmethod.java:209)         at org.jruby.internal.runtime.methods.dynamicmethod.call(dynamicmethod.java:205)         at org.jruby.runtime.callsite.cachingcallsite.call(cachingcallsite.java:137)         at org.jruby.ast.calloneargnode.interpret(calloneargnode.java:57)         at org.jruby.ast.newlinenode.interpret(newlinenode.java:103)         at org.jruby.ast.whilenode.interpret(whilenode.java:131)         at org.jruby.ast.newlinenode.interpret(newlinenode.java:103)         at org.jruby.ast.blocknode.interpret(blocknode.java:71)         at org.jruby.evaluator.astinterpreter.interpret_method(astinterpreter.java:74)         at org.jruby.internal.runtime.methods.interpretedmethod.call(interpretedmethod.java:169)         at org.jruby.internal.runtime.methods.defaultmethod.call(defaultmethod.java:171)         at org.jruby.runtime.callsite.cachingcallsite.cacheandcall(cachingcallsite.java:272)         at org.jruby.runtime.callsite.cachingcallsite.callblock(cachingcallsite.java:114)         at org.jruby.runtime.callsite.cachingcallsite.calliter(cachingcallsite.java:123)         at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$ruby$syntheticunloadregions(/usr/lib/hbase/bin/region_mover.rb:319)         at usr.lib.hbase.bin.region_mover.method__25$ruby$unloadregions(/usr/lib/hbase/bin/region_mover.rb:313)         at usr$lib$hbase$bin$region_mover#method__25$ruby$unloadregions.call(usr$lib$hbase$bin$region_mover#method__25$ruby$unloadregions:65535)         at usr$lib$hbase$bin$region_mover#method__25$ruby$unloadregions.call(usr$lib$hbase$bin$region_mover#method__25$ruby$unloadregions:65535)         at org.jruby.runtime.callsite.cachingcallsite.cacheandcall(cachingcallsite.java:302)         at org.jruby.runtime.callsite.cachingcallsite.call(cachingcallsite.java:173)         at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)         at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)         at org.jruby.ruby.runscript(ruby.java:670)         at org.jruby.ruby.runnormally(ruby.java:574)         at org.jruby.ruby.runfrommain(ruby.java:423)         at org.jruby.main.dorunfrommain(main.java:278)         at org.jruby.main.internalrun(main.java:198)         at org.jruby.main.run(main.java:164)         at org.jruby.main.run(main.java:148)         at org.jruby.main.main(main.java:128) ",
        "label": 242
    },
    {
        "text": "ugly ioe when region is being closed  rather  should nsre  i'm running 80/20 ycsb (80% reads/20% writes). i see this from time to time in logs (especially if i do big fat bulk upload at same time \u2013 having trouble making ycsb do heavy loading at mo): 2010-05-05 06:57:01,165 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region, no outstanding scanners on usertable,user1431413702,1273040674721 2010-05-05 06:57:01,165 debug org.apache.hadoop.hbase.regionserver.hregion: no more row locks outstanding on region usertable,user1431413702,1273040674721 2010-05-05 06:57:01,178 debug org.apache.hadoop.hbase.regionserver.store: closed cf 2010-05-05 06:57:01,178 info org.apache.hadoop.hbase.regionserver.hregion: closed usertable,user1431413702,1273040674721 2010-05-05 06:57:01,178 error org.apache.hadoop.hbase.regionserver.hregionserver: java.io.ioexception: region usertable,user1431413702,1273040674721 closed         at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1179)         at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1172)         at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2506)         at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2493)         at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1742)         at sun.reflect.generatedmethodaccessor12.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:657)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:915) this should be a nsre. j-d took a look and fact that we do an ioe 'region closed' is way old, from before the time forgot, so its just always been there just more obvious now that get is a scan. ",
        "label": 122
    },
    {
        "text": "only one coprocessor of each priority type can be loaded for a table  coprocessors are added to hbase using a treeset that is initialized with an environmentprioritycomparator. the net effect is that only one coprocessor of a given priority can be loaded at a time for a given table. this appears to be due to how the treeset uses the environmentprioritycomparator to determine whether there are duplicate entries - if the coprocessors have the same priority (e.g., user), they are considered the same and won't be added to the set. ",
        "label": 38
    },
    {
        "text": "provide client api to explicitly lock and unlock rows  we need to be able to perform a series of reads from and writes to a single row without any potential interference from other clients. unfortunately this is a bit involved because normal reads currently do not acquire row locks, so it requires adding additional get/getrow calls that obtain and release a row lock. in addition, there will be two additional client calls, lockrow/unlockrow, which actually acquire and release the locks. though each lock is associated with an hregion, this will be tracked within the hregionserver. when a lock is acquired from the client, it is handled much like a scanner. we obtain the row lock from the hregion, store the region name and lock identifier in a synchronized map, and also obtain a lease to ensure that the lock will eventually be released even if the client dies. this also required adding a rowlocklistener (implements leaselistener) private class in hrs to handle row lock lease expiration. hrs.lockrow will return a long lockid (as openscanner does) that will be used in subsequent client calls to reuse this existing row lock. these calls will check that the lock is valid and perform the operations without any locking (wrappers around get*, new versions of batchupdate, openscanner, etc). this is going to really add some noise to the list of available htable/client methods so i'm not sure if it's something people would want to commit into a normal release. regardless this does provide some very convenient functionality that may be useful to others. we are also looking into clint morgan's hbase-669, but one major downside is that there is a significant amount of overhead involved. this row locking is already built in and this will only extend the api to allow clients to work with them directly. there is little to no overhead at all. the only (obvious) performance consideration is that this should only used where necessary as rows will not be locked and unlocked as quickly with round-trip client calls. in our design, we will have specific notes in our schema about which tables (or even which families or columns) must be accessed with row locks at all times and which do not. this is my first attempt at adding any additional functionality, so comments, criticism, code reviews are encouraged. i should have a patch up tomorrow. ",
        "label": 247
    },
    {
        "text": "am nodedeleted and ssh races creating problems for regions under split  we tried to address the problems in master restart and rs restart while split region is in progress as part of hbase-5806.  while doing some more we found still there is one race condition.  -> split has just started and the znode is in rs_split state.  -> rs goes down.  -> first call back for ssh comes.  -> as part of the fix for hbase-5806 ssh knows that some region is in rit.  -> but now nodedeleted event comes for the split node and there we try to delete the rit.  -> after this we try to see in the ssh whether any node is in rit. as we dont find the region in rit the region is never assigned. when we fixed hbase-5806 step 6 happened first and then step 5 happened. so we missed it. now we found that. will come up with a patch shortly. ",
        "label": 544
    },
    {
        "text": "fix config typo in pluggable load balancer factory  hbase-4240 made loadbalancer pluggable. configuration it loads seems to be wrongly named and carries a typo: \"hbase.maser.loadbalancer.class\" could rather be \"hbase.master.loadbalancer.class\" luckily 0.92 is not out yet and we should fix it asap, before folks start using it. attaching patch. ",
        "label": 194
    },
    {
        "text": "can't truncate disable table that has rows in  meta  that have empty info regioninfo column  i somehow manufactured empty info:regioninfo cells in .meta. \u2013 still trying to figure how \u2013 but trying to drop the table i get npe error: java.lang.nullpointerexception: null backtrace: org/apache/hadoop/hbase/util/writables.java:75:in `getwritable'            org/apache/hadoop/hbase/util/writables.java:119:in `gethregioninfo'            org/apache/hadoop/hbase/client/hconnectionmanager.java:505:in `processrow'            org/apache/hadoop/hbase/client/metascanner.java:190:in `metascan'            org/apache/hadoop/hbase/client/metascanner.java:95:in `metascan'            org/apache/hadoop/hbase/client/metascanner.java:73:in `metascan'            org/apache/hadoop/hbase/client/hconnectionmanager.java:530:in `gethtabledescriptor'            org/apache/hadoop/hbase/client/htable.java:320:in `gettabledescriptor'            /home/stack/hbase/bin/../bin/../src/main/ruby/hbase/admin.rb:205:in `truncate'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands/truncate.rb:33:in `command'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:64:in `format_simple_command'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands/truncate.rb:31:in `command'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:31:in `command_safe'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:70:in `translate_hbase_exceptions'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell/commands.rb:31:in `command_safe'            /home/stack/hbase/bin/../bin/../src/main/ruby/shell.rb:106:in `command'            (eval):2:in `truncate'            (hbase):4:in `irb_binding' ",
        "label": 314
    },
    {
        "text": "remove references to deprecated methods in hadoop once hadoop is released  a number of methods in hadoop have been deprecated for release 0.17.0. once 0.17.0 is released, use preferred alternate. ",
        "label": 241
    },
    {
        "text": "unable to open the few links in http hbase apache org   few links in http://hbase.apache.org/ is not working.   for example, ref guide (multi-page) will actually link to http://hbase.apache.org/book/book.html and if i try to open this, page not found error is coming.  if i add /book in the url, like http://hbase.apache.org/book/book/book.html, it is taking me to the apache hbase reference guide i think the folder structure has been changed. ",
        "label": 314
    },
    {
        "text": "write bulk load commit events to wal  similar to writing flush (hbase-11511), compaction(hbase-2231) to wal and region open/close (hbase-11512) , we should persist bulk load events to wal.  this is especially important for secondary region replicas, since we can use this information to pick up primary regions' files from secondary replicas. a design doc for secondary replica replication can be found at hbase-11183. ",
        "label": 21
    },
    {
        "text": "npe in hbaseclient connection readresponse  stack is: 2013-04-19 09:22:45,991 warn  [ipc client (682317035) connection to ip-10-6-131-32.ec2.internal/10.6.131.32:60020 from root] ipc.hbaseclient (hbaseclient.java:run(664)) - ipc client (682317035) connection to ip-10-6-131-32.ec2.internal/10.6.131.32:60020 from root: unexpected exception receiving call responses java.lang.runtimeexception: java.lang.nullpointerexception at org.apache.hadoop.hbase.ipc.hbaseclient$connection.readresponse(hbaseclient.java:1017) at org.apache.hadoop.hbase.ipc.hbaseclient$connection.run(hbaseclient.java:661) caused by: java.lang.nullpointerexception at org.apache.hadoop.hbase.ipc.hbaseclient$connection.readresponse(hbaseclient.java:1013) ... 1 more  1197 sec: 3411081 operations; 324,27 current ops/sec; [insert averagelatency(us)=29332,6]  code:     protected void readresponse() {       if (shouldcloseconnection.get()) return;       touch();       try {         // see hbaseserver.call.setresponse for where we write out the response.         // total size of the response.  unused.  but have to read it in anyways.         /*int totalsize =*/ in.readint();         // read the header         responseheader responseheader = responseheader.parsedelimitedfrom(in);         int id = responseheader.getcallid();         if (log.isdebugenabled()) {           log.debug(getname() + \": got response header \" +             textformat.shortdebugstring(responseheader));         }         call call = calls.get(id);         if (responseheader.hasexception()) {           exceptionresponse exceptionresponse = responseheader.getexception();           remoteexception re = createremoteexception(exceptionresponse);           if (isfatalconnectionexception(exceptionresponse)) {             markclosed(re);           } else {             if (call != null) call.setexception(re);           }         } else {           message rpcresponsetype;           try {             // todo: why pb engine pollution in here in this class?  fix.             rpcresponsetype =               protobufrpcclientengine.invoker.getreturnprototype(                 reflectioncache.getmethod(remoteid.getprotocol(), call.method.getname()));  <=========== npe, because call is null           } catch (exception e) {             throw new runtimeexception(e); //local exception           } ",
        "label": 340
    },
    {
        "text": "remove the securerpcengine and merge the security related logic in the core engine  remove the securerpcengine and merge the security-related logic in the core engine. follow up to hbase-5727. ",
        "label": 139
    },
    {
        "text": "admin istableavailable returns incorrect result before daughter regions are opened  admin#istableavailable checks if it can getservername for the meta entries it reads. during the time of split server location are added to the meta entries in metatableaccessor#splitregion although the description of the method says \"does not add the location information to the daughter regions since they are not open yet.\". at this point during the split daughter regions are not actually open, so we can get to a state where parent is offline, daughters are not yet open but istableavailable returns true. ",
        "label": 3
    },
    {
        "text": " hbck2  update hbck2 readme to explain new  setregionstate  method   hbase-22143 introduced new setregionstate method that allows for manually setting a region to an specific state, in meta. this was added a client side logic for updating region state, to workout case where region is left in opening state (unassigns/assigns will fail to progress here) on releases prior to 2.1.2 (when scheduleservercrashprocedure was added to master's hbckservice). to reflect, this jira adds not only setregionstate description, but also additional explanations on the overview section of readme.md file. ",
        "label": 486
    },
    {
        "text": "regioncoprocesorhost prewalrestore throws npe in case there is no regionobserver registered   it seems the check to bypass the observers chain is at wrong place in case of pre/post walrestore. it should be inside the \"if statement\" that checks whether the cp is instance of regionobserver or not. ",
        "label": 199
    },
    {
        "text": "implement table checkanddelete   this issue is to implement table#checkanddelete() api. ",
        "label": 441
    },
    {
        "text": "reopening a region on a rs can leave it in pending open  i got this twice during the same test. if the region servers are slow enough and you run an online alter, it's possible for the rs to change the znode status to closed and have the master send an open before the region server is able to remove the region from it's list of rits. this is what the master sees: 011-12-21 22:24:09,498 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. (offlining)  2011-12-21 22:24:09,498 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x134589d3db033f7 creating unassigned node for 43123e2e3fc83ec25fe2a76b4f09077f in a closing state  2011-12-21 22:24:09,524 debug org.apache.hadoop.hbase.master.assignmentmanager: sent close to sv4r25s44,62023,1324494325099 for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.  2011-12-21 22:24:15,656 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_closed, server=sv4r25s44,62023,1324494325099, region=43123e2e3fc83ec25fe2a76b4f09077f  2011-12-21 22:24:15,656 debug org.apache.hadoop.hbase.master.handler.closedregionhandler: handling closed event for 43123e2e3fc83ec25fe2a76b4f09077f  2011-12-21 22:24:15,656 debug org.apache.hadoop.hbase.master.assignmentmanager: forcing offline; was=test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. state=closed, ts=1324506255629, server=sv4r25s44,62023,1324494325099  2011-12-21 22:24:15,656 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x134589d3db033f7 creating (or updating) unassigned node for 43123e2e3fc83ec25fe2a76b4f09077f with offline state  2011-12-21 22:24:15,663 debug org.apache.hadoop.hbase.master.assignmentmanager: found an existing plan for test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. destination server is + sv4r25s44,62023,1324494325099  2011-12-21 22:24:15,663 debug org.apache.hadoop.hbase.master.assignmentmanager: using pre-existing plan for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.; plan=hri=test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., src=, dest=sv4r25s44,62023,1324494325099  2011-12-21 22:24:15,663 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. to sv4r25s44,62023,1324494325099  2011-12-21 22:24:15,664 error org.apache.hadoop.hbase.master.assignmentmanager: failed assignment in: sv4r25s44,62023,1324494325099 due to org.apache.hadoop.hbase.regionserver.regionalreadyintransitionexception: received:open for the region:test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. ,which we are already trying to close. after that the master abandons. and the region server: 2011-12-21 22:24:09,523 info org.apache.hadoop.hbase.regionserver.hregionserver: received close region: test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.  2011-12-21 22:24:09,523 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: processing close of test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.  2011-12-21 22:24:09,524 debug org.apache.hadoop.hbase.regionserver.hregion: closing test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.: disabling compactions & flushes  2011-12-21 22:24:09,524 info org.apache.hadoop.hbase.regionserver.hregion: running close preflush of test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.  2011-12-21 22:24:09,524 debug org.apache.hadoop.hbase.regionserver.hregion: started memstore flush for test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., current region memstore size 40.5m  2011-12-21 22:24:09,524 debug org.apache.hadoop.hbase.regionserver.hregion: finished snapshotting test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., commencing wait for mvcc, flushsize=42482936  2011-12-21 22:24:13,368 debug org.apache.hadoop.hbase.regionserver.store: renaming flushed file at hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/.tmp/87d6944c54c7417e9a34a9f9542bcb72 to hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/87d6944c54c7417e9a34a9f9542bcb72  2011-12-21 22:24:13,568 info org.apache.hadoop.hbase.regionserver.store: added hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/87d6944c54c7417e9a34a9f9542bcb72, entries=54209, sequenceid=31451012, memsize=40.5m, filesize=31.4m  2011-12-21 22:24:14,381 info org.apache.hadoop.hbase.regionserver.hregion: finished memstore flush of ~40.5m/42482936, currentsize=218.9k/224128 for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. in 4856ms, sequenceid=31451012, compaction requested=true  2011-12-21 22:24:15,267 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.  2011-12-21 22:24:15,267 debug org.apache.hadoop.hbase.regionserver.hregion: started memstore flush for test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., current region memstore size 218.9k  2011-12-21 22:24:15,267 debug org.apache.hadoop.hbase.regionserver.hregion: finished snapshotting test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., commencing wait for mvcc, flushsize=224128  2011-12-21 22:24:15,330 debug org.apache.hadoop.hbase.regionserver.store: renaming flushed file at hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/.tmp/0a744b85cec5454e873a7c27bf9b3c53 to hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/0a744b85cec5454e873a7c27bf9b3c53  2011-12-21 22:24:15,346 info org.apache.hadoop.hbase.regionserver.store: added hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/0a744b85cec5454e873a7c27bf9b3c53, entries=286, sequenceid=31451619, memsize=218.9k, filesize=170.2k  2011-12-21 22:24:15,347 info org.apache.hadoop.hbase.regionserver.hregion: finished memstore flush of ~218.9k/224128, currentsize=0.0/0 for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. in 80ms, sequenceid=31451619, compaction requested=true  2011-12-21 22:24:15,365 info org.apache.hadoop.hbase.regionserver.hregion: closed test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.  2011-12-21 22:24:15,365 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:62023-0x134589d3db03403 attempting to transition node 43123e2e3fc83ec25fe2a76b4f09077f from m_zk_region_closing to rs_zk_region_closed  2011-12-21 22:24:15,637 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:62023-0x134589d3db03403 successfully transitioned node 43123e2e3fc83ec25fe2a76b4f09077f from m_zk_region_closing to rs_zk_region_closed  2011-12-21 22:24:15,670 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: set region closed state in zk successfully for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. sn name: sv4r25s44,62023,1324494325099  2011-12-21 22:24:15,670 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: closed region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. doing a force unassign in the shell fixes it. a small-ish fix would be to add to regionalreadyintransitionexception which state it's in so that we can detect this case and then just retry or open on another region server. this is critical for online altering to work, but i don't think it's likely to happen in other situations. ",
        "label": 544
    },
    {
        "text": "use saslutil to set sasl qop in 'thrift'  in configure the thrift gateway, it says \"set the property hbase.thrift.security.qop to one of the following three values: privacy, integrity, authentication\", which would lead to failure of starting up a thrift server.  in fact, the value of hbase.thrift.security.qop should be auth, auth-int, auth-conf, according to the documentation of sasl.qop ",
        "label": 370
    },
    {
        "text": "if the master is started with a wrong root dir  it gets stuck and can't be killed  reported by a new user on irc who tried to set hbase.rootdir to file:///~/hbase, the master gets stuck and cannot be killed. i tried something similar on my machine and it spins while logging: 2011-12-09 16:11:17,002 warn org.apache.hadoop.hbase.util.fsutils: unable to create version file at file:/bin/hbase, retrying: mkdirs failed to create file:/bin/hbase  2011-12-09 16:11:27,002 warn org.apache.hadoop.hbase.util.fsutils: unable to create version file at file:/bin/hbase, retrying: mkdirs failed to create file:/bin/hbase  2011-12-09 16:11:37,003 warn org.apache.hadoop.hbase.util.fsutils: unable to create version file at file:/bin/hbase, retrying: mkdirs failed to create file:/bin/hbase the reason it cannot be stopped is that the master's main thread is stuck in there and will never be notified: \"master:0;su-jdcryans-01.local,51116,1323475535684\" prio=5 tid=7f92b7a3c000 nid=0x1137ba000 waiting on condition [1137b9000]  java.lang.thread.state: timed_waiting (sleeping)  at java.lang.thread.sleep(native method)  at org.apache.hadoop.hbase.util.fsutils.setversion(fsutils.java:297)  at org.apache.hadoop.hbase.util.fsutils.setversion(fsutils.java:268)  at org.apache.hadoop.hbase.master.masterfilesystem.checkrootdir(masterfilesystem.java:339)  at org.apache.hadoop.hbase.master.masterfilesystem.createinitialfilesystemlayout(masterfilesystem.java:128)  at org.apache.hadoop.hbase.master.masterfilesystem.<init>(masterfilesystem.java:113)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:435)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:314)  at org.apache.hadoop.hbase.master.hmastercommandline$localhmaster.run(hmastercommandline.java:218)  at java.lang.thread.run(thread.java:680) it seems we should do a better handling of the exceptions we get in there, and die if we need to. it would make a better user experience. maybe also do a check on hbase.rootdir before even starting the master. ",
        "label": 409
    },
    {
        "text": "loss a mass of data when the sequenceid of cells greater than integer max  because memstoremergersegmentsiterator can not merge segments  if your memstore type is compactingmemstore\uff0cmemstoremergersegmentsiterator can not merge memstore segments when the seqid of cells greater than integer.max, as a result, lossing a mass of data. the reason is that memstoremergersegmentsiterator use integer.max as readpt when create scanner,  but the seqid of cell  may be greater than integer.max_value,  it`s type is long.   code as below: public memstoremergersegmentsiterator(list<immutablesegment> segments, cellcomparator comparator,     int compactionkvmax) throws ioexception {   super(compactionkvmax);   // create the list of scanners to traverse over all the data   // no dirty reads here as these are immutable segments   abstractmemstore.addtoscanners(segments, integer.max_value, scanners); //bug, should use long.max_value   heap = new keyvalueheap(scanners, comparator); } segmentscanner.java code as below protected void updatecurrent() {   cell startkv = current;   cell next = null;   try {     while (iter.hasnext()) {       next = iter.next();       // here, if seqid>readpoint(integer.max_value), never read cell, as a result, lossing lots of cells       if (next.getsequenceid() <= this.readpoint) {         current = next;         return;// skip irrelevant versions       }       if (stopskippingkvsifnextrow &&   // for backwardseek() stay in the           startkv != null &&        // boundaries of a single row           segment.comparerows(next, startkv) > 0) {         current = null;         return;       }     } // end of while     current = null; // nothing found   } finally {     if (next != null) {       // in all cases, remember the last kv we iterated to, needed for reseek()       last = next;     }   } } memstorecompactorsegmentsiterator has the same bug public memstorecompactorsegmentsiterator(list<immutablesegment> segments,     cellcomparator comparator, int compactionkvmax, hstore store) throws ioexception {   super(compactionkvmax);   list<keyvaluescanner> scanners = new arraylist<keyvaluescanner>();   abstractmemstore.addtoscanners(segments, integer.max_value, scanners);   //bug, should use long.max_value   // build the scanner based on query matcher   // reinitialize the compacting scanner for each instance of iterator   compactingscanner = createscanner(store, scanners);   refillkvs(); } ",
        "label": 94
    },
    {
        "text": "remove remove k  v  from type poolmap k v   i keep getting red cross in my eclipse, whatever the jdk (jdk6, jdk7, jdk8) name clash: the method remove(k, v) of type poolmap<k,v> has the same erasure as remove(object, object) of type map<k,v> but does not override it maybe related to hbase-10030 the solution i have is simply removing the deprecated method, and everything is fine. i am not sure of the backwards compatibility here. ",
        "label": 158
    },
    {
        "text": "tableservers   processbatchofrows   converts from list to       expensive copy  with autoflush to false and a large write buffer on htable, when we write bulk puts - tableserver # processbatchofrows , convert the input (list) to an [ ] , before sending down the wire. with a write buffer as large as 20 mb , that becomes an expensive copy when we do - list.toarray(new t[ ] ). may be - should we change the wire protocol to support list as well , and then revisit this to prevent the bulk copy ? batch b = new batch(this) {         @override         int docall(final list<row> currentlist, final byte [] row,           final byte [] tablename)         throws ioexception, runtimeexception {           *final put [] puts = currentlist.toarray(put_array_type);*           return getregionserverwithretries(new servercallable<integer>(this.c,               tablename, row) {             public integer call() throws ioexception {               return server.put(location.getregioninfo().getregionname(), puts);             }           });         } ",
        "label": 314
    },
    {
        "text": "fix flakey testasynctablegetmultithreaded  https://builds.apache.org/job/hbase-flaky-tests/job/master/2959/testreport/junit/org.apache.hadoop.hbase.client/testasynctablegetmultithreaded/test/ the error is thrown from an admin method, where we do not have any retries if the region is not online yet. should be a test issue, let me fix. ",
        "label": 149
    },
    {
        "text": "multiget  multidelete  and multiput   batched to the appropriate region servers  i've started to create a general interface for doing these batch/multi calls and would like to get some input and thoughts about how we should handle this and what the protocol should  look like. first naive patch, coming soon. ",
        "label": 303
    },
    {
        "text": "fix problem with default bind address of thriftserver  the command line help states that when no -b bind address is given it uses 0.0.0.0. that is not the case though:      inetaddress listenaddress = null;      if (cmd.hasoption(\"bind\")) {        try {          listenaddress = inetaddress.getbyname(cmd.getoptionvalue(\"bind\"));        } catch (unknownhostexception e) {          log.error(\"could not bind to provided ip address\", e);          printusageandexit(options, -1);        }      } else {        listenaddress = inetaddress.getlocalhost();      } the latter is not 0.0.0.0 but the current ip: beanshell% inetaddress.getlocalhost()  de1-app-mbp-2/10.0.0.65 so we either need to change the command line help or set the address to 0.0.0.0 instead. ",
        "label": 289
    },
    {
        "text": "notices txt refers to facebook for thrift  the notices.txt file says: in addition, this product includes software developed by: facebook, inc. (http://developers.facebook.com/thrift/ \u2013 page includes the thrift software license) since thrift is now an apache project, this seems to be obsolete. ",
        "label": 314
    },
    {
        "text": "regionobserver implementation whose prescanneropen and postscanneropen impl return null can stall the system initialization through npe  in hregionserver.java openscanner()       r.preparescanner(scan);       regionscanner s = null;       if (r.getcoprocessorhost() != null) {         s = r.getcoprocessorhost().prescanneropen(scan);       }       if (s == null) {         s = r.getscanner(scan);       }       if (r.getcoprocessorhost() != null) {         s = r.getcoprocessorhost().postscanneropen(scan, s);       } if we dont have implemention for postscanneropen the regionscanner is null and so throwing nullpointer java.lang.nullpointerexception at java.util.concurrent.concurrenthashmap.put(concurrenthashmap.java:881) at org.apache.hadoop.hbase.regionserver.hregionserver.addscanner(hregionserver.java:2282) at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:2272) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1326) making this defect as blocker.. pls feel free to change the priority if am wrong. also correct me if my way of trying out coprocessors without implementing postscanneropen is wrong. am just a learner. ",
        "label": 544
    },
    {
        "text": "testtableindex failed in hbasepatch build   testtableindex failed in hbase-patch build #14. see http://hudson.zones.apache.org/hudson/job/hbase-patch/14/testreport/ junit.framework.assertionfailederror  at org.apache.hadoop.hbase.multiregiontable.makemultiregiontable(multiregiontable.java:137)  at org.apache.hadoop.hbase.mapred.testtableindex.setup(testtableindex.java:125) ",
        "label": 241
    },
    {
        "text": "initial documentation for stripe compactions  initial documentation for stripe compactions (distill from attached docs, make up to date, put somewhere like book) ",
        "label": 406
    },
    {
        "text": "provide capability to delete named region  see discussion titled 'able to control routing to solr shards or not' on lily-discuss  user may want to quickly dispose of out of date records by deleting specific regions. ",
        "label": 333
    },
    {
        "text": "remove and convert  deprecated remoteexceptionhandler decoderemoteexception calls  moving issue w/ no recent movement out of 0.95 ",
        "label": 437
    },
    {
        "text": "potentially too many connections may be opened if threadlocalpool or roundrobinpool is used  see 'problem with hbase.client.ipc.pool.type=threadlocal in trunk' discussion started by lars george. from lars hofhansl:  looking at hbaseclient.getconnection(...) i see this:      synchronized (connections) {        connection = connections.get(remoteid);        if (connection == null) {          connection = new connection(remoteid);          connections.put(remoteid, connection);        }      } at the same time poolmap.threadlocalpool.put is defined like this:    public r put(r resource) {      r previousresource = get();      if (previousresource == null) { ...        if (poolsize.intvalue() >= maxsize) {          return null;        } ...    } so... if the threadlocalpool reaches its capacity it always returns null and hence all new threads will create a  new connection every time getconnection is called! i have also verified with a test program that works fine as long as the number of client threads (which include  the threads in htable's threadpool of course) is < poolsize. once that is no longer the case the number of  connections \"explodes\" and the program dies with oomes (mostly because each connection is associated with  yet another thread). it's not clear what should happen, though. maybe (1) the threadlocalpool should not have a limit, or maybe  (2) allocations past the pool size should throw an exception (i.e. there's a hard limit), or maybe (3) in that case  a single connection is returned for all threads while the pool it over its limit or (4) we start round robin with the other  connection in the other thread locals. for #1 means that the number of client threads needs to be more carefully managed by the client app.  in this case it would also be somewhat pointless that connection have their own threads, we just pass stuff  between threads.  #2 would work, but puts more logic in the client.  #3 would lead to hard to debug performance issues.  and #4 is messy from ted yu:  for hbaseclient, at least the javadoc doesn't match:    * @param config configuration    * @return either a {@link pooltype#reusable} or {@link pooltype#threadlocal}    */   private static pooltype getpooltype(configuration config) {     return pooltype.valueof(config.get(hconstants.hbase_client_ipc_pool_type),         pooltype.roundrobin, pooltype.threadlocal); i think for roundrobinpool, we shouldn't allow maxsize to be integer#max_value. otherwise connection explosion described by lars may incur. ",
        "label": 265
    },
    {
        "text": "clean up the replication queues in the postpeermodification stage when removing a peer  in the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by rs and if an rs is crashed then some queues may left there forever. that's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. with the new procedure based replication peer modification, i think we can do it cleanly. after the refreshpeerprocedures are done on all rses, we can make sure that no rs will create queue for this peer again, then we can iterate over all the queues for all rses and do another round of clean up. ",
        "label": 149
    },
    {
        "text": "add timeout limit for hbaseclient dataoutputstream  i run jstack at client host. the result is below.  \"hbase-tablepool-60-thread-34\" daemon prio=10 tid=0x00007f1e65a48000 nid=0x5173 runnable [0x00000000579cc000]  java.lang.thread.state: runnable  at sun.nio.ch.epollarraywrapper.epollwait(native method)  at sun.nio.ch.epollarraywrapper.poll(epollarraywrapper.java:210)  at sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:65)  at sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:69) locked <0x0000000758cb0780> (a sun.nio.ch.util$2) locked <0x0000000758cb0770> (a java.util.collections$unmodifiableset) locked <0x0000000758cb0548> (a sun.nio.ch.epollselectorimpl)  at sun.nio.ch.selectorimpl.select(selectorimpl.java:80)  at org.apache.hadoop.net.socketiowithtimeout$selectorpool.select(socketiowithtimeout.java:336)  at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:158)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:153)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:114)  at java.io.bufferedoutputstream.flushbuffer(bufferedoutputstream.java:65)  at java.io.bufferedoutputstream.flush(bufferedoutputstream.java:123) locked <0x0000000754e978a0> (a java.io.bufferedoutputstream)  at java.io.dataoutputstream.flush(dataoutputstream.java:106)  at org.apache.hadoop.hbase.ipc.hbaseclient$connection.sendparam(hbaseclient.java:620) locked <0x0000000754e97880> (a java.io.dataoutputstream)  at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:975)  at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:86)  at $proxy13.multi(unknown source)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3$1.call(hconnectionmanager.java:1395)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3$1.call(hconnectionmanager.java:1393)  at org.apache.hadoop.hbase.client.servercallable.withoutretries(servercallable.java:210)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3.call(hconnectionmanager.java:1402)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3.call(hconnectionmanager.java:1390)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) this thread have hung for one hours meanwhile other thread try to close connection \"ipc client (1983049639) connection to dump002030.cm6.tbsite.net/10.246.2.30:30020 from admin\" daemon prio=10 tid=0x00007f1e70674800 nid=0x3d76 waiting for monitor entry [0x000000004bc0f000]  java.lang.thread.state: blocked (on object monitor)  at java.io.bufferedoutputstream.flush(bufferedoutputstream.java:123) waiting to lock <0x0000000754e978a0> (a java.io.bufferedoutputstream)  at java.io.dataoutputstream.flush(dataoutputstream.java:106)  at java.io.filteroutputstream.close(filteroutputstream.java:140)  at org.apache.hadoop.io.ioutils.cleanup(ioutils.java:237)  at org.apache.hadoop.io.ioutils.closestream(ioutils.java:254)  at org.apache.hadoop.hbase.ipc.hbaseclient$connection.close(hbaseclient.java:715) locked <0x0000000754e7b818> (a org.apache.hadoop.hbase.ipc.hbaseclient$connection)  at org.apache.hadoop.hbase.ipc.hbaseclient$connection.run(hbaseclient.java:587) dump002030.cm6.tbsite.net is dead regionserver. i read hbase sourececode, discover connection.out doesn't set timeout  this.out = new dataoutputstream  (new bufferedoutputstream(netutils.getoutputstream(socket))); i see this mean epoll_wait will block indefinitely. ",
        "label": 290
    },
    {
        "text": "catalogjanitor can clear a daughter that split before processing its parent  i didn't dig a lot into this issue, but by splitting a table twice in a row i was able to trigger a situation where a daughter of the first split was deleted by the catalogjanitor before it processed its parent. will post log in a comment. ",
        "label": 314
    },
    {
        "text": "branch does not compile against hadoop  tested with mvn clean install -dhadoop.profile=3.0 -dhadoop-three.version=3.0.0-alpha4 [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[37,32] package org.apache.hadoop.mapred does not exist [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[38,35] package org.apache.hadoop.mapreduce does not exist [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[117,66] cannot find symbol [error]   symbol:   class job [error]   location: class org.apache.hadoop.hbase.security.user [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[129,46] cannot find symbol [error]   symbol:   class jobconf [error]   location: class org.apache.hadoop.hbase.security.user [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[344,59] cannot find symbol [error]   symbol:   class job [error]   location: class org.apache.hadoop.hbase.security.user.securehadoopuser [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[369,39] cannot find symbol [error]   symbol:   class jobconf [error]   location: class org.apache.hadoop.hbase.security.user.securehadoopuser [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[351,17] cannot find symbol [error]   symbol:   class job [error]   location: class org.apache.hadoop.hbase.security.user.securehadoopuser [error] /users/apurtell/src/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/security/user.java:[375,25] cannot find symbol [error]   symbol:   class jobconf [error]   location: class org.apache.hadoop.hbase.security.user.securehadoopuser ",
        "label": 38
    },
    {
        "text": "improve the assignment when node failures happen to choose the secondary rs as the new primary rs  ",
        "label": 139
    },
    {
        "text": "move example coprocessor into hbase examples  move the example co-processor into the hbase-examples module. also move the protobuf definition files into the module. ",
        "label": 154
    },
    {
        "text": "introduce new example and helper classes to tell cp users how to do filtering on scanners  finally we decided that cp users should not have the ability to create storescanner or storefilescanner, so it is impossible for them to filter out some cells when flush or compaction by simply provide a filter when constructing storescanner. but i think filtering out some cells is a very important usage for cp users, so we need to provide the ability in another way. theoretically it can be done with wrapping an internalscanner, but i think we need to give an example, or even some helper classes to help cp users. ",
        "label": 149
    },
    {
        "text": "scanner  every cell in a row has the same timestamp  a row can have multiple cells, and each cell can have a different timestamp. the get command in the shell demonstrates that cells are being stored with different timestamps: hbase(main):008:0> get 'table1', 'row2'   column                       cell   fam1:letters                timestamp=1215707612949, value=def   fam1:numbers                timestamp=1215707629064, value=123   fam2:letters                timestamp=1215711498969, value=abc  3 row(s) in 0.0100 seconds however, using the scanners to retrieve these cells shows that they all have the same timestamp: hbase(main):009:0> scan 'table1'   row                          column+cell  row2                        column=fam1:letters, timestamp=1215711498969, value=def   row2                        column=fam1:numbers, timestamp=1215711498969, value=123   row2                        column=fam2:letters, timestamp=1215711498969, value=abc  3 row(s) in 0.0600 seconds the scanners are losing timestamp information somewhere along the line. ",
        "label": 241
    },
    {
        "text": "testmasteroperationsforregionreplicas is flakey  java.lang.assertionerror: expected:<3> but was:<2> at org.apache.hadoop.hbase.master.testmasteroperationsforregionreplicas.validatefromsnapshotfrommeta(testmasteroperationsforregionreplicas.java:354) at org.apache.hadoop.hbase.master.testmasteroperationsforregionreplicas.testcreatetablewithmultiplereplicas(testmasteroperationsforregionreplicas.java:204) https://builds.apache.org/job/hbase-flaky-tests/job/master/3431/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.testmasteroperationsforregionreplicas-output.txt/*view*/ i think the problem here is that the regions are still under transition. so maybe we should disable balancer, and also wait until no rit before asserting. ",
        "label": 149
    },
    {
        "text": "add test to avoid unintentional reordering of items in hbaseobjectwritable  hbaseobjectwritable has a static initialization block that assigns ints to various classes. the int is assigned by using a local variable that is incremented after each use. if someone adds a line in the middle of the block, this throws off everything after the change, and can break client compatibility. there is already a comment to not add/remove lines at the beginning of this block. it might make sense to have a test against a static set of ids. if something gets changed unintentionally, it would at least fail the tests. if the change was intentional, at the very least the test would need to get updated, and it would be a conscious decision. https://issues.apache.org/jira/browse/hbase-5204 contains the the fix for one issue of this type. ",
        "label": 312
    },
    {
        "text": "update shading for javax activation  the javax.activation dependency is added in hadoop trunk (3.3.0, hadoop-15775) and hbase does not compile against hadoop trunk successfully. it is required for supporting jdk11 in hadoop. hbase-22087 will concern other dependencies. ",
        "label": 4
    },
    {
        "text": "support start row and stop row in hbase export  the org.apache.hadoop.hbase.mapreduce.export doesn't have a row start / row stop option. this option can be useful when a user wants to filter on start and end row. this feature was also requested by on hbase-users (see http://mail-archives.apache.org/mod_mbox/hbase-user/201310.mbox/%3c1382391635726-4051960.post@n3.nabble.com%3e). ",
        "label": 481
    },
    {
        "text": "thread safety issues with compression algorithm gz and compressiontest  i've seen some occasional nullpointerexceptions in zlibfactory.isnativezlibloaded(conf) during region server startups and the completebulkload process. this is being caused by a null configuration getting passed to the isnativezlibloaded method. i think this happens when 2 or more threads call the compressiontest.testcompression method at once. if the gz algorithm has not been tested yet both threads could continue on and attempt to load the compressor. for gz the getcodec method is not thread safe which could lead to one thread getting a reference to a gzipcodec that has a null configuration. current:       defaultcodec getcodec(configuration conf) {         if (codec == null) {           codec = new gzipcodec();           codec.setconf(new configuration(conf));         }         return codec;       } one possible fix would be something like this:       defaultcodec getcodec(configuration conf) {         if (codec == null) {           gzipcodec gzip = new gzipcodec();           gzip.setconf(new configuration(conf));           codec = gzip;         }         return codec;       } but that may not be totally safe without some synchronization. an upstream fix in compressiontest could also prevent multi thread access to gz.getcodec(conf) exceptions:  12/02/21 16:11:56 error handler.openregionhandler: failed open of region=all-monthly,,1326263896983.bf574519a95263ec23a2bad9f5b8cbf4.  java.io.ioexception: java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.compressiontest.testcompression(compressiontest.java:89)  at org.apache.hadoop.hbase.regionserver.hregion.checkcompressioncodecs(hregion.java:2670)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2659)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2647)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.openregion(openregionhandler.java:312)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:99)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:158)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.nullpointerexception  at org.apache.hadoop.io.compress.zlib.zlibfactory.isnativezlibloaded(zlibfactory.java:63)  at org.apache.hadoop.io.compress.gzipcodec.getcompressortype(gzipcodec.java:166)  at org.apache.hadoop.io.compress.codecpool.getcompressor(codecpool.java:100)  at org.apache.hadoop.io.compress.codecpool.getcompressor(codecpool.java:112)  at org.apache.hadoop.hbase.io.hfile.compression$algorithm.getcompressor(compression.java:236)  at org.apache.hadoop.hbase.util.compressiontest.testcompression(compressiontest.java:84)  ... 9 more caused by: java.io.ioexception: java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.compressiontest.testcompression(compressiontest.java:89)  at org.apache.hadoop.hbase.io.hfile.hfile$reader.readtrailer(hfile.java:890)  at org.apache.hadoop.hbase.io.hfile.hfile$reader.loadfileinfo(hfile.java:819)  at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles.grouporsplit(loadincrementalhfiles.java:405)  at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles$2.call(loadincrementalhfiles.java:323)  at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles$2.call(loadincrementalhfiles.java:321)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.nullpointerexception  at org.apache.hadoop.io.compress.zlib.zlibfactory.isnativezlibloaded(zlibfactory.java:63)  at org.apache.hadoop.io.compress.gzipcodec.getcompressortype(gzipcodec.java:166)  at org.apache.hadoop.io.compress.codecpool.getcompressor(codecpool.java:100)  at org.apache.hadoop.io.compress.codecpool.getcompressor(codecpool.java:112)  at org.apache.hadoop.hbase.io.hfile.compression$algorithm.getcompressor(compression.java:236)  at org.apache.hadoop.hbase.util.compressiontest.testcompression(compressiontest.java:84)  ... 10 more ",
        "label": 154
    },
    {
        "text": "loadzooconfig can mask true error  try {   properties.load(inputstream); } catch (ioexception e) {   string msg = \"fail to read properties from \" + zookeeper_config_name;   log.fatal(msg);   throw new ioexception(msg); } this masks the actual error, if there is one. ",
        "label": 86
    },
    {
        "text": "make use of storefile 'order' making certain decisions during merge sort  when we merge sort results currently there is no regard for storefile order. this issue is about exploiting store file order at certain junctures. for example, if we have n kvs all of the same coordinates \u2013 same r/f/q/type/ts \u2013 then the one from the storefile that was made most recently should prevail. also, we might consider order when looking at deletes so our tombstones are less tombstoney in that they'll only apply to values that are in storefiles older than the one that carries the delete marker (this latter sounds hard but putting it out there anyways). ",
        "label": 357
    },
    {
        "text": "make assignmentmanger enablingtables and disablinttables local variables  those enablingtables and disablingtables, are used only during the startup time. they should be some local variables. we can load them from zktable at the beginning instead of handling them per table. ",
        "label": 242
    },
    {
        "text": "combinedblockcache should overwrite cachestats rollmetricsperiod   it seems combinedblockcache should overwrite cachestats#rollmetricsperiod() as public void rollmetricsperiod() {   lrucachestats.rollmetricsperiod();   bucketcachestats.rollmetricsperiod(); } otherwise, combinedblockcache.gethitratiopastnperiods() and combinedblockcache.gethitcachingratiopastnperiods() will always return 0. ",
        "label": 238
    },
    {
        "text": "row cache of keyvalue should be cleared in readfields   keyvalue does not clear its row cache in reading new values (readfields()).  therefore, if a keyvalue (kv) which caches its row bytes reads another keyvalue instance, kv.getrow() returns a wrong value. ",
        "label": 442
    },
    {
        "text": "backport missing options in shell  recently i wanna try to alter the split policy for a table on my cluster which version is 1.2.6 and as far as i know the split_policy is an attribute of the htable so i run the command in hbase shell console below.   alter 'tablex',split_policy => 'org.apache.hadoop.hbase.regionserver.disabledregionsplitpolicy'  however, it gave the information like this and i confused   unknown argument ignored: split_policy  updating all regions with the new schema...  so i check the source code that admin.rb might miss the setting for this argument .  htd.setmaxfilesize(jlong.valueof(arg.delete(max_filesize))) if arg[max_filesize]  htd.setreadonly(jboolean.valueof(arg.delete(readonly))) if arg[readonly]  ...  so i think it may be a bug ,is it? ",
        "label": 561
    },
    {
        "text": "remove interface audience annotations in introduced by hbase  an interfaceaudience slipped into 0.94 here. it breaks 0.94 for older versions of hadoop. ",
        "label": 248
    },
    {
        "text": "develop hbase shell command tool to list table's region info through command line  i am going through the hbase shell commands to see if there is anything i can use to get all the regions info just for a particular table. i don\u2019t see any such command that provides me that information.  it would be better to have a command that provides region info, start key, end key etc taking a table name as the input parameter. this is available through hbase ui on clicking on a particular table's link a tool/shell command to get a list of regions for a table or all tables in a tabular structured output (that is machine readable) ",
        "label": 264
    },
    {
        "text": "support multi homing env for the publication of rs status with multicast  hbase status published   currently, when the publication feature is enabled (hbase.status.published=true), it uses the interface which is found first:  https://github.com/apache/hbase/blob/2e8bd0036dbdf3a99786e5531495d8d4cb51b86c/hbase-server/src/main/java/org/apache/hadoop/hbase/master/clusterstatuspublisher.java#l268-l275 this won't work when the host has the multiple network interfaces and the unreachable one to the other nodes is selected. the interface which can be used for the communication between cluster nodes should be configurable. ",
        "label": 455
    },
    {
        "text": "authentication for rest clients  like thrift, the rest gateway is not currently integrated into the authentication used for hbase rpc. currently this means the rest gateway cannot even be used when hbase security is active. for the rest gateway to be able to interoperate with hbase security: 1. the rest server needs to be able to login from a keytab on startup with its own server principal 2. rest clients need to be able to authenticate security with the rest server 3. the rest server needs to be able to act as a trusted proxy for the original client identities, so that the hbase authorization checks can be performed against the original client request like thrift, implementing step #1 as a bare minimum would at least allow deploying a rest server configured to login as the application user on startup. even without authenticating rest clients, this would allow the gateway to work when hbase security is active. for step #2, we can make use of spnego to provide kerberos/gssapi authentication of clients over http. the alfredo library from cloudera would hopefully make this relatively easy to do:  http://cloudera.github.com/alfredo/docs/latest/index.html ",
        "label": 314
    },
    {
        "text": "add support for tags to cell interface  cell interface has suppport for mvcc. the only thing we'd add to cell in the near future is support for tags it would seem. should be easy to add. should add it now. see backing discussion here: https://issues.apache.org/jira/browse/hbase-7233?focusedcommentid=13573784&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13573784 matt outlines what the additions to cell might look like here: https://issues.apache.org/jira/browse/hbase-7233?focusedcommentid=13531619&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13531619 would be good to get these in now. marking as 0.96. can more later. ",
        "label": 544
    },
    {
        "text": "hregion lock object is not being released properly  leading to snapshot failure  ",
        "label": 441
    },
    {
        "text": "testmetricmbeanbase testgetattribute is flakey under hadoop2 profile  this specific small unit tests flakes out occasionally and blocks the medium and large tests from running. here's an error trace: error message expected:<2.0> but was:<0.125> stacktrace junit.framework.assertionfailederror: expected:<2.0> but was:<0.125> at junit.framework.assert.fail(assert.java:57) at junit.framework.assert.failnotequals(assert.java:329) at junit.framework.assert.assertequals(assert.java:120) at junit.framework.assert.assertequals(assert.java:129) at junit.framework.testcase.assertequals(testcase.java:288) at org.apache.hadoop.hbase.metrics.testmetricsmbeanbase.testgetattribute(testmetricsmbeanbase.java:93) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at junit.framework.testcase.runtest(testcase.java:176) at junit.framework.testcase.runbare(testcase.java:141) at junit.framework.testresult$1.protect(testresult.java:122) at junit.framework.testresult.runprotected(testresult.java:142) at junit.framework.testresult.run(testresult.java:125) at junit.framework.testcase.run(testcase.java:129) at junit.framework.testsuite.runtest(testsuite.java:255) at junit.framework.testsuite.run(testsuite.java:250) at org.junit.internal.runners.junit38classrunner.run(junit38classrunner.java:84) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) elliott neil clark took a quick look and will chime in on this. ",
        "label": 154
    },
    {
        "text": "running hfile tool passing fully qualified filename i get 'illegalargumentexception  wrong fs'  this fixes it: diff --git a/src/main/java/org/apache/hadoop/hbase/io/hfile/hfile.java b/src/main/java/org/apache/hadoop/hbase/io/hfile/hfile.java index 65cbb9d..3966108 100644 --- a/src/main/java/org/apache/hadoop/hbase/io/hfile/hfile.java +++ b/src/main/java/org/apache/hadoop/hbase/io/hfile/hfile.java @@ -1826,6 +1826,8 @@ public class hfile {        configuration conf = hbaseconfiguration.create();        conf.set(\"fs.defaultfs\",          conf.get(org.apache.hadoop.hbase.hconstants.hbase_dir)); +      conf.set(\"fs.default.name\", +        conf.get(org.apache.hadoop.hbase.hconstants.hbase_dir));        filesystem fs = filesystem.get(conf);        arraylist<path> files = new arraylist<path>();        if (cmd.hasoption(\"f\")) { ",
        "label": 314
    },
    {
        "text": "deprecate remove assignmentmanager clearregionfromtransition  this method is essentially a dupe of assignment#regionoffline. as suggested in early review of hbase-5128 - deprecate up to 0.94 and remove from 0.96/trunk. ",
        "label": 248
    },
    {
        "text": "generate changes md and releasenotes md for  ",
        "label": 187
    },
    {
        "text": "port hbase integration tests on cluster are not getting picked up from distribution to  ",
        "label": 406
    },
    {
        "text": "missing copyright headers  hi, i'm starting a debian package of hbase and noticed, that the following files do not have copyright headers: ./java/org/apache/hadoop/hbase/io/hfile/simpleblockcache.java: no copyright unknown  ./java/org/apache/hadoop/hbase/io/reference.java: no copyright unknown  ./java/org/apache/hadoop/hbase/util/base64.java: no copyright unknown  ./java/org/apache/hadoop/hbase/util/pair.java: no copyright unknown  ./java/org/apache/hadoop/hbase/migration/nineteen/io/reference.java: no copyright unknown  ./java/org/apache/hadoop/hbase/client/unmodifyablehcolumndescriptor.java: no copyright unknown  ./java/org/apache/hadoop/hbase/client/metascanner.java: no copyright unknown  ./java/org/apache/hadoop/hbase/filter/columncountgetfilter.java: no copyright unknown  ./java/org/apache/hadoop/hbase/zookeeper/zkservertool.java: no copyright unknown ./contrib/ec2/bin/hbase-ec2-init-remote.sh: no copyright unknown  ./contrib/ec2/bin/functions.sh: no copyright unknown  ./contrib/ec2/bin/hbase-ec2-init-zookeeper-remote.sh: no copyright unknown you can make packaging easier by providing copyright headers for these files. thank you. ",
        "label": 38
    },
    {
        "text": "leaked references to storefile reader after hbase  we observed this production that after a region server dies there are huge number of hfiles in that region for the region server running the version with hbase-13082, in the doc it is given that it is expected to happen, but we found a one place where scanners are not being closed. if the scanners are not closed their references are not decremented and that is leading to the issue of huge number of store files not being finalized all i was able to find is in the selectscannersfrom, where we discard some of the scanners and we are not closing them. i am attaching a patch for that. also to avoid these issues should the files that are done be logged and finalized (moved to archive) as a part of region close operation. this will solve any leaks that can happen and does not cause any dire consequences? ",
        "label": 524
    },
    {
        "text": "compaction threads need names  the compactsplitthread creates executors for doing compaction work, but threads end up named things like \"pool-2-thread-1\" which isn't very useful. ",
        "label": 38
    },
    {
        "text": "design and document the official procedure for posting patches  commits  commit messages  etc  to smooth process and make integration with tools easier  i have been building a tool (currently called reposync) to help me keep the internal fb hbase-92-based branch up-to-date with the public branches. various inconsistencies in our process has made it difficult to automate a lot of this stuff. i'd like to work with everyone to come up with the official best practices and stick to it. i welcome all suggestions. among some of the things i'd like to nail down: commit message format best practice and commit message format for multiple commits multiple commits per jira vs. jira per commit, what are the exceptions and when affects vs. fix versions potential usage of [tags] in commit messages for things like book, scripts, shell... maybe even whatever is in the components field? increased usage of jira tags or labels to mark exactly which repos a jira has been committed to (potentially even internal repos? ways for a tool to keep track in jira?) we also need to be more strict about some things if we want to follow apache guidelines. for example, all final versions of a patch must be attached to jira so that the author properly assigns it to apache. ",
        "label": 330
    },
    {
        "text": "thrift server doesnt know about atomicincrement  the thrift server needs the atomicincrement api implemented ",
        "label": 451
    },
    {
        "text": "hbaseclient can get stuck in an infinite loop while attempting to contact a failed regionserver  while using hbase thrift server, if a regionserver goes down due to shutdown or failure clients will timeout because the thrift server cannot contact the dead regionserver. ",
        "label": 314
    },
    {
        "text": "implement async admin operations for draining region servers  ",
        "label": 187
    },
    {
        "text": "protobuf wal also needs a trailer  new protobuf wal has a header, but we will probably need a trailer as well, reserved for later usage. right now, we can we just serialize an empty trailer, but putting more metadata there, like range of sequence_id's, region names, table names etc might be needed in the future. ",
        "label": 199
    },
    {
        "text": "reusablestreamgzipcodec npe upon reset with ibm jdk  this is the same issue as described in hadoop-8419, repeat the issue description here: the reusablestreamgzipcodec will npe upon reset after finish when the native zlib codec is not loaded. when the native zlib is loaded the codec creates a compressoroutputstream that doesn't have the problem, otherwise, the reusablestreamgzipcodec uses gzipoutputstream which is extended to provide the resetstate method. since ibm jdk 6 sr9 fp2 including the current jdk 6 sr10, gzipoutputstream#finish will release the underlying deflater(calls the deflater's end method), which causes npe upon reset. this seems to be an ibm jdk quirk as sun jdk and openjdk doesn't have this issue. since in hbase-5387 hbase source has refactor its code not to use hadoop's gzipcodec during real compress/decompress, it's necessary to make a separate patch for hbase on the same issue ",
        "label": 372
    },
    {
        "text": "batch coprocessor  this is designed to improve the coprocessor invocation in the client side.   currently the coprocessor invocation is to send a call to each region. if there\u2019s one region server, and 100 regions are located in this server, each coprocessor invocation will send 100 calls, each call uses a single thread in the client side. the threads will run out soon when the coprocessor invocations are heavy.   in this design, all the calls to the same region server will be grouped into one in a single coprocessor invocation. this call will be spread into each region in the server side. ",
        "label": 243
    },
    {
        "text": "load balancer falls into pathological state if one server under average   slop  endless churn  i'm looking at a 0.20.4 cluster of 80 fast machines. it runs fine for a while and then falls over into crazy balancing churn (my view on logs is sporadic but have a log before me where this is happening). things i see that seem to be of 0.20.4 particularly: + we don't reach an equilibrium or at least it takes so long, its as though it wasn't every going to happen  + master log filled w/ open, close of one or two regions usually the same ones over and over (the regions that are candidates to close are provided by the rs. they are ordered by hash of their name. we return the top ten from this set every time. so, we always close the same regions all the time even if we just opened it)  + often, we'll tell an rs to close a region. it will do the job. in 0.20.4 we made it so if rs has any work at all for the master, that we return immediately rather than wait for the reporting period to elaspse. so, on these fast machines, it can be back near immediately if it just opened another some other region, say. it can get assigned the region it just closed. seems to happen frequently enough. for example, look at the below extract featuring a single regions life. its opened and closed 5 times in about 1/2 a second: 2010-05-25 11:01:05,488 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of genericmetastore,139757491,1274779304880, false, reassign: true 2010-05-25 11:01:05,489 info org.apache.hadoop.hbase.master.processregionclose$1: region set as unassigned: genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,490 info org.apache.hadoop.hbase.master.regionmanager: assigning region genericmetastore,139757491,1274779304880 to a025.example.com,60020,1274744064673 2010-05-25 11:01:05,510 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: genericmetastore,139757491,1274779304880 from a025.example.com,60020,1274744064673; 1 of 1 2010-05-25 11:01:05,510 info org.apache.hadoop.hbase.master.regionserveroperation: genericmetastore,139757491,1274779304880 open on 10.209.32.189:60020 2010-05-25 11:01:05,511 info org.apache.hadoop.hbase.master.regionserveroperation: updated row genericmetastore,139757491,1274779304880 in region .meta.,,1 with startcode=1274744064673, server=10.209.32.189:60020 2010-05-25 11:01:05,548 debug org.apache.hadoop.hbase.master.regionmanager: going to close region genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,552 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: genericmetastore,139757491,1274779304880 from a025.example.com,60020,1274744064673; 1 of 2 2010-05-25 11:01:05,552 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of genericmetastore,139757491,1274779304880, false, reassign: true 2010-05-25 11:01:05,552 info org.apache.hadoop.hbase.master.processregionclose$1: region set as unassigned: genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,556 info org.apache.hadoop.hbase.master.regionmanager: assigning region genericmetastore,139757491,1274779304880 to a028.example.com,60020,1274747560769 2010-05-25 11:01:05,578 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: genericmetastore,139757491,1274779304880 from a028.example.com,60020,1274747560769; 1 of 1 2010-05-25 11:01:05,578 info org.apache.hadoop.hbase.master.regionserveroperation: genericmetastore,139757491,1274779304880 open on 10.209.32.185:60020 2010-05-25 11:01:05,579 info org.apache.hadoop.hbase.master.regionserveroperation: updated row genericmetastore,139757491,1274779304880 in region .meta.,,1 with startcode=1274747560769, server=10.209.32.185:60020 2010-05-25 11:01:05,599 debug org.apache.hadoop.hbase.master.regionmanager: going to close region genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,605 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: genericmetastore,139757491,1274779304880 from a028.example.com,60020,1274747560769; 1 of 2 2010-05-25 11:01:05,605 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of genericmetastore,139757491,1274779304880, false, reassign: true 2010-05-25 11:01:05,606 info org.apache.hadoop.hbase.master.processregionclose$1: region set as unassigned: genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,607 info org.apache.hadoop.hbase.master.regionmanager: assigning region genericmetastore,139757491,1274779304880 to sjc1c104.example.com,60020,1274747062601 2010-05-25 11:01:05,640 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: genericmetastore,139757491,1274779304880 from sjc1c104.example.com,60020,1274747062601; 1 of 1 2010-05-25 11:01:05,640 info org.apache.hadoop.hbase.master.regionserveroperation: genericmetastore,139757491,1274779304880 open on 10.209.42.181:60020 2010-05-25 11:01:05,641 info org.apache.hadoop.hbase.master.regionserveroperation: updated row genericmetastore,139757491,1274779304880 in region .meta.,,1 with startcode=1274747062601, server=10.209.42.181:60020 2010-05-25 11:01:05,723 debug org.apache.hadoop.hbase.master.regionmanager: going to close region genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,729 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: genericmetastore,139757491,1274779304880 from sjc1c104.example.com,60020,1274747062601; 1 of 4 2010-05-25 11:01:05,729 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of genericmetastore,139757491,1274779304880, false, reassign: true 2010-05-25 11:01:05,730 info org.apache.hadoop.hbase.master.processregionclose$1: region set as unassigned: genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,731 info org.apache.hadoop.hbase.master.regionmanager: assigning region genericmetastore,139757491,1274779304880 to sjc1c091.example.com,60020,1274747056415 2010-05-25 11:01:05,751 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: genericmetastore,139757491,1274779304880 from sjc1c091.example.com,60020,1274747056415; 1 of 1 2010-05-25 11:01:05,752 info org.apache.hadoop.hbase.master.regionserveroperation: genericmetastore,139757491,1274779304880 open on 10.209.42.238:60020 2010-05-25 11:01:05,752 info org.apache.hadoop.hbase.master.regionserveroperation: updated row genericmetastore,139757491,1274779304880 in region .meta.,,1 with startcode=1274747056415, server=10.209.42.238:60020 2010-05-25 11:01:05,775 debug org.apache.hadoop.hbase.master.regionmanager: going to close region genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,780 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: genericmetastore,139757491,1274779304880 from sjc1c091.example.com,60020,1274747056415; 1 of 2 2010-05-25 11:01:05,780 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of genericmetastore,139757491,1274779304880, false, reassign: true 2010-05-25 11:01:05,780 info org.apache.hadoop.hbase.master.processregionclose$1: region set as unassigned: genericmetastore,139757491,1274779304880 2010-05-25 11:01:05,808 info org.apache.hadoop.hbase.master.regionmanager: assigning region genericmetastore,139757491,1274779304880 to sjc1a003.example.com,60020,1274747057557 2010-05-25 11:01:05,828 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: genericmetastore,139757491,1274779304880 from sjc1a003.example.com,60020,1274747057557; 1 of 1 2010-05-25 11:01:05,828 info org.apache.hadoop.hbase.master.regionserveroperation: genericmetastore,139757491,1274779304880 open on 10.209.32.148:60020 2010-05-25 11:01:05,829 info org.apache.hadoop.hbase.master.regionserveroperation: updated row genericmetastore,139757491,1274779304880 in region .meta.,,1 with startcode=1274747057557, server=10.209.32.148:60020 the culprit seems to be the code that wants to bring up underloaded regionservers up to average. that and something about the lightly loaded servers math that is off. i'm marking this as a blocker. i'm not sure why its not more common. there were some issues on this cluster regards disks filling but though such an event may have provoked the issue, we should have evened out eventually. making this a blocker on 0.20.5. need to fix it for this user at least. ",
        "label": 314
    },
    {
        "text": "rangerowfilter  allow to choose rows based on a  lexicographic  comparison to column's values  this allows one to do equal, greater than, etc comparisons on column values. only rows with columns which pass comparison get through. its cleaner and more powerful than the \"columnfilter\" in regexprowfilter which only provides equals. i think this functionality should be deprecated in regexprowfilter. ",
        "label": 110
    },
    {
        "text": "rename mvcc to mvcc  from multiversionconsistencycontrol to multiversionconcurrencycontrol  lars george noticed that our mvcc class has consistency as the first 'c' when it should be 'concurrency'. the issue that named this class, hbase-4544 talks about 'concurrency' but then it went in as consistency (why has no one noticed this before now? thanks lars george) ",
        "label": 284
    },
    {
        "text": "when error occurs in this parent close false  of split  the split region cannot write or read  follow below steps to replay the problem:  1. change the splittransaction.java as below,just like mock the hdfs error. splittransaction.java       list<storefile> hstorefilestosplit = this.parent.close(false);       throw new ioexception(\"some unexpected error in close store files\");     2. update the regionserver code,restart;  3. create a table & put some data to the table;  4. split the table;  5. scan the table,then it'll fail. we can fix the bug just use the patch. ",
        "label": 519
    },
    {
        "text": "two configs for snapshot timeout and better defaults  one of the clusters timed out taking a snapshot for a disabled table. the table is big enough, and the master operation takes more than 1 min to complete. however while trying to increase the timeout, we noticed that there are two parameters with very similar names configuring different things: hbase.snapshot.master.timeout.millis is defined in snapshotdescriptionutils and is send to client side and used in disabled table snapshot. hbase.snapshot.master.timeoutmillis is defined in snapshotmanager and used as the timeout for the procedure execution. so, there are a couple of improvements that we can do: 1 min is too low for big tables. we need to set this to 5 min or 10 min by default. even a 6t table which is medium sized fails. unify the two timeouts into one. decide on either of them, and deprecate the other. use the biggest one for bc. add the timeout to hbase-default.xml. why do we even have a timeout for disabled table snapshots? the master is doing the work so we should not timeout in any case. ",
        "label": 198
    },
    {
        "text": "fix other code review comments about filterlist improvement  open this issue to fix conflict , run hadoopqa and gather other feedback. ",
        "label": 514
    },
    {
        "text": "testjoinedscanners fails in trunk build on hadoop  from https://builds.apache.org/job/hbase-trunk-on-hadoop-2.0.0/353/testreport/junit/org.apache.hadoop.hbase.regionserver/testjoinedscanners/testjoinedscanners/ : 2013-01-17 13:41:18,113 warn  [regionserver:2;juno.apache.org,44920,1358430022974.cacheflusher] hdfs.dfsinputstream(664): dfs read org.apache.hadoop.hdfs.blockmissingexception: could not obtain block: bp-624872226-67.195.138.61-1358430016098:blk_2914319395625853770_1224 file=/user/jenkins/hbase/testjoinedscanners/4742bbe27e31aed5dfafd8b1e6dede03/.tmp/4ca392f6a0154df79cfafd3448f2c8aa at org.apache.hadoop.hdfs.dfsinputstream.choosedatanode(dfsinputstream.java:734) at org.apache.hadoop.hdfs.dfsinputstream.blockseekto(dfsinputstream.java:448) at org.apache.hadoop.hdfs.dfsinputstream.readwithstrategy(dfsinputstream.java:645) at org.apache.hadoop.hdfs.dfsinputstream.read(dfsinputstream.java:689) at java.io.datainputstream.readfully(datainputstream.java:178) at org.apache.hadoop.hbase.io.hfile.fixedfiletrailer.readfromstream(fixedfiletrailer.java:428) at org.apache.hadoop.hbase.io.hfile.hfile.pickreaderversion(hfile.java:555) at org.apache.hadoop.hbase.io.hfile.hfile.createreaderwithencoding(hfile.java:599) at org.apache.hadoop.hbase.regionserver.storefile$reader.<init>(storefile.java:1294) at org.apache.hadoop.hbase.regionserver.storefile.open(storefile.java:525) at org.apache.hadoop.hbase.regionserver.storefile.createreader(storefile.java:628) at org.apache.hadoop.hbase.regionserver.hstore.validatestorefile(hstore.java:1259) at org.apache.hadoop.hbase.regionserver.hstore.commitfile(hstore.java:844) at org.apache.hadoop.hbase.regionserver.hstore.access$400(hstore.java:108) at org.apache.hadoop.hbase.regionserver.hstore$storeflusherimpl.commit(hstore.java:1841) at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1606) at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1504) at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:1445) at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushregion(memstoreflusher.java:410) at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushregion(memstoreflusher.java:384) at org.apache.hadoop.hbase.regionserver.memstoreflusher.run(memstoreflusher.java:247) at java.lang.thread.run(thread.java:662) ",
        "label": 248
    },
    {
        "text": " replication  add the ability to enable disable streams  this jira was initially in the scope of hbase-2201, but was pushed out since it has low value compared to the required effort (and when want to ship 0.90.0 rather soonish). we need to design a way to enable/disable replication streams in a determinate fashion. ",
        "label": 442
    },
    {
        "text": "dev support test patch sh should compile against hadoop alpha instead of hadoop  test-patch.sh currently does this:   $mvn clean test -dskiptests -dhadoop.profile=23 -d${project_name}patchprocess > $patch_dir/trunk23javacwarnings.txt 2>&1 we should compile against hadoop 2.0.0-alpha ",
        "label": 441
    },
    {
        "text": "testreplicationadmin failed on branch  i notice it on hbase-15095. see  https://builds.apache.org/job/precommit-hbase-build/95/artifact/patchprocess/patch-unit-hbase-server-jdk1.8.0.txt ",
        "label": 198
    },
    {
        "text": "htable mutaterow does not collect stats  we are trying to fix the stats implementation, by moving it out of the result object and into an rpc payload (but not the 'cell payload', just as part of the values returned from the request). this change will also us use easily switch to asyncprocess as the executor, and support stats, for nearly all the rpc calls. however, that means when you upgrade the client or server, you will lose stats visibility until the other side is upgraded. we could keep around the result based stats storage to accommodate the old api and send both stats back from the server (in each result and in the rpc payload).  note that we will still be wire compatible - protobufs mean we can just ride over the lack of information.  the other tricky part of this is that result has a non-interfaceaudience.private getstatistics() method (along with two interfaceaudience.private addresults and setstatistics methods), so we might need a release to deprecate the getstats() method before throwing it out? ",
        "label": 198
    },
    {
        "text": "add permission check for executeprocedures in accesscontroller  this is important, the actual refresh on rs is trigger by the executeprocedure call and it will pass some information. these information should not be fully trusted since anyone can all this method. we need to make sure that the actual data/state for a replication peer is always loaded from the replication storage, not from the parameter of the executeprocedure call. ",
        "label": 149
    },
    {
        "text": "bit of polish on hbase  hbase-1018 added new attributes to serverload and then in the master ui, outputted the new detailed serverload. looking at this w/ jgray, if hundreds of regions per server, the ui will blow out trying to list per region load. jgray suggested that region-level detail instead should show over on the regionserver itself. will change the ui to do as he suggests and just leave the new summary memory used, etc., showing in the load box over in master ui. doing math, we at first were afraid that the new fatter hserverload instance would overload the master when hundreds of regions but doing the math and counting the attributes that make up the new hsl, it looks like about ~128 characters per server every 3 second heartbeat, not too much.. so that should be fine. ",
        "label": 314
    },
    {
        "text": "nulloutputstream removed from guava  com.google.common.io.nulloutputstream was dropped in guava 15.0 in favor of com.google.common.io.bytestreams.nulloutputstream() which prevents projects on this artifact from upgrading from guava 14 to guava 15. error 2013-09-26 17:46:12,229 [hbase.master.masterfilesystem] bootstrap org.apache.hadoop.hbase.droppedsnapshotexception: region: -root-,,0         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1608)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1482)         at org.apache.hadoop.hbase.regionserver.hregion.doclose(hregion.java:1011)         at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:959)         at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:930)         at org.apache.hadoop.hbase.master.masterfilesystem.bootstrap(masterfilesystem.java:447)         at org.apache.hadoop.hbase.master.masterfilesystem.checkrootdir(masterfilesystem.java:387)         at org.apache.hadoop.hbase.master.masterfilesystem.createinitialfilesystemlayout(masterfilesystem.java:134)         at org.apache.hadoop.hbase.master.masterfilesystem.<init>(masterfilesystem.java:119)         at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:536)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:395)         at java.lang.thread.run(thread.java:680) caused by: java.lang.noclassdeffounderror: com/google/common/io/nulloutputstream         at org.apache.hadoop.hbase.io.hfile.hfilewriterv2.close(hfilewriterv2.java:374)         at org.apache.hadoop.hbase.regionserver.storefile$writer.close(storefile.java:1283)         at org.apache.hadoop.hbase.regionserver.store.internalflushcache(store.java:836)         at org.apache.hadoop.hbase.regionserver.store.flushcache(store.java:747)         at org.apache.hadoop.hbase.regionserver.store$storeflusherimpl.flushcache(store.java:2229)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1583)         ... 11 more caused by: java.lang.classnotfoundexception: com.google.common.io.nulloutputstream         at java.net.urlclassloader$1.run(urlclassloader.java:202)         at java.security.accesscontroller.doprivileged(native method)         at java.net.urlclassloader.findclass(urlclassloader.java:190)         at java.lang.classloader.loadclass(classloader.java:306)         at sun.misc.launcher$appclassloader.loadclass(launcher.java:301)         at java.lang.classloader.loadclass(classloader.java:247)         ... 17 more ",
        "label": 340
    },
    {
        "text": "collect p50  p75 and p95 stats  stats in current versions of hbase are currently exposed as avg, min and max. this gives a skewed view of performance as the outliers are usually the indicators of problems. please revise the stats collection framework to use true buckets and expose the p50, p75 and p95 values of these buckets through jmx. ",
        "label": 181
    },
    {
        "text": "testmasterwrongrs flaky in trunk  i think this is just a flaky test. i saw: java.lang.assertionerror: expected:<2> but was:<3>  on the first:  assertequals(2, cluster.getliveregionserverthreads().size()); my guess is that the 2 second sleep is not good enough. we should probably either force a heartbeat somehow, or hook in so we can wait until there's been a heartbeat, rather than sleeping a hardcoded amount of time. ",
        "label": 229
    },
    {
        "text": "update to zookeeper   i tried updating zk over in hbase-3230 but testhquorum.... is failing. ",
        "label": 229
    },
    {
        "text": "integrationtestbackuprestore should warn about missing config  i was running integrationtestbackuprestore on a newly created cluster running hbase-2.  it failed with: caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception(org.apache.hadoop.hbase.donotretryioexception): org.apache.hadoop.hbase.donotretryioexception: the procedure is not registered: rolllog-proc at org.apache.hadoop.hbase.master.masterrpcservices.execprocedure(masterrpcservices.java:817) at org.apache.hadoop.hbase.shaded.protobuf.generated.masterprotos$masterservice$2.callblockingmethod(masterprotos.java) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:406) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:134) at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:325) at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:305) hbase backup command warns about missing config up-front.  integrationtestbackuprestore should do the same. ",
        "label": 51
    },
    {
        "text": "scanquerymatcher setrow is not always being called on a row transition   when a scan is used with a batch, setrow is called on the same row within each call to next. since setrow resets deletes, the result set contains kvs that should have been suppressed by a delete. ",
        "label": 300
    },
    {
        "text": "pass buffersize param to filelinkinputstream constructor within filelink open method  and remove unnecessary import packages   there are 2 minor fixes:  1) the call to filelinkinputstream constructor missed the buffer size param within filelink.open(final filesystem fs, int buffersize) implementation. 2) removed the unnecessary java pkg import:  import java.util.arraylist;  import org.apache.hadoop.conf.configuration;  import org.apache.hadoop.hbase.hconstants;  import org.apache.hadoop.hbase.util.fsutils; ",
        "label": 255
    },
    {
        "text": "testsecureloadincrementalhfiles tests timed out in trunk build on apache  opening an issue to keep an eye on these tests. looking at history, they've been failing irregularly over time. https://builds.apache.org/job/hbase-trunk/6907/testreport/ ... has a few tests in this suite timing out: org.junit.runners.model.testtimedoutexception: test timed out after 60000 milliseconds at java.lang.object.wait(native method) at java.lang.object.wait(object.java:461) at io.netty.util.concurrent.defaultpromise.await0(defaultpromise.java:355) at io.netty.util.concurrent.defaultpromise.await(defaultpromise.java:266) at io.netty.util.concurrent.abstractfuture.get(abstractfuture.java:42) at org.apache.hadoop.hbase.ipc.asyncrpcclient.call(asyncrpcclient.java:248) at org.apache.hadoop.hbase.ipc.abstractrpcclient.callblockingmethod(abstractrpcclient.java:217) at org.apache.hadoop.hbase.ipc.abstractrpcclient$blockingrpcchannelimplementation.callblockingmethod(abstractrpcclient.java:295) at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$blockingstub.disabletable(masterprotos.java:60544) at org.apache.hadoop.hbase.client.connectionimplementation$2.disabletable(connectionimplementation.java:1533) at org.apache.hadoop.hbase.client.hbaseadmin$10.call(hbaseadmin.java:1236) at org.apache.hadoop.hbase.client.hbaseadmin$10.call(hbaseadmin.java:1230) at org.apache.hadoop.hbase.client.rpcretryingcallerimpl.callwithretries(rpcretryingcallerimpl.java:118) at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:4115) at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:4108) at org.apache.hadoop.hbase.client.hbaseadmin.disabletableasync(hbaseadmin.java:1229) at org.apache.hadoop.hbase.client.hbaseadmin.disabletable(hbaseadmin.java:1188) at org.apache.hadoop.hbase.hbasetestingutility.deletetable(hbasetestingutility.java:1936) at org.apache.hadoop.hbase.mapreduce.testloadincrementalhfiles.runtest(testloadincrementalhfiles.java:275) at org.apache.hadoop.hbase.mapreduce.testloadincrementalhfiles.runtest(testloadincrementalhfiles.java:230) at org.apache.hadoop.hbase.mapreduce.testloadincrementalhfiles.runtest(testloadincrementalhfiles.java:217) at org.apache.hadoop.hbase.mapreduce.testloadincrementalhfiles.testregioncrossinghfilesplit(testloadincrementalhfiles.java:193) at org.apache.hadoop.hbase.mapreduce.testloadincrementalhfiles.testregioncrossinghfilesplit(testloadincrementalhfiles.java:171) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:50) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:47) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.internal.runners.statements.failontimeout$callablestatement.call(failontimeout.java:298) at org.junit.internal.runners.statements.failontimeout$callablestatement.call(failontimeout.java:292) at java.util.concurrent.futuretask.run(futuretask.java:262) at java.lang.thread.run(thread.java:744) ",
        "label": 314
    },
    {
        "text": "port hbase 'unable to run hbck on a secure cluster' to  ",
        "label": 441
    },
    {
        "text": "regionmetricsstorage incrnumericmetric is called too often  running an hbase scan load through the profiler revealed that regionmetricsstorage.incrnumericmetric is called way too often. it turns out that we make this call for each kv in storescanner.next(...).  incrementing atomiclong requires expensive memory barriers. the observation here is that storescanner.next(...) can maintain a simple   long in its internal loop and only update the metric upon exit. thus the atomiclong is not updated nearly as often. that cuts about 10% runtime from scan only load (i'll quantify this better soon). ",
        "label": 299
    },
    {
        "text": "add option to hfile tool to produce basic stats  in looking at hbase-3421 i wrote a small tool to scan an hfile and produce some basic statistics about it: min/mean/max key size, value size (uncompressed) min/mean/max number of columns per row (uncompressed) min/mean/max number of bytes per row (uncompressed) the key of the largest row ",
        "label": 309
    },
    {
        "text": "remove the deprecated methods in admin interface  for api cleanup, and will make the work in hbase-21718 a little easier. ",
        "label": 149
    },
    {
        "text": " fb  fast fail client operations if the regionserver is repeatedly unreachable   we have seen occassional rsw reboots in the production cluster. on the client end, the reduction in the operation throughput is a lot more than the %-age of nodes disconnected. this is because operations to the disconnected machines are holding up resources that could otherwise be used for successful operation. this change enables the client to detect when there are repeated failures to a regionserver, and fast fail operations so we do not hold up resources. ",
        "label": 34
    },
    {
        "text": " packaging  assembly tars up hbase in a subdir  i e  after untar hbase has a subdir named  reported by roman. ",
        "label": 382
    },
    {
        "text": "online snapshots  hbase-6055 will be closed when the offline snapshots pieces get merged with trunk. this umbrella issue has all the online snapshot specific patches. this will get merged once one of the implementations makes it into trunk. other flavors of online snapshots can then be done as normal patches instead of on a development branch. (was: hbase-6055 will be closed when the online snapshots pieces get merged with trunk. this umbrella issue has all the online snapshot specific patches. this will get merged once one of the implementations makes it into trunk. other flavors of online snapshots can then be done as normal patches instead of on a development branch.) (not a fan of the quick edit descirption jira feature) ",
        "label": 248
    },
    {
        "text": " regression  unable to delete a row in the future  deleting in the future doesn't work because kv resets everything to now. ",
        "label": 229
    },
    {
        "text": "hconnectionmanger listtables returns empty list if exception  though there may be many tables present   its a problem because commonly a check for existence will get list of current tables. yesterday saw problem when .meta. went off line. a piece of client code was asking for list of tables when .meta. was offline, it was getting back an empty list because listtables do while was seeing 'org.apache.hadoop.hbase.notservingregionexception: .meta.,,1' problem is the do while in hcm.listtables goes as long as startrow does not equal last_row but startrow is initialized with empty_start_row which is equal to last_row. ",
        "label": 86
    },
    {
        "text": "mvn  dhadoop profile  dhadoop two version snapshot fails because of undef class error wrt o a h idgenerator    <testcase time=\"1.009\" classname=\"org.apache.hadoop.hbase.replication.testmasterreplication\" name=\"testcyclicreplication\">     <error message=\"org/apache/hadoop/util/idgenerator\" type=\"java.lang.noclassdeffounderror\">java.lang.noclassdeffounderror: org/apache/hadoop/util/idgenerator         at org.apache.hadoop.hdfs.server.namenode.namenode.format(namenode.java:752)         at org.apache.hadoop.hdfs.server.namenode.namenode.format(namenode.java:261)         at org.apache.hadoop.hdfs.dfstestutil.formatnamenode(dfstestutil.java:146)         at org.apache.hadoop.hdfs.minidfscluster.createnamenodesandsetconf(minidfscluster.java:775)         at org.apache.hadoop.hdfs.minidfscluster.initminidfscluster(minidfscluster.java:642)         at org.apache.hadoop.hdfs.minidfscluster.&lt;init&gt;(minidfscluster.java:585) .... ",
        "label": 248
    },
    {
        "text": "example health checker script  ",
        "label": 462
    },
    {
        "text": "potentially improve block locality during major compaction for old regions  this might be a specific use case. but we have some regions which are no longer written to (due to the key). those regions have 1 store file and they are very old, they haven't been written to in a while. we still use these regions to read from so locality would be nice. i propose putting a configuration option: something like  hbase.hstore.min.locality.to.skip.major.compact [between 0 and 1] such that you can decide whether or not to skip major compaction for an old region with a single store file. i'll attach a patch, let me know what you guys think. ",
        "label": 522
    },
    {
        "text": "address ruby static analysis for shell module  ",
        "label": 320
    },
    {
        "text": "hbase broke replication  after running with hbase-6646 and replication enabled i encountered this: 2012-09-17 20:04:08,111 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: opening log for replication va1r3s24%2c10304%2c1347911704238.1347911706318 at 78617132 2012-09-17 20:04:08,120 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: break on ioe: hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2c10304%2c1347911704238.1347911706318, entrystart=78641557, pos=78771200, end=78771200, edit=84 2012-09-17 20:04:08,120 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: currentnboperations:164529 and seenentries:84 and size: 154068 2012-09-17 20:04:08,120 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: replicating 84 2012-09-17 20:04:08,146 info org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager: going to report log #va1r3s24%2c10304%2c1347911704238.1347911706318 for position 78771200 in hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2c10304%2c1347911704238.1347911706318 2012-09-17 20:04:08,158 info org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager: removing 0 logs in the list: [] 2012-09-17 20:04:08,158 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: replicated in total: 93234 2012-09-17 20:04:08,158 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: opening log for replication va1r3s24%2c10304%2c1347911704238.1347911706318 at 78771200 2012-09-17 20:04:08,163 error org.apache.hadoop.hbase.replication.regionserver.replicationsource: unexpected exception in replicationsource, currentpath=hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2c10304%2c1347911704238.1347911706318 java.lang.indexoutofboundsexception         at java.io.datainputstream.readfully(datainputstream.java:175)         at org.apache.hadoop.io.dataoutputbuffer$buffer.write(dataoutputbuffer.java:63)         at org.apache.hadoop.io.dataoutputbuffer.write(dataoutputbuffer.java:101)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:2001)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1901)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1947)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.next(sequencefilelogreader.java:235)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.readallentriestoreplicateornextfile(replicationsource.java:394)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:307) there's something weird at the end of the file and it's killing replication. we used to just retry. ",
        "label": 139
    },
    {
        "text": "testmaster testmasteropswhilesplitting fails  this is the top flaky test: java.lang.assertionerror: expected:<3> but was:<1> at org.apache.hadoop.hbase.master.testmaster.testmasteropswhilesplitting(testmaster.java:121) after brief check, the test failure seems to be introduced by hbase-19127 ",
        "label": 499
    },
    {
        "text": "bound table pool size in thrift server  looking at the thrifthbaseservicehandler, it has this code:   thrifthbaseservicehandler(configuration conf) {     htablepool = new htablepool(conf, integer.max_value);   } it seems like this is excessive. we should have a config to set this from outside and default it to a sane upper limit. ",
        "label": 285
    },
    {
        "text": "npe in tableinputformatbase tablerecordreader restart if zoo cfg is wrong on tasktrackers  minor nit. if zoo.cfg is missing or incorrect on the task trackers, you'll get this npe when the child task starts: 09/05/08 02:46:15 info mapred.jobclient: task id : attempt_200905080240_0002_m_000000_2, status : failed  java.lang.nullpointerexception  at org.apache.hadoop.hbase.mapred.tableinputformatbase$tablerecordreader.restart(tableinputformatbase.java:117)  at org.apache.hadoop.hbase.mapred.tableinputformatbase$tablerecordreader.init(tableinputformatbase.java:128)  at org.apache.hadoop.hbase.mapred.tableinputformatbase.getrecordreader(tableinputformatbase.java:249)  at org.apache.hadoop.mapred.maptask.runoldmapper(maptask.java:336)  at org.apache.hadoop.mapred.maptask.run(maptask.java:305)  at org.apache.hadoop.mapred.child.main(child.java:170) ",
        "label": 38
    },
    {
        "text": "bug in calls to regionobsever postscannerfilterrow  just noticed that while looking at hbase-10047.  in 0.94 (and presumably in trunk, will check later) we have this:     protected boolean nextrow(byte [] currentrow, int offset, short length) throws ioexception {      ...       if (this.region.getcoprocessorhost() != null) {         return this.region.getcoprocessorhost().postscannerfilterrow(this, currentrow);       }       return true;     } notice how we only pass currentrow into the coprocessor, but not offset and length. anything using this hook currently is 100% broken. the hook was added in 0.94.5 (hbase-5664), it never worked correctly. anoop sam john, you had added the hook. do you still need it? we can either remove it (i'd prefer that in the light of the performance issued observed in hbase-10047, we can leave the stub in baseregionobserver in 0.94, but document that it is no-op), or we'd have to have change its signature to be able to pass offset and length as well. since nobody noticed nobody is using this hook currently, so both should be valid options. (making a new standalone copy of the rowkey just to pass into this method absolutely out of the question for performance reasons). ",
        "label": 46
    },
    {
        "text": "reenable test of filterlist using must pass one and two familyfilters  the hbase-18410 feature branch started with the test from hbase-18957 disabled. we need to enable it before merging. ",
        "label": 514
    },
    {
        "text": "disable integrationtestrebalanceandkillserverstargeted temporarily  disabling integrationtestrebalanceandkillserverstargeted until hbase-7520 is fixed so can move forward with getting hbase-it running on bigtop infra. ",
        "label": 314
    },
    {
        "text": "bulk load and other utilities should not create tables for user  loadincrementalhfiles and importtsv will create a table with the default setting when the target table does not exist. i think this is an anti-feature. neither tool provide a mechanism for the user to configure the creation parameters of that table, resulting in a new table with the default settings. i think it is unlikely that the default settings are what the user actually wants. in the event of a table-name typo, that means data is silently loaded into the wrong place. the tools should error when the destination table does not exist. ",
        "label": 53
    },
    {
        "text": "eliminate use of readwritelock in regionobserver coprocessor invocation  follow-up to a discussion on the dev list: http://search-hadoop.com/m/joovv1uajbp the coprocessorhost reentrantreadwritelock is imposing some overhead on data read/write operations, even when no coprocessors are loaded. currently execution of regioncoprocessorhost pre/postxxx() methods are guarded by acquiring the coprocessor read lock. this is used to prevent coprocessor registration from modifying the coprocessor collection while upcall hooks are in progress. on further discussion, and looking at the locking in hregion, it should be sufficient to just use a copyonwritearraylist for the coprocessor collection. we can then remove the coprocessor lock and eliminate the associated overhead without having to special case the \"no loaded coprocessors\" condition. ",
        "label": 180
    },
    {
        "text": "javadoc error  during an ant build, i get the following warning:  [javadoc] javadoc: warning - error fetching url: http://java.sun.com/javase/6/docs/api/index.html/package-list i suspect that this is related to build.xml, but don't know for certain. this needs to be fixed. ",
        "label": 314
    },
    {
        "text": "port hbase 'disable show table metrics in bulk loader' to  ",
        "label": 441
    },
    {
        "text": "set name for flushhandler thread  the flushhandler thread in memstoreflusher class uses default thread name (thread -xx). this is un-intentional and also confusing in case when there are multiple handlers. current stack trace looks like this: \"thread-18\" prio=10 tid=0x00007f4e8cb21800 nid=0x356e waiting on condition [0x00007f4e6d49a000]    java.lang.thread.state: timed_waiting (parking) at sun.misc.unsafe.park(native method) - parking to wait for  <0x00000004e5684b00> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject) at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:196) at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.awaitnanos(abstractqueuedsynchronizer.java:2025) at java.util.concurrent.delayqueue.poll(delayqueue.java:201) at java.util.concurrent.delayqueue.poll(delayqueue.java:39) at org.apache.hadoop.hbase.regionserver.memstoreflusher$flushhandler.run(memstoreflusher.java:228) at java.lang.thread.run(thread.java:662) ",
        "label": 199
    },
    {
        "text": "introducing waitforcondition function into test cases  recently i'm working on flaky test cases and found we have many places using while loop and sleep to wait for a condition to be true. there are several issues in existing ways: 1) many similar code doing the same thing  2) when time out happens, different errors are reported without explicitly indicating a time out situation  3) when we want to increase the max timeout value to verify if a test case fails due to a not-enough time out value, we have to recompile & redeploy code i propose to create a waitforcondition function as a test utility function like the following:     public interface waitcheck {         public boolean check() ;     }     public boolean waitforcondition(int timeoutinmilliseconds, int checkintervalinmilliseconds, waitcheck s)             throws interruptedexception {         int multiplier = 1;         string multiplierprop = system.getproperty(\"extremewaitmultiplier\");         if(multiplierprop != null) {             multiplier = integer.parseint(multiplierprop);             if(multiplier < 1) {                 log.warn(string.format(\"invalid extremewaitmultiplier property value:%s. is ignored.\", multiplierprop));                 multiplier = 1;             }         }         int timeelapsed = 0;         while(timeelapsed < timeoutinmilliseconds * multiplier) {             if(s.check()) {                 return true;             }             thread.sleep(checkintervalinmilliseconds);             timeelapsed += checkintervalinmilliseconds;         }         asserttrue(\"waitforcondition failed due to time out(\" + timeoutinmilliseconds + \" milliseconds expired)\",                 false);         return false;     } by doing the above way, there are several advantages: 1) clearly report time out error when such situation happens  2) use system property extremewaitmultiplier to increase max time out dynamically for a quick verification  3) standardize current wait situations pleas let me know what your thoughts on this. thanks,  -jeffrey ",
        "label": 233
    },
    {
        "text": "auto drop rollback snapshot for snapshot restore  below is an excerpt from snapshot restore javadoc:    * restore the specified snapshot on the original table. (the table must be disabled)    * before restoring the table, a new snapshot with the current table state is created.    * in case of failure, the table will be rolled back to the its original state. we can improve the handling of rollbacksnapshot in two ways: 1. give better name to the rollbacksnapshot (adding '-for-rollback-' ). currently the name is of the form:  string rollbacksnapshot = snapshotname + \"-\" + environmentedgemanager.currenttimemillis(); 2. drop rollbacksnapshot at the end of restoresnapshot() if the restore is successful. we can introduce new config param, named 'hbase.snapshot.restore.drop.rollback', to keep compatibility with current behavior. ",
        "label": 309
    },
    {
        "text": "failed split  ioe 'file is corrupt '   sync length not being written out to sequencefile  we saw this on one of our clusters: 2010-09-07 18:07:16,229 warn org.apache.hadoop.hbase.master.regionserveroperationqueue: failed processing: processservershutdown of sv4borg18,60020,1283516293515; putting onto delayed todo queue java.io.ioexception: file is corrupt!         at org.apache.hadoop.io.sequencefile$reader.readrecordlength(sequencefile.java:1907)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1932)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1837)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1883)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.next(sequencefilelogreader.java:121)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.next(sequencefilelogreader.java:113)         at org.apache.hadoop.hbase.regionserver.wal.hlog.parsehlog(hlog.java:1493)         at org.apache.hadoop.hbase.regionserver.wal.hlog.splitlog(hlog.java:1256)         at org.apache.hadoop.hbase.regionserver.wal.hlog.splitlog(hlog.java:1143)         at org.apache.hadoop.hbase.master.processservershutdown.process(processservershutdown.java:299)         at org.apache.hadoop.hbase.master.regionserveroperationqueue.process(regionserveroperationqueue.java:147)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:532) because it was an ioe, it got requeued. each time around we failed on it again. a few things: + this exception needs to add filename and the position in file at which problem found.  + need to commit little patch over in hbase-2889 that outputs position and ordinal of wal edit because it helps diagnose these kinds of issues.  + we should be able to skip the bad edit; just postion ourselves at byte past the bad sync and start reading again  + there must be something about our setup that makes it so we fail write of the sync 16 random bytes that make up the sf 'sync' marker though oddly for one of the files, the sync failure happens at 1/3rd of the way into a 64mb wal, edit #2000 out of 130k odd edits. ",
        "label": 314
    },
    {
        "text": "add longcomparator for filter  add longcomparator for filter. ",
        "label": 411
    },
    {
        "text": "tableinputformatbase createrecordreader  doesn't initialize tablerecordreader which causes npe  while running hive(0.9.0) query over hbase(0.94.0) with hive-hbase-handler, there always throws a null pointer exception on scanner object. since the tableinputformatbase#createrecordreader() missed the initialization of tablerecordreader object. the scanner will be null in that case. this issue causes hive query fails. ",
        "label": 239
    },
    {
        "text": "remove coprocessorprotocol support and implementations  the coprocessorprotocol mechanism for registering and calling coprocessor endpoints has been supplanted by protocol buffer based service registrations in hbase-5448. this means the writable-based coprocessorprotocol mechanism will need to be removed at some point. in discussion on the dev list, general consensus was that it is better to remove the coprocessorprotocol mechanism completely in 0.96, where other backwards-incompatible changes are being made, and before it is adopted by more clients:  http://mail-archives.apache.org/mod_mbox/hbase-dev/201209.mbox/%3ccadfysxtvqeu3sifywdzci9kkpw1hf9eoakvgfocbwrpskdacng%40mail.gmail.com%3e this is an umbrella issue to track what changes are required to completely remove the coprocessorprotocol support. ",
        "label": 314
    },
    {
        "text": "allow the canary in regionserver mode to try all regions on the server  not just one  we want a pretty in-depth canary that will try every region on a cluster. when doing that for the whole cluster one machine is too slow, so we wanted to split it up and have each regionserver run a canary. that works however the canary does less work as it just tries one random region. lets add a flag that will allow the canary to try all regions on a regionserver. ",
        "label": 397
    },
    {
        "text": "break circle replication when doing bulkload  when enabled master-master bulkload replication, hfiles will be replicated circularly between two clusters ",
        "label": 486
    },
    {
        "text": "unregister replicationsource metric bean when the replication source thread is terminated  each replication source thread will register a metric bean to show its statistics. the source threads will be terminated when region server exit and the metric beans will be removed. however, replication source thread may also be terminated when user removing the peer explicitly or it just takes a recover queue and finished replicating the queued hlogs. in these situations, the metric bean won't be unregistered and user may be confused to always see the statistics from terminated replication source threads. maybe, it is more clear to remove the metric bean after replication source thread terminated? then, the statistics will only from active replication sources. ",
        "label": 238
    },
    {
        "text": "call me maybe hbase links haved moved  the links to the yammer engineering blog have moved. please use the following links in section 83.5. network consistency and partition tolerance http://old.eng.yammer.com/call-me-maybe-hbase/  http://old.eng.yammer.com/call-me-maybe-hbase-addendum/ thanks ",
        "label": 38
    },
    {
        "text": "testreadwriteconsistencycontrol should be renamed  testreadwriteconsistencycontrol tests multiversionconsistencycontrol so it should be named testmultiversionconsistencycontrol ",
        "label": 103
    },
    {
        "text": "hfileblockdefaultencodingcontext isn't thread safe but is used by all readers  breaks column encoding  getting an error when opening a scanner on a file that has no encoding. ",
        "label": 229
    },
    {
        "text": "change local mode back to one rs thread by default  hbase-10340/hbase-9892 changed the default number of rs threads from 1 to 3. this is confusing. this is to set the default back to 1. ",
        "label": 286
    },
    {
        "text": "client side changes for moving peer modification from zk watcher to procedure  ",
        "label": 187
    },
    {
        "text": "hfile's hdfs op latency sampling code is not used  hfilereaderv2 calls hfile#offerreadlatency and hfilewriterv2 calls hfile#offerwritelatency but the samples are never retrieved. there are no callers of hfile#getreadlatenciesnanos, hfile#getwritelatenciesnanos, and related. the three arrayblockingqueues we are using as sample buffers in hfile will fill quickly and are never drained. there are also no callers of hfile#getreadtimems or hfile#getwritetimems, and related, so we are incrementing a set of atomiclong counters that will never be read nor reset. we are calling system.nanotime in block read and write paths twice but not utilizing the measurements. we should hook this code back up to metrics or remove it. we are also not using hfile#getchecksumfailurescount anywhere but in some unit test code. ",
        "label": 38
    },
    {
        "text": "update documentation about major compaction algorithm  [14:20:38] <jdcryans> seems that there's http://hbase.apache.org/book.html#compaction and http://hbase.apache.org/book.html#managed.compactions  [14:20:56] <jdcryans> the latter doesn't say much, except that you should manage them  [14:21:44] <jdcryans> the former gives a good description of the old selection algo [14:45:25] <jdcryans> this is the new selection algo since c5 / 0.96.0: https://issues.apache.org/jira/browse/hbase-7842 ",
        "label": 330
    },
    {
        "text": "hbaseadmin checkhbaseavailable  doesn't close zookeeper connections  hbaseadmin.checkhbaseavailable(conf) clones the passed connection, later in hbaseadmin constructor the connection is cloned again. thus a new hconnection object with zookeeper connections is created. ",
        "label": 425
    },
    {
        "text": "testcompactionindeadregionserver is top of the flakies charts   the test came in recently as part of hbase-17712 \"remove/simplify the logic of regionscannerimpl.handlefilenotfound\" [~apache9] when you have a chance, help me out. i was going to just remove the test since it made no sense to me but then i saw you wrote it (smile). when the region.compact(true); is called on the end, what is supposed to be going on? when i trace, the compact is not done because the region is not writeenabled (we check if region is writeenabled down in store before we go ahead and compact). so, i thought the problem was that the region reference was stale because it came from the rstosuspend which had just been killed. after a while, i figured that you intend the region reference to be stale so you can try an append after the wal has been taken over by wal splitter. but the writeenabled flag is set so compactions don't run. i tried unsetting this flag and closed flags and but compaction won't run. was this your intent sir? if so, i'll work w/ it np. just looking for clarity. thanks. ",
        "label": 441
    },
    {
        "text": "document and test rolling updates from    i think 1.0 should be rolling upgradable from 0.98 unless we break it intentionally for a specific reason. unless there is such an issue, lets document that 1.0 and 0.98 should be rolling upgrade compatible. we should also test this before the 0.99 release. ",
        "label": 314
    },
    {
        "text": "some cleanup of log messages in rs and m  i did a little edit of logging. we do way too much but am not going to do a big overhaul just yet. here's a few small changes saving a few lines, some redundancy, and making others look like surrounding log lines. ",
        "label": 314
    },
    {
        "text": "backport hbase and hbase  avoid lazy seek  placeholder for lars when looking at 0.94.9. hbase-8001, hbase-8012 ",
        "label": 286
    },
    {
        "text": "icv has a subtle race condition only visible under high load  icv demonstrates a race condition under high load. the result is a duplicate keyvalue with the same timestamp, at first in the memcache, and in hfile, then both in hfile. the get/scan code doesnt know which one to read, and picks one arbitrarily. one of the keyvalues is correct, one is incorrect. what happens at a deeper level: we start an icv a snapshot happens and moves the memstore to the snapshot the icv code puts a key-value into memstore that has the same timestamp as a keyvalue in the snapshot. this is a deep race condition and several attempts to fix it failed in production here at su. this issue is about a more permanent fix. ",
        "label": 547
    },
    {
        "text": "document release managers for non deprecated branches  new development goes against trunk and is backported as desired to existing release branches. from what i have seen on the jira, it looks like each branch's release manager makes the call on backporting a particular issue. we should document both this norm and who the relevant release manager is for each branch. in the current docs, i'd suggest adding the rm list to the \"codelines\" section (18.11.1) and add a brief explanation of pinging the rm as a new section after \"submitting a patch again\" (18.12.6). post hbase-4593, the note about pinging a prior branch rm should just go as a bullet in the \"patch workflow.\" ",
        "label": 330
    },
    {
        "text": "add more info on zombies to test patch sh  ",
        "label": 314
    },
    {
        "text": "introduce hregion dominibatchmutation   from anoop under thread 'can there be a dominibatchdelete in hregion': the htable#delete(list<delete>) groups the deletes for the same rs and make one n/w call only. but within the rs, there will be n number of delete calls on the region one by one. this will include n number of hlog write and sync. if this also can be grouped can we get better performance for the multi row delete. i have made the new minibatchdelete () and made the htable#delete(list<delete>) to call this new batch delete.  just tested initially with the one node cluster. in that itself i am getting a performance boost which is very much promising.  only one cf and qualifier.  10k total rows delete with a batch of 100 deletes. only deletes happening on the table from one thread.  with the new way the net time taken is reduced by more than 1/10  will test in a 4 node cluster also. i think it will worth doing this change. ",
        "label": 46
    },
    {
        "text": "override needbalance in stochasticloadbalancer  stochasticloadbalancer includes cost functions to compute the cost of region rount, r/w qps, table load, region locality, memstore size, and storefile size. every cost function returns a number between 0 and 1 inclusive and the computed costs are scaled by their respective multipliers. the bigger multiplier means that the respective cost function have the bigger weight. but needbalance decide whether to balance only by region count and doesn't consider r/w qps, locality even you config these cost function with bigger multiplier. stochasticloadbalancer should override needbalance and decide whether to balance by it's configs of cost functions. add one new config hbase.master.balancer.stochastic.mincostneedbalance, cluster need balance when (total cost / sum multiplier) > mincostneedbalance. ",
        "label": 187
    },
    {
        "text": "add support for  and byte integers in orderedbytes and provide types  ",
        "label": 196
    },
    {
        "text": "make the method getcurrentpoolsize of htablepool public  we use htablepool to manager opened htable in our applications. we want to track the usage of htablepool for different table names. then we discover that htablepool#getcurrentpoolsize could help us:   int getcurrentpoolsize(string tablename) {     return tables.size(tablename);   } however, this method could only be called in the hbase client package. can we make this method public? ",
        "label": 238
    },
    {
        "text": "check admin table to ensure all operations go via accesscontrol  a cursory review of admin interface has a bunch of methods as open, with out accesscontrol checks. for example, procedure executor has not check on it. this issue is about given the admin and table interfaces a once-over to see what is missing and to fill in access control where missing. this is a follow-on from work over in hbase-19048 ",
        "label": 60
    },
    {
        "text": "introduce append addcolumn as a replacement for append add  we have put#addcolumn and increment#addcolumn but there is no append#addcolumn. we should add append#addcolumn for consistency. ",
        "label": 226
    },
    {
        "text": "add read and write qps metrics at server level and table level  use hbase\u2018s existing class dropwizardmeter to collect read and write qps. the collected location is the same as metrics readrequestscount and writerequestscount. ",
        "label": 59
    },
    {
        "text": "document how to use shell enhancements from hbase  hbase-5548 introduced new behavior for shell commands like 'list' to make them act more ruby-like. there is no documentation for this in the refguide. we should   1) have an example in the shell sectoin  2) document the that new '=> #xxx.x..' line is expected we can probably lift a bunch of docs from hbase-5548. ",
        "label": 248
    },
    {
        "text": " hbasecon  update the hbasecon archive  the hbase pmc now hosts the hbasecon archive. it is here: http://hbase.apache.org/www.hbasecon.com/ the homepage needs an edit to remove the 'hosted by cloudera' but also to fill in hbasecon east 2016, hbasecon west 2017, and the recent hbasecon asia 2017. the slideshare for hbasecon east needs to have video links added. ditto for hbasecon west and hbasecon asia. todo: a blog post on apache hbase site that points at the new updated archive. there are also some great pictures from hbasecon asia at least that could do w/ uploading. links: https://www.youtube.com/channel/ucy25rifxwrbokfg-2cm83bq/videos?sort=dd&shelf_id=0&view=0  https://www.slideshare.net/search/slideshow?searchfrom=header&q=hbaseconeast2016  https://www.slideshare.net/search/slideshow?searchfrom=header&q=hbasecon2017&ud=any&ft=all&lang=en&sort=  look for achieving multitenancy.mp4  https://www.slideshare.net/search/slideshow?searchfrom=header&q=hbaseconasia2017&ud=any&ft=all&lang=en&sort= marking this as a beginner task since it mostly admin. i keep meaning to just do it but never get to it. ",
        "label": 109
    },
    {
        "text": "ioe  stream closed exception all over logs  from the list: 2008-07-22 12:29:52,759 warn org.apache.hadoop.hbase.regionserver.hstore: exception closing reader for 242866774/new java.io.ioexception: stream closed    at org.apache.hadoop.dfs.dfsclient$dfsinputstream.close(dfsclient.java:1319)    at java.io.filterinputstream.close(filterinputstream.java:155)    at org.apache.hadoop.io.sequencefile$reader.close(sequencefile.java:1581)    at org.apache.hadoop.io.mapfile$reader.close(mapfile.java:577)    at org.apache.hadoop.hbase.regionserver.hstore.closecompactionreaders(hstore.java:917)    at org.apache.hadoop.hbase.regionserver.hstore.compacthstorefiles(hstore.java:910)    at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:787)    at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:887)    at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:847)    at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:84)  ",
        "label": 314
    },
    {
        "text": "add max rpc version to meta region server zk node   for clients to boot strap themselves they need to know the max rpc version that the meta server will accept. we should add that to the zookeeper node. ",
        "label": 154
    },
    {
        "text": "htabledescriptor warn is not actionable  notice this while testing another patch in standalone mode. i see warn lines like the following 2015-08-25 14:19:47,057 warn  [1758008124@qtp-1276709283-0] hbase.htabledescriptor: use addcoprocessor* methods to add a coprocessor instead this appears to come from htabledescriptor#setvalue(bytes,bytes). ",
        "label": 284
    },
    {
        "text": " fb  prevent oom possibility due to per connection responsequeue being unbounded  the per connection responsequeue is an unbounded queue. the request handler threads today try to send the response in line, but if things start to backup, the response is sent via a per connection responder thread. this intermediate queue, because it has no bounds, can be another source of ooms. [have not looked at this issue in trunk. so it may or may not be applicable there.] ",
        "label": 317
    },
    {
        "text": "add convenience method to hbaseadmin to get a collection of hregioninfo objects for each table  when we try to list all of the regions associated with a table, (e.g. in webapps/master/master.jsp or thrift/thriftserver.java#gettableregions, we either have to make do with hserverinfo objects or use the htable#getregionsinfo method. it may be useful to add a method to the hbaseadmin object instead. ",
        "label": 326
    },
    {
        "text": "fixing the non deterministic failure of testhfileoutputformat   the test creates regions in the meta region and waits for the master to allot them to regionservers. when the favored nodes were set, the timout was not completely reliable to say that they will be set. instead setting the favored nodes while creating the regions directly in the starting would ensure that the master creates the regions assigning selected favored nodes. ",
        "label": 154
    },
    {
        "text": "bin graceful stop sh does not return the balancer to original state  graceful_stop.sh starts by explicitly disabling the balancer. it should make note of the original balancer state and then restore that state upon completion. ",
        "label": 230
    },
    {
        "text": "hbase metrics miss all operations submitted via multiaction  a client application (loadtesttool) calls put() on htables. internally to the hbase client those puts are batched into multiactions. the total number of put operations shown in the regionserver's put metrics histogram never increases from 0 even though millions of such operations are made. needless to say the latency for those operations are not measured either. the value of hbase-5533 metrics are suspect given the client will batch put and delete ops like this. i had a fix in progress but hbase-6284 messed it up. before, multiaction processing in hregionserver would distingush between puts and deletes and dispatch them separately. it was easy to account for the time for them. now both puts and deletes are submitted in batch together as mutations. ",
        "label": 38
    },
    {
        "text": "alter hbase meta  hbase:meta is currently hardcoded. its schema cannot be change. this issue is about allowing edits to hbase:meta schema. it will allow our being able to set encodings such as the block-with-indexes which will help quell cpu usage on host carrying hbase:meta. a dynamic hbase:meta is first step on road to being able to split meta. ",
        "label": 314
    },
    {
        "text": "compile hbase against hadoop  pom contains a profile for hadoop-0.20 and one for hadoop-0.23, but not one for hadoop-0.22. when overriding hadoop.version to 0.22, then the (compile-time) dependency on hadoop-annotations cannot be met.  that exists on 0.23 and 0.24/trunk, but not on 0.22. ",
        "label": 245
    },
    {
        "text": "on master start  deadlock if refresh ui  playing w/ mttr recovery on trunk, master starting up deadlocked: waiting to finish active master initialization: \"activemastermanager\" daemon prio=10 tid=0x00007fafb5dc3800 nid=0x5fb5 waiting for monitor entry [0x00007faf8f57d000]    java.lang.thread.state: blocked (on object monitor)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.getkeepalivezookeeperwatcher(connectionmanager.java:1683)         - waiting to lock <0x000000064ab4b9a8> (a java.lang.object)         at org.apache.hadoop.hbase.client.zookeeperregistry.getmetaregionlocation(zookeeperregistry.java:53)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1029)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:989)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.getregionlocation(connectionmanager.java:830)         at org.apache.hadoop.hbase.client.connectionadapter.getregionlocation(connectionadapter.java:305)         at org.apache.hadoop.hbase.client.regionservercallable.prepare(regionservercallable.java:77)         at org.apache.hadoop.hbase.client.scannercallable.prepare(scannercallable.java:118)         at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:101)         at org.apache.hadoop.hbase.client.clientscanner.nextscanner(clientscanner.java:264)         at org.apache.hadoop.hbase.client.clientscanner.initializescannerinconstruction(clientscanner.java:169)         at org.apache.hadoop.hbase.client.clientscanner.<init>(clientscanner.java:164)         at org.apache.hadoop.hbase.client.clientscanner.<init>(clientscanner.java:107)         at org.apache.hadoop.hbase.client.htable.getscanner(htable.java:766)         at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:539)         at org.apache.hadoop.hbase.catalog.metareader.fullscanofmeta(metareader.java:140)         at org.apache.hadoop.hbase.catalog.metamigrationconvertingtopb.ismetatableupdated(metamigrationconvertingtopb.java:164)         at org.apache.hadoop.hbase.catalog.metamigrationconvertingtopb.updatemetaifnecessary(metamigrationconvertingtopb.java:131)         at org.apache.hadoop.hbase.master.hmaster.finishactivemasterinitialization(hmaster.java:567)         at org.apache.hadoop.hbase.master.hmaster.access$500(hmaster.java:147)         at org.apache.hadoop.hbase.master.hmaster$1.run(hmaster.java:1242)         at java.lang.thread.run(thread.java:744) ... but the master servlet has the lock while trying to access master: \"686004346@qtp-2101021459-0\" daemon prio=10 tid=0x00007fafb5d2a800 nid=0x5fb1 waiting on condition [0x00007faf8f87f000]    java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation$stubmaker.makestub(connectionmanager.java:1562)         - locked <0x000000064ab4b9a8> (a java.lang.object)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation$masterservicestubmaker.makestub(connectionmanager.java:1597)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.getkeepalivemasterservice(connectionmanager.java:1805)         - locked <0x000000064ab4b9a8> (a java.lang.object)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.listtables(connectionmanager.java:2481)         at org.apache.hadoop.hbase.client.hbaseadmin.listtables(hbaseadmin.java:321)         at org.apache.hadoop.hbase.tmpl.master.masterstatustmplimpl.__jamon_innerunit__usertables(masterstatustmplimpl.java:530)         at org.apache.hadoop.hbase.tmpl.master.masterstatustmplimpl.rendernoflush(masterstatustmplimpl.java:255)         at org.apache.hadoop.hbase.tmpl.master.masterstatustmpl.rendernoflush(masterstatustmpl.java:382)         at org.apache.hadoop.hbase.tmpl.master.masterstatustmpl.render(masterstatustmpl.java:372)         at org.apache.hadoop.hbase.master.masterstatusservlet.doget(masterstatusservlet.java:102) ... ",
        "label": 242
    },
    {
        "text": "fix hardcoding of second socket timeout down in hbaseclient  there is this code in hbaseclient:             netutils.connect(this.socket, remoteid.getaddress(), 20000); we need to be able to set this if only for testing. ",
        "label": 38
    },
    {
        "text": "improvements to prewarm meta cache on clients  a couple different use cases cause storms of reads to meta during startup. for example, a large mr job will cause each map task to hit meta since it starts with an empty cache. a couple possible improvements have been proposed: mr jobs could ship a copy of meta for the table in the distributedcache clients could prewarm cache by doing a large scan of all the meta for the table instead of random reads for each miss each miss could fetch ahead some number of rows in meta ",
        "label": 327
    },
    {
        "text": "remove the deprecated bulkload method in asyncclusterconnection  the class is ia.private and it has not been released yet so let's just remove this method to keep the class clean. ",
        "label": 149
    },
    {
        "text": "npe running hbck on out of reporttablesinflux  got this playing w/ hbck going against the 0.94rc: 12/04/16 17:03:14 info util.hbasefsck: gethtabledescriptors == tablenames => [] exception in thread \"main\" java.lang.nullpointerexception         at org.apache.hadoop.hbase.util.hbasefsck.reporttablesinflux(hbasefsck.java:553)         at org.apache.hadoop.hbase.util.hbasefsck.onlineconsistencyrepair(hbasefsck.java:344)         at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:380)         at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:3033) ",
        "label": 46
    },
    {
        "text": "testmigrate failing on hudson  testmigrate is failing up on hudson. sometimes it fails for me locally but usually it passes. failure is in the new verification stage opening scanner on migrated table. it hangs on first region get never moving on through the rows. ",
        "label": 314
    },
    {
        "text": "add attribution for code added by hbase metrics  see the comment over in https://issues.apache.org/jira/browse/hbase-5533?focusedcommentid=13283920&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13283920 the metrics histogram code was copied w/o attribution. fix. ",
        "label": 314
    },
    {
        "text": "optimize stochasticloadbalancer  on a 5 node trunk cluster, i ran into a weird problem with stochasticloadbalancer: server1 thu mar 14 03:42:50 utc 2013 0.0 33  server2 thu mar 14 03:47:53 utc 2013 0.0 34  server3 thu mar 14 03:46:53 utc 2013 465.0 42  server4 thu mar 14 03:47:53 utc 2013 11455.0 282  server5 thu mar 14 03:47:53 utc 2013 0.0 34  total:5 11920 425 notice that server4 has 282 regions, while the others have much less. plus for one table with 260 regions has been super imbalanced: regions by region server region server region count http://server3:60030/  10 http://server4:60030/  250 ",
        "label": 155
    },
    {
        "text": " hlc  finishing cleanups  track all types of cleanups here: (done in 001.patch) -rename classes to more consistent naming: systemclock, systemmonotonicclock, hybridlogicalclock- (done in 001.patch) -move implementations out from clock interface. it's a simple interface of 6 fns but very overloaded right now with everything put inside it.- (done in 004.patch) -maybe encapsulate all clocks in rs/master into a new class. then rsservices can just have getclocks() function.- class clocks {   // all 3 types of clocks.   // fns:   - update(clocktype, timestamp)   - updateall(list\\<nodetime>)   - int64 now(clocktype)   - list\\<nodetime> nowall()   } ",
        "label": 32
    },
    {
        "text": "remove replicationstate completely  the stop/start replication functionality is confusing. more times then not a user really only wants to stop/start replication on a peer by peer basis. enable/disable peer handles this need. removing replicationstate and the admin commands associated with it will simplify the code without losing any major functionality. this jira was created based on a discussion in hbase-8844. ",
        "label": 103
    },
    {
        "text": "apply version of hbase to branch  consider adding a version of hbase-4015 to 0.90. it changes hregioninterface so would need move change to end of the interface and then test that it doesn't break rolling restart. ",
        "label": 544
    },
    {
        "text": "regionmover should use the configured default port number and not the one from hconstants  the issue i ran into hbase-19499 was due regionmover not using the port used by hbase-site.xml. the tool should use the value used in the configuration before falling back to the hardcoded value hconstants.default_regionserver_port ",
        "label": 455
    },
    {
        "text": "testremoteadmin testclusterstatus should not assume 'requests' does not change  from a failed 0.94 build on ec2 jenkins: failed tests:   testclusterstatus(org.apache.hadoop.hbase.rest.client.testremoteadmin): number of requests from cluster status and returned status did not match up.  expected:<4> but was:<7> testremoteadmin#testclusterstatus should not assume status does not change from when the client gets it directly and when the client asks again via rest. ",
        "label": 38
    },
    {
        "text": "htable getrow  should receive rowresult objects  currently still uses hbasemapwritable of hstorekeys and cells. hstorekeys will have a lot of redundant data (rows and timestamps), and we have to do a transformation when it gets to the client to make it a sortedmap<text, cell>. take advantage of the rowresult class to greatly simplify. maybe getrow should just return a straight rowresult? ",
        "label": 86
    },
    {
        "text": "improve hbase thrift v1 to return results in sorted order  hbase natively stores columns sorted based on the column qualifier. a scan is guaranteed to return sorted columns. the java api works fine but the thrift api is broken. hbase uses treemap that ensures that sort order is maintained. however hbase thrift specification uses a simple map to store the data. a map, since it is unordered doesn't result in columns being returned in a sort order that is consistent with their storage in hbase. ",
        "label": 416
    },
    {
        "text": "update hadoop versions grid in refguide adding hadoop x and a note on hadoop x versions  need to update our hadoop versions grid. add notes on hadoop-2.1 and hadoop-2.0 (we do the former, not the latter) ",
        "label": 314
    },
    {
        "text": "cleanup the locking contention in the master  the new master uses a lot of synchronized blocks to be safe, but it only takes a few jstacks to see that there's multiple layers of lock contention when a bunch of regions are moving (like when the balancer runs). the main culprits are regionintransition in assignmentmanager, zkassign that uses zkw.getznnodes (basically another set of region in transitions), and locking at the regionstate level. my understanding is that even tho we have multiple threads to handle regions in transition, everything is actually serialized. most of the time, lock holders are talking to zk or a region server, which can take a few milliseconds. a simple example is when assignmentmanager wants to update the timers for all the regions on a rs, it will usually be waiting on another thread that's holding the lock while talking to zk. ",
        "label": 229
    },
    {
        "text": "new rpc metric  number of active handler  the attached patch adds a new metric: number of active handler threads. we found this is a good metric to measure how busy of a server. if this number is too high (compared to the total number of handlers), the server has risks in getting call queue full. we used to monitor # reads or # writes. however we found this often produce false alerts, because a read touching hdfs will produce much high workload than a block-cached read. the attached patch is based on our internal 0.94 branch, but i think it pretty easy to port to rebase to other branches if you think it is useful. ",
        "label": 92
    },
    {
        "text": "enable  fix  test for hbase  hbase-2156 added a test called \"testscannervariablereuse\" in \"testfromclientside\". due to a missing annotation this test never ran. i'll add a very simple patch to enable the test but at least for me it fails. ",
        "label": 229
    },
    {
        "text": "do not need to persist default rs group now  as now the rs group info for a table is stored in the table metadata, we only need to store the servers of a rs group to the rs group table, so we do not need to store the default rs group anymore. the servers in default group will be refreshed automatically. ",
        "label": 492
    },
    {
        "text": "get counter value is never used   ",
        "label": 285
    },
    {
        "text": "add regionobserver pre hooks that operate under row lock  the coprocessor hooks were placed outside of row locks. this was meant to sidestep performance issues arising from significant work done within hook invocations. however as the security code increases in sophistication we are now running into concurrency issues trying to use them as a result of that early decision. since the initial introduction of coprocessor upcalls there has been some significant refactoring done around them and concurrency control in core has become more complex. this is potentially an issue for many coprocessor users. we should do either: move all existing regionobserver pre* hooks to execute under row lock. introduce a new set of regionobserver pre* hooks that execute under row lock, named to indicate such. the second option is less likely to lead to surprises. all regionobserver hook javadoc should be updated with advice to the coprocessor implementor not to take their own row locks in the hook. if the current thread happens to already have a row lock and they try to take a lock on another row, there is a deadlock risk. as always a drawback of adding hooks is the potential for performance impact. we should benchmark the impact and decide if the second option above is a viable choice or if the first option is required. finally, we should introduce a higher level interface for managing the registration of 'user' code for execution from the low level hooks. i filed hbase-11125 to discuss this further. ",
        "label": 544
    },
    {
        "text": "region threads open close threads should be daemon threads  can save up to 30 secs during shutdown. no point waiting for these threads. ",
        "label": 154
    },
    {
        "text": "possible performance improvement in client batch operations  presplit and send in background  today batch algo is: for operation o: list<op>{   add o to todolist   if todolist > maxsize or o last in list     split todolist per location     send split lists to region servers     clear todolist     wait } we could: create immediately the final object instead of an intermediate array split per location immediately instead of sending when the list as a whole is full, send it when there is enough data for a single location it would be: for operation o: list<op>{   get location   add o to todo location.todolist   if (location.todolist > maxlocationsize)     send location.todolist to region server      clear location.todolist     // don't wait, continue the loop } send remaining wait it's not trivial to write if you add error management: retried list must be shared with the operations added in the todolist. but it's doable.  it's interesting mainly for 'big' writes ",
        "label": 340
    },
    {
        "text": "allow ioexceptions to be thrown from filter methods  currently there is no way to throw custom ioexceptions from any of the filter methods.  for implementers of custom filters that presents a problem.  for example there are scenarios where the filter would want to indicate to the client that there it should not retry. currently there is no way of doing that. ",
        "label": 36
    },
    {
        "text": "implement a bufferedmutator for async client  need this if we want to replace the old implementation for sync client with async client. ",
        "label": 149
    },
    {
        "text": " hbck  fix intermittent failures on testhbasefsck testhbasefsck  its seems that on the 0.92 branch in particular, testhbasefsck.testhbasefsck is intermittently failing. in the test, a region's assignment is purposely changed in meta but not in zk. after the equivalent of 'hbck -fix', a subsequent check that should be clean comes up with a new zk assignment but with meta still being inconsistent with zk. the rs in zk sometimes this points to the same rs, but sometimes it \"moves\" to another zk. ",
        "label": 248
    },
    {
        "text": "testmultiparallel testflushcommitsnoabort fails frequently in  here's a run (with jdk7, but i've seen it with 0.96 as well).  https://builds.apache.org/job/hbase-0.94-jdk7/17/testreport/junit/org.apache.hadoop.hbase.client/testmultiparallel/testflushcommitsnoabort/ error message count of regions=10 stacktrace java.lang.assertionerror: count of regions=10 at org.junit.assert.fail(assert.java:88) at org.junit.assert.asserttrue(assert.java:41) at org.apache.hadoop.hbase.client.testmultiparallel.dotestflushcommits(testmultiparallel.java:289) at org.apache.hadoop.hbase.client.testmultiparallel.testflushcommitsnoabort(testmultiparallel.java:222)         ... this might be a side-effect of: hbase-10259 ",
        "label": 286
    },
    {
        "text": "hbase master dies with stack overflow error if rootdir isn't qualified  with a relative rootdir (/hbase/), the hbase master throws this on startup: 08/04/11 17:53:00 error hbase.hmaster: can not start master  java.lang.reflect.invocationtargetexception  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(unknown source)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(unknown source)  at java.lang.reflect.constructor.newinstance(unknown source)  at org.apache.hadoop.hbase.hmaster.domain(hmaster.java:3329)  at org.apache.hadoop.hbase.hmaster.main(hmaster.java:3363)  caused by: java.lang.stackoverflowerror  at java.net.uri$parser.checkchars(unknown source) ",
        "label": 241
    },
    {
        "text": "put back testreplicationkillmasterrscompressed when fixed over in hbase  the suite of testreplicationkillrs* tests were removed temporarily. put them back after they've been fixed. ",
        "label": 441
    },
    {
        "text": " book  filter language documentation is hidden  there is excellent documentation for how to use the filter language (including examples!), implemented in hbase-4176. unfortunately, it's hidden as the only section in the otherwise empty thrift chapter. i suggest it be moved to it's own appendix and referenced by all the places that expose it (currently just thrift and the shell, eventually rest \u2013 pending completion of hbase-5417). ",
        "label": 330
    },
    {
        "text": "update refguide on getting x to run on hadoop   http://hbase.apache.org/book.html#d248e643 contains steps for rebuilding 0.94 code base to run on hadoop 2.2.0+ however, the files under src/main/java/org/apache/hadoop/hbase/protobuf/generated were produced by protoc 2.4.0  these files need to be regenerated. see http://search-hadoop.com/m/dhed4j7um02/hbase+0.94+on+hadoop+2.2.0&subj=re+hbase+0+94+on+hadoop+2+2+0+2+4+0+ this issue is to update refguide with this regeneration step. ",
        "label": 191
    },
    {
        "text": "race between rs shutdown thread and openregionhandler causes region to get stuck  2013-01-22 17:59:03,237 info [shutdown of org.apache.hadoop.hbase.fs.hfilesystem@5984cf08] hbase.minihbasecluster$singlefilesystemshutdownthread(186): hook closing fs=org.apache.hadoop.hbase.fs.hfilesystem@5984cf08  ...  2013-01-22 17:59:03,411 debug [rs_open_region-10.11.2.92,50661,1358906192942-0] regionserver.hregion(1001): closing integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.: disabling compactions & flushes  2013-01-22 17:59:03,411 debug [rs_open_region-10.11.2.92,50661,1358906192942-0] regionserver.hregion(1023): updates disabled for region integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.  2013-01-22 17:59:03,415 error [rs_open_region-10.11.2.92,50661,1358906192942-0] executor.eventhandler(205): caught throwable while processing event m_rs_open_region  java.io.ioexception: java.io.ioexception: java.io.ioexception: filesystem closed  at org.apache.hadoop.hbase.regionserver.hregion.doclose(hregion.java:1058)  at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:974)  at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:945)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.cleanupfailedopen(openregionhandler.java:459)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:143)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:202)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:680) trytransitionfromopeningtofailedopen or transitiontoopened below is never called and region can get stuck.  as an added benefit, the meta is already written by that time. ",
        "label": 544
    },
    {
        "text": "add a utility to reload configurations in the master  ",
        "label": 154
    },
    {
        "text": "dev support test patch sh does not have execute perms  when i checkout a tree from trunk, i notice that dev-support/test-patch.sh does not come with execute permissions enabled by default, while the rest of the scripts in dev-support do have +x set. ",
        "label": 131
    },
    {
        "text": "fallback to default group to choose rs when there are no rs in current group  we configure one regionserver for hbase system table. but when rolling upgrade, you need move the region to other regionservers. but because there are no other regionservers in this group, you cannot move the region...     ",
        "label": 430
    },
    {
        "text": "junit and surefire trunk hbase plugins need a new home  people.apache.org cannot currently host personal or transient maven repos. $ curl --connect-timeout 60 -v  http://people.apache.org/~garyh/mvn/org/apache/maven/plugins/maven-remote-resources-plugin/1.4/maven-remote-resources-plugin-1.4.pom * about to connect() to people.apache.org port 80 (#0) *   trying 140.211.11.9... * connection timed out after 60064 milliseconds * closing connection 0 curl: (28) connection timed out after 60064 milliseconds all builds are at the moment broken if the hbase custom junit or surefire jars are not already in cache. even if this is a temporary condition, we should find a new home for these artifacts, upgrade to versions that include our submitted changes (if any), or fall back to release versions. ",
        "label": 180
    },
    {
        "text": "extend bin hbase to print a  mapreduce classpath   for tools like pig and hive, blindly appending the full output of `bin/hbase classpath` to their own classpath is excessive. they already build classpath entries for hadoop. all they need from us is the delta entries, the dependencies we require w/o hadoop and all of it's transitive deps. this is also a kindness for windows, where there's a shorter limit on the length of commandline arguments. see also hive-2055 for additional discussion. ",
        "label": 339
    },
    {
        "text": "optimize hfile index key  leveldb uses bytewisecomparatorimpl::findshortestseparator() & findshortsuccessor() to reduce index key size, it would be helpful under special conditions. ",
        "label": 290
    },
    {
        "text": "make sure javadoc is included in tarball bundle when we release  0.92.0 doesn't have javadoc in the tarball. fix. ",
        "label": 409
    },
    {
        "text": " amv2  enable aggregation of rpcs  assigns unassigns  etc   machinery is in place to coalesce amv2 rpcs (assigns, unassigns). it needs enabling and verification. from '6.3 we don\u2019t do the aggregating of assigns' of https://docs.google.com/document/d/1evka7fhdeoj1-9o8yzcotaqbv0u0bblblcczvsin69g/edit#heading=h.uuwvci2r2tz4 ",
        "label": 459
    },
    {
        "text": "stripecompaction may not obey the offpeak rule to compaction  this is just a written mistake. the parameters in stripepolicy.applycompactionpolicy() of stripecompactionpolicy.java is wrong. just swap the 'mightbestuck' and 'mayuseoffpeak'. ",
        "label": 466
    },
    {
        "text": "fix flaky condition for org apache hadoop hbase testregionrebalancing testrebalanceonregionservernumberchange  the balancer doesn't run in case a region is in-transition. the check to confirm whether there all regions are assigned looks for region count > 22, where the total regions are 27. this may result in a failure: java.lang.assertionerror: after 5 attempts, region assignments were not balanced. at org.junit.assert.fail(assert.java:93) at org.apache.hadoop.hbase.testregionrebalancing.assertregionsarebalanced(testregionrebalancing.java:203) at org.apache.hadoop.hbase.testregionrebalancing.testrebalanceonregionservernumberchange(testregionrebalancing.java:123) ..... 2012-12-11 13:47:02,231 info  [pool-1-thread-1] hbase.testregionrebalancing(120): added fourth server=p0118.mtv.cloudera.com,44414,1355262422083 2012-12-11 13:47:02,231 info  [regionserver:3;p0118.mtv.cloudera.com,44414,1355262422083] regionserver.hregionserver(3769): registered regionserver mxbean 2012-12-11 13:47:02,231 debug [pool-1-thread-1] master.hmaster(987): not running balancer because 1 region(s) in transition: {c786446fb2542f190e937057cdc79d9d=test,kkk,1355262401365.c786446fb2542f190e937057cdc79d9d. state=opening, ts=1355262421037, server=p0118.mtv.cloudera.com,54281,1355262419765} 2012-12-11 13:47:02,232 debug [pool-1-thread-1] hbase.testregionrebalancing(165): there are 4 servers and 26 regions. load average: 13.0 low border: 9, up border: 16; attempt: 0 2012-12-11 13:47:02,232 debug [pool-1-thread-1] hbase.testregionrebalancing(171): p0118.mtv.cloudera.com,51590,1355262395329 avg: 13.0 actual: 11 2012-12-11 13:47:02,232 debug [pool-1-thread-1] hbase.testregionrebalancing(171): p0118.mtv.cloudera.com,52987,1355262407916 avg: 13.0 actual: 15 2012-12-11 13:47:02,233 debug [pool-1-thread-1] hbase.testregionrebalancing(171): p0118.mtv.cloudera.com,48044,1355262421787 avg: 13.0 actual: 0 2012-12-11 13:47:02,233 debug [pool-1-thread-1] hbase.testregionrebalancing(179): p0118.mtv.cloudera.com,48044,1355262421787 isn't balanced!!! avg: 13.0 actual: 0 slop: 0.2 2012-12-11 13:47:12,233 debug [pool-1-thread-1] master.hmaster(987): not running balancer because 1 region(s) in transition:  ",
        "label": 199
    },
    {
        "text": "interfaceaudience annotation in catalogjanitor uses fully qualified name  catalogjanitor class's interfaceaudience annotation uses the fully-qualified name instead of just @interfaceaudience.private. ",
        "label": 334
    },
    {
        "text": "splittransaction has a window where clients can get regionofflineexception  i just witnessed a job having failed tasks because of regionofflineexception. this should normally happen because the table is disabled, but this can also happen because the parent is offline. probably 99.999% of the time users don't hit it because splittransaction is able to offline the parent and add the first daughter quickly enough, but in my case the cluster was so slow that i was able to see. maybe we should check in hcm not only if the region is offline but also if it's split, in which case we should retry? ",
        "label": 229
    },
    {
        "text": "null check missing in storefile reader getmaxtimestamp   we just ran into a scenario where we got the following npe: 13/03/08 11:52:13 info regionserver.store: successfully loaded store file file:/tmp/hfile-import-00dxx0000001lmj-09cxx00000000jm/colfam/file09cxx00000000jm into store colfam (new location: file:/tmp/localhbase/data/sfdc.entity_history_archive/aeacee43aaf1748c6e60b9cc12bcac3d/colfam/120d683414e44478984b50ddd79b6826) 13/03/08 11:52:13 error regionserver.hregionserver: failed openscanner java.lang.nullpointerexception     at org.apache.hadoop.hbase.regionserver.storefile$reader.getmaxtimestamp(storefile.java:1702)     at org.apache.hadoop.hbase.regionserver.storefilescanner.requestseek(storefilescanner.java:301)     at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:127)     at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:2070)     at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.<init>(hregion.java:3383)     at org.apache.hadoop.hbase.regionserver.hregion.instantiateregionscanner(hregion.java:1628)     at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1620)     at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1596)     at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:2342)     at sun.reflect.generatedmethodaccessor13.invoke(unknown source)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364)     at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1400) 13/03/08 11:52:14 error regionserver.hregionserver: failed openscanner it's not clear, yet, how we got into this situation (we are generating hfiles via hfileoutputformat and bulk load those). it seems that can only happen when the hfile itself is corrupted. looking at the code, though, i see this is the only place where we access storefile.reader.timerangetracker without a null check. so it appears we are expecting scenarios in which it can be null. a simple fix would be:     public long getmaxtimestamp() {       return timerangetracker == null ? long.max_value : timerangetracker.maximumtimestamp;     } ",
        "label": 286
    },
    {
        "text": "opening a table also opens the metatable and never closes it   having upgraded to cdh3u3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is inccount and deccount in the hconnection class, when a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. this caused the count in the hconnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling  hconnectionmanager.deleteconnection(config, true); ",
        "label": 54
    },
    {
        "text": "methods missing rpc timeout parameter in htable  when revisiting hbase-15645, i found that two methods (mutaterow and checkandmutate) miss the rpctimeout parameter to newcaller() in htable:     return rpccallerfactory.<boolean> newcaller().callwithretries(callable, this.operationtimeout); i checked branch-1.2 other branch(es) may have the same problem ",
        "label": 188
    },
    {
        "text": "cleanerchore logging improvements  the following code is accidentally using string concatenation instead of logging parameters so the logging will be something like: size={}123 log.warn(\"use full core processors to scan dir, size={}\" + size); fixed that and a few other logging improvements. ",
        "label": 130
    },
    {
        "text": "performanceevaluation does not set the correct classpath for mr because it lives in the test jar  note: this was discovered running the cdh version hbase-0.90.4-cdh3u2 running the performanceevaluation as follows:  $hadoop_home/bin/hadoop org.apache.hadoop.hbase.performanceevaluation scan 5 fails because the mr tasks do not get the hbase jar on the cp, and thus hit classnotfoundexceptions. the job gets the following only:  file:/users/tim/dev/hadoop/hbase-0.90.4-cdh3u2/hbase-0.90.4-cdh3u2-tests.jar  file:/users/tim/dev/hadoop/hadoop-0.20.2-cdh3u2/hadoop-core-0.20.2-cdh3u2.jar  file:/users/tim/dev/hadoop/hbase-0.90.4-cdh3u2/lib/zookeeper-3.3.3-cdh3u2.jar the rowcounter etc all work because they live in the hbase jar, not the test jar, and they get the following   file:/users/tim/dev/hadoop/hbase-0.90.4-cdh3u2/lib/guava-r06.jar  file:/users/tim/dev/hadoop/hadoop-0.20.2-cdh3u2/hadoop-core-0.20.2-cdh3u2.jar  file:/users/tim/dev/hadoop/hbase-0.90.4-cdh3u2/hbase-0.90.4-cdh3u2.jar  file:/users/tim/dev/hadoop/hbase-0.90.4-cdh3u2/lib/zookeeper-3.3.3-cdh3u2.jar presumably this relates to   job.setjarbyclass(performanceevaluation.class);  ...  tablemapreduceutil.adddependencyjars(job); a (cowboy) workaround to run pe is to unpack the jars, and copy the performanceevaluation* classes building a patched jar. ",
        "label": 248
    },
    {
        "text": "remove concept of zookeeper from hconnection interface  the concept of zookeeper is really an implementation detail and should not be exposed in the hconnection interface. therefore, i suggest removing the hconnection.getzookeeperwrapper() method from the interface. i couldn't find any uses of this method within the hbase code base except for in one of the unit tests: org.apache.hadoop.hbase.testzookeeper. this unit test should be changed to instantiate the implementation of hconnection directly, allowing it to use the getzookeeperwrapper() method. this requires making org.apache.hadoop.hbase.client.hconnectionmanager.tableservers public. (i actually think tableservers should be moved out into an outer class, but in the spirit of small patches, i'll refrain from suggesting that in this issue). i'll attach a patch for: 1. the removal of hconnection.getzookeeperwrapper() 2. change of tableservers class from private to public 3. direct instantiation of tableservers within testzookeeper. ",
        "label": 314
    },
    {
        "text": "hba logs at info level errors that won't show in the shell  there is a weird interaction between the shell and hba. when you try to close a region that doesn't exist, it doesn't throw any error: hbase(main):029:0> close_region 'thisisaninvalidregion' 0 row(s) in 0.0580 seconds normally one should get unknownregionexception. starting the shell with \"-d\" i see what a non-shell user would see along with a ton of logging from zk (skipped here): info client.hbaseadmin: no server in .meta. for thisisaninvalidregion; pair=null but again this is not the right message, it should have shown info client.hbaseadmin: no server in .meta. for thisisaninvalidregion; pair=null and this is because that part of the code treats both unknownregionexception and noserverforregionexception like if it was the same thing. there is also some ugliness in flush, compact, and split but it normally doesn't show since the code treats everything like it's a table and sends a tablenotfoundexception. this jira is about making sure that the exceptions are correctly coming out. ",
        "label": 229
    },
    {
        "text": "start replication from a point in time  one way to set up a cluster for replication is to distcp all files then start the replication. we need a way to make sure we don't miss any edits by being able to start a process that reads old log files from a defined point in time and send them to a specific slave cluster and then catch up with normal replication. ",
        "label": 229
    },
    {
        "text": " compat  hide walkey  it has all the below issues and it uses our internal pb. see head of class where it says it should go away... hbase-server-1.0.0.jar, waledit.class  package org.apache.hadoop.hbase.regionserver.wal  [\u2212] waledit.getcompaction ( cell kv ) [static] : walprotos.compactiondescriptor 1   org/apache/hadoop/hbase/regionserver/wal/waledit.getcompaction:(lorg/apache/hadoop/hbase/cell;)lorg/apache/hadoop/hbase/protobuf/generated/walprotos$compactiondescriptor;  change effect  1 return value type has been changed from org.apache.hadoop.hbase.protobuf.generated.walprotos.compactiondescriptor to org.apache.hadoop.hbase.shaded.protobuf.generated.walprotos.compactiondescriptor. this method has been removed because the return type is part of the method signature. a client program may be interrupted by nosuchmethoderror exception. [+] waledit.getflushdescriptor ( cell cell ) [static] : walprotos.flushdescriptor 1   [+] waledit.getregioneventdescriptor ( cell cell ) [static] : walprotos.regioneventdescriptor 1 hbase-server-1.0.0.jar, walkey.class  package org.apache.hadoop.hbase.wal  [+] walkey.getbuilder ( walcellcodec.bytestringcompressor compressor ) : walprotos.walkey.builder 1 ",
        "label": 402
    },
    {
        "text": "clientscanner skips too many rows on recovery if using scanner caching  this can cause rows to be lost from a scan. see this thread where the issue was brought up: http://search-hadoop.com/m/xitbq136xgj1 if hbase.regionserver.lease.period is higher on the client than the server we can get this series of events: 1. client is scanning along happily, and does something slow.  2. scanner times out on region server  3. client calls htable.clientscanner.next()  4. the region server throws an unknownscannerexception  5. client catches exception and sees that it's not longer then it's hbase.regionserver.lease.period config, so it doesn't throw a scannertimeoutexception. instead, it treats it like a nsre. right now the workaround is to make sure the configs are consistent. a possible fix would be to use whatever the region server's scanner timeout is, rather than the local one. ",
        "label": 403
    },
    {
        "text": "make multi wal work with wals other than fshlog  the multi wal should not be bound with fshlog. ",
        "label": 149
    },
    {
        "text": " fb  adding a rack in hbase in online fashion  regionplacement -exprack to add a new rack into the region placement. so far we have a static assignment plan and 3 preferred locations for a region, such that the primary is the location which is serving all requests and hdfs locality is very high on all 3 nodes.   -exprack is moving the the third preferred location on the new rack such that it leaves the cluster in balanced state. after a few days locality is high enough and we make some of this new locations primaries such that again we have a balanced state in number of primaries per regionservers in the entire cluster. ",
        "label": 6
    },
    {
        "text": "hrs should report to master when hmsg are available  it still takes a lot of time for the client to see splits or just regions that move around, with default pe it takes around 4 seconds and creating a table takes a bit more than 2 seconds. i remember having the discussion with stack that hrs.run was not suppose to sleep if any message to send. turns out it does sleep. ",
        "label": 314
    },
    {
        "text": "correct the docs for mutation setcellvisibility    /**    * sets the visibility expression associated with cells in this mutation.    * it is illegal to set <code>cellvisibility</code> on <code>delete</code> mutation.    * @param expression    */ hbase-10885 enable delete to be specified with cell visibility, so we should remove the comment \" it is illegal to set...\" ",
        "label": 281
    },
    {
        "text": " replication  hlog zk node will not be deleted if client roll hlog  if we use the hbase shell command \"hlog_roll\" on a regionserver which is configured replication. the hlog zk node under /hbase/replication/rs/1 can not be deleted. this issue is caused by hbase-6758. ",
        "label": 229
    },
    {
        "text": "canary does not accept config params from command line  at present there are few configs which needs to be present in hbase-site or default xml for it to work. following are the list.  hbase.canary.threads.num  hbase.canary.sink.class  hbase.client.keytab.file  hbase.client.kerberos.principal execution in secure expects keytab and princ to be present 2016-02-05 05:58:44,024 error [main] hbase.authutil - error while trying to perform the initial login: running in secure mode, but config doesn't have a keytab java.io.ioexception: running in secure mode, but config doesn't have a keytab at org.apache.hadoop.security.securityutil.login(securityutil.java:236) at org.apache.hadoop.hbase.security.user$securehadoopuser.login(user.java:392) at org.apache.hadoop.hbase.security.user.login(user.java:259) at org.apache.hadoop.hbase.security.userprovider.login(userprovider.java:116) at org.apache.hadoop.hbase.authutil.launchauthchore(authutil.java:64) at org.apache.hadoop.hbase.tool.canary.main(canary.java:1146) exception in thread \"main\" java.io.ioexception: running in secure mode, but config doesn't have a keytab at org.apache.hadoop.security.securityutil.login(securityutil.java:236) at org.apache.hadoop.hbase.security.user$securehadoopuser.login(user.java:392) at org.apache.hadoop.hbase.security.user.login(user.java:259) at org.apache.hadoop.hbase.security.userprovider.login(userprovider.java:116) at org.apache.hadoop.hbase.authutil.launchauthchore(authutil.java:64) at org.apache.hadoop.hbase.tool.canary.main(canary.java:1146) public static void main(string[] args) throws exception {     final configuration conf = hbaseconfiguration.create();     authutil.launchauthchore(conf);     int numthreads = conf.getint(\"hbase.canary.threads.num\", max_threads_num);     executorservice executor = new scheduledthreadpoolexecutor(numthreads);     class<? extends sink> sinkclass =         conf.getclass(\"hbase.canary.sink.class\", stdoutsink.class, sink.class);     sink sink = reflectionutils.newinstance(sinkclass);     int exitcode = toolrunner.run(conf, new canary(executor, sink), args);     executor.shutdown();     system.exit(exitcode);   } in main class these params should be parsed and updated. else for any change to these value hbase-stie.xml needs to be updated ",
        "label": 475
    },
    {
        "text": "npe in memcache  java.io.ioexception: java.io.ioexception: java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.memcache.internalgetkeys(memcache.java:585)  at org.apache.hadoop.hbase.regionserver.memcache.getkeys(memcache.java:551)  at org.apache.hadoop.hbase.regionserver.hstore.getkeys(hstore.java:1437)  at org.apache.hadoop.hbase.regionserver.hregion.getkeys(hregion.java:1243)  at org.apache.hadoop.hbase.regionserver.hregion.deletemultiple(hregion.java:1498)  at org.apache.hadoop.hbase.regionserver.hregion.deleteall(hregion.java:1424)  at org.apache.hadoop.hbase.regionserver.hregionserver.deleteall(hregionserver.java:1266)  at sun.reflect.generatedmethodaccessor6.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:424)  at org.apache.hadoop.ipc.server$handler.run(server.java:867) ",
        "label": 241
    },
    {
        "text": "a new write thread model for hlog to improve the overall hbase write throughput  in current write model, each write handler thread (executing put()) will individually go through a full 'append (hlog local buffer) => hlog writer append (write to hdfs) => hlog writer sync (sync hdfs)' cycle for each write, which incurs heavy race condition on updatelock and flushlock. the only optimization where checking if current synctillhere > txid in expectation for other thread help write/sync its own txid to hdfs and omitting the write/sync actually help much less than expectation. three of my colleagues(ye hangjun / wu zesheng / zhang peng) at xiaomi proposed a new write thread model for writing hdfs sequence file and the prototype implementation shows a 4x improvement for throughput (from 17000 to 70000+). i apply this new write thread model in hlog and the performance test in our test cluster shows about 3x throughput improvement (from 12150 to 31520 for 1 rs, from 22000 to 70000 for 5 rs), the 1 rs write throughput (1k row-size) even beats the one of bigtable (precolator published in 2011 says bigtable's write throughput then is 31002). i can provide the detailed performance test results if anyone is interested. the change for new write thread model is as below:  1> all put handler threads append the edits to hlog's local pending buffer; (it notifies asyncwriter thread that there is new edits in local buffer)  2> all put handler threads wait in hlog.syncer() function for underlying threads to finish the sync that contains its txid;  3> an single asyncwriter thread is responsible for retrieve all the buffered edits in hlog's local pending buffer and write to the hdfs (hlog.writer.append); (it notifies asyncflusher thread that there is new writes to hdfs that needs a sync)  4> an single asyncflusher thread is responsible for issuing a sync to hdfs to persist the writes by asyncwriter; (it notifies the asyncnotifier thread that sync watermark increases)  5> an single asyncnotifier thread is responsible for notifying all pending put handler threads which are waiting in the hlog.syncer() function  6> no logsyncer thread any more (since there is always asyncwriter/asyncflusher threads do the same job it does) ",
        "label": 203
    },
    {
        "text": "  upgrade junit to  right now we're using a custom version it seems: 4.10-hbase-1. let's upgrade that to 4.11.  see parent for rationale. ",
        "label": 286
    },
    {
        "text": "binary row keys in hbck and other miscellaneous binary key display issues  this is a patch of miscellany that addresses print out of binary keys in zk and in hbck. fixes small issue too in hbck where it says all tables are inconsistent when later in its display it says they are not....(and they are not). ",
        "label": 314
    },
    {
        "text": "add  idea to rat excludes  ",
        "label": 154
    },
    {
        "text": "the initial size of rwqueuerpcexecutor queues should be  numwritequeues   numreadqueues   numscanqueues   the rwqueuerpcexecutor.queues will be initialized as:     queues = new arraylist<blockingqueue<callrunner>>(writehandlerscount + readhandlerscount); it seems this could be improved as:     queues = new arraylist<blockingqueue<callrunner>>(numwritequeues + numreadqueues + numscanqueues); suggestions are welcomed. ",
        "label": 238
    },
    {
        "text": "hbase policy xml is improperly set thus all rules in it can be by passed  this should be a code error.  in hbasepolicyprovider.java, hbase-policy.xml file was set as,  conf.set(\"hadoop.policy.file\", \"hbase-policy.xml\");  but in hadoop serviceauthorizationmanager.java, policy file was get as,  string policyfile =   system.getproperty(\"hadoop.policy.file\", hadoop_policy_file); the result is, the mentioned hbase-policy.xml file won't be used, and   default hadoop-policy.xml file can be used, which is unexpected. ",
        "label": 260
    },
    {
        "text": "make webapps work in distributed mode again and make webapps deploy at   instead of at  webapps master master jsp  ui is not showing up in right place when you run hbase unless you run it in-situ; it don't work properly when distributed mode. ",
        "label": 314
    },
    {
        "text": "cluster hangs if rs serving root fails during startup sequence  on a large-ish cluster, the following sequence of events was seen to happen: master started, root and meta were both unassigned root is assigned to rs01 meta is assigned to rs02 upon open of meta, it writes its location into root on rs01 rs01 crashes while appending to its hlog due to some other bug rs02 fails the region open sequence master notices that rs01 has crashed, and enqueues a servershutdownhandler servershutdownhandler blocks on catalogtracker.waitformeta() since root and meta are not assigned yet master times out assignment of meta, but never succeeds because root location is still marked as rs01 this causes the cluster to never start up. ",
        "label": 326
    },
    {
        "text": "verify that fsdataoutputstream sync  works  in order to guarantee that an hlog sync() flushes the data to the hdfs, we will need to invoke fsdataoutputstream.sync() per hadoop-4379. currently, there is no access to the underlying fsdataoutputstream from sequencefile.writer, as it is a package private member. ",
        "label": 314
    },
    {
        "text": " fb  expose more detailed info through the retryexhausedexception in hbaseclient  currently, hbaseclient would throw retry_exhausted_exception to the client, which is not very informative to debug the reliability issues. so the plan is the hbaseclient could expose more details information in these exceptions. for example, what's the region server or region for the exception ? what's the server side exception in details ? most the information might be already there but we need to expose them in a uniform format. ",
        "label": 378
    },
    {
        "text": "backport hbase 'modify pom and jenkins jobs for hadoop versions' to branch  ",
        "label": 149
    },
    {
        "text": "define prefetcher resultsize max as percentage  currently \"hbase.hregionserver.prefetcher.resultsize.max\" defines global limit for prefetching.  the default value is 256mb. it would be more flexible to define this measure as a percentage of the heap. ",
        "label": 242
    },
    {
        "text": "book is inconsistent regarding disabling   major compaction  it seems that the book has some inconsistencies regarding the way to disable major compactions according to the book in chapter 2.6.1.1. hbase default configuration hbase.hregion.majorcompaction - the time (in miliseconds) between 'major' compactions of all hstorefiles in a region. default: 1 day. set to 0 to disable automated major compactions.  default: 86400000 (http://hbase.apache.org/book.html#hbase_default_configurations) according to the book at chapter 2.8.2.8. managed compactions  \"a common administrative technique is to manage major compactions manually, rather than letting hbase do it. by default, hconstants.major_compaction_period is one day and major compactions may kick in when you least desire it - especially on a busy system. to \"turn off\" automatic major compactions set the value to long.max_value.\" according to the code org.apache.hadoop.hbase.regionserver.store.java, \"0\" is the right answer. (affect all documentation from 0.90.1) ",
        "label": 146
    },
    {
        "text": "region may be opened on two regionservers  found this problem when run itbll with our internal branch which is based on branch-2.2. so mark this as a blocker for 2.2.0. a region 7ebdca9cd09e26074749b546586e2156 is moved from rs-st99 to rs-st98 and the trsp succeed. meanwhile, rs-st99 crashed and schedule a new scp for rs-st99. so scp initialized subprocedures for 7ebdca9cd09e26074749b546586e2156, too. then the 7ebdca9cd09e26074749b546586e2156 was assigned to two regionservers. ",
        "label": 149
    },
    {
        "text": "compatibility checker complaining about hash collisions  had the compatibility checker complain about a hash collision. this fixed it: --- java-acc/modules/internals/basic.pm.old 2017-11-29 11:50:41.000000000 -0800 +++ java-acc/modules/internals/basic.pm 2017-11-29 11:07:26.000000000 -0800 @@ -25,7 +25,7 @@    my %cache;   -my $md5_len = 8; +my $md5_len = 10;    sub getosgroup()  { not sure how best to fix this. upstream pr? patch locally? ",
        "label": 320
    },
    {
        "text": "compute  weighted  median using aggregateprotocol  suppose cf:cq1 stores numeric values and optionally cf:cq2 stores weights. this task finds out the median value among the values of cf:cq1 (see http://www.stat.ucl.ac.be/isdidactique/rhelp/library/r.basic/html/weighted.median.html) this can be done in two passes.  the first pass utilizes aggregateprotocol where the following tuple is returned from each region:  (partial-sum-of-values, partial-sum-of-weights)  the start rowkey (supplied by coprocessor framework) would be used to sort the tuples. this way we can determine which region (called r) contains the (weighted) median. partial-sum-of-weights can be 0 if unweighted median is sought the second pass involves scanning the table, beginning with startrow of region r and computing partial (weighted) sum until the threshold of s/2 is crossed. the (weighted) median is returned. however, this approach wouldn't work if there is mutation in the underlying table between pass one and pass two. in that case, sequential scanning seems to be the solution which is slower than the above approach. ",
        "label": 441
    },
    {
        "text": "namespacejanitor is spammy when the namespace table moves  although region movements are part of a healthy hbase lifestyle, the namespacejanitor warns about it: 2013-08-22 22:35:48,872 warn  [namespacejanitor-jdec2hbase0403-1:60000] org.apache.hadoop.hbase.client.rpcretryingcaller: call exception, tries=0, retries=350, retrytime=-4ms org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: region is not online: 640d4b4d9432f23f1638700217d34764 at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27) at java.lang.reflect.constructor.newinstance(constructor.java:513) at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:95) at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:79) at org.apache.hadoop.hbase.protobuf.protobufutil.getremoteexception(protobufutil.java:235) at org.apache.hadoop.hbase.client.scannercallable.openscanner(scannercallable.java:300) at org.apache.hadoop.hbase.client.scannercallable.call(scannercallable.java:148) at org.apache.hadoop.hbase.client.scannercallable.call(scannercallable.java:57) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:120) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:98) at org.apache.hadoop.hbase.client.clientscanner.nextscanner(clientscanner.java:239) at org.apache.hadoop.hbase.client.clientscanner.<init>(clientscanner.java:153) at org.apache.hadoop.hbase.client.clientscanner.<init>(clientscanner.java:100) at org.apache.hadoop.hbase.client.htable.getscanner(htable.java:696) at org.apache.hadoop.hbase.client.htable.getscanner(htable.java:707) at org.apache.hadoop.hbase.master.tablenamespacemanager.list(tablenamespacemanager.java:185) at org.apache.hadoop.hbase.master.hmaster.listnamespacedescriptors(hmaster.java:3149) at org.apache.hadoop.hbase.master.namespacejanitor.removeorphans(namespacejanitor.java:102) at org.apache.hadoop.hbase.master.namespacejanitor.chore(namespacejanitor.java:86) at org.apache.hadoop.hbase.chore.run(chore.java:80) at java.lang.thread.run(thread.java:662) caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception: org.apache.hadoop.hbase.notservingregionexception: region is not online: 640d4b4d9432f23f1638700217d34764 at org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname(hregionserver.java:2565) at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:3927) at org.apache.hadoop.hbase.regionserver.hregionserver.scan(hregionserver.java:3004) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26847) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2156) at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1861) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1426) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1630) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1687) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$blockingstub.scan(clientprotos.java:27303) at org.apache.hadoop.hbase.client.scannercallable.openscanner(scannercallable.java:291) ... 15 more this should not be printed. ",
        "label": 229
    },
    {
        "text": "deprecate all remaining methods that take text as a parameter  hbase-796 deprecated methods in htable that take text as a parameter. however there are still methods in the public api in hcolumndescriptor, hbaseadmin, batchupdate that should also be deprecated. while we're at it we should deprecate all methods that take text as a parameter. ",
        "label": 229
    },
    {
        "text": "checktyle plugin shouldn't check jamon generated java classes  ",
        "label": 323
    },
    {
        "text": "scan for columns   some timestamp  i would like to be able to scan for rows columns that have a timestamp > some time. if this could be done by a rowinterfacefilter that would be acceptable. but you might also add parameters to htable#getscanner methods as well. it would be preferable for this to not depend on hbase-880 depending on the timeline for that issue. i.e. if hbase-880 won't be released for 6 months, an improvement on the current api would be greatly appreciated. ",
        "label": 247
    },
    {
        "text": "resolve warning introduced by hbase spark module  fix the following warning that is a result of something in the modules pom file [warning] warning: class org.apache.hadoop.mapred.minimrcluster not found - continuing with a stub.  [warning] one warning found ",
        "label": 48
    },
    {
        "text": "make hbase less of a fainting lily when running beside a hogging tasktracker  from irc (with some improving text added \u2013 finishes on a good point made by jgray): 18:53 < st^ack> the coupling in a mr cluster is looser than it is in hbase cluster 18:53 < st^ack> tts only need report in every ten minutes and a task can fail and be restarted 18:54 < st^ack> whereas with hbase, it must report in at least every two minutes (iirc) and cannot 'redo' lost edit -- no chance of a redo 18:54 < st^ack> so, your mr job can be rougher about getting to the finish line; messier. 18:55 < tim_s> yeah.  18:55 < st^ack> if mr is running on same nodes as those hosting hbase, then it can rob resources from hbase in a way that damages hbase but not tt 18:56 < st^ack> so, maybe we need to look at the hbase config; make it more tolerant when its running beside a hogging tt job 18:57 < tim_s> hmm, that would be lovely 18:57 < st^ack> need to look at hdfs; see how 'fragile' it is too; hbase should be at least that 'fragile' 18:57 < jgray> yeah, most issues we see come from resource issues on shared hdfs/tt/rs nodes 18:57 < tim_s> so is it common to host hbase elsewhere? 18:57 < st^ack> let me make an issue on it because this is common failure case for hbase (setup hbase then run your old mr job as though nothing has changed -- then surprise when the little hbase lady faints) 18:57 < jgray> tim_s: currently, no.  common practice is shared 18:58 < st^ack> ... and its better if shared -- locality benefits 18:58 < tim_s> would that be a good idea though? cause i don't really need to have hadoop local i guess. 18:58 < tim_s> ahh 18:58 < apurtell> we share also ... 18:59 < jgray> beyond locality, sharing makes sense as hdfs and hbase nodes have different requirements... hdfs being heaviest on io (where hbase has no use), hbase heavy in memory, tts vary greatly but most often heavy in cpu/io ",
        "label": 314
    },
    {
        "text": "jsps don't html escape literals  ie  table names  region names  start   end keys   similar to hbase-1298, the various jsps included with hbase for monitoring the system don't seem to do any html escaping when displaying user entered data which may contain special characters: table names, region names, start keys, or end keys ",
        "label": 339
    },
    {
        "text": "hbase broke testthriftserver  fix and reenable  ",
        "label": 314
    },
    {
        "text": "backport hbase 'testwalprocedurestoreonhdfs is super duper flaky' to branch  this one fails consistently for me and there's a fix hanging out upstream. let's bring it back. ",
        "label": 198
    },
    {
        "text": "the hbase shell clone snaphost command returns bad error message  when you call the hbase shell clone_snapshot command with a target namespace that doesn't exist, you get an error message, but the variable used to identify the inexistent namespace is wrong: hbase(main):001:0> clone_snapshot 'somesnapshotname', 'somenamespacename:sometablename' error: unknown namespace somesnapshotname! create a new table by cloning the snapshot content. there're no copies of data involved. and writing on the newly created table will not influence the snapshot data. examples:   hbase> clone_snapshot 'snapshotname', 'tablename'   hbase> clone_snapshot 'snapshotname', 'namespace:tablename' it should rather say: error: unknown namespace somenamespacename! ",
        "label": 448
    },
    {
        "text": "hbase thirdparty l n refer to items not actually in the src release  from josh elser's vote on 2.0-rc0: l&n not entirely accurate, imo. they state that things are included in the src release which are not. i think it would be more appropriate to push the relevant information down into src/main/apppended-resources for each module (e.g. hbase-shaded-protobuf would have src/main/appended-resources/{license,notice}) which have the relevant l&n content for the products being bundled. thus, we'd have nothing in the 3rdparty l&n which reflects the src release. ",
        "label": 252
    },
    {
        "text": "regions's in opening state from failed regionservers takes a long time to recover  we have seen a pattern in tests, that the regions are stuck in opening state for a very long time when the region server who is opening the region fails. my understanding of the process: master calls rs to open the region. if rs is offline, a new plan is generated (a new rs is chosen). regionstate is set to pending_open (only in master memory, zk still shows offline). see hregionserver.openregion(), hmaster.assign() regionserver, starts opening a region, changes the state in znode. but that znode is not ephemeral. (see zkassign) rs transitions zk node from offline to opening. see openregionhandler.process() rs then opens the region, and changes znode from opening to opened when rs is killed between opening and opened states, then zk shows opening state, and the master just waits for rs to change the region state, but since rs is down, that wont happen. there is a assignmentmanager.timeoutmonitor, which does exactly guard against these kind of conditions. it periodically checks (every 10 sec by default) the regions in transition to see whether they timedout (hbase.master.assignment.timeoutmonitor.timeout). default timeout is 30 min, which explains what you and i are seeing. servershutdownhandler in master does not reassign regions in opening state, although it handles other states. lowering that threshold from the configuration is one option, but still i think we can do better. will investigate more. ",
        "label": 242
    },
    {
        "text": "convert zk root region server znode content to pb  move the root-region-server znode content from the versioned bytes that servername.getversionedbytes outputs to instead be pb. ",
        "label": 314
    },
    {
        "text": "syncfuture hangs when sequence is  in syncfuture, not_done = 0. the initial value of the ringbuffer is -1. so ringbuffer.next() gives 0 for the first call. if we create a syncfuture with sequence = 0, even when we set it's done (ie. donesequence = 0), it is still not done since donesequence == not_done == 0. can we set not_done to -1, and the initial donesequence to -2? ",
        "label": 242
    },
    {
        "text": "enabletablehandler races with itself  very often when we try to enable a big table we get something like: 2011-09-02 12:21:56,619 fatal org.apache.hadoop.hbase.master.hmaster: unexpected state trying to offline; huge_ass_region_name state=pending_open, ts=1314991316616  java.lang.illegalstateexception  at org.apache.hadoop.hbase.master.assignmentmanager.setofflineinzookeeper(assignmentmanager.java:1074)  at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1030)  at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:858)  at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:838)  at org.apache.hadoop.hbase.master.handler.enabletablehandler$bulkenabler$1.run(enabletablehandler.java:154)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  2011-09-02 12:21:56,620 info org.apache.hadoop.hbase.master.hmaster: aborting the issue is that enabletablehandler calls multiple bulkenabler and it's possible that by the time it calls it a second time, using a stale list of still-not-enabled regions, that it tries to set one region offline in zk but just after its state changed. case in point: 2011-09-02 12:21:56,616 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region huge_ass_region_name to sv4r23s16,60020,1314880035029  2011-09-02 12:21:56,619 fatal org.apache.hadoop.hbase.master.hmaster: unexpected state trying to offline; huge_ass_region_name state=pending_open, ts=1314991316616 here the first line is the first assign done in the first thread, and the second line is the second thread that got to process the same region around the same time. 3ms difference in time. after that, the master dies, and it's pretty sad when it restarts because it failovers an enabling table and it's ungodly slow. i'm pretty sure there's a window where double assignment are possible. talking with stack, it doesn't really make sense to call multiple enables since the list of regions is static (the table is disabled!). we should just call it and wait. also there's a lot of cleanup to do in enabletablehandler since it refers to disabling the table (copy pasta i guess). ",
        "label": 229
    },
    {
        "text": "logging improvements for snapshot operations w large manifests  working through some issues with snapshotting in the presence of a substantial (millions-of) hfiles. log messages about the state of creating or restoring span multiple hundreds of mb of logs, so more context in individual lines would make it easier to orient myself. restoresnapshothelper has a few different stanzas that essentially all say \"i'm creating an hfilelink for this thing\". e.g.     for (snapshotregionmanifest.familyfiles familyfiles: manifest.getfamilyfileslist()) {       path familydir = new path(regiondir, familyfiles.getfamilyname().tostringutf8());       for (snapshotregionmanifest.storefile storefile: familyfiles.getstorefileslist()) {         log.info(\"adding hfilelink \" + storefile.getname() + \" to table=\" + tablename);         restorestorefile(familydir, snapshotregioninfo, storefile, createbackrefs);       }     } if would be helpful if 1) these stanzas offered something in the logged text to tell them apart (i think the coincidentally differ in log level, but that's esoteric and easy to lose over time)  2) it would be nice if we included the snapshot name in the log message so i can more easily pull out all log lines related to the snapshot  3) these stanzas indicated \"progress\" of sorts by giving how many storefiles are present and where we are in that list. sort of like this bit we log when creating the snapshot manifest:       // 2.2. iterate through all the store's files and create \"references\".       for (int i = 0, sz = storefiles.size(); i < sz; i++) {         hstorefile storefile = storefiles.get(i);         monitor.rethrowexception();         // create \"reference\" to this store file.         log.debug(\"adding reference for file (\" + (i+1) + \"/\" + sz + \"): \" + storefile.getpath());         visitor.storefile(regiondata, familydata, storefile.getfileinfo());       } and speaking of the above bit in snapshotmanifest, it would be nice if it included the name of the snapshot in the log message so it's easier to pull out all log lines related to the snapshot. ",
        "label": 363
    },
    {
        "text": "re enable testavoidcellreferencesintoshippedblocks  ",
        "label": 149
    },
    {
        "text": "provide an option for clients to find the server hosting meta that does not involve the zookeeper client  clients are required to connect to zookeeper to find the location of the regionserver hosting the meta table region. site configuration provides the client a list of zk quorum peers and the client uses an embedded zk client to query meta location. timeouts and retry behavior of this embedded zk client are managed orthogonally to hbase layer settings and in some cases the zk cannot manage what in theory the hbase client can, i.e. fail fast upon outage or network partition. we should consider new configuration settings that provide a list of well-known master and backup master locations, and with this information the client can contact any of the master processes directly. any master in either active or passive state will track meta location and respond to requests for it with its cached last known location. if this location is stale, the client can ask again with a flag set that requests the master refresh its location cache and return the up-to-date location. every client interaction with the cluster thus uses only hbase rpc as transport, with appropriate settings applied to the connection. the configuration toggle that enables this alternative meta location lookup should be false by default. this removes the requirement that hbase clients embed the zk client and contact the zk service directly at the beginning of the connection lifecycle. this has several benefits. zk service need not be exposed to clients, and their potential abuse, yet no benefit zk provides the hbase server cluster is compromised. normalizing hbase client and zk client timeout settings and retry behavior - in some cases, impossible, i.e. for fail-fast - is no longer necessary. and, from gary helmling: there is an additional complication here for token-based authentication. when a delegation token is used for sasl authentication, the client uses the cluster id obtained from zookeeper to select the token identifier to use. so there would also need to be some zookeeper-less, unauthenticated way to obtain the cluster id as well. ",
        "label": 71
    },
    {
        "text": "testregionrebalancing broken by running of hdfs shutdown thread   needs to be fixed. ",
        "label": 314
    },
    {
        "text": "cleanup the uis  uis have had a bunch of stuff dumped into them of late. its all good stuff its just not sitting nicely in the web page. fix. ",
        "label": 314
    },
    {
        "text": "master   dont shut down cluster if you run into a fatal error  saw this message: 2010-09-22 13:10:03,547 fatal org.apache.hadoop.hbase.master.metascanner: caught error. starting shutdown.  java.lang.outofmemoryerror: unable to create new native thread  at java.lang.thread.start0(native method)  at java.lang.thread.start(thread.java:597)  at org.apache.hadoop.hbase.ipc.hbaseclient$connection.setupiostreams(hbaseclient.java:328)  at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:857)  at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:725)  at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:252)  at $proxy1.openscanner(unknown source)  at org.apache.hadoop.hbase.master.basescanner.scanregion(basescanner.java:182)  at org.apache.hadoop.hbase.master.metascanner.scanonemetaregion(metascanner.java:73)  at org.apache.hadoop.hbase.master.metascanner.maintenancescan(metascanner.java:129)  at org.apache.hadoop.hbase.master.basescanner.chore(basescanner.java:156)  at org.apache.hadoop.hbase.chore.run(chore.java:68) at this point the regionservers were instructed to exit, which caused more problems than if the master just terminated itself. this would prevent a backup master from picking up since the cluster is terminating! ",
        "label": 314
    },
    {
        "text": "npe in thrift deleteall  got a npe when trying to delete all cells of an entire family. ",
        "label": 284
    },
    {
        "text": "s ismajorcompaction  throws npe will cause current major compaction checking abort  2012-05-05 00:49:43,265 error org.apache.hadoop.hbase.regionserver.hregionserver$majorcompactionchecker: caught exception  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.store.ismajorcompaction(store.java:938)  at org.apache.hadoop.hbase.regionserver.store.ismajorcompaction(store.java:917)  at org.apache.hadoop.hbase.regionserver.hregion.ismajorcompaction(hregion.java:3250)  at org.apache.hadoop.hbase.regionserver.hregionserver$majorcompactionchecker.chore(hregionserver.java:1222)  at org.apache.hadoop.hbase.chore.run(chore.java:66) ",
        "label": 518
    },
    {
        "text": "major compaction check should use new timestamp meta information in hfiles  rather than dfs timestamp  along with ttl to allow major even if single file  periodic major compactions have a separate set of checks prior to submitting the compaction request. currently, if there is a single file, and it is the result of a major compaction, then it is skipped. however, there is a check that will still allow it if the timestamp of the file is older than the ttl of that store. this is not ideal because the timestamp of the file is the latest timestamp in the file rather than the oldest. meta information was introduced to hfiles that stores the max/min timestamp of kvs in the file. we should use the min timestamp from that meta info rather than the file stamp itself. ",
        "label": 247
    },
    {
        "text": "htable getrow  returns null if the row does no exist  the hbase api docs says when the row does not exist, getrow() returns  rowresult is empty if row does not exist. however, in regionserver/hregionserver.java's getrow():  if (result == null || result.isempty())  return null;  return new rowresult(row, result); it actually returns null. either fix the code or the document. ",
        "label": 385
    },
    {
        "text": "whilematchfilter reset should call encapsulated filter reset  bumped into this when trying to encapsulate a singlevaluecolumnfilter in a whilematchfilter.   a scanner would grab all the rows after the first matched row in the table ",
        "label": 314
    },
    {
        "text": "region splits by size not being triggered in at least some cases  we seem to have lost the triggering of region splits by size somewhere in trunk. running a simple test to load data only: 1. create 'usertable', 'f1' in hbase shell  2. run a ycsb load of 10m records i wind up with a single region containing all records, around 13gb, despite max region size being configured to 640mb.     ip-10-160-217-155.us-west-1.compute.internal:8120 1316045713501         requestspersecond=0, numberofonlineregions=1, usedheapmb=1544, maxheapmb=2962         usertable,,1316045755455.1e11a9f71072113258942e03dabaa468.             numberofstores=1, numberofstorefiles=16, storefileuncompressedsizemb=13611, storefilesizemb=13621, compressionratio=1.0007, memstoresizemb=50, storefileindexsizemb=0, readrequestscount=0, writerequestscount=1930, rootindexsizekb=108, totalstaticindexsizekb=10511, totalstaticbloomsizekb=0, totalcompactingkvs=3356000, currentcompactedkvs=3356000, compactionprogresspct=1.0 as best i can tell, the changes introduced in hbase-3797 and hbase-1476 dropped some cases where we were triggering region splits when we didn't compact. ",
        "label": 180
    },
    {
        "text": "use of random nextlong  in hregionserver addscanner   scannerids are currently assigned by getting a random long. while it would be a rare occurrence that two scanners received the same ids on the same region server the results would seem to be... bad.  a client scanner would get results from a different server scanner, and maybe only from some of the region servers. a safer approach would be using an atomiclong. we do not have to worry about running of numbers: if we got 10000 scanners per second it'd take > 2.9m years to reach 2^63. then again the same reasoning would imply that this collisions would be happening too rarely to be of concern (assuming a good random number generator). so maybe this is a none-issue. atomiclong would also imply a minor performance hit on multi core machines, as it would force a memory barrier. ",
        "label": 286
    },
    {
        "text": "testloadandswitchencodeondisk fails sometimes  looks like its dependent on istableenabled actually returning true when the table is enabled only, istableenabled looks like its set whenever any region from a table is enabled which is not the semantic i remember it always having. this needs fixing. meantime the test testloadandswitchencodeondisk will fail for me sometimes. ",
        "label": 314
    },
    {
        "text": "add sasl support for fan out outputstream  otherwise we can not use it in secure environment. should be a netty handler, but see https://github.com/netty/netty/issues/1966 i do not think it will be available in the near future, so we need to do it by ourselves. ",
        "label": 149
    },
    {
        "text": "remove misleading and chatty debug message in replicationlogcleaner  the pv2 logs from replicationlogcleaner is misleading and chatty. let's remove them. 2017-12-27 19:59:02,261 debug [forkjoinpool-1-worker-17] cleaner.cleanerchore: cleanertask 391 starts cleaning dirs and files under hdfs://ve0524.halxg.cloudera.com:8020/hbase/oldwals and itself. 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000001.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000002.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000003.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000004.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000005.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000006.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000007.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000008.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000009.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000010.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000011.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000012.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000013.log 2017-12-27 19:59:02,279 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000014.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000015.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000016.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000017.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000018.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000019.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000020.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000021.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000022.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000023.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000024.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000025.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000026.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000027.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000028.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000029.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000030.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000031.log 2017-12-27 19:59:02,280 debug [forkjoinpool-1-worker-17] master.replicationlogcleaner: didn't find this log in zk, deleting: pv2-00000000000000000032.log ",
        "label": 370
    },
    {
        "text": "shell processlist command is broken  hbase(main):008:0> help 'processlist' show regionserver task list.   hbase> processlist   hbase> processlist 'all'   hbase> processlist 'general'   hbase> processlist 'handler'   hbase> processlist 'rpc'   hbase> processlist 'operation'   hbase> processlist 'all','host187.example.com'   hbase> processlist 'all','host187.example.com,16020'   hbase> processlist 'all','host187.example.com,16020,1289493121758' hbase(main):009:0> processlist 'all' 3377 tasks as of: 2019-11-13 22:58:57 error: too few arguments for usage try 'help \"processlist\"' took 2.2107 seconds ",
        "label": 328
    },
    {
        "text": "add rs group management methods in admin and asyncadmin  ",
        "label": 492
    },
    {
        "text": "zookeeper client wont reconnect if there is a problem  my regionserver got wedged:  2009-03-02 15:43:30,938 warn org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: failed to create /hbase:  org.apache.zookeeper.keeperexception$sessionexpiredexception: keepererrorcode = session expired for /hbase  at org.apache.zookeeper.keeperexception.create(keeperexception.java:87)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:35)  at org.apache.zookeeper.zookeeper.create(zookeeper.java:482)  at org.apache.hadoop.hbase.zookeeper.zookeeperwrapper.ensureexists(zookeeperwrapper.java:219)  at org.apache.hadoop.hbase.zookeeper.zookeeperwrapper.ensureparentexists(zookeeperwrapper.java:240)  at org.apache.hadoop.hbase.zookeeper.zookeeperwrapper.checkoutofsafemode(zookeeperwrapper.java:328)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locaterootregion(hconnectionmanager.java:783)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:468)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:443)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:518)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:477)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.relocateregion(hconnectionmanager.java:450)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getregionlocation(hconnectionmanager.java:295)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getregionlocationforrowwithretries(hconnectionmanager.java:919)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.processbatchofrows(hconnectionmanager.java:950)  at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1370)  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1314)  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1294)  at org.apache.hadoop.hbase.regionhistorian.add(regionhistorian.java:237)  at org.apache.hadoop.hbase.regionhistorian.add(regionhistorian.java:216)  at org.apache.hadoop.hbase.regionhistorian.addregionsplit(regionhistorian.java:174)  at org.apache.hadoop.hbase.regionserver.hregion.splitregion(hregion.java:607)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.split(compactsplitthread.java:174)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:107) this message repeats over and over. looking at the code in question:  private boolean ensureexists(final string znode) {  try { zookeeper.create(znode, new byte[0], ids.open_acl_unsafe, createmode.persistent); log.debug(\"created znode \" + znode); return true; } catch (keeperexception.nodeexistsexception e) { return true; // ok, move on. } catch (keeperexception.nonodeexception e) { return ensureparentexists(znode) && ensureexists(znode); } catch (keeperexception e) { log.warn(\"failed to create \" + znode + \":\", e); } catch (interruptedexception e) { log.warn(\"failed to create \" + znode + \":\", e); } return false;  } we need to catch this exception specifically and reopen the zk connection. ",
        "label": 342
    },
    {
        "text": "provide the ability to delete multiple snapshots through single command  currently hbaseadmin#deletesnapshot() accepts name of single snapshot. it is desirable to allow user to delete multiple snapshots by issuing one command.  e.g. user may use regular expression to specify the names of snapshots to delete. ",
        "label": 441
    },
    {
        "text": " hbck  refactor so that arbitrary  d cmdline options are included  a review of hbase-9831 pointed out the fact that -d options aren't being passed into the configuration object used by hbck. this means overriding -d options will not work unless special hooks are for specific options. a first attempt to fix this was in hbase-9831 but it affected many other files. the right approach would be to create a new hbcktool class that had the configured interface and change to existing hbasefsck main to instantiate that to have it parse args, and then create the hbasefsck object inside run. ",
        "label": 248
    },
    {
        "text": "operationstatus success failure not run  are not visible to 3rd party coprocessors  prebatchmutate is useless for some operation due to this. see also tephra-299. this looks like an oversight. minibatchoperationinprogress has limited visibility for coprocessors. operationstatus and operationstatuscode should have the same. ",
        "label": 38
    },
    {
        "text": "replicationprotbufutil tohlogentries needs to be either removed or deprecated  currently, due to hbase-9442 replicationprotbufutil.tohlogentries is rendered useless. given that it isn't really used by hbase itself, perhaps it needs to be removed/deprecated? ",
        "label": 382
    },
    {
        "text": "testtablelockmanager testdelete may occasionally fail due to lack of synchronization between test and handler thread  java.lang.assertionerror at org.junit.assert.fail(assert.java:86) at org.junit.assert.asserttrue(assert.java:41) at org.junit.assert.asserttrue(assert.java:52) at org.apache.hadoop.hbase.master.testtablelockmanager.testdelete(testtablelockmanager.java:250) it corresponds to this assertion:     asserttrue(zkutil.checkexists(zkwatcher,         zkutil.joinznode(zkwatcher.tablelockznode, bytes.tostring(table_name))) < 0); in test output (https://builds.apache.org/job/hbase-trunk/4056/testreport/junit/org.apache.hadoop.hbase.master/testtablelockmanager/testdelete/): 2013-04-12 10:46:02,642 debug [master_table_operations-janus.apache.org,36265,1365763551669-0] handler.deletetablehandler(113): table 'testtablelevellocks' archived! 2013-04-12 10:46:02,642 debug [master_table_operations-janus.apache.org,36265,1365763551669-0] handler.deletetablehandler(117): removing 'testtablelevellocks' descriptor. 2013-04-12 10:46:02,643 debug [master_table_operations-janus.apache.org,36265,1365763551669-0] handler.deletetablehandler(121): marking 'testtablelevellocks' as deleted. 2013-04-12 10:46:02,658 debug [master_table_operations-janus.apache.org,36265,1365763551669-0] master.tablelockmanager$zktablelockmanager$tablelockimpl(253): attempt to release table write lock on :testtablelevellocks 2013-04-12 10:46:02,667 debug [rs_close_meta-janus.apache.org,54489,1365763551867-0] util.fsutils(211): creating file=hdfs://localhost:52497/user/jenkins/hbase/.meta./1028785192/.tmp/59ec68e4587d43d6bc026e4d6f2aafa2 with permission=rwxrwxrwx 2013-04-12 10:46:02,674 debug [master_table_operations-janus.apache.org,36265,1365763551669-0] lock.zkinterprocesslockbase(297): successfully released /hbase/table-lock/testtablelevellocks/write-master:362650000000002 2013-04-12 10:46:02,675 debug [master_table_operations-janus.apache.org,36265,1365763551669-0] master.tablelockmanager$zktablelockmanager$tablelockimpl(269): released table lock on :testtablelevellocks this seems to be timing issue: when admin.deletetable(table_name) returned, tablelockmanager hadn't got around to finishing the release of table lock. ",
        "label": 441
    },
    {
        "text": "in assignmentmanager failover mode  use servershutdownhandler to handle dead regions  in assignmentmanager failover mode, a special failoverprocessedregions map is used to manage regions in transition. it complicates the code. should we use servershutdownhander to process those regions? so that we can share some code and make the logic of assignmentmanager a little bit simpler. ",
        "label": 242
    },
    {
        "text": "cst requestcompaction semantics changed  logs are now spammed when too many store files  another bug i'm not so sure what's going on. i see this in my log: 2011-10-12 00:23:43,435 debug org.apache.hadoop.hbase.regionserver.store: info: no store files to compact  2011-10-12 00:23:44,335 debug org.apache.hadoop.hbase.regionserver.store: info: no store files to compact  2011-10-12 00:23:45,236 debug org.apache.hadoop.hbase.regionserver.store: info: no store files to compact  2011-10-12 00:23:46,136 debug org.apache.hadoop.hbase.regionserver.store: info: no store files to compact  2011-10-12 00:23:47,036 debug org.apache.hadoop.hbase.regionserver.store: info: no store files to compact  2011-10-12 00:23:47,936 debug org.apache.hadoop.hbase.regionserver.store: info: no store files to compact it spams for a while, and a little later instead i get: 2011-10-12 00:26:52,139 debug org.apache.hadoop.hbase.regionserver.store: skipped compaction of info. only 2 file(s) of size 176.4m have met compaction criteria.  2011-10-12 00:26:53,040 debug org.apache.hadoop.hbase.regionserver.store: skipped compaction of info. only 2 file(s) of size 176.4m have met compaction criteria.  2011-10-12 00:26:53,940 debug org.apache.hadoop.hbase.regionserver.store: skipped compaction of info. only 2 file(s) of size 176.4m have met compaction criteria.  2011-10-12 00:26:54,840 debug org.apache.hadoop.hbase.regionserver.store: skipped compaction of info. only 2 file(s) of size 176.4m have met compaction criteria.  2011-10-12 00:26:55,741 debug org.apache.hadoop.hbase.regionserver.store: skipped compaction of info. only 2 file(s) of size 176.4m have met compaction criteria.  2011-10-12 00:26:56,641 debug org.apache.hadoop.hbase.regionserver.store: skipped compaction of info. only 2 file(s) of size 176.4m have met compaction criteria. i believe i also saw something like that for flushes, but the region was closing so at least i know why it was spamming (would be nice if it just unrequested the flush): 2011-10-12 00:26:40,693 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,694 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false  2011-10-12 00:26:40,733 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,733 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false  2011-10-12 00:26:40,873 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,873 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false  2011-10-12 00:26:40,873 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,873 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false  2011-10-12 00:26:40,921 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,922 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false  2011-10-12 00:26:40,923 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,923 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false  2011-10-12 00:26:40,923 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.  2011-10-12 00:26:40,923 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region testtable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesenabled=false ",
        "label": 229
    },
    {
        "text": "fix failed ut testshell  failed on master branch. need debug. [info] running org.apache.hadoop.hbase.client.testshell  [error] tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 722.737 s <<< failure! - in org.apache.hadoop.hbase.client.testshell  [error] testrunshelltests(org.apache.hadoop.hbase.client.testshell) time elapsed: 699.473 s <<< error!  org.jruby.embed.evalfailedexception: (runtimeerror) shell unit tests failed. check output file for details.  at org.apache.hadoop.hbase.client.testshell.testrunshelltests(testshell.java:36)  caused by: org.jruby.exceptions.raiseexception: (runtimeerror) shell unit tests failed. check output file for details. ",
        "label": 187
    },
    {
        "text": "improve master region assignment function  we would like the master's region assignment function to take into account more factors when choosing where to assign regions. more advanced accounting of load on regionserver - memory, # requests, etc don't deploy both daughter regions to the same regionserver assign regions where the underlying dfs blocks are hosted if possible please add additional ideas in comments as they come up. ",
        "label": 86
    },
    {
        "text": "branch am can get stuck when meta moves  when regions are moving master can get totally stuck trying to talk to meta. ",
        "label": 154
    },
    {
        "text": "testdistributedlogsplitting fails  would you mind taking a look at a recent set of failures please jeffrey zhong? it seems to be failing more recently of late: https://issues.apache.org/jira/browse/hbase-9438?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedcommentid=13759275#comment-13759275 https://builds.apache.org/job/precommit-hbase-build/7060//testreport/ https://builds.apache.org/job/hbase-trunk-on-hadoop-2.0.0/703/ thank you. ",
        "label": 242
    },
    {
        "text": "support of list     in hbaseoutputwritable for serialization  on a higher language semantics , list < > are very useful for manipulation and when finally sent down to the wire - the protocol currently seems to take only [ ] through an expensive copy. supporting list <t > directly would save us the copy in terms of memory and add to faster / deserialization. ",
        "label": 266
    },
    {
        "text": "in zookeeper sh  use localhost instead of   fella on irc said standalone wouldn't work on mac unless he changed 127.0.0.1 to localhost. his ssh'ing failed. should we change it to localhost? any downsides? ",
        "label": 314
    },
    {
        "text": "hregionserver hangs upon exit due to dfsclient exception  several hregionservers hang around indefinitely well after the hmaster has exited. this was triggered executing $hbase_home/bin/stop-hbase.sh. the hmaster exists fine, but here is what happens on one of the hregionservers: 2008-01-02 18:54:01,907 info org.apache.hadoop.hbase.hregionserver: got regionserver stop message  2008-01-02 18:54:01,907 info org.apache.hadoop.hbase.leases: regionserver/0.0.0.0:60020 closing leases  2008-01-02 18:54:01,907 info org.apache.hadoop.hbase.leases$leasemonitor: regionserver/0.0.0.0:60020.leasechecker exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.hbase.leases: regionserver/0.0.0.0:60020 closed leases  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: stopping server on 60020  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 2 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 0 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 7 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 3 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 5 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 9 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 6 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 4 on 60020: exiting  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 1 on 60020: exiting  2008-01-02 18:54:01,909 info org.apache.hadoop.ipc.server: stopping ipc server listener on 60020  2008-01-02 18:54:01,908 info org.apache.hadoop.ipc.server: ipc server handler 8 on 60020: exiting  2008-01-02 18:54:01,909 info org.apache.hadoop.hbase.hregionserver: stopping infoserver  2008-01-02 18:54:01,909 debug org.mortbay.util.container: stopping org.mortbay.jetty.server@62c09554  2008-01-02 18:54:01,909 debug org.mortbay.util.threadedserver: closing serversocket[addr=0.0.0.0/0.0.0.0,port=0,localport=60030]  2008-01-02 18:54:01,909 debug org.mortbay.util.threadedserver: ignored  java.net.socketexception: socket closed  at java.net.plainsocketimpl.socketaccept(native method)  at java.net.plainsocketimpl.accept(plainsocketimpl.java:384)  at java.net.serversocket.implaccept(serversocket.java:453)  at java.net.serversocket.accept(serversocket.java:421)  at org.mortbay.util.threadedserver.acceptsocket(threadedserver.java:432)  at org.mortbay.util.threadedserver$acceptor.run(threadedserver.java:631)  2008-01-02 18:54:01,910 info org.mortbay.util.threadedserver: stopping acceptor serversocket[addr=0.0.0.0/0.0.0.0,port=0,localport=60030]  2008-01-02 18:54:01,910 debug org.mortbay.util.threadedserver: self connect to close listener /127.0.0.1:60030  2008-01-02 18:54:01,911 debug org.mortbay.util.threadedserver: problem stopping acceptor /127.0.0.1:  2008-01-02 18:54:01,911 debug org.mortbay.util.threadedserver: problem stopping acceptor /127.0.0.1:  java.net.connectexception: connection refused  at java.net.plainsocketimpl.socketconnect(native method)  at java.net.plainsocketimpl.doconnect(plainsocketimpl.java:333)  at java.net.plainsocketimpl.connecttoaddress(plainsocketimpl.java:195)  at java.net.plainsocketimpl.connect(plainsocketimpl.java:182)  at java.net.sockssocketimpl.connect(sockssocketimpl.java:366)  at java.net.socket.connect(socket.java:519)  at java.net.socket.connect(socket.java:469)  at java.net.socket.<init>(socket.java:366)  at java.net.socket.<init>(socket.java:209)  at org.mortbay.util.threadedserver$acceptor.forcestop(threadedserver.java:682)  at org.mortbay.util.threadedserver.stop(threadedserver.java:557)  at org.mortbay.http.socketlistener.stop(socketlistener.java:211)  at org.mortbay.http.httpserver.dostop(httpserver.java:781)  at org.mortbay.util.container.stop(container.java:154)  at org.apache.hadoop.hbase.util.infoserver.stop(infoserver.java:237)  at org.apache.hadoop.hbase.hregionserver.run(hregionserver.java:835)  at java.lang.thread.run(thread.java:619)  2008-01-02 18:54:01,911 info org.mortbay.http.socketlistener: stopped socketlistener on 0.0.0.0:60030  2008-01-02 18:54:01,912 debug org.mortbay.util.container: stopping httpcontext[/static,/static]  2008-01-02 18:54:01,912 debug org.mortbay.http.handler.abstracthttphandler: stopped org.mortbay.http.handler.resourcehandler in httpcontext[/static,/static]  2008-01-02 18:54:02,039 info org.mortbay.util.container: stopped httpcontext[/static,/static]  2008-01-02 18:54:02,039 debug org.mortbay.util.container: stopping httpcontext[/logs,/logs]  2008-01-02 18:54:02,039 debug org.mortbay.http.handler.abstracthttphandler: stopped org.mortbay.http.handler.resourcehandler in httpcontext[/logs,/logs]  2008-01-02 18:54:02,154 info org.mortbay.util.container: stopped httpcontext[/logs,/logs]  2008-01-02 18:54:02,154 debug org.mortbay.util.container: stopping webapplicationcontext[/,/]  2008-01-02 18:54:02,154 debug org.mortbay.util.container: stopping org.mortbay.jetty.servlet.webapplicationhandler@7ec5495e  2008-01-02 18:54:02,155 info org.mortbay.util.container: stopped org.mortbay.jetty.servlet.webapplicationhandler@7ec5495e  2008-01-02 18:54:02,277 debug org.mortbay.jetty.servlet.abstractsessionmanager: session scavenger exited  2008-01-02 18:54:02,278 debug org.mortbay.util.container: remove component: org.mortbay.jetty.servlet.webapplicationhandler@7ec5495e  2008-01-02 18:54:02,278 info org.mortbay.util.container: stopped webapplicationcontext[/,/]  2008-01-02 18:54:02,278 info org.mortbay.util.container: stopped org.mortbay.jetty.server@62c09554  2008-01-02 18:54:02,278 debug org.apache.hadoop.hbase.hregionserver: closing region spider_pages,10_131455761,1198140179439  2008-01-02 18:54:02,278 info org.apache.hadoop.hbase.hregionserver: regionserver/0.0.0.0:60020.cacheflusher exiting  2008-01-02 18:54:02,278 info org.apache.hadoop.hbase.hregionserver: regionserver/0.0.0.0:60020.compactor exiting  2008-01-02 18:54:02,278 info org.apache.hadoop.hbase.hregionserver: regionserver/0.0.0.0:60020.splitter exiting  2008-01-02 18:54:02,279 debug org.apache.hadoop.hbase.hstore: closed spider_pages,10_131455761,1198140179439/search (1501227429/search)  2008-01-02 18:54:02,279 debug org.apache.hadoop.hbase.hstore: closed spider_pages,10_131455761,1198140179439/profile (1501227429/profile)  2008-01-02 18:54:02,279 debug org.apache.hadoop.hbase.hstore: closed spider_pages,10_131455761,1198140179439/meta (1501227429/meta)  2008-01-02 18:54:02,279 info org.apache.hadoop.hbase.hregion: closed spider_pages,10_131455761,1198140179439  2008-01-02 18:54:02,279 debug org.apache.hadoop.hbase.hregionserver: closing region spider_pages,10_486594261,1198319654267  2008-01-02 18:54:02,280 debug org.apache.hadoop.hbase.hstore: closed spider_pages,10_486594261,1198319654267/search (364081590/search)  2008-01-02 18:54:02,280 debug org.apache.hadoop.hbase.hstore: closed spider_pages,10_486594261,1198319654267/profile (364081590/profile)  2008-01-02 18:54:02,280 debug org.apache.hadoop.hbase.hstore: closed spider_pages,10_486594261,1198319654267/meta (364081590/meta)  2008-01-02 18:54:02,280 info org.apache.hadoop.hbase.hregion: closed spider_pages,10_486594261,1198319654267  ...  ... this closing of regions goes on for a while  ...  ... the following continues until a kill -9  ...  2008-01-02 20:39:20,552 info org.apache.hadoop.fs.dfsclient: could not obtain block blk_5124700261538503923 from any node: java.io.ioexception: no live nodes contain current block  2008-01-02 20:40:23,556 info org.apache.hadoop.fs.dfsclient: could not obtain block blk_5124700261538503923 from any node: java.io.ioexception: no live nodes contain current block  2008-01-02 20:41:26,560 info org.apache.hadoop.fs.dfsclient: could not obtain block blk_5124700261538503923 from any node: java.io.ioexception: no live nodes contain current block  2008-01-02 20:42:29,566 info org.apache.hadoop.fs.dfsclient: could not obtain block blk_5124700261538503923 from any node: java.io.ioexception: no live nodes contain current block  2008-01-02 20:43:32,571 info org.apache.hadoop.fs.dfsclient: could not obtain block blk_5124700261538503923 from any node: java.io.ioexception: no live nodes contain current block  ... ",
        "label": 241
    },
    {
        "text": "npe in keyvalue kvcomparator compare when compacting  while testing normal insertion via pe, i got this recurrent npe coming out of keyvalue$kvcomparator.compare while it's compacting. so far i saw 2 different stack traces: java.lang.nullpointerexception at org.apache.hadoop.hbase.keyvalue$kvcomparator.compare(keyvalue.java:1356) at org.apache.hadoop.hbase.regionserver.keyvalueheap.reseek(keyvalueheap.java:250) at org.apache.hadoop.hbase.regionserver.storescanner.reseek(storescanner.java:385) at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:291) at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:324) at org.apache.hadoop.hbase.regionserver.store.compact(store.java:926) at org.apache.hadoop.hbase.regionserver.store.compact(store.java:734) java.lang.nullpointerexception at org.apache.hadoop.hbase.keyvalue$kvcomparator.compare(keyvalue.java:1375) at org.apache.hadoop.hbase.regionserver.keyvalueheap$kvscannercomparator.compare(keyvalueheap.java:180) at org.apache.hadoop.hbase.regionserver.keyvalueheap$kvscannercomparator.compare(keyvalueheap.java:156) at org.apache.hadoop.hbase.regionserver.keyvalueheap$kvscannercomparator.compare(keyvalueheap.java:146) at java.util.priorityqueue.siftupusingcomparator(priorityqueue.java:594) at java.util.priorityqueue.siftup(priorityqueue.java:572) at java.util.priorityqueue.offer(priorityqueue.java:274) at java.util.priorityqueue.add(priorityqueue.java:251) at org.apache.hadoop.hbase.regionserver.keyvalueheap.reseek(keyvalueheap.java:258) at org.apache.hadoop.hbase.regionserver.storescanner.reseek(storescanner.java:385) at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:291) at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:324) at org.apache.hadoop.hbase.regionserver.store.compact(store.java:926) at org.apache.hadoop.hbase.regionserver.store.compact(store.java:734) ",
        "label": 547
    },
    {
        "text": " replication  remove dead sinks from replicationsource currentpeers and pick new ones  i happened to look at a log today where i saw a lot lines like this: 2012-12-06 23:29:08,318 info org.apache.hadoop.hbase.replication.regionserver.replicationsource: slave cluster looks down: this server is in the failed servers list: sv4r20s49/10.4.20.49:10304 2012-12-06 23:29:15,987 warn org.apache.hadoop.hbase.replication.regionserver.replicationsource: can't replicate because of a local or network error:  java.net.connectexception: connection refused at sun.nio.ch.socketchannelimpl.checkconnect(native method) at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:567) at org.apache.hadoop.net.socketiowithtimeout.connect(socketiowithtimeout.java:206) at org.apache.hadoop.net.netutils.connect(netutils.java:519) at org.apache.hadoop.net.netutils.connect(netutils.java:484) at org.apache.hadoop.hbase.ipc.hbaseclient$connection.setupconnection(hbaseclient.java:416) at org.apache.hadoop.hbase.ipc.hbaseclient$connection.setupiostreams(hbaseclient.java:462) at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:1150) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:1000) at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:150) at $proxy14.replicatelogentries(unknown source) at org.apache.hadoop.hbase.replication.regionserver.replicationsource.shipedits(replicationsource.java:627) at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:365) 2012-12-06 23:29:15,988 info org.apache.hadoop.hbase.replication.regionserver.replicationsource: slave cluster looks down: connection refused what struck me as weird is this had been going on for some days, i would expect the rs to find new servers if it wasn't able to replicate. but the reality is that only a few of the chosen sink rs were down so eventually the source hits one that's good and is never able to refresh its list of servers. we should remove the dead servers, it's spammy and probably adds some slave lag. ",
        "label": 286
    },
    {
        "text": "premptive call me maybe hbase  aphyr wrote an interesting article on c* [1]. some awkward-looking issues were turned up though it seems the author is purportedly doing nothing but exercising the software within spec; he is just paying close attention to what is being returned. it does not look like aphyr will be coming our way any time soon [2] \u2013 thanks ian varley \u2013 but he could change his mind. wouldn't it be coolio if we'd already run his test suite and found any bugs and fixed them before he came by? this issue is about running his article against hbase so we find the embarrassing before he does. 1. http://aphyr.com/posts/294-call-me-maybe-cassandra  2. https://twitter.com/aphyr/status/335082835868254209 ",
        "label": 380
    },
    {
        "text": "incrementing binary rows cause strange behavior once table splits  we're now using incrementing binary row keys and to this point had only been doing small tests with them, never having actually had a table split. i'm still working through the logs but it seems that there's a problem somewhere with startkey and endkeys. binary in general is not well supported (inconsistent in display in the logs, very odd rendering in the web ui, hard to interpret in the shell, etc..) once we figure out these serious bugs we will spend some time trying to clean that up. but right now this makes things even harder to debug. the actual symptoms are that my import eventually started to throw (in the client and on the region server): org.apache.hadoop.hbase.regionserver.wrongregionexception: org.apache.hadoop.hbase.regionserver.wrongregionexception: requested row out of range for hregion sources,,1220546297947, startkey='', getendkey()='  \u00ef\u00bf\u00bd', row='c\u00ef\u00bf\u00bd'  at org.apache.hadoop.hbase.regionserver.hregion.checkrow(hregion.java:1775)  at org.apache.hadoop.hbase.regionserver.hregion.obtainrowlock(hregion.java:1831)  at org.apache.hadoop.hbase.regionserver.hregion.batchupdate(hregion.java:1387)  at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1145)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:616)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:473)  at org.apache.hadoop.ipc.server$handler.run(server.java:888) there are 3 regionservers, but this error only happens on one of them (while the other two always continue normally, allowing updates to this same table). the regionserver that this happens on is special for two reasons, one it is hosting the meta table. and secondly it also hosts the first region in the table, startkey = ''. i'm unsure which is the cause, i have a clue leading to both. after about 15 minutes, the regionserver sees: 2008-09-04 09:52:57,948 debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region .meta.,,1. current region memcache size 24.5k  2008-09-04 09:52:58,003 debug org.apache.hadoop.hbase.regionserver.hstore: added /hbase/.meta./1028785192/historian/mapfiles/8699673838203663799 with 106 entries, sequence id 25341510, data size 8.9k, file size 10.6k  2008-09-04 09:52:58,050 debug org.apache.hadoop.hbase.regionserver.hstore: added /hbase/.meta./1028785192/info/mapfiles/1791564557665476834 with 96 entries, sequence id 25341510, data size 14.2k, file size 15.8k  2008-09-04 09:52:58,050 debug org.apache.hadoop.hbase.regionserver.hregion: finished memcache flush for region .meta.,,1 in 102ms, sequence id=25341510, compaction requested=true  2008-09-04 09:52:58,050 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region: .meta.,,1  2008-09-04 09:52:58,051 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region .meta.,,1  2008-09-04 09:52:58,055 debug org.apache.hadoop.hbase.regionserver.hstore: compaction size of 1028785192/historian: 41.9k; skipped 1 files , size: 21957  2008-09-04 09:52:58,088 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 2 files into /hbase/.meta./compaction.dir/1028785192/historian/mapfiles/6948796056606699674  2008-09-04 09:52:58,128 debug org.apache.hadoop.hbase.regionserver.hstore: moving /hbase/.meta./compaction.dir/1028785192/historian/mapfiles/6948796056606699674 to /hbase/.meta./1028785192/historian/mapfiles/75733875840914142  2008-09-04 09:52:58,175 debug org.apache.hadoop.hbase.regionserver.hstore: completed compaction of 1028785192/historian store size is 41.1k; time since last major compaction: 5426 seconds  2008-09-04 09:52:58,179 debug org.apache.hadoop.hbase.regionserver.hstore: compaction size of 1028785192/info: 61.9k; skipped 0 files , size: 0  2008-09-04 09:52:58,192 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 3 files into /hbase/.meta./compaction.dir/1028785192/info/mapfiles/7781013568996125923  2008-09-04 09:52:58,260 debug org.apache.hadoop.hbase.regionserver.hstore: moving /hbase/.meta./compaction.dir/1028785192/info/mapfiles/7781013568996125923 to /hbase/.meta./1028785192/info/mapfiles/2187291308709057119  2008-09-04 09:52:58,290 debug org.apache.hadoop.hbase.regionserver.hstore: completed compaction of 1028785192/info store size is 61.0k; time since last major compaction: 32534 seconds  2008-09-04 09:52:58,296 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region .meta.,,1 in 0sec  2008-09-04 09:53:09,620 info org.apache.hadoop.hbase.regionserver.hregionserver: scanner -2085474968086468199 lease expired  2008-09-04 09:54:35,449 info org.apache.hadoop.hbase.regionserver.logroller: rolling hlog. number of entries: 30009 following this, insertion continues normally. this leads me to believe there's an issue with the meta table memcache, but oddly the other regions of this table on other regionservers continue on fine. as for the hosting the first region of the table on this region server, it seems to be consistent that i get the row out of range errors when looking for a region with startkey = '', although there are 5 other regions on this rs. will attach full logs from master and three rs. also a couple screenshots showing weird behavior in listing the regions in the table. ",
        "label": 247
    },
    {
        "text": "server side  remove convertion from pb type to client type before we call method  in the regionserver, when the rpc receives a call, the call is described using protobufs. before we make the server-side invocation, we do a transform on the pb param objects to make a native pojo \u2013 e.g. from a pb puts into an hbase o.a.h.h.client.put \u2013 and only then do we make the call against the server. on the way out, similar, before putting the result on the wire, we will do a convertion from o.a.h.h.client.result into pb result. this issue is about our first investigating if it is possible to do away w/ this marshalling/unmarshalling serverside especially given the pb objects themselves are rich in accessor and getters, etc. if it is possible to do w/ pbs alone serverside, then we should go ahead and rip out all the serverside convertions. ",
        "label": 46
    },
    {
        "text": "spurious warnings from util sleeper  many of this in the logs warn util.sleeper: we slept 59993ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://wiki.apache.org/hadoop/hbase/troubleshooting#a9 except this is the metascanner and the sleeper has a period of 60000ms. i have one of 30000ms in stargate which is doing the same thing now. i see the intent but these spurious warnings defeat the intent here. marked as blocker as this will cause no end of users writing in on the list. should be a test of the actual sleep time against the configured interval, not a constant. ",
        "label": 229
    },
    {
        "text": "optimize time range scans using a delete bloom filter  to speed up time range scans we need to seek to the maximum timestamp of the requested range,instead of going to the first kv of the (row, column) pair and iterating from there. if we don't know the (row, column), e.g. if it is not specified in the query, we need to go to end of the current row/column pair first, get a kv from there, and do another seek to (row', column', timerange_max) from there. we can only skip over to the timerange_max timestamp when we know that there are no deletecolumn records at the top of that row/column with a higher timestamp. we can utilize another bloom filter keyed on (row, column) to quickly find that out. ",
        "label": 359
    },
    {
        "text": "nullpointerexception in construction of regionserver in security cluster  the initialization of secure rpc server depends on regionserver's servername and zookeeper watcher. but, after hbase-10569, they are null when creating secure rpc services. jimmy xiang caused by: java.lang.nullpointerexception at org.apache.hadoop.hbase.ipc.rpcserver.createsecretmanager(rpcserver.java:1974) at org.apache.hadoop.hbase.ipc.rpcserver.start(rpcserver.java:1945) at org.apache.hadoop.hbase.regionserver.rsrpcservices.<init>(rsrpcservices.java:706) at org.apache.hadoop.hbase.master.masterrpcservices.<init>(masterrpcservices.java:190) at org.apache.hadoop.hbase.master.hmaster.createrpcservices(hmaster.java:297) at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:431) at org.apache.hadoop.hbase.master.hmaster.<init>(hmaster.java:234) ",
        "label": 242
    },
    {
        "text": "add in memory caching of data  bigtable provides two in-memory caches: one for row/column data and one for disk block caches. the size of each cache should be configurable, data should be loaded lazily, and the cache managed by an lru mechanism. one complication of the block cache is that all data is read through a sequencefile.reader which ultimately reads data off of disk via a rpc proxy for clientprotocol. this would imply that the block caching would have to be pushed down to either the dfsclient or sequencefile.reader ",
        "label": 447
    },
    {
        "text": "deprecate tablemapreduce adddependencyjars configuration  class     we expose two public static methods names adddependencyjars. one of them, void adddependencyjars(job, is very helpful \u2013 goes out of its way to detect job dependencies as well as shipping all the necessary hbase dependencies. the other is shfty and nefarious, void adddependencyjars(configuration, class<?>...) \u2013 it only adds exactly what the user requests, forcing them to resolve dependencies themselves and giving a false sense of security. we should deprecate the latter throw a big giant warning when people use that one. the handy functionality of providing help when our heuristics fail can be added via a new method signature, something like void adddependencyjars(job, class<?> .... this method would do everything void adddependencyjars(job does, plus let the user specify arbitrary additional classes. that way hbase still can help the user, but also gives them super-powers to compensate for when our heuristics fail. for reference, this appears to be the reason why hbase + pig doesn't really work out of the box. see hbasestorage.java ",
        "label": 402
    },
    {
        "text": "thrift server to match the new java api   this mutaterows, etc.. is a little confusing compared to the new cleaner java client.  thinking of ways to make a thrift client that is just as elegant. something like: void put(1:bytes table, 2:tput put) throws (1:ioerror io) with: struct tcolumn {  1:bytes family,  2:bytes qualifier,  3:i64 timestamp  } struct tput {  1:bytes row,  2:map<tcolumn, bytes> values  } this creates more verbose rpc than if the columns in tput were just map<bytes, map<bytes, bytes>>, but that is harder to fit timestamps into and still be intuitive from say python. presumably the goal of a thrift gateway is to be easy first. ",
        "label": 451
    },
    {
        "text": "bitcomparator bug   arrayindexoutofboundsexception  the hbase 0.94.1 bitcomparator introduced a bug in the method \"compareto\": @override   public int compareto(byte[] value, int offset, int length) {     if (length != this.value.length) {       return 1;     }     int b = 0;     //iterating backwards is faster because we can quit after one non-zero byte.     for (int i = value.length - 1; i >= 0 && b == 0; i--) {       switch (bitoperator) {         case and:           b = (this.value[i] & value[i+offset]) & 0xff;           break;         case or:           b = (this.value[i] | value[i+offset]) & 0xff;           break;         case xor:           b = (this.value[i] ^ value[i+offset]) & 0xff;           break;       }     }     return b == 0 ? 1 : 0;   } i've encountered this problem when using a bitcomparator with a configured this.value.length=8, and in the hbase table there were keyvalues with keyvalue.getbuffer().length=207911 bytes. in this case:     for (int i = 207910; i >= 0 && b == 0; i--) {       switch (bitoperator) {         case and:           b = (this.value[207910] ... ==> arrayindexoutofboundsexception           break; that loop should use:   for (int i = length - 1; i >= 0 && b == 0; i--) { (or this.value.length.) should i provide a patch for correcting the problem? ",
        "label": 296
    },
    {
        "text": "hbck need an option  help to printout usage  hbck doesn't have an option to print out usage although we have this function. it is better to have it so that we don't need to remember them all, especially the list is keeping growing. ",
        "label": 242
    },
    {
        "text": "audit hbase for usage of deprecated hadoop x property names   many xml config properties in hadoop have changed in 0.23. we should audit hbase to insulate it from hadoop property name changes. here is a list of the hadoop property name changes: hadoop.native.lib -> io.native.lib.available  mapred.job.classpath.archives -> mapreduce.job.classpath.archives  mapred.map.tasks.speculative.execution -> mapreduce.map.speculative  mapred.task.id -> mapreduce.task.attempt.id  mapred.output.compress -> mapreduce.output.fileoutputformat.compress  mapred.output.compression.codec -> mapreduce.output.fileoutputformat.compress.codec  mapred.output.compression.type -> mapreduce.output.fileoutputformat.compress.type  mapred.reduce.tasks.speculative.execution -> mapreduce.reduce.speculative  mapred.input.dir -> mapreduce.input.fileinputformat.inputdir  mapred.job.name -> mapreduce.job.name  mapred.local.dir -> mapreduce.cluster.local.dir  mapred.temp.dir -> mapreduce.cluster.temp.dir   mapred.system.dir -> mapreduce.jobtracker.system.dir  mapred.working.dir -> mapreduce.job.working.dir  mapred.job.tracker -> mapreduce.jobtracker.address  heartbeat.recheck.interval -> dfs.namenode.heartbeat.recheck-interval  dfs.socket.timeout -> dfs.client.socket-timeout  dfs.block.size -> dfs.blocksize  io.sort.mb -> mapreduce.task.io.sort.mb  mapred.input.dir -> mapreduce.input.fileinputformat.inputdir  mapred.input.dir -> mapreduce.input.fileinputformat.inputdir  min.num.spills.for.combine -> mapreduce.map.combine.minspills  mapred.map.max.attempts -> mapreduce.map.max.attempts  dfs.socket.timeout -> dfs.client.socket-timeout  dfs.datanode.max.xcievers -> dfs.datanode.max.transfer.threads ",
        "label": 423
    },
    {
        "text": "get rid of zktable  and table enable disable state in zk  as discussed table state in zookeeper for enable/disable state breaks our zookeeper contract. it is also very intrusive, used from the client side, master and region servers. we should get rid of it. ",
        "label": 41
    },
    {
        "text": "does not recover if hrs carrying  root  goes down  from the list: trunk does not recover if root goes down. had an instance of  this last night. andy > > from: stack <stack@duboce.net>  > > subject: re: recovering hbase   [...]  > > system should recover when region hosting root goes down.  > > which version of hbase (pardon me if you've already said  > > which version)?  > >   > > st.ack ",
        "label": 241
    },
    {
        "text": "support dataframe writer to the spark connector  currently, the connector only support read path. a complete solution should support both read and writer. this subtask add write support. ",
        "label": 512
    },
    {
        "text": "server startup fails during startup due to failure in loading all table descriptors  we should ignore  logs oldlogs corrupt meta root  folders while reading descriptors  2011-06-23 21:39:52,524 warn org.apache.hadoop.hbase.monitoring.taskmonitor: status org.apache.hadoop.hbase.monitoring.monitoredtaskimpl@2f56f920 appears to have been leaked  2011-06-23 21:40:06,465 warn org.apache.hadoop.hbase.master.hmaster: failed getting all descriptors  java.io.filenotfoundexception: no status for hdfs://ciq.com:9000/hbase/.corrupt  at org.apache.hadoop.hbase.util.fsutils.gettableinfomodtime(fsutils.java:888)  at org.apache.hadoop.hbase.util.fstabledescriptors.get(fstabledescriptors.java:122)  at org.apache.hadoop.hbase.util.fstabledescriptors.getall(fstabledescriptors.java:149)  at org.apache.hadoop.hbase.master.hmaster.gethtabledescriptors(hmaster.java:1442)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:340)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1138)  2011-06-23 21:40:26,790 warn org.apache.hadoop.hbase.master.hmaster: failed getting all descriptors  java.io.filenotfoundexception: no status for hdfs://ciq.com:9000/hbase/.corrupt  at org.apache.hadoop.hbase.util.fsutils.gettableinfomodtime(fsutils.java:888)  at org.apache.hadoop.hbase.util.fstabledescriptors.get(fstabledescriptors.java:122)  at org.apache.hadoop.hbase.util.fstabledescriptors.getall(fstabledescriptors.java:149)  at org.apache.hadoop.hbase.master.hmaster.gethtabledescriptors(hmaster.java:1442)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:340)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1138) ",
        "label": 428
    },
    {
        "text": "hbase client throws nosuchelementexception  soft reference objects, which are cleared at the discretion of the   garbage collector in response to memory demand. i used ycsb to put data and threw exception. >>>>   >>>> hbase code:  >>>> // cut the cache so that we only get the part that could contain  >>>> // regions that match our key  >>>> softvaluesortedmap<byte[], hregionlocation> matchingregions =  >>>> tablelocations.headmap(row);  >>>>   >>>> // if that portion of the map is empty, then we're done. otherwise,  >>>> // we need to examine the cached location to verify that it is  >>>> // a match by end key as well.  >>>> if (!matchingregions.isempty()) {  >>>> hregionlocation possibleregion =  >>>> matchingregions.get(matchingregions.lastkey());  >>>>   >>>> ycsb client log:  >>>>   >>>> [java] begin statusthread run  >>>> [java] java.util.nosuchelementexception  >>>> [java] at java.util.treemap.key(treemap.java:1206)  >>>> [java] at  >> java.util.treemap$navigablesubmap.lastkey(treemap.java:1435)  >>>> [java] at  >> org.apache.hadoop.hbase.util.softvaluesortedmap.lastkey(softvaluesort  >> edmap.java:131)  >>>> [java] at  >> org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplemen  >> tation.getcachedlocation(hconnectionmanager.java:841)  >>>> [java] at  >> org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplemen  >> tation.locateregioninmeta(hconnectionmanager.java:664)  >>>> [java] at  >> org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplemen  >> tation.locateregion(hconnectionmanager.java:590)  >>>> [java] at  >> org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplemen  >> tation.processbatch(hconnectionmanager.java:1114)  >>>> [java] at  >> org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplemen  >> tation.processbatchofputs(hconnectionmanager.java:1234)  >>>> [java] at  >> org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:819)  >>>> [java] at  >> org.apache.hadoop.hbase.client.htable.doput(htable.java:675)  >>>> [java] at  >> org.apache.hadoop.hbase.client.htable.put(htable.java:665)  >>>> [java] at com.yahoo.ycsb.db.hbaseclient.update(unknown source)  >>>> [java] at com.yahoo.ycsb.db.hbaseclient.insert(unknown source)  >>>> [java] at com.yahoo.ycsb.dbwrapper.insert(unknown source)  >>>> [java] at com.yahoo.ycsb.workloads.myworkload.doinsert(unknown  >> source)  >>>> [java] at com.yahoo.ycsb.clientthread.run(unknown source) ",
        "label": 441
    },
    {
        "text": "remove unused member and local variables from hregionserver  while working on hbase-6493, i noticed an unused local and member variable. ",
        "label": 339
    },
    {
        "text": "can't start stop start  cluster using new master  currently you might start a small cluster the first time on trunk \u2013 i.e. new master \u2013 but second time you do the startup you run into a couple of interesting issues: + the old root-region-location is still in place. it gets cleaned later but for a while on startup it does not have the 'right' address.  + regionserver (or a client) on startup creates a catalogtracker, a class that notices changes in meta tables keeping up catalog table locations. starting the catalogtracker results in a check for current catalog locations. as part of this process, since root-region-location \"exists\", catalogtracker tries to verify root's location by doing a noop against root host, only, to do this it needs to do the initial rpc proxy setup. it can so happen that the old root address was that of the current regionserver trying to initialize so we'll be trying to connect to ourself to verify root location only, we're doing this before we've setup the rpcserver and handlers \u2013 so we block, and as it happens there is no timeout on proxy setup (todd ran into this yesterday, i ran into it today \u2013 its easy to manufacture).  + so regionserver can't progress. meantime the master can't progress because there are no regionservers checking in. and you can't shut it down because we're not looking at the right 'stop' flag ",
        "label": 314
    },
    {
        "text": "merge tool won't merge two overlapping regions  hbase-480 adds a merge tool. seems to work for two adjacent regions in tests but not for the case where we've made two overlapping regions: i.e. both have a start key of 'a' but one regions endkey is less than the end key of the others (see up on lars cluster hbase-471). i ran the merge and it made a new region with the right start and end keys but there was nothing in it. we need this tool to do fix up when hbase goes awry. ",
        "label": 241
    },
    {
        "text": "bulk insert with multiple reducers broken due to improper immutablebyteswritable comparator  if i run mr to prepare hfiles with more than one reducer then some values for keys are not appeared in the table after loadtable.rb script execution. with one reducer everything works fine. references:  http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapreduce/package-summary.html#bulk the row id must be formatted as a immutablebyteswritable mr job should ensure a total ordering among all keys mapreduce-366 (patch-5668-3.txt) totalorderpartitioner that uses the new api (attached) hbase-2063 patched hfileoutputformat (attached) input data (attached): my_sample_log_1k.txt - sample data, input for myhfileswriter source (attached): mykeycomparator.java - comparator for my immutablebyteswritable keys testtotalorderpartitionerformykeys.java - test case for my keys (note that i've set up mykeycomparator to pass that test) myhfileswriter.java - my mr job to prepare hfiles hfileoutputformat.java - from mapreduce-366 totalorderpartitioner.java - from mapreduce-366 mysampler.java - my randomsampler based on sampler from mapreduce-366 but i've put the following string into getsample method (without that string it doesn't work):             reader.initialize(splits.get(i), new taskattemptcontext(job.getconfiguration(), new taskattemptid())); test case: 1. comment the following string in myhfileswriter: //job.setsortcomparatorclass(mykeycomparator.class); 2. hadoop jar keyvalue-poc.jar myhfileswriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/01/ -r 1 3. hadoop jar keyvalue-poc.jar myhfileswriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/02/ -r 2 4. hbase> create 'tst_hfiles_01', {name => 'vals'}  # hbase> create 'tst_hfiles_02', {name => 'vals'} 5. hbase org.jruby.main /usr/lib/hbase-0.20/bin/loadtable.rb tst_hfiles_01 /test_hbase/hfiles/01 6. hbase org.jruby.main /usr/lib/hbase-0.20/bin/loadtable.rb tst_hfiles_02 /test_hbase/hfiles/02 7. check values for keys 8. uncomment the following string in myhfileswriter: //job.setsortcomparatorclass(mykeycomparator.class); 9. hadoop jar keyvalue-poc.jar myhfileswriter -in /test_hbase/my_sample_log_1k.txt -out /test_hbase/hfiles/03/ -r 2 for example, results: hbase(main):006:0* count 'tst_hfiles_01', 100  current count: 100, row: 0.14.usa.il.602.elmhurst.1.1.0.0                                                      current count: 200, row: 0.245.usa.me.500.portland.1.1.0.0                                                     current count: 300, row: 0.34.usa.fl.rollup.rollup.1.1.0.0                                                     current count: 400, row: 0.443.usa.ca.803.los.angeles.1.1.0                                                    current count: 500, row: 0.8.usa.co.751.castle.rock.1.1.0                                                      current count: 600, row: 1.14.dza.rollup.rollup.rollup.1.1.0.1                                                 current count: 700, row: 1.159.swe.ab.rollup.rollup.1.1.0.1                                                    current count: 800, row: 1.17.usa.tn.659.clarksville.1.1.0.1                                                   current count: 900, row: 1.220.usa.mi.505.southfield.1.1.0.1                                                   999 row(s) in 0.0930 seconds hbase(main):007:0> count 'tst_hfiles_02', 100 current count: 100, row: 0.231.usa.ga.524.buford.1.1.0.1                                                       current count: 200, row: 0.4.usa.va.573.rollup.1.1.0.0                                                         current count: 300, row: 0.9.rou.b.-1.bucharest.1.1.0.0                                                        current count: 400, row: 1.16.usa.ia.679.rollup.1.1.1.0                                                        current count: 500, row: 1.245.nor.03.-1.oslo.1.1.0.0                                                          current count: 600, row: 0.245.gbr.eng.826005.bexley.1.1.0.1                                                   current count: 700, row: 0.48.gbr.eng.826027.rollup.1.1.0.1                                                    current count: 800, row: 1.14.swe.rollup.rollup.rollup.1.1.0.1                                                 current count: 900, row: 1.201.gbr.eng.826005.london.1.1.0.1                                                   999 row(s) in 0.1630 seconds hbase(main):008:0> get 'tst_hfiles_01', '0.14.usa.il.602.elmhurst.1.1.0.0' column                       cell                                                                               vals:key0                   timestamp=1269542753914, value=0                                                   vals:key1                   timestamp=1269542753914, value=14                                                  vals:key2                   timestamp=1269542753914, value=usa                                                 vals:key3                   timestamp=1269542753914, value=il                                                  vals:key4                   timestamp=1269542753914, value=602                                                 vals:key5                   timestamp=1269542753914, value=elmhurst                                            vals:key6                   timestamp=1269542753914, value=1                                                   vals:key7                   timestamp=1269542753914, value=1                                                   vals:key8                   timestamp=1269542753914, value=0                                                   vals:key9                   timestamp=1269542753914, value=0                                                   vals:val0                   timestamp=1269542753914, value=2                                                  11 row(s) in 0.0160 seconds hbase(main):009:0> get 'tst_hfiles_02', '0.14.usa.il.602.elmhurst.1.1.0.0' column                       cell                                                                              0 row(s) in 0.0220 seconds with mykeycomparator java.io.ioexception: added a key not lexically larger than previous key=.103.fra.v.-1.lyon.1.1.0.0valskey0'xxx, lastkey=1.20.usa.aol.0.aol.1.1.0.0valsval0'xxx at org.apache.hadoop.hbase.io.hfile.hfile$writer.checkkey(hfile.java:551) at org.apache.hadoop.hbase.io.hfile.hfile$writer.append(hfile.java:513) at org.apache.hadoop.hbase.io.hfile.hfile$writer.append(hfile.java:481) at com.contextweb.hadoop.hbase.mapred.hfileoutputformat$1.write(hfileoutputformat.java:77) at com.contextweb.hadoop.hbase.mapred.hfileoutputformat$1.write(hfileoutputformat.java:49) at org.apache.hadoop.mapred.reducetask$newtrackingrecordwriter.write(reducetask.java:508) at org.apache.hadoop.mapreduce.taskinputoutputcontext.write(taskinputoutputcontext.java:80) at org.apache.hadoop.hbase.mapreduce.keyvaluesortreducer.reduce(keyvaluesortreducer.java:46) at org.apache.hadoop.hbase.mapreduce.keyvaluesortreducer.reduce(keyvaluesortreducer.java:35) at org.apache.hadoop.mapreduce.reducer.run(reducer.java:176) at org.apache.hadoop.mapred.reducetask.runnewreducer(reducetask.java:566) at org.apache.hadoop.mapred.reducetask.run(reducetask.java:408) at org.apache.hadoop.mapred.child.main(child.java:170) ",
        "label": 453
    },
    {
        "text": " visibilitycontroller  system table backed scanlabelgenerator  a scanlabelgenerator that retrieves a static set of authorizations for a user or group from a new hbase system table, and insures these auths are part of the effective set. useful for forcing a baseline set of auths for a user. ",
        "label": 38
    },
    {
        "text": "migration script to up the versions in catalog tables  rewrite meta regions with the versions set to 10 instead of 1. see hbase-993. ",
        "label": 314
    },
    {
        "text": "zookeeper cleanup and refactor  currently almost everything we do with zookeeper is stuffed into a single class zookeeperwrapper. this issue will deal with cleaning up our usage of zk, adding some new abstractions to help with the master changes, splitting up watchers from utility methods, and nailing down the contracts of our zk methods with respect to setting watchers, throwing exceptions, etc... ",
        "label": 247
    },
    {
        "text": "splitlogmanager   prevent unnecessary attempts to resubmits  currently once a watch fires that the task node has been updated (hearbeated) by the worker, the splitlogmanager still quite some time before it updates the \"last heard from\" time. this is because the manager currently schedules another getdatasetwatch() and only after that finishes will it update the task's \"last heard from\" time. this leads to a large number of zk-badversion warnings when resubmission is continuously attempted and it fails. two changes should be made  (1) on a resubmission failure because of badversion the task's lastupdate time should get upped.  (2) the task's lastupdate time should get upped as soon as the nodedatachanged() watch fires and without waiting for getdatasetwatch() to complete. ",
        "label": 356
    },
    {
        "text": "use global procedure to flush table memstore cache  currently, user can trigger table flush through hbase shell or hbaseadmin api. to flush the table cache, each region server hosting the regions is contacted and flushed sequentially, which is less efficient.  in hbase snapshot global procedure is used to coordinate and flush the regions in a distributed way.  let's provide a distributed table flush for general use. ",
        "label": 234
    },
    {
        "text": "failed taking snapshot   manifest proto message too large  the size of a protobuf message is 64mb (default). but the size of snapshot meta is over 64mb. caused by: com.google.protobuf.invalidprotocolbufferexception via failed taking snapshot { ss=snapshot_xxx table=xxx type=flush } due to exception:protocol message was too large. may be malicious. use codedinputstream.setsizelimit() to increase the size limit.:com.google.protobuf.invalidprotocolbufferexception: protocol message was too large. may be malicious. use codedinputstream.setsizelimit() to increase the size limit.  at org.apache.hadoop.hbase.errorhandling.foreignexceptiondispatcher.rethrowexception(foreignexceptiondispatcher.java:83)  at org.apache.hadoop.hbase.master.snapshot.takesnapshothandler.rethrowexceptioniffailed(takesnapshothandler.java:307)  at org.apache.hadoop.hbase.master.snapshot.snapshotmanager.issnapshotdone(snapshotmanager.java:341)  ... 10 more  caused by: com.google.protobuf.invalidprotocolbufferexception: protocol message was too large. may be malicious. use codedinputstream.setsizelimit() to increase the size limit.  at com.google.protobuf.invalidprotocolbufferexception.sizelimitexceeded(invalidprotocolbufferexception.java:110)  at com.google.protobuf.codedinputstream.refillbuffer(codedinputstream.java:755)  at com.google.protobuf.codedinputstream.readrawbytes(codedinputstream.java:811)  at com.google.protobuf.codedinputstream.readbytes(codedinputstream.java:329)  at org.apache.hadoop.hbase.protobuf.generated.hbaseprotos$regioninfo.<init>(hbaseprotos.java:3767)  at org.apache.hadoop.hbase.protobuf.generated.hbaseprotos$regioninfo.<init>(hbaseprotos.java:3699)  at org.apache.hadoop.hbase.protobuf.generated.hbaseprotos$regioninfo$1.parsepartialfrom(hbaseprotos.java:3815)  at org.apache.hadoop.hbase.protobuf.generated.hbaseprotos$regioninfo$1.parsepartialfrom(hbaseprotos.java:3810)  at com.google.protobuf.codedinputstream.readmessage(codedinputstream.java:309)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotregionmanifest.<init>(snapshotprotos.java:1152)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotregionmanifest.<init>(snapshotprotos.java:1094)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotregionmanifest$1.parsepartialfrom(snapshotprotos.java:1201)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotregionmanifest$1.parsepartialfrom(snapshotprotos.java:1196)  at com.google.protobuf.codedinputstream.readmessage(codedinputstream.java:309)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotdatamanifest.<init>(snapshotprotos.java:3858)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotdatamanifest.<init>(snapshotprotos.java:3792)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotdatamanifest$1.parsepartialfrom(snapshotprotos.java:3894)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotdatamanifest$1.parsepartialfrom(snapshotprotos.java:3889)  at com.google.protobuf.abstractparser.parsepartialfrom(abstractparser.java:200)  at com.google.protobuf.abstractparser.parsefrom(abstractparser.java:217)  at com.google.protobuf.abstractparser.parsefrom(abstractparser.java:223)  at com.google.protobuf.abstractparser.parsefrom(abstractparser.java:49)  at org.apache.hadoop.hbase.protobuf.generated.snapshotprotos$snapshotdatamanifest.parsefrom(snapshotprotos.java:4094)  at org.apache.hadoop.hbase.snapshot.snapshotmanifest.readdatamanifest(snapshotmanifest.java:433)  at org.apache.hadoop.hbase.snapshot.snapshotmanifest.load(snapshotmanifest.java:273)  at org.apache.hadoop.hbase.snapshot.snapshotmanifest.open(snapshotmanifest.java:119)  at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifysnapshot(mastersnapshotverifier.java:106 ",
        "label": 256
    },
    {
        "text": "throwing ioe read only when should be throwing nsre  am seeing exceptions like the following during 'normal' operation though the region has not been explicitly set to be read-only (new feature added with commit of hbase-62). 2008-07-21 20:50:25,071 info org.apache.hadoop.ipc.server: ipc server handler 3 on 60020, call batchupdate([b@63443c, row => 0000791906, {column => info:data, value => '...'}) from xx.xx.xx.139:59778: error: java.io.ioexception: region is read only java.io.ioexception: region is read only         at org.apache.hadoop.hbase.regionserver.hregion.batchupdate(hregion.java:1322)         at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1151)         at sun.reflect.generatedmethodaccessor5.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)         at org.apache.hadoop.ipc.server$handler.run(server.java:896) ",
        "label": 314
    },
    {
        "text": "add kerberos http spnego authentication support to hbase web consoles  like hadoop-7119, the same motivations: hadoop rpc already supports kerberos authentication. as does the hbase secure rpc engine. kerberos enables single sign-on. popular browsers (firefox and internet explorer) have support for kerberos http spnego. adding support for kerberos http spnego to [hbase] web consoles would provide a unified authentication mechanism and single sign-on for web ui and rpc. also like hadoop-7119, the same solution: a servlet filter is configured in front of all hadoop web consoles for authentication. this filter verifies if the incoming request is already authenticated by the presence of a signed http cookie. if the cookie is present, its signature is valid and its value didn't expire; then the request continues its way to the page invoked by the request. if the cookie is not present, it is invalid or it expired; then the request is delegated to an authenticator handler. the authenticator handler then is responsible for requesting/validating the user-agent for the user credentials. this may require one or more additional interactions between the authenticator handler and the user-agent (which will be multiple http requests). once the authenticator handler verifies the credentials and generates an authentication token, a signed cookie is returned to the user-agent for all subsequent invocations. the authenticator handler is pluggable and 2 implementations are provided out of the box: pseudo/simple and kerberos. 1. the pseudo/simple authenticator handler is equivalent to the hadoop pseudo/simple authentication. it trusts the value of the user.name query string parameter. the pseudo/simple authenticator handler supports an anonymous mode which accepts any request without requiring the user.name query string parameter to create the token. this is the default behavior, preserving the behavior of the hbase web consoles before this patch. 2. the kerberos authenticator handler implements the kerberos http spnego implementation. this authenticator handler will generate a token only if a successful kerberos http spnego interaction is performed between the user-agent and the authenticator. browsers like firefox and internet explorer support kerberos http spnego. we can build on the support added to hadoop via hadoop-7119. should just be a matter of wiring up the filter to our infoservers in a similar manner. and from https://issues.apache.org/jira/browse/hbase-5050?focusedcommentid=13171086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13171086 hadoop 0.23 onwards has a hadoop-auth artifact that provides spnego/kerberos authentication for webapps via a filter. you should consider using it. you don't have to move hbase to 0.23 for that, just consume the hadoop-auth artifact, which has no dependencies on the rest of hadoop 0.23 artifacts. ",
        "label": 252
    },
    {
        "text": "documentation should have more information of lru stats  unfortunately, there's no documentation to explain the meaning of each lru stats in the regionserver logs. so this is for creating a new paragraph regarding this. my current idea is below, but it's a little bit difficult to explain the difference between 'cachingaccesses' and 'accesses' from an administrator or a user views.  could you guys help to improve the content? total: the current memory size of the cache in use. free: the total free memory currently available to store more cache entries. max: maximum allowed memory size of the cache. blocks: caches store blocks of data; this number is the current # of blocks stored, which use up the \"total\" memory space. accesses: the total number of times the cache was accessed, regardless of result. hits: the total number of times the cache was accessed and the result was a successful hit (presence of looked up element in cache is a hit). hitratio: the current percentage for \"hits / accesses\". ==== unclear: cachingaccesses: cachinghits + the number of getblock requests that were cache misses, but only from requests that were set to use the block cache. cachinghits: the number of getblock requests that were cache hits, but only from requests that were set to use the block cache. this is because all reads ===== cachinghitsratio: the current percentage for \"cachinthits / cachingaccesses\" evictions: the total number of times an eviction has occurred (based on the use of the lru algorithm) evicted: the total number of blocks that have been evicted (based on the use of the lru algorithm) evictedperrun: the total number of blocks that have been evicted overall / the number of times an eviction has occurred overall and also, where should we add this paragraph in the documentation? ",
        "label": 330
    },
    {
        "text": "prefixfilter with or condition gives wrong results  prefixfilter when used with a singlecolumnvaluefilter with an or condition gives wrong results. in below example, each filter when evaluated separately gives 1 row each. the or condition with the two filters gives 3 rows instead of 2. repro below: create 't1', 'f1'  put 't1','a1','f1:q2','111'  put 't1','b1','f1:q1','112'  put 't1','c1','f1:q1','113' hbase(main):020:0> scan 't1', {filter => \"prefixfilter ('b') or singlecolumnvaluefilter('f1', 'q1', =, 'binary:113')\"} row column+cell  a1 column=f1:q2, timestamp=1381468905492, value=111  b1 column=f1:q1, timestamp=1381468905518, value=112  c1 column=f1:q1, timestamp=1381468905549, value=113  3 row(s) in 0.1020 seconds hbase(main):021:0> scan 't1', {filter => \"prefixfilter ('b')\"} row column+cell  b1 column=f1:q1, timestamp=1381468905518, value=112  1 row(s) in 0.0150 seconds hbase(main):002:0> scan 't1', {filter => \"singlecolumnvaluefilter('f1', 'q1', =, 'binary:113')\"} row column+cell  c1 column=f1:q1, timestamp=1381469178679, value=113  1 row(s) in 0.0140 seconds ",
        "label": 7
    },
    {
        "text": "admin move without specifying destination does not go through accesscontroller      if (destservername == null || destservername.length == 0) {       log.info(\"passed destination servername is null/empty so \" +         \"choosing a server at random\");       this.assignmentmanager.clearregionplan(hri);       // unassign will reassign it elsewhere choosing random server.       this.assignmentmanager.unassign(hri); i think we should go through security to see if there is sufficient permissions to do this operation? ",
        "label": 543
    },
    {
        "text": "document the removal of replication state aka start stop replication  the first two tutorials for enabling replication that google gives me [1], [2] take very different tones with regard to stop_replication. the hbase docs [1] make it sound fine to start and stop replication as desired. the cloudera docs [2] say it may cause data loss. which is true? if data loss is possible, are we talking about data loss in the primary cluster, or data loss in the standby cluster (presumably would require reinitializing the sync with a new copytable). [1] http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/replication/package-summary.html#requirements  [2] http://www.cloudera.com/content/cloudera-content/cloudera-docs/cdh4/4.2.0/cdh4-installation-guide/cdh4ig_topic_20_11.html ",
        "label": 330
    },
    {
        "text": "hbaseclient wastes tcp packet per rpc  in ipc/hbaseclient.java, the method sendparam does: out.writeint(datalength);      //first put the data length out.write(data, 0, datalength);//write the data while analyzing some tcpdump traces tonight, i saw that this consistently translates to 1 tcp packet with a 4 byte payload followed by another tcp packet with the rpc itself. this makes inefficient use of network resources and adversely affects tcp throughput. i believe each of those lines translates to a write system call on the socket's file descriptor (unnecessary system calls are also bad for performance). the code attempts to call out.flush(); but this approach is ineffective on sockets in java (as far as i empirically noticed over the past few months). ",
        "label": 453
    },
    {
        "text": "testregionmergetransactiononcluster testwholesomemerge may fail due to race in opening region  from http://54.241.6.143/job/hbase-trunk-hadoop-2/org.apache.hbase$hbase-server/395/testreport/org.apache.hadoop.hbase.regionserver/testregionmergetransactiononcluster/testwholesomemerge/ : 013-07-11 09:33:44,154 info  [am.zk.worker-pool-2-thread-2] master.regionstates(309): offlined 3ffefd878a234031675de6b2c70b2ead from ip-10-174-118-204.us-west-1.compute.internal,60498,1373535184820 2013-07-11 09:33:44,154 info  [am.zk.worker-pool-2-thread-2] master.assignmentmanager$4(1223): the master has opened testwholesomemerge,testrow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead. that was online on ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884 2013-07-11 09:33:44,182 debug [rs_open_region-ip-10-174-118-204:59210-1] zookeeper.zkassign(862): regionserver:59210-0x13fcd13a20c0002 successfully transitioned node 3ffefd878a234031675de6b2c70b2ead from rs_zk_region_opening to rs_zk_region_opened 2013-07-11 09:33:44,182 info  [master_table_operations-ip-10-174-118-204:39405-0] handler.dispatchmergingregionhandler(154): failed send merge regions rpc to server ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884 for region testwholesomemerge,,1373535210124.efcb10dcfa250e31bfd50dc6c7049f32.,testwholesomemerge,testrow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead., focible=false, org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: 3ffefd878a234031675de6b2c70b2ead at org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname(hregionserver.java:2566) at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:3862) at org.apache.hadoop.hbase.regionserver.hregionserver.mergeregions(hregionserver.java:3649) at org.apache.hadoop.hbase.protobuf.generated.adminprotos$adminservice$2.callblockingmethod(adminprotos.java:14400) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2124) at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1831) 2013-07-11 09:33:44,182 debug [rs_open_region-ip-10-174-118-204:59210-1] handler.openregionhandler(373): region transitioned to opened in zookeeper: {encoded => 3ffefd878a234031675de6b2c70b2ead, name => 'testwholesomemerge,testrow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead.', startkey => 'testrow0020', endkey => 'testrow0040'}, server: ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884 2013-07-11 09:33:44,183 debug [rs_open_region-ip-10-174-118-204:59210-1] handler.openregionhandler(186): opened testwholesomemerge,testrow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead. on server:ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884 we can see that master_table_operations thread couldn't get region 3ffefd878a234031675de6b2c70b2ead because rs_open_region thread finished region opening 1 millisecond later. one solution is to retry operation when receiving regionopeningexception ",
        "label": 107
    },
    {
        "text": "make hbase more 'live' when comes to noticing table creation  splits  etc  for  clusters > 10 are less frequent that smaller clusters. defaults for when messages are sent from regionserver to master would seem to favor clusters that are much larger than ten nodes. this issue is about changing them so the basic hbase setup is more live. its about downing the interval between rs sending messages from 3 to 1 second and of making clients retry more frequently at least at the start of the timeout period so they pick up changes the quicker. this should help with stuff like noticing splits and new regions coming on line. see hbase-1892 for background. ",
        "label": 229
    },
    {
        "text": "meta region edits not recovered while migrating to  i was doing the migration testing from 0.94.11-snapshot to 0.95.0, and faced this issue. 1) do some edits in meta table (for eg, create a table). 2) kill the cluster.  (i used kill because we would be doing log splitting when upgrading anyway). 3) there is some dependency on wals. upgrade the bits to 0.95.2-snapshot. start the cluster.  every thing comes up. i see log splitting happening as expected. but, the wal-data for meta table is missing. i could see recovered.edits file for meta created, and placed at the right location. it is just that the new hmaster code tries to recover meta by looking at meta prefix in the log name, and if it didn't find one, just opens the meta region. so, the recovered.edits file, created afterwards, is not honored. opening this jira to let folks give their opinions about how to tackle this migration issue. ",
        "label": 199
    },
    {
        "text": "silent data offlining during hdfs flakiness  see hbase-1436 . the bug fix for this jira is a temporary workaround for improperly moving partially-written files from tmp into the region directory when a fs error occurs. unfortunately, the fix is to ignore all io exceptions, which masks off-lining due to fs flakiness. we need to permanently fix the problem that created hbase-1436 & then at least have the option to not open a region during times of flakey fs. ",
        "label": 359
    },
    {
        "text": "hregionserver should be stopped even if no meta regions are hosted by the hregionserver  hregionserver always makes sure one meta region is hosted for it to stop. this should be changed so that even if no meta regions are hosted, the hregionserver should be stopped once all user regions are closed. ",
        "label": 11
    },
    {
        "text": "get cells via shell with a time range predicate  hbase shell allows to specify a timestamp to get a value get 't1', 'r1', {column => 'c1', timestamp => ts1} if you don't give the exact timestamp, you get nothing... so it's difficult to get the cell previous versions. it would be fine to have a \"time range\" predicate based get.  the shell syntax could be (depending on technical feasibility) get 't1', 'r1', {column => 'c1', timerange => (start_timestamp, end_timestamp)} ",
        "label": 441
    },
    {
        "text": "add an option to enable disable rs group feature  i think we'd better follow the way of acl, although the balancer could always be rsgroupbasedloadbalancer, but the rsgroupinfomanager can be different. if we disable rs group feature, we can introduced a disabledrsgroupinfomanager. ",
        "label": 149
    },
    {
        "text": "prefixfilter filterkeyvalue  should perform filtering on row key  niels reported an issue under the thread 'trouble writing custom filter for use in filterlist' where his custom filter used in filterlist along with prefixfilter produced an unexpected results. his test can be found here:  https://github.com/nielsbasjes/hbase-filter-problem this is due to prefixfilter#filterkeyvalue() using filterbase#filterkeyvalue() which returns returncode.include  when filterlist.operator.must_pass_one is specified, filterlist#filterkeyvalue() would return returncode.include even when row key prefix doesn't match meanwhile the other filter's filterkeyvalue() returns returncode.next_col ",
        "label": 441
    },
    {
        "text": "npe in distributed log splitting  there is an issue with the log splitting under the specific condition of edits belonging to a non existing region (which went away after a split for example). the hlogsplitter fails to check the condition, which is handled on a lower level, logging manifests it as 2011-05-16 13:56:10,300 info org.apache.hadoop.hbase.regionserver.wal.hlogsplitter: this region's directory doesn't exist: hdfs://localhost:8020/hbase/usertable/30c4d0a47703214845d0676d0c7b36f0. it is very likely that it was already split so it's safe to discard those edits. the code returns a null reference which is not check in hlogsplitter.splitlogfiletotemp(): ...         writerandpath wap = (writerandpath)o;         if (wap == null) {           wap = createwap(region, entry, rootdir, tmpname, fs, conf);           if (wap == null) {             logwriters.put(region, bad_writer);           } else {             logwriters.put(region, wap);           }         }         wap.w.append(entry); ... the createwap does return \"null\" when the above message is logged based on the obsolete region reference in the edit. what made this difficult to detect is that the error (and others) are silently ignored in splitlogworker.grabtask(). i added a catch and error logging to see the npe that was caused by the above. ...           break;       }     } catch (exception e) {       log.error(\"an error occurred.\", e);     } finally {       if (t > 0) { ... as a side note, there are other errors/asserts triggered that this try/finally not handles. for example 2011-05-16 13:58:30,647 warn org.apache.hadoop.hbase.regionserver.splitlogworker: badversion failed to assert ownership for /hbase/splitlog/hdfs%3a%2f%2flocalhost%2fhbase%2f.logs%2f10.0.0.65%2c60020%2c1305406356765%2f10.0.0.65%252c60020%252c1305406356765.1305409968389 org.apache.zookeeper.keeperexception$badversionexception: keepererrorcode = badversion for /hbase/splitlog/hdfs%3a%2f%2flocalhost%2fhbase%2f.logs%2f10.0.0.65%2c60020%2c1305406356765%2f10.0.0.65%252c60020%252c1305406356765.1305409968389         at org.apache.zookeeper.keeperexception.create(keeperexception.java:106)         at org.apache.zookeeper.keeperexception.create(keeperexception.java:42)         at org.apache.zookeeper.zookeeper.setdata(zookeeper.java:1038)         at org.apache.hadoop.hbase.regionserver.splitlogworker.owntask(splitlogworker.java:329)         at org.apache.hadoop.hbase.regionserver.splitlogworker.access$100(splitlogworker.java:68)         at org.apache.hadoop.hbase.regionserver.splitlogworker$2.progress(splitlogworker.java:265)         at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlogfiletotemp(hlogsplitter.java:432)         at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlogfiletotemp(hlogsplitter.java:354)         at org.apache.hadoop.hbase.regionserver.splitlogworker$1.exec(splitlogworker.java:113)         at org.apache.hadoop.hbase.regionserver.splitlogworker.grabtask(splitlogworker.java:260)         at org.apache.hadoop.hbase.regionserver.splitlogworker.taskloop(splitlogworker.java:191)         at org.apache.hadoop.hbase.regionserver.splitlogworker.run(splitlogworker.java:164)         at java.lang.thread.run(thread.java:680) this should probably be handled - or at least documented - in another issue? the npe made the log split end and the splitlogmanager add an endless amount of rescan entries as this never came to an end. ",
        "label": 44
    },
    {
        "text": "tinylfu based blockcache  lrublockcache uses the segmented lru (slru) policy to capture frequency and recency of the working set. it achieves concurrency by using an o( n ) background thread to prioritize the entries and evict. accessing an entry is o(1) by a hash table lookup, recording its logical access time, and setting a frequency flag. a write is performed in o(1) time by updating the hash table and triggering an async eviction thread. this provides ideal concurrency and minimizes the latencies by penalizing the thread instead of the caller. however the policy does not age the frequencies and may not be resilient to various workload patterns. w-tinylfu (research paper) records the frequency in a counting sketch, ages periodically by halving the counters, and orders entries by slru. an entry is discarded by comparing the frequency of the new arrival (candidate) to the slru's victim, and keeping the one with the highest frequency. this allows the operations to be performed in o(1) time and, though the use of a compact sketch, a much larger history is retained beyond the current working set. in a variety of real world traces the policy had near optimal hit rates. concurrency is achieved by buffering and replaying the operations, similar to a write-ahead log. a read is recorded into a striped ring buffer and writes to a queue. the operations are applied in batches under a try-lock by an asynchronous thread, thereby track the usage pattern without incurring high latencies (benchmarks). in ycsb benchmarks the results were inconclusive. for a large cache (99% hit rates) the two caches have near identical throughput and latencies with lrublockcache narrowly winning. at medium and small caches, tinylfu had a 1-4% hit rate improvement and therefore lower latencies. the lack luster result is because a synthetic zipfian distribution is used, which slru performs optimally. in a more varied, real-world workload we'd expect to see improvements by being able to make smarter predictions. the provided patch implements blockcache using the caffeine caching library (see highscalability article). edward bortnikov and eshcar hillel have graciously provided guidance for evaluating this patch (github branch). ",
        "label": 65
    },
    {
        "text": "update our thrift to  hbase-3117 was about updating to 0.5. moaz reyad over in that issue is trying to move us to 0.6. lets move the 0.6 upgrade effort here. ",
        "label": 331
    },
    {
        "text": "use rsgroupinfomanager to get rsgroups in master ui's rsgroup part  now it use rsgrouptableaccessor to get all rsgroups. list<rsgroupinfo> groups = rsgrouptableaccessor.getallrsgroupinfo(master.getconnection()); ",
        "label": 188
    },
    {
        "text": "remove out of date comments in storeflusher java   this comment is out-of-date because the code already removed. // if we know that this kv is going to be included always, then let us  // set its memstorets to 0. this will help us save space when writing to  // disk. ",
        "label": 344
    },
    {
        "text": "regionserver stuck  hlog  could not append  requesting close of log java io ioexception  could not get block locations  aborting   hdfs went wonky. manifest itself in regionserver with below: i.e. first can't replicate and then the exceptions every time we append and then every time we try to rotate the log file. exception is always: \"ioexception: could not get block locations. aborting...\" meantime the hdfs is fixed but for whatever reason the hrs just keeps on with the below failings; as though the error is stuck in the dfsclient. ... 2008-10-15 02:54:59,867 debug org.apache.hadoop.hbase.regionserver.hstore: completed compaction of 1399814750/alternate_url store size is 2.9m 2008-10-15 02:54:59,868 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region enwiki_old,7qe8tui5v-nizu_9ozhajf==,1221856888271 in 38sec 2008-10-15 02:59:08,507 info org.apache.hadoop.dfs.dfsclient: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: file /hbase/aa0-000-8.u.powerset.com/log_208.76.45.180_1223616861290_60020/hlog.dat.1224038569764 could only be re plicated to 0 nodes, instead of 1         at org.apache.hadoop.dfs.fsnamesystem.getadditionalblock(fsnamesystem.java:1117)         at org.apache.hadoop.dfs.namenode.addblock(namenode.java:330)         at sun.reflect.generatedmethodaccessor15.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)         at java.lang.reflect.method.invoke(unknown source)         at org.apache.hadoop.ipc.rpc$server.call(rpc.java:452)         at org.apache.hadoop.ipc.server$handler.run(server.java:888)         at org.apache.hadoop.ipc.client.call(client.java:715)         at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:216)         at org.apache.hadoop.dfs.$proxy1.addblock(unknown source)         at sun.reflect.generatedmethodaccessor9.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)         at java.lang.reflect.method.invoke(unknown source)         at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:82)         at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:59)         at org.apache.hadoop.dfs.$proxy1.addblock(unknown source)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.locatefollowingblock(dfsclient.java:2440)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2323)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.access$1800(dfsclient.java:1735)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:1912) ... 2008-10-15 02:59:11,357 warn org.apache.hadoop.dfs.dfsclient: notreplicatedyetexception sleeping /hbase/aa0-000-8.u.powerset.com/log_208.76.45.180_1223616861290_60020/hlog.dat.1224038569764 retries left 1 2008-10-15 02:59:14,566 warn org.apache.hadoop.dfs.dfsclient: datastreamer exception: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: file /hbase/aa0-000-8.u.powerset.com/log_208.76.45.180_1223616861290_60020/hlog.dat.122403 8569764 could only be replicated to 0 nodes, instead of 1         at org.apache.hadoop.dfs.fsnamesystem.getadditionalblock(fsnamesystem.java:1117)         at org.apache.hadoop.dfs.namenode.addblock(namenode.java:330)         at sun.reflect.generatedmethodaccessor15.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)         at java.lang.reflect.method.invoke(unknown source)         at org.apache.hadoop.ipc.rpc$server.call(rpc.java:452)         at org.apache.hadoop.ipc.server$handler.run(server.java:888)         at org.apache.hadoop.ipc.client.call(client.java:715)         at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:216)         at org.apache.hadoop.dfs.$proxy1.addblock(unknown source)         at sun.reflect.generatedmethodaccessor9.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)         at java.lang.reflect.method.invoke(unknown source)         at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:82)         at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:59)         at org.apache.hadoop.dfs.$proxy1.addblock(unknown source)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.locatefollowingblock(dfsclient.java:2440)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2323)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.access$1800(dfsclient.java:1735)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:1912) 2008-10-15 02:59:14,566 warn org.apache.hadoop.dfs.dfsclient: error recovery for block null bad datanode[0] 2008-10-15 02:59:14,566 fatal org.apache.hadoop.hbase.regionserver.hlog: could not append. requesting close of log java.io.ioexception: could not get block locations. aborting...         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.processdatanodeerror(dfsclient.java:2143)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.access$1400(dfsclient.java:1735)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:1889) 2008-10-15 02:59:14,567 info org.apache.hadoop.hbase.regionserver.logroller: rolling hlog. number of entries: 81 2008-10-15 02:59:14,567 info org.apache.hadoop.ipc.server: ipc server handler 8 on 60020, call batchupdate([b@3e4afee5, row => dnsw4rukhj5-0fqddy35mf==, {column => misc:upload_time, value => '...', column => page:mime, value => '...', c olumn => page:content, value => '...', column => page:url, value => '...'}, -1) from 208.76.44.183:59031: error: java.io.ioexception: could not get block locations. aborting... java.io.ioexception: could not get block locations. aborting...         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.processdatanodeerror(dfsclient.java:2143)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.access$1400(dfsclient.java:1735)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:1889) 2008-10-15 02:59:14,588 error org.apache.hadoop.hbase.regionserver.logroller: log rolling failed java.io.ioexception: could not get block locations. aborting...         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.processdatanodeerror(dfsclient.java:2143)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.access$1400(dfsclient.java:1735)         at org.apache.hadoop.dfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:1889) ... for now adding abort of regionserver if we can't close the log roller file. if we can't close, then we're losing edits. if this behavior causes us close to much, then need to dig in more. happened on pset cluster running 0.18.1 hadoop and 0.18.0 hbase. ",
        "label": 314
    },
    {
        "text": "testheapsize failing on hudson  testheapsize worked for me when tested locally but failing up on hudson:     [junit] running org.apache.hadoop.hbase.io.testheapsize     [junit] 2009-06-20 01:29:11,577 debug [main] util.classsize(121): bytes     [junit]  class [b     [junit] 2009-06-20 01:29:11,580 debug [main] util.classsize(121): offset     [junit]  int     [junit] 2009-06-20 01:29:11,580 debug [main] util.classsize(121): length     [junit]  int     [junit] 2009-06-20 01:29:11,581 debug [main] util.classsize(147): primitives 8, arrays 1, references(inlcuding 16, for object overhead) 2, refsize 4, size 32     [junit] tests run: 1, failures: 1, errors: 0, time elapsed: 0.181 sec     [junit] test org.apache.hadoop.hbase.io.testheapsize failed ",
        "label": 247
    },
    {
        "text": "hfile min blocksize size ignored documentation wrong  there is a property in hbase-default.xml called hfile.min.blocksize.size set to 65536. the description says: minimum store file block size. the smaller you make this, the bigger your index and the less you fetch on a random-access. set size down if you have small cells and want faster random-access of individual cells. this property is only used in the hfileoutputformat and nowhere else. so we should at least change the description to something more meaningful. the other option i see would be: hfile now has a default_blocksize field which could be moved to hconstants and hfile could somehow read the hfile.min.blocksize.size from the configuration or use hconstansts.default_blocksize if it's not defined. i believe this is what's happening to the other config variables? ",
        "label": 314
    },
    {
        "text": "spacequotas   getnumregions  returning wrong number of regions due to region replicas  space quota: space quota issue: if a table is created with region replica then quota calculation is not happening steps: 1: create a table with 100 regions with region replica 3 2: observe that 'hbase:quota' table doesn't have entry of usage for this table so in ui only policy limit and policy is shown but not usage and state. reason:  it looks like file system utilization core is sending data of 100 reasons but not the size of region replicas.  but in quota observer chore, it is considering total region(actual regions+ replica reasons)  so the ratio of reported regions is less then configured percentregionsreportedthreshold.  so quota calculation is not happening ",
        "label": 412
    },
    {
        "text": "testminiclusterloadsequential fails in trunk build on hadoop  from hbase-trunk-on-hadoop-2.0.0 #354:  loadtest[0](org.apache.hadoop.hbase.util.testminiclusterloadsequential): test timed out after 120000 milliseconds  loadtest[1](org.apache.hadoop.hbase.util.testminiclusterloadsequential): test timed out after 120000 milliseconds  loadtest[2](org.apache.hadoop.hbase.util.testminiclusterloadsequential): test timed out after 120000 milliseconds  loadtest[3](org.apache.hadoop.hbase.util.testminiclusterloadsequential): test timed out after 120000 milliseconds ",
        "label": 248
    },
    {
        "text": "assertionerror in loadbalancer  while running pe with low splitting configuration, i got this: 2010-11-24 23:23:24,508 info org.apache.hadoop.hbase.master.servermanager: received region_split: testtable,0002485653,... 2010-11-24 23:23:26,129 info org.apache.hadoop.hbase.master.servermanager: received region_split: testtable,0004309306,... 2010-11-24 23:23:26,132 info org.apache.hadoop.hbase.master.servermanager: received region_split: testtable,0001281491... 2010-11-24 23:23:26,162 fatal org.apache.hadoop.hbase.master.hmaster$1: sv2borg180:61000-balancerchoreerror java.lang.assertionerror at org.apache.hadoop.hbase.master.loadbalancer.balancecluster(loadbalancer.java:296) at org.apache.hadoop.hbase.master.hmaster.balance(hmaster.java:679) at org.apache.hadoop.hbase.master.hmaster$1.chore(hmaster.java:578) at org.apache.hadoop.hbase.chore.run(chore.java:66) 2010-11-24 23:23:26,163 info org.apache.hadoop.hbase.master.hmaster$1: sv2borg180:61000-balancerchore exiting 2010-11-24 23:23:26,236 info org.apache.hadoop.hbase.master.servermanager: received region_split: testtable,0000921369,... the thread dies but the master survives. there's nothing specific before that in the log, just regions splitting. the line in loadbalancer is:     assert(regionidx == regionstomove.size()); ",
        "label": 247
    },
    {
        "text": "add metrics for snapshots  metrics should be added for snapshot. from matteo: output that we have in snapshotinfo should be covered: \"%d hfiles (%d in archive), total size %s (%.2f%% %s shared with the source table)\" jesse mentioned snaphot counts, average time to completion.  i think we should have counts for successful / failed snapshots. ",
        "label": 309
    },
    {
        "text": "region server reports storefilesizemb bigger than storefileuncompressedsizemb  minor issue while looking at the rs metrics: numberofstorefiles=8, storefileuncompressedsizemb=2418, storefilesizemb=2420, compressionratio=1.0008 i guess there's a truncation somewhere when it's adding the numbers up. fwiw there's no compression on that table. ",
        "label": 529
    },
    {
        "text": "per rs get metric is time based  per region metric is size based  we have metrics for get operations at the region server level and region level.    \"get_num_ops\" : 4837505,     \"get_min\" : 0,     \"get_max\" : 296,     \"get_mean\" : 0.2934618155433431,     \"get_median\" : 0.0,     \"get_75th_percentile\" : 0.0,     \"get_95th_percentile\" : 1.0,     \"get_99th_percentile\" : 1.0, and    \"namespace_hbase_table_meta_region_1588230740_metric_get_num_ops\" : 103,     \"namespace_hbase_table_meta_region_1588230740_metric_get_min\" : 450,     \"namespace_hbase_table_meta_region_1588230740_metric_get_max\" : 470,     \"namespace_hbase_table_meta_region_1588230740_metric_get_mean\" : 450.19417475728153,     \"namespace_hbase_table_meta_region_1588230740_metric_get_median\" : 460.0,     \"namespace_hbase_table_meta_region_1588230740_metric_get_75th_percentile\" : 470.0,     \"namespace_hbase_table_meta_region_1588230740_metric_get_95th_percentile\" : 470.0,     \"namespace_hbase_table_meta_region_1588230740_metric_get_99th_percentile\" : 470.0, the problem is that the report values for the region server shows the latency, versus the reported values for the region shows the response sizes. there is no way of telling this without reading the source code. i think we should deprecate response size histograms in favor of latency histograms. see also hbase-15376. ",
        "label": 198
    },
    {
        "text": "extend testhbasefsck with a complete  meta  recovery scenario  we should have a unit test that launches a minicluster and constructs a few tables, then deletes meta files on disk, then bounces the master, then recovers the result with hbck. perhaps it is possible to extend testhbasefsck to do this. ",
        "label": 314
    },
    {
        "text": "backport to and   npe reading zk config in hbase  ",
        "label": 309
    },
    {
        "text": "scanner is jumping a full cache of rows  i ran my program multiple times and this is happening almost all the time. basically, the resultscanner is skipping a full cache of rows. i set my caching size to 1000, and when i expect to see row 60000 it gave me 60999. my code (will attach here) is creating a new table with a single column family and qualifier, and then write 1 million rows with ascending keys, then immediately read them back to verify. the hbase code is from:  http://svn.apache.org/repos/asf/hadoop/hbase/trunk at  exported revision 821973. ",
        "label": 30
    },
    {
        "text": "a command line  hbase shell  interface to retreive the replication metrics and show replication lag  this jira is to provide a command line (hbase shell) interface to retreive the replication metrics info such as:ageoflastshippedop, timestampsoflastshippedop, sizeoflogqueue ageoflastappliedop, and timestampsoflastappliedop. and also to provide a point of time info of the lag of replication(source only) understand that hbase is using hadoop metrics(http://hbase.apache.org/metrics.html), which is a common way to monitor metric info. this jira is to serve as a light-weight client interface, comparing to a completed(certainly better, but heavier)gui monitoring package. i made the code works on 0.94.9 now, and like to use this jira to get opinions about whether the feature is valuable to other users/workshop. if so, i will build a trunk patch. all inputs are greatly appreciated. thank you! the overall design is to reuse the existing logic which supports hbase shell command 'status', and invent a new module, called replicationload. in hregionserver.buildserverload() , use the local replication service objects to get their loads which could be wrapped in a replicationload object and then simply pass it to the serverload. in replicationsourcemetrics and replicationsinkmetrics, a few getters and setters will be created, and ask replication to build a \"replicationload\". (many thanks to jean-daniel for his kindly suggestions through dev email list) the replication lag will be calculated for source only, and use this formula: replication lag if sizeoflogqueue != 0 then max(ageoflastshippedop, (current time - timestampsoflastshippedop)) //err on the large side else if (current time - timestampsoflastshippedop) < 2* ageoflastshippedop then lag = ageoflastshippedop // last shipped happen recently          else lag = 0 // last shipped may happens last night, so no real lag although ageoflastshippedop is non-zero external will look something like: status 'replication' hbase(main):001:0> status 'replication' version 0.94.9 3 live servers     hdtest017.svl.ibm.com:         source:peerid=1, ageoflastshippedop=14, sizeoflogqueue=0, timestampsoflastshippedop=wed sep 04 14:49:48 pdt 2013         sink  :ageoflastappliedop=0, timestampsoflastappliedop=wed sep 04 14:48:48 pdt 2013     hdtest018.svl.ibm.com:         source:peerid=1, ageoflastshippedop=0, sizeoflogqueue=0, timestampsoflastshippedop=wed sep 04 14:48:48 pdt 2013         sink  :ageoflastappliedop=14, timestampsoflastappliedop=wed sep 04 14:50:59 pdt 2013     hdtest015.svl.ibm.com:         source:peerid=1, ageoflastshippedop=0, sizeoflogqueue=0, timestampsoflastshippedop=wed sep 04 14:48:48 pdt 2013         sink  :ageoflastappliedop=0, timestampsoflastappliedop=wed sep 04 14:48:48 pdt 2013 hbase(main):002:0> status 'replication','source' version 0.94.9 3 live servers     hdtest017.svl.ibm.com:         source:peerid=1, ageoflastshippedop=14, sizeoflogqueue=0, timestampsoflastshippedop=wed sep 04 14:49:48 pdt 2013     hdtest018.svl.ibm.com:         source:peerid=1, ageoflastshippedop=0, sizeoflogqueue=0, timestampsoflastshippedop=wed sep 04 14:48:48 pdt 2013     hdtest015.svl.ibm.com:         source:peerid=1, ageoflastshippedop=0, sizeoflogqueue=0, timestampsoflastshippedop=wed sep 04 14:48:48 pdt 2013 hbase(main):003:0> status 'replication','sink' version 0.94.9 3 live servers     hdtest017.svl.ibm.com:         sink  :ageoflastappliedop=0, timestampsoflastappliedop=wed sep 04 14:48:48 pdt 2013     hdtest018.svl.ibm.com:         sink  :ageoflastappliedop=14, timestampsoflastappliedop=wed sep 04 14:50:59 pdt 2013     hdtest015.svl.ibm.com:         sink  :ageoflastappliedop=0, timestampsoflastappliedop=wed sep 04 14:48:48 pdt 2013 hbase(main):003:0> status 'replication','lag'  version 0.94.9 3 live servers     hdtest017.svl.ibm.com: lag = 0     hdtest018.svl.ibm.com: lag = 14     hdtest015.svl.ibm.com: lag = 0 ",
        "label": 53
    },
    {
        "text": "fnfe during scans and flushes  we see fnfe exceptions on our 1.3 clusters when scans and flushes happen at the same time. this causes regionserver to throw a unknownscannerexception and client retries. this happens during the following sequence: 1. scanner open, client fetched some rows from regionserver and working on it  2. flush happens and storescanner is updated with flushed files (storescanner.updatereaders())  3. compaction happens on the region while scanner is still open  4. compaction discharger runs and cleans up the newly flushed file as we don't have new scanners on it yet.  5. client issues scan.next and during storescanner.resetscannerstack(), we get a fnfe. regionserver throws a unknownscannerthe client retries in 1.3. with branch-1.4, the scan fails with a donotretryioexception. ramkrishna.s.vasudevan, my proposal is to increment the reader count during updatereaders() and decrement it during resetscannerstack(), so discharger doesn't clean it up. scan lease expiries also have to be taken care of. am i missing anything? is there a better approach? ",
        "label": 544
    },
    {
        "text": "hbck should be updated to do replica related checks  hbck should be updated to have a check for whether the replicas are assigned to the right machines (default and non-default replicas ideally should not be in the same server if there is more than one server in the cluster and such scenarios). jonathan hsieh suggested this in hbase-10362. ",
        "label": 139
    },
    {
        "text": "writes should not block reads on blocking updates to memstores  we have a big data use case where we turn off wal and have a ton of reads and writes. we found that: 1. flushing a memstore takes a while (gzip compression)  2. incoming writes cause the new memstore to grow in an unbounded fashion  3. this triggers blocking memstore updates  4. in turn, this causes all the rpc handler threads to block on writes to that memstore  5. we are not able to read during this time as rpc handlers are blocked at a higher level, we should not hold up the rpc threads while blocking updates, and we should build in some sort of rate control. ",
        "label": 242
    },
    {
        "text": " hadoop check  test is running all the time instead of just when changes to java  see for example hbase-19058, which just changes the dockerfile in dev-support. +1 hadoopcheck 39m 59s patch does not cause any errors with hadoop 2.6.1 2.6.2 2.6.3 2.6.4 2.6.5 2.7.1 2.7.2 2.7.3 or 3.0.0-alpha4. that's 30-40 minutes we're adding to precommit builds that don't need it. ",
        "label": 402
    },
    {
        "text": "master stuck thinking hbase namespace is assigned after restart preventing intialization  our romil choksi brought this one to my attention after trying to get some cluster tests running. the master seems to have gotten stuck never initializing after it thinks that hbase:namespace was already deployed on the cluster when it actually was not. on a master restart, it reads the location out of meta and assumes that it's there (i assume this invalid entry is the issue): 2017-11-08 00:29:17,556 info  [ctr-e134-1499953498516-282290-01-000003:20000.mastermanager] assignment.regionstatestore: load hbase:meta entry region={encoded => f147f204a579b885c351bdc0a7ebbf94, name => 'hbase:namespace,,1510084256045.f147f204a579b885c351bdc0a7ebbf94.', startkey => '', endkey => ''} regionstate=opening lasthost=ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510084579728 regionlocation=ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510100695534 prior to this, the rs5 went through the servercrashprocedure, but it looks like this bailed out unexpectedly: 2017-11-08 00:25:25,187 warn  [ctr-e134-1499953498516-282290-01-000003:20000.mastermanager] master.servermanager: expiration of ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510084579728 but server not online 2017-11-08 00:25:25,187 info  [procexecwrkr-5] procedure.servercrashprocedure: start pid=36, state=runnable:server_crash_start; servercrashprocedure server=ctr-e134-1499953498516-282290-01-000003.hwx.site,16020,1510084580111, splitwal=t rue, meta=false 2017-11-08 00:25:25,188 info  [ctr-e134-1499953498516-282290-01-000003:20000.mastermanager] master.servermanager: processing expiration of ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510084579728 on ctr-e134-1499953498516-28 2290-01-000003.hwx.site,20000,1510100690324 ... 2017-11-08 00:25:27,211 error [procexecwrkr-22] procedure2.procedureexecutor: code-bug: uncaught runtime exception: pid=40, ppid=37, state=runnable:region_transition_queue; assignprocedure table=hbase:namespace, region=f147f204a579b885c351bdc0a7ebbf94 java.lang.nullpointerexception         at java.util.concurrent.concurrenthashmap.get(concurrenthashmap.java:936)         at org.apache.hadoop.hbase.procedure2.remoteproceduredispatcher.addoperationtonode(remoteproceduredispatcher.java:171)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.addtoremotedispatcher(regiontransitionprocedure.java:223)         at org.apache.hadoop.hbase.master.assignment.assignprocedure.updatetransition(assignprocedure.java:252)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:309)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:82)         at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:845)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1452)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1221)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$800(procedureexecutor.java:77)         at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1731) 2017-11-08 00:25:27,239 fatal [procexecwrkr-22] procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=37, state=failed:server_crash_finish, exception=java.lang.nullpointerexception via code-bug: uncaught runtime exception: pid=40, ppid=37, state=runnable:region_transition_queue; assignprocedure table=hbase:namespace, region=f147f204a579b885c351bdc0a7ebbf94:java.lang.nullpointerexception; servercrashprocedure server=ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510084579728, splitwal=true, meta=false java.lang.unsupportedoperationexception: unhandled state=server_crash_finish         at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:236)         at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:56)         at org.apache.hadoop.hbase.procedure2.statemachineprocedure.rollback(statemachineprocedure.java:198)         at org.apache.hadoop.hbase.procedure2.procedure.dorollback(procedure.java:859)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executerollback(procedureexecutor.java:1353)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executerollback(procedureexecutor.java:1309)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1178)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$800(procedureexecutor.java:77)         at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1731) 2017-11-08 00:25:27,344 fatal [procexecwrkr-8] procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=37, state=failed:server_crash_finish, exception=java.lang.nullpointerexception via code-bug: uncaught runtime exception: pid=40, ppid=37, state=runnable:region_transition_queue; assignprocedure table=hbase:namespace, region=f147f204a579b885c351bdc0a7ebbf94:java.lang.nullpointerexception; servercrashprocedure server=ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510084579728, splitwal=true, meta=false java.lang.unsupportedoperationexception: unhandled state=server_crash_finish         at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:236)         at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:56)         at org.apache.hadoop.hbase.procedure2.statemachineprocedure.rollback(statemachineprocedure.java:198)         at org.apache.hadoop.hbase.procedure2.procedure.dorollback(procedure.java:859)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executerollback(procedureexecutor.java:1353)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executerollback(procedureexecutor.java:1309)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1178)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$800(procedureexecutor.java:77)         at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1731) 2017-11-08 00:25:27,356 info  [procexecwrkr-5] procedure2.procedureexecutor: rolled back pid=37, state=rolledback, exception=java.lang.nullpointerexception via code-bug: uncaught runtime exception: pid=40, ppid=37, state=runnable:region_transition_queue; assignprocedure table=hbase:namespace, region=f147f204a579b885c351bdc0a7ebbf94:java.lang.nullpointerexception; servercrashprocedure server=ctr-e134-1499953498516-282290-01-000005.hwx.site,16020,1510084579728, splitwal=true, meta=false exec-time=2.1650sec shortly after this, the master was restarted. my hunch is that because the scp crashed, we never invalidated the meta entries and got us stuck thinking the region was assigned when it wasn't? ",
        "label": 314
    },
    {
        "text": "make fileinfo more readable in hfileprettyprinter  fileinfo as follows. some fields can print readable values. fileinfo:     bloom_filter_type = row     delete_family_count = \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00     earliest_put_ts = \\x00\\x00\\x01^q\\xf6\\x0e\\xcf     key_value_version = \\x00\\x00\\x00\\x01     last_bloom_key = f     major_compaction_key = \\xff     max_memstore_ts_key = \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18     max_seq_id_key = 27     timerange = 1504613633743....1506570582061     hfile.avg_key_len = 14     hfile.avg_value_len = 3     hfile.create_time_ts = \\x00\\x00\\x01^\\xc6\\xb4j\\xf9     hfile.lastkey = \\x00\\x01f\\x01i\\x00\\x00\\x01^\\xc6\\x9a\\xb8-\\x04 ",
        "label": 188
    },
    {
        "text": "support visibility expressions on deletes  accumulo can specify visibility expressions for delete markers. during compaction the cells covered by the tombstone are determined in part by matching the visibility expression. this is useful for the use case of data set coalescing, where entries from multiple data sets carrying different labels are combined into one common large table. later, a subset of entries can be conveniently removed using visibility expressions. currently doing the same in hbase would only be possible with a custom coprocessor. otherwise, a delete will affect all cells covered by the tombstone regardless of any visibility expression scoping. this is correct behavior in that no data spill is possible, but certainly could be surprising, and is only meant to be transitional. we decided not to support visibility expressions on deletes to control the complexity of the initial implementation. ",
        "label": 544
    },
    {
        "text": "region splitting not happened for long time due to zk exception while creating rs zk splitting node  region splitting not happened for long time due to zk exception while creating rs_zk_splitting node 2012-05-24 01:45:41,363 info org.apache.zookeeper.clientcnxn: client session timed out, have not heard from server in 26668ms for sessionid 0x1377a75f41d0012, closing socket connection and attempting reconnect 2012-05-24 01:45:41,464 warn org.apache.hadoop.hbase.zookeeper.recoverablezookeeper: possibly transient zookeeper exception: org.apache.zookeeper.keeperexception$connectionlossexception: keepererrorcode = connectionloss for /hbase/unassigned/bd1079bf948c672e493432020dc0e144 2012-05-24 01:45:43,300 debug org.apache.hadoop.hbase.regionserver.wal.hlog: cleanupcurrentwriter  waiting for transactions to get synced  total 189377 synced till here 189365 2012-05-24 01:45:48,474 info org.apache.hadoop.hbase.regionserver.splitrequest: running rollback/cleanup of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.; failed setting splitting znode on ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144. java.io.ioexception: failed setting splitting znode on ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144. at org.apache.hadoop.hbase.regionserver.splittransaction.createdaughters(splittransaction.java:242) at org.apache.hadoop.hbase.regionserver.splittransaction.execute(splittransaction.java:450) at org.apache.hadoop.hbase.regionserver.splitrequest.run(splitrequest.java:67) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: org.apache.zookeeper.keeperexception$badversionexception: keepererrorcode = badversion for /hbase/unassigned/bd1079bf948c672e493432020dc0e144 at org.apache.zookeeper.keeperexception.create(keeperexception.java:115) at org.apache.zookeeper.keeperexception.create(keeperexception.java:51) at org.apache.zookeeper.zookeeper.setdata(zookeeper.java:1246) at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.setdata(recoverablezookeeper.java:321) at org.apache.hadoop.hbase.zookeeper.zkutil.setdata(zkutil.java:659) at org.apache.hadoop.hbase.zookeeper.zkassign.transitionnode(zkassign.java:811) at org.apache.hadoop.hbase.zookeeper.zkassign.transitionnode(zkassign.java:747) at org.apache.hadoop.hbase.regionserver.splittransaction.transitionnodesplitting(splittransaction.java:919) at org.apache.hadoop.hbase.regionserver.splittransaction.createnodesplitting(splittransaction.java:869) at org.apache.hadoop.hbase.regionserver.splittransaction.createdaughters(splittransaction.java:239) ... 5 more 2012-05-24 01:45:48,476 info org.apache.hadoop.hbase.regionserver.splitrequest: successful rollback of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144. 2012-05-24 01:47:28,141 error org.apache.hadoop.hbase.zookeeper.recoverablezookeeper: node /hbase/unassigned/bd1079bf948c672e493432020dc0e144 already exists and this is not a retry 2012-05-24 01:47:28,142 info org.apache.hadoop.hbase.regionserver.splitrequest: running rollback/cleanup of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.; failed create of ephemeral /hbase/unassigned/bd1079bf948c672e493432020dc0e144 java.io.ioexception: failed create of ephemeral /hbase/unassigned/bd1079bf948c672e493432020dc0e144 at org.apache.hadoop.hbase.regionserver.splittransaction.createnodesplitting(splittransaction.java:865) at org.apache.hadoop.hbase.regionserver.splittransaction.createdaughters(splittransaction.java:239) at org.apache.hadoop.hbase.regionserver.splittransaction.execute(splittransaction.java:450) at org.apache.hadoop.hbase.regionserver.splitrequest.run(splitrequest.java:67) due to the above exception, region splitting was failing contineously more than 5hrs ",
        "label": 543
    },
    {
        "text": "logrollbackupsubprocedure will fail if we use asyncfswal instead of fshlog  in the rsrolllogtask it will cast a wal to fshlog. ",
        "label": 478
    },
    {
        "text": "region split requests are always audited with  hbase  user rather than request user  [~madhan.neethiraj] from ranger reported that when a region split request is initiated from the user, we always audit (and do the permission check) against the hbase user, not the request user. the issue is that a split request that is coming from the user is only processed at a later time from the compactsplitthread asynchronously to the splitregion rpc.  rsrpcservices.splitregion() only does a flush from the handler thread and then calls regionserver.compactsplitthread.requestsplit() which puts a splitrequest to the split queue. the split request is handled by the split executor from compactsplitthread.  since the split is actually executed from the compact split thread, the presplit() for the accesscontroller is called from the executor thread. in this thread, we no longer have the user who initially requested the split, so the user in the context (ugi) is \"hbase\", causing the ac.presplit() access control check to be always be performed against the hbase user, not the user who have submitted the request. the audit log also contains \"hbase\" user rather than the actual user. luckily, the split forces a flush to the region in-line (from the handler thread), which requires a create|admin permission. split requires admin, but due to this bug create is also sufficient (although we have not verified it manually). create permission can do flush and compactions, so this is not a security issue (i think). ",
        "label": 441
    },
    {
        "text": "bin hbase doesn't work in situ in maven  on trunk i can't use the normal start-hbase.sh scripts, etc, after doing mvn install - instead have to do the whole assembly nonsense, which is a pain when trying to quickly iterate on changes. ",
        "label": 453
    },
    {
        "text": "regionservers fail to start when setting hbase ipc server callqueue scan ratio to  i set the following configuration in hbase-site.xml. <property>   <name>hbase.ipc.server.callqueue.read.ratio</name>   <value>0.5</value> </property> <property>   <name>hbase.ipc.server.callqueue.scan.ratio</name>   <value>0</value> <property> then, the regionserver failed to start and i saw the following log: 2015-08-19 14:30:19,561 error org.apache.hadoop.hbase.regionserver.hregionservercommandline: region server exiting java.lang.runtimeexception: failed construction of regionserver: class org.apache.hadoop.hbase.regionserver.hregionserver         at org.apache.hadoop.hbase.regionserver.hregionserver.constructregionserver(hregionserver.java:2457)         at org.apache.hadoop.hbase.regionserver.hregionservercommandline.start(hregionservercommandline.java:61)         at org.apache.hadoop.hbase.regionserver.hregionservercommandline.run(hregionservercommandline.java:85)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)         at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:126)         at org.apache.hadoop.hbase.regionserver.hregionserver.main(hregionserver.java:2472) caused by: java.lang.reflect.invocationtargetexception         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)         at java.lang.reflect.constructor.newinstance(constructor.java:526)         at org.apache.hadoop.hbase.regionserver.hregionserver.constructregionserver(hregionserver.java:2455)         ... 5 more caused by: java.lang.illegalargumentexception: queue size is <= 0, must be at least 1         at com.google.common.base.preconditions.checkargument(preconditions.java:92)         at org.apache.hadoop.hbase.ipc.rpcexecutor.getbalancer(rpcexecutor.java:139)         at org.apache.hadoop.hbase.ipc.rwqueuerpcexecutor.<init>(rwqueuerpcexecutor.java:121)         at org.apache.hadoop.hbase.ipc.rwqueuerpcexecutor.<init>(rwqueuerpcexecutor.java:83)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.<init>(simplerpcscheduler.java:129)         at org.apache.hadoop.hbase.regionserver.simplerpcschedulerfactory.create(simplerpcschedulerfactory.java:36)         at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:610)         ... 10 more the doc of \"hbase.ipc.server.callqueue.scan.ratio\" says \"a value of 0 or 1 indicate to use the same set of queues for gets and scans.\".  i think that there is a bug in validation. ",
        "label": 505
    },
    {
        "text": "intermittent testlogrollingnocluster testcontendedlogrolling failure  from https://builds.apache.org/job/hbase-trunk/lastcompletedbuild/testreport/org.apache.hadoop.hbase.regionserver.wal/testlogrollingnocluster/testcontendedlogrolling/ : java.lang.assertionerror at org.junit.assert.fail(assert.java:86) at org.junit.assert.asserttrue(assert.java:41) at org.junit.assert.assertfalse(assert.java:64) at org.junit.assert.assertfalse(assert.java:74) at org.apache.hadoop.hbase.regionserver.wal.testlogrollingnocluster.testcontendedlogrolling(testlogrollingnocluster.java:79) ... 2013-10-23 00:20:36,872 info  [18] wal.fshlog(527): rolled wal /home/jenkins/jenkins-slave/workspace/hbase-trunk/trunk/hbase-server/target/test-data/39875b32-a156-48ac-875f-9f67689d3a2e/logs/hlog.1382487636870 with entries=28, filesize=30.0k; new wal /home/jenkins/jenkins-slave/workspace/hbase-trunk/trunk/hbase-server/target/test-data/39875b32-a156-48ac-875f-9f67689d3a2e/logs/hlog.1382487636872 2013-10-23 00:20:36,873 info  [51] wal.testlogrollingnocluster$appender(135): caught exception from appender:51 java.io.ioexception: cannot get log writer at org.apache.hadoop.hbase.regionserver.wal.hlogfactory.createwriter(hlogfactory.java:197) at org.apache.hadoop.hbase.regionserver.wal.hlogfactory.createwalwriter(hlogfactory.java:177) at org.apache.hadoop.hbase.regionserver.wal.fshlog.createwriterinstance(fshlog.java:566) at org.apache.hadoop.hbase.regionserver.wal.fshlog.rollwriter(fshlog.java:509) at org.apache.hadoop.hbase.regionserver.wal.fshlog.rollwriter(fshlog.java:470) at org.apache.hadoop.hbase.regionserver.wal.testlogrollingnocluster$appender.run(testlogrollingnocluster.java:118) caused by: java.io.ioexception: file already exists:/home/jenkins/jenkins-slave/workspace/hbase-trunk/trunk/hbase-server/target/test-data/39875b32-a156-48ac-875f-9f67689d3a2e/logs/hlog.1382487636872 at org.apache.hadoop.fs.rawlocalfilesystem.create(rawlocalfilesystem.java:249) at org.apache.hadoop.fs.rawlocalfilesystem.create(rawlocalfilesystem.java:241) at org.apache.hadoop.fs.checksumfilesystem$checksumfsoutputsummer.<init>(checksumfilesystem.java:335) at org.apache.hadoop.fs.checksumfilesystem.create(checksumfilesystem.java:381) at org.apache.hadoop.fs.checksumfilesystem.createnonrecursive(checksumfilesystem.java:395) at org.apache.hadoop.fs.filesystem.createnonrecursive(filesystem.java:610) at org.apache.hadoop.hbase.regionserver.wal.protobuflogwriter.init(protobuflogwriter.java:68) at org.apache.hadoop.hbase.regionserver.wal.hlogfactory.createwriter(hlogfactory.java:194) looks like target/test-data/39875b32-a156-48ac-875f-9f67689d3a2e/logs/hlog.1382487636872 was created already and the second filesystem.createnonrecursive() call failed. ",
        "label": 242
    },
    {
        "text": "testtableresource flaky on branch  occasional npe flaked tests:   org.apache.hadoop.hbase.rest.testtableresource.testtableinfopb(org.apache.hadoop.hbase.rest.testtableresource)  run 1: testtableresource.testtableinfopb:271->checktableinfo:184 nullpointer  run 2: pass org.apache.hadoop.hbase.rest.testtableresource.testtableinfoxml(org.apache.hadoop.hbase.rest.testtableresource)  run 1: testtableresource.testtableinfoxml:254->checktableinfo:184 nullpointer  run 2: pass tests run: 213, failures: 0, errors: 0, skipped: 0, flakes: 2 ",
        "label": 346
    },
    {
        "text": "coprocessors  shell support for listing currently loaded coprocessor set  add support to the shell for listing the coprocessors loaded globally on the regionserver and those loaded on a per-table basis. perhaps by extending the 'status' command. ",
        "label": 164
    },
    {
        "text": "add avro support for spark hbase connector  avro is a popular format for hbase storage. user may want the support natively in the connector. with the support, user can save serialized avro into hbase table, and then query on top of it using spark sql. the conversion between avro and catalyst datatype will be handled automatically. this is one way of support complex data types. otherwise, user has to define their own customized serdes to support complex data types. ",
        "label": 512
    },
    {
        "text": "improve eclipse documentation and project file generation  import via m2eclipse ask a few build path fixes. this should be documented in the hbase book. mvn eclipse:eclipse is helped with the build-helper-maven-plugin plugin where additional folder (target/...) are listed. the listed jamon folder is wrong. (putting these 2 concerns on same jira as they are more or less related, avoiding jira proliferation). ",
        "label": 158
    },
    {
        "text": "regionserver is not using the name given it by the master  double entry in master listing of servers  our man ted dunning found the following where rs checks in with one name, the master tells it use another name but we seem to go ahead and continue with our original name. in rs logs i see: 2011-01-07 15:45:50,757 info  org.apache.hadoop.hbase.regionserver.hregionserver [regionserver60020]: master passed us address to use. was=perfnode11:60020, now=10.10.30.11:60020 on master i see 2011-01-07 15:45:38,613 info  org.apache.hadoop.hbase.master.servermanager [ipc server handler 0 on 60000]: registering server=10.10.30.11,60020,1294443935414, regioncount=0, userload=false .... then later 2011-01-07 15:45:44,247 info  org.apache.hadoop.hbase.master.servermanager [ipc server handler 2 on 60000]: registering server=perfnode11,60020,1294443935414, regioncount=0, userload=true this might be since we started letting servers register in other than with the reportstartup. ",
        "label": 314
    },
    {
        "text": "integrationtestingestwithacl is not setting up loadtesttool correctly  integrationtestingestwithacl is not setting up loadtesttool correctly. tests run: 1, failures: 1, errors: 0, skipped: 0, time elapsed: 601.709 sec <<< failure! testingest(org.apache.hadoop.hbase.integrationtestingestwithacl)  time elapsed: 601.489 sec  <<< failure! java.lang.assertionerror: failed to initialize loadtesttool expected:<0> but was:<1>         at org.junit.assert.fail(assert.java:88)         at org.junit.assert.failnotequals(assert.java:743)         at org.junit.assert.assertequals(assert.java:118)         at org.junit.assert.assertequals(assert.java:555)         at org.apache.hadoop.hbase.integrationtestingest.inittable(integrationtestingest.java:74)         at org.apache.hadoop.hbase.integrationtestingest.setupcluster(integrationtestingest.java:69)         at org.apache.hadoop.hbase.integrationtestingestwithacl.setupcluster(integrationtestingestwithacl.java:58)         at org.apache.hadoop.hbase.integrationtestbase.setup(integrationtestbase.java:89) could be related to hbase-10675? ",
        "label": 462
    },
    {
        "text": "very inefficient behaviour of scan using filterlist  the behaviour of scan is very inefficient when using with filterlist. the filterlist rewrites the return code from next_row to skip from a filter if operator.must_pass_all is used. this happens when using columnprefixfilter. even though the columnprefixfilter indicates to jump to next_row because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the returncode from columnprefixfilter has been rewritten by the filterlist from next_row to skip. this is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name. ",
        "label": 286
    },
    {
        "text": "backport hbase and hbase to branch  branch-2 and master have good validation checks when constructing keyvalues. we should also have them on branch-1. ",
        "label": 38
    },
    {
        "text": "handle post namespace snapshot files when checking for hfile v1  when checking for hfilev1 before upgrading to 96, the snapshot file links tries to read from post-namespace locations. the migration script needs to be run on 94 cluster, and it requires reading the old (94) layout to check for hfilev1. got exception while reading trailer for file: hdfs://xxx:41020/cops/cluster_collection_events_snapshot/2086db948c484be62dcd76c170fe0b17/meta/cluster_collection_event=42037b88dbc34abff6cbfbb1fde2c900-c24b358ddd2f4429a7287258142841a2 java.io.filenotfoundexception: unable to open link: org.apache.hadoop.hbase.io.hfilelink locations=[hdfs://xxx:41020/hbase-96/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2, hdfs://xxx:41020/hbase-96/.tmp/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2, hdfs://xxx:41020/hbase-96/archive/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2] ",
        "label": 199
    },
    {
        "text": "npe in htablepool closetablepool  when i use htablepool and try to close it on application shutdown i've got npe calling closetablepool method because i didn't borrow any tables with the given name. could you please add a null check for queue in closetablepool or add ability to get all table names used in a pool or just add a destroy method to close all existed table in a pool. ",
        "label": 441
    },
    {
        "text": "hbase rest checkanddeleteapi should be able to delete more cells  java checkanddelete api accepts delete object which can be used to delete (a cell / cell version / multiple cells / column family or a row), but the rest api only allows to delete the cell (without any version) need to add this capability to rest api. ",
        "label": 10
    },
    {
        "text": " hbck  catch and handle notservingregionexception when close region attempt fails  currently, if hbck attempts to close a region and catches a notserverregionexception, hbck may hang outputting a stack trace. since the goal is to close the region at a particular server, and since it is not serving the region, the region is closed, and we should just warn and eat this exception. exception in thread \"main\" org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hbase.notservingregionexception: received close for <regionid> but we are not serving it at org.apache.hadoop.hbase.regionserver.hregionserver.closeregion(hregionserver.java:2162) at sun.reflect.generatedmethodaccessor36.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1039) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:771) at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:257) at $proxy5.closeregion(unknown source) at org.apache.hadoop.hbase.util.hbasefsckrepair.closeregionsilentlyandwait(hbasefsckrepair.java:165) at org.apache.hadoop.hbase.util.hbasefsck.closeregion(hbasefsck.java:1185) at org.apache.hadoop.hbase.util.hbasefsck.checkregionconsistency(hbasefsck.java:1302) at org.apache.hadoop.hbase.util.hbasefsck.checkandfixconsistency(hbasefsck.java:1065) at org.apache.hadoop.hbase.util.hbasefsck.onlineconsistencyrepair(hbasefsck.java:351) at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:370) at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:3001) ",
        "label": 290
    },
    {
        "text": "atomic check and save in htable  check and save is a simple operation where one gives both a batchupdate with updates and a map mapping columns to expected values (byte[] -> byte[]). the operation works as follows: 1. server gets locks on row.  2. server checks that the actual values of the specified columns match the given expected values  3. if false, return false, if true update the row  4. unlock row. pretty simple... but useful. included in the attached patch are the necessary updates for htable, hregionserver, regionserverinterface, and hregion. i also added a small unit test to htable where the test checks that checkandsave succeeds when the expected values line up and fail when the values are different. ",
        "label": 313
    },
    {
        "text": "build error on master  in https://builds.apache.org/job/precommit-hbase-build/15325//console /home/jenkins/tools/maven/latest/bin/mvn clean package checkstyle:checkstyle-aggregate findbugs:findbugs -dskiptests -dhbasepatchprocess > /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/trunkjavacwarnings.txt 2>&1  mvn exit code was 0  mv: cannot stat 'target/checkstyle-result.xml': no such file or directory  usage: edu.umd.cs.findbugs.workflow.unionresults [options] [<results1> <results2> ... <resultsn>]   expected 2...2147483647 file arguments, found 1  options:  -withmessages generated xml should contain msgs for external processing  -output <outputfile> file in which to store combined results  exception in thread \"main\" java.io.filenotfoundexception: /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/trunkfindbugswarnings.xml (no such file or directory)  at java.io.fileinputstream.open(native method)  at java.io.fileinputstream.<init>(fileinputstream.java:146)  at edu.umd.cs.findbugs.sortedbugcollection.progessmonitoredinputstream(sortedbugcollection.java:1231)  at edu.umd.cs.findbugs.sortedbugcollection.readxml(sortedbugcollection.java:308)  at edu.umd.cs.findbugs.sortedbugcollection.readxml(sortedbugcollection.java:295)  at edu.umd.cs.findbugs.printingbugreporter.xslt(printingbugreporter.java:235)  at edu.umd.cs.findbugs.printingbugreporter.main(printingbugreporter.java:159)  /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/trunkfindbugswarnings.xml: error: cannot open `/home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/trunkfindbugswarnings.xml' (no such file or directory)  /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/trunkfindbugswarnings.html: error: cannot open `/home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/trunkfindbugswarnings.html' (no such file or directory) /home/jenkins/tools/maven/latest/bin/mvn clean install -dskiptests -dhbasepatchprocess -dhadoop-two.version=2.4.0 > /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/patchjavacwithhadoop-2.4.0.txt 2>&1  ======================================================================  there are compilation errors with hadoop version 2.4.0.  ======================================================================  [error] error invoking method 'get(java.lang.integer)' in java.util.arraylist at meta-inf/license.vm[line 1619, column 22]  [error] failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hbase-assembly: error rendering velocity resource. error invoking method 'get(java.lang.integer)' in java.util.arraylist at meta-inf/license.vm[line 1619, column 22]: invocationtargetexception: index: 0, size: 0 -> [help 1]  [error]   [error] to see the full stack trace of the errors, re-run maven with the -e switch.  [error] re-run maven using the -x switch to enable full debug logging.  [error]   [error] for more information about the errors and possible solutions, please read the following articles:  [error] [help 1] http://cwiki.apache.org/confluence/display/maven/mojoexecutionexception  [error]   [error] after correcting the problems, you can resume the build with the command  [error] mvn <goals> -rf :hbase-assembly ",
        "label": 402
    },
    {
        "text": "generate a globally unique identifier for a cluster and store in  hbase hbase id  we don't currently have a way to uniquely identify an hbase cluster, apart for where it's stored in hdfs or configuration of the zookeeper quorum managing it. it would be generally useful to be able to identify a cluster via api. the proposal here is pretty simple: 1. when master initializes the filesystem, generate a globally unique id and store in /hbase/hbase.id 2. for existing clusters, generate hbase.id on master startup if it does not exist 3. include unique id in clusterstatus returned from master for token authentication, this will be required to allow selecting the correct token to pass to a cluster when a single client is communicating to more than one hbase instance. chatting with j-d, replication stores it's own cluster id in place with each hlog edit, so requires as small as possible an identifier, but i think we could automate a mapping from unique cluster id -> short id if we had the unique id available. ",
        "label": 180
    },
    {
        "text": "the integration test in master branch's nightly job has error  error  only found rows   https://builds.apache.org/job/hbase%20nightly/job/master/1075/artifact/output-integration/hadoop-2.log https://builds.apache.org/job/hbase%20nightly/job/master/1075/artifact/output-integration/hadoop-3.log ",
        "label": 187
    },
    {
        "text": "create split strategy for ycsb benchmark  talked with lars about how we can make it easier for users to run the ycsb benchmarks against hbase & get realistic results. currently, hbase is optimized for the random/uniform read/write case, which is the ycsb load. the initial reason why we perform bad when users test against us is because they do not presplit regions & have the split ratio really low. we need a one-line way for a user to create a table that is pre-split to 200 regions (or some decent number) by default & disable splitting. realistically, this is how a uniform load cluster should scale, so it's not a hack. this will also give us a good use case to point to for how users should pre-split regions. ",
        "label": 298
    },
    {
        "text": "replication source in memory accounting should not include bulk transfer hfiles  in replicationsourcewalreaderthread we maintain a global quota on enqueued replication work for preventing oom by queuing up too many edits into queues on heap. when calculating the size of a given replication queue entry, if it has associated hfiles (is a bulk load to be replicated as a batch of hfiles), we get the file sizes and include the sum. we then apply that result to the quota. this isn't quite right. those hfiles will be pulled by the sink as a file copy, not pushed by the source. the cells in those files are not queued in memory at the source and therefore shouldn't be counted against the quota. related, the sum of the hfile sizes are also included when checking if queued work exceeds the configured replication queue capacity, which is by default 64 mb. hfiles are commonly much larger than this. so what happens is when we encounter a bulk load replication entry typically both the quota and capacity limits are exceeded, we break out of loops, and send right away. what is transferred on the wire via hbase rpc though has only a partial relationship to the calculation. depending how you look at it, it makes sense to factor hfile file sizes against replication queue capacity limits. the sink will be occupied transferring those files at the hdfs level. anyway, this is how we have been doing it and it is too late to change now. i do not however think it is correct to apply hfile file sizes against a quota for in memory state on the source. the source doesn't queue or even transfer those bytes. something i noticed while working on hbase-18027. ",
        "label": 494
    },
    {
        "text": "webui says  meta  table but table got renames to  hbase meta  meed to update the ui   in the ui, we say \"the .meta. table holds references to all user table  regions\" but the table name is \"hbase:meta\" and not \".meta.\" we need to update this. ",
        "label": 314
    },
    {
        "text": "support for custom filters with pb based rpc  ",
        "label": 186
    },
    {
        "text": "minor integration test framework fixes  made filesystem on hbasetestingutil.createmulti() not expect mini cluster added check if server is not running before deciding to restore a server ",
        "label": 174
    },
    {
        "text": "hashset of byte array is being used in couple of places  while working on a jira i realized i had made a mistake of making a hashset of byte array.   then out of curiosity i checked if we do same any where else in code base. i came with following files. 1. /src/main/java/org/apache/hadoop/hbase/mapreduce/rowcounter.java: set<byte []> qualifiers = new hashset<byte[]>(); 2. /src/main/java/org/apache/hadoop/hbase/regionserver/hregionserver.java: columnfamilies = new hashset<byte[]>(); 3. /src/test/java/org/apache/hadoop/hbase/filter/testfirstkeyvaluematchingqualifiersfilter.java: set<byte[]> quals = new hashset<byte[]>(); 4. /src/test/java/org/apache/hadoop/hbase/regionserver/metrics/testschemametrics.java: set<byte[]> families = new hashset<byte[]>();  (1) and (3) are mine and i will fix them (not yet committed). quoting the exact reference from (2) below :  @override                                                                        public getstorefileresponse getstorefile(final rpccontroller controller,                 final getstorefilerequest request) throws serviceexception {                   try {                                                                              hregion region = getregion(request.getregion());                                 requestcount.incrementandget();                                                  set<byte[]> columnfamilies = null;                                               if (request.getfamilycount() == 0) {                                               columnfamilies = region.getstores().keyset();                                  } else {                                                                           columnfamilies = new hashset<byte[]>();                                          for (bytestring cf: request.getfamilylist()) {                                     columnfamilies.add(cf.tobytearray());                                          }                                                                              }   ",
        "label": 339
    },
    {
        "text": "testing in place does not work if not building with default profile  if i build with the hadoop 2 profile, for example: mvn -dhadoop.profile=2.0 -dhadoop.version=2.0.3-snapshot and then try to run daemons like so: ./bin/hbase master start this will fail, because the launch script will invoke maven to build the cached classpath selecting whatever is the default profile, currently hadoop 1. startup will actually get pretty far, until: 12/12/04 11:42:13 warn regionserver.hregionserver: error telling master we are up com.google.protobuf.serviceexception: java.lang.noclassdeffounderror: org/apache/hadoop/net/socketinputwrapper at org.apache.hadoop.hbase.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:189) at $proxy10.regionserverstartup(unknown source) at org.apache.hadoop.hbase.regionserver.hregionserver.reportforduty(hregionserver.java:1844) at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:843) at java.lang.thread.run(thread.java:679) caused by: java.lang.noclassdeffounderror: org/apache/hadoop/net/socketinputwrapper at org.apache.hadoop.hbase.ipc.hbaseclient.createconnection(hbaseclient.java:317) at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:1415) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:1278) at org.apache.hadoop.hbase.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:177) ... 4 more caused by: java.lang.classnotfoundexception: org.apache.hadoop.net.socketinputwrapper there doesn't appear to be a way to supply additional arguments to the launch script for directing maven which profile(s) to activate. ",
        "label": 340
    },
    {
        "text": "handle regionalreadyintransitionexception in assignmentmanager  comment from stack over in hbase-3741: question: looking at this patch again, if we throw a regionalreadyintransitionexception, won't we just assign the region elsewhere though regionalreadyintransitionexception in at least one case here is saying that the region is already open on this regionserver? indeed looking at the code it's going to be handled the same way other exceptions are. need to add special cases for assign and unassign. ",
        "label": 544
    },
    {
        "text": "split happened replica region can not be deleted after deleting table successfully and restarting regionserver  [test step]  1.create a table (set regionreplication=2).  2.insert data to the table utill region be splitted.  3.disable and drop the table.  4.parent replica region holding regionserver, kill forcefully   5.hbase webui will show that the replica regions will be in rit. [expect output]  parent replica region should be deleted. [actual output]  parent replica region still exists. ",
        "label": 496
    },
    {
        "text": "add hbase spark integration tests to it jenkins job  expand the set of its we run to include the new hbase-spark tests. ",
        "label": 402
    },
    {
        "text": "ensure hbase is covered in thrift  hbase-8695 is about using the config file, make sure thrift 2 is doing the same. ",
        "label": 285
    },
    {
        "text": "hbaseconfiguration can carry a main method that dumps xml output for debug purposes  just like the configuration class carries a main() method in it, that simply loads itself and writes xml out to system.out, hbaseconfiguration can use the same kinda method. that way we can do \"hbase org.apache.hadoop.\u2026.hbaseconfiguration\" to get an xml dump of things hbaseconfiguration has properly loaded. nifty in checking app classpaths sometimes. ",
        "label": 408
    },
    {
        "text": "hbase server doesn't preserve sasl sequence number on the network  when auth-conf is enabled on rpc, the server encrypt response in setreponse() using saslserver. the generated cryptogram included a sequence number manage by saslserver. but then, when the response is sent over the network, the sequence number order is not preserved. the client receives reply in the wrong order, leading to a log message from digestmd5base: sasl:1481  - digest41:unmatched macs then the message is discarded, leading the client to a timeout. i propose a fix here: https://github.com/sbarnoud/hbase-release/commit/ce9894ffe0e4039deecd1ed51fa135f64b311d41 it seems that any hbase 1.x is affected. this part of code has been fully rewritten in hbase 2.x, and i haven't do the analysis on hbase 2.x which may be affected.   here, an extract of client log that i added to help me to understand: \u2026 2019-05-28 12:53:48,644 debug [default-ipc-nioeventloopgroup-1-32] nettyrpcduplexhandler:80  - callid: 5846 /192.163.201.65:58870 -> dtltstap004.fr.world.socgen/192.163.201.72:16020 2019-05-28 12:53:48,651 info  [default-ipc-nioeventloopgroup-1-18] nioeventloop:101  - sg: channel ready to read 1315913615 unsafe 1493023957 /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 2019-05-28 12:53:48,651 info  [default-ipc-nioeventloopgroup-1-18] saslunwraphandler:78  - sg: after unwrap:46 -> 29 for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 seqnum 150 2019-05-28 12:53:48,652 debug [default-ipc-nioeventloopgroup-1-18] nettyrpcduplexhandler:192  - callid: 5801 received totalsize:25 message:20 scannersize:(null)/192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 2019-05-28 12:53:48,652 info  [default-ipc-nioeventloopgroup-1-18] sasl:1481  - digest41:unmatched macs 2019-05-28 12:53:48,652 warn  [default-ipc-nioeventloopgroup-1-18] saslunwraphandler:70  - sasl error (probably invalid mac) detected for /192.163.201.65:44236 -> dtltstap008.fr.world.socgen/192.163.201.109:16020 saslclient @4ac31121 ctx @14fb001d msg @140313192718406 len 118 data:1c^g?^p?3??h?k??????\"??x?$^_??^d;^]7^es??em?c?w^r^bl?????????x??omg?z?i???45}???de?^\\^s>d?^????/4f?^^?? ?^e????d?????????d?km^@^a^@^@^@? readerindex 118 writerindex 118 seqnum 152  we can see that the client unwraps the sasl message with sequence number 152 before sequence number 151 and fails with the unmatched mac.   i opened a case to oracle because we should had an error (and not the message ignored). that's because the jdk doesn't controls integrity in the right way. https://github.com/openjdk/jdk/blob/master/src/java.security.sasl/share/classes/com/sun/security/sasl/digest/digestmd5base.java the actual jdk controls the hmac before the sequence number and hides the real error (bad sequence number) because sasl is stateful. the jdk should check first the sequence number and then the hmac. when (and if) the jdk will be patched, and accordingly to https://www.ietf.org/rfc/rfc2831.txt , we will get an exception in that case instead of having the message ignored.     ",
        "label": 434
    },
    {
        "text": "asynchbaseadmin should use exponential backoff when polling the procedure result  now it polls too frequent and in tests there are so many debug logs... ",
        "label": 149
    },
    {
        "text": "regions not assigned after hbase went in  looking into the hudson failure, http://hudson.zones.apache.org/hudson/view/hbase/job/hbase-trunk/1339/ at the failing org.apache.hadoop.hbase.client.testfromclientside.testregioncacheprewarm test, i seem to be seeing a case of regions being added to unassigned up in zk but then subsequently nothing. adjacent regions are being similarily added but these are being assigned out. somethings up. it seems to be causing the above test failure. its hard to see in the logs.... grep for this line:  log.info(\"starting testregioncacheprewarm\"); the unit test then does test_util.createmultiregions. you'll see all regions being created and then they unassigned znodes are created. grep for the 'eee' row from testcacheprewarm table (be careful, there is also logging for testcacheprewarm2 in this log). the last thing you'll see is: 2010-06-19 19:34:39,628 debug [regionmanager.metascanner] master.regionmanager(1006): created unassigned znode testcacheprewarm,eee,1276976076048.557068905bf2abfe84a5e49953a23c02. in state m2zk_region_offline for ddd and fff, you'll see those assigned out. maybe the test is not hanging about long enough but wonder why its skipped? ",
        "label": 453
    },
    {
        "text": "update config field names in hbase default xml description for hbase hregion memstore block multiplier  i noticed several field names in the description for hbase.hregion.memstore.block.multiplier were the old names and not the current ones. patch attached. ",
        "label": 13
    },
    {
        "text": "perf test doing all mutation steps under row lock  this issue is about perf testing a redo of the write pipeline so that rather than: take rowlock start mvcc append to wal add to memstore sync wal let go of rowlock finish up mvcc instead.... try... take rowlock start mvcc append to wal sync wal add to memstore finish up mvcc let go of rowlock the latter is more straight-forward undoing need of rolling back memstore if all does not succeed. it might be slower though. this issue is a look-see/try it. the redo will also help address the parent issue in a more general way so we can do without the special-casing done for branch-1.0 and branch-1.1 done in a sibling subtask. other benefits are that the current write pipeline is copy/pasted in a few places \u2013 in append, increment and checkand* \u2013 and a refactor will allow us to fix this duplication. ",
        "label": 314
    },
    {
        "text": "reassigning region stuck in open still may not work correctly due to leftover zk node  (logs grepped by region name, and abridged. meta server was dead so openregionhandler for the region took a while, and was interrupted: 2013-02-08 14:35:01,555 debug [rs_open_region-10.11.2.92,64485,1360362800564-2] handler.openregionhandler(255): interrupting thread thread[postopendeploytasks:871d1c3bdf98a2c93b527cb6cc61327d,5,main] then master tried to force region offline and reassign: 2013-02-08 14:35:06,500 info  [master_server_operations-10.11.2.92,64483,1360362800340-1] master.regionstates(347): found opening region {integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=opening, ts=1360362901596, server=10.11.2.92,64485,1360362800564} to be reassigned by ssh for 10.11.2.92,64485,1360362800564 2013-02-08 14:35:06,500 info  [master_server_operations-10.11.2.92,64483,1360362800340-1] master.regionstates(242): region {name => 'integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d.', startkey => '7333332c', endkey => '7ffffff8', encoded => 871d1c3bdf98a2c93b527cb6cc61327d,} transitioned from {integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=opening, ts=1360362901596, server=10.11.2.92,64485,1360362800564} to {integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=closed, ts=1360362906500, server=null} 2013-02-08 14:35:06,505 debug [10.11.2.92,64483,1360362800340-generalbulkassigner-1] master.assignmentmanager(1530): forcing offline; was={integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=closed, ts=1360362906500, server=null} 2013-02-08 14:35:06,506 debug [10.11.2.92,64483,1360362800340-generalbulkassigner-1] zookeeper.zkassign(176): master:64483-0x13cbbf1025d0000 async create of unassigned node for 871d1c3bdf98a2c93b527cb6cc61327d with offline state but didn't delete the original zk node? 2013-02-08 14:35:06,509 warn  [main-eventthread] master.offlinecallback(59): node for /hbase/region-in-transition/871d1c3bdf98a2c93b527cb6cc61327d already exists 2013-02-08 14:35:06,509 debug [main-eventthread] master.offlinecallback(69): rs={integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=offline, ts=1360362906506, server=null}, server=10.11.2.92,64488,1360362800651 2013-02-08 14:35:06,512 debug [main-eventthread] master.offlinecallback$existcallback(106): rs={integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=offline, ts=1360362906506, server=null}, server=10.11.2.92,64488,1360362800651 so it went into infinite cycle of failing to assign due to this: 2013-02-08 14:35:06,517 info  [pri ipc server handler 7 on 64488] regionserver.hregionserver(3435): received request to open region: integrationtestrebalanceandkillserverstargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. on 10.11.2.92,64488,1360362800651 2013-02-08 14:35:06,521 warn  [rs_open_region-10.11.2.92,64488,1360362800651-0] zookeeper.zkassign(762): regionserver:64488-0x13cbbf1025d0004 attempt to transition the unassigned node for 871d1c3bdf98a2c93b527cb6cc61327d from m_zk_region_offline to rs_zk_region_opening failed, the node existed but was in the state rs_zk_region_opening set by the server [wrong server name redacted, see hbase-7798] transitioning failed-to-open similarly fails. it seems like master needs to nuke zk node unconditionally to offline? ",
        "label": 242
    },
    {
        "text": "enable testfavoredstochasticbalancerpickers testpickers that was disabled by proc v2 am in hbase  the testpickers in testfavoredstochasticbalancerpickers hangs after applying the change in core proc-v2 am in hbase-14614. it was disabled. this jira tracks the work to enable it. ",
        "label": 446
    },
    {
        "text": "asynctableresultscanner will hang when scan wrong column family  asyntableresultscanner did not call notify(). so the next will hang on wait(). it is easy to fix. and will add a ut for this. ",
        "label": 187
    },
    {
        "text": " log  'group not found for table' is chatty  when rsgroup feature is enabled, there are few places when 'rsgroupbasedloadbalancer' can't find group of a table and print a lot of group not found for table ***, using default which is chatty and useless in master log. plan to make it debug level or remove it, since we can always know what tables a rsgroup contains by using hbase shell. ",
        "label": 334
    },
    {
        "text": "hbase broke testget2 unit test  and broke the build   ",
        "label": 314
    },
    {
        "text": "invalidcolumnnameexception not passed through to thrift interface  if an invalid column name is passed to the thrift server, the invalidcolumnnameexception is not caught and turned into a thrift exception. debug output: 08/05/29 20:34:30 debug thrift.thriftserver$hbasehandler: get: table=test_table, row=todd, col=foobar  08/05/29 20:34:30 debug hbase.htable: reloading table servers because: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901) 08/05/29 20:34:40 debug hbase.htable: reloading table servers because: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901) 08/05/29 20:34:50 debug hbase.htable: reloading table servers because: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901) 08/05/29 20:35:00 debug hbase.htable: reloading table servers because: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901) 08/05/29 20:35:10 debug hbase.htable: trying to contact region server for row 'todd', but failed after 5 attempts.  exception 1:  org.apache.hadoop.hbase.invalidcolumnnameexception: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901)  exception 1:  org.apache.hadoop.hbase.invalidcolumnnameexception: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901)  exception 1:  org.apache.hadoop.hbase.invalidcolumnnameexception: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901)  exception 1:  org.apache.hadoop.hbase.invalidcolumnnameexception: org.apache.hadoop.hbase.invalidcolumnnameexception: foobar is missing the colon family/qualifier separator  at org.apache.hadoop.hbase.hstorekey.getcolonoffset(hstorekey.java:335)  at org.apache.hadoop.hbase.hstorekey.extractfamily(hstorekey.java:295)  at org.apache.hadoop.hbase.hregion.checkcolumn(hregion.java:1676)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1191)  at org.apache.hadoop.hbase.hregion.get(hregion.java:1154)  at org.apache.hadoop.hbase.hregionserver.get(hregionserver.java:1402)  at sun.reflect.generatedmethodaccessor5.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:585)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)  at org.apache.hadoop.ipc.server$handler.run(server.java:901) ",
        "label": 451
    },
    {
        "text": "add a faq item for updating a maven managed application from     in 0.96 we changed artifact structure, so that clients need to rely on an artifact specific to some module (hopefully hbase-client) instead of a single fat jar. we should add a faq item that points people towards hbase-client, to ease those updating downstream applications from 0.94 to 0.98+. showing an example pom entry for e.g. org.apache.hbase:hbase:0.94.22 and one for e.g. org.apache.hbase:hbase-client:0.98.5 should be sufficient. ",
        "label": 330
    },
    {
        "text": "include block content verification logic used in lrucache in bucketcache  with off-heap/bucketcache being used to cache data blocks without going through on-heap cache, the logic used in lrucache to check the content of already cached block need to be included in bucketcache. please see this discussion for details. ",
        "label": 72
    },
    {
        "text": "addition of a column pagination filter  client applications may need to do pagination, depending on the number of columns returned, it may be more efficient to perform pagination algorithms at the database level (similar to sql's limit and offset). this will be an additional filter taking two parameters: page pagesize for every row, that gets returned, only a subset of columns are returned based on page and pagesize if the page / pagesize column goes over the limits, then no results are returned from the filter. a practical example for using a filter like this may be for folks doing row-based indexing with hbase. ",
        "label": 349
    },
    {
        "text": "hmaster's clear command could conflict with jmx ports  hmaster's clear command still could bind the the master's jmx ports because it was using the wrong hbase_opts. ",
        "label": 154
    },
    {
        "text": "testsplittransactiononcluster fails because  region not moved off  meta  server   here: http://54.241.6.143/job/hbase-0.95/org.apache.hbase$hbase-server/596/testreport/org.apache.hadoop.hbase.regionserver/testsplittransactiononcluster/testsplitregionwithnostorefiles/ and here: http://54.241.6.143/job/hbase-0.95-hadoop-2/org.apache.hbase$hbase-server/597/testreport/org.apache.hadoop.hbase.regionserver/testsplittransactiononcluster/testrssplitephemeralsdisappearbutdaughtersareonlinedaftershutdownhandling/ and here: http://54.241.6.143/job/hbase-0.95/org.apache.hbase$hbase-server/598/testreport/org.apache.hadoop.hbase.regionserver/testsplittransactiononcluster/testrssplitephemeralsdisappearbutdaughtersareonlinedaftershutdownhandling/ and here: http://54.241.6.143/job/hbase-trunk-hadoop-2/org.apache.hbase$hbase-server/395/testreport/org.apache.hadoop.hbase.regionserver/testsplittransactiononcluster/testrssplitephemeralsdisappearbutdaughtersareonlinedaftershutdownhandling/ ... etc. looking, the move operation just evaporates. the region we want to move has not finished opening yet so when move comes into the master it does not proceed (because region is currently in rit). ",
        "label": 314
    },
    {
        "text": "size based scan metric broken by protobufs  see scannercallable. hbase-7215 comments that portion, but it did not work before, because results.bytes is no longer used with protobufs. ",
        "label": 406
    },
    {
        "text": "oracle java 8u144 downloader broken in precommit check  precommit job fails to install oracle java 8 to docker image which is due to oracle's new java version, 8u151. as this thread point out we probably need to upgrade to latest java 8 version: https://ubuntuforums.org/showthread.php?t=2374686 06:45:14 setting up java-common (0.51) ... 06:45:14 setting up oracle-java8-installer (8u144-1~webupd8~0) ... 06:45:14 \u001b[91mno /var/cache/oracle-jdk8-installer/wgetrc file found. 06:45:14 creating /var/cache/oracle-jdk8-installer/wgetrc and 06:45:14 using default oracle-java8-installer wgetrc settings for it. 06:45:14 downloading oracle java 8... 06:45:14 \u001b[0m\u001b[91m--2017-10-18 13:45:14--  http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz 06:45:14 resolving download.oracle.com (download.oracle.com)... \u001b[0m\u001b[91m23.59.189.81, 23.59.189.91 06:45:14 connecting to download.oracle.com (download.oracle.com)|23.59.189.81|:80... \u001b[0m\u001b[91mconnected. 06:45:14 http request sent, awaiting response... \u001b[0m\u001b[91m302 moved temporarily 06:45:14 location: https://edelivery.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz [following] 06:45:14 --2017-10-18 13:45:14--  https://edelivery.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz 06:45:14 \u001b[0m\u001b[91mresolving edelivery.oracle.com (edelivery.oracle.com)... \u001b[0m\u001b[91m23.39.16.136, 2600:1409:a:39e::2d3e, 2600:1409:a:39c::2d3e 06:45:14 connecting to edelivery.oracle.com (edelivery.oracle.com)|23.39.16.136|:443... \u001b[0m\u001b[91mconnected. 06:45:14 \u001b[0m\u001b[91mhttp request sent, awaiting response... \u001b[0m\u001b[91m302 moved temporarily 06:45:14 location: http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz?authparam=1508334434_7da3c9610b0368a45f954cd47d91121c [following] 06:45:14 --2017-10-18 13:45:14--  http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz?authparam=1508334434_7da3c9610b0368a45f954cd47d91121c 06:45:14 connecting to download.oracle.com (download.oracle.com)|23.59.189.81|:80... \u001b[0m\u001b[91mconnected. 06:45:14 http request sent, awaiting response... \u001b[0m\u001b[91m404 not found 06:45:14 2017-10-18 13:45:14 error 404: not found. 06:45:14  06:45:14 \u001b[0m\u001b[91mdownload failed 06:45:14 oracle jdk 8 is not installed. 06:45:14 \u001b[0m\u001b[91mdpkg: error processing package oracle-java8-installer (--configure): 06:45:14  subprocess installed post-installation script returned error exit status 1 06:45:14 \u001b[0m\u001b[91merrors were encountered while processing: 06:45:14  oracle-java8-installer 06:45:29 \u001b[0m\u001b[91me: sub-process /usr/bin/dpkg returned an error code (1) 06:45:29 \u001b[0mthe command '/bin/sh -c apt-get -q update && apt-get -q install --no-install-recommends -y oracle-java8-installer' returned a non-zero code: 100 06:45:29  06:45:29 total elapsed time:   3m 19s 06:45:29  06:45:29 error: docker failed to build image. workaround mentioned in the forum post: sudo sed -i 's|java_version=8u144|java_version=8u152|' oracle-java8-installer.* sudo sed -i 's|partner_url=http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/|partner_url=http://download.oracle.com/otn-pub/java/jdk/8u152-b16/aa0333dd3019491ca4f6ddbe78cdb6d0/|' oracle-java8-installer.* sudo sed -i 's|sha256sum_tgz=\"e8a341ce566f32c3d06f6d0f0eeea9a0f434f538d22af949ae58bc86f2eeaae4\"|sha256sum_tgz=\"218b3b340c3f6d05d940b817d0270dfe0cfd657a636bad074dcabe0c111961bf\"|' oracle-java8-installer.* sudo sed -i 's|j_dir=jdk1.8.0_144|j_dir=jdk1.8.0_152|' oracle-java8-installer.* ",
        "label": 149
    },
    {
        "text": "hbase rest server crashes if client tries to retrieve data size   mb  i have a cf with one qualifier, data size is > 5 mb, when i try to read the raw binary data as octet-stream using curl, rest server got crashed and curl throws exception as  curl -v -h \"accept: application/octet-stream\" http://abcdefgh-hbase003.test1.test.com:9090/table1/row_key1/cf:qualifer1 > /tmp/out * about to connect() to abcdefgh-hbase003.test1.test.com port 9090 *   trying xx.xx.xx.xxx... connected * connected to abcdefgh-hbase003.test1.test.com (xx.xxx.xx.xxx) port 9090 > get /table1/row_key1/cf:qualifer1 http/1.1 > user-agent: curl/7.15.5 (x86_64-redhat-linux-gnu) libcurl/7.15.5 openssl/0.9.8b zlib/1.2.3 libidn/0.6.5 > host: abcdefgh-hbase003.test1.test.com:9090 > accept: application/octet-stream >    % total    % received % xferd  average speed   time    time     time  current                                  dload  upload   total   spent    left  speed   0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0< http/1.1 200 ok < content-length: 5129836 < x-timestamp: 1347338813129 < content-type: application/octet-stream   0 5009k    0 16272    0     0   7460      0  0:11:27  0:00:02  0:11:25 13872transfer closed with 1148524 bytes remaining to read  77 5009k   77 3888k    0     0  1765k      0  0:00:02  0:00:02 --:--:-- 3253k* closing connection #0 curl: (18) transfer closed with 1148524 bytes remaining to read couldn't find the exception in rest server log or no core dump either. this issue is constantly reproducible. even i tried with hbase rest client (hremotetable) and i could recreate this issue if the data size is > 10 mb (even with mime_protobuf accept header) ",
        "label": 242
    },
    {
        "text": "make testsplitmerge more stable  sometimes it fails in pre commit. ",
        "label": 149
    },
    {
        "text": "batch deletes in mapreduce jobs    we found that some of our copy table job run for many hours, even when there isn't that much data to copy. vikas vishwakarma did his magic and found that the issue is with copying delete markers (we use raw mode to also move deletes across).  looking at the code in 0.98 it's immediately obvious that deletes (unlike puts) are not batched and hence sent to the other side one by one, causing a network rtt for each delete marker. looks like in trunk it's doing the right thing (using bufferedmutators for all mutations in tableoutputformat). so likely only a 0.98 (and 1.0, 1.1, 1.2?) issue. ",
        "label": 17
    },
    {
        "text": "fix unused protobuf warning in admin proto  warning: unused import: \"admin.proto\" imports \"client.proto\" which is not used. ",
        "label": 499
    },
    {
        "text": "master can't exit when open port failed  when hmaster crashed and restart , the hmaster is hung up.  // start up all service threads.  startservicethreads(); ----this open port failed!  // wait for region servers to report in. returns count of regions.  int regioncount = this.servermanager.waitforregionservers();  // todo: should do this in background rather than block master startup  this.filesystemmanager.  splitlogafterstartup(this.servermanager.getonlineservers());  // make sure root and meta assigned before proceeding.  assignrootandmeta(); \u2014 hung up this function, because of root can't be assigned.  if (!catalogtracker.verifyrootregionlocation(timeout)) {  this.assignmentmanager.assignroot();  this.catalogtracker.waitforroot(); \u2014 this statement code is hung up.   assigned++;  } log is as\uff1a 2011-04-07 16:38:22,850 info org.mortbay.log: logging to org.slf4j.impl.log4jloggeradapter(org.mortbay.log) via org.mortbay.log.slf4jlog  2011-04-07 16:38:22,908 info org.apache.hadoop.http.httpserver: port returned by webserver.getconnectors()[0].getlocalport() before open() is -1. opening the listener on 60010  2011-04-07 16:38:22,909 fatal org.apache.hadoop.hbase.master.hmaster: failed startup  java.net.bindexception: address already in use  at sun.nio.ch.net.bind(native method)  at sun.nio.ch.serversocketchannelimpl.bind(serversocketchannelimpl.java:119)  at sun.nio.ch.serversocketadaptor.bind(serversocketadaptor.java:59)  at org.mortbay.jetty.nio.selectchannelconnector.open(selectchannelconnector.java:216)  at org.apache.hadoop.http.httpserver.start(httpserver.java:445)  at org.apache.hadoop.hbase.master.hmaster.startservicethreads(hmaster.java:542)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:373)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:278)  2011-04-07 16:38:22,910 info org.apache.hadoop.hbase.master.hmaster: aborting  2011-04-07 16:38:22,911 info org.apache.hadoop.hbase.master.servermanager: exiting wait on regionserver(s) to checkin; count=0, stopped=true, count of regions out on cluster=0  2011-04-07 16:38:22,914 debug org.apache.hadoop.hbase.master.masterfilesystem: no log files to split, proceeding...  2011-04-07 16:38:22,930 info org.apache.hadoop.ipc.hbaserpc: server at 167-6-1-12/167.6.1.12:60020 could not be reached after 1 tries, giving up.  2011-04-07 16:38:22,930 info org.apache.hadoop.hbase.catalog.rootlocationeditor: unsetting root region location in zookeeper  2011-04-07 16:38:22,941 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x22f2c49d2590021 creating (or updating) unassigned node for 70236052 with offline state  2011-04-07 16:38:22,956 debug org.apache.hadoop.hbase.master.assignmentmanager: server stopped; skipping assign of root,,0.70236052 state=offline, ts=1302165502941  2011-04-07 16:38:32,746 info org.apache.hadoop.hbase.master.assignmentmanager$timeoutmonitor: 167-6-1-11:60000.timeoutmonitor exiting  2011-04-07 16:39:22,770 info org.apache.hadoop.hbase.master.logcleaner: master-167-6-1-11:60000.oldlogcleaner exiting ",
        "label": 529
    },
    {
        "text": " move  region right after a region split is dangerous  i ran into a situation where the cm issued a move for a region just after a region was split. the master went bonkers since the master honored the cm request, and assigned the split region, but subsequently all the region state assumptions on this (split)region was messed up. i started seeing log lines lines like \"this should not happen\". also, it created other problems - a compaction on original region happened on the new assignee, and then the daughter regions started seeing issues to do with store files missing, etc., etc. i will upload the logs shortly. ",
        "label": 242
    },
    {
        "text": "add a multi region merge  for fixing overlaps  etc   mergetableregionprocedure does two regions at a time only. we have hardcoded notion that merge has an 'a' and 'b' parent. this issue is about making it so we merge n regions all in the one go. we need this facility generally; will simplify operator's life being able to merge up in one go rather than in steps of two. in particular, would make fixing overlaps easier if could just merge all regions in the overlapping area. ",
        "label": 314
    },
    {
        "text": "got exception when manually triggers a split on an empty region  we should allow a region to split successfully even if it does not yet have storefiles. ",
        "label": 482
    },
    {
        "text": "typo in block cache monitoring documentation  ",
        "label": 488
    },
    {
        "text": "make minidfscluster run faster  daryn proposed the following change in hdfs-6773: editlogfileoutputstream.setshouldskipfsyncfortesting(true); with this change in hbasetestingutility#startminidfscluster(), runtime for testadmin went from 8:35 min to 7 min ",
        "label": 441
    },
    {
        "text": "procedure v2   web ui displaying store state  the procedure webui page should show information about the walprocedurestore. number/list/size of wals active we may extract the \"sync wait %s, slotindex=%s , totalsynced=%s (%s/sec)\" that today is only in log.trace() we have a getmillistonextperiodicroll() and getmillisfromlastroll() if anyone want to see that ",
        "label": 393
    },
    {
        "text": "update hadoop support description to explain  not tested  vs  not supported   from nick dimiduk in thread about hadoop 2.6.1+: while we're in there, we should also clarify the meaning of \"not supported\"  vs \"not tested\". it seems we don't say what we mean by these distinctions. ",
        "label": 330
    },
    {
        "text": "thriftutilities getfromthrift should set filter when not set columns  thriftutilities.getfromthrift, if tget wihtout columns the filter is ignore. if (!in.issetcolumns()) {  return out; } for (tcolumn column : in.getcolumns()) {  if (column.issetqualifier()) {  out.addcolumn(column.getfamily(), column.getqualifier());  } else {  out.addfamily(column.getfamily());  } } if (in.issetfilterbytes()) {  out.setfilter(filterfromthrift(in.getfilterbytes())); } ",
        "label": 74
    },
    {
        "text": "cluster can't start when log splitting at startup time and the master's web ui is refreshed a few times  it looks like we cannot show the master's web ui at start time when there are logs to split because we can't reach the namespace regions. so it means that you can't see how things are progressing without tailing the log while waiting on your cluster to boot up. this wasn't the case in 0.94 see this jstack: \"606214580@qtp-2001431298-3\" prio=10 tid=0x00007f6ac8040000 nid=0x7b1 in object.wait() [0x00007f6aa82bf000]    java.lang.thread.state: timed_waiting (on object monitor) at java.lang.object.wait(native method) - waiting on <0x00000000bc0c1460> (a org.apache.hadoop.hbase.ipc.rpcclient$call) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1416) - locked <0x00000000bc0c1460> (a org.apache.hadoop.hbase.ipc.rpcclient$call) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1634) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1691) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$blockingstub.listtabledescriptorsbynamespace(masteradminprotos.java:35031) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$5.listtabledescriptorsbynamespace(hconnectionmanager.java:2181) at org.apache.hadoop.hbase.client.hbaseadmin$22.call(hbaseadmin.java:2265) at org.apache.hadoop.hbase.client.hbaseadmin$22.call(hbaseadmin.java:2262) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:116) - locked <0x00000000c09baf20> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:94) - locked <0x00000000c09baf20> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:3155) at org.apache.hadoop.hbase.client.hbaseadmin.listtabledescriptorsbynamespace(hbaseadmin.java:2261) at org.apache.hadoop.hbase.tmpl.master.masterstatustmplimpl.__jamon_innerunit__catalogtables(masterstatustmplimpl.java:461) at org.apache.hadoop.hbase.tmpl.master.masterstatustmplimpl.rendernoflush(masterstatustmplimpl.java:270) at org.apache.hadoop.hbase.tmpl.master.masterstatustmpl.rendernoflush(masterstatustmpl.java:382) at org.apache.hadoop.hbase.tmpl.master.masterstatustmpl.render(masterstatustmpl.java:372) at org.apache.hadoop.hbase.master.masterstatusservlet.doget(masterstatusservlet.java:95) at javax.servlet.http.httpservlet.service(httpservlet.java:707) at javax.servlet.http.httpservlet.service(httpservlet.java:820) at org.mortbay.jetty.servlet.servletholder.handle(servletholder.java:511) at org.mortbay.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1221) at org.apache.hadoop.http.httpserver$quotinginputfilter.dofilter(httpserver.java:850) at org.mortbay.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1212) at org.mortbay.jetty.servlet.servlethandler.handle(servlethandler.java:399) at org.mortbay.jetty.security.securityhandler.handle(securityhandler.java:216) at org.mortbay.jetty.servlet.sessionhandler.handle(sessionhandler.java:182) at org.mortbay.jetty.handler.contexthandler.handle(contexthandler.java:766) at org.mortbay.jetty.webapp.webappcontext.handle(webappcontext.java:450) at org.mortbay.jetty.handler.contexthandlercollection.handle(contexthandlercollection.java:230) at org.mortbay.jetty.handler.handlerwrapper.handle(handlerwrapper.java:152) at org.mortbay.jetty.server.handle(server.java:326) at org.mortbay.jetty.httpconnection.handlerequest(httpconnection.java:542) at org.mortbay.jetty.httpconnection$requesthandler.headercomplete(httpconnection.java:928) at org.mortbay.jetty.httpparser.parsenext(httpparser.java:549) at org.mortbay.jetty.httpparser.parseavailable(httpparser.java:212) at org.mortbay.jetty.httpconnection.handle(httpconnection.java:404) at org.mortbay.io.nio.selectchannelendpoint.run(selectchannelendpoint.java:410) at org.mortbay.thread.queuedthreadpool$poolthread.run(queuedthreadpool.java:582) ... \"rpcserver.handler=28,port=60000\" daemon prio=10 tid=0x00007f6ad08a5800 nid=0x77e waiting on condition [0x00007f6aa97d5000]    java.lang.thread.state: timed_waiting (sleeping) at java.lang.thread.sleep(native method) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:148) - locked <0x00000000c0909178> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.htable.get(htable.java:760) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:171) at org.apache.hadoop.hbase.master.tablenamespacemanager.getnamespacetable(tablenamespacemanager.java:119) - locked <0x00000000c1958890> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:155) - locked <0x00000000c1958890> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3088) at org.apache.hadoop.hbase.master.hmaster.listtabledescriptorsbynamespace(hmaster.java:3102) at org.apache.hadoop.hbase.master.hmaster.listtabledescriptorsbynamespace(hmaster.java:3012) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:32908) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2146) at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1851) ",
        "label": 229
    },
    {
        "text": "rely on nightly tests for findbugs compliance on existing branch  the \"-1\" for extant findbugs warnings has confused interpretation of our precommit checks enough that we should switch to non-strict mode. it will still record the number of findbugs warnings present before the patch, but it'll vote \"0\" rather than calling attention to things via a -1. ",
        "label": 402
    },
    {
        "text": "npes in various places  hregion get  hrs close  ttr on irc reported that he was unable to get/scan sometimes, was getting npes. the root cause is a delayed init of the regionscanner.storeheap means it can be null, not all accessors of it (specifically in close()) checked for that. ",
        "label": 547
    },
    {
        "text": "if rs looses lease  we used to restart by default  reinstitute  ",
        "label": 340
    },
    {
        "text": "remove log  error snapshot snapshotfilecache  snapshot directory doesn't exist   when i start up a trunk local cluster from some 0.94 data, i got a bunch of: 13/02/27 15:57:45 error snapshot.snapshotfilecache: snapshot directory: file:/tmp/hbase-enis/hbase/.snapshot doesn't exist 13/02/27 15:57:45 error snapshot.snapshotfilecache: snapshot directory: file:/tmp/hbase-enis/hbase/.snapshot doesn't exist 13/02/27 15:57:45 error snapshot.snapshotfilecache: snapshot directory: file:/tmp/hbase-enis/hbase/.snapshot doesn't exist ",
        "label": 155
    },
    {
        "text": "apply hadoop to hbase mapfile  the patch in hadoop-5369 reduces the memory overhead of mapfile indexes \u2013 especially when block compression is used. it should be applied to the hbase copy of mapfile. ",
        "label": 66
    },
    {
        "text": "convert security related shell commands to use pb based accesscontrolservice  the security-related hbase shell commands (grant, revoke, user_permission) are still using the old coprocessorprotocol-based accesscontrollerprotocol endpoint for dynamic rpc. these need to be converted to use the protocol buffer based accesscontrolservice interface added in hbase-5448. ",
        "label": 242
    },
    {
        "text": "rc1 can not build its hadoop profile  the hadoop .23 version needs to be bumped to 0.23.1-snapshot ",
        "label": 382
    },
    {
        "text": "some files in hbase examples module miss license header  trunk build 3530 got to building hbase-examples module but failed: [info] hbase - examples .................................. failure [3.222s] [info] ------------------------------------------------------------------------ [info] build failure [info] ------------------------------------------------------------------------ [info] total time: 29:21.569s [info] finished at: sun nov 11 15:17:35 utc 2012 [info] final memory: 68m/642m [info] ------------------------------------------------------------------------ [error] failed to execute goal org.apache.rat:apache-rat-plugin:0.8:check (default) on project hbase-examples: too many unapproved licenses: 20 -> [help 1] looks like license headers are missing in some of the files in hbase-examples module ",
        "label": 155
    },
    {
        "text": "hbase regionserver global memstore lowerlimit is too low  the default value of hbase.regionserver.global.memstore.lowerlimit of 25% is very wrong and in almost all cases was problematic (i've seen this in at least 3 occurrences). the cost of flushing a memstore is fairly high and when the global size reaches 40% then all inserts are blocked. this means that with a heap of 1gb you could be flushing for 10-20 seconds or worse. i suggest a default setting of 38% or even 40% so that only a region or two will be flushed (the biggest ones) for maximum availability. ",
        "label": 314
    },
    {
        "text": "stackoverflowerror in reverse scan  a stack overflow may occur when a reverse scan is done. to reproduce (on a mac), use the following steps: download the phoenix 4.5.0 rc here: https://dist.apache.org/repos/dist/dev/phoenix/phoenix-4.5.0-hbase-1.1-rc0/bin/ copy the phoenix-4.5.0-hbase-1.1-server.jar into the hbase lib directory (removing any earlier phoenix version if there was one installed) stop and restart hbase from the bin directory of the phoenix binary distribution, start sqlline like this: ./sqlline.py localhost create a new table and populate it like this: create table desctest (k varchar primary key desc); upsert into desctest values ('a'); upsert into desctest values ('ab'); upsert into desctest values ('b'); note that the following query works fine at this point: select * from desctest order by k; +------------------------------------------+ |                    k                     | +------------------------------------------+ | a                                        | | ab                                       | | b                                        | +------------------------------------------+ stop and start hbase rerun the above query again and you'll get a stackoverflowerror at storefilescanner.seektopreviousrow() select * from desctest order by k; java.lang.runtimeexception: org.apache.phoenix.exception.phoenixioexception: org.apache.phoenix.exception.phoenixioexception: org.apache.hadoop.hbase.donotretryioexception: desctest,,1437847235264.a74d70e6a8b36e24d1ea1a70edb0cdf7.: null at org.apache.phoenix.util.serverutil.createioexception(serverutil.java:84) at org.apache.phoenix.util.serverutil.throwioexception(serverutil.java:52) at org.apache.phoenix.coprocessor.basescannerregionobserver$2.nextraw(basescannerregionobserver.java:352) at org.apache.phoenix.coprocessor.delegateregionscanner.nextraw(delegateregionscanner.java:77) at org.apache.hadoop.hbase.regionserver.rsrpcservices.scan(rsrpcservices.java:2393) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32205) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2112) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:101) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:107) at java.lang.thread.run(thread.java:745) caused by: java.lang.stackoverflowerror at org.apache.hadoop.hbase.io.hfile.checksumutil.numchunks(checksumutil.java:201) at org.apache.hadoop.hbase.io.hfile.checksumutil.numbytes(checksumutil.java:189) at org.apache.hadoop.hbase.io.hfile.hfileblock.totalchecksumbytes(hfileblock.java:1826) at org.apache.hadoop.hbase.io.hfile.hfileblock.getbufferreadonly(hfileblock.java:356) at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$encodedscannerv2.getencodedbuffer(hfilereaderv2.java:1211) at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$encodedscannerv2.getfirstkeyinblock(hfilereaderv2.java:1307) at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$abstractscannerv2.seekbefore(hfilereaderv2.java:657) at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$abstractscannerv2.seekbefore(hfilereaderv2.java:646) at org.apache.hadoop.hbase.regionserver.storefilescanner.seektopreviousrow(storefilescanner.java:425) at org.apache.hadoop.hbase.regionserver.storefilescanner.seektopreviousrow(storefilescanner.java:449) at org.apache.hadoop.hbase.regionserver.storefilescanner.seektopreviousrow(storefilescanner.java:449) at org.apache.hadoop.hbase.regionserver.storefilescanner.seektopreviousrow(storefilescanner.java:449) i've attempted to reproduce this in a standalone hbase unit test, but have not been able to (but i'll attach my attempt which mimics what phoenix is doing). ",
        "label": 544
    },
    {
        "text": " meta  may not come back if regionserver crashes  if a regionserver crashes under .meta., then the cluster might not recover properly. there are some bugs that keep the .meta. from coming back. ",
        "label": 342
    },
    {
        "text": "hbase improperly breaks public api hregioninfo gettabledesc  after hbase-451, hregioninfo#gettabledesc has been modified to always return null. one immediate effect is broken unit tests. that aside, it is not in the spirit of deprecation to actually break the method until after the deprecation cycle, it's a bug. ",
        "label": 314
    },
    {
        "text": "intra row scanning  part deux   dave revell was asking on irc today if there's a way to scan ranges of qualifiers within a row. that is, to be able to specify a start qualifier and an end qualifier so that the get or scan seeks directly to the first qualifier and stops at some point which can be predeterminate by a qualifier or simply a batch configuration (already exists). this is particularly useful for large rows with time-based qualifiers. dave also mentioned that another popular database has such a feature that they call \"column slices\". ",
        "label": 126
    },
    {
        "text": "categorize tests in hbase prefix tree module  jeff bowles discovered that tests in hbase-prefix-tree module, e.g. testtimestampencoder, don't have test category ",
        "label": 371
    },
    {
        "text": "tablerecordreader may skip first row of region  after the following scenario, the first record of region is skipped, without being sent to mapper: the reader is initialized with tablerecordreader.init() then nextkeyvalue is called, causing call to scanner.next() - here scannertimeoutexception occurs the scanner is restarted by call to restart() and then two calls to scanner.next() occur, causing we have lost the first row ",
        "label": 326
    },
    {
        "text": "user getcurrent  can fail to initialize the current user  when testing with miniclusters that shutdown and are restarted, sometimes a call to user.getcurrent().getname() npes when attempting to restart hbase. oddly this happens consistently on particular branches and not on others. i don't know or understand why this happens but it has something to do with the getcurrentugi call in o.a.h.h.security.user.hadoopuser sometimes returning null and sometimes returning data.    private hadoopuser() {       try {         ugi = (usergroupinformation) callstatic(\"getcurrentugi\");         if (ugi == null) {           log.warn(\"although successfully retrieved usergroupinformation\"                + \"  it was null!\");         }       } catch (runtimeexception re) { this patch essentially is a workaround \u2013 it propagates the null so that clients can check and avoid the npe. ",
        "label": 180
    },
    {
        "text": "hbase meta's regions can be replicated  as mentioned elsewhere, we can leverage hbase-10070 features to create replicas for the meta tables regions so that:   1. meta hotspotting can be circumvented   2. meta becomes highly available for reading ",
        "label": 139
    },
    {
        "text": "failed to upgrade to  as the global permission which storaged in zk is not right  before 2.2, the global permission which storaged in zk is not right. it was storaged as a table permission. after hbase-21255, 2.2+ will read it as a global permission and will throw a cast error.   caused by: java.lang.classcastexception: org.apache.hadoop.hbase.security.access.tablepermission cannot be cast to org.apache.hadoop.hbase.security.access.globalpermission at org.apache.hadoop.hbase.security.access.authmanager.updateglobalcache(authmanager.java:171) at org.apache.hadoop.hbase.security.access.authmanager.refreshtablecachefromwritable(authmanager.java:129) at org.apache.hadoop.hbase.security.access.zkpermissionwatcher.refreshauthmanager(zkpermissionwatcher.java:252) at org.apache.hadoop.hbase.security.access.zkpermissionwatcher.refreshnodes(zkpermissionwatcher.java:235)   before 2.2, the acl update logic is complicated. client sent the grant/revoke rpc call to accesscontrol coprocessor directly. and only the rs which has acl region will put a acl record to hbase:acl table. and the accesscontrol override postput/postdelete method, too. it will update zk when postput/postdelete found this is a put/delete for acl region...and there is a todo \"global entry should be handled differently\". the global entry was handled as a table permission, too.   private static pair<string, tablepermission> parsepermissionrecord(     byte[] entryname, cell kv) {   // return x given a set of permissions encoded in the permissionrecord kv.   byte[] family = cellutil.clonefamily(kv);   if (!bytes.equals(family, acl_list_family)) {     return null;   }   byte[] key = cellutil.clonequalifier(kv);   byte[] value = cellutil.clonevalue(kv);   if (log.isdebugenabled()) {     log.debug(\"read acl: kv [\"+         bytes.tostringbinary(key)+\": \"+         bytes.tostringbinary(value)+\"]\");   }   // check for a column family appended to the key   // todo: avoid the string conversion to make this more efficient   string username = bytes.tostring(key);   //handle namespace entry   if(isnamespaceentry(entryname)) {     return new pair<>(username, new tablepermission(bytes.tostring(fromnamespaceentry(entryname)), value));   }   //handle table and global entry   //todo global entry should be handled differently   int idx = username.indexof(acl_key_delimiter);   byte[] permfamily = null;   byte[] permqualifier = null;   if (idx > 0 && idx < username.length()-1) {     string remainder = username.substring(idx+1);     username = username.substring(0, idx);     idx = remainder.indexof(acl_key_delimiter);     if (idx > 0 && idx < remainder.length()-1) {       permfamily = bytes.tobytes(remainder.substring(0, idx));       permqualifier = bytes.tobytes(remainder.substring(idx+1));     } else {       permfamily = bytes.tobytes(remainder);     }   }   return new pair<>(username, new tablepermission(tablename.valueof(entryname), permfamily, permqualifier, value)); }     ",
        "label": 187
    },
    {
        "text": "master clean start up and partially enabled tables make region assignment inconsistent   if we have a table in partially enabled state (enabling) then on hmaster restart we treat it as a clean cluster start up and do a bulk assign. currently in 0.94 bulk assign will not handle already_opened scenarios and it leads to region assignment problems. analysing more on this we found that we have better way to handle these scenarios. if (false == checkifregionbelongstodisabled(regioninfo)             && false == checkifregionsbelongstoenabling(regioninfo)) {           synchronized (this.regions) {             regions.put(regioninfo, regionlocation);             addtoservers(regionlocation, regioninfo);           } we dont add to regions map so that enable table handler can handle it. but as nothing is added to regions map we think it as a clean cluster start up.  will come up with a patch tomorrow. ",
        "label": 543
    },
    {
        "text": "bit encoding of regionnames waaaaaaayyyyy too susceptible to hash clashes  kannan tripped over two regionnames that hashed the same: here is code demo'ing that his two names hash the same: package org; import org.apache.hadoop.hbase.util.bytes; import org.apache.hadoop.hbase.util.jenkinshash; public class testing {   public static void main(final string [] args) {     system.out.println(encoderegionname(bytes.tobytes(\"test1,6838000000,1273541236167\")));     system.out.println(encoderegionname(bytes.tobytes(\"test1,0520100000,1273541610201\")));   }   /**    * @param regionname    * @return the encodedname    */   public static int encoderegionname(final byte [] regionname) {     return math.abs(jenkinshash.getinstance().hash(regionname, regionname.length, 0));   } } need new encoding mechanism. will need to migrate old regions to new schema. ",
        "label": 263
    },
    {
        "text": "ui listing regions should be sorted by address and show additional region state  currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (was seen by billy yesterday and psaab today). ui should list out all of its attributes. also sort region listings by server address so easier finding servers. ",
        "label": 229
    },
    {
        "text": "master hangs forever if recovermeta send assign meta region request to target server fail  2017-11-10 19:26:56,019 info [procexecwrkr-1] procedure.recovermetaprocedure: pid=138, state=runnable:recover_meta_assign_regions; recovermetaprocedure failedmetaserver=null, splitwal=true; retaining meta assignment to server=hadoop-slave1.hadoop,16020,1510341981454  2017-11-10 19:26:56,029 info [procexecwrkr-1] procedure2.procedureexecutor: initialized subprocedures=[ {pid=139, ppid=138, state=runnable:region_transition_queue; assignprocedure table=hbase:meta, region=1588230740, target=hadoop-slave1.hadoop,16020,1510341981454} ]  2017-11-10 19:26:56,067 info [procexecwrkr-2] procedure.masterprocedurescheduler: pid=139, ppid=138, state=runnable:region_transition_queue; assignprocedure table=hbase:meta, region=1588230740, target=hadoop-slave1.hadoop,16020,1510341981454 hbase:meta hbase:meta,,1.1588230740  2017-11-10 19:26:56,071 info [procexecwrkr-2] assignment.assignprocedure: start pid=139, ppid=138, state=runnable:region_transition_queue; assignprocedure table=hbase:meta, region=1588230740, target=hadoop-slave1.hadoop,16020,1510341981454; rit=offline, location=hadoop-slave1.hadoop,16020,1510341981454; forcenewplan=false, retain=false  2017-11-10 19:26:56,224 info [procexecwrkr-4] zookeeper.metatablelocator: setting hbase:meta (replicaid=0) location in zookeeper as hadoop-slave2.hadoop,16020,1510341988652  2017-11-10 19:26:56,230 info [procexecwrkr-4] assignment.regiontransitionprocedure: dispatch pid=139, ppid=138, state=runnable:region_transition_dispatch; assignprocedure table=hbase:meta, region=1588230740, target=hadoop-slave1.hadoop,16020,1510341981454; rit=opening, location=hadoop-slave2.hadoop,16020,1510341988652  2017-11-10 19:26:56,382 info [proceduredispatchertimeoutthread] procedure.rsproceduredispatcher: using procedure batch rpc execution for servername=hadoop-slave2.hadoop,16020,1510341988652 version=2097152  2017-11-10 19:26:57,542 info [main-eventthread] zookeeper.regionservertracker: regionserver ephemeral node deleted, processing expiration [hadoop-slave2.hadoop,16020,1510341988652]  2017-11-10 19:26:57,543 info [main-eventthread] master.servermanager: master doesn't enable servershutdownhandler during initialization, delay expiring server hadoop-slave2.hadoop,16020,1510341988652  2017-11-10 19:26:58,875 info [rpcserver.default.fpbq.fifo.handler=29,queue=2,port=16000] master.servermanager: registering server=hadoop-slave1.hadoop,16020,1510342016106  2017-11-10 19:27:05,832 info [rpcserver.default.fpbq.fifo.handler=29,queue=2,port=16000] master.servermanager: registering server=hadoop-slave2.hadoop,16020,1510342023184  2017-11-10 19:27:05,832 info [rpcserver.default.fpbq.fifo.handler=29,queue=2,port=16000] master.servermanager: triggering server recovery; existingserver hadoop-slave2.hadoop,16020,1510341988652 looks stale, new server:hadoop-slave2.hadoop,16020,1510342023184  2017-11-10 19:27:05,832 info [rpcserver.default.fpbq.fifo.handler=29,queue=2,port=16000] master.servermanager: master doesn't enable servershutdownhandler during initialization, delay expiring server hadoop-slave2.hadoop,16020,1510341988652  2017-11-10 19:27:49,815 info [rpcserver.default.fpbq.fifo.handler=29,queue=2,port=16000] client.rpcretryingcallerimpl: tarted=38594 ms ago, cancelled=false, msg=org.apache.hadoop.hbase.notservingregionexception: hbase:meta,,1 is not online on hadoop-slave2.hadoop,16020,1510342023184  at org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname(hregionserver.java:3290)  at org.apache.hadoop.hbase.regionserver.rsrpcservices.getregion(rsrpcservices.java:1370)  at org.apache.hadoop.hbase.regionserver.rsrpcservices.get(rsrpcservices.java:2401)  at org.apache.hadoop.hbase.shaded.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:41544)  at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:406)  at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:133)  at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:278)  at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:258)  row 'hbase:namespace' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=hadoop-slave2.hadoop,16020,1510341988652, seqnum=0 ",
        "label": 499
    },
    {
        "text": "rewrite rsgroup related uts with the new methods introduced in hbase  ",
        "label": 149
    },
    {
        "text": "deletesnapshot  call may be skipped in testflushsnapshotfromclient tests  at the end of each test, we have this call:  admin.deletesnapshot(snapshotname); however it is not placed in finally block. when assertion in earlier part of the test fails (see [1]), snapshot cleanup would be skipped, leading to snapshottestingutils.assertnosnapshots() failing for subsequent tests. [1]: https://builds.apache.org/job/hbase-0.95-on-hadoop2/197/testreport/org.apache.hadoop.hbase.snapshot/testflushsnapshotfromclient/testflushcreatelistdestroy/ ",
        "label": 441
    },
    {
        "text": "start mini cluster once before class for testfifocompactionpolicy  ",
        "label": 149
    },
    {
        "text": "add pre post roll to walobserver  currently the walobserver has only a pre/post write. it will be useful to have a pre/post roll too. ",
        "label": 309
    },
    {
        "text": "tool to test binary compatibility  stack and i were discussing of ways to make binary compatibility easier to test than doing it completely by hand. one idea would be to have a tool that uses reflection to generate code that calls all the public methods from a list of classes. you would then compile this code against the current version you are on, then try it out with different hbase jars without recompiling. ",
        "label": 141
    },
    {
        "text": "scanner returns values from before startrow  ben maurer reports our returning rows from before startrow \u2013 lots of concurrent scanners run out of mapreduce jobs that are updating rows as they go. ",
        "label": 66
    },
    {
        "text": "forward port  hbase new rpc metric  number of active handler  the metrics implementation has changed a lot in 0.96.  forward port hbase-10212 to 0.96 and later. ",
        "label": 290
    },
    {
        "text": "hbase test artifacts are missing from maven central  could someone with enough karma, please, publish the test artifacts for 0.92.0? ",
        "label": 314
    },
    {
        "text": "is enabled from shell returns differently from pre  and post  hbase  if i launch an hbase shell that uses hbase and zookeeper without hbase-5155, against hbase servers with hbase-5155, then is_enabled for a table always returns false even if the table is considered enabled by the servers from the logs. if i then do the same thing but with an hbase shell and zookeeper with hbase-5155, then is_enabled returns as expected. if i launch an hbase shell that uses hbase and zookeeper without hbase-5155, against hbase servers also without hbase-5155, then is_enabled works as you'd expect. but if i then do the same thing but with an hbase shell and zookeeper with hbase-5155, then is_enabled returns false even though the table is considered enabled by the servers from the logs. additionally, if i then try to enable the table from the hbase-5155-containing shell, it hangs because the zookeeper code waits for the znode to be updated with \"enabled\" in the data field, but what actually happens is that the znode gets deleted since the servers are running without hbase-5155. i think the culprit is that the indication of how a table is considered enabled inside zookeeper has changed with hbase-5155. before hbase-5155, a table was considered enabled if the znode for it did not exist. after hbase-5155, a table is considered enabled if the znode for it exists and has \"enabled\" in its data. i think the current code is incompatible when running clients and servers where one side has hbase-5155 and the other side does not. ",
        "label": 131
    },
    {
        "text": "tablemapreduceutil doesn't include all dependency jars in new modular build  from post to hbase-dev: i'm seeing classnotfoundexceptions when running importtsv against trunk. presumably the old instructions for classpath assembly need an update. i guess i could create a hadoop jar that ships the classpath entries in lib, but imho that's not a viable solution for users who want to run one of the jobs we distribute. am i missing something? for you reference: $ hadoop_classpath=$(./bin/hbase classpath) hadoop jar ./hbase-server/target/hbase-server-0.95-snapshot.jar importtsv -dimporttsv.columns=hbase_row_key,d:c1,d:c2 -dimporttsv.bulk.output=hdfs:///tmp/hfiles simple hdfs:///tmp/simple.tsv ... error: java.lang.classnotfoundexception: org.apache.hadoop.hbase.protobuf.generated.clientprotos$mutate$mutatetype at java.net.urlclassloader$1.run(urlclassloader.java:202) at java.security.accesscontroller.doprivileged(native method) at java.net.urlclassloader.findclass(urlclassloader.java:190) at java.lang.classloader.loadclass(classloader.java:306) at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) at java.lang.classloader.loadclass(classloader.java:247) at org.apache.hadoop.hbase.mapreduce.mutationserialization$mutationserializer.serialize(mutationserialization.java:87) at org.apache.hadoop.hbase.mapreduce.mutationserialization$mutationserializer.serialize(mutationserialization.java:70) at org.apache.hadoop.mapred.maptask$mapoutputbuffer.collect(maptask.java:1069) at org.apache.hadoop.mapred.maptask$newoutputcollector.write(maptask.java:691) at org.apache.hadoop.mapreduce.taskinputoutputcontext.write(taskinputoutputcontext.java:80) at org.apache.hadoop.hbase.mapreduce.tsvimportermapper.map(tsvimportermapper.java:151) at org.apache.hadoop.hbase.mapreduce.tsvimportermapper.map(tsvimportermapper.java:37) at org.apache.hadoop.mapreduce.mapper.run(mapper.java:144) at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:764) at org.apache.hadoop.mapred.maptask.run(maptask.java:370) at org.apache.hadoop.mapred.child$4.run(child.java:255) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:396) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1121) at org.apache.hadoop.mapred.child.main(child.java:249) ",
        "label": 339
    },
    {
        "text": " performance  distributed splitting of regionserver commit logs  hbase-1008 has some improvements to our log splitting on regionserver crash; but it needs to run even faster. (below is from hbase-1008) in bigtable paper, the split is distributed. if we're going to have 1000 logs, we need to distribute or at least multithread the splitting. 1. as is, regions starting up expect to find one reconstruction log only. need to make it so pick up a bunch of edit logs and it should be fine that logs are elsewhere in hdfs in an output directory written by all split participants whether multithreaded or a mapreduce-like distributed process (lets write our distributed sort first as a mr so we learn whats involved; distributed sort, as much as possible should use mr framework pieces). on startup, regions go to this directory and pick up the files written by split participants deleting and clearing the dir when all have been read in. making it so can take multiple logs for input, can also make the split process more robust rather than current tenuous process which loses all edits if it doesn't make it to the end without error.  2. each column family rereads the reconstruction log to find its edits. need to fix that. split can sort the edits by column family so store only reads its edits. ",
        "label": 356
    },
    {
        "text": " hbase  make batchupdate public in the api  today, when you want to interact with a row in hbase, you start an update, make changes, and then commit the lock. this is fine for very simple applications. however, when you try to do things like support table operations as part of a mapreduce job, it becomes more difficult to support. i propose that we create a new class, rowmutation (a la the bigtable paper), which encapsulates a group of actions on a row, and make this available to api consumers. it might look something like: rowmutation r = table.getmutation(row_key); r.settimestamp(1111); r.put(new text(\"colfam1:name\", value)); r.delete(new text(\"colfam2:deleted\")); table.commit(r); this syntax would supercede the existing startupdate/commit format, which could be deprecated and mapped to a rowmutation behind the scenes. ",
        "label": 86
    },
    {
        "text": "add histogram representative of row key distribution inside a region   using histogram information, users can parallelize the scan workload into equal sized scans based on the estimated size from the histogram information. this will help in enabling systems which are trying to perform queries on top of hbase to do cost based optimization while scanning. the idea is to keep this histogram information in the hfile in the trailer and populate this on compaction and flush. the hregioninterface can expose an api to return the histogram information of a region, which can be generated by merging histograms of all the hfiles. implementing the histogram on the basis of   http://jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf  http://dl.acm.org/citation.cfm?id=1951376  and numerichistogram from hive. ",
        "label": 154
    },
    {
        "text": "testhlog testsplit hangs  this a blocker had it blocks and times out hudson. it seems that when we upgraded to latest of 0.20-append we got into a new situation where we can't recover a file that's empty if the original writer is still alive: 2010-06-24 10:41:20,645 debug [main] wal.hlog(1281): splitting hlog 4 of 4: hdfs://localhost:64456/hbase/testsplit/.logs/hlog.1277401279534, length=0 2010-06-24 10:41:20,645 info  [main] util.fsutils(612): recovering filehdfs://localhost:64456/hbase/testsplit/.logs/hlog.1277401279534 2010-06-24 10:41:20,647 warn  [ipc server handler 5 on 64456] namenode.fsnamesystem(1156): dir* namesystem.startfile:  failed to create file /hbase/testsplit/.logs/hlog.1277401279534 for dfsclient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file ... 2010-06-24 10:42:24,919 warn  [ipc server handler 0 on 64456] namenode.fsnamesystem(1156): dir* namesystem.startfile:  failed to create file /hbase/testsplit/.logs/hlog.1277401279534 for dfsclient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file. 2010-06-24 10:42:24,919 warn  [main] util.fsutils(631): waited 64274ms for lease recovery on  hdfs://localhost:64456/hbase/testsplit/.logs/hlog.1277401279534:org.apache.hadoop.hdfs.protocol.alreadybeingcreatedexception:  failed to create file /hbase/testsplit/.logs/hlog.1277401279534 for dfsclient_-1981892617 on client 127.0.0.1 because current leaseholder is trying to recreate file.         at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfileinternal(fsnamesystem.java:1058)         at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.appendfile(fsnamesystem.java:1171)         at org.apache.hadoop.hdfs.server.namenode.namenode.append(namenode.java:396) we could just not roll the latest log and it would probably fix the issue, but i wonder if we could change something in hdfs instead. todd? ",
        "label": 341
    },
    {
        "text": "if a server's lease times out or the server dies  all regions will get reassigned even split or offline ones   if a server's lease times out or a server dies (essentially the same thing), when the master tries to find the regions it was serving, it does not check to see if the region has been offlined or split. in processservershutdown.scanmetaregion, the code:         } else {           // get region reassigned           regions.add(info);         } should be:         } else {           if (!info.isoffline() && !info.issplit()) {             // get region reassigned             regions.add(info);           }         } ",
        "label": 229
    },
    {
        "text": "npe in master assignmentmanager if all region servers shut down  10/10/18 16:26:44 info catalog.catalogtracker: acer,60020,1287443908850 carrying .meta.; unsetting .meta. location  10/10/18 16:26:44 info catalog.catalogtracker: current cached meta location is not valid, resetting  10/10/18 16:26:44 info handler.servershutdownhandler: splitting logs for acer,60020,1287443908850  10/10/18 16:26:44 info zookeeper.zkutil: hconnection-0x12bc1a2f0a60001 set watcher on existing znode /hbase/root-region-server  10/10/18 16:26:44 info catalog.rootlocationeditor: unsetting root region location in zookeeper  10/10/18 16:26:44 debug zookeeper.zkassign: master:60000-0x12bc1a2f0a60000 creating (or updating) unassigned node for 70236052 with offline state  10/10/18 16:26:44 warn master.loadbalancer: wanted to do random assignment but no servers to assign to  10/10/18 16:26:44 error executor.eventhandler: caught throwable while processing event m_server_shutdown  java.lang.nullpointerexception  at org.apache.hadoop.hbase.master.loadbalancer$regionplan.tostring(loadbalancer.java:595)  at java.lang.string.valueof(string.java:2826)  at java.lang.stringbuilder.append(stringbuilder.java:115)  at org.apache.hadoop.hbase.master.assignmentmanager.getregionplan(assignmentmanager.java:803)  at org.apache.hadoop.hbase.master.assignmentmanager.getregionplan(assignmentmanager.java:777)  at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:720)  at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:640)  at org.apache.hadoop.hbase.master.assignmentmanager.assignroot(assignmentmanager.java:922)  at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:97)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:150)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619) ",
        "label": 314
    },
    {
        "text": "regionserver not shutting down upon keeperexception in open region  we ran into a situation where due to a kerberos configuration problem one of our region server could not connect to zk when opening a region. instead of shutting down it continue to try to reconnect. eventually the master would assign the region to another region server. each time that region server was assigned a region it would sit there for 5 mins with the region offline. it would have been better if the region server had shut itself down. this is in the logs: 2013-08-16 17:31:35,999 warn org.apache.hadoop.hbase.zookeeper.zkutil: hconnection-0x2407b842ff2012d-0x2407b842ff2012d-0x2407b842ff2012d unable to set watcher on znode (/hbase/hbaseid)  org.apache.zookeeper.keeperexception$authfailedexception: keepererrorcode = authfailed for /hbase/hbaseid  at org.apache.zookeeper.keeperexception.create(keeperexception.java:123)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)  at org.apache.zookeeper.zookeeper.exists(zookeeper.java:1041)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:172)  at org.apache.hadoop.hbase.zookeeper.zkutil.checkexists(zkutil.java:450)  at org.apache.hadoop.hbase.zookeeper.clusterid.readclusteridznode(clusterid.java:61)  at org.apache.hadoop.hbase.zookeeper.clusterid.getid(clusterid.java:50)  at org.apache.hadoop.hbase.zookeeper.clusterid.hasid(clusterid.java:44)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.ensurezookeepertrackers(hconnectionmanager.java:616)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:882)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:857)  at org.apache.hadoop.hbase.client.htable.finishsetup(htable.java:233)  at org.apache.hadoop.hbase.client.htable.<init>(htable.java:173)  at org.apache.hadoop.hbase.catalog.metareader.gethtable(metareader.java:201)  at org.apache.hadoop.hbase.catalog.metareader.getmetahtable(metareader.java:227)  at org.apache.hadoop.hbase.catalog.metareader.getcataloghtable(metareader.java:214)  at org.apache.hadoop.hbase.catalog.metaeditor.puttocatalogtable(metaeditor.java:91)  at org.apache.hadoop.hbase.catalog.metaeditor.updatelocation(metaeditor.java:296)  at org.apache.hadoop.hbase.catalog.metaeditor.updateregionlocation(metaeditor.java:276)  at org.apache.hadoop.hbase.regionserver.hregionserver.postopendeploytasks(hregionserver.java:1828)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler$postopendeploytasksthread.run(openregionhandler.java:240) i think the rs should shut itself down instead. ",
        "label": 286
    },
    {
        "text": "update our jruby to  we reverted our jruby jar to 1.0.3 because of licensing issues in 1.5.x. the jruby crew fixed the licensing issues in 1.6.0rc2 (which is released but not yet in a mvn repo). this issue is about updating our ruby. the old ruby 'works' but is bad in many ways; bad parse errors, missing language support that made us redo a bunch of our script to not use import (and removal of testshell, our unit test that ran jruby tests). this issue is about updating our jruby and in particular, reenabling the testshell unit test removed by hbase-3374. ",
        "label": 314
    },
    {
        "text": "replication tablecfs should be a pb object rather than a string  we concatenate the list of tables and column families in format \"table1:cf1,cf2;table2:cfa,cfb\" in zookeeper for table-cf to replication peer mapping. this results in ugly parsing code. we should do this a pb object. ",
        "label": 198
    },
    {
        "text": "bufferunderflowexception after last cell fetched from an hfile block served from l2 offheap cache  while running the newer patches on our production system, i saw this error come couple of times ipc.rpcserver: unexpected throwable object  2016-01-01 16:42:56,090 error [b.defaultrpcserver.handler=20,queue=20,port=60020] ipc.rpcserver: unexpected throwable object  java.nio.bufferunderflowexception at java.nio.buffer.nextgetindex(buffer.java:500) at java.nio.directbytebuffer.get(directbytebuffer.java:249) at org.apache.hadoop.hbase.nio.multibytebuff.get(multibytebuff.java:494) at org.apache.hadoop.hbase.io.encoding.fastdiffdeltaencoder$1.decode(fastdiffdeltaencoder.java:402)  at org.apache.hadoop.hbase.io.encoding.fastdiffdeltaencoder$1.decodenext(fastdiffdeltaencoder.java:517)  at org.apache.hadoop.hbase.io.encoding.buffereddatablockencoder$bufferedencodedseeker.next(buffereddatablockencoder.java:815) at org.apache.hadoop.hbase.regionserver.storefilescanner.next(storefilescanner.java:138) looking at the get code if (this.curitem.remaining() == 0) {       if (items.length - 1 == this.curitemindex) {         // means cur item is the last one and we wont be able to read a long. throw exception         throw new bufferunderflowexception();       }       this.curitemindex++;       this.curitem = this.items[this.curitemindex];     } return this.curitem.get(); can the new currentitem have zero elements (position == limit), does it make sense to change the if to while ? while (this.curitem.remaining() == 0). this logic is repeated may make sense abstract to a new function if we plan to change to if to while ",
        "label": 46
    },
    {
        "text": "backport 'hbase change default hadoop two version to x and remove the x hadoop checks' to branch  ",
        "label": 149
    },
    {
        "text": "bucketcache all the time   one way to realize the parent issue is to just enable bucket cache all the time; i.e. always have offheap enabled. would have to do some work to make it drop-dead simple on initial setup (i think it doable). so, upside would be the offheap upsides (less gc, less likely to go away and never come back because of full gc when heap is large, etc.). downside is higher latency. in nick's blockcache 101 there is little to no difference between onheap and offheap. in a basic compare doing scans and gets \u2013 details to follow \u2013 i have bucketcache deploy about 20% less ops than lrubc when all incache and maybe 10% less ops when falling out of cache. i can't tell difference in means and 95th and 99th are roughly same (more stable with bucketcache). gc profile is much better with bucketcache \u2013 way less. bucketcache uses about 7% more user cpu. more detail on comparison to follow. i think the numbers disagree enough we should probably do the [~lhofhansl] suggestion, that we allow you to have a table sit in lrubc, something the current bucket cache layout does not do. ",
        "label": 314
    },
    {
        "text": "regionserver should refuse to be assigned a region that use lzo when lzo isn't available  if a regionserver is assigned a region that uses lzo but the required libraries aren't installed on that regionserver, the server will fail unexpectedly after throwing a java.lang.classnotfoundexception: com.hadoop.compression.lzo.lzocodec 2010-05-04 16:57:27,258 fatal org.apache.hadoop.hbase.regionserver.memstoreflusher: replay of hlog required. forcing server shutdown org.apache.hadoop.hbase.droppedsnapshotexception: region: tsdb,,1273011287339         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:994)         at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:887)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushregion(memstoreflusher.java:255)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.run(memstoreflusher.java:142) caused by: java.lang.runtimeexception: java.lang.classnotfoundexception: com.hadoop.compression.lzo.lzocodec         at org.apache.hadoop.hbase.io.hfile.compression$algorithm$1.getcodec(compression.java:91)         at org.apache.hadoop.hbase.io.hfile.compression$algorithm.getcompressor(compression.java:196)         at org.apache.hadoop.hbase.io.hfile.hfile$writer.getcompressingstream(hfile.java:388)         at org.apache.hadoop.hbase.io.hfile.hfile$writer.newblock(hfile.java:374)         at org.apache.hadoop.hbase.io.hfile.hfile$writer.checkblockboundary(hfile.java:345)         at org.apache.hadoop.hbase.io.hfile.hfile$writer.append(hfile.java:517)         at org.apache.hadoop.hbase.io.hfile.hfile$writer.append(hfile.java:482)         at org.apache.hadoop.hbase.regionserver.store.internalflushcache(store.java:558)         at org.apache.hadoop.hbase.regionserver.store.flushcache(store.java:522)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:979)         ... 3 more caused by: java.lang.classnotfoundexception: com.hadoop.compression.lzo.lzocodec         at java.net.urlclassloader$1.run(urlclassloader.java:200)         at java.security.accesscontroller.doprivileged(native method)         at java.net.urlclassloader.findclass(urlclassloader.java:188)         at java.lang.classloader.loadclass(classloader.java:315)         at sun.misc.launcher$appclassloader.loadclass(launcher.java:330)         at java.lang.classloader.loadclass(classloader.java:250)         at org.apache.hadoop.hbase.io.hfile.compression$algorithm$1.getcodec(compression.java:87)         ... 12 more ",
        "label": 547
    },
    {
        "text": "make store flush algorithm pluggable  the idea is to make \"storeflusher\" an interface instead of an implementation class, and have the original storeflusher as the default store flush impl. ",
        "label": 406
    },
    {
        "text": "timeout in indexrecordwriter  a mapreduce job to generate lucene indexes from hbase will fail on sufficiently large tables. after the indexing finished, the close() method of indexrecordwriter is called. the writer.optimize() call in this method can take many minutes, forcing most mapreduce tasks to timeout. there is a heartbeatsthread, but it does not seem to send progress updates. a suggested fix may be to add context.progress(); in the heardbeatsthread run() method, after the context.setstatus call. not sure why context.setstatus is not \"good enough\". ",
        "label": 78
    },
    {
        "text": "client needs to reconnect if it expires its zk session  clients use an hconnection down in their guts to connect to the hbase cluster. master-is-running and root-region-location are up in zk. setup of a new hconnection sets up a connection to zookeeper. if the session with zk expires for whatever reason \u2013 in tests they would expire because zk ensemble was restarted across tests or we might expire because of a long gc, well, it'll be frustrating to users if we do not just try and resetup the zk connection. ",
        "label": 229
    },
    {
        "text": "random zookeeper port in test can overrun max port         while (true) {         try {           standaloneserverfactory = new nioservercnxnfactory();           standaloneserverfactory.configure(             new inetsocketaddress(tentativeport),             configuration.getint(hconstants.zookeeper_max_client_cnxns,               1000));         } catch (bindexception e) {           log.debug(\"failed binding zk server to client port: \" +               tentativeport);           // this port is already in use, try to use another.           tentativeport++;           continue;         }         break;       } in the case of failure and all the above ports have already been binded, you can extend past the max port. need to check against a max value. ",
        "label": 290
    },
    {
        "text": "replace o a h c interfaceaudience by o a h h c interfaceaudience  some classes still import the hadoop's interfaceaudience. 1. interns 2. metricsinfoimpl 3. restcsrfpreventionfilter 4. abstractfilestatusfilter 5. filestatusfilter ",
        "label": 98
    },
    {
        "text": "a column family can have versions less than zero  user can create/alter a columnfam and set its version(aka maxversions) to a negative or zero value. although there is a checking in hcolumndesciptor#construtor, hbase shell command will invoke the setter(setmaxversions and setminversions) directly, hence by pass the checking. for example: set versions = -1 hbase(main):016:0> create 't5_dn',{name=>'cf1',versions=>-1} 0 row(s) in 1.0420 seconds hbase(main):017:0> put 't5_dn','row1','cf1:q1','row1cf1_v1' 0 row(s) in 0.0700 seconds hbase(main):018:0> scan 't5_dn' row                   column+cell                                               0 row(s) in 0.0090 seconds hbase(main):019:0> describe 't5_dn' description                                          enabled                     't5_dn', {name => 'cf1', replication_scope => '0',  true                       keep_deleted_cells => 'false', compression => 'none                             ', encode_on_disk => 'true', blockcache => 'true',                             min_versions => '0', data_block_encoding => 'none',                              in_memory => 'false', bloomfilter => 'none', ttl =                             > '2147483647', versions => '-1', blocksize => '655                             36'}                                                                           1 row(s) in 0.0410 seconds above example shows versions => '-1', and put/scan doesn't keep the data ",
        "label": 134
    },
    {
        "text": "testcoprocessorscanpolicy is sometimes flaky when run locally  the problem is not seen in jenkins build.   when we run testcoprocessorscanpolicy.testbasecases locally or in our internal jenkins we tend to get random failures. the reason is the 2 puts that we do here is sometimes getting the same timestamp. this is leading to improper scan results as the version check tends to skip one of the row seeing the timestamp to be same. marking this as minor. as we are trying to solve testcase related failures just raising this incase we need to resolve this also. for eg,  both the puts are getting the time time 1347635287360 time 1347635287360 ",
        "label": 286
    },
    {
        "text": "delete multiple columns by regular expression  hi.  i tried to find a way to delete multiple columns that their names match some regular expression, but this functionality is missing (i think).  is it possible to add such functionality ? ",
        "label": 419
    },
    {
        "text": "npe in htable close  with asyncprocess  when running hbase org.apache.hadoop.hbase.test.integrationtestbiglinkedlist --monkey slowdeterministic one task failed with the following stack trace: 2013-09-10 01:56:03,115 warn [htable-pool1-t134] org.apache.hadoop.hbase.client.asyncprocess: attempt #35/35 failed for 3 ops on server02,60020,1378776046122 not resubmitting.region=integrationtestbiglinkedlist,\\xa6\\x10\\x9c\\x85,1378776439065.766ab62aa30fa94c9014f09738698922., hostname=server02,60020,1378776046122, seqnum=16146143 2013-09-10 01:56:03,115 warn [htable-pool1-t119] org.apache.hadoop.hbase.client.asyncprocess: attempt #35/35 failed for 6 ops on server02,60020,1378775896233 not resubmitting.region=integrationtestbiglinkedlist,\\x9d\\x95>\\xdb\\xcb\\xd5\\xe2\\xad\\x7f\\xcb\\x1d\\xbcn~\\xf2u,1378774537592.b2534e273feecba91db43496efa1cd12., hostname=server02,60020,1378775896233, seqnum=14890994 2013-09-10 01:56:03,655 warn [htable-pool1-t119] org.apache.hadoop.hbase.client.asyncprocess: attempt #35/35 failed for 9 ops on server01,60020,1378775896233 not resubmitting.region=integrationtestbiglinkedlist,\\xb8\\x0b.\\x8c\\x12px\\x88>\\x10\\xa4\\x07\\x9fj\\x97\\xd0,1378775167749.7c0f1c17bc5f02e41e02939187304976., hostname=server01,60020,1378775896233, seqnum=15863492 2013-09-10 01:56:03,818 warn [main] org.apache.hadoop.mapred.yarnchild: exception running child : java.lang.nullpointerexception at org.apache.hadoop.hbase.client.asyncprocess.finddestlocation(asyncprocess.java:289) at org.apache.hadoop.hbase.client.asyncprocess.submit(asyncprocess.java:234) at org.apache.hadoop.hbase.client.htable.backgroundflushcommits(htable.java:894) at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1275) at org.apache.hadoop.hbase.client.htable.close(htable.java:1313) at org.apache.hadoop.hbase.test.integrationtestbiglinkedlist$generator$generatormapper.cleanup(integrationtestbiglinkedlist.java:352) at org.apache.hadoop.mapreduce.mapper.run(mapper.java:148) at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:763) at org.apache.hadoop.mapred.maptask.run(maptask.java:339) at org.apache.hadoop.mapred.yarnchild$2.run(yarnchild.java:162) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:396) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1477) at org.apache.hadoop.mapred.yarnchild.main(yarnchild.java:157) seems worth investigating. ",
        "label": 340
    },
    {
        "text": "set hadoop check versions for branch and branch x in pre commit  now it will use the hadoop versions for branch-1. i do not know how to set the fix versions as the code will be committed to master but the branch in trouble is branch-2... ",
        "label": 149
    },
    {
        "text": "document feature  hbase regionserver codecs   a new feature \"hbase.regionserver.codecs\" allows you to poison pill your regionservers if any of the codecs listed do not test operational on server startup. the format of the value is \"lzo,gz\" with strings separated by commas. ",
        "label": 314
    },
    {
        "text": "improve mapfile performance for start and end key  keeping a mapfile's start and end key in cache would save us some seeks, see if it can be done. ",
        "label": 314
    },
    {
        "text": "add test that confirms familyfilters in a filterlist using must pass one operator will return results that match either of the familyfilters and revert as needed to make it pass   we need a test that shows the expected behavior for filter lists that rely on or prior to our filterlist improvements so we have a baseline to show compatibility (and/or document incompatibilities that end up being introduced). specifically (paraphrased from hbase-18368 description): using 2 familyfilters in a filterlist using must_pass_one operator should return results that match either of the familyfilters. ",
        "label": 352
    },
    {
        "text": "hbase hbck  false positive error reported for parent regions that are in offline state in meta after a split  hbase checker will sometimes report something like the following: error: region test1,9922400000,1279934604048.8cb65b1882960f230abb97860dd13c53. is not served by any region server but is listed in meta to be on server null the region in question is a parent region that has been offlined following a split. meta still contains for the above region only because there are daughter regions which still have references to the parent region. once the daughter regions undergo compaction, these references will be gone; and the parent region's entry will be removed from meta. but \"hbck\" should detect entries in this condition and not complain.  test1,9922400000,12799346 column=info:regioninfo, timestamp=1279938675016, value=region => {name =>  04048.8cb65b1882960f230ab  'test1,9922400000,1279934604048.8cb65b1882960f230abb97860dd13c53.', star  b97860dd13c53.            tkey => '9922400000', endkey => '', encoded => 8cb65b1882960f230abb97860d                            d13c53, offline => true, split => true, table => {{name => 'test1', famil                            ies => [{name => 'actions', bloomfilter => 'none', replication_scope => '                            0', versions => '3', compression => 'none', ttl => '2147483647', blocksiz                            e => '65536', in_memory => 'false', blockcache => 'true'}]}}  test1,9922400000,12799346 column=info:server, timestamp=1279938675016, value=  04048.8cb65b1882960f230ab  b97860dd13c53.  test1,9922400000,12799346 column=info:serverstartcode, timestamp=1279938675016, value=  04048.8cb65b1882960f230ab  b97860dd13c53.  test1,9922400000,12799346 column=info:splita, timestamp=1279938675016, value=\\x00\\x0a9961500000\\x00  04048.8cb65b1882960f230ab \\x00\\x00\\x01*\\x02j9'@test1,9922400000,1279938672935.94425ba581acd336d1cbd  b97860dd13c53.            11181ee2785.\\x00\\x0a9922400000\\x00\\x00\\x00\\x05\\x05test1\\x00\\x00\\x00\\x00\\x                            00\\x02\\x00\\x00\\x00\\x07is_root\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x07is_meta                            \\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x01\\x08\\x07actions\\x00\\x00\\x00\\x08\\x00\\                            x00\\x00\\x0bbloomfilter\\x00\\x00\\x00\\x04none\\x00\\x00\\x00\\x11replication_sco                            pe\\x00\\x00\\x00\\x010\\x00\\x00\\x00\\x0bcompression\\x00\\x00\\x00\\x04none\\x00\\x0                            0\\x00\\x08versions\\x00\\x00\\x00\\x013\\x00\\x00\\x00\\x03ttl\\x00\\x00\\x00\\x0a2147                            483647\\x00\\x00\\x00\\x09blocksize\\x00\\x00\\x00\\x0565536\\x00\\x00\\x00\\x09in_me                            mory\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0ablockcache\\x00\\x00\\x00\\x04true\\x                            b7\\x04q\\x18  test1,9922400000,12799346 column=info:splitb, timestamp=1279938675016, value=\\x00\\x00\\x00\\x00\\x00\\x  04048.8cb65b1882960f230ab 01*\\x02j9'@test1,9961500000,1279938672935.bb521c9d8c51fd8133f145dc3c75013  b97860dd13c53.            6.\\x00\\x0a9961500000\\x00\\x00\\x00\\x05\\x05test1\\x00\\x00\\x00\\x00\\x00\\x02\\x00                            \\x00\\x00\\x07is_root\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x07is_meta\\x00\\x00\\x                            00\\x05false\\x00\\x00\\x00\\x01\\x08\\x07actions\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x0                            bbloomfilter\\x00\\x00\\x00\\x04none\\x00\\x00\\x00\\x11replication_scope\\x00\\x00                            \\x00\\x010\\x00\\x00\\x00\\x0bcompression\\x00\\x00\\x00\\x04none\\x00\\x00\\x00\\x08v                            ersions\\x00\\x00\\x00\\x013\\x00\\x00\\x00\\x03ttl\\x00\\x00\\x00\\x0a2147483647\\x00                            \\x00\\x00\\x09blocksize\\x00\\x00\\x00\\x0565536\\x00\\x00\\x00\\x09in_memory\\x00\\x                            00\\x00\\x05false\\x00\\x00\\x00\\x0ablockcache\\x00\\x00\\x00\\x04true\"\\xe8xd ",
        "label": 314
    },
    {
        "text": "newly created table ends up disabled instead of assigned  something that was seen by someone on the channel yesterday and by me this morning, it's possible to create a table that ends up disabled and the 'create' calls times out. the master log looks like: 2010-12-01 19:32:52,350 info org.apache.hadoop.hbase.regionserver.wal.hlog: hlog configuration: blocksize=64 mb, rollsize=60.8 mb, enabled=true, flushlogentries=1, optionallogflushinternal=1000ms 2010-12-01 19:32:52,450 info org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter: using syncfs -- hdfs-200 2010-12-01 19:32:52,451 info org.apache.hadoop.hbase.regionserver.wal.hlog: new hlog /hbase/testtable/2e2099cd5fce907e670ce8596d9b2368/.logs/hlog.1291231972350 2010-12-01 19:32:52,451 info org.apache.hadoop.hbase.regionserver.wal.hlog: using getnumcurrentreplicas--hdfs-826 2010-12-01 19:32:52,452 debug org.apache.hadoop.hbase.regionserver.hregion: instantiated testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. 2010-12-01 19:32:52,645 info org.apache.hadoop.hbase.regionserver.hregion: onlined testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.; next sequenceid=1 2010-12-01 19:32:52,784 info org.apache.hadoop.hbase.catalog.metaeditor: added region testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. to meta 2010-12-01 19:32:52,784 debug org.apache.hadoop.hbase.regionserver.hregion: closing testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.: disabling compactions & flushes 2010-12-01 19:32:52,784 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. 2010-12-01 19:32:52,784 debug org.apache.hadoop.hbase.regionserver.store: closed info 2010-12-01 19:32:52,784 info org.apache.hadoop.hbase.regionserver.hregion: closed testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. 2010-12-01 19:32:52,784 debug org.apache.hadoop.hbase.regionserver.wal.hlog: ipc server handler 4 on 61000.logsyncer interrupted while waiting for sync requests 2010-12-01 19:32:52,784 info org.apache.hadoop.hbase.regionserver.wal.hlog: ipc server handler 4 on 61000.logsyncer exiting 2010-12-01 19:32:52,784 debug org.apache.hadoop.hbase.regionserver.wal.hlog: closing hlog writer in hdfs://sv2borg180:9100/hbase/testtable/2e2099cd5fce907e670ce8596d9b2368/.logs 2010-12-01 19:32:52,908 debug org.apache.hadoop.hbase.regionserver.wal.hlog: moved 1 log files to /hbase/testtable/2e2099cd5fce907e670ce8596d9b2368/.oldlogs 2010-12-01 19:32:52,966 info org.apache.hadoop.hbase.master.assignmentmanager: table testtable disabled; skipping assign of testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. 2010-12-01 19:32:52,967 debug org.apache.hadoop.hbase.master.assignmentmanager: table being disabled so deleting zk node  and removing from regions in transition, skipping assignment of region testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. 2010-12-01 19:32:52,967 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:61000-0x12c84725b4b00b6  deleting existing unassigned node for 2e2099cd5fce907e670ce8596d9b2368 that is in expected state rs_zk_region_closed 2010-12-01 19:32:52,968 debug org.apache.hadoop.hbase.master.assignmentmanager: tried to delete closed node for region =>  {name => 'testtable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.', startkey => '', endkey => '', encoded =>  2e2099cd5fce907e670ce8596d9b2368, table => {{name => 'testtable', families => [{name => 'info', bloomfilter  => 'none', replication_scope => '0', versions => '3', compression => 'none', ttl => '2147483647', blocksize =>  '65536', in_memory => 'false', blockcache => 'true'}]}} but it does not exist so just offlining ",
        "label": 229
    },
    {
        "text": "root not reassigned if only one regionserver left  yannis on the list uncovered an assignment bug: i performed additional testing with some alternate configurations and the problem arises (only) when there is only one regionserver left which has the meta table already assigned to it. in this case the root table does not get assigned to the last regionserver (which holds the meta table). interestingly enough though when there is only one regionserver left that has the root table already assign to it then it can also have the meta table re-assigned to it (if again is the only server - i.e. in this scenario you can have one regionserver holding both the meta and root tables). unless i am missing something i cannot find any reason why we cannot assign the root table to the regionserver that manages the meta table if it is the only one remaining (again it is an extreme case i agree that this can happen). i applied and tested a fix (at the hbase-0.20.0 codebase) in the regionmanager::regionsawaitingassignment where i add the root table in the regionstoassign set if the it is the metaserver and also the only server. ",
        "label": 247
    },
    {
        "text": "data loss after compaction when a row has more than integer max value columns  we have lost the data in our development environment when a row has more than integer.max_value columns after compaction. i think the reason is type of storescanner's countperrow is int. https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storescanner.java#l67 after changing the type to long, it seems to be fixed. ",
        "label": 455
    },
    {
        "text": "master log start filling with  flush journal status  messages  takes a while to get into this condition. not each to tell how because all logs have rolled off and i only have logs filled w/ below: 2020-01-09 07:01:01,723 debug org.apache.hadoop.hbase.regionserver.hregion: flush status journal: acquiring readlock on region at 1578553261723 flush successful flush result:cannot_flush_memstore_empty, failurereason:nothing to flush,flush seq id45226854 at 1578553261723 2020-01-09 07:01:01,723 debug org.apache.hadoop.hbase.regionserver.hregion: flush status journal: acquiring readlock on region at 1578553261723 flush successful flush result:cannot_flush_memstore_empty, failurereason:nothing to flush,flush seq id45226855 at 1578553261723 2020-01-09 07:01:01,723 debug org.apache.hadoop.hbase.regionserver.hregion: flush status journal: acquiring readlock on region at 1578553261723 flush successful flush result:cannot_flush_memstore_empty, failurereason:nothing to flush,flush seq id45226856 at 1578553261723 2020-01-09 07:01:01,723 debug org.apache.hadoop.hbase.regionserver.hregion: flush status journal: acquiring readlock on region at 1578553261723 flush successful flush result:cannot_flush_memstore_empty, failurereason:nothing to flush,flush seq id45226857 at 1578553261723 ... i added the printing of flushresult... i.e. cannot flush because store is empty. digging. ",
        "label": 314
    },
    {
        "text": "versioning site  part two  publish site and add link from main site  do the rest of site versioning. ",
        "label": 314
    },
    {
        "text": "update hadoop prerequisites docs to call out  the hadoop pmc has release 2.8.1 with the same \"not ready for production\" caveat as 2.8.0 (ref announce email) we should update our docs proactively. ",
        "label": 98
    },
    {
        "text": "enable blockcache by default  was   reevaluate hbase block caching work   go back and take another look at the tom white work. we've gotten boost in writing and scanning because of j-d work. hbase-288 looks like it boosts sequential reads and perhaps random read a little. take another look. ",
        "label": 314
    },
    {
        "text": "close out github pull requests that aren't likely to update into a usable patch  we have a bunch of open github prs. some of them might turn into things we can close as a part of pushing a fix; mostly they're just missing a jira because folks showed up at github first. others look like mistaken pull requests or are things we've already pushed but without noting it. to clean up the latter, we should push an empty commit that says it closes the issues. ",
        "label": 402
    },
    {
        "text": "testtablemapreduce failed in hbase patch   testtablemapreduce failed in hbase-patch #15 see: http://hudson.zones.apache.org/hudson/job/hbase-patch/15/testreport/ java.lang.nullpointerexception  at org.apache.hadoop.hbase.hregionserver$queueentry.hashcode(hregionserver.java:187)  at org.apache.hadoop.hbase.hregionserver$queueentry.equals(hregionserver.java:181)  at java.util.abstractcollection.contains(abstractcollection.java:101)  at org.apache.hadoop.hbase.hregionserver$flusher.flushrequested(hregionserver.java:494)  at org.apache.hadoop.hbase.multiregiontable.makemultiregiontable(multiregiontable.java:107)  at org.apache.hadoop.hbase.mapred.testtablemapreduce.localtestmultiregiontable(testtablemapreduce.java:284)  at org.apache.hadoop.hbase.mapred.testtablemapreduce.testtablemapreduce(testtablemapreduce.java:205) ",
        "label": 241
    },
    {
        "text": "set permission to  top hfile in loadincrementalhfiles  set the same \"-rwxrwxrwx\" permission to .top file as .bottom and _tmp ",
        "label": 173
    },
    {
        "text": "incorrect table status in hbase shell describe  describe output of table which is disabled shows as enabled. ",
        "label": 371
    },
    {
        "text": "rawcell gettags should return the iterator tag  in order to avoid iterating through whole tag array at once  ",
        "label": 544
    },
    {
        "text": "git jira release audit tool  before cutting rc, need to make reconciliation between what's in jira and then what has been actually committed to ensure jira is accurate before starting the rc build (in order to ensure changes/releasenotes are accurate, etc.). would be good to have a tool that compared git log for the release to fixversion in jira (and then fixversion in jira to what is in git) to ensure matching mentions in both places. it can get complicated when there has been reverts in git or when an issue is an umbrella issue w/ no direct patch associated in jira but tool could start out simple dumping out a list of hashes/jiras for the rm to 'check' where anomalies; i.e. mention in one system but not in the other. this would save on a bunch of work aligning the two systems. could also do stuff like check git log to ensure all commits have associated jira, and so on. (should there be a pr component?). ",
        "label": 339
    },
    {
        "text": "waledits under replay will also be replicated  i need to verify this but seeing the code try {         // we are about to append this edit; update the region-scoped sequence number.  do it         // here inside this single appending/writing thread.  events are ordered on the ringbuffer         // so region sequenceids will also be in order.         regionsequenceid = entry.stampregionsequenceid();         // edits are empty, there is nothing to append.  maybe empty when we are looking for a         // region sequence id only, a region edit/sequence id that is not associated with an actual         // edit. it has to go through all the rigmarole to be sure we have the right ordering.         if (entry.getedit().isempty()) {           return;         }         // coprocessor hook.         if (!coprocessorhost.prewalwrite(entry.gethregioninfo(), entry.getkey(),             entry.getedit())) {           if (entry.getedit().isreplay()) {             // set replication scope null so that this won't be replicated             entry.getkey().setscopes(null);           }         }         if (!listeners.isempty()) {           for (walactionslistener i: listeners) {             // todo: why does listener take a table description and cps take a regioninfo?  fix.             i.visitlogentrybeforewrite(entry.gethtabledescriptor(), entry.getkey(),               entry.getedit());           }         } when a waledit is in replay we set the logkey to null. but in the visitlogentrybeforewrite() we again set the logkey based on the replication scope associated with the cells. so the previous step of setting null does not work here? ",
        "label": 198
    },
    {
        "text": "fix coprocessor handling of duplicate classes  while discussing with misty linville over on hbase-13907 we noticed some inconsistency when copros are loaded. sometimes you can load them more than once, sometimes you can not. need to consolidate. ",
        "label": 314
    },
    {
        "text": " replication  npe while replicating a log that is acquiring a new block from hdfs  we're getting an npe during replication, which causes replication for that regionserver to stop until we restart it. 2013-03-10 12:49:12,679 error org.apache.hadoop.hbase.replication.regionserver.replicationsource: unexpected exception in replicationsource, currentpath=hdfs://hmaster1:9000/hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2c60020%2c1362549511446.1362944946489 java.lang.nullpointerexception         at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.updateblockinfo(dfsclient.java:1882)         at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.openinfo(dfsclient.java:1855)         at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.<init>(dfsclient.java:1831)         at org.apache.hadoop.hdfs.dfsclient.open(dfsclient.java:578)         at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:154)         at org.apache.hadoop.fs.filterfilesystem.open(filterfilesystem.java:108)         at org.apache.hadoop.io.sequencefile$reader.openfile(sequencefile.java:1495)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader$walreader.openfile(sequencefilelogreader.java:62)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1482)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1475)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1470)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader$walreader.<init>(sequencefilelogreader.java:55)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.reset(sequencefilelogreader.java:308)         at org.apache.hadoop.hbase.replication.regionserver.replicationhlogreadermanager.openreader(replicationhlogreadermanager.java:69)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.openreader(replicationsource.java:505)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:313) some extra digging into the datanode and namenode logs makes this seem related to hbase-7530 and hdfs-4380 here's the relevant snipped portions of the rs, dn, and nn logs: rs 2013-03-10 12:49:12,618 info org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager: going to report log #hslave1177%2c60020%2c1362549511446.1362944946489 for position 59670826 in hdfs://hmaster1:9000/hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2c60020%2c1362549511446.1362944946489 rs 2013-03-10 12:49:12,621 debug org.apache.hadoop.hbase.regionserver.logroller: hlog roll requested rs 2013-03-10 12:49:12,623 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: replicated in total: 31500300 rs 2013-03-10 12:49:12,623 debug org.apache.hadoop.hbase.replication.regionserver.replicationsource: opening log for replication hslave1177%2c60020%2c1362549511446.1362944946489 at 59670826 nn 2013-03-10 12:49:12,627 info org.apache.hadoop.hdfs.statechange: block* namesystem.allocateblock: /hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2c60020%2c1362549511446.1362944946489. blk_6905758215335505153_656717631 rs 2013-03-10 12:49:12,679 error org.apache.hadoop.hbase.replication.regionserver.replicationsource: unexpected exception in replicationsource, currentpath=hdfs://hmaster1:9000/hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2c60020%2c1362549511446.1362944946489 dn 2013-03-10 12:49:12,680 info org.apache.hadoop.hdfs.server.datanode.datanode: receiving block blk_6905758215335505153_656717631 src: /192.168.44.1:43503 dest: /192.168.44.1:50010 nn 2013-03-10 12:49:12,804 info org.apache.hadoop.hdfs.statechange: block* namesystem.fsync: file /hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2c60020%2c1362549511446.1362944946489 for dfsclient_hb_rs_hslave1177,60020,1362549511446 ",
        "label": 125
    },
    {
        "text": "upgrade jackson dependencies in branch  avoid jackson versions and dependencies with known cves ",
        "label": 473
    },
    {
        "text": "regionserver attempting to open reassigned region but master ignoring  thinks region is still closing  write heavy load (splitting) causes some regionservers to report overloading to the master. master closes the regions on the affected regionservers and reassigns them. regionserver(s) that get new assignments attempt to open the region, but master ingores the open request because \"region is closing\". more information forthcoming once i have a chance to analyze the logs. ",
        "label": 241
    },
    {
        "text": "replication can overrun  meta  scans on cluster re start  when restarting a large set of regions on a reasonably small cluster the replication from another cluster tied up every xceiver meaning nothing could be onlined. ",
        "label": 199
    },
    {
        "text": "hbase config sh needs to be updated so it can auto detects the sun jre provided by rhel6  rhel6 will install its sun jdk in /usr/lib/jvm/java-1.6.0-sun-1.6.0.<update_version>.<arch>.  so this ticket is about adding this path to the jdk autodetection mechanism used in hbase-config.sh ",
        "label": 382
    },
    {
        "text": "fix coverage for org apache hadoop hbase mapreduce  fix coverage org.apache.hadoop.hbase.mapreduce patch hbase-8534-0.94.patch for branch-0.94  patch hbase-8534-trunk.patch for branch-0.95 and trunk ",
        "label": 16
    },
    {
        "text": "transacitonal improvments and fixes  a number of improvements and fixes to the transactional server: use a lease listener to detect and remove dead transactions handle splitting/closing during transactions improve 2-phase commit protocol to save some traffic for read only commits expose as a jta resource so we can work with other transactional resources additional logging ",
        "label": 110
    },
    {
        "text": "forward port hbase size based hbaseserver callqueue throttle from 89fb branch  forward port the size base throttle that is out in 0.89fb branch. its nicer than what we have in trunk where we just count queue items. ",
        "label": 441
    },
    {
        "text": "testinterfaceaudienceannotations fails on branch  2017-10-10 09:15:15,609 info  [main] hbase.testinterfaceaudienceannotations(254): these are the classes that do not have @interfaceaudience annotation: 2017-10-10 09:15:15,609 info  [main] hbase.testinterfaceaudienceannotations(256): class org.apache.hadoop.hbase.version ",
        "label": 38
    },
    {
        "text": "php thrift's getrow  would throw an exception if the row does not exist  i've been played with thrift recently, and observed an unexpected behavior: when getrow() encounters an non-existent row key, it throws an exception like this: php fatal error: uncaught exception 'exception' with message 'getrow failed: unknown result' in pear/thrift/packages/hbase/hbase.php:715 stack trace:  #0 pear/thrift/packages/hbase/hbase.php(666): hbaseclient->recv_getrow()  #1 htdocs/hbase/democlient.php(174): hbaseclient->getrow('demo_table', '00100-xxxx')  #2 {main} thrown in pear/thrift/packages/hbase/hbase.php on line 715 i would expect when we pass a non-existent key, it can throw something like notfound (as in scanner) or one can test with rowresult.isempty() just like in java api. ",
        "label": 385
    },
    {
        "text": "add a clock skew warning threshold  there's currently an exception thrown by the master when a region server attempts to start up with clock skew greater than some configured amount (defaulting to 30 seconds). however, it'd be nice to get some warnings logged at a value that isn't severe enough to warrant killing the rs, but still represents significant skew that could affect correctness. will attach a simple patch to add this as a setting. ",
        "label": 209
    },
    {
        "text": "result of integer multiplication cast to long in hregionfilesystem sleepbeforeretry       threads.sleep(basesleepbeforeretries * sleepmultiplier); both basesleepbeforeretries and sleepmultiplier are integers. without proper casting, their product may be negative.  here is an example:   static int i = integer.max_value-1;   static long j = i * 2; value of j above is -4 while 4294967292 was the expected value. ",
        "label": 548
    },
    {
        "text": "rowlock release problem with thread interruptions in batchmutate  an earlier version of snapshots would thread interrupt operations. in longer term testing we ran into an exception stack trace that indicated that a rowlock was taken an never released. 2013-01-26 01:54:56,417 error org.apache.hadoop.hbase.procedure.proceduremember: propagating foreign exception to subprocedure pe-1 org.apache.hadoop.hbase.errorhandling.foreignexception$proxythrowable via timer-java.util.timer@1cea3151:org.apache.hadoop.hbase.errorhandling.foreignexception$proxythrowable: org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign e xception start:1359194035004, end:1359194095004, diff:60000, max:60000 ms         at org.apache.hadoop.hbase.errorhandling.foreignexception.deserialize(foreignexception.java:184)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.abort(zkprocedurememberrpcs.java:321)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.watchforabortedprocedures(zkprocedurememberrpcs.java:150)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.access$200(zkprocedurememberrpcs.java:56)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs$1.nodechildrenchanged(zkprocedurememberrpcs.java:112)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:315)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) caused by: org.apache.hadoop.hbase.errorhandling.foreignexception$proxythrowable: org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1359194035004, end:1359194095004, diff:60000, max:60000 ms         at org.apache.hadoop.hbase.errorhandling.timeoutexceptioninjector$1.run(timeoutexceptioninjector.java:71)         at java.util.timerthread.mainloop(timer.java:512)         at java.util.timerthread.run(timer.java:462) 2013-01-26 01:54:56,648 warn org.apache.hadoop.hbase.regionserver.hregion: failed getting lock in batch put, row=0001558252 java.io.ioexception: timed out on getting lock for row=0001558252         at org.apache.hadoop.hbase.regionserver.hregion.internalobtainrowlock(hregion.java:3239)         at org.apache.hadoop.hbase.regionserver.hregion.getlock(hregion.java:3315)         at org.apache.hadoop.hbase.regionserver.hregion.dominibatchmutation(hregion.java:2150)         at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2021)         at org.apache.hadoop.hbase.regionserver.hregionserver.multi(hregionserver.java:3511)         at sun.reflect.generatedmethodaccessor46.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1400) .... .. every snapshot attempt that used this region for the next two days encountered this problem. snapshots will now bypass this problem with the fix in hbase-7703. however, we should make sure hbase regionserver operations are safe when interrupted. ",
        "label": 441
    },
    {
        "text": "support operationattributes in importtsv parser  this jira aims at supporting the operation attributes in bulk loads. ideally this operation attributes once extracted has to be set in the mappers/reducers.  in case of mappers using tableoutputformat this would be very helpful when the puts are done through htable. ",
        "label": 544
    },
    {
        "text": "  fstabledescriptors caching is racy  an occasionally failing test in 0.92 branch that concurrently executes master operations on a single table found this problem in fstabledescriptors: diff --git src/main/java/org/apache/hadoop/hbase/util/fstabledescriptors.java src/main/java/org/apache/hadoop/hbase/util/fstabledescriptors.java index e882621..b0042cd 100644 --- src/main/java/org/apache/hadoop/hbase/util/fstabledescriptors.java +++ src/main/java/org/apache/hadoop/hbase/util/fstabledescriptors.java @@ -221,8 +221,15 @@ public class fstabledescriptors implements tabledescriptors {      if (hconstants.hbase_non_user_table_dirs.contains(htd.getnameasstring())) {        throw new notimplementedexception();      } -    if (!this.fsreadonly) updatehtabledescriptor(this.fs, this.rootdir, htd); -    long modtime = gettableinfomodtime(this.fs, this.rootdir, htd.getnameasstring()); +    if (fsreadonly) { +      // cannot cache here. +      // we can't know if a modtime from the most recent file found in a +      // directory listing at some arbitrary point in time still corresponds +      // to the latest, nor that our htd is the latest. +      return; +    } +    // cache with the modtime of the descriptor we wrote +    long modtime = updatehtabledescriptor(this.fs, this.rootdir, htd).getmodificationtime();      this.cache.put(htd.getnameasstring(), new tabledescriptormodtime(modtime, htd));    } after hbase-7305 master operations are serialized by a write lock on the table. however, 0.94 has code with the same issue:   @override   public void add(htabledescriptor htd) throws ioexception {     if (bytes.equals(hconstants.root_table_name, htd.getname())) {       throw new notimplementedexception();     }     if (bytes.equals(hconstants.meta_table_name, htd.getname())) {       throw new notimplementedexception();     }     if (hconstants.hbase_non_user_table_dirs.contains(htd.getnameasstring())) {       throw new notimplementedexception();     }     if (!this.fsreadonly) updatehtabledescriptor(this.fs, this.rootdir, htd);     string tablename = htd.getnameasstring();     long modtime = gettableinfomodtime(this.fs, this.rootdir, tablename);     long dirmodtime = gettabledirmodtime(this.fs, this.rootdir, tablename);     this.cache.put(tablename, new tabledescriptormodtime(modtime, dirmodtime, htd));   } ",
        "label": 371
    },
    {
        "text": "re add bloomfilter test over zealously removed by hbase  i removed testbytebloomfilter when i shouldn't have when i removed all related to unused dynamic bloomfilters. readd. ",
        "label": 314
    },
    {
        "text": "create hbase client module  i just tried creating a project that uses 0.95-snapshot and had to import org.apache.hbase:hbase-server as the module that i depend on. this will be confusing to users. in addition this brings in lots of dependencies that are not really needed. let's create a client module that has all of the client in it. ",
        "label": 154
    },
    {
        "text": "asyncrpcretryingcaller will not schedule retry if we hit a notservingregionexception but there is no tablename provided  this is a bug on master branch but the condition described in title can only happen on hbase-21512 where we also use asyncconnection to replay edits. so not a big problem for the current hbase but we need to fix it. ",
        "label": 149
    },
    {
        "text": "have shell print regioninfo and location on first load if debug enabled  i think it helps debugging \u2013 you can find quickly which region a row belongs to and what .meta. thinks about that region: index: src/java/org/apache/hadoop/hbase/client/hconnectionmanager.java =================================================================== --- src/java/org/apache/hadoop/hbase/client/hconnectionmanager.java     (revision 771129) +++ src/java/org/apache/hadoop/hbase/client/hconnectionmanager.java     (working copy) @@ -553,6 +553,7 @@            // instantiate the location            location = new hregionlocation(regioninfo,                new hserveraddress(serveraddress)); +          log.debug(location);            cachelocation(tablename, location);            return location;          } catch (tablenotfoundexception e) { ",
        "label": 314
    },
    {
        "text": "fsutils leaserecovery for non hdfs filesystems   fsutils.recoverfilelease uses hdfs's recoverlease method to get lease before splitting hlog file.  this might not work for other filesystem implementations. ",
        "label": 295
    },
    {
        "text": "backwards compatiblity support for new masterobserver apis  the group assignment changes introduce new methods to the masterobserver interface. this is a concern for things like apache phoenix. (see their indexmasterobserver, etc.) we can handle this by using compatibility helpers that won't attempt to invoke the new apis on masterobservers that do not implement them. ",
        "label": 174
    },
    {
        "text": "make compaction code standalone  this is part of hbase-2462. make the compaction code standalone so can run it independent of hbase. will make it easier to profile and try stuff out. ",
        "label": 314
    },
    {
        "text": "potential leak of htable instances when using htablepool with pooltype threadlocal  (initially discussed in hbase-4150) in htablepool, when obtaining a table: private htableinterface findorcreatetable(string tablename) {     htableinterface table = tables.get(tablename);     if (table == null) {       table = createhtable(tablename);     }     return table;   } in the case of threadlocalpool, it seems like there's an exposure here between when the table is created initially and when threadlocalpool.put() is called to set the thread local variable (on pooledhtable.close()). potential solution described by karthick sankarachary: for one thing, we might want to clear the tables variable when the htablepool is closed (as shown below). for another, we should override threadlocalpool#get method so that it removes the resource, otherwise it might end up referencing a htableinterface that's has been released. 1 diff --git a/src/main/java/org/apache/hadoop/hbase/client/htablepool.java b/src/main/java/org/apache/hadoop/hbase/client/htablepool.java       2 index 952a3aa..c198f15 100755       3 --- a/src/main/java/org/apache/hadoop/hbase/client/htablepool.java       4 +++ b/src/main/java/org/apache/hadoop/hbase/client/htablepool.java      13 @@ -309,6 +310,7 @@ public class htablepool implements closeable {      14      for (string tablename : tables.keyset()) {      15        closetablepool(tablename);      16      }      17 +    this.tables.clear();      18    } ",
        "label": 265
    },
    {
        "text": "baseloadbalancer should consider region replicas when randomassignment and roundrobinassignment  copied the comment in hbase-23035.   there are two problems about the loadbalancer.   1. the cluster means the cluster state of the whole cluster. but hasregionreplica is false, so it only create clusterstate by the regions which need to assign, not the whole cluster... cluster cluster = createcluster(servers, regions, false); list<regioninfo> unassignedregions = new arraylist<>(); roundrobinassignment(cluster, regions, unassignedregions,   servers, assignments);   protected cluster createcluster(list<servername> servers, collection<regioninfo> regions,       boolean hasregionreplica) {     // get the snapshot of the current assignments for the regions in question, and then create     // a cluster out of it. note that we might have replicas already assigned to some servers     // earlier. so we want to get the snapshot to see those assignments, but this will only contain     // replicas of the regions that are passed (for performance).     map<servername, list<regioninfo>> clusterstate = null;     if (!hasregionreplica) {       clusterstate = getregionassignmentsbyserver(regions);     } else {       // for the case where we have region replica it is better we get the entire cluster's snapshot       clusterstate = getregionassignmentsbyserver(null);     }    for (servername server : servers) {       if (!clusterstate.containskey(server)) {         clusterstate.put(server, empty_region_list);       }     }     return new cluster(regions, clusterstate, null, this.regionfinder,         rackmanager);   } 2. wouldloweravailability method only consider the primary regions. the replica region can't assign to same server with primary region. but can be assigned to same server with other replica regions. ",
        "label": 187
    },
    {
        "text": "testacidguarantee broken on trunk  testacidguarantee has a test whereby it attempts to read a number of columns from a row, and every so often the first column of n is different, when it should be the same. this is a bug deep inside the scanner whereby the first peek() of a row is done at time t then the rest of the read is done at t+1 after a flush, thus the memstorets data is lost, and previously 'uncommitted' data becomes committed and flushed to disk. one possible solution is to introduce the memstorets (or similarly equivalent value) to the hfile thus allowing us to preserve read consistency past flushes. another solution involves fixing the scanners so that peek() is not destructive (and thus might return different things at different times alas). ",
        "label": 34
    },
    {
        "text": "fix testinfoservers  with the recent port to modules, we broke a couple of tests, including this one. the fix needs to ensure that the webapp still works from the in-situ and packaged running of hbase. ",
        "label": 236
    },
    {
        "text": "cleanup where we keep  proto files  i see andrew for his pb work over in rest has .protos files under src/main/resources. we should unify where these files live. the recently added .protos place them under src/main/protobuf its confusing. the thift idl files are here under resources too. seems like we should move src/main/protobuf under src/resources to be consistent. ",
        "label": 314
    },
    {
        "text": " meta  may not come back online if   number of executors servers crash and one of those   number of executors was carrying meta  this is a duplicate of another issue but at the moment i cannot find the original. if you had a 700 node cluster and then you ran something on the cluster which killed 100 nodes, and .meta. had been running on one of those downed nodes, well, you'll have all of your master executors processing servershutdowns and more than likely non of the currently processing executors will be servicing the shutdown of the server that was carrying .meta. well, for server shutdown to complete at the moment, an online .meta. is required. so, in the above case, we'll be stuck. the current executors will not be able to clear to make space for the processing of the server carrying .meta. because they need .meta. to complete. we can make the master handlers have no bound so it will expand to accomodate all crashed servers \u2013 so it'll have the one .meta. in its queue \u2013 or we can change it so shutdown handling doesn't require .meta. to be on-line (its used to figure the regions the server was carrying); we could use the master's in-memory picture of the cluster (but iirc, there may be holes ....tbd) ",
        "label": 107
    },
    {
        "text": "ability to open a hregion from hdfs snapshot   now that hdfs snapshots are here, we started to run our mapreduce jobs over hdfs snapshots. the thing is, hdfs snapshots are read-only point-in-time copies of the file system. thus we had to modify the section of code that initialized the region internals in hregion. we have to skip cleanup of certain directories if the hregion is backed by a hdfs snapshot. i have a patch for trunk with some basic tests if folks are interested. ",
        "label": 522
    },
    {
        "text": "partitioner class not used in tablemapreduceutil inittablereducejob   upon checking the available utility methods in tablemapreduceutil i came across this code   public static void inittablereducejob(string table,     class<? extends tablereduce> reducer, jobconf job, class partitioner)   throws ioexception {     job.setoutputformat(tableoutputformat.class);     job.setreducerclass(reducer);     job.set(tableoutputformat.output_table, table);     job.setoutputkeyclass(immutablebyteswritable.class);     job.setoutputvalueclass(batchupdate.class);     if (partitioner != null) {       job.setpartitionerclass(hregionpartitioner.class);       htable outputtable = new htable(new hbaseconfiguration(job), table);       int regions = outputtable.getregionsinfo().size();       if (job.getnumreducetasks() > regions){      job.setnumreducetasks(outputtable.getregionsinfo().size());       }     }   } it seems though as it should be     if (partitioner != null) {       job.setpartitionerclass(partitioner); and the provided hregionpartitioner can be handed in to that call or a custom one can be provided. ",
        "label": 73
    },
    {
        "text": "usage of modules  with hbase-4336, hbase will have the ability to add multiple modules for different aspects of the codebase (less tests, see hbase-4336 for details). we need to set a policy for when modules should be used versus putting the code into a single existing module or dispersed across modules. ",
        "label": 236
    },
    {
        "text": "hbaseclient and hbaseserver should use hbase security authentication when negotiating authentication  this came up in the context of testing hbase-6788. currently hbaseclient and hbaseserver call usergroupinformation.issecurityenabled() when determining whether or not to use sasl to negotiate connections. this means they are using the hadoop.security.authentication configuration value. since this is in the context of hbase rpc connections, it seems more correct to use the hbase.security.authentication configuration value by calling user.ishbasesecurityenabled(). ",
        "label": 180
    },
    {
        "text": "code coverage improvement  create unit tests for connectionid  ",
        "label": 363
    },
    {
        "text": "hregionserver getwalgroupsreplicationstatus  throws npe  precondition: hbase.balancer.tablesonmaster = true hbase.balancer.tablesonmaster.systemtablesonly = true   open the rs page of the master throws nullpointexception, because replicationsourcehandler never initialized. hregionserver#getwalgroupsreplicationstatus() need check [is hmaster && can't host user region]. ",
        "label": 542
    },
    {
        "text": "output to multiple tables from hadoop mr without use of htable  o.a.h.h.mapreduce.tableoutputformat allows writing to a single table as output from a map/reduce job in the natural way. it requires that the user specify the table name ahead of time and can only write to one table. i had a need to write to multiple tables from the same job (write my data to one table, and also write to index tables), and i wanted to have a consistent api whether writing to one or many tables. attached multitableoutputformat takes the table name as the key and the put or delete as the value. also included is an example demonstrating the usage. ",
        "label": 276
    },
    {
        "text": "double and float converters for bytes class  is there any reason why there are no double and float converters for bytes class? they will certainly come in handy. ",
        "label": 144
    },
    {
        "text": "npe in hstorekey  from the list: 2008-10-14 14:23:55,705 info org.apache.hadoop.ipc.server: ipc server handler 7 on 60020, call getrow([b@17dc1ef, [b@1474316, null, 9223372036854775807, -1) from 192.168.1.10:49676: error: java.io.ioexception: java.lang.nullpointerexception java.io.ioexception: java.lang.nullpointerexception        at org.apache.hadoop.hbase.hstorekey.compareto(hstorekey.java:354)        at org.apache.hadoop.hbase.hstorekey$hstorekeywritablecomparator.compare(hstorekey.java:593)        at org.apache.hadoop.io.mapfile$reader.seekinternal(mapfile.java:436)        at org.apache.hadoop.io.mapfile$reader.getclosest(mapfile.java:558)        at org.apache.hadoop.io.mapfile$reader.getclosest(mapfile.java:541)        at org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader.getclosest(hstorefile.java:761)        at org.apache.hadoop.hbase.regionserver.hstore.getfullfrommapfile(hstore.java:1179)        at org.apache.hadoop.hbase.regionserver.hstore.getfull(hstore.java:1160)        at org.apache.hadoop.hbase.regionserver.hregion.getfull(hregion.java:1221)        at org.apache.hadoop.hbase.regionserver.hregionserver.getrow(hregionserver.java:1036)        at sun.reflect.generatedmethodaccessor21.invoke(unknown source)        at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)        at java.lang.reflect.method.invoke(method.java:597)        at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:554)        at org.apache.hadoop.ipc.server$handler.run(server.java:888) ",
        "label": 314
    },
    {
        "text": "regionremoteprocedurebase should override settimeoutfailure  in our test environment, we found that, when openregionprocedure failed by timeout, though it doesn't support rollback, procedureexcutor will delete the opr node in the store in its own rollback steps and make the rit stay there looping rollback exception. servercrashprocedure will detect this rit but cannot deal with it, because no node in the store and it will encounters npe, as a result, scp aborts.   logs are as follows, 2019-08-29,07:27:35,710 info [peworker-15] org.apache.hadoop.hbase.master.procedure.servercrashprocedure: pid=70206, state=runnable:server_crash_assign, locked=true; servercrashprocedure server=c3-hadoop-srv-st297.bj,21600,1567012736142, splitwal=true, meta=true found rit pid=56043, ppid=55625, state=rolledback; transitregionstateprocedure table=c3srv_galaxy:tsdb, region=c1ccd94593bf1b87269cec98d6ffaaae, assign; rit=open, location=c3-hadoop-srv-st297.bj,21600,1567012736142, table=c3srv_galaxy:tsdb, region=c1ccd94593bf1b87269cec98d6ffaaae  2019-08-29,07:27:35,711 error [peworker-15] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception: pid=70206, state=runnable:server_crash_assign, locked=true; servercrashprocedure server=c3-hadoop-srv-st297.bj,21600,1567012736142, splitwal=true, meta=true  java.lang.nullpointerexception  at org.apache.hadoop.hbase.procedure2.store.procedurestoretracker.update(procedurestoretracker.java:140)  at org.apache.hadoop.hbase.procedure2.store.procedurestoretracker.update(procedurestoretracker.java:133)  at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.updatestoretracker(walprocedurestore.java:782)  at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.pushdata(walprocedurestore.java:737)  at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.update(walprocedurestore.java:604)  at org.apache.hadoop.hbase.master.assignment.regionremoteprocedurebase.persistandwake(regionremoteprocedurebase.java:182)  at org.apache.hadoop.hbase.master.assignment.regionremoteprocedurebase.servercrashed(regionremoteprocedurebase.java:239)  at org.apache.hadoop.hbase.master.assignment.transitregionstateprocedure.servercrashed(transitregionstateprocedure.java:398)  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.assignregions(servercrashprocedure.java:461)  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.executefromstate(servercrashprocedure.java:221)  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.executefromstate(servercrashprocedure.java:64)  at org.apache.hadoop.hbase.procedure2.statemachineprocedure.execute(statemachineprocedure.java:189)  at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:962)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1645)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1392)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$1100(procedureexecutor.java:78)  at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1962)  2019-08-29,07:27:35,714 error [peworker-15] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=70206, state=failed:server_crash_split_logs, locked=true, exception=java.lang.nullpointerexception via code-bug: uncaught runtime exception: pid=70206, state=runnable:server_crash_assign, locked=true; servercrashprocedure server=c3-hadoop-srv-st297.bj,21600,1567012736142, splitwal=true, meta=true:java.lang.nullpointerexception; servercrashprocedure server=c3-hadoop-srv-st297.bj,21600,1567012736142, splitwal=true, meta=true  java.lang.unsupportedoperationexception: unhandled state=server_crash_assign  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:333)  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:64)  at org.apache.hadoop.hbase.procedure2.statemachineprocedure.rollback(statemachineprocedure.java:208)  at org.apache.hadoop.hbase.procedure2.procedure.dorollback(procedure.java:979)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.executerollback(procedureexecutor.java:1566)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.executerollback(procedureexecutor.java:1498)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1349)  at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$1100(procedureexecutor.java:78)  at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1962)  2019-08-29,07:27:35,716 error [peworker-15] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=70206, state=failed:server_crash_get_regions, locked=true, exception=java.lang.nullpointerexception via code-bug: uncaught runtime exception: pid=70206, state=runnable:server_crash_assign, locked=true; servercrashprocedure server=c3-hadoop-srv-st297.bj,21600,1567012736142, splitwal=true, meta=true:java.lang.nullpointerexception; servercrashprocedure server=c3-hadoop-srv-st297.bj,21600,1567012736142, splitwal=true, meta=true  java.lang.unsupportedoperationexception: unhandled state=server_crash_split_logs  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:333)  at org.apache.hadoop.hbase.master.procedure.servercrashprocedure.rollbackstate(servercrashprocedure.java:64)  at org.apache.hadoop.hbase.procedure2.statemachineprocedure.rollback(statemachineprocedure.java:208)  at org.apache.hadoop.hbase.procedure2.procedure.dorollback(procedure.java:979)   ------------- [work@c3-hadoop-srv-ct30 log]$ grep 'pid=56659' *  hbase.4500.log:2019-08-29,01:40:31,103 info [master/c3-hadoop-srv-ct30:21500:becomeactivemaster] org.apache.hadoop.hbase.master.procedure.masterprocedurescheduler: took xlock for pid=56659, ppid=56653, state=waiting:region_state_transition_confirm_opened; transitregionstateprocedure table=c3prck8smonitorc3:tsdb-service, region=413a5199ff68f3505f490c8341efe459, assign  hbase.4500.log:2019-08-29,01:40:31,107 info [master/c3-hadoop-srv-ct30:21500:becomeactivemaster] org.apache.hadoop.hbase.procedure2.timeoutexecutorthread: added pid=56661, ppid=56659, state=waiting_timeout; org.apache.hadoop.hbase.master.assignment.openregionprocedure; timeout=1006, timestamp=1567014012995  hbase.4500.log:2019-08-29,01:40:31,186 info [master/c3-hadoop-srv-ct30:21500:becomeactivemaster] org.apache.hadoop.hbase.master.assignment.assignmentmanager: attach pid=56659, ppid=56653, state=waiting:region_state_transition_confirm_opened; transitregionstateprocedure table=c3prck8smonitorc3:tsdb-service, region=413a5199ff68f3505f490c8341efe459, assign to rit=offline, location=null, table=c3prck8smonitorc3:tsdb-service, region=413a5199ff68f3505f490c8341efe459 to restore rit  hbase.4500.log:2019-08-29,01:40:31,676 info [peworker-16] org.apache.hadoop.hbase.procedure2.procedureexecutor: lock_event_wait rollback...pid=56661, ppid=56659, state=failed, exception=org.apache.hadoop.hbase.exceptions.timeoutioexception via procedureexecutor:org.apache.hadoop.hbase.exceptions.timeoutioexception: operation timed out after 19.6860sec; org.apache.hadoop.hbase.master.assignment.openregionprocedure  hbase.4500.log:2019-08-29,01:40:43,155 error [peworker-7] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=56661, ppid=56659, state=failed, locked=true, exception=org.apache.hadoop.hbase.exceptions.timeoutioexception via procedureexecutor:org.apache.hadoop.hbase.exceptions.timeoutioexception: operation timed out after 19.6860sec; org.apache.hadoop.hbase.master.assignment.openregionprocedure  hbase.4500.log:2019-08-29,01:40:43,759 error [peworker-7] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=56661, ppid=56659, state=failed, locked=true, exception=org.apache.hadoop.hbase.exceptions.timeoutioexception via procedureexecutor:org.apache.hadoop.hbase.exceptions.timeoutioexception: operation timed out after 19.6860sec; org.apache.hadoop.hbase.master.assignment.openregionprocedure  hbase.4500.log:2019-08-29,01:40:44,064 error [peworker-7] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=56659, ppid=56653, state=waiting:region_state_transition_open, locked=true; transitregionstateprocedure table=c3prck8smonitorc3:tsdb-service, region=413a5199ff68f3505f490c8341efe459, assign  hbase.4500.log:2019-08-29,01:40:44,471 info [peworker-7] org.apache.hadoop.hbase.master.procedure.masterprocedurescheduler: took xlock for pid=56659, ppid=56653, state=waiting:region_state_transition_open; transitregionstateprocedure table=c3prck8smonitorc3:tsdb-service, region=413a5199ff68f3505f490c8341efe459, assign  hbase.4500.log:2019-08-29,01:40:44,573 error [peworker-7] org.apache.hadoop.hbase.procedure2.procedureexecutor: code-bug: uncaught runtime exception for pid=56659, ppid=56653, state=waiting:region_state_transition_get_assign_candidate, locked=true; transitregionstateprocedure table=c3prck8smonitorc3:tsdb-service, region=413a5199ff68f3505f490c8341efe459, assign ",
        "label": 149
    },
    {
        "text": "sporadic testzkprocedurecontrollers failures on trunk  see https://builds.apache.org/job/precommit-hbase-build/4865//artifact/trunk/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.procedure.testzkprocedurecontrollers.txt and  https://builds.apache.org/job/precommit-hbase-build/4865//artifact/trunk/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.procedure.testzkprocedurecontrollers-output.txt i see this in the output: 2013-03-18 17:30:46,672 debug [thread-2-eventthread] zookeeper.zkutil(1682): testing utility-0x13d7e8da7590000 retrieved 0 byte(s) of data from znode /hbase/testsimple/acquired/instancetest; data=empty 2013-03-18 17:30:46,672 debug [thread-2-eventthread] procedure.zkprocedurememberrpcs(206): start proc data length is 0 2013-03-18 17:30:46,672 error [thread-2-eventthread] procedure.zkprocedurememberrpcs(210): data in for starting procuedure instancetest is illegally formatted. killing the procedure. 2013-03-18 17:30:46,673 error [thread-2-eventthread] procedure.zkprocedurememberrpcs(218): illegal argument exception java.lang.illegalargumentexception: data in for starting procuedure instancetest is illegally formatted. killing the procedure. at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:211) at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.waitfornewprocedures(zkprocedurememberrpcs.java:175) at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.access$100(zkprocedurememberrpcs.java:56) at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs$1.nodechildrenchanged(zkprocedurememberrpcs.java:109) at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:312) at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519) at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) 2013-03-18 17:30:46,675 error [thread-2-eventthread] procedure.zkprocedurememberrpcs(281): failed due to null subprocedure java.lang.illegalargumentexception via expected:java.lang.illegalargumentexception: data in for starting procuedure instancetest is illegally formatted. killing the procedure. at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:219) at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.waitfornewprocedures(zkprocedurememberrpcs.java:175) at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.access$100(zkprocedurememberrpcs.java:56) at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs$1.nodechildrenchanged(zkprocedurememberrpcs.java:109) at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:312) at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519) at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) caused by: java.lang.illegalargumentexception: data in for starting procuedure instancetest is illegally formatted. killing the procedure. at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:211) ... 6 more the znode has zero data (usually it has 7 bytes when test runs fine). is the latch being triggered on node create before data is written? pointers appreciated. ",
        "label": 233
    },
    {
        "text": "the version string in build xml is wrong  the version string should be 0.18.1-dev since 0.18.0 is released. this affects 0.18 branch only. ",
        "label": 385
    },
    {
        "text": " shell  cannot describe meta tables  hbase(main):005:0* describe '.meta.'  08/08/29 04:39:49 debug client.hconnectionmanager$tableservers: cache hit in table locations for row <> and tablename .meta.: location server 208.76.44.142:60020, location region name .meta.,,1  argumenterror: failed to find table named .meta.   from /home/stack/trunk/bin/../bin/hbase.rb:61:in `describe'   from /home/stack/trunk/bin/../bin/hirb.rb:246:in `describe'   from (hbase):6:in `binding'  code} ",
        "label": 218
    },
    {
        "text": "add an option to have strict number of mapper per job in hbase streaming  currently there is only one configuration knob available for controlling the number of mappers per job in hbase streaming. the current option is number of mappers per region. this options tries to maintain the locality for the mapper and the region servers. however, in certain certain scenarios where the table has a higher number of regions, the number of mapper/region can lead to explosion of the number of mappers. hence, we need one more option to control the strict number of mappers per job. ",
        "label": 154
    },
    {
        "text": "update hadoop support matrix to list  as supported  the hadoop community responded very well to our request for more maintenance releases and have now put out 2.6.1 - 2.6.3. the first of those included the fix for our catastrophic failure under hdfs encryption. we should update the book to point out those versions are fine. ",
        "label": 402
    },
    {
        "text": "remove hlogedit  from https://issues.apache.org/jira/browse/hbase-1403?focusedcommentid=12708553&page=com.atlassian.jira.plugin.system.issuetabpanels%3acomment-tabpanel#action_12708553 @stack:   i'm thinking of the hlog and hcd. hlog values are a hlogedit which is now just the new keyvalue class + a thbase flag. in interests of performance, i'd like to not have to create a hlogedit at all and just shove in the keyvalue. ",
        "label": 314
    },
    {
        "text": "unsychronized logwriters map is mutated from several threads in hlog splitting  in splitlog, the logwriters map is an unsynchronized collection, and the spawned threads mutate it. in practice i've now seen several times a situation where one of the puts into this map is lost, and a thread ends up renaming a file it's in the process of writing to, causing those edits to be lost when the log is split. ",
        "label": 453
    },
    {
        "text": "failure on enable disable table will cause table state in zk to be left as enabling disabling until master is restarted  in enable/disabletablehandler code, if something goes wrong in handling, the table state in zk is left as enabling / disabling. after that we cannot force any more action from the api or cli, and the only recovery path is restarting the master.     if (done) {       // flip the table to enabled.       this.assignmentmanager.getzktable().setenabledtable(         this.tablenamestr);       log.info(\"table '\" + this.tablenamestr       + \"' was successfully enabled. status: done=\" + done);     } else {       log.warn(\"table '\" + this.tablenamestr       + \"' wasn't successfully enabled. status: done=\" + done);     } here, if done is false, the table state is not changed. there is also no way to set skiptablestatecheck from cli / api. we have run into this issue a couple of times before. ",
        "label": 543
    },
    {
        "text": "hbaseadmin tableexists  should not require a full meta scan  hbaseadmin.tableexists() actually makes a call to hbaseadmin.listtables() and then checks if the table you are looking for is in the list. listtables() uses a metascanner/metavisitor, thus requiring an entire meta table scan. we should not require a full meta scan to check for the existence of a single table. ",
        "label": 314
    },
    {
        "text": "rename hbaseadmin getcompletedsnapshots as hbaseadmin listsnapshots  during review of hbase-7360, it was noted that hbaseadmin#listsnapshots aligns with existing hbaseadmin#listtables better this jira renames hbaseadmin#getcompletedsnapshots as hbaseadmin#listsnapshots ",
        "label": 441
    },
    {
        "text": "metrics servlet throws npe  in branch-1 at least we put up a servlet on \"/metrics\" that is hadoop's metricsservlet. however hbase users are expected to pick up metrics via \"/jmx\". we don't mention \"/metrics\" or link to it on the ui. if you attempt to access \"/metrics\" with head of branch-1 it errors out due to a npe 2017-12-04 16:06:37,403 error [1874557409@qtp-1910896157-3] mortbay.log: /metrics java.lang.nullpointerexception at org.apache.hadoop.http.httpserver2.isinstrumentationaccessallowed(httpserver2.java:1049) at org.apache.hadoop.metrics.metricsservlet.doget(metricsservlet.java:109) at javax.servlet.http.httpservlet.service(httpservlet.java:707) ",
        "label": 455
    },
    {
        "text": "add new metrics to better monitor recovery process  ",
        "label": 233
    },
    {
        "text": "rest server  configure number of threads for jetty  i am running a mapreduce job (~400 simultaneous map tasks) that makes random reads from hbase. i have put a varnishd reverse proxy cache in front of a single rest server. the single rest server's jetty appears to be running out of threads: 2008-02-01 23:17:16,971 info org.mortbay.http.socketlistener: low on threads ((256-256+1)<2) on socketlistener0@0.0.0.0:61234  2008-02-01 23:17:17,116 warn org.mortbay.http.socketlistener: out of threads: socketlistener0@0.0.0.0:61234  2008-02-01 23:17:19,255 info org.mortbay.http.socketlistener: low on threads ((256-256+1)<2) on socketlistener0@0.0.0.0:61234 the default for jetty is to use a thread pool of 256 threads. but i'd like to be able to specify (preferably in hadoop-site.xml) how large the thread pool should be \u2013 in this case it needs to have as many threads as i have simultaneous map tasks. ",
        "label": 314
    },
    {
        "text": " site  fix home page so it shows the search hadoop com box  search is present on most other pages, not the home page. ",
        "label": 314
    },
    {
        "text": "major compaction can create empty store files  causing aioob when trying to read  here is the backtrace: caused by: java.lang.arrayindexoutofboundsexception: 0  at org.apache.hadoop.hbase.io.hfile.hfile$reader.getfirstkey(hfile.java:991)  at org.apache.hadoop.hbase.regionserver.storefilegetscan.getstorefile(storefilegetscan.java:84)  at org.apache.hadoop.hbase.regionserver.storefilegetscan.get(storefilegetscan.java:65)  at org.apache.hadoop.hbase.regionserver.store.get(store.java:1548)  at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2263)  at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2252)  at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1739) this can happen if your table only has deletes, and everything evaporates during a major compaction. ",
        "label": 314
    },
    {
        "text": "create an hbase native client repository and commit the native client code to it  ",
        "label": 187
    },
    {
        "text": " replication  zk dump prints the raw pbuf for the hlog positions  looking at the zk dump in the master's web ui, i can see that we're not trying to parse the positions for the hlogs so it looks like \"pbu\ufffd\ufffd\ufffd\" ",
        "label": 199
    },
    {
        "text": "each time around the regionserver core loop  we clear the messages to pass master  even if we failed to deliver them  at the head of the regionserver run loop we do this:           synchronized(this.outboundmsgs) {             outboundarray =               this.outboundmsgs.toarray(new hmsg[outboundmsgs.size()]);             this.outboundmsgs.clear();           } we do this even if we failed to deliver the message to the master \u2013 connection refused or whatever. ",
        "label": 314
    },
    {
        "text": "tests that use hbasetestingutility startminicluster n  should shutdown with hbasetestingutility shutdownminicluster   most tests that use mini clusters use this pattern  private final static hbasetestingutility util = new hbasetestingutility();   @beforeclass   public static void beforeclass() throws exception {     util.startminicluster(1);   }   @afterclass   public static void afterclass() throws ioexception {     util.shutdownminicluster();   } some tests (like hbase-4269)   @beforeclass   public static void beforeclass() throws exception {     util.startminicluster(1);   }   @afterclass   public static void afterclass() throws ioexception {     util.getminicluster().shutdown();     // or util.shutdownminihbasecluster();     // and likely others.   } there is a difference between the two shutdown \u2013 the former deletes files created during the tests while the latter does not. this funny state persisting (zk or hbase/mr data) may be the cause of strange inter-testcase problems when full suites are run. ",
        "label": 340
    },
    {
        "text": "add cp hooks around storefilereader creation  ",
        "label": 46
    },
    {
        "text": "remove deprecated field from metricsreplicationsourcesource  the field source_shipped_kbs in the class metricsreplicationsourcesource is deprecated and should be removed in 3.0.0. ",
        "label": 398
    },
    {
        "text": "table should not be required in accesscontrolservice  we should fix the proto file, add unit test for this case, and verify it works from hbase shell with table to be nil. ",
        "label": 242
    },
    {
        "text": "min latency in latency histograms are emitted as long max value  see attached graph. ",
        "label": 155
    },
    {
        "text": "backport to  hbase remove update  and improve explicitcolumntracker performance   columntracker.update() is no longer called from anywhere but a test; we should remove it. ",
        "label": 286
    },
    {
        "text": "use hdfs acl to give user the ability to read snapshot directly on hdfs  on the dev meetup notes in shenzhen after hbasecon asia, there is a topic about the permission to read hfiles on hdfs directly. for client-side scanner going against hfiles directly; is there a means of being able to pass the permissions from hbase to hdfs? and at xiaomi we also face the same problem. snapshotscanner is much faster and consumes less resources, but only super use has the ability to read hfile directly on hdfs. so here we want to use hdfs acl to address this problem. https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfspermissionsguide.html#acls_file_system_api the basic idea is to set acl and default acl on the ns/table/cf directory on hdfs for the users who have the permission to read the table on hbase. suggestions are welcomed. ",
        "label": 500
    },
    {
        "text": "jdiff script no longer works as usage instructions indicate  i pasted the command from the usage instructions embedded in the script, but it fails as follows: [misty@cheezel dev-support](master)$ bash ./jdiffhbasepublicapi.sh https://github.com/apache/hbase.git 0.94 https://github.com/my_repo/hbase.git 0.94  jdiff evaluation beginning:  determining if this is a local directory or a git repo.  looks like https://github.com/apache/hbase.git is a git repo  determining if this is a local directory or a git repo.  looks like https://github.com/my_repo/hbase.git is a git repo  we are going to compare source 1 which is a git_repo and source 2, which is a git_repo  0.94  0.94  jdiff_working_directory not set. that's not an issue. we will default it to /tmp/jdiff.  % total % received % xferd average speed time time time current  dload upload total spent left speed  100 183 100 183 0 0 447 0 -::- -::- -::- 448  archive: jdiff-1.1.1-with-incompatible-option.zip  end-of-central-directory signature not found. either this file is not  a zipfile, or it constitutes one disk of a multi-part archive. in the  latter case the central directory and zipfile comment will be found on  the last disk(s) of this archive.  unzip: cannot find zipfile directory in one of jdiff-1.1.1-with-incompatible-option.zip or  jdiff-1.1.1-with-incompatible-option.zip.zip, and cannot find jdiff-1.1.1-with-incompatible-option.zip.zip, period. ",
        "label": 141
    },
    {
        "text": "master won't go down  stuck waiting on  meta  to come on line   master came up w/ no regionservers. i then tried to shut it down. you can see in below that it started to go down.... 2013-04-24 14:28:49,770 info  [ipc server handler 7 on 60000] org.apache.hadoop.hbase.master.hmaster: cluster shutdown requested 2013-04-24 14:28:49,815 info  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.servermanager: finished waiting for region servers count to settle; checked in 0, slept for 2818 ms, expecting minimum of 1, maximum of 2147483647, master is stopped. 2013-04-24 14:28:49,815 warn  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.masterfilesystem: master stopped while splitting logs 2013-04-24 14:28:50,104 info  [stack-1.ent.cloudera.com,60000,1366838923135.splitlogmanagertimeoutmonitor] org.apache.hadoop.hbase.master.splitlogmanager$timeoutmonitor: stack-1.ent.cloudera.com,60000,1366838923135.splitlogmanagertimeoutmonitor exiting 2013-04-24 14:28:50,850 info  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.zookeeper.zookeepernodetracker: unsetting meta region location in zookeeper 2013-04-24 14:28:50,884 warn  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.zookeeper.recoverablezookeeper: node /hbase/meta-region-server already deleted, retry=false 2013-04-24 14:28:50,884 info  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.assignmentmanager: cluster shutdown is set; skipping assign of .meta.,,1.1028785192 2013-04-24 14:28:50,884 info  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.servermanager: assignmentmanager hasn't finished failover cleanup 2013-04-24 14:29:46,188 info  [master-stack-1.ent.cloudera.com,60000,1366838923135.oldlogcleaner] org.apache.hadoop.hbase.master.cleaner.logcleaner: master-stack-1.ent.cloudera.com,60000,1366838923135.oldlogcleaner exiting 2013-04-24 14:29:46,193 info  [master-stack-1.ent.cloudera.com,60000,1366838923135.archivedhfilecleaner] org.apache.hadoop.hbase.master.cleaner.hfilecleaner: master-stack-1.ent.cloudera.com,60000,1366838923135.archivedhfilecleaner exiting ... but not it is stuck. we keep looping here: \"master-stack-1.ent.cloudera.com,60000,1366838923135\" prio=10 tid=0x00007f154853f000 nid=0x18b in object.wait() [0x00007f1545fde000]    java.lang.thread.state: timed_waiting (on object monitor)         at java.lang.object.wait(native method)         - waiting on <0x00000000c727d738> (a org.apache.hadoop.hbase.zookeeper.metaregiontracker)         at org.apache.hadoop.hbase.zookeeper.zookeepernodetracker.blockuntilavailable(zookeepernodetracker.java:161)         - locked <0x00000000c727d738> (a org.apache.hadoop.hbase.zookeeper.metaregiontracker)         at org.apache.hadoop.hbase.zookeeper.metaregiontracker.waitmetaregionlocation(metaregiontracker.java:105)         at org.apache.hadoop.hbase.catalog.catalogtracker.waitformeta(catalogtracker.java:250)         at org.apache.hadoop.hbase.catalog.catalogtracker.waitformeta(catalogtracker.java:299)         at org.apache.hadoop.hbase.master.hmaster.enablesshandwaitformeta(hmaster.java:905)         at org.apache.hadoop.hbase.master.hmaster.assignmeta(hmaster.java:879)         at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:764)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:522)         at java.lang.thread.run(thread.java:722) odd. it is supposed to be checking the 'stopped' flag; maybe it has wrong stop flag. ",
        "label": 543
    },
    {
        "text": "on restart of branch  master complains about not being able to set safe mode  a few fellows have reported this issue. i see it myself if i restart the cluster. you'll see: zookeeperwrapper: failed to create out of safe mode in zookeeper: org.apache.zookeeper.keeperexception$nodeexistsexception: keepererrorcode = nodeexists for /hbase/safe-mode ... over and over again in master log. ",
        "label": 314
    },
    {
        "text": " replication  lower the amount of logging to a more human readable level  we need stop logging every time replication decides to do something. it used to be extremely useful when the code base was younger but now it should be possible to bring it down while keeping it relevant. ",
        "label": 229
    },
    {
        "text": "bytebloomfilter's performance can be improved by avoiding multiplication when generating hash  bytebloomfilter's performance can be optimized by avoiding multiplication operation when generating hash ",
        "label": 404
    },
    {
        "text": "if server hosting meta dies or is stopping while processing another server shutdown  ioe accessing meta stop shutdown handler from finishing  in testrollingrestart, there is a test which kills server hosting root then immediately kills server hosting meta. in a recent run this turned up a small race condition if the server hosting meta is closing while we process shutdown of server hosting root. 2010-10-31 20:41:34,621 error [master_meta_server_operations-dev692.sf2p.facebook.com:54989-0] executor.eventhandler(154): caught throwable while processing event m_meta_server_shutdown org.apache.hadoop.ipc.remoteexception: java.io.ioexception: server not running         at org.apache.hadoop.hbase.regionserver.hregionserver.checkopen(hregionserver.java:2216)         at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:1652)         at sun.reflect.generatedmethodaccessor19.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:561)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1025)         at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:749)         at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:255)         at $proxy8.openscanner(unknown source)         at org.apache.hadoop.hbase.catalog.metareader.getserveruserregions(metareader.java:495)         at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:125)         at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:151)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:885)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)         at java.lang.thread.run(thread.java:619) ",
        "label": 247
    },
    {
        "text": "we need a hbase partitioner for tablemapreduceutil inittablereducejob mr jobs  when we run say 20 reducers they all get ~1/20th of the data to output to the table. the problem for us on large import jobs is the data gets sorted by key and the all reducers   pound one region at a time. we need to add onto the tablemapreduceutil.inittablereducejob method so it can set the partitioner   and set the number of reducers = number of regions as the table map does for maps. then the partitioner will send all the batchupdates for one region to one reducer.  so we get a more even spread of writers to the regions this would assure that only one reducer will send   updates to one region keeping any one region from getting more overloaded the others. ",
        "label": 73
    },
    {
        "text": "improve exceptions that come out on client side  client-side exceptions should contain regionserver and region client was going against. looking at an unknownscannerexception that came out of a client, it looks like something happened over on the regionserver but my cluster has a 20-plus machines and i don't know which i should be looking at. ",
        "label": 241
    },
    {
        "text": "pull hbase spark module out of branch  see discuss here:  https://s.apache.org/ujaf sadly, feature is slipping out of branch-2. we can work out inclusion for downstream once we have some inertia again. ",
        "label": 402
    },
    {
        "text": "region in transition metric is broken  ritcount stays 0 no matter what ",
        "label": 205
    },
    {
        "text": "use zookeeper multi to clear znodes  in zkprocedureutil, clearchildznodes() and clearznodes(string procedurename) should utilize zookeeper multi so that they're atomic ",
        "label": 367
    },
    {
        "text": "upgrade hadoop version to which is stable  here is related discussion:  http://search-hadoop.com/m/na71y1kkhdm1/hadoop+version+1.2.1+%2528stable%2529+released&subj=re+announce+hadoop+version+1+2+1+stable+released older hadoop 1 artifacts would be phased out. ",
        "label": 441
    },
    {
        "text": "pseudo distributed mode in localhbasecluster  after changes to config and adding of parameter:   <property>     <name>hbase.cluster.distributed</name>     <value>false</value>     <description>the mode the cluster will be in. possible values are       false: standalone and pseudo-distributed setups with managed zookeeper       true: fully-distributed with unmanaged zookeeper quorum (see hbase-env.sh)     </description>   </property> running hbase in pseudo destr mode become not so easy, i think description for false variant is wrong and you can't get pseudo-distributed mode setting this option to false now. i wish to have some option in localhbasecluster for pseudo destr mode where i wish to have: separate processes for master and hrss ability to dynamically change hrs number in cluster managed zk cluster (like it is in local mode now) managed small local hdfs cluster i think it is very useful for testing, debugging and for user experience with hbase, who want to try something close to real hbase not some emulation in standalone mode. ",
        "label": 341
    },
    {
        "text": "fix type mismatch on container access in quotacache chore  there is a mismatch in type in using the \"collection.contains\" method in quotacache#chore so this patch changes the two uses of the method to map.containskey. found this issue through one of the active alerts through lgtm. ",
        "label": 32
    },
    {
        "text": "report metrics from jvmpausemonitor  we have jvmpausemonitor for detecting jvm pauses; pauses are logged at warn. would also be good to expose this information on a dashboard via metrics system \u2013 make it easier to get this info off the host and into a central location for the operator. ",
        "label": 38
    },
    {
        "text": " compat  hcd remove and removeconfiguration change return type  change made in hbase-18008. asking chia-ping tsai if ok to undo. here is complaint from jacc: hbase-client-1.2.6.jar, hcolumndescriptor.class package org.apache.hadoop.hbase [\u2212] hcolumndescriptor.remove ( byte[ ] key )  :  void  1   org/apache/hadoop/hbase/hcolumndescriptor.remove:([b)v change effect 1 return value type has been changed from void to hcolumndescriptor. this method has been removed because the return type is part of the method signature. [\u2212] hcolumndescriptor.removeconfiguration ( string key )  :  void  1   org/apache/hadoop/hbase/hcolumndescriptor.removeconfiguration:(ljava/lang/string;)v change effect 1 return value type has been changed from void to hcolumndescriptor. this method has been removed because the return type is part of the method signature. binary breaking but not src breaking. see https://stackoverflow.com/questions/3589946/retrofitting-void-methods-to-return-its-argument-to-facilitate-fluency-breaking for good discussion. probably not a prob. but just in case and if not really needed, will purge. ",
        "label": 98
    },
    {
        "text": "zookeeper warn spits out lots of useless messages  example: 2009-03-19 22:18:28,139 warn [main-sendthread] zookeeper.clientcnxn$sendthread(932): ignoring exception during shutdown input  java.net.socketexception: socket is not connected  at sun.nio.ch.socketchannelimpl.shutdown(native method)  at sun.nio.ch.socketchannelimpl.shutdowninput(socketchannelimpl.java:640)  at sun.nio.ch.socketadaptor.shutdowninput(socketadaptor.java:360)  at org.apache.zookeeper.clientcnxn$sendthread.cleanup(clientcnxn.java:930)  at org.apache.zookeeper.clientcnxn$sendthread.run(clientcnxn.java:901) ",
        "label": 342
    },
    {
        "text": "pe sequentialwrite is 7x slower because of memstoreflusher checkstorefilecount  ",
        "label": 247
    },
    {
        "text": "fix flaky test testreplicationadminwithclusters testdisableandenablereplication  if we run testdisableandenablereplication, we will get the following error message. testdisableandenablereplication(org.apache.hadoop.hbase.client.replication.testreplicationadminwithclusters)  time elapsed: 2.046 sec  <<< failure! java.lang.assertionerror: expected:<1> but was:<0>         at org.junit.assert.fail(assert.java:88)         at org.junit.assert.failnotequals(assert.java:834)         at org.junit.assert.assertequals(assert.java:645)         at org.junit.assert.assertequals(assert.java:631)         at org.apache.hadoop.hbase.client.replication.testreplicationadminwithclusters.testdisableandenablereplication(testreplicationadminwithclusters.java:160) the critical code is shown below.     admin1.disabletablereplication(tablename);     htabledescriptor table = admin1.gettabledescriptor(tablename);     for (hcolumndescriptor fam : table.getcolumnfamilies()) {       assertequals(fam.getscope(), hconstants.replication_scope_local);     }     table = admin2.gettabledescriptor(tablename);     for (hcolumndescriptor fam : table.getcolumnfamilies()) {       assertequals(fam.getscope(), hconstants.replication_scope_local);     } is htd got from admin2 affected by admin1? i don't think so. we should remove the related assertion. ",
        "label": 187
    },
    {
        "text": "prefixfilter should seek to first matching row  currently a prefixfilter will happily scan all kvs < prefix.  if should seek forward to the prefix if the current kv < prefix. ",
        "label": 286
    },
    {
        "text": "can't open region because can't open  regioninfo because alreadybeingcreatedexception  testing killing .meta. i tripped over this one. last thing seen on regionserver killed was: 2011-02-02 21:44:48,379 debug org.apache.hadoop.hbase.regionserver.hregion: instantiated testtable,0591556500,1296683085472.76c9a32c5f068d16240e42a15fed8417. ... which means we could have been inside checkregioninfoonfilesystem when we were killed. this tries to create the .regioninfo file. seems like that was started over at the nn but then the rs died shortly afterward. its stopping the region opening. i suppose i could try and open it for append to shut it then reopen? ",
        "label": 314
    },
    {
        "text": "regions in transition do not get reassigned by master when rs crashes  very similar to hbase-1928, but for the general case (not just root/meta): if a region is in transition on a rs when the rs crashes, the master does not remove it from regionsintransition when processing the rs shutdown. this is fairly easy to trigger by bringing up a rs and kill -9ing it just as it starts to get regions assigned. those regions will get permanently lost since they're stuck in regionsintransition and thus don't get assigned by the metascanner. ",
        "label": 453
    },
    {
        "text": "hbase shell help info  these lines below are in the alter table help section should not be using the words \"create a table\" should be something like \"to alter a table to add columns 'xx', 'yy', 'zz' using all defaults\" keep the confusion out of it.            to create a table with an 'f1', 'f2', and 'f3' using all defaults:            hbase> alter 't1', {name => 'f1'}, {name => 'f2'}, {name => 'f3'} ",
        "label": 314
    },
    {
        "text": "intermittent testreplicationsyncuptool failure  from https://builds.apache.org/job/precommit-hbase-build/8200//testreport/ : java.lang.assertionerror: t2_syncup has 201 rows on source, and 200 on slave1 expected:&lt;200> but was:&lt;125>   at org.junit.assert.fail(assert.java:88)   at org.junit.assert.failnotequals(assert.java:743)   at org.junit.assert.assertequals(assert.java:118)   at org.junit.assert.assertequals(assert.java:555)   at org.apache.hadoop.hbase.replication.testreplicationsyncuptool.putandreplicaterows(testreplicationsyncuptool.java:245)   at org.apache.hadoop.hbase.replication.testreplicationsyncuptool.testsyncuptool(testreplicationsyncuptool.java:116) ",
        "label": 134
    },
    {
        "text": "output hbase hadoop jvm version as well as java opts  ulimit  into master regionserver log on startup  ",
        "label": 314
    },
    {
        "text": "misconfigured addition of peers leads to cluster shutdown   recently we added a peer to a production cluster which were in different kerberos realm. steps to reproduce:  1. add a misconfigured peer which is in different kerberos realm.  2. remove that peer.  3. all region servers will start to crash. rca  enabled trace logging on one region server for a short amount of time.  after adding peer, saw the following log lines. 2019-06-18 22:19:20,949 info  [main-eventthread] replication.replicationtrackerzkimpl - /hbase/replication/peers znode expired, triggering peerlistchanged event 2019-06-18 22:19:20,992 info  [main-eventthread] replication.replicationpeerszkimpl - added new peer cluster=<dev-cluster-zookeeper>:/hbase 2019-06-18 22:19:21,113 info  [main-eventthread] zookeeper.recoverablezookeeper - process identifier=hconnection-0x794a56d6 connecting to zookeeper ensemble=<dev-cluster-zookeeper> 2019-06-18 22:20:01,280 warn  [main-eventthread] zookeeper.zkutil - hconnection-0x794a56d6-0x16b56265fbebb1b, quorum=<dev-cluster-zookeeper>, baseznode=/hbase unable to set watcher on znode (/hbase/hbaseid) org.apache.zookeeper.keeperexception$authfailedexception: keepererrorcode = authfailed for /hbase/hbaseid         at org.apache.zookeeper.keeperexception.create(keeperexception.java:123)         at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)         at org.apache.zookeeper.zookeeper.exists(zookeeper.java:1102)         at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:220)         at org.apache.hadoop.hbase.zookeeper.zkutil.checkexists(zkutil.java:421)         at org.apache.hadoop.hbase.zookeeper.zkclusterid.readclusteridznode(zkclusterid.java:65)         at org.apache.hadoop.hbase.client.zookeeperregistry.getclusterid(zookeeperregistry.java:105)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.retrieveclusterid(connectionmanager.java:922)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.<init>(connectionmanager.java:706)         at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.<init>(connectionmanager.java:638)         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)         at java.lang.reflect.constructor.newinstance(constructor.java:423)         at org.apache.hadoop.hbase.client.connectionfactory.createconnection(connectionfactory.java:238)         at org.apache.hadoop.hbase.client.connectionmanager.createconnection(connectionmanager.java:432)         at org.apache.hadoop.hbase.client.connectionmanager.createconnectioninternal(connectionmanager.java:341)         at org.apache.hadoop.hbase.client.hconnectionmanager.createconnection(hconnectionmanager.java:144)         at org.apache.hadoop.hbase.replication.regionserver.hbaseinterclusterreplicationendpoint.init(hbaseinterclusterreplicationendpoint.java:135)         at com.salesforce.hbase.replication.tenantreplicationendpoint.init(tenantreplicationendpoint.java:30)         at org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager.getreplicationsource(replicationsourcemanager.java:517)         at org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager.addsource(replicationsourcemanager.java:273)         at org.apache.hadoop.hbase.replication.regionserver.replicationsourcemanager.peerlistchanged(replicationsourcemanager.java:635)         at org.apache.hadoop.hbase.replication.replicationtrackerzkimpl$peerswatcher.nodechildrenchanged(replicationtrackerzkimpl.java:192)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:643)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:544)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:519) 2019-06-18 22:20:42,999 warn  [source,<dev-cluster>] zookeeper.zkutil - connection to cluster: <dev-cluster>-0x26b56265fe7b5cd, quorum=<dev-cluster-zookeeper>, baseznode=/hbase unable to set watcher on znode (/hbase/hbaseid) org.apache.zookeeper.keeperexception$authfailedexception: keepererrorcode = authfailed for /hbase/hbaseid         at org.apache.zookeeper.keeperexception.create(keeperexception.java:123)         at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)         at org.apache.zookeeper.zookeeper.exists(zookeeper.java:1102)         at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:220)         at org.apache.hadoop.hbase.zookeeper.zkutil.checkexists(zkutil.java:421)         at org.apache.hadoop.hbase.zookeeper.zkclusterid.readclusteridznode(zkclusterid.java:65)         at org.apache.hadoop.hbase.zookeeper.zkclusterid.getuuidforcluster(zkclusterid.java:96)         at org.apache.hadoop.hbase.replication.hbasereplicationendpoint.getpeeruuid(hbasereplicationendpoint.java:104)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:304) 2019-06-18 22:20:43,002 trace [source,<dev-cluster>] regionserver.replicationsource - cannot contact the peer's zk ensemble, sleeping 1000 times 1 <same stack trace as before> 2019-06-18 22:20:44,008 trace [source,<dev-cluster>] regionserver.replicationsource - cannot contact the peer's zk ensemble, sleeping 1000 times 2 <same stack trace as before> this goes on and on until we removed the peer.  after removing the peer, 2019-06-18 22:21:20,731 info  [main-eventthread] replication.replicationtrackerzkimpl - /hbase/replication/peers/<dev-cluster> znode expired, triggering peerremoved event 2019-06-18 22:21:20,731 info  [main-eventthread] regionserver.replicationsourcemanager - closing the following queue <dev-cluster>, currently have 2 and another 0 that were recovered 2019-06-18 22:21:20,733 info  [main-eventthread] regionserver.replicationsourcemanager - number of deleted recovered sources for <dev-cluster>: 0 2019-06-18 22:21:20,734 info  [main-eventthread] regionserver.replicationsource - closing source <dev-cluster> because: replication stream was removed by a user 2019-06-18 22:21:20,734 info  [main-eventthread] replication.tenantreplicationendpoint - stopping endpoint 2019-06-18 22:21:20,736 info  [main-eventthread] client.connectionmanager$hconnectionimplementation - closing zookeeper sessionid=0x16b56265fbebb1b 2019-06-18 22:21:20,736 debug [main-eventthread] ipc.rpcclientimpl - stopping rpc client 2019-06-18 22:21:20,738 info  [main-eventthread] regionserver.replicationsourcemanager - done with the queue <dev-cluster> 2019-06-18 22:21:20,744 debug [main-eventthread] replication.replicationqueueszkimpl - peer /hbase/replication/hfile-refs/<dev-cluster> not found in hfile reference queue. 2019-06-18 22:21:28,066 info  [source,<dev-cluster>] regionserver.replicationsource - replicating d12a5eb3-16bd-4910-91ed-41e0e7990007 -> null 2019-06-18 22:21:28,067 debug [source,<dev-cluster>] regionserver.replicationsource - starting up worker for wal group <rs-serner-name>%2c60020%2c1560896050398 2019-06-18 22:21:28,069 error [050398,<dev-cluster>] regionserver.replicationsource - unexpected exception in replicationsourceworkerthread, currentpath=null java.lang.illegalargumentexception: peer with id= <dev-cluster> is not connected         at org.apache.hadoop.hbase.replication.replicationpeerszkimpl.getstatusofpeer(replicationpeerszkimpl.java:217)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.ispeerenabled(replicationsource.java:363)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource$replicationsourceworkerthread.run(replicationsource.java:549) 2019-06-18 22:21:28,070 info  [050398,<dev-cluster>] regionserver.hregionserver - stopped: unexpected exception in replicationsourceworkerthread 2019-06-18 22:21:28,071 info  [/10.231.90.212:60020] regionserver.splitlogworker - sending interrupt to stop the worker thread 2019-06-18 22:21:28,073 info  [/10.231.90.212:60020] regionserver.hregionserver - stopping infoserver 2019-06-18 22:21:28,075 info  [as-dnds4-3-prd:60020] regionserver.splitlogworker - splitlogworker interrupted. exiting.  2019-06-18 22:21:28,076 info  [as-dnds4-3-prd:60020] regionserver.splitlogworker - splitlogworker <rs-server-name>,60020,1560896050398 exiting 2019-06-18 22:21:28,181 info  [/10.231.90.212:60020] regionserver.heapmemorymanager - stopping heapmemorytuner chore. 2019-06-18 22:21:28,181 info  [/10.231.90.212:60020] flush.regionserverflushtableproceduremanager - stopping region server flush procedure manager gracefully. also verified from jstacks i captured between addition of peer and removal of peer, the replication source thread was sleeping waiting to connect to peer. \"main-eventthread.replicationsource,<dev-cluster>\" #1923 daemon prio=5 os_prio=0 tid=0x00007f1ad18f4800 nid=0x25a47  waiting on condition [0x00007f1220314000]    java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.sleepforretries(replicationsource.java:349)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:306) according to me, the bug lies here: replicationsource.java // some comments here   public void run() { { .... ....  while (this.issourceactive() && this.peerclusterid == null) {       this.peerclusterid = replicationendpoint.getpeeruuid();       if (this.issourceactive() && this.peerclusterid == null) {         if (sleepforretries(\"cannot contact the peer's zk ensemble\", sleepmultiplier)) {           sleepmultiplier++;         }       }     } !-- there is a bug here. we should check that peerclusterid is not null. if null, then terminate the thread. -->     // in rare case, zookeeper setting may be messed up. that leads to the incorrect     // peerclusterid value, which is the same as the source clusterid     if (clusterid.equals(peerclusterid) && !replicationendpoint.canreplicatetosamecluster()) {       this.terminate(\"clusterid \" + clusterid + \" is replicating to itself: peerclusterid \"           + peerclusterid + \" which is not allowed by replicationendpoint:\"           + replicationendpoint.getclass().getname(), null, false);       this.manager.closequeue(this);       return;     } .... .... } ",
        "label": 387
    },
    {
        "text": "linear reseek in memstore  this is to address the linear reseek in memstorescanner. currently reseek iterates over the kvset and the snapshot linearly by just calling next repeatedly. the new solution is to do this linear seek up to a configurable maximum amount of times then if the seek is not yet complete fall back to logarithmic seek. ",
        "label": 154
    },
    {
        "text": "testregionservercoprocessorexceptionwithremove mentions master in javadoc  from testregionservercoprocessorexceptionwithremove :  * expected result is that the master will remove the buggy coprocessor from looks like a copy-and-paste error. ",
        "label": 441
    },
    {
        "text": "compaction failure in scanwildcardcolumntracker checkcolumn  i have at least one region that won't compact. hbase> status 'detailed' [...] content,0b41cdd2ee9b36b0674ad423089800ba,1248257816633      stores=3, storefiles=113, storefilesizemb=1213, memstoresizemb=0, storefileindexsizemb=0 [...] hbase> major_compact 'content' [...] hbase> status 'detailed' [...] content,0b41cdd2ee9b36b0674ad423089800ba,1248257816633      stores=3, storefiles=113, storefilesizemb=1213, memstoresizemb=0, storefileindexsizemb=0 [...] hbase> major_compact 'content,0b41cdd2ee9b36b0674ad423089800ba,1248257816633' [...] hbase> status 'detailed' [...] content,0b41cdd2ee9b36b0674ad423089800ba,1248257816633      stores=3, storefiles=113, storefilesizemb=1213, memstoresizemb=0, storefileindexsizemb=0 [...] $ hadoop fs -ls /hbase/content/1226419153/content found 37 items -rw-r--r--   2 hadoop supergroup   86422089 2009-07-24 16:11 /hbase/content/1226419153/content/1045181559873943545 -rw-r--r--   2 hadoop supergroup    7167852 2009-07-26 05:13 /hbase/content/1226419153/content/1225211885117827793 -rw-r--r--   2 hadoop supergroup       2678 2009-07-24 20:24 /hbase/content/1226419153/content/1275251824404920815 -rw-r--r--   2 hadoop supergroup   10288489 2009-07-26 02:16 /hbase/content/1226419153/content/1386224161790061095 -rw-r--r--   2 hadoop supergroup   16111798 2009-07-24 23:43 /hbase/content/1226419153/content/1454452327579297030 -rw-r--r--   2 hadoop supergroup   12650208 2009-07-26 23:31 /hbase/content/1226419153/content/1534514745508157864 -rw-r--r--   2 hadoop supergroup    6994590 2009-07-27 10:45 /hbase/content/1226419153/content/1706519919143970421 -rw-r--r--   2 hadoop supergroup   13449537 2009-07-26 20:53 /hbase/content/1226419153/content/2186668356269910871 -rw-r--r--   2 hadoop supergroup   10397457 2009-07-26 22:15 /hbase/content/1226419153/content/2292706332643873473 -rw-r--r--   2 hadoop supergroup    6481576 2009-07-27 05:59 /hbase/content/1226419153/content/2438489739664745000 -rw-r--r--   2 hadoop supergroup   10810191 2009-07-25 19:48 /hbase/content/1226419153/content/2538008511360230014 -rw-r--r--   2 hadoop supergroup   13877559 2009-07-27 18:26 /hbase/content/1226419153/content/2772019170563217117 -rw-r--r--   2 hadoop supergroup   12796040 2009-07-26 10:14 /hbase/content/1226419153/content/2902101782772083009 -rw-r--r--   2 hadoop supergroup    6541657 2009-07-27 01:06 /hbase/content/1226419153/content/3113808018684114931 -rw-r--r--   2 hadoop supergroup    8655428 2009-07-25 18:17 /hbase/content/1226419153/content/3817932211925236778 -rw-r--r--   2 hadoop supergroup    1445535 2009-07-27 16:10 /hbase/content/1226419153/content/4286233593585189878 -rw-r--r--   2 hadoop supergroup    9139508 2009-07-27 02:11 /hbase/content/1226419153/content/495340788313226264 -rw-r--r--   2 hadoop supergroup    3313459 2009-07-26 05:57 /hbase/content/1226419153/content/5167064731599803595 -rw-r--r--   2 hadoop supergroup    9473393 2009-07-26 14:44 /hbase/content/1226419153/content/5490426319631514899 -rw-r--r--   2 hadoop supergroup    9321224 2009-07-26 19:40 /hbase/content/1226419153/content/5790825797519034907 -rw-r--r--   2 hadoop supergroup   12171283 2009-07-27 12:20 /hbase/content/1226419153/content/6036401533248383324 -rw-r--r--   2 hadoop supergroup  824790136 2009-07-23 22:28 /hbase/content/1226419153/content/6211942192349190964 -rw-r--r--   2 hadoop supergroup    9905606 2009-07-27 07:45 /hbase/content/1226419153/content/6295275445036553977 -rw-r--r--   2 hadoop supergroup   23857510 2009-07-27 17:02 /hbase/content/1226419153/content/6535266251812885635 -rw-r--r--   2 hadoop supergroup    9177439 2009-07-26 11:21 /hbase/content/1226419153/content/6625735185665629662 -rw-r--r--   2 hadoop supergroup    6916543 2009-07-27 20:15 /hbase/content/1226419153/content/6934569497672884872 -rw-r--r--   2 hadoop supergroup    8818427 2009-07-27 13:48 /hbase/content/1226419153/content/7162767181372457089 -rw-r--r--   2 hadoop supergroup   12028925 2009-07-25 08:48 /hbase/content/1226419153/content/7254368961746328584 -rw-r--r--   2 hadoop supergroup   14695089 2009-07-27 09:16 /hbase/content/1226419153/content/728058506789102941 -rw-r--r--   2 hadoop supergroup    9154325 2009-07-27 03:43 /hbase/content/1226419153/content/7462549804082617977 -rw-r--r--   2 hadoop supergroup   22295283 2009-07-27 21:46 /hbase/content/1226419153/content/7498645200757573769 -rw-r--r--   2 hadoop supergroup    5414671 2009-07-27 15:27 /hbase/content/1226419153/content/758918234053985701 -rw-r--r--   2 hadoop supergroup   11260015 2009-07-26 08:16 /hbase/content/1226419153/content/7626050291010778604 -rw-r--r--   2 hadoop supergroup   11309116 2009-07-26 12:57 /hbase/content/1226419153/content/7704085331915902813 -rw-r--r--   2 hadoop supergroup    6326623 2009-07-26 17:58 /hbase/content/1226419153/content/7735737241739599956 -rw-r--r--   2 hadoop supergroup        832 2009-07-24 20:15 /hbase/content/1226419153/content/8170327402351444879 -rw-r--r--   2 hadoop supergroup   13480887 2009-07-26 16:32 /hbase/content/1226419153/content/890729481029340216 relevant nonstandard options: <property>   <name>hbase.hregion.max.filesize</name>   <value>1073741824</value> </property> ",
        "label": 38
    },
    {
        "text": "refactor the timeoutmonitor to make it less racy  the current implementation of the timeoutmonitor acts like a race condition generator, mostly making things worse rather than better. it does it's own thing for a while without caring for what's happening in the rest of the master. the first thing that needs to happen is that the regions should not be processed in one big batch, because that sometimes can take minutes to process (meanwhile a region that timed out opening might have opened, then what happens is it will be reassigned by the timeoutmonitor generating the never ending pending_open situation). those operations should also be done more atomically, although i'm not sure how to do it in a scalable way in this case. ",
        "label": 544
    },
    {
        "text": "store could miss rows during flush  while looking at hbase-4344 i found that my change hbase-4241 contains a critical mistake:  the while(scanner.next(kvs)) loop is incorrect and might miss the last edits. ",
        "label": 286
    },
    {
        "text": "avoid using ' tmp' directory in our unit tests  i'm used to run unit tests on a remote server before summit a large patch because sometimes a testcase failure can not be reproduced locally. recently phil yang and me share a remote server with a different account. we found that some of the unit tests write to '/tmp' with a fixed name and do not clean the file after test. this cause that the unit test can only be succeeded for one person... open an umbrella issue to address this. ",
        "label": 149
    },
    {
        "text": "add support for exporting hbase metrics via jmx  current hbase metrics collection and reporting (for the 0.19 releases at least) doesn't seem to export any of the metrics as mbeans via jmx. this patch adds jmx mbean support for reporting metrics collected in 3 areas: org.apache.hadoop.hbase.ipc.hbaserpcmetrics org.apache.hadoop.hbase.master.metrics.mastermetrics org.apache.hadoop.hbase.regionserver.metrics.regionservermetrics this patch is only against the 0.19 branch, as the metrics api seems to have changed a bit in hadoop 0.20, including adding support for dynamic mbeans. i'll add a patch reworked for trunk shortly. ",
        "label": 180
    },
    {
        "text": "speed up testrestoresnapshotfromclient  looking through the longest running test in 0.94 i noticed that testrestoresnapshotfromclient runs for over 10 minutes on the jenkins boxes (264s on my local box). ",
        "label": 286
    },
    {
        "text": "add test coverag ereport in hudson build  in hbase-789, we have coverage report targets added in build.xml. we need to tweak the hudson build so that it can automatically calls the coverage report target for us, just like hadoop. ",
        "label": 314
    },
    {
        "text": "catalogjanitor cleanmergequalifier may clean wrong parent regions  2019-09-17,19:42:40,539 info [peworker-1] org.apache.hadoop.hbase.procedure2.procedureexecutor: finished pid=1223589, state=success; gcmultiplemergedregionsprocedure child=647600d28633bb2fe06b40682bab0593, parents:[81b6fc3c560a00692bc7c3cd266a626a], [472500358997b0dc8f0002ec86593dcf] in 2.6470sec  2019-09-17,19:59:54,179 info [peworker-6] org.apache.hadoop.hbase.procedure2.procedureexecutor: finished pid=1223651, state=success; gcmultiplemergedregionsprocedure child=647600d28633bb2fe06b40682bab0593, parents:[9c52f24e0a9cc9b4959c1ebdfea29d64], [a623f298870df5581bcfae7f83311b33] in 1.0340sec the child is same region 647600d28633bb2fe06b40682bab0593 but the parent regions are different. mergetableregionprocedure#preparemergeregion will try to cleanmergequalifier for the regions to merge. for (regioninfo ri: this.regionstomerge) {       if (!catalogjanitor.cleanmergequalifier(ri)) {         string msg = \"skip merging \" + regioninfo.getshortnametolog(regionstomerge) +             \", because parent \" + regioninfo.getshortnametolog(ri) + \" has a merge qualifier\";         log.warn(msg);         throw new mergeregionexception(msg);       } if region a and b merge to c, region d and e merge to f. when merge c and f, it will try to cleanmergequalifier for c and f. catalogjanitor.cleanmergequalifier for region c succeed but catalogjanitor.cleanmergequalifier for region f failed as there are references in region f.  when merge c and f again, it will try to cleanmergequalifier for c and f again. but metatableaccessor.getmergeregions will get wrong parents now. it use scan with filter to scan result. but region c's mergequalifier already was deleted before. then the scan will return a wrong result, may be anther region...... public boolean cleanmergequalifier(final regioninfo region) throws ioexception {     // get merge regions if it is a merged region and already has merge qualifier     list<regioninfo> parents = metatableaccessor.getmergeregions(this.services.getconnection(),         region.getregionname());     if (parents == null || parents.isempty()) {       // it doesn't have merge qualifier, no need to clean       return true;     }     return cleanmergeregion(region, parents);   } public static list<regioninfo> getmergeregions(connection connection, byte[] regionname)       throws ioexception {     return getmergeregions(getmergeregionsraw(connection, regionname));   } private static cell [] getmergeregionsraw(connection connection, byte [] regionname)       throws ioexception {     scan scan = new scan().withstartrow(regionname).         setonerowlimit().         readversions(1).         addfamily(hconstants.catalog_family).         setfilter(new qualifierfilter(compareoperator.equal,           new regexstringcomparator(hconstants.merge_qualifier_prefix_str+ \".*\")));     try (table m = getmetahtable(connection); resultscanner scanner = m.getscanner(scan)) {       // should be only one result in this scanner if any.       result result = scanner.next();       if (result == null) {         return null;       }       // should be safe to just return all cells found since we had filter in place.       // all values should be regioninfos or something wrong.       return result.rawcells();     }   } ",
        "label": 187
    },
    {
        "text": "client stuck in treemap remove  testing 0.20_pre_durability@934691 my client got permanently stuck with one thread looping inside treemap.remove. see attached stack. ",
        "label": 453
    },
    {
        "text": "startup stuck  waiting for root region   seems easy to reproduce. i am trying to start a master then start a regionserver. i'm using local filesystem. i start master like this: ./bin/hbase master start the regionserver similarly. all comes up. then i try to run sequentialwriter from pe. here is what shows in master log: ... 08/10/27 15:20:00 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:02 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:04 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:06 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:06 info master.basescanner: regionmanager.rootscanner scanning meta region {regionname: -root-,,0, startkey: <>, server: 208.84.6.64:60020} 08/10/27 15:20:07 debug master.basescanner: regionmanager.rootscanner region => {name => '.meta.,,1', startkey => '', endkey => '', encoded => 1028785192, table => {{name => '.meta.', is_root => 'false', is_meta => 'true', families => [{name => 'info', bloomfilter => 'false', versions => '1', compression => 'none', length => '2147483647', ttl => '-1', in_memory => 'false', blockcache => 'false'}, {name => 'historian', bloomfilter => 'false', versions => '2147483647', compression => 'none', length => '2147483647', ttl => '-1', in_memory => 'false', blockcache => 'false'}]}}}, server => '208.84.6.64:60020', startcode => 1225145942330 08/10/27 15:20:07 info master.basescanner: regionmanager.rootscanner scan of meta region {regionname: -root-,,0, startkey: <>, server: 208.84.6.64:60020} complete 08/10/27 15:20:08 debug client.hconnectionmanager$tableservers: attempt 0 of 10 failed with <org.apache.hadoop.hbase.client.noserverforregionexception: timed out trying to locate root region>. retrying after sleep of 2000 08/10/27 15:20:08 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:10 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. ... here is the regionserver log: .... 08/10/27 15:19:09 debug regionserver.compactsplitthread: compaction requested for region: .meta.,,1 08/10/27 15:19:09 info regionserver.hregion: starting compaction on region .meta.,,1 08/10/27 15:19:09 info regionserver.hregion: compaction completed on region .meta.,,1 in 0sec 08/10/27 15:19:48 info regionserver.hregionserver: msg_region_open: testtable,,1225145988632 08/10/27 15:19:48 info regionserver.hregionserver: msg_region_open: testtable,,1225145988632 08/10/27 15:19:48 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:19:50 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:19:52 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:19:54 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:19:56 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:19:58 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:00 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:02 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:04 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:07 debug client.hconnectionmanager$tableservers: sleeping 2000ms, waiting for root region. 08/10/27 15:20:09 debug client.hconnectionmanager$tableservers: attempt 0 of 10 failed with <org.apache.hadoop.hbase.client.noserverforregionexception: timed out trying to locate root region>. retrying after sleep of 2000 ... and so on. giving to jim since he was hereabouts most recently. ",
        "label": 241
    },
    {
        "text": "show table throttle quotas in table jsp  currently, table jsp shows space quotas but has no throttle quotas. ",
        "label": 500
    },
    {
        "text": "xss in the webui  there are possibilities of xss in the webui. if columnfamily or region splitting keys are like bytes.tobytes(\"<script>alert('js')</script>\") then browsers run the javascript code.  i tested on hbase-0.90.0 . ",
        "label": 290
    },
    {
        "text": "upgrade hadoop dependency to hadoop  ",
        "label": 441
    },
    {
        "text": "hbaseadmin testtableexists can go zombie  see it here as a zombie in hadoopqa: https://builds.apache.org/job/precommit-hbase-build/6687/consoletext looking at it, we seem stuck in here: \"rpcserver.handler=1,port=51776\" daemon prio=10 tid=0x72001400 nid=0x17ea waiting on condition [0x71cd4000]    java.lang.thread.state: timed_waiting (sleeping) at java.lang.thread.sleep(native method) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:150) - locked <0x81042070> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.htable.get(htable.java:732) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:111) - locked <0x7f71ba70> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3076) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1779) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1820) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:26698) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2068) at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1807) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:113) at java.lang.thread.run(thread.java:662) this lock is held: 0x7f71ba70 we are doing retries against the new ns table. a bunch other threads are trying to get in here while we are retrying: \"rpcserver.handler=0,port=51776\" daemon prio=10 tid=0x72000400 nid=0x17e9 waiting for monitor entry [0x71d25000]    java.lang.thread.state: blocked (on object monitor) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:111) - waiting to lock <0x7f71ba70> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3076) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1779) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1820) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:26698) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2068) at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1807) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:113) at java.lang.thread.run(thread.java:662) ... \"rpcserver.handler=4,port=51776\" daemon prio=10 tid=0x72cc9000 nid=0x17ed waiting for monitor entry [0x71be1000]    java.lang.thread.state: blocked (on object monitor) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:111) - waiting to lock <0x7f71ba70> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3076) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1779) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1820) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:26698) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2068) at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1807) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:113) at java.lang.thread.run(thread.java:662) \"rpcserver.handler=3,port=51776\" daemon prio=10 tid=0x72cc7800 nid=0x17ec waiting for monitor entry [0x71c32000]    java.lang.thread.state: blocked (on object monitor) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:111) - waiting to lock <0x7f71ba70> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3076) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1779) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1820) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:26698) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2068) at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1807) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:113) at java.lang.thread.run(thread.java:662) \"rpcserver.handler=2,port=51776\" daemon prio=10 tid=0x72002c00 nid=0x17eb waiting for monitor entry [0x71c83000]    java.lang.thread.state: blocked (on object monitor) at org.apache.hadoop.hbase.master.tablenamespacemanager.get(tablenamespacemanager.java:111) - waiting to lock <0x7f71ba70> (a org.apache.hadoop.hbase.master.tablenamespacemanager) at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3076) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1779) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1820) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:26698) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2068) at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1807) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:113) at java.lang.thread.run(thread.java:662) i'd guess no one is getting in here till we finish our 35 retryings (almost ten minutes, which makes us look like a zombie). seems like we need to be able to interrupt in here when done or at least add logging why we are in here having trouble trying to get from the ns table? ",
        "label": 314
    },
    {
        "text": "metricsregionsourceimpl creates metrics that start with a lower case  noticed something while looking at mbeans; some metric names are capitalized while others aren't. this has the unwanted property of changing the metric names sorting. for example: namespace_default_table_t_region_0122d54a90c9dc191857daf11e8a047d_metric_scannext_num_ops  namespace_default_table_t_region_0122d54a90c9dc191857daf11e8a047d_metric_appendcount the reason for this is that mutablehistogram uses stringutils.capitalize() on the metrics name (as create by metricsregionsourceimpl with \"namespace_\"). i'm not sure what the intent was with capitalization, but either way it's a simple fix. elliott neil clark? ",
        "label": 229
    },
    {
        "text": "typo in javadoc of aggregationclient rowcount   firstkeyvaluefilter -> firstkeyvaluefilter ",
        "label": 248
    },
    {
        "text": "the whilematchfilter doesn't delegate the call to filterrow   while testing the recent memstorescanner slowness i noticed that each scan in the randomseekscan test takes about 19 seconds to complete. the scan in question provides a startrow and a whilematchfilter containing a pagefilter that asks for 120 rows. i would have expected this scan to return in roughly the same amount of time as a scan that specifies a startrow and stoprow that spans a similar number of rows. as it turns out this is an issue with the whilematchfilter. the whilematchfilter is not delegating the call the filterrow() down the the pagefilter. as a result the pagefilter never increments the rowsaccepted counter. ",
        "label": 314
    },
    {
        "text": "bypassing default actions in preput fails sometimes with htable client  while testing some other scenario i found calling coprocessorenvironment.bypass() fails if all trailing puts in a batch are bypassed that way. by extension a single bypassed put will also fail. the problem is that the puts are removed from the batch in a way that does not align them with the result-status, and in addition the result is never marked as success. a possible fix is to just mark bypassed puts as success and filter them in the following logic.  (i also contemplated a new bypassed operationstatuscode, but that turned out to be not necessary). ",
        "label": 286
    },
    {
        "text": "get rid of hbase hstore compaction complete setting  hbase.hstore.compaction.complete is a strange setting that causes the finished compaction to not complete (files are just left in tmp) in hstore.   it's used by one test.  the setting with the same name is also used by compactiontool, but that usage is semi-unrelated and could probably be removed easily. ",
        "label": 281
    },
    {
        "text": "fix all dead links in our hbase book  see here to find the dead links. ",
        "label": 509
    },
    {
        "text": "usage of filterlist slows down scans  when using a filterlist the scan is much slower compared to a scan with only a single filter (tested singlecolumnvaluefilter and prefixfilter). the difference is extrem for very small ranges: if the range is only 10 rows the scan is 10 times slower when using the filterlist. is the cause just gc or object serialization/deserialization? for a simple test i used the performanceevaluation tool and created the testtable with only 10 rows:  $ bin/hbase org.apache.hadoop.hbase.performanceevaluation --rows=10 sequentialwrite 1 the attached test performs 100 scan using diffent filters. the filter is then wrapped into a filterlist (with only a single filter). this filterlist is then nested two more times into another filterlist. for each nested level the scan gets slower and slower. the test created the following output: scan null filter (10): 391ms  scan filterlist with null filter (0): 4788ms  scan nested filterlist with null filter (0): 8303ms  scan nested nested filterlist with null filter (0): 11915ms scan singlecolumvaluefilter equal (0): 257ms  scan filterlist with singlecolumvaluefilter equal (0): 4121ms  scan nested filterlist with singlecolumvaluefilter equal (0): 7965ms  scan nested nested filterlist with singlecolumvaluefilter equal (0): 11600ms scan singlecolumvaluefilter not equal (10): 912ms  scan filterlist with singlecolumvaluefilter not equal (10): 4542ms  scan nested filterlist with singlecolumvaluefilter not equal (10): 8459ms  scan nested nested filterlist with singlecolumvaluefilter not equal (10): 11513ms scan prefixfilter (10): 306ms  scan filterlist with prefixfilter (10): 3695ms  scan nested filterlist with prefixfilter (10): 7762ms  scan nested nested filterlist with prefixfilter (10): 11721ms get: 245ms ",
        "label": 229
    },
    {
        "text": "hbaseadmin deletetable should relocate meta when cached location is stale  after hbase-14275, in hbaseadmin#deletetable, when using metareader to wait until all regions are deleted, we won't attempt to relocate meta should its cached location be stale. ",
        "label": 38
    },
    {
        "text": "client compatibility breaks between versions and  opentsdb uses asynchbase as its client, rather than using the traditional hbase client. from version 1.2 to 1.3, the clientprotos have been changed. newer fields are added to scanresponse proto. for a typical scan request in 1.2, would require caller to make an openscanner request, getnextrows request and a closescanner request, based on more_rows boolean field in the scanresponse proto. however, from 1.3, new parameter more_results_in_region was added, which limits the results per region. therefore the client has to now manage sending all the requests for each region. further more, if the results are exhausted from a particular region, the scanresponse will set more_results_in_region to false, but more_results can still be true. whenever the former is set to false, the regionscanner will also be closed. opentsdb makes an openscanner request and receives all its results in the first scanresponse itself, thus creating a condition as described in above paragraph. since more_rows is true, it will proceed to send next request at which point the rsrpcservices will throw unknownscannerexception. the protobuf client compatibility is maintained but expected behavior is modified. ",
        "label": 149
    },
    {
        "text": "make regionserverservices and masterservices extend server  see hbase-3698 and discussion \"hbase-2001 added getzookeeperwatcher and getcatalogtracker to regionserverservices but these two methods are in server\" up on dev list. ",
        "label": 162
    },
    {
        "text": "npe when altering a table that has moving regions  i'm still not a 100% sure on the source of this error, but here's what i was able to get twice while altering a table that was doing a bunch of splits: 2011-10-11 23:48:59,344 info org.apache.hadoop.hbase.master.handler.splitregionhandler: handled split report); parent=testtable,0002608338,1318376880454.a75d6815fdfc513fb1c8aabe086c6763. daughter a=testtable,0002608338,1318376938764.ef170ff6cd8695dc8aec92e542dc9ac1.daughter b=testtable,0003301408,1318376938764.36eb2530341bd46888ede312c5559b5d.  2011-10-11 23:49:09,579 debug org.apache.hadoop.hbase.master.handler.tableeventhandler: ignoring table not disabled exception for supporting online schema changes.  2011-10-11 23:49:09,580 info org.apache.hadoop.hbase.master.handler.tableeventhandler: handling table operation c_m_modify_table on table testtable  2011-10-11 23:49:09,612 info org.apache.hadoop.hbase.util.fsutils: tableinfopath = hdfs://sv4r11s38:9100/hbase/testtable/.tableinfo tmppath = hdfs://sv4r11s38:9100/hbase/testtable/.tmp/.tableinfo.1318376949612  2011-10-11 23:49:09,692 info org.apache.hadoop.hbase.util.fsutils: tabledescriptor stored. tableinfopath = hdfs://sv4r11s38:9100/hbase/testtable/.tableinfo  2011-10-11 23:49:09,693 info org.apache.hadoop.hbase.util.fsutils: updated tableinfo=hdfs://sv4r11s38:9100/hbase/testtable/.tableinfo to blah  2011-10-11 23:49:09,695 info org.apache.hadoop.hbase.master.handler.tableeventhandler: bucketing regions by region server...  2011-10-11 23:49:09,695 debug org.apache.hadoop.hbase.client.metascanner: scanning .meta. starting at row=testtable,,00000000000000 for max=2147483647 rows  2011-10-11 23:49:09,709 debug org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation: the connection to hconnection-0x132f043bbde02e9 has been closed.  2011-10-11 23:49:09,709 error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event c_m_modify_table  java.lang.nullpointerexception  at java.util.treemap.getentry(treemap.java:324)  at java.util.treemap.containskey(treemap.java:209)  at org.apache.hadoop.hbase.master.handler.tableeventhandler.reopenallregions(tableeventhandler.java:114)  at org.apache.hadoop.hbase.master.handler.tableeventhandler.process(tableeventhandler.java:90)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:168)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) the first time the shell reported that all the regions were updated correctly, the second time it got stuck for a while: 6/14 regions updated.  0/14 regions updated.  ...  0/14 regions updated.  2/16 regions updated.  ...  2/16 regions updated.  8/9 regions updated.  ...  8/9 regions updated. after which i killed it, redid the alter and it worked. ",
        "label": 529
    },
    {
        "text": "expose sleep time as a command line argument of intergationtestbackuprestore  extend command line arguments of intergationtestbackuprestore with a sleep time of chaos monkey options to be able to setup policy of region server restarts more granularly. ",
        "label": 438
    },
    {
        "text": "minizookeepercluster startup  should refer to hbase zookeeper property maxclientcnxns  currently the number of the client connections is hard-wired to 1000:         standaloneserverfactory = new nioservercnxnfactory();         standaloneserverfactory.configure(new inetsocketaddress(clientport),1000);       } catch (bindexception e) {   this should be set according to the test environment's hbase configuration. the property in   question is : hbase.zookeeper.property.maxclientcnxns. currently some tests such as org.apache.hadoop.hbase.client.testhcm fail because the number of connections used by the hbase client exceeds 1000. recently max_cached_hbase_instances increased from 31 to 2000 on 0.90 branch: http://svn.apache.org/viewvc/hbase/branches/0.90/src/main/java/org/apache/hadoop/hbase/client/hconnectionmanager.java?p2=%2fhbase%2fbranches%2f0.90%2fsrc%2fmain%2fjava%2forg%2fapache%2fhadoop%2fhbase%2fclient%2fhconnectionmanager.java&p1=%2fhbase%2fbranches%2f0.90%2fsrc%2fmain%2fjava%2forg%2fapache%2fhadoop%2fhbase%2fclient%2fhconnectionmanager.java&r1=1096818&r2=1096817&view=diff&pathrev=1096818 and correspondingly the hbase config on the zookeeper server-side also increased in hbase-default.xml: http://svn.apache.org/viewvc/hbase/branches/0.90/src/main/resources/hbase-default.xml?p2=%2fhbase%2fbranches%2f0.90%2fsrc%2fmain%2fresources%2fhbase-default.xml&p1=%2fhbase%2fbranches%2f0.90%2fsrc%2fmain%2fresources%2fhbase-default.xml&r1=1091594&r2=1091593&view=diff&pathrev=1091594 so if minizkcluster looks at this setting, the test won't have this failure. ",
        "label": 164
    },
    {
        "text": "reduce allocation of objects in metrics  we use jmx and o.a.h.metrics2 to do some metrics on regions, tables, region servers and cluster. we use metricsinfo to show the information of metrics, and we use interns to cache metricsinfo objects because it won't be changed. however, in interns there are some static values to limit the max cached objects. we can only cache 2010 metrics, but we have dozens of metrics for one region and we have some rs-level metrics in each rs and all metrics for all regions will be saved in master. so each server will have thousands of metrics, and we can not cache most of them. when we collect metrics by jmx, we will create many objects which can be avoid. it increases the pressure of gc and jmx has some caching logic so the objects can not be removed immediately which increases the pressure more. interns is in hadoop project, and i think the implementation is not suitable for hbase. because we can not know how many metricsinfo we have, it depends on the number of regions. and we can not set it unlimited because we should remove the objects whose region is split, moved, or dropped. i think we can use guava's cache with expireafteraccess which is very simple and convenient. so we can add a new interns class in hbase project first, and put it to upstream later. moreover, in mutablehistogram#snapshot we create same strings each time, we can create them only in the first time. ",
        "label": 353
    },
    {
        "text": "port hbase hbase remote copytable not working when security enabled to trunk  excerpt about the choice of solution from : the first option was actually quite messy to implement. clusterid and conf are fixed in hbaseclient when it's created and cached by securerpcengine, so to implement the fix here i would have had to pass the different cluster confs up through hconnectionmanager and hbaserpc in order to override the clusterid in secureclient#secureconnection. i've gone with the second option of creating and caching different secureclients for the local and remote clusters in securerpcengine - keyed off of the clusterid instead of the default socketfactory. i think this is a cleaner solution. ",
        "label": 180
    },
    {
        "text": "hlog sequence number is obtained outside updatelock  in one of the overloads of hlog.append, obtainseqnum() is called before this.updatelock is acquired. this means that it's possible for some other thread to have a higher sequence number but achieve its write first, and hence the lastseqwritten map can become incorrect. ",
        "label": 453
    },
    {
        "text": "expose getclosestrowbefore in htable  exposes the functionality of getclosestrowbefore in htable. it can be useful in combination with atomic check and save in the following manner to create a global auto-id creation scheme: presteps:  1. in your application, have a reserved value.  2. create a reverse index table of your main table (best algorithm in my opinion is bitwise-not of key and append max-utf8 character)  3. have an offset row in said reverse index table that your application does not allow to be set (usually this will be a bitwise inverse of some prefix + maximum utf-8 character) algorithm:  1. use getclosestrowbefore to find the last created row before the offset (which will be the last row created)  2. deinverse key and then increment the row key of the row returned by getclosestrowbefore  3. attempt to reserve the row key with checkandsave checking if reserved has been set.  4. if it fails try again. attached is a patch to expose getclosestrowbefore. comments are of course always welcome =). ",
        "label": 313
    },
    {
        "text": "get rid of hcolumndescriptor mapfile index interval   now that we have hfile, it seems that the constant http://hadoop.apache.org/hbase/docs/r0.20.4/api/org/apache/hadoop/hbase/hcolumndescriptor.html#mapfile_index_interval is no longer useful. i've grepped around the codebase and couldn't find it used elsewhere. perhaps kept around in case folks decide to store their hbase data into a mapfile? ",
        "label": 247
    },
    {
        "text": "stochastic load balancer assigns replica regions to the same rs  trying out region replica and its assignment i can see that some times the default lb stocahstic load balancer assigns replica regions to the same rs. this happens when we have 3 rs checked in and we have a table with 3 replicas. when a rs goes down then the replicas being assigned to same rs is acceptable but the case when we have enough rs to assign this behaviour is undesirable and does not solve the purpose of replicas.   hua xiang and enis soztutar. ",
        "label": 314
    },
    {
        "text": "provide backward compatibility for hbase coprocessor service names  attempting to run a map reduce job with a 1.3 client on a secure cluster running 1.2 is failing when making the coprocessor rpc to obtain a delegation token: exception in thread \"main\" org.apache.hadoop.hbase.exceptions.unknownprotocolexception: org.apache.hadoop.hbase.exceptions.unknownprotocolexception: no registered coprocessor service found for name hbase.pb.authenticationservice in region hbase:meta,,1         at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:7741)         at org.apache.hadoop.hbase.regionserver.rsrpcservices.execserviceonregion(rsrpcservices.java:1988)         at org.apache.hadoop.hbase.regionserver.rsrpcservices.execservice(rsrpcservices.java:1970)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:33652)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2170)         at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:109)         at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:137)         at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:112)         at java.lang.thread.run(thread.java:745)         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)         at java.lang.reflect.constructor.newinstance(constructor.java:422)         at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:106)         at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:95)         at org.apache.hadoop.hbase.protobuf.protobufutil.getremoteexception(protobufutil.java:332)         at org.apache.hadoop.hbase.protobuf.protobufutil.execservice(protobufutil.java:1631)         at org.apache.hadoop.hbase.ipc.regioncoprocessorrpcchannel$1.call(regioncoprocessorrpcchannel.java:104)         at org.apache.hadoop.hbase.ipc.regioncoprocessorrpcchannel$1.call(regioncoprocessorrpcchannel.java:94)         at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:137)         at org.apache.hadoop.hbase.ipc.regioncoprocessorrpcchannel.callexecservice(regioncoprocessorrpcchannel.java:108)         at org.apache.hadoop.hbase.ipc.coprocessorrpcchannel.callblockingmethod(coprocessorrpcchannel.java:73)         at org.apache.hadoop.hbase.protobuf.generated.authenticationprotos$authenticationservice$blockingstub.getauthenticationtoken(authenticationprotos.java:4512)         at org.apache.hadoop.hbase.security.token.tokenutil.obtaintoken(tokenutil.java:86)         at org.apache.hadoop.hbase.security.token.tokenutil$1.run(tokenutil.java:111)         at org.apache.hadoop.hbase.security.token.tokenutil$1.run(tokenutil.java:108)         at java.security.accesscontroller.doprivileged(native method)         at javax.security.auth.subject.doas(subject.java:422)         at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1657)         at org.apache.hadoop.hbase.security.user$securehadoopuser.runas(user.java:340)         at org.apache.hadoop.hbase.security.token.tokenutil.obtaintoken(tokenutil.java:108)         at org.apache.hadoop.hbase.security.token.tokenutil.addtokenforjob(tokenutil.java:329)         at org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.initcredentials(tablemapreduceutil.java:490)         at org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.inittablemapperjob(tablemapreduceutil.java:209)         at org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.inittablemapperjob(tablemapreduceutil.java:162)         at org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.inittablemapperjob(tablemapreduceutil.java:285)         at org.apache.hadoop.hbase.mapreduce.tablemapreduceutil.inittablemapperjob(tablemapreduceutil.java:86)         at org.apache.hadoop.hbase.mapreduce.cellcounter.createsubmittablejob(cellcounter.java:193)         at org.apache.hadoop.hbase.mapreduce.cellcounter.main(cellcounter.java:290) caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception(org.apache.hadoop.hbase.exceptions.unknownprotocolexception): org.apache.hadoop.hbase.exceptions.unknownprotocolexception: no registered coprocessor service found for name hbase.pb.authenticationservice in region hbase:meta,,1         at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:7741)         at org.apache.hadoop.hbase.regionserver.rsrpcservices.execserviceonregion(rsrpcservices.java:1988)         at org.apache.hadoop.hbase.regionserver.rsrpcservices.execservice(rsrpcservices.java:1970)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:33652)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2170)         at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:109)         at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:137)         at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:112)         at java.lang.thread.run(thread.java:745)         at org.apache.hadoop.hbase.ipc.rpcclientimpl.call(rpcclientimpl.java:1270)         at org.apache.hadoop.hbase.ipc.abstractrpcclient.callblockingmethod(abstractrpcclient.java:226)         at org.apache.hadoop.hbase.ipc.abstractrpcclient$blockingrpcchannelimplementation.callblockingmethod(abstractrpcclient.java:331)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$blockingstub.execservice(clientprotos.java:35420)         at org.apache.hadoop.hbase.protobuf.protobufutil.execservice(protobufutil.java:1628)         ... 22 more this is talking to a 1.2 server. running with a 1.2 client works. i believe this is due to hbase-14077, where we added package names to the protobuf files. this is causing 1.2 and 1.3 to disagree on the name of the authenticationservice: 1.2 = authenticationservice 1.3 = hbase.pb.authenticationservice i think this is effectively a break in client-server wire compatibility between 1.2 and 1.3, since this is calling a built-in coprocessor required for security. ",
        "label": 180
    },
    {
        "text": "miscellaneous hbck report page cleanup  a bunch of touch up on the hbck report page: add a bit of javadoc around serialreplicationchecker. miniscule edit to the profiler jsp page and then a bit of doc on how to make it work that might help. add some detail if npe getting bitsetnode to help w/ debug. change hbckchore to log region names instead of encoded names; helps doing diagnostics; can take region name and query in shell to find out all about the region according to hbase:meta. add some fix-it help inline in the hbck report page \u2013 how to fix. add counts in procedures page so can see if making progress; move listing of wals to end of the page. ",
        "label": 314
    },
    {
        "text": "enable cleaners required for snapshots by default  currently, snapshots require admins to add configuration to their hbase-site.xml to have snapshot functionality available. it is at the moment off by default.  <property>     <name>hbase.snapshot.enabled</name>     <value>true</value>   </property> maybe we should just enable snapshots by default. discuss. ",
        "label": 441
    },
    {
        "text": "nightly fails rat check down in the dev support hbase nightly source artifact sh check  nightlies include a nice check that runs through the rc-making steps. see dev-support/hbase_nightly_source-artifact.sh. currently the nightly is failing here which is causing the nightly runs fail though often enough all tests pass. it looks like cause is the rat check. unfortunately, running the nightly script locally, all comes up smelling sweet \u2013 its a context thing. ",
        "label": 314
    },
    {
        "text": "allow hbase version to be passed in as command line argument  currently the build always produces the jars and tarball according to the version baked into the pom.  when we modify this to allow the version to be passed in as a command-line argument, it can still default to the same behavior, yet give the flexibility for an internal build to tag on own version. ",
        "label": 245
    },
    {
        "text": "fix a minor bug in shell command with clone snapshot table error  in hbase shell, the syntax for clone_snapshot is:  hbase> clone_snapshot 'snapshotname', 'tablename' if the target table already exists, we'll get an error. for example:  ------------------  hbase(main):011:0> clone_snapshot 'mysnapshot1', 'testtable' error: table already exists: mysnapshot1! here is some help for this command:  create a new table by cloning the snapshot content.  there're no copies of data involved.  and writing on the newly created table will not influence the snapshot data. examples:  hbase> clone_snapshot 'snapshotname', 'tablename'  ---------------------- the bug is in the error message:  error: table already exists: mysnapshot1! we should output the table name, not the snapshot name. currently, in command.rb, we have the output fixed as args.first for tableexistsexception:       def translate_hbase_exceptions(*args)         yield       rescue org.apache.hadoop.hbase.exceptions.tablenotfoundexception         raise \"unknown table #{args.first}!\"       rescue org.apache.hadoop.hbase.exceptions.nosuchcolumnfamilyexception         valid_cols = table(args.first).get_all_columns.map { |c| c + '*' }         raise \"unknown column family! valid column names: #{valid_cols.join(\", \")}\"       rescue org.apache.hadoop.hbase.exceptions.tableexistsexception         raise \"table already exists: #{args.first}!\"       end this is fine with commands like 'create tablename ...' but not 'clone_snapshot snapshotname tablename'. ",
        "label": 441
    },
    {
        "text": "timeout monitor races with table disable handler  here is what j-d described here:  https://issues.apache.org/jira/browse/hbase-5119?focusedcommentid=13179176&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13179176 i think i will retract from my statement that it \"used to be extremely racy and caused more troubles than it fixed\", on my first test i got a stuck region in transition instead of being able to recover. the timeout was set to 2 minutes to be sure i hit it. first the region gets closed 2012-01-04 00:16:25,811 debug org.apache.hadoop.hbase.master.assignmentmanager: sent close to sv4r5s38,62023,1325635980913 for region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. 2 minutes later it times out: 2012-01-04 00:18:30,026 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=pending_close, ts=1325636185810, server=null  2012-01-04 00:18:30,026 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_close for too long, running forced unassign again on region=test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.  2012-01-04 00:18:30,027 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. (offlining) 100ms later the master finally gets the event: 2012-01-04 00:18:30,129 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_closed, server=sv4r5s38,62023,1325635980913, region=1a4b111bcc228043e89f59c4c3f6a791, which is more than 15 seconds late  2012-01-04 00:18:30,129 debug org.apache.hadoop.hbase.master.handler.closedregionhandler: handling closed event for 1a4b111bcc228043e89f59c4c3f6a791  2012-01-04 00:18:30,129 debug org.apache.hadoop.hbase.master.assignmentmanager: table being disabled so deleting zk node and removing from regions in transition, skipping assignment of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.  2012-01-04 00:18:30,129 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x134589d3db03587 deleting existing unassigned node for 1a4b111bcc228043e89f59c4c3f6a791 that is in expected state rs_zk_region_closed  2012-01-04 00:18:30,166 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x134589d3db03587 successfully deleted unassigned node for region 1a4b111bcc228043e89f59c4c3f6a791 in expected state rs_zk_region_closed at this point everything is fine, the region was processed as closed. but wait, remember that line where it said it was going to force an unassign? 2012-01-04 00:18:30,322 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x134589d3db03587 creating unassigned node for 1a4b111bcc228043e89f59c4c3f6a791 in a closing state  2012-01-04 00:18:30,328 info org.apache.hadoop.hbase.master.assignmentmanager: server null returned java.lang.nullpointerexception: passed server is null for 1a4b111bcc228043e89f59c4c3f6a791 now the master is confused, it recreated the rit znode but the region doesn't even exist anymore. it even tries to shut it down but is blocked by npes. now this is what's going on. the late zk notification that the znode was deleted (but it got recreated after): 2012-01-04 00:19:33,285 debug org.apache.hadoop.hbase.master.assignmentmanager: the znode of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. has been deleted. then it prints this, and much later tries to unassign it again: 2012-01-04 00:19:46,607 debug org.apache.hadoop.hbase.master.handler.deletetablehandler: waiting on region to clear regions in transition; test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=pending_close, ts=1325636310328, server=null  ...  2012-01-04 00:20:39,623 debug org.apache.hadoop.hbase.master.handler.deletetablehandler: waiting on region to clear regions in transition; test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=pending_close, ts=1325636310328, server=null  2012-01-04 00:20:39,864 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. state=pending_close, ts=1325636310328, server=null  2012-01-04 00:20:39,864 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_close for too long, running forced unassign again on region=test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791.  2012-01-04 00:20:39,865 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. (offlining)  2012-01-04 00:20:39,865 debug org.apache.hadoop.hbase.master.assignmentmanager: attempted to unassign region test1,089cd0c9,1325635015491.1a4b111bcc228043e89f59c4c3f6a791. but it is not currently assigned anywhere and this is still ongoing. ",
        "label": 544
    },
    {
        "text": "majorcompaction may affect scan's correctness  in our test, there are two families' keyvalue for one row. but we could find a infrequent problem when doing scan's next if majorcompaction happens concurrently.  in the client's two continuous doing scan.next():  1.first time, scan's next returns the result where family a is null.  2.second time, scan's next returns the result where family b is null.  the two next()'s result have the same row. if there are more families, i think the scenario will be more strange... we find the reason is that storescanner.peek() is changed after majorcompaction if there are delete type keyvalue.  this change causes the priorityqueue<keyvaluescanner> of regionscanner's heap is not sure to be sorted. ",
        "label": 107
    },
    {
        "text": "table mutation operations should check table level rights  not global rights  drop/modify/disable/enable etc table operations should not check for global create/admin rights, but table create/admin rights. since we check for global permissions first for table permissions, configuring table access using global permissions will continue to work. ",
        "label": 287
    },
    {
        "text": "testnamespacesinstanceresource fails  this is the top flaky test.  the following can be reproduced: java.net.socketexception: connection reset at org.apache.hadoop.hbase.rest.testnamespacesinstanceresource.testinvalidnamespacepostsandputs(testnamespacesinstanceresource.java:271) with commit e320df5a0c267258c03909da8d0eee4c0e287532, the test passes.  with commit 5facaded902a13556952b1f9d26b768cb86e6599, the test fails. ",
        "label": 441
    },
    {
        "text": "npe in regionreplicareplicationendpoint  https://builds.apache.org/job/hbase-flaky-tests/job/hbase-21512/1197/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.replication.regionserver.testregionreplicareplicationendpoint-output.txt/*view*/ 2019-06-07 20:54:10,610 error [rs-eventloopgroup-4-17] util.futureutils(70): unexpected error caught when processing completablefuture java.lang.nullpointerexception at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.replicate(regionreplicareplicationendpoint.java:207) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.lambda$replicate$2(regionreplicareplicationendpoint.java:249) at org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) at java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) at java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) at java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) at java.util.concurrent.completablefuture.complete(completablefuture.java:1962) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.lambda$getregionlocations$0(regionreplicareplicationendpoint.java:161) at org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) at java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) at java.util.concurrent.completablefuture.uniwhencompletestage(completablefuture.java:778) at java.util.concurrent.completablefuture.whencomplete(completablefuture.java:2140) at org.apache.hadoop.hbase.util.futureutils.addlistener(futureutils.java:61) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.getregionlocations(regionreplicareplicationendpoint.java:153) at org.apache.hadoop.hbase.replication.regionserver.regionreplicareplicationendpoint.lambda$getregionlocations$0(regionreplicareplicationendpoint.java:173) at org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) at java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) at java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) at java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) at java.util.concurrent.completablefuture.complete(completablefuture.java:1962) at org.apache.hadoop.hbase.client.asyncnonmetaregionlocator$tablecache.trycomplete(asyncnonmetaregionlocator.java:186) at org.apache.hadoop.hbase.client.asyncnonmetaregionlocator$tablecache.clearcompletedrequests(asyncnonmetaregionlocator.java:152) at org.apache.hadoop.hbase.client.asyncnonmetaregionlocator.complete(asyncnonmetaregionlocator.java:289) at org.apache.hadoop.hbase.client.asyncnonmetaregionlocator.onscannext(asyncnonmetaregionlocator.java:348) at org.apache.hadoop.hbase.client.asyncnonmetaregionlocator.access$600(asyncnonmetaregionlocator.java:71) at org.apache.hadoop.hbase.client.asyncnonmetaregionlocator$1.onnext(asyncnonmetaregionlocator.java:469) at org.apache.hadoop.hbase.client.asyncscansingleregionrpcretryingcaller.oncomplete(asyncscansingleregionrpcretryingcaller.java:513) at org.apache.hadoop.hbase.client.asyncscansingleregionrpcretryingcaller.start(asyncscansingleregionrpcretryingcaller.java:605) at org.apache.hadoop.hbase.client.asyncrpcretryingcallerfactory$scansingleregioncallerbuilder.start(asyncrpcretryingcallerfactory.java:304) at org.apache.hadoop.hbase.client.asyncclientscanner.startscan(asyncclientscanner.java:174) at org.apache.hadoop.hbase.client.asyncclientscanner.lambda$openscanner$2(asyncclientscanner.java:212) at org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) at java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) at java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) at java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) at java.util.concurrent.completablefuture.complete(completablefuture.java:1962) at org.apache.hadoop.hbase.client.asyncsinglerequestrpcretryingcaller.lambda$call$4(asyncsinglerequestrpcretryingcaller.java:90) at org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) at java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) at java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) at java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) at java.util.concurrent.completablefuture.complete(completablefuture.java:1962) at org.apache.hadoop.hbase.client.asyncclientscanner.lambda$callopenscanner$0(asyncclientscanner.java:157) at org.apache.hbase.thirdparty.com.google.protobuf.rpcutil$1.run(rpcutil.java:79) at org.apache.hbase.thirdparty.com.google.protobuf.rpcutil$1.run(rpcutil.java:70) at org.apache.hadoop.hbase.ipc.abstractrpcclient.oncallfinished(abstractrpcclient.java:397) at org.apache.hadoop.hbase.ipc.abstractrpcclient.access$100(abstractrpcclient.java:97) at org.apache.hadoop.hbase.ipc.abstractrpcclient$3.run(abstractrpcclient.java:423) at org.apache.hadoop.hbase.ipc.abstractrpcclient$3.run(abstractrpcclient.java:419) at org.apache.hadoop.hbase.ipc.call.callcomplete(call.java:103) at org.apache.hadoop.hbase.ipc.call.setresponse(call.java:135) at org.apache.hadoop.hbase.ipc.nettyrpcduplexhandler.readresponse(nettyrpcduplexhandler.java:184) at org.apache.hadoop.hbase.ipc.nettyrpcduplexhandler.channelread(nettyrpcduplexhandler.java:192) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:359) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:345) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:337) at org.apache.hbase.thirdparty.io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:323) at org.apache.hbase.thirdparty.io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:297) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:359) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:345) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:337) at org.apache.hbase.thirdparty.io.netty.handler.timeout.idlestatehandler.channelread(idlestatehandler.java:286) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:359) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:345) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:337) at org.apache.hbase.thirdparty.io.netty.channel.defaultchannelpipeline$headcontext.channelread(defaultchannelpipeline.java:1408) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:359) at org.apache.hbase.thirdparty.io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:345) at org.apache.hbase.thirdparty.io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:930) at org.apache.hbase.thirdparty.io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:796) at org.apache.hbase.thirdparty.io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:427) at org.apache.hbase.thirdparty.io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:328) at org.apache.hbase.thirdparty.io.netty.util.concurrent.singlethreadeventexecutor$5.run(singlethreadeventexecutor.java:905) at org.apache.hbase.thirdparty.io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) at java.lang.thread.run(thread.java:748) ",
        "label": 149
    },
    {
        "text": "reading datablock throws  invalid hfile block magic  and can not switch to hdfs checksum  when using hbase checksum,call readblockdatainternal() in hfileblock.java, it could happen file corruption but it only can switch to hdfs checksum inputstream till validateblockchecksum(). if the datablock's header corrupted when b = new hfileblock(),it throws the exception \"invalid hfile block magic\" and the rpc call fail ",
        "label": 48
    },
    {
        "text": "can't recover from a dead root server if any exceptions happens during log splitting  there's an almost easy way to get stuck after a rs holding root dies, usually from a gc-like event. it happens frequently to my testreplication in hbase-2223. some logs: 2010-06-10 11:35:52,090 info  [master] wal.hlog(1175): spliting is done. removing old log dir hdfs://localhost:55814/user/jdcryans/.logs/10.10.1.63,55846,1276194933831 2010-06-10 11:35:52,095 warn  [master] master.regionserveroperationqueue(183): failed processing: processservershutdown of 10.10.1.63,55846,1276194933831; putting onto delayed todo queue java.io.ioexception: cannot delete: hdfs://localhost:55814/user/jdcryans/.logs/10.10.1.63,55846,1276194933831         at org.apache.hadoop.hbase.regionserver.wal.hlog.splitlog(hlog.java:1179)         at org.apache.hadoop.hbase.master.processservershutdown.process(processservershutdown.java:298)         at org.apache.hadoop.hbase.master.regionserveroperationqueue.process(regionserveroperationqueue.java:149)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:456) caused by: java.io.ioexception: java.io.ioexception: /user/jdcryans/.logs/10.10.1.63,55846,1276194933831 is non empty 2010-06-10 11:35:52,097 debug [master] master.regionserveroperationqueue(126): -root- isn't online, can't process delayedtodoqueue items 2010-06-10 11:35:53,098 debug [master] master.regionserveroperationqueue(126): -root- isn't online, can't process delayedtodoqueue items 2010-06-10 11:35:53,523 info  [main.servermonitor] master.servermanager$servermonitor(131): 1 region servers, 1 dead, average load 14.0[10.10.1.63,55846,1276194933831] 2010-06-10 11:35:54,099 debug [master] master.regionserveroperationqueue(126): -root- isn't online, can't process delayedtodoqueue items 2010-06-10 11:35:55,101 debug [master] master.regionserveroperationqueue(126): -root- isn't online, can't process delayedtodoqueue items the last lines are my own debug. since we don't process the delayed todo if root isn't online, we'll never reassign the regions. ",
        "label": 314
    },
    {
        "text": "pluggable compaction and scan policies via coprocessors  when implementing higher level stores on top of hbase it is necessary to allow dynamic control over how long kvs must be kept around.  semi-static config options for columnfamilies (# of version or ttl) is not sufficient. this can be done with a few additional coprocessor hooks, or by makeing store.scaninfo pluggable. was:  the simplest way to achieve this is to have a pluggable class to determine the smallestreadpoint for region. that way outside code can control what kvs to retain. ",
        "label": 286
    },
    {
        "text": "region server is starting normally even if clock skew is more than default seconds or any configured    regionserver node time is greater than master node time  when region server's time is ahead of master's time and the difference is more than hbase.master.maxclockskew value, region server startup is not failing with clockoutofsyncexception.  this causes some abnormal behavior as detected by our tests.  servermanager.java#checkclockskew  long skew = system.currenttimemillis() - servercurrenttime;  if (skew > maxskew) { string message = \"server \" + servername + \" has been \" + \"rejected; reported time is too far out of sync with master. \" + \"time difference of \" + skew + \"ms > max allowed of \" + maxskew + \"ms\"; log.warn(message); throw new clockoutofsyncexception(message); } above line results in negative value when master's time is lesser than region server time and \" if (skew > maxskew) \" check fails to find the skew in this case.  please note: this was tested in hbase 0.94.11 version and the trunk also currently has the same logic. the fix for the same would be to make the skew positive value first as below:  long skew = system.currenttimemillis() - servercurrenttime;  skew = (skew < 0 ? -skew : skew);  if (skew > maxskew) {..... ",
        "label": 269
    },
    {
        "text": "quick start guide shows stable version as  in the stable folder it is  in the quick start guide for hbase - http://hbase.apache.org/book/quickstart.html  the stable version is mentioned as - 0.95 in the line -  \"choose a download site from this list of apache download mirrors. click on the suggested top link. this will take you to a mirror of hbase releases. click on the folder named stable and then download the file that ends in .tar.gz to your local filesystem; e.g. hbase-0.95-snapshot.tar.gz.\" but in the download folder at - http://apache.techartifact.com/mirror/hbase/stable/  the version that can be found is - hbase-0.94.2-security.tar.gz i.e. 0.94  so either the documentation or the download folder needs to be updated. ",
        "label": 314
    },
    {
        "text": "rpcserver threads can wedge under high load  this is with 0.98.0 in an insecure setup with 7u55 and 7u60. under high load, rpcserver threads can wedge, fail to make progess, and consume 100% cpu time on a core indefinitely. dumping threads, all threads are in blocked or in_native state. the in_native threads are mostly in epollarraywrapper.epollwait or filedispatcherimpl.read0. the number of threads found in filedispatcherimpl.read0 correspond to the number of runaway threads expected based on looking at 'top' output. these look like: thread 64758: (state = in_native)  - sun.nio.ch.filedispatcherimpl.read0(java.io.filedescriptor, long, int) @bci=0 (compiled frame; information may be imprecise)  - sun.nio.ch.socketdispatcher.read(java.io.filedescriptor, long, int) @bci=4, line=39 (compiled frame)  - sun.nio.ch.ioutil.readintonativebuffer(java.io.filedescriptor, java.nio.bytebuffer, long, sun.nio.ch.nativedispatcher) @bci=114, line=223 (compil ed frame)  - sun.nio.ch.ioutil.read(java.io.filedescriptor, java.nio.bytebuffer, long, sun.nio.ch.nativedispatcher) @bci=48, line=197 (compiled frame)  - sun.nio.ch.socketchannelimpl.read(java.nio.bytebuffer) @bci=234, line=379 (compiled frame)  - org.apache.hadoop.hbase.ipc.rpcserver.channelread(java.nio.channels.readablebytechannel, java.nio.bytebuffer) @bci=12, line=2224 (compiled frame)  - org.apache.hadoop.hbase.ipc.rpcserver$connection.readandprocess() @bci=509, line=1488 (compiled frame)  - org.apache.hadoop.hbase.ipc.rpcserver$listener.doread(java.nio.channels.selectionkey) @bci=23, line=790 (compiled frame)  - org.apache.hadoop.hbase.ipc.rpcserver$listener$reader.dorunloop() @bci=97, line=581 (compiled frame)  - org.apache.hadoop.hbase.ipc.rpcserver$listener$reader.run() @bci=1, line=556 (interpreted frame)  - java.util.concurrent.threadpoolexecutor.runworker(java.util.concurrent.threadpoolexecutor$worker) @bci=95, line=1145 (interpreted frame)  - java.util.concurrent.threadpoolexecutor$worker.run() @bci=5, line=615 (interpreted frame)  - java.lang.thread.run() @bci=11, line=745 (interpreted frame) ",
        "label": 38
    },
    {
        "text": "release  ",
        "label": 187
    },
    {
        "text": "the way we process connection preamble in simplerpcserver is broken  though very rare, but if the preamble is not sent at once, the logic will be broken. ",
        "label": 149
    },
    {
        "text": "add string versions of get  scanner  put in htable  we have string overloads for some of the htable and hbaseadmin methods. we also have text overloads for all of our native byte [] methods. we need to add string if only to make life easier on jirb. ",
        "label": 314
    },
    {
        "text": "hbase connectors hbasecontext should use most recent delegation token  for a long-running hbasecontext, it keeps referring to the persisting credential instead of the most recent token. this would cause the spark job failed due to \"token has expired\" issue if the hbasecontext's lifetime is longer than the token's lifetime. ",
        "label": 215
    },
    {
        "text": "asyncnonmetaregionlocator should not cache hregionlocation with null location  https://builds.apache.org/job/hbase-flaky-tests/job/master/2992/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.testasynctablegetmultithreaded-output.txt/*view*/ after this line 2019-04-14 04:44:41,736 info  [peworker-12] procedure2.procedureexecutor(1410): finished pid=117, state=success, haslock=false; transitregionstateprocedure table=hbase:meta, region=1588230740, reopen/move in 2.0690sec seems we just do nothing until the test is timed out. and there is no main thread in the output hanging thread, which is a bit strange, although all the get threads are hanging there. let me add some logs for better debugging first. ",
        "label": 149
    },
    {
        "text": "tableinputformatbase nextkeyvalue catches the wrong exception  tableinputformatbase#nextkeyvalue only catches unknownscannerexception from scanner.next. however, scanner may throw other exceptions: /* from htable.clientscanner#next */           try {             values = getconnection().getregionserverwithretries(callable);           } catch (ioexception e) {             if (e instanceof unknownscannerexception &&                 lastnext + scannertimeout < system.currenttimemillis()) {               scannertimeoutexception ex = new scannertimeoutexception();               ex.initcause(e);               throw ex;             }             throw e;           } is there any reason why tifb does not catch scannertimeoutexception? ",
        "label": 144
    },
    {
        "text": "metrics intern table names cause eventual permgen oom in  as part of the metrics system introduced in hbase-4768 there are two places that hbase uses string interning ( schemaconfigured and schemametrics ). this includes interning table names. we have long running environment where we run regular integration tests on our application using hbase. those tests create and drop tables with new names regularly. these leads to filling up the permgen with interned table names. workaround is to periodically restart the region servers. ",
        "label": 441
    },
    {
        "text": "blog post for hbtop on hbase apache org  ",
        "label": 455
    },
    {
        "text": "javadoc creation needs jsr305  cryptic failure trying to build beta-1 rc. fails like this: [info] build failure [info] ------------------------------------------------------------------------ [info] total time: 03:54 min [info] finished at: 2017-12-29t01:13:15-08:00 [info] final memory: 381m/9165m [info] ------------------------------------------------------------------------ [error] failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.4:site (default-site) on project hbase: error generating maven-javadoc-plugin:2.10.3:aggregate: [error] exit code: 1 - warning: unknown enum constant when.always [error] reason: class file for javax.annotation.meta.when not found [error] warning: unknown enum constant when.unknown [error] warning: unknown enum constant when.maybe [error] /home/stack/hbase.git/hbase-common/src/main/java/org/apache/hadoop/hbase/cellutil.java:762: warning - tag @link: malformed: \"#matchingrows(cell, byte[]))\" [error] /home/stack/hbase.git/hbase-common/src/main/java/org/apache/hadoop/hbase/cellutil.java:762: warning - tag @link: reference not found: #matchingrows(cell, byte[])) [error] /home/stack/hbase.git/hbase-common/src/main/java/org/apache/hadoop/hbase/cellutil.java:762: warning - tag @link: reference not found: #matchingrows(cell, byte[])) [error] javadoc: warning - class javax.annotation.nonnull not found. [error] javadoc: error - class file for javax.annotation.meta.typequalifiernickname not found [error] [error] command line was: /home/stack/bin/jdk1.8.0_151/jre/../bin/javadoc -j-xmx2g @options @packages [error] [error] refer to the generated javadoc files in '/home/stack/hbase.git/target/site/apidocs' dir. [error] -> [help 1] [error] [error] to see the full stack trace of the errors, re-run maven with the -e switch. [error] re-run maven using the -x switch to enable full debug logging. [error] [error] for more information about the errors and possible solutions, please read the following articles: [error] [help 1] http://cwiki.apache.org/confluence/display/maven/mojoexecutionexception javax.annotation.meta.typequalifiernickname is out of jsr305 but we don't include this anywhere according to mvn dependency. happens building the user api both test and main. excluding these lines gets us passing again:   3511               <doclet>   3512                 org.apache.yetus.audience.tools.includepublicannotationsstandarddoclet   3513               </doclet>   3514               <docletartifact>   3515                 <groupid>org.apache.yetus</groupid>   3516                 <artifactid>audience-annotations</artifactid>   3517                 <version>${audience-annotations.version}</version>   3518               </docletartifact> + 3519               <usestandarddocletoptions>true</usestandarddocletoptions> tried upgrading to newer mvn site (ours is three years old) but that a different set of problems. ",
        "label": 402
    },
    {
        "text": "server shutdown processor stuck because meta not online  playing with rolling restart i see that the server hosting root and meta can go down close to each other. in below, note how we are processing server hosting root and part of its processing involves reading .meta. content to see what servers it was carrying. well, note that .meta. is offline at time (our verification attempt failed because server had just been shutdown and verification got connectexception). so we pause the server shutdown processing till .meta. comes back online \u2013 only it never does. 2010-10-21 07:32:23,931 info org.apache.hadoop.hbase.catalog.rootlocationeditor: unsetting root region location in zookeeper 2010-10-21 07:32:23,953 info org.apache.hadoop.hbase.master.handler.servershutdownhandler: splitting logs for sv2borg182,60020,1287645693959                                                                                       2010-10-21 07:32:23,994 info org.apache.hadoop.hbase.catalog.rootlocationeditor: unsetting root region location in zookeeper 2010-10-21 07:32:24,020 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x12bcd5b344b0115 creating (or updating) unassigned node for 70236052 with offline state                                                    2010-10-21 07:32:24,045 debug org.apache.hadoop.hbase.master.assignmentmanager: no previous transition plan for -root-,,0.70236052 so generated a random one; hri=-root-,,0.70236052, src=, dest=sv2borg181,60020,1287646329081; 8 (online=8, exclude=null) available servers                                                                                                                                                                                         2010-10-21 07:32:24,045 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region -root-,,0.70236052 to sv2borg181,60020,1287646329081                                                                              2010-10-21 07:32:24,048 info org.apache.hadoop.hbase.catalog.catalogtracker: failed verification of .meta.,,1; java.net.connectexception: connection refused 2010-10-21 07:32:24,048 info org.apache.hadoop.hbase.catalog.catalogtracker: current cached meta location is not valid, resetting 2010-10-21 07:32:24,079 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_opening, server=sv2borg181,60020,1287646329081, region=70236052/-root-                                            2010-10-21 07:32:24,162 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_opening, server=sv2borg181,60020,1287646329081, region=70236052/-root- 2010-10-21 07:32:24,212 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_opened, server=sv2borg181,60020,1287646329081, region=70236052/-root-                                             2010-10-21 07:32:24,212 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: handling opened event for 70236052; deleting unassigned node                                                                             2010-10-21 07:32:24,212 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x12bcd5b344b0115 deleting existing unassigned node for 70236052 that is in expected state rs_zk_region_opened                              2010-10-21 07:32:24,238 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: opened region -root-,,0.70236052                                                                                                         2010-10-21 07:32:27,902 info org.apache.hadoop.hbase.master.servermanager: registering server=sv2borg183,60020,1287646347597, regioncount=0, userload=false                                                                        2010-10-21 07:32:30,523 info org.apache.hadoop.hbase.zookeeper.regionservertracker: regionserver ephemeral node deleted, processing expiration [sv2borg184,60020,1287645693960]                                                    2010-10-21 07:32:30,523 debug org.apache.hadoop.hbase.master.servermanager: added=sv2borg184,60020,1287645693960 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:36,254 info org.apache.hadoop.hbase.master.servermanager: registering server=sv2borg184,60020,1287646355951, regioncount=0, userload=false                                                                        2010-10-21 07:32:39,567 info org.apache.hadoop.hbase.zookeeper.regionservertracker: regionserver ephemeral node deleted, processing expiration [sv2borg185,60020,1287645693959]                                                    2010-10-21 07:32:39,567 debug org.apache.hadoop.hbase.master.servermanager: added=sv2borg185,60020,1287645693959 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:45,614 info org.apache.hadoop.hbase.master.servermanager: registering server=sv2borg185,60020,1287646365304, regioncount=0, userload=false                                                                        2010-10-21 07:32:48,652 info org.apache.hadoop.hbase.zookeeper.regionservertracker: regionserver ephemeral node deleted, processing expiration [sv2borg186,60020,1287645693962]                                                    2010-10-21 07:32:48,652 debug org.apache.hadoop.hbase.master.servermanager: added=sv2borg186,60020,1287645693962 to dead servers, submitted shutdown handler to be executed                                                        2010-10-21 07:32:50,097 info org.apache.hadoop.hbase.master.servermanager: regionservers=8, averageload=93.38, deadservers=[sv2borg185,60020,1287645693959, sv2borg183,60020,1287645693959, sv2borg182,60020,1287645693959,        sv2borg184,60020,1287645693960, sv2borg186,60020,1287645693962] .... we're supposed to have a thread of 5 executors to handle server shutdowns. i see an executor stuck waiting on .meta. but i dont see any others running. odd. trying to figure why executors are 1 only. \"master_server_operations-sv2borg180:60000-1\" daemon prio=10 tid=0x0000000041dc7000 nid=0x50a4 in object.wait() [0x00007f285d537000]    java.lang.thread.state: timed_waiting (on object monitor)     at java.lang.object.wait(native method)     at org.apache.hadoop.hbase.catalog.catalogtracker.waitformeta(catalogtracker.java:324)     - locked <0x00007f286d150ce8> (a java.util.concurrent.atomic.atomicboolean)     at org.apache.hadoop.hbase.catalog.catalogtracker.waitformetaserverconnectiondefault(catalogtracker.java:359)     at org.apache.hadoop.hbase.catalog.metareader.getserveruserregions(metareader.java:487)     at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:115)     at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:150)     at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)     at java.lang.thread.run(thread.java:619) ",
        "label": 314
    },
    {
        "text": "hbaserpc unknownprotocolexception should abort any client retries in hconnectionmanager  while hbaserpc$unknownprotocolexception currently extends donotretryioexception, it's still allowing retries of client rpcs when encountered in hconnectionmanager.getregionserverwithretries(). it turns out that unknownprotocolexception is missing a public constructor taking a single string argument, which is required when unwrapping an ioexception from a remoteexception in remoteexceptionhandler.decoderemoteexception(). ",
        "label": 180
    },
    {
        "text": "exec throws a npe while writing a method that has a null value argument  exec write method invokes getclass() on its arguments list for finding the argument's class, which gives a npe in case the argument is null. there is already an parameterclasses array in invoker (its super class), which is populated with correct values (by method.getparametertypes()). one can use this array. ",
        "label": 180
    },
    {
        "text": "filters are not properly applied in certain cases  steps to reproduce: create a table, load data into it. flush the table. do a scan with  1. some filter which should not match the first entry in the scan  2. where one specifies a family and column.  you will notice that the first entry is returned even though it doesn't match the filter. it looks like the when the first keyvalue of a scan in the column from the point of view of the code hregion.java } else if (kv != null && !kv.isinternal() && filterrowkey(currentrow)) { is generated by public static keyvalue createlastonrow(final byte [] row, final int roffset, final int rlength, final byte [] family, final int foffset, final int flength, final byte [] qualifier, final int qoffset, final int qlength) { return new keyvalue(row, roffset, rlength, family, foffset, flength, qualifier, qoffset, qlength, hconstants.oldest_timestamp, type.minimum, null, 0, 0); } so it is always internal from that point of the code. only later from within  storescanner.java public synchronized boolean next(list<keyvalue> outresult, int limit, string metric) throws ioexception { .... loop: while((kv = this.heap.peek()) != null) { ( the second time through) do we get the actual kv, with a proper type and timestamp. this seems to mess with filtering. ",
        "label": 286
    },
    {
        "text": "reimplement cluster startup to use new load balancer  the new load balancer provides a special call for cluster startup. ideally this takes into account block locations so we can get good data locality, but primarily this will make it so cluster startup does not rely on the metascanner and normal assignment process as it does today. ",
        "label": 247
    },
    {
        "text": "scan formatter is not applied for columns using non printable name in shell  the \"formatter\" does not work if the target cell`s column uses binary name.  hbase> create \"test1\", \"f\"  hbase> incr \"test1\", \"row1\" , \"f:a\", 1  hbase> incr \"test1\", \"row1\" , \"f:\\x11\", 1  hbase> scan \"test1\", columns=>[\"f:\\x11:tolong\",\"f:a:tolong\"]  row column+cell  row1 column=f:\\x11, ..., value=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01  row1 column=f:a, ..., value=1 ",
        "label": 153
    },
    {
        "text": "indexedregion does not properly handle deletes  i've been using the indexedtable stuff from contrib and come across a bit of an issue. when i delete a column my indexes are removed for that column. i've run through the code in indexedregion and used very similar code in my own classes to recreate the index after i've run the delete. i've also noticed that if i run a put after the delete then the index will be re-created. neither the delete or the subsequent put in the second example uses any of the columns that are part of the index (either indexed or additional columns). org.apache.hadoop.hbase.regionserver.tableindexed.indexedregion.java @override  public void delete(delete delete, final integer lockid, boolean writetowal)      throws ioexception {    if (!getindexes().isempty()) {      // need all columns      navigableset<byte[]> neededcolumns = getcolumnsforindexes(getindexes());      get get = new get(delete.getrow());      for (byte [] col : neededcolumns) {       get.addcolumn(col);      }      result oldrow = super.get(get, null);      sortedmap<byte[], byte[]> oldcolumnvalues = converttovaluemap(oldrow);      for (indexspecification indexspec : getindexes()) {        removeoldindexentry(indexspec, delete.getrow(), oldcolumnvalues);      }      // handle if there is still a version visible.      if (delete.gettimestamp() != hconstants.latest_timestamp) {        get.settimerange(1, delete.gettimestamp());        oldrow = super.get(get, null);        sortedmap<byte[], byte[]> currentcolumnvalues = converttovaluemap(oldrow);        log.debug(\"there are \" + currentcolumnvalues + \" entries to re-index\");        for (indexspecification indexspec : getindexes()) {          if (indexmaintenanceutils.doesapplytoindex(indexspec, currentcolumnvalues)) {            updateindex(indexspec, delete.getrow(), currentcolumnvalues);          }        }      }    }    super.delete(delete, lockid, writetowal);  } it seems that any delete will remove the indexes, but they will only be rebuilt if the delete is of a previous version for the row, and then the index will then be built using data from the version prior to that which you've just deleted - which seems to mean it would, more often than not, always be out of date. more broadly it also occurs to me that it may make sense not to delete the indexes at all unless the delete would otherwise affect them. in my case there isn't really any reason to remove the indexes, the column i'm deleting is completely unrelated. will follow with a patch shortly to resolve at least the first part of the issue. ",
        "label": 110
    },
    {
        "text": "if key does not exist  return null in getrow rather than an empty rowresult  ",
        "label": 38
    },
    {
        "text": " snapshots  refactor snapshot file cleaner cache to use the snapshot filevisitor  ",
        "label": 309
    },
    {
        "text": "master passes ip and not hostname back to region server  starting my little test cluster on the latest from 0.90, i see: 2010-11-29 23:21:34,131 info org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 1024 region(s) across 9 server(s), retainassignment=true 2010-11-29 23:21:34,134 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 22 region(s) to sv2borg181,61020,1291072886282 2010-11-29 23:21:34,135 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 24 region(s) to sv2borg182,61020,1291072885473 2010-11-29 23:21:34,135 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 37 region(s) to sv2borg183,61020,1291072885646 2010-11-29 23:21:34,135 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 25 region(s) to sv2borg184,61020,1291072886734 2010-11-29 23:21:34,135 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 26 region(s) to sv2borg185,61020,1291072886606 2010-11-29 23:21:34,136 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 70 region(s) to sv2borg186,61020,1291072885486 2010-11-29 23:21:34,136 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 30 region(s) to sv2borg187,61020,1291072886355 2010-11-29 23:21:34,136 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 89 region(s) to sv2borg188,61020,1291072885926 2010-11-29 23:21:34,136 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 701 region(s) to sv2borg189,61020,1291072886739 after another restart: 2010-11-30 00:03:38,100 info org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 1024 region(s) across 9 server(s), retainassignment=true 2010-11-30 00:03:38,103 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 71 region(s) to sv2borg181,61020,1291075409984 2010-11-30 00:03:38,103 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 82 region(s) to sv2borg182,61020,1291075409956 2010-11-30 00:03:38,104 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 61 region(s) to sv2borg183,61020,1291075409952 2010-11-30 00:03:38,104 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 122 region(s) to sv2borg184,61020,1291075409957 2010-11-30 00:03:38,104 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 59 region(s) to sv2borg185,61020,1291075409955 2010-11-30 00:03:38,104 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 71 region(s) to sv2borg186,61020,1291075409963 2010-11-30 00:03:38,105 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 52 region(s) to sv2borg187,61020,1291075411049 2010-11-30 00:03:38,105 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 254 region(s) to sv2borg188,61020,1291075410360 2010-11-30 00:03:38,105 debug org.apache.hadoop.hbase.master.assignmentmanager: bulk assigning 252 region(s) to sv2borg189,61020,1291075409959 i also saw one time where everything was assigned to 189. ",
        "label": 229
    },
    {
        "text": "provide admin interface to abstract hbaseadmin  as hbaseadmin is essentially the administrative api, it would seem to follow java best practices to provide an interface to access it instead of requiring applications to use the raw object. i am proposing (and would be happy to develop): a new interface, hbaseadmininterface, that captures the signatures of the api (hbaseadmin will implement this interface) a new method, hconnection.gethbaseadmin(), that returns an instance of the interface ",
        "label": 90
    },
    {
        "text": "metrics for mob  we need to make sure to capture metrics about mobs.  some basic ones include: 1. of mob writes 2. of mob reads 3. avg size of mob 4. mob files 5. of mob compactions / sweeps ",
        "label": 243
    },
    {
        "text": "drop hadoop support  as per thread:  http://mail-archives.apache.org/mod_mbox/hbase-dev/201403.mbox/%3ccamuu0w93mgp7zbbxgccov+be3etmkvn5atzowvzqd_gegdk4nq@mail.gmail.com%3e it seems that the consensus is that supporting hadoop-1 in hbase-1.x will be costly, so we should drop the support. in this issue: we'll document that hadoop-1 support is deprecated in hbase-0.98. and users should switch to hadoop-2.2+ anyway. document that upcoming hbase-0.99 and hbase-1.0 releases will not have hadoop-1 support. document that there is no rolling upgrade support for going between hadoop-1 and hadoop-2 (using hbase-0.96 or 0.98). release artifacts won't contain hbase build with hadoop-1. we may keep the profile, jenkins job etc if we want. ",
        "label": 314
    },
    {
        "text": "we don't recover if hrs hosting  root meta  goes down  to replicate, set up a cluster with a master and a regionserver. start up the the cluster. kill the regionserver. master just does this over and over: ... 2008-10-14 18:54:14,737 info org.apache.hadoop.hbase.master.basescanner: regionmanager.metascanner scanning meta region {regionname: .meta.,,1, startkey: <>, server: xx.xx.xx.xx:60020} 2008-10-14 18:54:15,739 info org.apache.hadoop.ipc.client: retrying connect to server:xx.xx.xx.xx:60020. already tried 0 time(s). 2008-10-14 18:54:16,742 info org.apache.hadoop.ipc.client: retrying connect to server: xx.xx.xx.xx:60020. already tried 1 time(s). 2008-10-14 18:54:17,744 info org.apache.hadoop.ipc.client: retrying connect to server: xx.xx.xx.xx:60020. already tried 2 time(s). 2008-10-14 18:54:18,747 info org.apache.hadoop.ipc.client: retrying connect to server:xx.xx.xx.xx:60020. already tried 3 time(s). 2008-10-14 18:54:19,749 info org.apache.hadoop.ipc.client: retrying connect to server: xx.xx.xx.xx:60020. already tried 4 time(s). 2008-10-14 18:54:20,752 info org.apache.hadoop.ipc.client: retrying connect to server: xx.xx.xx.xx:60020. already tried 5 time(s). 2008-10-14 18:54:21,755 info org.apache.hadoop.ipc.client: retrying connect to server: xx.xx.xx.xx:60020. already tried 6 time(s). 2008-10-14 18:54:22,757 info org.apache.hadoop.ipc.client: retrying connect to server:xx.xx.xx.xx:60020. already tried 7 time(s). 2008-10-14 18:54:23,759 info org.apache.hadoop.ipc.client: retrying connect to server:xx.xx.xx.xx:60020. already tried 8 time(s). 2008-10-14 18:54:24,762 info org.apache.hadoop.ipc.client: retrying connect to server:xx.xx.xx.xx:60020. already tried 9 time(s). 2008-10-14 18:54:24,763 warn org.apache.hadoop.hbase.master.basescanner: scan one meta region: {regionname: .meta.,,1, startkey: <>, server: xx.xx.xx.xx:60020} java.io.ioexception: call failed on local exception         at org.apache.hadoop.ipc.client.call(client.java:718)         at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:245)         at $proxy2.openscanner(unknown source)         at org.apache.hadoop.hbase.master.basescanner.scanregion(basescanner.java:159)         at org.apache.hadoop.hbase.master.metascanner.scanonemetaregion(metascanner.java:74)         at org.apache.hadoop.hbase.master.metascanner.maintenancescan(metascanner.java:129)         at org.apache.hadoop.hbase.master.basescanner.chore(basescanner.java:139)         at org.apache.hadoop.hbase.chore.run(chore.java:62) caused by: java.net.connectexception: connection refused         at sun.nio.ch.socketchannelimpl.checkconnect(native method)         at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:592)         at sun.nio.ch.socketadaptor.connect(socketadaptor.java:118)         at org.apache.hadoop.ipc.client$connection.setupiostreams(client.java:300)         at org.apache.hadoop.ipc.client$connection.access$1700(client.java:177)         at org.apache.hadoop.ipc.client.getconnection(client.java:789)         at org.apache.hadoop.ipc.client.call(client.java:704)         ... 7 more 2008-10-14 18:54:24,766 info org.apache.hadoop.hbase.master.basescanner: all meta regions scanned ... made it a blocker. ",
        "label": 241
    },
    {
        "text": " c  remove hbase macros h  ",
        "label": 155
    },
    {
        "text": "ringbuffertruck does not release its payload  run a write-heavy workload (perfeval sequentialwrite) out of a trunk sandbox and watch as hbase eventually dies with an oom: heap space. examining the heap dump shows an extremely large retained size of keyvalue and ringbuffertrunk instances. by my eye, the default value of hbase.regionserver.wal.disruptor.event.count is too large for such a small default heap size, or the rbt instances need to release their payloads after consumers retrieve them. ",
        "label": 339
    },
    {
        "text": "update instructions in thrift demo files  the instructions in thrift demo files point to the file : ../../../src/java/org/apache/hadoop/hbase/thrift/hbase.thrift while the correct location is : ../../../src/main/resources/org/apache/hadoop/hbase/thrift/hbase.thrift here is a patch that fixes this. ",
        "label": 331
    },
    {
        "text": "table ttl expires before its real expiration time which cause row key disappear from scanner  when we create a table with the following schema: {name => 'jobs_global', is_root => 'false', is_meta => 'false', max_filesize => '134217728', families => [ {nam e => 'job', bloomfilter => 'false', compression => 'none', versions => '1', length => '2147483647', ttl => '86 400', in_memory => 'false', blockcache => 'false'} ], indexes => []} the ttl is set to 86400 which should expire after 1 day, but the truth is that it expired before 86400 seconds.  to reproduce, create a table with the above schema and run some stress testing to create some splits and compaction,  usually in 4 - 5 hours, the row key will start missing from the scanners. by invoking htable.get() and htable.getrow(), the column appears to exist.  but if you launch a scanner or a mapreduce task to scan the table, the key will be missing. by running a simple mapreduce task that prints out all the key value, you can tell some keys are already missing prior to its expiration time. when we alter the table's ttl to a longer time, e.g. 604800, the row key appears in the scanner. ",
        "label": 38
    },
    {
        "text": "investigate why disabling hadoop short circuit read is required to make recovery tests pass consistently under hadoop2  hbase-7636 makes some testdistributedlogsplitting pass consistently by disabling hdfs short circuit reads.   hbase-8349 makes datanode node death recovery pass consistently by disabling hdfs short circuit reads. this will likely require configuration modifications to fix and may have different fixes for hadoop1, hadoop2 (hdfs-2246), and hadoop3 (hdfs-347)... ",
        "label": 248
    },
    {
        "text": "split doesn't handle ioexceptions when creating new region reference files  i was testing an hdfs patch which had a bug in it, so it happened to throw an npe during a split with the following trace: 2010-04-16 19:18:20,727 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region testtable,-1945465867<1271449232310>,1271453785648  java.lang.nullpointerexception  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.enqueuecurrentpacket(dfsclient.java:3124)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.flushinternal(dfsclient.java:3220)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.closeinternal(dfsclient.java:3306)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.close(dfsclient.java:3255)  at org.apache.hadoop.fs.fsdataoutputstream$positioncache.close(fsdataoutputstream.java:61)  at org.apache.hadoop.fs.fsdataoutputstream.close(fsdataoutputstream.java:86)  at org.apache.hadoop.fs.filesystem.createnewfile(filesystem.java:560)  at org.apache.hadoop.hbase.util.fsutils.create(fsutils.java:95)  at org.apache.hadoop.hbase.io.reference.write(reference.java:129)  at org.apache.hadoop.hbase.regionserver.storefile.split(storefile.java:498)  at org.apache.hadoop.hbase.regionserver.hregion.splitregion(hregion.java:682)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.split(compactsplitthread.java:162)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:95) after that, my region was gone, any further writes to it would fail. ",
        "label": 314
    },
    {
        "text": "bufferunderflowexception for un named htabledescriptors  passing an htabledescriptor without a name to hbaseadmin.createtable(htabledescriptor) causes a java.nio.bufferunderflowexception. reproduce by creating an htabledescriptor with the default constructor and passing to createtable. ",
        "label": 314
    },
    {
        "text": "illegalargumentexception when manually splitting table from web ui  clicked split once, then again, got an error: http://monster01.sf.cloudera.com:60010/table.jsp?action=split&name=verifiableeditor&key=  java.lang.illegalargumentexception: not a host:port pair:   at org.apache.hadoop.hbase.hserveraddress.(hserveraddress.java:57)  at org.apache.hadoop.hbase.master.hmaster.gettableregions(hmaster.java:841)  at org.apache.hadoop.hbase.master.hmaster.modifytable(hmaster.java:981) ",
        "label": 453
    },
    {
        "text": "after a connection sees connectionclosingexception it never recovers  ",
        "label": 154
    },
    {
        "text": "cannot add columnfamily in shell  from jeremyp on irc: \"alter 'mytable', {name=>'newcolumn'}\" 21:05 < jeremyp> nativeexception: org.apache.hadoop.hbase.master.invalidcolumnnameexception: org.apache.hadoop.hbase.master.invalidcolumnnameexception: column family 'newcolumn' doesn't exist, so cannot be modified. 21:05 < jeremyp> iat org.apache.hadoop.hbase.master.modifycolumn.postprocessmeta(modifycolumn.java:50) 21:05 < jeremyp> iat org.apache.hadoop.hbase.master.tableoperation$processtableoperation.call(tableoperation.java:132) 21:05 < jeremyp> iat org.apache.hadoop.hbase.master.tableoperation$processtableoperation.call(tableoperation.java:70) 21:05 < jeremyp> iat org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:61) 21:05 < jeremyp> iat org.apache.hadoop.hbase.master.tableoperation.process(tableoperation.java:143) 21:05 < jeremyp> iat org.apache.hadoop.hbase.master.hmaster.modifycolumn(hmaster.java:653) fix this in 0.18.2 if we make one. ",
        "label": 385
    },
    {
        "text": "remove contrib module from hbase  stargate has been moved up into core. all that is left in contrib is an old version of replication that is busy being moved into contrib. this issue is about removing the contrib module from hbase altogether. we'll still have the core module. whether core is removed is still being discussed. ",
        "label": 314
    },
    {
        "text": "invert the name of namespace commands to match the snapshots and replication form  in our shell, we are inconsistent in the way that we color/group commands. the majority though seem to be <command> '-' <group> as in list_snapshots and list_peers for snapshotting and replication respectively wherease namespaces does 'namespaces_list'. if we could start over, i'd have gone w/ the namespaces format but we are where we are..... lets fix this before we release post ns commit. ",
        "label": 314
    },
    {
        "text": " snapshots  add snapshot clone restore export docs to ref guide  this will include additions to the ref guide about the different operations provided and how to use them. ",
        "label": 309
    },
    {
        "text": "reseek directly to next row  when done with the current row, reseek directly to the next row rather than spending time reading more keys of current row which are not required. ",
        "label": 357
    },
    {
        "text": "add more logging to triage hbase  closed parent region present in hlog lastseqwritten  it is hard to find out what exactly caused hbase-5312. some logging will be helpful to shine some lights. ",
        "label": 242
    },
    {
        "text": "grant   revoke namespace admin permission to group  hi, we are looking to roll out hbase and are in the process to design the security model.   we are looking to implement global dbas and namespace specific administrators.   so for example the global dba would create a namespace and grant a user/group admin privileges within that ns.   so that a given ns admin can in turn create objects and grant permission within the given ns only. we have run into some issues at the ns admin level. it appears that a ns admin can not grant to a grop unless it also has global admin privilege. but once it has global admin privilege it can grant in any ns not just the one where it has admin privileges. based on the hbase documentation at http://hbase.apache.org/book.html#appendix_acl_matrix table 13. acl matrix   interface operation permissions   accesscontroller grant(global level) global(a)   grant(namespace level) global(a)|ns(a) grant at a namespace level should be possible for someone with global a or (|) ns a permission.   as you will see in our test it does not work if ns a permission is granted but global a permission is not. here you can see that group hbaseappltest_ns1admin has xca permission on ns1. hbase(main):011:0> scan 'hbase:acl'  row column+cell  @ns1 column=l:@hbaseappltest_ns1admin, timestamp=1446676679787, value=xca  however:   here you can see that a user who is member of the group hbaseappltest_ns1admin can not grant a wrx privilege to a group as it is missing global a privilege. $hbase shell  15/11/13 10:02:23 info configuration.deprecation: hadoop.native.lib is deprecated. instead, use io.native.lib.available  hbase shell; enter 'help<return>' for list of supported commands.  type \"exit<return>\" to leave the hbase shell  version 1.0.0-cdh5.4.7, runknown, thu sep 17 02:25:03 pdt 2015  hbase(main):001:0> whoami  ns1admin@wlab.net (auth:kerberos)  groups: hbaseappltest_ns1admin  hbase(main):002:0> grant '@hbaseappltest_ns1funct' ,'rwx','@ns1'  error: org.apache.hadoop.hbase.security.accessdeniedexception: insufficient permissions for user 'ns1admin' (global, action=admin)  the way i read the documentation a ns admin should be able to grant as it has ns level a privilege not only object level permission. cdh is a version 5.4.7 and hbase is version 1.0. regards,   steven ",
        "label": 441
    },
    {
        "text": "move clusterid and clusterup  shutdown  znodes over to pb  ",
        "label": 314
    },
    {
        "text": "alter in the shell can be too quick and return before the table is altered  this seems to be a recent change in behavior but i'm still not sure where it's coming from. the shell is able to call hmaster.getalterstatus before the tableeventhandler is able call am.setregionstoreopen so that the returned status shows no pending regions. it means that the alter seems \"instantaneous\" although it's far from completed. ",
        "label": 341
    },
    {
        "text": "npe in hregion bulkloadhfiles   was playing with \"completebulkload\", and ran into an npe.  the problem is here (hregion.bulkloadhfiles(...)). store store = getstore(familyname); if (store == null) {   ioexception ioe = new donotretryioexception(       \"no such column family \" + bytes.tostringbinary(familyname));   ioes.add(ioe);   failures.add(p); } try {   store.assertbulkloadhfileok(new path(path)); } catch (wrongregionexception wre) {   // recoverable (file doesn't fit in region)   failures.add(p); } catch (ioexception ioe) {   // unrecoverable (hdfs problem)   ioes.add(ioe); } this should be store store = getstore(familyname); if (store == null) { ... } else {   try {     store.assertbulkloadhfileok(new path(path)); ... } ",
        "label": 286
    },
    {
        "text": "make it so can run pe w o having to put hbase jar on classpath  i need this: diff --git a/src/test/java/org/apache/hadoop/hbase/performanceevaluation.java b/src/test/java/org/apache/hadoop/hbase/performanceevaluation.java index 3982eff..ef47d0d 100644 --- a/src/test/java/org/apache/hadoop/hbase/performanceevaluation.java +++ b/src/test/java/org/apache/hadoop/hbase/performanceevaluation.java @@ -570,6 +570,9 @@ public class performanceevaluation {      textoutputformat.setoutputpath(job, new path(inputdir,\"outputs\"));        tablemapreduceutil.adddependencyjars(job); +    // add a class from the hbase.jar so it gets registered too. +    tablemapreduceutil.adddependencyjars(job.getconfiguration(), +      org.apache.hadoop.hbase.util.bytes.class);      job.waitforcompletion(true);    } ",
        "label": 314
    },
    {
        "text": "meta doesn't get assigned in a master failure scenario  the flow:  1. cluster is up, meta is assigned to some server  2. master is killed  3. master is brought up, it is initializing. it learns about the meta server (in assignmeta).  4. server holding meta is killed  5. meta never gets reassigned since the ssh wasn't enabled ",
        "label": 139
    },
    {
        "text": "master won't go down because joined on a rootscanner that is waiting for ever  the below wait depends on an open event hitting the master. won't happen if we're shutting down. \"regionmanager.rootscanner\" daemon prio=10 tid=0x00007fdc98197c00 nid=0x7538 in object.wait() [0x0000000040e7f000..0x0000000040e7fa80]    java.lang.thread.state: waiting (on object monitor)     at java.lang.object.wait(native method)     at java.lang.object.wait(object.java:485)     at org.apache.hadoop.hbase.master.regionmanager.waitforrootregionlocation(regionmanager.java:981)     - locked <0x00007fdcad0cacd0> (a java.util.concurrent.atomic.atomicreference)     at org.apache.hadoop.hbase.master.hmaster.waitforrootregionlocation(hmaster.java:362)     at org.apache.hadoop.hbase.master.rootscanner.scanroot(rootscanner.java:45)     at org.apache.hadoop.hbase.master.rootscanner.maintenancescan(rootscanner.java:79)     at org.apache.hadoop.hbase.master.basescanner.chore(basescanner.java:135)     at org.apache.hadoop.hbase.chore.run(chore.java:68) ",
        "label": 314
    },
    {
        "text": "remove flaky testcatalogtrackeroncluster  it's not even clear what this test is verifying? valid but nonexistent hostname? invalid hostname? existing hostname but nobody listening on the specific port? i would like to simply remove this test altogether.  comments? ",
        "label": 286
    },
    {
        "text": "allow atomic put delete in one call  right now we have the following calls: put(put)  delete(delete)  increment(increments) but we cannot combine all of the above in a single call, complete with a single row lock. it would be nice to do that. it would also allow us to do a cas where we could do a put/increment if the check succeeded. amendment:  since increment does not currently support mvcc it cannot be included in an atomic operation.  so this for put and delete only. ",
        "label": 286
    },
    {
        "text": "hregionserver sometimes does not shut down   note that i initially assumed this to be a phoenix bug. but i tracked it down to hbase. i noticed that recently only. latest build from hbase's branch-1 and latest build from phoenix' 4.x-hbase-1.5. i don't know, yet, whether it's a phoenix or an hbase issues. just filing it here for later reference. jstack show this thread as the only non-daemon thread: \"pool-11-thread-1\" #470 prio=5 os_prio=0 tid=0x0000558a709a4800 nid=0x238e waiting on condition [0x00007f213ad68000]    java.lang.thread.state: waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x000000058eafece8> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)         at java.util.concurrent.locks.locksupport.park(locksupport.java:175)         at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:2039)         at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:442)         at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1074)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1134)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)         at java.lang.thread.run(thread.java:748) no other information. somebody created a thread pool somewhere and forgot to set the threads to daemon or is not shutting down the pool properly. edit: i looked for other reference of the locked objects in the stack dump, but didn't find any.     ",
        "label": 286
    },
    {
        "text": "with hfile v2  the region server will always perform an extra copy of source files  with hfile v2 implementation in hbase 0.94 & 0.96, the region server will use hfilesystem as its fs. when it performs bulk load in store.bulkloadhfile(), it checks if its fs is the same as srcfs, which however will be distributedfilesystem. consequently, it will always perform an extra copy of source files. ",
        "label": 239
    },
    {
        "text": "wal based log splitting resubmit threshold may result in a task being stuck forever  not sure if this is handled better in procedure based wal splitting; in any case it affects versions before that.  the problem is not in zk as such but in internal state tracking in master, it seems. master: 2019-04-21 01:49:49,584 info  [master/<master>:17000.splitlogmanager..chore.1] coordination.splitlogmanagercoordination: resubmitting task <path>.1555831286638 worker-rs, split fails .... 2019-04-21 02:05:31,774 info  [rs_log_replay_ops-regionserver/<worker-rs>:17020-1] wal.walsplitter: processed 24 edits across 2 regions; edits skipped=457; log file=<path>.1555831286638, length=2156363702, corrupted=false, progress failed=true master (not sure about the delay of the acquired-message; at any rate it seems to detect the failure fine from this server) 2019-04-21 02:11:14,928 info  [main-eventthread] coordination.splitlogmanagercoordination: task <path>.1555831286638 acquired by <worker-rs>,17020,1555539815097 2019-04-21 02:19:41,264 info  [master/<master>:17000.splitlogmanager..chore.1] coordination.splitlogmanagercoordination: skipping resubmissions of task <path>.1555831286638 because threshold 3 reached after that this task is stuck in the limbo forever with the old worker, and never resubmitted.   rs never logs anything else for this task.  killing the rs on the worker unblocked the task and some other server did the split very quickly, so seems like master doesn't clear the worker name in its internal state when hitting the threshold... master never restarted so restarting the master might have also cleared it.  this is extracted from splitlogmanager log messages, note the times. 2019-04-21 02:2   1555831286638=last_update = 1555837874928 last_version = 11 cur_worker_name = <worker-rs>,17020,1555539815097 status = in_progress incarnation = 3 resubmits = 3 batch = installed = 24 done = 3 error = 20,  .... 2019-04-22 11:1   1555831286638=last_update = 1555837874928 last_version = 11 cur_worker_name = <worker-rs>,17020,1555539815097 status = in_progress incarnation = 3 resubmits = 3 batch = installed = 24 done = 3 error = 20} ",
        "label": 406
    },
    {
        "text": "bulk load mvcc and seqid issues with native hfiles  there are mvcc and seqid issues when bulk load native hfiles \u2013 meaning hfiles that are direct file copy-out from hbase, not from hfileoutputformat job. there are differences between these two types of hfiles.  native hfiles have possible non-zero max_memstore_ts_key value and non-zero mvcc values in cells.   native hfiles also have max_seq_id_key.  native hfiles do not have bulkload_time_key. here are a couple of problems i observed when bulk load native hfiles. 1. cells in newly bulk loaded hfiles can be invisible to scan. it is easy to re-create.  bulk load a native hfile that has a larger mvcc value in cells, e.g 10  if the current readpoint when initiating a scan is less than 10, the cells in the new hfile are skipped, thus become invisible.  we don't reset the readpoint of a region after bulk load. 2. the current storefile.isbulkloadresult() is implemented as: return metadatamap.containskey(bulkload_time_key) which does not detect bulkloaded native hfiles. 3. another observed problem is possible data loss during log recovery.   it is similar to hbase-10958 reported by jean-daniel cryans. borrow the re-create steps from hbase-10958. 1) create an empty table  2) put one row in it (let's say it gets seqid 1)  3) bulk load one native hfile with large seqid ( e.g. 100). the native hfile can be obtained by copying out from existing table.  4) kill the region server that holds the table's region.  scan the table once the region is made available again. the first row, at seqid 1, will be missing since the hfile with seqid 100 makes us believe that everything that came before it was flushed. the problem 3 is probably related to 2. we will be ok if we get the appended seqid during bulk load instead of 100 from inside the file. ",
        "label": 234
    },
    {
        "text": "client sync block can cause thread of a multi threaded client to block all others  take a highly multithreaded client, processing a few thousand requests a second. if a table goes offline, one thread will get stuck in \"locateregioninmeta\" which is located inside the following sync block:  synchronized(userregionlock) { return locateregioninmeta(meta_table_name, tablename, row, usecache); } so when other threads need to find a region (even if its cached!!!) it will encounter this sync and wait. this can become an issue on a busy thrift server (where i first noticed the problem), one region offline can prevent access to all other regions! potential solution: narrow this lock, or perhaps just get rid of it completely. ",
        "label": 268
    },
    {
        "text": "define semantics of cell timestamps versions  there is a lot of general confusion over the semantics of the cell timestamp. in particular, a couple questions that often come up: if multiple writes to a cell have the same timestamp, are all versions maintained or just the last? is it ok to write cells in a non-increasing timestamp order? let's discuss, figure out what semantics make sense, and then move towards (a) documentation, (b) unit tests that prove we have those semantics. ",
        "label": 357
    },
    {
        "text": "run tests with non secure random  some tests hang otherwise  testhcm#testclosing fails on linux if not enough entropy is available in /dev/random ",
        "label": 286
    },
    {
        "text": "master rewrite and cleanup for  this is the parent issue for master changes targeted at 0.90 release. changes done as part of this issue grew out of work done over in hbase-2485 to move region transitions into zk. in addition to that work, this issue will include general hmaster and zookeeper refactorings and cleanups. ",
        "label": 247
    },
    {
        "text": "rest documentation under package html should go to the book  it seems that we have more up to date and better documentation under hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/package.html than in the book. we should merge these two. the package.html is only accessible if you know where to look. misty linville fyi. ",
        "label": 330
    },
    {
        "text": " performance  investigate locking   contention in the write path  when doing a large number of bulk updates from different clients, i noticed that there was a high level of lock contention for stuff like locking the hlog. it seems that each thread acquires the lock for a single batchupdate, releases the lock then another thread owns the lock before the initial writer gets to the next update. having the threads bounce around may lead to suboptimal performance. should be benchmarked & maybe changed to have less context switching. ",
        "label": 314
    },
    {
        "text": " migration    migration  hfile  hcd changes  hsk changes   ",
        "label": 314
    },
    {
        "text": "if hmaster is started after nn without starting dn in hbase then hmaster is not able to start due to alreadycreatedexception for  hbase hbase version  it reproduces when hmaster is started for the first time and nn is started without starting dn hmaster logs:  2011-04-19 16:49:09,208 debug org.apache.hadoop.hbase.master.activemastermanager: a master is now available  2011-04-19 16:49:09,400 warn org.apache.hadoop.hbase.util.fsutils: version file was empty, odd, will try to set it.  2011-04-19 16:51:09,674 warn org.apache.hadoop.hdfs.dfsclient: datastreamer exception: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: file /hbase/hbase.version could only be replicated to 0 nodes, instead of 1  ........... 2011-04-19 16:51:09,674 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block null bad datanode[0] nodes == null  2011-04-19 16:51:09,674 warn org.apache.hadoop.hdfs.dfsclient: could not get block locations. source file \"/hbase/hbase.version\" - aborting...  2011-04-19 16:51:09,674 warn org.apache.hadoop.hbase.util.fsutils: unable to create version file at hdfs://c4c1:9000/hbase, retrying: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: file /hbase/hbase.version could only be replicated to 0 nodes, instead of 1  ........... 2011-04-19 16:56:19,695 warn org.apache.hadoop.hbase.util.fsutils: unable to create version file at hdfs://c4c1:9000/hbase, retrying: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hdfs.protocol.alreadybeingcreatedexception: failed to create file /hbase/hbase.version for dfsclient_hb_m_c4c1.site:60000_1303202948768 on client 157.5.100.1 because current leaseholder is trying to recreate file.  org.apache.hadoop.hdfs.protocol.alreadybeingcreatedexception: failed to create file /hbase/hbase.version for dfsclient_hb_m_c4c1.site:60000_1303202948768 on client 157.5.100.1 because current leaseholder is trying to recreate file.  at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfileinternal(fsnamesystem.java:1068)  .... ",
        "label": 38
    },
    {
        "text": "make cell  interfaceaudience public and evolving   from discussion in hbase-9359, keyvalue was made @interfaceaudience.private. cell was not made @interfaceaudience.public. fix this. ",
        "label": 248
    },
    {
        "text": "please add apache trafodion and esgyndb to  powered by apache hbase  page  please add the following two entries to the web page \"powered by apache hbase\u2122\" (https://hbase.apache.org/poweredbyhbase.html): trafodion (link https://trafodion.apache.org/)  apache trafodion(tm) is a webscale sql-on-hadoop solution enabling transactional or operational workloads. it uses hbase as its storage engine for sql tables. esgyndb (link https://esgyn.com/) esgyndb, powered by apache trafodion(tm), provides enterprise sql on hadoop. it includes full acid transactions, online transaction processing and online analytic processing, along with enterprise features such as disaster recovery and full backup/restore. native tables are stored in hbase, but read and write access to various other file formats such as apache parquet and orc is also supported.  ",
        "label": 62
    },
    {
        "text": "pass region info in loadbalancer randomassignment list servername  servers   in lb there is randomassignment(list<servername servers>) api which will be used by am to assign  a region from a down rs. [this will be also used in other cases like call to assign() api from client]  i feel it would be better to pass the hregioninfo also into this method. when the lb making a choice for a region  assignment, when one rs is down, it would be nice that the lb knows for which region it is doing this server selection. scenario  while one rs down, we wanted the regions to get moved to other rss but a set of regions stay together. we are having custom load balancer but with the current way of lb interface this is not possible. another way is i can allow a random assignment of the regions at the rs down time. later with a cluster balance i can balance the regions as i need. but this might make regions assign 1st to one rs and then again move to another. also for some time period my business use case can not get satisfied. also i have seen some issue in jira which speaks about making sure that root and meta regions always sit in some specific rss. with the current lb api this wont be possible in future. ",
        "label": 46
    },
    {
        "text": " hbase  master should allocate regions to regionservers based upon data locality and rack awareness  currently, regions are assigned regionservers based off a basic loading attribute. a factor to include in the assignment calcuation is the location of the region in hdfs; i.e. servers hosting region replicas. if the cluster is such that regionservers are being run on the same nodes as those running hdfs, then ideally the regionserver for a particular region should be running on the same server as hosts a region replica. ",
        "label": 288
    },
    {
        "text": "improve how htable handles threads used for multi actions  when creating a new htable we have to query zk to learn about the number of region servers in the cluster. that is done for every single one of them, i think instead we should do it once per jvm and then reuse that number for all the others. ",
        "label": 229
    },
    {
        "text": "remove references to date tiered compaction from branch and branch ref guide  i have download hbase 1.2.6 src package. but not found class datetieredstoreengine, which is present in document: https://hbase.apache.org/1.2/book.html. ",
        "label": 402
    },
    {
        "text": "hbase vote should tee build and test output to console  the hbase-vote script should tee the build and test output to console in addition to the output file so the user does not become suspicious about progress. ",
        "label": 38
    },
    {
        "text": "add help to the start of our project keys file  some other projects have nice help messages at the top of their keys file, explaining how ot use and update it: nifi: https://dist.apache.org/repos/dist/release/nifi/keys maven: https://dist.apache.org/repos/dist/release/maven/keys would like something similar in hbase land ",
        "label": 38
    },
    {
        "text": "hmaster will exit when starting with stale data in cached locations such as  root  or  meta   later edit: i've mixed up two issues here. the main problem is that a client (that could be hmaster) will read stale data from root or .meta. and not deal correctly with the raised exceptions. i've noticed this when the ip on my machine changed (it's even easier to detect when lzo doesn't work) master loads .meta. successfully and then starts assigning regions.  however lzo doesn't work so hregionserver can't open the regions.   a client attempts to get data from a table so it reads the location from .meta. but goes to a totally different server (the old value in .meta.) this could happen without the lzo story too. ",
        "label": 314
    },
    {
        "text": "if hregionpartitioner is used in mapreduce  client side configurations are overwritten by hbase site xml   if hregionpartitioner is used in mapreduce, client side configurations are overwritten by hbase-site.xml. we can reproduce the problem by the following instructions: - add hregionpartitioner.class to the 4th argument of tablemapreduceutil#inittablereducerjob() at line around 133 in src/test/java/org/apache/hadoop/hbase/mapreduce/testtablemapreduce.java - change or remove \"hbase.zookeeper.property.clientport\" property in hbase-site.xml ( for example, changed to 12345 ). - run testmultiregiontable() then i got error messages as following: 2011-09-12 22:28:51,020 debug [thread-832] zookeeper.zkutil(93): hconnection opening connection to zookeeper with ensemble (localhost:12345) 2011-09-12 22:28:51,022 info  [thread-832] zookeeper.recoverablezookeeper(89): the identifier of this process is 43200@imac.local 2011-09-12 22:28:51,123 warn  [thread-832] zookeeper.recoverablezookeeper(161): possibly transient zookeeper exception: org.apache.zookeeper.keeperexception$connectionlossexception: keepererrorcode = connectionloss for /hbase/master 2011-09-12 22:28:51,123 info  [thread-832] zookeeper.recoverablezookeeper(173): the 1 times to retry zookeeper after sleeping 1000 ms  ===== 2011-09-12 22:29:02,418 error [thread-832] mapreduce.hregionpartitioner(125): java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2e54e48d closed 2011-09-12 22:29:02,422 warn  [thread-832] mapred.localjobrunner$job(256): job_local_0001 java.lang.nullpointerexception        at org.apache.hadoop.hbase.mapreduce.hregionpartitioner.setconf(hregionpartitioner.java:128)        at org.apache.hadoop.util.reflectionutils.setconf(reflectionutils.java:62)        at org.apache.hadoop.util.reflectionutils.newinstance(reflectionutils.java:117)        at org.apache.hadoop.mapred.maptask$newoutputcollector.<init>(maptask.java:527)        at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:613)        at org.apache.hadoop.mapred.maptask.run(maptask.java:305)        at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:177) i think htable should connect to zookeeper at port 21818 configured at client side instead of 12345 in hbase-site.xml  and it might be caused by \"hbaseconfiguration.addhbaseresources(conf);\" in hregionpartitioner#setconf(configuration). and this might mean that all of client side configurations, also configured in hbase-site.xml, are overwritten caused by this problem. ",
        "label": 436
    },
    {
        "text": "can't append to hlog  can't roll log  infinite cycle  another spin on hbase   saw below loop in ryan rawson logs: .... 2009-01-16 15:32:43,001 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_-2067415907098101353_164148 2009-01-16 15:32:45,561 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.io.ioexception: could not read from stream 2009-01-16 15:32:45,561 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_4699358014912484437_164148 2009-01-16 15:32:49,004 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.io.ioexception: bad connect ack with firstbadlink 10.10.20.19:50010 2009-01-16 15:32:49,004 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_-8649135750875451286_164148 2009-01-16 15:32:51,562 warn org.apache.hadoop.hdfs.dfsclient: datastreamer exception: java.io.ioexception: unable to create new block.     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2723)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183) 2009-01-16 15:32:51,562 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_4699358014912484437_164148 bad datanode[0] nodes == null 2009-01-16 15:32:51,562 warn org.apache.hadoop.hdfs.dfsclient: could not get block locations. aborting... 2009-01-16 15:32:51,562 fatal org.apache.hadoop.hbase.regionserver.hlog: could not append. requesting close of log java.io.ioexception: could not read from stream     at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:119)     at java.io.datainputstream.readbyte(datainputstream.java:265)     at org.apache.hadoop.io.writableutils.readvlong(writableutils.java:325)     at org.apache.hadoop.io.writableutils.readvint(writableutils.java:346)     at org.apache.hadoop.io.text.readstring(text.java:400)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2779)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2704)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183) 2009-01-16 15:32:51,563 error org.apache.hadoop.hbase.regionserver.logroller: log rolling failed with ioe: java.io.ioexception: could not read from stream     at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:119)     at java.io.datainputstream.readbyte(datainputstream.java:265)     at org.apache.hadoop.io.writableutils.readvlong(writableutils.java:325)     at org.apache.hadoop.io.writableutils.readvint(writableutils.java:346)     at org.apache.hadoop.io.text.readstring(text.java:400)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2779)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2704)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183) 2009-01-16 15:32:51,564 fatal org.apache.hadoop.hbase.regionserver.hlog: could not append. requesting close of log java.io.ioexception: could not read from stream     at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:119)     at java.io.datainputstream.readbyte(datainputstream.java:265)     at org.apache.hadoop.io.writableutils.readvlong(writableutils.java:325)     at org.apache.hadoop.io.writableutils.readvint(writableutils.java:346)     at org.apache.hadoop.io.text.readstring(text.java:400)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2779)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2704)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183) 2009-01-16 15:32:51,563 error org.apache.hadoop.hbase.regionserver.hregionserver: java.io.ioexception: could not read from stream 2009-01-16 15:32:51,564 error org.apache.hadoop.hbase.regionserver.hregionserver: java.io.ioexception: could not read from stream 2009-01-16 15:32:51,564 fatal org.apache.hadoop.hbase.regionserver.hlog: could not append. requesting close of log java.io.ioexception: could not read from stream     at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:119)     at java.io.datainputstream.readbyte(datainputstream.java:265)     at org.apache.hadoop.io.writableutils.readvlong(writableutils.java:325)     at org.apache.hadoop.io.writableutils.readvint(writableutils.java:346)     at org.apache.hadoop.io.text.readstring(text.java:400)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2779)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2704)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)     at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183) ... for 930, for different exception type, we triggered abort. should do same here. if ioe and \"can't read from stream\", shut down. the filesystem check seems to be coming back fine and dandy. ",
        "label": 314
    },
    {
        "text": "fix site target post modularization  ",
        "label": 314
    },
    {
        "text": "allow configuration option for parent znode in loadtesttool  i saw the following running hoya functional test which involves loadtesttool: 2014-01-16 19:06:03,443 [thread-2] info  client.hconnectionmanager$hconnectionimplementation (hconnectionmanager.java:makestub(1572)) - getmaster attempt 8 of 35 failed;  retrying after sleep of 10098, exception=org.apache.hadoop.hbase.masternotrunningexception: the node /hbase is not in zookeeper. it should have been written by the        master. check the value configured in 'zookeeper.znode.parent'. there could be a mismatch with the one configured in the master. loadtesttool was reading from correct zookeeper quorum but it wasn't able to find parent znode. an option should be added to loadtesttool so that user can specify parent znode in zookeeper. ",
        "label": 441
    },
    {
        "text": "the path where a dynamically loaded coprocessor jar is copied on the local file system depends on the region name  and implicitly  the start key   when loading a coprocessor from hdfs, the jar file gets copied to a path on the local filesystem, which depends on the region name, and the region start key. the name is \"cleaned\", but not enough, so when you have filesystem unfriendly characters (/?:, etc), the coprocessor is not loaded, and an error is thrown ",
        "label": 37
    },
    {
        "text": "rs should communicate fatal  aborts  back to the master  when a region server aborts, it should attempt to send an rpc to the master that contains (a) the reason for aborting, and (b) the last several kb of log messages, if available. this should help a lot in debugging. ",
        "label": 453
    },
    {
        "text": "saveversion sh doesnt properly grab the git revision  saveversion.sh runs in $base/core which does not have a .git directory which is what it is testing for. needs to test for ../.git i think ",
        "label": 547
    },
    {
        "text": "logic errror causes infinite loop in hregion bulkloadhfiles list   the wrong logic is here:  when a columnfamily does not exist, it gets a null store object, then ioes.add(ioe); failures.add(p)  but the code below, if (failures.size() != 0), it prints a warn log and return false, so it will never go into the code if (ioes.size() != 0) below, and ioexception will not be thrown, then the client will keep retry forever.  there is the same situation when doing store.assertbulkloadhfileok, if any wrongregionexception is caught and failures.add(p), then all the other ioexception thrown by assertbulkloadhfileok will be ignored.  so i think if (failures.size() != 0) {} should be dealt with after if (ioes.size() !=0) {} for (pair<byte[], string> p : familypaths) {     byte[] familyname = p.getfirst();     string path = p.getsecond();     store store = getstore(familyname);     if (store == null) {         ioexception ioe = new donotretryioexception(                 \"no such column family \" + bytes.tostringbinary(familyname));         ioes.add(ioe);         failures.add(p);     } else {         try {             store.assertbulkloadhfileok(new path(path));         } catch (wrongregionexception wre) {             // recoverable (file doesn't fit in region)             failures.add(p);         } catch (ioexception ioe) {             // unrecoverable (hdfs problem)             ioes.add(ioe);         }     } } // validation failed, bail out before doing anything permanent. if (failures.size() != 0) {     stringbuilder list = new stringbuilder();     for (pair<byte[], string> p : failures) {         list.append(\"\\n\").append(bytes.tostring(p.getfirst())).append(\" : \")             .append(p.getsecond());     }     // problem when validating     log.warn(\"there was a recoverable bulk load failure likely due to a\" +             \" split.  these (family, hfile) pairs were not loaded: \" + list);     return false; } // validation failed because of some sort of io problem. if (ioes.size() != 0) {     log.error(\"there were io errors when checking if bulk load is ok.  \" +             \"throwing exception!\");     throw multipleioexception.createioexception(ioes); } ",
        "label": 96
    },
    {
        "text": "add uts for testing concurrent modifications on replication peer  ",
        "label": 187
    },
    {
        "text": "use the jdk in the precommit env for trunk  hbase today uses the jdk 1.6. in the past it created issues when we tried to use 1.7 for the core build while the precommit was on 1.6. having the precommit on 1.7 would solve this. the best is to start with trunk. likely 0.95 will come next, and may be, a day, 0.94. ",
        "label": 183
    },
    {
        "text": "add a setting for priority queue length  meta can have very different request rates than any other region. we shouldn't use the same call queue length. for example:  if a normal request takes 100ms ( long scan or large get )  but a call to meta is all in memory so it takes < 1ms so a call queue length of 100 represents multiple seconds of work for normal requests, but less than a second for meta. ",
        "label": 154
    },
    {
        "text": "allow hconnectionimplementation to recover from zk connection loss  for only   just realized that without this hbase-4805 is broken.  i.e. there's no point keeping a persistent hconnection around if it can be rendered permanently unusable if the zk connection is lost temporarily.  note that this is fixed in 0.96 with hbase-5399 (but that seems to big to backport) ",
        "label": 286
    },
    {
        "text": "add best practice to book for loading row key only  book and wiki faqs are missing guidance on the recommended practice for loading row keys only during a scan. patch attached based on jdcryans' feedback from irc. ",
        "label": 162
    },
    {
        "text": "on oome  regionserver sticks around and doesn't go down with cluster  on john gray cluster, an errant, massive, store file caused us oome. shutdown of cluster left this regionserver in place. a thread dump failed with oome. here is last thing in log: 2008-06-25 03:21:55,111 info org.apache.hadoop.hbase.hregionserver: worker thread exiting 2008-06-25 03:24:26,923 fatal org.apache.hadoop.hbase.hregionserver: set stop flag in regionserver/0:0:0:0:0:0:0:0:60020.cacheflusher java.lang.outofmemoryerror: java heap space         at java.util.hashmap.<init>(hashmap.java:226)         at java.util.hashset.<init>(hashset.java:103)         at org.apache.hadoop.hbase.hregionserver.getregionstocheck(hregionserver.java:1789)         at org.apache.hadoop.hbase.hregionserver$flusher.enqueueoptionalflushregions(hregionserver.java:479)         at org.apache.hadoop.hbase.hregionserver$flusher.run(hregionserver.java:385) 2008-06-25 03:24:26,923 info org.apache.hadoop.ipc.server: ipc server handler 2 on 60020, call batchupdate(items,,1214272763124, 9223372036854775807, org.apache.hadoop.hbase.io.batchupdate@67d6b1e2) from 192.168.249.230:38278: error: java.io.ioexception: server not running java.io.ioexception: server not running         at org.apache.hadoop.hbase.hregionserver.checkopen(hregionserver.java:1758)         at org.apache.hadoop.hbase.hregionserver.batchupdate(hregionserver.java:1547)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:616)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)         at org.apache.hadoop.ipc.server$handler.run(server.java:901) if i get an oome just trying to threaddump, would seem to indicate we need to start keeping a little memory resevoir around for emergencies such as this just so we can shutdown clean. moving this into 0.2. seems important to fix if robustness is name of the game. ",
        "label": 229
    },
    {
        "text": "ioe ignored during flush on close causes dataloss  if the rs experiences an exception during the flush of a region while closing it, it currently catches the exception, logs a warning, and keeps going. if the exception was a droppedsnapshotexception, this means that it will silently drop any data that was in memstore when the region was closed. instead, the rs should do a hard abort so that its logs will be replayed. ",
        "label": 453
    },
    {
        "text": "testzkbasedopencloseregion testreopenregion fails occasionally  failed here: https://builds.apache.org/job/hbase-trunk-on-hadoop-2.0.0/607/testreport/org.apache.hadoop.hbase.master/testzkbasedopencloseregion/testreopenregion/ and https://builds.apache.org/job/hbase-trunk/4232/testreport/org.apache.hadoop.hbase.master/testzkbasedopencloseregion/testreopenregion/ in both cases, we fail for the same reason. the previous test closes a region. the failing test starts but we are reopening the region we'd just closed as we are wont to do. this is the region that gets chosen by the failing test to practise reopen on only it is not yet fully open so in comes the close and it gets stomped on by the ongoing open. let me post some log. ",
        "label": 314
    },
    {
        "text": "max mapfiles to compact at one time  currently we compact all map files with no upper limit this could cause a regionserver to oome if the compaction get behind and the number of mapfiles build up. ",
        "label": 73
    },
    {
        "text": "look up of region in master by encoded region name is o n   {{ public regionstate getregionstate(final string encodedregionname) {  // todo: need a map <encodedname, ...> but it is just dispatch merge...  for (regionstatenode node: regionsmap.values()) {  if (node.getregioninfo().getencodedname().equals(encodedregionname)) { return node.toregionstate(); } }  return null;  }}} it is not used much so making it trivial. ",
        "label": 329
    },
    {
        "text": "thrift server closes htable of open scanners  thrifthbaseservicehandler.openscanner() does this:  1. table = pool.gettable()  2. scanner = table.getscanner()  3. table.close()  4. return scanner while back porting the thrift server to 0.92.6, i found that table.close() calls connection.close(). further calls to scanner.next() raise a connectionclosed exception. the unit tests do not catch this since they reuse an open hconnection instance. this might work on trunk, but depends on the implementations of htablepool, htable and hconnectionmanager. even with the pool wrapper, if the pool is full, table.close() may be called, which may invalidate the table. also, htable is not thread-safe, but they are being reused since they go back in the pool. i suggest storing the table handle along with the scanner. ",
        "label": 285
    },
    {
        "text": "avoid temp bytebuffer allocation in blockingrpcconnection writerequest  currently blockingrpcconnection use bytebufferoutputstream to serialize cellblock which involve temp bytebuffer allocation, we can replace it with netty\u2019s bytebufoutputstream, just like what nettyrpcduplexhandler#writerequest doing ",
        "label": 521
    },
    {
        "text": "improve the speed of hbase thirft batch mutation for deletes  a batch mutate operation does both puts and deletes. batch mutate for put uses table.put(puts) however batch mutate for delete loops over all deletes and calls table.delete for every single cell. this causes delete performance to degrade. ",
        "label": 416
    },
    {
        "text": "implement a new dfsoutputstream for logging wal only  the original dfsoutputstream is very powerful and aims to serve all purposes. but in fact, we do not need most of the features if we only want to log wal. for example, we do not need pipeline recovery since we could just close the old logger and open a new one. and also, we do not need to write multiple blocks since we could also open a new logger if the old file is too large. and the most important thing is that, it is hard to handle all the corner cases to avoid data loss or data inconsistency(such as hbase-14004) when using original dfsoutputstream due to its complicated logic. and the complicated logic also force us to use some magical tricks to increase performance. for example, we need to use multiple threads to call hflush when logging, and now we use 5 threads. but why 5 not 10 or 100? so here, i propose we should implement our own dfsoutputstream when logging wal. for correctness, and also for performance. ",
        "label": 149
    },
    {
        "text": "improve cache related stats rendered on rs ui  the stats currently rendered for l1 and l2 cache are incorrect. refer to the attached screenshots of stats from a cluster showing the combined cache stats, l1 stats and l2 stats. for e.g. the combined stats shows 38 gb used for cache while if we sum size of l1 and l2 cache the value is way less. one way we can improve this is to use the same stats used to populate the combined stats to render the values of l1 & l2 cache. also for usability we can remove the table with details with bucketcache buckets from the l2 cache stats since this is going to be long for any installation using l2 cache. this will help in understanding the cache usage better. thoughts? if there are no concerns will submit a patch. ",
        "label": 72
    },
    {
        "text": "mvn site target fails when building with maven  mvn site fails when building with mvn 3.1 due to various class changes inside maven. they promise that switching to new versions of some mvn modules will result in builds that work in both 3.0.x and 3.1: https://cwiki.apache.org/confluence/display/maven/aetherclassnotfound ",
        "label": 314
    },
    {
        "text": "branch  and probably others  fail check of generated source artifact  this shows up in branch-2 nightly: https://builds.apache.org/job/hbase%20nightly/job/branch-2/62/console pulling out the failure: 00:06:59 diff against source tree [pipeline] } [pipeline] // dir [pipeline] sh 00:07:00 [hbase_nightly_branch-2-mo2i3ryuaq2ot52z2b54xso6w34zhp3ju2da5uqycavns747rzwq] running shell script 00:07:01 checking against things we don't expect to include in the source tarball (git related, hbase-native-client, etc.) 00:07:01 any output here are unexpected differences between the source artifact we'd make for an rc and the current branch. 00:07:01 the expected differences are on the < side and the current differences are on the > side. 00:07:01 in a given set of differences, '.' refers to the branch in the repo and 'unpacked_src_tarball' refers to what we pulled out of the tarball. 00:07:01 3a4 00:07:01 > only in ./hbase-backup: .ds_store 00:07:01 4a6 00:07:01 > only in .: .pylintrc [pipeline] } looks like this says the source checkout has two files we don't see in the source tarball. .ds_store looks like a bad commit from someone on a mac and .plyintrc is an old oversight, should probably be included in the source artifact. ",
        "label": 402
    },
    {
        "text": "enable disabled tests in testhcm that were disabled by proc v2 am in hbase  the core proc-v2 am change in hbase-14614 disabled two tests intesthcm: testmulti and testregioncaching this jira tracks the work to enable them. ",
        "label": 314
    },
    {
        "text": "processservershutdown fails if meta moves  orphaning lots of regions  i ran a rolling restart on a 5 node cluster with lots of regions, and afterwards had lots of regions left orphaned. the issue appears to be that processservershutdown failed because the server hosting meta was restarted around the same time as another server was being processed ",
        "label": 314
    },
    {
        "text": "pseudo distributed mode does not work as documented  after master-rs colocation, now the pseudo dist-mode does not work as documented since you cannot start a region server in the same port 16020. i think we can either select a random port (and info port) for the master's region server, or document how to do a pseudo-distributed setup in the book. jimmy xiang wdyt? ",
        "label": 242
    },
    {
        "text": "introduce a special walprovider for synchronous replication  ",
        "label": 149
    },
    {
        "text": "replicationsourcemanager sources is not thread safe  in replicationsourcemanager, sources is used to record the peers. it could be removed in removepeer method, and read in prelogroll method. it is not thread safe.  replicationsourcemanager#296   void prelogroll(path newlog) throws ioexception {     synchronized (this.hlogsbyid) {       string name = newlog.getname();       for (replicationsourceinterface source : this.sources) {         try {           this.replicationqueues.addlog(source.getpeerclusterznode(), name);         } catch (replicationexception e) {           throw new ioexception(\"cannot add log to replication queue with id=\"               + source.getpeerclusterznode() + \", filename=\" + name, e);         }       }       for (sortedset<string> hlogs : this.hlogsbyid.values()) {         if (this.sources.isempty()) {           // if there's no slaves, don't need to keep the old hlogs since           // we only consider the last one when a new slave comes in           hlogs.clear();         }         hlogs.add(name);       }     }     this.latestpath = newlog;   } replicationsourcemanager#392   public void removepeer(string id) {     log.info(\"closing the following queue \" + id + \", currently have \"         + sources.size() + \" and another \"         + oldsources.size() + \" that were recovered\");     string terminatemessage = \"replication stream was removed by a user\";     replicationsourceinterface srctoremove = null;     list<replicationsourceinterface> oldsourcestodelete =         new arraylist<replicationsourceinterface>();     // first close all the recovered sources for this peer     for (replicationsourceinterface src : oldsources) {       if (id.equals(src.getpeerclusterid())) {         oldsourcestodelete.add(src);       }     }     for (replicationsourceinterface src : oldsourcestodelete) {       src.terminate(terminatemessage);       closerecoveredqueue((src));     }     log.info(\"number of deleted recovered sources for \" + id + \": \"         + oldsourcestodelete.size());     // now look for the one on this cluster     for (replicationsourceinterface src : this.sources) {       if (id.equals(src.getpeerclusterid())) {         srctoremove = src;         break;       }     }     if (srctoremove == null) {       log.error(\"the queue we wanted to close is missing \" + id);       return;     }     srctoremove.terminate(terminatemessage);     this.sources.remove(srctoremove);     deletesource(id, true);   } ",
        "label": 362
    },
    {
        "text": "remove unnecessary throws ioexception from bytes readvlong  remove the throws ioexception so that caller doesn't have to catch and ignore.   public static long readvlong(final byte [] buffer, final int offset)   throws ioexception also, add   public static int readvint(final byte [] buffer, final int offset)   throws ioexception {     return (int)readvlong(buffer,offset);   } and these are useful too:     /**      * put long as variable length encoded number at the offset in      * the result byte array.      * @param vint integer to make a vint of.      * @param result buffer to put vint into      * @return vint length in bytes of vint      */     public static int vinttobytes(byte[] result, int offset, final long vint) {       long i = vint;       if (i >= -112 && i <= 127) {         result[offset] = (byte) i;         return 1;       }       int len = -112;       if (i < 0) {         i ^= -1l; // take one's complement'         len = -120;       }       long tmp = i;       while (tmp != 0) {         tmp = tmp >> 8;         len--;       }       result[offset++] = (byte) len;       len = (len < -120) ? -(len + 120) : -(len + 112);       for (int idx = len; idx != 0; idx--) {         int shiftbits = (idx - 1) * 8;         long mask = 0xffl << shiftbits;         result[offset++] = (byte)((i & mask) >> shiftbits);       }       return len + 1;     }     /**      * decode a vint from the buffer pointed at to by ptr and      * increment the offset of the ptr by the length of the      * vint.      * @param ptr a pointer to a byte array buffer      * @return the decoded vint value as an int      */     public static int vintfrombytes(immutablebyteswritable ptr) {         return (int) vlongfrombytes(ptr);     }          /**      * decode a vint from the buffer pointed at to by ptr and      * increment the offset of the ptr by the length of the      * vint.      * @param ptr a pointer to a byte array buffer      * @return the decoded vint value as a long      */     public static long vlongfrombytes(immutablebyteswritable ptr) {         final byte [] buffer = ptr.get();         final int offset = ptr.getoffset();         byte firstbyte = buffer[offset];         int len = writableutils.decodevintsize(firstbyte);         if (len == 1) {             ptr.set(buffer, offset+1, ptr.getlength());             return firstbyte;         }         long i = 0;         for (int idx = 0; idx < len-1; idx++) {             byte b = buffer[offset + 1 + idx];             i = i << 8;             i = i | (b & 0xff);         }         ptr.set(buffer, offset+len, ptr.getlength());         return (writableutils.isnegativevint(firstbyte) ? ~i : i);     } ",
        "label": 48
    },
    {
        "text": "restore testmultitableinputformat  testmultitableinputformat is removed in hbase-9009 for this test made the ci failed. but in hbase-10692 we need to add a new test testsecuremultitableinputformat which is depends on it. so we try to restore it in this issue. i rerun the test for several times and it passed. running org.apache.hadoop.hbase.mapreduce.testmultitableinputformat tests run: 5, failures: 0, errors: 0, skipped: 0, time elapsed: 314.163 sec michael stack ",
        "label": 411
    },
    {
        "text": "hbase regionserver never stops when running  hbase daemon sh stop regionserver   currently running full stack (hadoop + hbase + zookeeper) on a vm i want to reboot, every services stopped but hbase regionserver. for 2 days now, it's aligning dots on the console ...  i propose a timeout (default 600 overidable by $hbase_stop_timeout) to force kill -9 on the process. ",
        "label": 116
    },
    {
        "text": "enable gc logging by default  i think we should enable gc by default. its pretty frictionless apparently and could help in the case where folks are getting off the ground. ",
        "label": 441
    },
    {
        "text": " ui  catalog tables section isn't aligned  i attached a picture of what i got. you can see it doesn't look right. one more thing is that the page doesn't auto-refresh when you switch tabs. for example, click catalog tables, create a new table, click user tables, you don't see the new table. you need to refresh the page to see it. ",
        "label": 154
    },
    {
        "text": "secureconnection can't be closed when secureclient is stopping because interruptedexception won't be caught in secureclient setupiostreams   when hbaseclient.stop() is invoked, all cached hbaseclient.connections will be interrupted, marked as closed and then finally closed. hbaseclient.stop() won't return before all cached hbaseclient.connections have been closed. however, secureconnection might not be closed after hbaseclient.stop() is invoked because of the following code in secureclient,setupiostreams(): ... try {      ...             try {               continuesasl =                 ticket.runas(new privilegedexceptionaction<boolean>() {                   @override                   public boolean run() throws ioexception {                     return setupsaslconnection(in2, out2);                   }                 });             } catch (exception ex) {               if (rand == null) {                 rand = new random();               }               // == interruptedexception won't be caught ==               handlesaslconnectionfailure(numretries++, max_retries, ex, rand, ticket);               continue;               ...             }         }       } catch (ioexception e) {         markclosed(e);         close();         throw e;       } ... secureclient.handlesaslconnectionfailure(...) will throw interruptedexception exception(there will be thread.sleep() in secureclient.handlesaslconnectionfailure(...)) when hbaseclient#stop() is invoked. however, this interruptedexception won't be caught, making this secureconnection can't be closed. this problem might make regionserver can't exit because regionserver will wait all secureconnection closed when exiting. a simple way to fix this bug is to catch interruptedexception in secureclient#setupiostreams() and close the secureconnection. ",
        "label": 238
    },
    {
        "text": " hbtop  top n heavy hitter user and client drill downs  after hbase-15519, or after an additional change on top of it that provides necessary data in clusterstatus, add drill down top-n views of activity aggregated per user or per client ip. only a relatively small n of the heavy hitters need be tracked assuming this will be most useful when one or a handful of users or clients is generating problematic load and hbtop is invoked to learn their identity. this is a critical missing piece. after drilling down to find hot regions or tables, sometimes that is not enough, we also need to know which application or subset of clients out of many may be the source of the hot spotting load. ",
        "label": 45
    },
    {
        "text": "teststore testdeleteexpiredstorefiles is flaky  teststore.testdeleteexpiredstorefiles relies on wall clock time, if there is a blip on the machine running the test, first compaction might be delayed enough in order to compact away multiple of the files, and have the test fail. the simplest fix is to just double the time given from 1s/file to 2s/file. ",
        "label": 286
    },
    {
        "text": "the default value of  hbase regions slop  in hbase default xml is obsolete  the default value of hbase.regions.slop is 0.001 in stochasticloadbalancer (which is the default setting of hbase.master.loadbalancer.class). however, in the both hbase-default.xml and online docs, the default value is 0.2. this value is specified in baseloadbalancer (which is an abstract class) and is inherited by simpleloadbalancer. however, as simpleloadbalancer is no longer used as the default load balancer, the value is obsolete. the code structure is: baseloadbalancer.java  public abstract class baseloadbalancer implements loadbalancer {    ...    protected void setslop(configuration conf) {      this.slop = conf.getfloat(\"hbase.regions.slop\", (float) 0.2);    } stochasticloadbalancer.java  public class stochasticloadbalancer extends baseloadbalancer {    ...    @override    protected void setslop(configuration conf) {      this.slop = conf.getfloat(\"hbase.regions.slop\", 0.001f);    } i suggest to make the manual entry of hbase.regions.slop specify the different default values in different balancer classes. ",
        "label": 449
    },
    {
        "text": "zookeeper log4j property set to error on default  same output when cluster working and not working  i was having some issues with getting zookeeper running, default log level being at error meant that i didn't actually know whether the zk cluster was working or not. info is fairly noisy but might be best? ",
        "label": 247
    },
    {
        "text": "intermittent incrementcolumnvalue failure in testhregion  i first saw this in a hudson build, but can reproduce locally with enough test runs (5-10 times): ------------------------------------------------------------------------------- test set: org.apache.hadoop.hbase.regionserver.testhregion ------------------------------------------------------------------------------- tests run: 51, failures: 1, errors: 0, skipped: 0, time elapsed: 39.413 sec <<< failure! testincrementcolumnvalue_updatinginplace(org.apache.hadoop.hbase.regionserver.testhregion)  time elapsed: 0.079 sec  <<< failure! junit.framework.assertionfailederror: expected:<1> but was:<2>         at junit.framework.assert.fail(assert.java:47)         at junit.framework.assert.failnotequals(assert.java:283)         at junit.framework.assert.assertequals(assert.java:64)         at junit.framework.assert.assertequals(assert.java:195)         at junit.framework.assert.assertequals(assert.java:201)         at org.apache.hadoop.hbase.regionserver.testhregion.testincrementcolumnvalue_updatinginplace(testhregion.java:1889)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) alternately, the failure can also show up in testincrementcolumnvalue_updatinginplace_negative(): testincrementcolumnvalue_updatinginplace_negative(org.apache.hadoop.hbase.regionserver.testhregion)  time elapsed: 0.03 sec  <<< failure! junit.framework.assertionfailederror: expected:<2> but was:<3>         at junit.framework.assert.fail(assert.java:47)         at junit.framework.assert.failnotequals(assert.java:283)         at junit.framework.assert.assertequals(assert.java:64)         at junit.framework.assert.assertequals(assert.java:130)         at junit.framework.assert.assertequals(assert.java:136)         at org.apache.hadoop.hbase.regionserver.testhregion.asserticv(testhregion.java:2081)         at org.apache.hadoop.hbase.regionserver.testhregion.testincrementcolumnvalue_updatinginplace_negative(testhregion.java:1990)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) ",
        "label": 180
    },
    {
        "text": "let the hfile pretty printer print all the key values for a specific row   when using hfile pretty printer to debug hbase issues,   it would very nice to allow the pretty printer to seek to a specific row, and only print all the key values for this row. ",
        "label": 294
    },
    {
        "text": "offlining of table does not run reliably  i have a table of 4 regions made w/ pe. i cannot reliably offline it. i'm using 'disable testtable' and have traced it to ensure its not a problem in hql. what i see is that one region will get the offlined mark or maybe two.. but never all. jim in irc suggested that if we did the .table. catalog table, offlining the entry there might be more reliable than trying to offline all regions in a table. ",
        "label": 241
    },
    {
        "text": "add getscanner scan scan  list keyvaluescanner  additionalscanners  api into region interface  hregion method getscanner(scan scan, list<keyvaluescanner> additionalscanners, boolean copycellsfromsharedmem) is protected.  in apache trafodion, we need to invoke this getscanner method from a coprocessor. since it is protected, trafodion must overload the hregion class and overload this method into a public method.  it will be good to make this method public. it is very useful when one needs to combine several scan result in a single scanner. thanks,  ming ",
        "label": 46
    },
    {
        "text": "wrap long lines in recently added source files  due to ineffective line length detection, several newly added files have long lines in them.  the following is a partial list: integrationtesttablesnapshotinputformat.java  clientsideregionscanner.java  tablesnapshotinputformat.java  performanceevaluationscan.java  testtablesnapshotscanner.java  testtablesnapshotinputformat.java long lines in these files should be wrapped. ",
        "label": 191
    },
    {
        "text": " meta  by passes cache  blockcache 'false'  in a new install, if i describe '.meta.', it says: description                                                             enabled                                 {name => '.meta.', is_meta => 'true', memstore_flushsize => '16384', f true                                    amilies => [{name => 'historian', compression => 'none', versions => '                                         2147483647', ttl => '604800', blocksize => '8192', in_memory => 'false                                         ', blockcache => 'false'}, {name => 'info', compression => 'none', ver                                         sions => '10', ttl => '2147483647', blocksize => '8192', in_memory =>                                          'false', blockcache => 'true'}]}  blockcache is 'true' for the 'info' family (yes historian is still in 0.20 branch). but, if i add logging to hfile and storefile and store, blockcache is 'false' \u2013 there is no cache constructed for use by the hfile. this is killing cluster performance. it looks like a problem parsing the 'true' value in columnfamily. i'll put up a patch in the morning. meantime, marking as blocker on 0.20.4. ",
        "label": 314
    },
    {
        "text": "add total time rpc call metrics  right now we have: queuecalltime exposes the ammount of time that a call is in the queue processcalltime exposes how long a call was processing we don't have a stat for total time. it's pretty easy to think that the two metrics might not track together so getting a total time could be useful. ",
        "label": 335
    },
    {
        "text": "allow proper fsync support for hbase  at least get recommendation into 0.96 doc and some numbers running w/ this hdfs feature enabled. ",
        "label": 286
    },
    {
        "text": "improve the performance of block cache keys  doing a pure random read test on data that's 100% block cache, i see that we are spending quite some time in getblockcachekey: \"ipc server handler 19 on 62023\" daemon prio=10 tid=0x00007fe0501ff800 nid=0x6c87 runnable [0x00007fe0577f6000]  java.lang.thread.state: runnable  at java.util.arrays.copyof(arrays.java:2882)  at java.lang.abstractstringbuilder.expandcapacity(abstractstringbuilder.java:100)  at java.lang.abstractstringbuilder.append(abstractstringbuilder.java:390)  at java.lang.stringbuilder.append(stringbuilder.java:119)  at org.apache.hadoop.hbase.io.hfile.hfile.getblockcachekey(hfile.java:457)  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2.readblock(hfilereaderv2.java:249)  at org.apache.hadoop.hbase.io.hfile.hfileblockindex$blockindexreader.seektodatablock(hfileblockindex.java:209)  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$scannerv2.seekto(hfilereaderv2.java:521)  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$scannerv2.seekto(hfilereaderv2.java:536)  at org.apache.hadoop.hbase.regionserver.storefilescanner.seekatorafter(storefilescanner.java:178)  at org.apache.hadoop.hbase.regionserver.storefilescanner.seek(storefilescanner.java:111)  at org.apache.hadoop.hbase.regionserver.storefilescanner.seekexactly(storefilescanner.java:219)  at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:80)  at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:1689)  at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.<init>(hregion.java:2857) since the hfile name size is known and the offset is a long, it should be possible to allocate exactly what we need. maybe use byte[] as the key and drop the separator too. ",
        "label": 286
    },
    {
        "text": "you can't specify split points when you create a table in the shell  it would be nice to be able to specify split points when you create a table from the shell. this could be done by either passing split points in the shell:   hbase> create 't1', 'f1', {splits => ['01', '02', '03', '04']} or by passing a file with split points in it:   $ cat splits.txt 01 02 03  04   hbase> create 't1', 'f1', {splits_file => 'splits.txt'} ",
        "label": 246
    },
    {
        "text": " mvn  assembly assembly does not include hbase x x x test jar  means i can't run stuff like the performanceevaluation ",
        "label": 350
    },
    {
        "text": "fix error handling in get list get  gets   see hbase-3634 for details. the get(list<get> gets) call needs to catch (or rather use a try/finally) the exception thrown by batch() and copy the result instances over and return it. if that is not intended then we need to fix the javadoc in htableinterface to reflect the new behavior. in general it seems to make sense to check the various methods (list based put, get, delete compared to batch) and agree on the correct behavior. ",
        "label": 194
    },
    {
        "text": "convert some hfile metadata to pb  see hbase-7201 convertion should be in a manner that does not prevent our being able to read old style hfiles with writable metadata. ",
        "label": 38
    },
    {
        "text": "skipfilter javadoc is incorrect  the javadoc for skipfilter (http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/skipfilter.html) states : a wrapper filter that filters an entire row if any of the keyvalue checks do not pass. but the example same javadocs gives to support this statement is wrong. the *scan.setfilter(new skipfilter(new valuefilter(compareop.equal,  new binarycomparator(bytes.tobytes(0))));* , will only emit rows which have all column values zero. in other words it is going to skip all rows for which   valuefilter(compareop.equal, new binarycomparator(bytes.tobytes(0))) does not pass , which happen to be all non zero valued cells. in the same example a valuefilter created with compareop.not_equal will filter out the rows which have a column value zero. ",
        "label": 417
    },
    {
        "text": "automatically run rat check on mvn release builds  some of the recent hbase release failed rat checks (mvn rat:check). we should add checks likely in the mvn package phase so that this becomes a non-issue in the future. here's an example from whirr:  https://github.com/apache/whirr/blob/trunk/pom.xml line 388 for an example. ",
        "label": 248
    },
    {
        "text": "update mailing list page on web site  mailing list page on the web site has not been updated for latest mailing list changes. namely, the hbase-issues list. ",
        "label": 229
    },
    {
        "text": "custom value for bucket cache buckets key should be sorted  user can pass bucket sizes through \"hbase.bucketcache.bucket.sizes\" config entry.  the sizes are supposed to be in increasing order. validation should be added in cacheconfig#getl2(). ",
        "label": 191
    },
    {
        "text": "cleanup thrift server  remove text and profuse debug messaging  ambiguous issue name.. sorry. the thrift server has loads of gettext(..) calls. which is a local function that checks for utf8 compliance, we don't need them anywhere, because we don't use text anymore. there is probably other things we missed last time we updated the api, that we should also clean up while we're at it. open to suggestions. ",
        "label": 451
    },
    {
        "text": "introduce update a script to generate jdiff reports  we've had a few issues now where we've removed api's without deprecating or deprecating in the late release. (hbase-9142, hbase-9093) we should just have a tool that enforces our api deprecation policy as a release time check or as a precommit check. ",
        "label": 15
    },
    {
        "text": "small changes to master to make it more testable  here are some small changes in master that make it more testable. included tests stand up a master and then fake it into thinking that three regionservers are registering making master assign root and meta, etc. ",
        "label": 314
    },
    {
        "text": "missing  regioninfo file during daughter open processing  under cluster stress testing, there are a fair amount of warnings like this: 2014-01-12 04:52:29,183 warn  [test-1,8120,1389467616661-daughteropener=490a58c14b14a59e8d303d310684f0b0] regionserver.hregionfilesystem: .regioninfo file not found for region: 490a58c14b14a59e8d303d310684f0b0 this is from hregionfilesystem#checkregioninfoonfilesystem, which catches a filenotfoundexception in this case and calls writeregioninfoonfilesystem to fix up the issue. is this a bug in splitting? ",
        "label": 309
    },
    {
        "text": "readblock in hfile reader not kb  but 6mb  we found a strange problem in our read test.  it is a 5 nodes cluster.four of our 5 regionservers set \"hfile.block.cache.size\"=0.4, one of them is 0.1(we call it node a). when we random read from a 2tb data table we found node a's network reached 100mb, and others are less than 10mb. so the read speed is low.  we set node a's \"hfile.block.cache.size\"=0.2, then all the nodes's network are 10mb, that's right. to find why is this we debug with btrace and find \"readblock\" in hfile.reader become abnormal.we know hbase read a block which is 64 kb from disks and put it into blockcache. but when we set \"hfile.block.cache.size\"=0.1, it is not 64kb, it is 5~6mb one time after about 1 minute we restart hbase.  why not 64 kb? the btrace code and results are in the attachments. ",
        "label": 289
    },
    {
        "text": "fsutils waitonsafemode can incorrectly loop on standby nn  we encountered an issue where hmaster failed to start with an active nn not in safe mode and a standby nn in safemode. the relevant lines in fsutils.java show the issue:     while (dfs.setsafemode(org.apache.hadoop.hdfs.protocol.fsconstants.safemodeaction.safemode_get)) { this call skips the normal client failover from the standby to active nn, so it will loop polling the standby nn if it unfortunately talks to the standby first. ",
        "label": 441
    },
    {
        "text": "snapshot ttl  snapshots have a lifecycle that is independent from the table from which they are created. although data in a table may be stored with ttl the data files containing them become frozen by the snapshot. space consumed by expired cells will not be reclaimed by normal table housekeeping like compaction. while this is expected it can be inconvenient at scale. when many snapshots are under management and the data in various tables is expired by ttl some notion of optional ttl (and optional default ttl) for snapshots could be useful. it will help prevent the accumulation of junk files by automatically dropping the snapshot after the assigned ttl, making their data files eligible for cleaning. more comprehensive snapshot lifecycle management may be considered in the future but this one case is expected to be immediately useful given ttls on data are commonly applied for similar convenience.  ",
        "label": 473
    },
    {
        "text": "correct the directory of openjdk for jenkins  10:22:28 java_home: /home/jenkins/tools/java/latest1.7 does not exist. dockermode: attempting to switch to another. 10:22:28 setting /usr/lib/jvm/java-7-openjdk-amd64 as the java_home. 10:22:28 warning: cannot locate jdk directory /usr/lib/jvm/java-8-oracle: ignoring 10:22:28 modes:  multijdk  jenkins  robot  docker  resetrepo  unittests  it breaks the multi-jdk check, such as doc and compile. ",
        "label": 402
    },
    {
        "text": "hbase does overzealous pruning of seqids  working w/ j-d on failing replication test turned up hole in seqids made by the patch over in hbase-4789. with this patch in place we see lots of instances of the suspicious: 'last sequenceid written is empty. deleting all old hlogs' at a minimum, these lines need removing: diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/hlog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/hlog.java index 623edbe..a0bbe01 100644 --- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/hlog.java +++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/hlog.java @@ -1359,11 +1359,6 @@ public class hlog implements syncable {        // cleaning up of lastseqwritten is in the finally clause because we        // don't want to confuse getoldestoutstandingseqnum()        this.lastseqwritten.remove(getsnapshotname(encodedregionname)); -      long l = this.lastseqwritten.remove(encodedregionname); -      if (l != null) { -        log.warn(\"why is there a raw encodedregionname in lastseqwritten? name=\" + -          bytes.tostring(encodedregionname) + \", seqid=\" + l); -       }        this.cacheflushlock.unlock();      }    } ... but above is no good w/o figuring why wals are not being rotated off. ",
        "label": 314
    },
    {
        "text": "fast fail behavior for unauthenticated user  in case of unauthenticated users in secure hbase, hbase shell does a connection retry at two levels:  a) hconnection: it retries hbase.client.retries.number times in the getmaster()  b) hbaseadmin: it again retries hbase.client.retries.number times in its ctr so, hbase shell retries square number of times of the configured setting. we can make it failfast (no retries) in case the user is not authenticated (no valid kerberos credentials). ",
        "label": 199
    },
    {
        "text": "inconsistent naming for zk config parameters  i've found some misnaming of certain zk config options. make them consistent. ",
        "label": 314
    },
    {
        "text": "hbase hconnection threads max should not be configurable else you get rejectedexecutionexception  trying to set hbase.hconnection.threads.max to a lower number than its default of integer.max_value simply results in a rejectedexecutionexception when the max is reached. it seems there's no good reason to keep this configurable. ",
        "label": 154
    },
    {
        "text": "replace stringbuffer with stringbuilder for hbase server  replace the thread-safe stringbuffer class with the un-synchronized stringbuilder class for better performance. ",
        "label": 130
    },
    {
        "text": "improve coverage in package org apache hadoop hbase security  ",
        "label": 40
    },
    {
        "text": "zk dependencies   explicitly add them until zk artifacts are published to mvn repository  currently we include the binary of zookeeper but we need to add the dependencies explicitly as well ( similar to a recent issue , related to thrift ). zk depends on log4j / jline .   log4j is already in. this patch adds jline to the dependencies explicitly. ",
        "label": 266
    },
    {
        "text": "cachecontrol flags should be tunable per table schema per cf  cachecontrol flags should be tunable per table schema per cf, especially  cachedataonwrite, also cacheindexesonwrite and cachebloomsonwrite. it looks like store uses cacheconfig(configuration conf, hcolumndescriptor family) to construct the cacheconfig, so it's a simple change there to override configuration properties with values of table schema attributes if present. ",
        "label": 38
    },
    {
        "text": "include junit in the libs when packaging so that testacidgaurntee can run  if junit is not in the libs folder running the test acid command fails. ",
        "label": 154
    },
    {
        "text": "update hbase shaded content checker after guava update in hadoop branch to jre  i'm updating guava version from 11.0.2 to 27.0-jre in hadoop-15960 because of a cve. i will create a patch for branch-3.0, 3.1, 3.2 and trunk (3.3).   i wanted to be sure that hbase works with the updated guava, i compiled and run the hbase tests with my hadoop snapshot containing the updated version, but there were some issues that i had to fix: new shaded dependency: org.checkerframework new license needs to be added to license.vm: apache 2.0 ",
        "label": 176
    },
    {
        "text": "shell should allow deletions in  meta  and  root  tables  for administrative and debugging purposes, it would be nice to be able to delete rows from .meta. via the shell. the alternative is writing custom java code to do such operations, which is just ridiculous. the reality of hbase's maturity is that from time to time we're going to have to reach into the .meta. and root tables to fix things, so i think the shell should be where that happens. currently, attempting to delete from either table gives a \"non-existant table\" error. ",
        "label": 152
    },
    {
        "text": "ref guide includes dev guidance only applicable to eom versions  the ref guide section on developer guidance has this blurb: implementing writable applies pre-0.96 only in 0.96, hbase moved to protocol buffers (protobufs). the below section on writables applies to 0.94.x and previous, not to 0.96 and beyond. every class returned by regionservers must implement the writable interface. if you are creating a new class that needs to implement this interface, do not forget the default constructor. (ref) this should be removed. ",
        "label": 328
    },
    {
        "text": "disable testregionplacement  a flakey test for an unfinished feature  running on my internal rig, this test fails on each fifth run or so: failed tests:  testregionplacement.testregionplacement:163->killrandomserverandverifyassignment:240 region {encoded => 811be453eeaac4c1a0c3f079addbf14e, name => 'testregionassignment,\\x01\\x01\\x01,1443049910229.811be453eeaac4c1a0c3f079addbf14e.', startkey => '\\x01\\x01\\x01', endkey => '\\x02\\x02\\x02'} not present on any of the expected servers ",
        "label": 314
    },
    {
        "text": "allow to disable automatic shipping of dependency jars for mapreduce jobs  since hbase-3001, tablemapreduceutil.inittablemap/reducejob will automatically upload the hbase jars needed to execute a map reduce job. in my case i am building a job jar using maven's assembly plugin, this way all the necessary dependencies are in the job jar. so in such case the default behavior of hbase causes some needless upload work. it also uploads hadoop-core itself which is not necessary. therefore i propose to add a variant of the inittablemap/reducejob methods with an extra boolean argument to disable the automatic adding of dependency jars. i will attach a patch with the proposed change. note that everything works as is, this is just an optimization. ",
        "label": 83
    },
    {
        "text": "add a metric that tracks the current number of used rpc threads on the regionservers  one way to detect that you're hitting a \"john wayne\" disk[1] would be if we could see when region servers exhausted their rpc handlers. this would also be useful when tuning the cluster for your workload to make sure that reads or writes were not starving the other operations out. [1] http://hbase.apache.org/book.html#bad.disk ",
        "label": 154
    },
    {
        "text": "src saveversion sh bails on non standard bourne shells  e g  dash   src/saveversion.sh assumes shell understands the extended [[ ]] test construct. standard bourne doesn't include this syntax and some system /bin/sh versions are not bash - example is dash used as /bin/sh on debian/ubuntu. sync with hadoop core sh/saveversion.sh to pull in the regular test syntax and other updates (\"none\" scm, branch version, et al). ",
        "label": 259
    },
    {
        "text": "track meta in transition  with zk-less region assignment, user regions in transition are tracked in meta. we need a way to track meta in transition too. ",
        "label": 41
    },
    {
        "text": "handling a big rebalance  we can queue multiple instances of a close event  messes up state  this is pretty ugly. in short, on a heavily loaded cluster, we are queuing multiple instances of region close. they all try to run confusing state. long version: i have a messy cluster. its 16k regions on 8 servers. one node has 5k or so regions on it. heaps are 1g all around. my master had oome'd. not sure why but not too worried about it for now. so, new master comes up and is trying to rebalance the cluster: 2011-01-05 00:48:07,385 info org.apache.hadoop.hbase.master.loadbalancer: calculated a load balance in 14ms. moving 3666 regions off of 6 overloaded servers onto 3 less loaded servers the balancer ends up sending many closes to a single overloaded server are taking so long, the close times out in rit. we then do this:               case closed:                 log.info(\"region has been closed for too long, \" +                     \"retriggering closedregionhandler\");                 assignmentmanager.this.executorservice.submit(                     new closedregionhandler(master, assignmentmanager.this,                         regionstate.getregion()));                 break; we queue a new close (should we?). we time out a few more times (9 times) and each time we queue a new close. eventually the close succeeds, the region gets assigned a new location. then the next close pops off the eventhandler queue. here is the telltale signature of stuff gone amiss: 2011-01-05 00:52:19,379 debug org.apache.hadoop.hbase.master.assignmentmanager: forcing offline; was=testtable,0487405776,1294125523541.b1fa38bb610943e9eadc604babe4d041. state=open, ts=1294188709030 notice how state is open when we are forcing offline (it was actually just successfully opened). we end up assigning same server because plan was still around: 2011-01-05 00:52:20,705 warn org.apache.hadoop.hbase.regionserver.handler.openregionhandler: attempted open of testtable,0487405776,1294125523541.b1fa38bb610943e9eadc604babe4d041. but already online on this server but later when plan is cleared, we assign new server and we have dbl-assignment. ",
        "label": 314
    },
    {
        "text": "hbase use consistent package name dregs  i didn't get it all over in hbase-7797. elliott just noticed. ",
        "label": 314
    },
    {
        "text": "when there is a hole  loadincrementalhfiles will hang in an infinite loop   first,i explan my test steps.  1.importtsv  2.split the region  3.delete the region info from .meta.(make a hole)  4.loadincrementalhfiles (this step will hung up in an infinite loop)  i check the log,there are two issues  1.it create _tmp folder in an infinite loop.  hdfs://hacluster/output3/i/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/test_table,136.bottom  2.when slpliting the hfile,it put the first line data(1211) into two files(top and bottom)  input file=hdfs://hacluster/output3/i/3ac6ec287c644a8fb72d96b13e31f576,outfile=hdfs://hacluster/output3/i/_tmp/test_table,2.top,keyvalue=1211/i:value/1390469306407/put/vlen=1/ts=0  input file=hdfs://hacluster/output3/i/3ac6ec287c644a8fb72d96b13e31f576,outfile=hdfs://hacluster/output3/i/_tmp/test_table,2.bottom,keyvalue=1211/i:value/1390469306407/put/vlen=1/ts=0  and then i check the code.  so i think before spliting the hfile,we should check the consistency of startkey and endkey,if something wrong,we should throw the exception,and stop loadincrementalhfiles. ",
        "label": 558
    },
    {
        "text": "hbasetestingutility truncatetable  not acting like cli  i would like to discuss the behavior of the truncatetable() method of hbasetestingutility. it's currently only removing the data through a scan/delete pattern. however, the truncate command in cli is doing additional things: it disables the tables, drop, creates (with similar column descriptors) and then enables the table. i think the truncatetable() method is misleading; for example i used it to force a coprocessor to be reloaded, but it did not. of course i can disable and enable the table by myself within my unit test, but perhaps it deserves to be discussed? ",
        "label": 402
    },
    {
        "text": "  secureserver insecure versions is declared incorrectly  i just found spurious messages of the form:  2013-08-27 18:52:58,389 warn org.apache.hadoop.hbase.ipc.secureserver: incorrect header or version mismatch from <host:port> got version 3 expected version 4 version 3 means insecure and the code tries to test for it, but the insecure version are declared in set<byte> and are then tested again an int, which apparently is always false. ",
        "label": 286
    },
    {
        "text": "testofflinemetarebuildbase testmetarebuild occasionally fails  looks: https://builds.apache.org/job/hbase-trunk-security/7/testreport/org.apache.hadoop.hbase.util.hbck/testofflinemetarebuildbase/testmetarebuild/  please review, see whether the method makes sense?   if it makes sense, i will check other cases? ",
        "label": 529
    },
    {
        "text": "memcache size unreliable  multiple updates against same row/column/ts will be seen as increments to cache size on insert but when we then play the memcache at flush time, we'll only see the most recent entry and decrement the memcache size by whatever its size; memcache will be off. ",
        "label": 241
    },
    {
        "text": "persist master in memory state so on restart or failover  new instance can pick up where the old left off  today there was some good stuff up on irc on how transitions won't always make it across master failovers in multi-master deploy because transitions are kept in in-memory structure up in the master and so on master crash, the new master will be missing state on startup (todd was main promulgator of this observation and of the opinion that while master rewrite is scheduled for 0.21, some part needs to be done for 0.20.5). a few suggestions were made: transitions should be file-backed somehow, etc. let this issue be about the subset we want to do for 0.20.5. of the in-memory state queues, there is at least the master tasks queue \u2013 process region opens, closes, regionserver crashes, etc. \u2013 where tasks must be done in order and iirc, tasks are fairly idempotent (at least in the server crash case, its multi-step and we'll put the crash event back on the queue if we cannot do all steps in the one go). perhaps this queue could be done using the new queue facility in zk 3.3.0 (i haven't looked to check if possible, just suggesting). another suggestion was a file to which we'd append queue items, requeueing, and marking the file with task complete, etc. on master restart or fail-over, we'd replay the queue log. there is also the map of regions-in-transition. yesterday we learned that there is a bug where server shutdown processing does not iterate the map of regions-in-transition. this map may hold regions that are in \"opening\" or \"opened\" state but haven't yet had the fact added to .meta. by master. meantime the hosting server can crash. regions that were opening will stay in the regions-in-transition and those in opened-but-not-yet-added-to-meta will go ahead and add a crashed server to .meta. (currently regions-in-transition does not record server the region opening/open is happening on so it doesn't have enough info to be processed as part of server shutdown). regions-in-transition also needs to be persistant. on startup, regions-in-transition can get kinda hectic on a big cluster. ordering is not so important here i believe. a directory in zk might work (for 1m regions in a big cluster, that'd be about 2m creates and 2m deletes during startup \u2013 thats too much?). or we could write a wal-like log again of region transitions (we'd have to develop a little vocabulary) that got reread by a new master. ",
        "label": 247
    },
    {
        "text": "possible data loss when rs goes into gc pause while rolling hlog  there is a very corner case when bad things could happen(ie data loss): 1) rs #1 is going to roll its hlog - not yet created the new one, old one will get no more writes  2) rs #1 enters gc pause of death  3) master lists hlog files of rs#1 that is has to split as rs#1 is dead, starts splitting  4) rs #1 wakes up, created the new hlog (previous one was rolled) and appends an edit - which is lost the following seems like a possible solution: 1) master detects rs#1 is dead  2) the master renames the /hbase/.logs/<regionserver name> directory to something else (say /hbase/.logs/<regionserver name>-dead)  3) add mkdir support (as opposed to mkdirs) to hdfs - so that a file create fails if the directory doesn't exist. dhruba tells me this is very doable.  4) rs#1 comes back up and is not able create the new hlog. it restarts itself. ",
        "label": 341
    },
    {
        "text": "canary tool does not return non zero exit code when one of regions is in stuck state  2016-02-05 12:24:18,571 error [pool-2-thread-7] tool.canary - read from region can_1,\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00,1454667477865.00e77d07b8defe10704417fb99aa0418. column family 0 failed org.apache.hadoop.hbase.client.retriesexhaustedexception: failed after attempts=2, exceptions: fri feb 05 12:24:15 gmt 2016, org.apache.hadoop.hbase.client.rpcretryingcaller@54c9fea0, org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: region can_1,\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00,1454667477865.00e77d07b8defe10704417fb99aa0418. is not online on isthbase02-dnds1-3-crd.eng.sfdc.net,60020,1454669984738 at org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname(hregionserver.java:2852) at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:4468) at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:2984) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:31186) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2149) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:104) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:133) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:108) at java.lang.thread.run(thread.java:745) -------- -bash-4.1$ echo $? 0 below code prints the error but it does sets/returns the exit code. due to this tool can't be integrated with nagios or other alerting. ideally it should return error for failures. as pre the documentation: <snip>  this tool will return non zero error codes to user for collaborating with other monitoring tools, such as nagios. the error code definitions are: private static final int usage_exit_code = 1;  private static final int init_error_exit_code = 2;  private static final int timeout_error_exit_code = 3;  private static final int error_exit_code = 4; </snip> org.apache.hadoop.hbase.tool.canary.regiontask  public void read() {       ....       try {         table = connection.gettable(region.gettable());         tabledesc = table.gettabledescriptor();       } catch (ioexception e) {         log.debug(\"sniffregion failed\", e);         sink.publishreadfailure(region, e);        ...         return null;       } ",
        "label": 441
    },
    {
        "text": "configurable default durability for synchronous wal  at present we do not have an option to hsync wal edits to the disk for better durability. in our local tests we see 10-15% latency impact of using hsync instead of hflush which is not very high.   we should have a configurable option to hysnc wal edits instead of just sync/hflush which will call the corresponding api on the hadoop side. currently hbase handles both sync_wal and fsync_wal as the same calling fsdataoutputstream sync/hflush on the hadoop side. this can be modified to let fsync_wal call hsync on the hadoop side instead of sync/hflush. we can keep the default value to sync as the current behavior and hsync can be enabled based on explicit configuration. ",
        "label": 195
    },
    {
        "text": "thrift server metrics should be long instead of int  as we measure our thrift call latencies in nanoseconds, we need to make latencies long instead of int everywhere. ",
        "label": 324
    },
    {
        "text": "reenable testnamespaceauditor  was disabled over in hbase-14569 because flakey. reenable when fixed. ",
        "label": 198
    },
    {
        "text": "basescanner says  current assignment of x is not valid  over and over for same region  from irc today 12:41 < cmorgan> hey guys. i'm having a recent  issue with a single node cluster running 0.20.4. after stopping for a backup i now get region assignment churn. seems master keeps thinking that region                  assignment is not valid even when it is. following is a log snippet: 12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        hmaster] debug ter.regionserveroperationqueue  - processing todo: pendingopenoperation from localhost.,7802,1274425405680 12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        hmaster] info  e.master.regionserveroperation  - net_troove_coin_account_accountcredentials,,1234913258116 open on 127.0.0.1:7802 12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        hmaster] info  e.master.regionserveroperation  - updated row net_troove_coin_account_accountcredentials,,1234913258116 in region .meta.,,1 with                  startcode=1274425405680, server=127.0.0.1:7802 12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        hmaster] debug ter.regionserveroperationqueue  - processing todo: pendingopenoperation from localhost.,7802,1274425405680 12:41 < cmorgan> [21/05/10 00:59:42] 3443246 [        hmaster] info  e.master.regionserveroperation  - net_troove_application_request_temporaryrequest,,1234913268355 open on 127.0.0.1:7802 12:41 < cmorgan> [21/05/10 00:59:42] 3443247 [        hmaster] info  e.master.regionserveroperation  - updated row net_troove_application_request_temporaryrequest,,1234913268355 in region .meta.,,1 with                  startcode=1274425405680, server=127.0.0.1:7802 12:41 < cmorgan> [21/05/10 00:59:42] 3443247 [ger.metascanner] debug adoop.hbase.master.basescanner  - current assignment of net_troove_coin_account_accountentry,,1271448856984 is not valid;                  serveraddress=127.0.0.1:7802, startcode=1274425405680 unknown. 12:41 < cmorgan> [21/05/10 00:59:42] 3443248 [ger.metascanner] debug adoop.hbase.master.basescanner  - current assignment of net_troove_coin_account_accountentry-base_entryday_descending,,1273266418876                  is not valid;  serveraddress=127.0.0.1:7802, startcode=1274425405680 unknown. 12:41 < cmorgan> [21/05/10 00:59:42] 3443251 [ger.metascanner] debug adoop.hbase.master.basescanner  - current assignment of net_troove_coin_bank_bankstatement,,1266433980935 is not valid;                  serveraddress=127.0.0.1:7802, startcode=1274425405680 unknown. 12:58 < cmorgan> stack: i'd been running with 0.20.4 for a week or so starting/stopping every night. now this happens... 14:11 < cmorgan> stack: some more info: on our mini production server the regionserver is getting \"my address is localhost.:7802\" (notice the dot after localhost). but the master is also sometimes                  referring to it as 127.0.0.1. i just used the same data and config on my laptop, and its binding to my external lan ip (\"my address is 10.0.1.4:7802\"). under this setup hbase comes up                  stable (no region assignment churn). looking at this, i think issue is that when we register a server we use a getservername on a hserverinfo provided by the regionserver (though we are on the master side) but basescanner uses a getservername that is made by doing a dns lookup using the ip that it finds in the server column of .meta. my sense is that is possible for the regionserver hostname and what the master finds when it does a lookup against dns can disagree, fatally. this issue seems popular over last few weeks. was reported at least once more on a standalone instance and also on krispykola's 15-node ec2 cluster (he went back to 0.20.3 and then it went away?). it made for what looked like double-assignment in his case (our attempt at caching dns names may be amiss \u2013 i tihnk tht the main diff between 0.20.3 and 0.20.4 in this area). my thought is to purge dns from the hserverinfo passed by the rs to master on startup and heartbeating and to use ips only (and even then, the ip that the master tells the rs to use, its remote address as seen by the master). we might have to do this fix for 0.20.5 since it seems to happen more in 0.20.4. i'm looking into this. opinions welcome. ",
        "label": 314
    },
    {
        "text": "testmetareadereditor is broken in trunk  hangs  it gets stuck. lars f reported it today. ",
        "label": 314
    },
    {
        "text": "rename rest server from main to restserver  with jps, the rest server is shown as main because the class is main. is there any special reason for it to be called main? if not, i suggest we change it to restserver instead. ",
        "label": 242
    },
    {
        "text": "excessive readpoints checks in storefilescanner  it seems that usage of skipkvsnewerthanreadpoint in storefilescanner can be greatly reduced or even eliminated all together (hfiles are immutable and no new kvs can be inserted after scanner instance is created). the same is true for memstorescanner which checks readpoint on every next() and seek(). each readpoint check is threadlocal.get() and it is quite expensive. ",
        "label": 286
    },
    {
        "text": "provide a asyncadminbuilder to create new asyncadmin instance  similar with asynctablebuilder, user can only set the configs they care about when create a new asyncadmin instance. it is easy to update the rpc timeout or operation timeout config when they take some time-consuming admin operations. ",
        "label": 187
    },
    {
        "text": "memory size of java objects   make cacheable objects implement heapsize  in order to make our efforts as generic as possible over in hbase-1186, we need to add heapsize functionality to more classes. this issue is so we have a central place to post code and discuss the issues related to determining the size of a java object in memory. erik holstad and i have made good progress and have done significant research into other java caching projects. we will report our findings here. ",
        "label": 247
    },
    {
        "text": " meta  timeout value is incorrect  we are seeing the timeout value of 2147483647ms which is ~24days. that seems a little high for not talking to meta. 2012-08-27 21:57:04,572 info org.apache.hadoop.hbase.regionserver.compactsplitthread: running rollback/cleanup of failed split of table,pge:3659323005:read:\\x7f\\xff\\xfe\\xc  6\\xc9\\x9ds\\x7f,1346030679280.94cf5ab361b0e7d92b0b263ffb995852.; timed out (2147483647ms)  org.apache.hadoop.hbase.notallmetaregionsonlineexception: timed out (2147483647ms)  at org.apache.hadoop.hbase.catalog.catalogtracker.waitformeta(catalogtracker.java:390)  at org.apache.hadoop.hbase.catalog.catalogtracker.waitformetaserverconnectiondefault(catalogtracker.java:422)  at org.apache.hadoop.hbase.catalog.metaeditor.offlineparentinmeta(metaeditor.java:109)  at org.apache.hadoop.hbase.regionserver.splittransaction.execute(splittransaction.java:290)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.split(compactsplitthread.java:156)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:87)  2012-08-27 21:57:04,585 fatal org.apache.hadoop.hbase.regionserver.hregionserver: aborting region server servername=hdpr002.va.cust.it,60020,1345819108288, load=(requests=102, regions=187, usedheap=4699, maxheap=12281): abort; we got an error after point-of-no-return ",
        "label": 275
    },
    {
        "text": "importtsv fails if rowkey length exceeds max row length  importtsv maptask fails with this error, when long row key (exceeds max_row_length) was given. 11/03/30 04:59:16 info mapred.jobclient: task id : attempt_201103252231_0077_m_000003_0, status : failed  java.lang.illegalargumentexception: row key is invalid  at org.apache.hadoop.hbase.client.put.<init>(put.java:101)  at org.apache.hadoop.hbase.client.put.<init>(put.java:80)  at org.apache.hadoop.hbase.client.put.<init>(put.java:71)  at org.apache.hadoop.hbase.mapreduce.importtsv$tsvimporter.map(importtsv.java:235)  at org.apache.hadoop.hbase.mapreduce.importtsv$tsvimporter.map(importtsv.java:190)  at org.apache.hadoop.mapreduce.mapper.run(mapper.java:144)  at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:646)  at org.apache.hadoop.mapred.maptask.run(maptask.java:322)  at org.apache.hadoop.mapred.child$4.run(child.java:240)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1115)  at org.apache.hadoop.mapred.child.main(child.java:234) ",
        "label": 270
    },
    {
        "text": "remove asked if splittable log messages  i have found this log message in hbase log: 2017-11-22 11:16:54,133 info  [rpcserver.priority.fpbq.fifo.handler=5,queue=0,port=52586] regionserver.hregion(1309): asked if splittable true 0a66d6e20801eec2c6cd1204fedde592 java.lang.throwable: logging: remove at org.apache.hadoop.hbase.regionserver.hregion.issplittable(hregion.java:1310) at org.apache.hadoop.hbase.regionserver.rsrpcservices.getregioninfo(rsrpcservices.java:1665) at org.apache.hadoop.hbase.shaded.protobuf.generated.adminprotos$adminservice$2.callblockingmethod(adminprotos.java:28159) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:406) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:325) at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:305) still we need this? it was introduced in commit dc1065a85 by michael stack and matteo bertozzi. ",
        "label": 60
    },
    {
        "text": "region is on service before openregionhandler completes  may cause data loss  openregionhandler in regionserver is processed as the following steps: 1.openregion()(through it, closed = false, closing = false) 2.addtoonlineregions(region) 3.update .meta. table  4.update zk's node state to rs_zk_region_opend we can find that region is on service before step 4.  it means client could put data to this region after step 3.  what will happen if step 4 is failed processing?  it will execute openregionhandler#cleanupfailedopen which will do closing region, and master assign this region to another regionserver.  if closing region is failed, the data which is put between step 3 and step 4 may loss, because the region has been opend on another regionserver and be put new data. therefore, it may not be recoverd through replayrecoverededit() because the edit's logseqid is smaller than current region seqid. ",
        "label": 107
    },
    {
        "text": "set version to in branch for first rc of  ",
        "label": 187
    },
    {
        "text": "show quota infos in master ui  add a page in master ui to show the following quota infos: if rpc throttle is enabled; if exceed throttle quota is enabled; namespace throtlles; user throttles. ",
        "label": 500
    },
    {
        "text": "rowcounter argument input parse error  i'm tried to use https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/rowcounter.java code and package a new jar then excuted following shell script: hadoop jar test.jar <tablename> --range=row001,row002 cf:c2 then i got \"nosuchcolumnfamilyexception\". it seems input argument parsing problem. and i tried to add continue;  after #l123 to avoid \"--range=*\" string be appended to qualifer. then the problem seems solve. \u2013  data in table: row cf:c1 cf:c2 cf:c3 cf:c4 row001 v1 v2     row002   v2 v3   row003     v3 v4 row004 v1     v4 exception message: org.apache.hadoop.hbase.regionserver.nosuchcolumnfamilyexception: org.apache.hadoop.hbase.regionserver.nosuchcolumnfamilyexception: column family --range=row001,row002 does not exist in region frank_rowcounttest1,,1446191360354.6c52c71a82f0fa041c467002a2bf433c. in table 'frank_rowcounttest1', {name => 'cf', data_block_encoding => 'none', bloomfilter => 'row', replication_scope => '0', compression => 'none', versions => '1', ttl => 'forever', min_versions => '0', keep_deleted_cells => 'false', blocksize => '65536', in_memory => 'false', blockcache => 'true'} ",
        "label": 9
    },
    {
        "text": "with accessdeniedexception  hbase shell would be better to just display the error message to be user friendly  when access unauthorized resource like table, accessdeniedexception will be thrown. in hbase shell, the error message with stack trace will be displayed as follows. it would be better to just display the error message avoiding the stack trace to be user friendly. error: org.apache.hadoop.hbase.security.accessdeniedexception: insufficient permissions for user 'u1' for scanner open on table t1         at org.apache.hadoop.hbase.security.access.accesscontroller.prescanneropen(accesscontroller.java:1116)         at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.prescanneropen(regioncoprocessorhost.java:1293)         at org.apache.hadoop.hbase.regionserver.hregionserver.scan(hregionserver.java:3026)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26971)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2083)         at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1820)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:113)         at java.lang.thread.run(thread.java:662) ",
        "label": 497
    },
    {
        "text": "incorrect assert condition in orderedbytes decoding  the following assert condition is incorrect when decoding blob var byte array.     assert t == 0 : \"unexpected bits remaining after decoding blob.\"; when the number of bytes to decode is multiples of 8 (i.e the original number of bytes is multiples of 7), this assert may fail. ",
        "label": 339
    },
    {
        "text": "mapred tablesnapshotinputformat is missing interfaceaudience annotation  looks like this slipped through review on hbase-11137. ",
        "label": 339
    },
    {
        "text": "hbase default xml hbase status multicast address port does not match code  in hbase-default.xml +  <property> +    <name>hbase.status.multicast.address.port</name> +    <value>6100</value> +    <description> +      multicast port to use for the status publication by multicast. +    </description> +  </property> in hconstants it was 60100.   public static final string status_multicast_port = \"hbase.status.multicast.port\";   public static final int default_status_multicast_port = 16100; (it was 60100 in the code for 0.96 and 0.98.) i lean towards going with the code as opposed to the config file. ",
        "label": 340
    },
    {
        "text": "there are multiple hconstants for configuring zookeeper timeout  from ted yu to dev@: there are two constants with the same value:  hconstants.zookeeper_session_timeout and hconstants.zk_session_timeout hconstants.zookeeper_session_timeout is only used in tests.  hconstants.zk_session_timeout is used by zkutil shall we remove hconstants.zookeeper_session_timeout and let tests use  hconstants.zk_session_timeout ? ",
        "label": 339
    },
    {
        "text": "allow hmsg's carry a payload  e g  exception that happened over on the remote side   will make it so can just look in master log and get a general sense of failure types, etc., across the cluster. ",
        "label": 314
    },
    {
        "text": "document bulk loaded hfile replication  after hbase-13153 is committed we need to add that information under the cluster replication section in hbase book. ",
        "label": 483
    },
    {
        "text": "result current  throws arrayindexoutofboundsexception after calling advance   on a result object, if current() method is called after advance() returns false, this throws an arrayindexoutofboundsexception. the expectation here is that it should return a null value. ",
        "label": 311
    },
    {
        "text": "expand  thirdparty  reference to give examples of setting netty location in common testing modules  we should give examples of how folks using e.g. the maven surefire plugin can configure netty correctly. e.g.:  you'll need to update your surefire plugin settings to define the given flag: ...       <plugin>         <artifactid>maven-surefire-plugin</artifactid>         <configuration>           <systempropertyvariables>             <org.apache.hadoop.hbase.shaded.io.netty.packageprefix>org.apache.hadoop.hbase.shaded.</org.apache.hadoop.hbase.shaded.io.netty.packageprefix>           </systempropertyvariables>         </configuration>       </plugin> ... maybe failsafe too? maybe scala-test? ",
        "label": 314
    },
    {
        "text": "make important configurations more obvious to new users  over the last 2 weeks, i encountered many situations where people didn't set file descriptors and xcievers higher and that was causing a ton of problems that are hard to debug if you're not used to them. to improve that we should: refuse to start hbase if ulimit -n returns some small number smaller than 2048, or at least print out in big red blinking letters that the current configuration is bad and then link to a simple troubleshooting entry on the wiki. write a clearer getting started document where we don't give as much explanations but add more stuff like \"this is what your hbase-site.xml/hdfs-site/xml should look like now\" and give a complete file example. at this point we don't even give a number for xcievers and we expect new users to come up with one. any other low hanging fruit others can think of? ",
        "label": 330
    },
    {
        "text": "make rowcounter mr job do the key only fast scanning of hbase  ",
        "label": 122
    },
    {
        "text": "reimplement replicationpeers with the new replication storage interface  ",
        "label": 514
    },
    {
        "text": " hbase thirdparty  upgrade thirdparty dependencies  first guava has a cve so we need to upgrade to at least 26.0, better to the newest 27.1. and we can also upgrade the other dependencies to the newest version at the same time. ",
        "label": 314
    },
    {
        "text": "increment   decrement of rpccount in rpcserver connection is not protected by synchronization  here is related code:     /* decrement the outstanding rpc count */     protected void decrpccount() {       rpccount--;     }     /* increment the outstanding rpc count */     protected void incrpccount() {       rpccount++;     } even though rpccount is volatile, in non atomic operations (increment / decrement) different threads may get unexpected result. see http://stackoverflow.com/questions/7805192/is-a-volatile-int-in-java-thread-safe ",
        "label": 441
    },
    {
        "text": "testzkprocedure testmulticohortwithmembertimeoutduringprepare is flaky  recently there is a failure from jenkins build:https://builds.apache.org/job/hbase-0.98/364/testreport/junit/org.apache.hadoop.hbase.procedure/testzkprocedure/testmulticohortwithmembertimeoutduringprepare/. below are related log message and member: 'one' joining twice: 2014-06-29 19:26:34,101 debug [member: 'three' subprocedure-pool11-thread-1] procedure.zkprocedurememberrpcs(237): member: 'one' joining acquired barrier for procedure (op) in zk 2014-06-29 19:26:34,101 debug [member: 'one' subprocedure-pool9-thread-1] procedure.subprocedure(162): subprocedure 'op' locally acquired 2014-06-29 19:26:34,101 debug [member: 'one' subprocedure-pool9-thread-1] procedure.zkprocedurememberrpcs(237): member: 'one' joining acquired barrier for procedure (op) in zk ",
        "label": 233
    },
    {
        "text": "hmaster getregiontableclosest should not return null for closed regions  raised in the review of hbase-2560: there are a couple functions in hmaster which return null when a region has not been deployed. instead, they should return a pair<hregioninfo, hserveraddress> where only the address is null (since the info is still in meta!) ",
        "label": 453
    },
    {
        "text": "address part of config option hbase regionserver unnecessary  we have a configuration option \"hbase.regionserver\" that specifies address + port the region servers should bind to. i believe all of our users don't require the address part of it and always leave it as 0.0.0.0. most people rsync their configs to all the machines in the cluster, so anything other than 0.0.0.0 doesn't really make sense. we should change this option into just hbase.regionserver.port, like we have with the master now. ",
        "label": 342
    },
    {
        "text": "fix javadoc warning in storefilemanager java  from https://builds.apache.org/job/precommit-hbase-build/7779/artifact/trunk/patchprocess/patchjavadocwarnings.txt : [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/storefilemanager.java:53: warning - @param argument \"sf\" is not a parameter name. ",
        "label": 406
    },
    {
        "text": "display hbase metrics in ui  in versions of hadoop, there is a metrics servlet that will dump out metrics for all registered contexts. attached is some doc for hbase on how to get the metrics servlet displaying metrics. this issue is about adding a bit to the ui that searches for the metrics servlet and if present, show a link on master and rs pages (or not...). ",
        "label": 154
    },
    {
        "text": "display configured max size of memstore and cache on rs ui  displaying the configured size of memstore and cache will help non-admin users understand the cluster capacity. attached screenshot with proposed usability related changes and the current rs ui. ",
        "label": 72
    },
    {
        "text": "why do we have a lock in compactsplitthread when only ever one instance of this thread   this lock in compactsplitthread doesn't do anything best as i can tell:   private final reentrantlock lock = new reentrantlock(); if two instances of compactsplitthread, it would not prevent the two threads contending since its local to the instance. remove it. ",
        "label": 246
    },
    {
        "text": " rest  support impersonation  currently, our client api uses a fixed user: the current user. it should accept a user passed in, if authenticated. ",
        "label": 242
    },
    {
        "text": "the heapallocation in webui is not accurate  heapallocation in webui is always 0, the same reason as hbase-22663 ",
        "label": 521
    },
    {
        "text": "synctable  sourcehashdir is supposed to be optional but won't work without  1) synctable code is contradictory. usage said sourcehashdir is optional (https://github.com/apache/hbase/blob/ad3feaa44800f10d102255a240c38ccf23a82d49/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/synctable.java#l687). however, the command won't run if sourcehashdir is missing (https://github.com/apache/hbase/blob/ad3feaa44800f10d102255a240c38ccf23a82d49/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/synctable.java#l83-l85) 2) there is no documentation on how to create the desired sourcehash ",
        "label": 125
    },
    {
        "text": "wal splitting dirs are not deleted after replay   i build a small cluster (20 nodes, several hundred regions) with hbase-0.98.4. and i found some splitting directories in /hbase/wals/ today, which is very strange because those logs should have been replayed and deleted. even though the zk nodes of the dead rs had been deleted, these splitting directories still can cause a serious trouble for cluster restart. it resplitted and replayed all the splitting directories every time i restart my cluster, and cost a huge amount of time. can't imagine what could happened if it's a cluster with hundreds of nodes and tens of thousands of regions. found 56 items  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:21 /hbase/wals/hdpdev1.cm6.tbsite.net,60020,1406714828440-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev1.cm6.tbsite.net,60020,1406716991836-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:16 /hbase/wals/hdpdev1.cm6.tbsite.net,60020,1406778815585  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:26 /hbase/wals/hdpdev10.cm6.tbsite.net,60020,1406526862752-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev10.cm6.tbsite.net,60020,1406716933471-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:02 /hbase/wals/hdpdev10.cm6.tbsite.net,60020,1406778815536  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:26 /hbase/wals/hdpdev11.cm6.tbsite.net,60020,1406526862802-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev11.cm6.tbsite.net,60020,1406716992986-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:14 /hbase/wals/hdpdev11.cm6.tbsite.net,60020,1406778815552  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:15 /hbase/wals/hdpdev12.cm6.tbsite.net,60020,1406526862752-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev12.cm6.tbsite.net,60020,1406716992874-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:20 /hbase/wals/hdpdev12.cm6.tbsite.net,60020,1406778816074  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:27 /hbase/wals/hdpdev13.cm6.tbsite.net,60020,1406526862832-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev13.cm6.tbsite.net,60020,1406716992753-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:15 /hbase/wals/hdpdev13.cm6.tbsite.net,60020,1406857929773  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:26 /hbase/wals/hdpdev14.cm6.tbsite.net,60020,1406526862736-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev14.cm6.tbsite.net,60020,1406716992923-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:25 /hbase/wals/hdpdev14.cm6.tbsite.net,60020,1406778815595  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:19 /hbase/wals/hdpdev15.cm6.tbsite.net,60020,1406526862821-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev15.cm6.tbsite.net,60020,1406716993082-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:15 /hbase/wals/hdpdev15.cm6.tbsite.net,60020,1406778815578  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:21 /hbase/wals/hdpdev16.cm6.tbsite.net,60020,1406526862816-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev16.cm6.tbsite.net,60020,1406716992787-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:15 /hbase/wals/hdpdev16.cm6.tbsite.net,60020,1406778816006  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev17.cm6.tbsite.net,60020,1406716992814-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:26 /hbase/wals/hdpdev17.cm6.tbsite.net,60020,1406778815579  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev18.cm6.tbsite.net,60020,1406716993051-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:24 /hbase/wals/hdpdev18.cm6.tbsite.net,60020,1406778815587  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:23 /hbase/wals/hdpdev19.cm6.tbsite.net,60020,1406526862720-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev19.cm6.tbsite.net,60020,1406716992736-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:22 /hbase/wals/hdpdev19.cm6.tbsite.net,60020,1406865567732  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:21 /hbase/wals/hdpdev2.cm6.tbsite.net,60020,1406778815846  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:27 /hbase/wals/hdpdev20.cm6.tbsite.net,60020,1406714346484-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev20.cm6.tbsite.net,60020,1406716991741-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:27 /hbase/wals/hdpdev20.cm6.tbsite.net,60020,1406778815555  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:25 /hbase/wals/hdpdev3.cm6.tbsite.net,60020,1406714830504-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev3.cm6.tbsite.net,60020,1406716992137-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:24 /hbase/wals/hdpdev3.cm6.tbsite.net,60020,1406778815585  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:21 /hbase/wals/hdpdev4.cm6.tbsite.net,60020,1406714829881-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev4.cm6.tbsite.net,60020,1406716992118-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:13 /hbase/wals/hdpdev4.cm6.tbsite.net,60020,1406864942962  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:10 /hbase/wals/hdpdev5.cm6.tbsite.net,60020,1406526862790-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:22 /hbase/wals/hdpdev5.cm6.tbsite.net,60020,1406715762598-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev5.cm6.tbsite.net,60020,1406716991309-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-31 15:39 /hbase/wals/hdpdev5.cm6.tbsite.net,60020,1406778815529-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:13 /hbase/wals/hdpdev5.cm6.tbsite.net,60020,1406941782379  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev6.cm6.tbsite.net,60020,1406716992903-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:19 /hbase/wals/hdpdev6.cm6.tbsite.net,60020,1406778815530  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:24 /hbase/wals/hdpdev7.cm6.tbsite.net,60020,1406526862796-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev7.cm6.tbsite.net,60020,1406716993002-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:24 /hbase/wals/hdpdev7.cm6.tbsite.net,60020,1406778815785  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev8.cm6.tbsite.net,60020,1406716991377-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:21 /hbase/wals/hdpdev8.cm6.tbsite.net,60020,1406778815557  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:28 /hbase/wals/hdpdev9.cm6.tbsite.net,60020,1406716099285-splitting  drwxr-xr-x - hadoop hadoop 0 2014-07-30 18:43 /hbase/wals/hdpdev9.cm6.tbsite.net,60020,1406716991336-splitting  drwxr-xr-x - hadoop hadoop 0 2014-08-02 19:10 /hbase/wals/hdpdev9.cm6.tbsite.net,60020,1406778815554 ",
        "label": 466
    },
    {
        "text": " findbugs  fix null pointer warnings   see https://builds.apache.org/job/precommit-hbase-build/1313//artifact/trunk/patchprocess/newpatchfindbugswarnings.html fix the np category ",
        "label": 458
    },
    {
        "text": "reuse of keyvalue during log replay could cause the wrong data to be used  our meta table got a row key of metarow in it. hard to explain how it happened, but under code inspection stack found that we are reusing the same kv instance for each replayed key. ",
        "label": 247
    },
    {
        "text": "hfile and memstore should maintain minimum and maximum timestamps  in order to fix hbase-1485 and hbase-29, it would be very helpful to have hfile and memstore track their maximum and minimum timestamps. this has the following nice properties: for a straight get, if an entry has been already been found with timestamp x, and x >= hfile.maxtimestamp, the hfile doesn't need to be checked. thus, the current fast behavior of get can be maintained for those who use strictly increasing timestamps, but \"correct\" behavior for those who sometimes write out-of-order. for a scan, the \"latest timestamp\" of the storage can be used to decide which cell wins, even if the timestamp of the cells is equal. in essence, rather than comparing timestamps, instead you are able to compare tuples of (row timestamp, storage.max_timestamp) in general, min_timestamp(storage a) >= max_timestamp(storage b) if storage a was flushed after storage b. ",
        "label": 357
    },
    {
        "text": "testreplicationqueuefailover  runs for a minute  spews 4million lines complaining 'filesystem closed'  has an npe  and still passes   i was trying to look at why the odd time hudson oomes trying to make a report on 0.95 build #4 https://builds.apache.org/job/hbase-0.95/4/console: error: failed to archive test reports hudson.util.ioexception2: remote file operation failed: /home/jenkins/jenkins-slave/workspace/hbase-0.95 at hudson.remoting.channel@151a4e3e:ubuntu3 at hudson.filepath.act(filepath.java:861) at hudson.filepath.act(filepath.java:838) at hudson.tasks.junit.junitparser.parse(junitparser.java:87) at  ... caused by: java.lang.outofmemoryerror: java heap space at java.nio.heapcharbuffer.<init>(heapcharbuffer.java:57) at java.nio.charbuffer.allocate(charbuffer.java:329) at java.nio.charset.charsetdecoder.decode(charsetdecoder.java:792) at java.nio.charset.charset.decode(charset.java:791) at hudson.tasks.junit.suiteresult.<init>(suiteresult.java:215) ... we are trying to allocate a big buffer and failing. looking at reports being generated, we have quite a few that are > 10mb in size: durruti:0.95 stack$ find hbase-* -type f -size +10000k -exec ls -la {} \\; -rw-r--r--@ 1 stack  staff  11126492 feb 27 06:14 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.backup.testhfilearchiving-output.txt -rw-r--r--@ 1 stack  staff  13296009 feb 27 05:47 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.testfromclientside3-output.txt -rw-r--r--@ 1 stack  staff  10541898 feb 27 05:47 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.testmultiparallel-output.txt -rw-r--r--@ 1 stack  staff  25344601 feb 27 05:51 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.testrestoresnapshotfromclient-output.txt -rw-r--r--@ 1 stack  staff  17966969 feb 27 06:12 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.regionserver.testendtoendsplittransaction-output.txt -rw-r--r--@ 1 stack  staff  17699068 feb 27 06:09 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.regionserver.wal.testhlogsplit-output.txt -rw-r--r--@ 1 stack  staff  17701832 feb 27 06:07 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.regionserver.wal.testhlogsplitcompressed-output.txt -rw-r--r--@ 1 stack  staff  717853709 feb 27 06:17 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.replication.testreplicationqueuefailover-output.txt -rw-r--r--@ 1 stack  staff  563616793 feb 27 06:17 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.replication.testreplicationqueuefailovercompressed-output.txt ... with testreplicationqueuefailover* being order of magnitude bigger than the others. looking in the test i see both spewing between 800 and 900 thousand lines in about a minute. here is their fixation: 8908998 2013-02-27 06:17:48,176 error [regionserver:1;hemera.apache.org,35712,1361945801803.logsyncer] wal.fshlog$logsyncer(1012): error while syncing, requesting close of hlog. 8908999 java.io.ioexception: filesystem closed 8909000 ,...at org.apache.hadoop.hdfs.dfsclient.checkopen(dfsclient.java:319) 8909001 ,...at org.apache.hadoop.hdfs.dfsclient.access$1200(dfsclient.java:78) 8909002 ,...at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.sync(dfsclient.java:3843) 8909003 ,...at org.apache.hadoop.fs.fsdataoutputstream.sync(fsdataoutputstream.java:97) 8909004 ,...at org.apache.hadoop.io.sequencefile$writer.syncfs(sequencefile.java:999) 8909005 ,...at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.sync(sequencefilelogwriter.java:248) 8909006 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog.syncer(fshlog.java:1120) 8909007 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog.syncer(fshlog.java:1058) 8909008 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog.sync(fshlog.java:1228) 8909009 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog$logsyncer.run(fshlog.java:1010) 8909010 ,...at java.lang.thread.run(thread.java:722) 8909011 2013-02-27 06:17:48,176 fatal [regionserver:1;hemera.apache.org,35712,1361945801803.logsyncer] wal.fshlog(1140): could not sync. requesting close of hlog 8909012 java.io.ioexception: filesystem closed 8909013 ,...at org.apache.hadoop.hdfs.dfsclient.checkopen(dfsclient.java:319) 8909014 ,...at org.apache.hadoop.hdfs.dfsclient.access$1200(dfsclient.java:78) 8909015 ,...at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.sync(dfsclient.java:3843) 8909016 ,...at org.apache.hadoop.fs.fsdataoutputstream.sync(fsdataoutputstream.java:97) 8909017 ,...at org.apache.hadoop.io.sequencefile$writer.syncfs(sequencefile.java:999) 8909018 ,...at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.sync(sequencefilelogwriter.java:248) 8909019 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog.syncer(fshlog.java:1120) 8909020 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog.syncer(fshlog.java:1058) 8909021 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog.sync(fshlog.java:1228) 8909022 ,...at org.apache.hadoop.hbase.regionserver.wal.fshlog$logsyncer.run(fshlog.java:1010) 8909023 ,...at java.lang.thread.run(thread.java:722) ... these tests are 'succeeding'? i also see in both:    3891 java.lang.nullpointerexception    3892 ,...at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.seek(sequencefilelogreader.java:261)    3893 ,...at org.apache.hadoop.hbase.replication.regionserver.replicationhlogreadermanager.seek(replicationhlogreadermanager.java:103)    3894 ,...at org.apache.hadoop.hbase.replication.regionserver.replicationsource.readallentriestoreplicateornextfile(replicationsource.java:415)    3895 ,...at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:333) ",
        "label": 233
    },
    {
        "text": "incremental backup and bulk loading  currently, incremental backup is based on wal files. bulk data loading bypasses wals for obvious reasons, breaking incremental backups. the only way to continue backups after bulk loading is to create new full backup of a table. this may not be feasible for customers who do bulk loading regularly (say, every day). here is the review board (out of date):  https://reviews.apache.org/r/54258/ in order not to miss the hfiles which are loaded into region directories in a situation where postbulkloadhfile() hook is not called (bulk load being interrupted), we record hfile names thru precommitstorefile() hook.  at time of incremental backup, we check the presence of such hfiles. if they are present, they become part of the incremental backup image. here is review board:  https://reviews.apache.org/r/57790/ google doc for design:  https://docs.google.com/document/d/1acclsechdvzvsasorgqqrnrlogx4mnyibvau7lq5lje ",
        "label": 441
    },
    {
        "text": "if new master crashes  restart is messy  if master crashes, the cluster-is-up flag is left stuck on. on restart of cluster, regionservers may come up before the master. they'll have registered themselves in zk by time the master assumes its role and master will think its joining an up and running cluster when in fact this is a fresh startup. other probs. are that there'll be a root region that is bad up in zk. same for meta and at moment we're not handling bad root and meta very well. here's sample of kinda of issues we're running into: 2010-09-25 23:53:13,938 fatal org.apache.hadoop.hbase.master.hmaster: unhandled exception. starting shutdown. java.io.ioexception: call to /10.20.20.188:60020 failed on local exception: java.io.ioexception: connection reset by peer    at org.apache.hadoop.hbase.ipc.hbaseclient.wrapexception(hbaseclient.java:781)    at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:750)    at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:255)    at $proxy1.getprotocolversion(unknown source)    at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:412)    at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:388)    at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:435)    at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:345)    at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:889)    at org.apache.hadoop.hbase.catalog.catalogtracker.getcachedconnection(catalogtracker.java:350)    at org.apache.hadoop.hbase.catalog.catalogtracker.getrootserverconnection(catalogtracker.java:209)    at org.apache.hadoop.hbase.catalog.catalogtracker.getmetaserverconnection(catalogtracker.java:241)    at org.apache.hadoop.hbase.catalog.catalogtracker.waitformeta(catalogtracker.java:286)    at org.apache.hadoop.hbase.catalog.catalogtracker.waitformetaserverconnectiondefault(catalogtracker.java:326)    at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:157)    at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:140)    at org.apache.hadoop.hbase.master.assignmentmanager.rebuilduserregions(assignmentmanager.java:753)    at org.apache.hadoop.hbase.master.assignmentmanager.processfailover(assignmentmanager.java:174)    at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:314) caused by: java.io.ioexception: connection reset by peer    at sun.nio.ch.filedispatcher.read0(native method)    at sun.nio.ch.socketdispatcher.read(socketdispatcher.java:21)    at sun.nio.ch.ioutil.readintonativebuffer(ioutil.java:233)    at sun.nio.ch.ioutil.read(ioutil.java:206) notice, we think its a case of processfailover so we think we can just scan meta to fixup our inmemory picture of the running cluster, only the scan of meta fails because the meta isn not assigned. ",
        "label": 314
    },
    {
        "text": "fix deadlock during cluster shutdown due to concurrent connection close  caught while testing branch-1.0, shutting down testmastermetricswrapper. found one java-level deadlock:  =============================  \"master_meta_server_operations-ip-10-32-130-237:55342-0\":  waiting to lock monitor 0x00007f2a040051c8 (object 0x00000007e36108a8, a org.apache.hadoop.hbase.util.poolmap),  which is held by \"m:0;ip-10-32-130-237:55342\"  \"m:0;ip-10-32-130-237:55342\":  waiting to lock monitor 0x00007f2a04005118 (object 0x00000007e3610b00, a org.apache.hadoop.hbase.ipc.rpcclientimpl$connection),  which is held by \"master_meta_server_operations-ip-10-32-130-237:55342-0\" full stack dump and deadlock debug output attached. root cause:  in rpcclientimpl#close(), we obtain lock on connections first:     synchronized (connections) {       for (connection conn : connections.values()) { then markclosed() tries to obtain lock on connection object:         if (!conn.isalive()) {           conn.markclosed(new interruptedioexception(\"rpcclient is closing\"));           conn.close(); another thread, metaservershutdownhandler, calls rpcclientimpl$connection#setupiostreams() where :         markclosed(e);         close(); lock on connection object is obtained first, then lock on connections is attempted, leading to deadlock:       synchronized (connections) {         connections.removevalue(remoteid, this);       } ",
        "label": 441
    },
    {
        "text": "allow per process svn version information  currently, we can use bin/hbase versioninfo to get jar svn version information, but that is not necessarily the jar of the currently-running process. hbase.getclusterstatus() contains svn info for the running primary hmaster process. we need to add a way to get svn version information for online regionservers (more important) & backup hmasters (not so important). with the addition of rolling upgrades, we are more likely to get into a situation where part of the rs in our cluster is on a different version. if we start seeing problems during rolling, it would be nice to know if this might be a versioning problem. ",
        "label": 341
    },
    {
        "text": "move rsgroupinfomanager to hbase server  ",
        "label": 149
    },
    {
        "text": "add cp hook before setting ponr in split  this hook helps to perform split on user region and corresponding index region such that both will be split or none.  with this hook split for user and index region as follows user region  ===========  1) create splitting znode for user region split  2) close parent user region  3) split user region storefiles  4) instantiate child regions of user region through the new hook we can call index region transitions as below index region  ===========  5) create splitting znode for index region split  6) close parent index region  7) split storefiles of index region  8) instantiate child regions of the index region if any failures in 5,6,7,8 rollback the steps and return null, on null return throw exception to rollback for 1,2,3,4 9) set ponr  10) do batch put of offline and split entries for user and index regions  index region  ============  11) open daughers of index regions and transition znode to split. this step we will do through presplitafterponr hook. opening index regions before opening user regions helps to avoid put failures if there is colocation mismatch(this can happen if user regions opening completed but index regions opening in progress) user region  ===========  12) open daughers of user regions and transition znode to split. even if region server crashes also at the end both user and index regions will be split or none ",
        "label": 543
    },
    {
        "text": "hserverload storefileindexsizemb should be changed to storefileindexsizekb  related to hbase-3927, matt proposed changing hserverload.storefileindexsizemb to storefileindexsizekb so that user can see the size of small store file index. ",
        "label": 42
    },
    {
        "text": "hbaseadmin shouldn't expect hconnection to be an hconnectionimplementation  currently, the hbaseadmin has a constructor that takes an hconnection, but then immediately casts it to an hconnectionmanager.hconnectionimplementation:   public hbaseadmin(hconnection connection)       throws masternotrunningexception, zookeeperconnectionexception {     this.conf = connection.getconfiguration();     // we want the real class, without showing it our public interface,     //  hence the cast.     this.connection = (hconnectionmanager.hconnectionimplementation)connection; however, this breaks the explicit contract in the javadocs and makes it basically impossible to mock out the hbaseadmin. we need to either make the hbaseadmin use a basic hconnection and optimize for cases where its smarter or bring up the couple of methods in hconnectionmanager.hconnectionimplementation to the hconnection interface. ",
        "label": 236
    },
    {
        "text": "performance regression caused by hbase  the patch in hbase-4054 switches the pooledhtable to extend htable as opposed to implement htableinterface. since htable does not have an empty constructor, the patch added a call to the super() constructor, which though does trigger the zookeeper and meta scan, causing a considerable delay. with multiple threads using the pool in parallel, the first thread is holding up all the subsequent ones, in effect it negates the whole reason we have a htable pool. we should complete hbase-5728, or alternatively add a protected, empty constructor the htable. i am +1 for the former. ",
        "label": 242
    },
    {
        "text": "doc the major differences between and  a distillation of release notes for those w  limited attention  hbase-8450 changes base configs in some ways that may be surprising. we should mnention this in any release note distillation. ",
        "label": 314
    },
    {
        "text": "regionserver expects all scanner to be subclasses of hregion regionscanner  returning just an internalscanner from regionobsever. {pre|post} openscanner leads to the following exception when using the scanner. java.io.ioexception: internalscanner implementation is expected to be hregion.regionscanner.  at org.apache.hadoop.hbase.regionserver.hregionserver.next(hregionserver.java:2023)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:616)  at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:314)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1225) the problem is in hregionserver.next(...):       internalscanner s = this.scanners.get(scannername); ...       // call coprocessor. get region info from scanner.       hregion region = null;       if (s instanceof hregion.regionscanner) {         hregion.regionscanner rs = (hregion.regionscanner) s;         region = getregion(rs.getregionname().getregionname());       } else {         throw new ioexception(\"internalscanner implementation is expected \" +             \"to be hregion.regionscanner.\");       } ",
        "label": 286
    },
    {
        "text": "meta gets inconsistent in a number of crash scenarios  (forking this issue off from hbase-2235). during load testing, in a number of failure scenarios (unexpected region server deaths) etc., we notice that meta can get inconsistent. this primarily happens for regions which are in the process of being split. manually running add_table.rb seems to fix the tables meta data just fine. but it would be good to do automatic cleansing (as part of meta scanners work) and/or avoid these inconsistent states altogether. for example, for a particular startkey, i see all these entries: test1,1204765,1266569946560 column=info:regioninfo, timestamp=1266581302018, value=region => {name => 'test1,                              1204765,1266569946560', startkey => '1204765', endkey => '1441091', encoded => 18                              19368969, offline => true, split => true, table => {{name => 'test1', families =>                               [{name => 'actions', versions => '3', compression => 'none', ttl => '2147483647'                              , blocksize => '65536', in_memory => 'false', blockcache => 'true'}]}}  test1,1204765,1266569946560 column=info:server, timestamp=1266570029133, value=10.129.68.212:60020  test1,1204765,1266569946560 column=info:serverstartcode, timestamp=1266570029133, value=1266562597546  test1,1204765,1266569946560 column=info:splitb, timestamp=1266581302018, value=\\x00\\x071441091\\x00\\x00\\x00\\x0                              1\\x26\\xe6\\x1f\\xdf\\x27\\x1btest1,1290703,1266581233447\\x00\\x071290703\\x00\\x00\\x00\\x                              05\\x05test1\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x07is_root\\x00\\x00\\x00\\x05false\\x                              00\\x00\\x00\\x07is_meta\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x01\\x07\\x07actions\\x00\\x00                              \\x00\\x07\\x00\\x00\\x00\\x0bbloomfilter\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0bcompressi                              on\\x00\\x00\\x00\\x04none\\x00\\x00\\x00\\x08versions\\x00\\x00\\x00\\x013\\x00\\x00\\x00\\x03tt                              l\\x00\\x00\\x00\\x0a2147483647\\x00\\x00\\x00\\x09blocksize\\x00\\x00\\x00\\x0565536\\x00\\x00                              \\x00\\x09in_memory\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0ablockcache\\x00\\x00\\x00\\x04t                              rueh\\x0fq\\xcf  test1,1204765,1266581233447 column=info:regioninfo, timestamp=1266609172177, value=region => {name => 'test1,                              1204765,1266581233447', startkey => '1204765', endkey => '1290703', encoded => 13                              73493090, offline => true, split => true, table => {{name => 'test1', families =>                               [{name => 'actions', versions => '3', compression => 'none', ttl => '2147483647'                              , blocksize => '65536', in_memory => 'false', blockcache => 'true'}]}}  test1,1204765,1266581233447 column=info:server, timestamp=1266604768670, value=10.129.68.213:60020  test1,1204765,1266581233447 column=info:serverstartcode, timestamp=1266604768670, value=1266562597511  test1,1204765,1266581233447 column=info:splita, timestamp=1266609172177, value=\\x00\\x071226169\\x00\\x00\\x00\\x0                              1\\x26\\xe7\\xca,\\x7d\\x1btest1,1204765,1266609171581\\x00\\x071204765\\x00\\x00\\x00\\x05\\                              x05test1\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x07is_root\\x00\\x00\\x00\\x05false\\x00\\                              x00\\x00\\x07is_meta\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x01\\x07\\x07actions\\x00\\x00\\x0                              0\\x07\\x00\\x00\\x00\\x0bbloomfilter\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0bcompression\\                              x00\\x00\\x00\\x04none\\x00\\x00\\x00\\x08versions\\x00\\x00\\x00\\x013\\x00\\x00\\x00\\x03ttl\\x                              00\\x00\\x00\\x0a2147483647\\x00\\x00\\x00\\x09blocksize\\x00\\x00\\x00\\x0565536\\x00\\x00\\x0                              0\\x09in_memory\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0ablockcache\\x00\\x00\\x00\\x04true                              \\xb9\\xbd\\xfeo  test1,1204765,1266581233447 column=info:splitb, timestamp=1266609172177, value=\\x00\\x071290703\\x00\\x00\\x00\\x0                              1\\x26\\xe7\\xca,\\x7d\\x1btest1,1226169,1266609171581\\x00\\x071226169\\x00\\x00\\x00\\x05\\                              x05test1\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x07is_root\\x00\\x00\\x00\\x05false\\x00\\                              x00\\x00\\x07is_meta\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x01\\x07\\x07actions\\x00\\x00\\x0                              0\\x07\\x00\\x00\\x00\\x0bbloomfilter\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0bcompression\\                              x00\\x00\\x00\\x04none\\x00\\x00\\x00\\x08versions\\x00\\x00\\x00\\x013\\x00\\x00\\x00\\x03ttl\\x                              00\\x00\\x00\\x0a2147483647\\x00\\x00\\x00\\x09blocksize\\x00\\x00\\x00\\x0565536\\x00\\x00\\x0                              0\\x09in_memory\\x00\\x00\\x00\\x05false\\x00\\x00\\x00\\x0ablockcache\\x00\\x00\\x00\\x04true                              \\xe1\\xdf\\xf8p  test1,1204765,1266609171581 column=info:regioninfo, timestamp=1266609172212, value=region => {name => 'test1,                              1204765,1266609171581', startkey => '1204765', endkey => '1226169', encoded => 21                              34878372, table => {{name => 'test1', families => [{name => 'actions', versions =                              > '3', compression => 'none', ttl => '2147483647', blocksize => '65536', in_memor                              y => 'false', blockcache => 'true'}]}} ",
        "label": 314
    },
    {
        "text": "add more flexibility for input directory structure to loadincrementalhfiles  use case: we were trying to combine two very large tables into a single table. thus we ran jobs in one datacenter that populated certain column families and another datacenter which populated other column families. took a snapshot and exported them to their respective datacenters. wanted to simply take the hdfs restored snapshot and use loadincremental to merge the data. it would be nice to add support where we could run loadincremental on a directory where the depth of store files is something other than two (current behavior). with snapshots it would be nice if you could pass a restored hdfs snapshot's directory and have the tool run. i am attaching a patch where i parameterize the bulkload timeout as well as the default store file depth. ",
        "label": 522
    },
    {
        "text": "key  index  only fetches  when you retrieve data from hbase you get key (row+column+timestamp) + values. it would be nice to have a mode where we only fetch the keys (i.e. the index) but not the values. ",
        "label": 247
    },
    {
        "text": "unknownregionexception blocks hbck from sideline big overlap regions  before sidelining a big overlap region, hbck tries to close it and offline it at first. however, sometimes, it throws notservingregion or unknownregionexception.  it could be because the region is not open/assigned at all, or some other issue.  we should figure out why and fix it. by the way, it's better to print out in the log the command line to bulk load back sidelined regions, if any. ",
        "label": 242
    },
    {
        "text": "hbase daemons should log version info at startup and possibly periodically  as someone who deals with different clusters running different versions (sometimes pre-releases), i often end up with an error log but not great info as the exact revision the log came from. this is even more tricky with things like rolling update. to help with this, the rs and master should log their complete version string at startup. potentially, they could also log it once every 12 hours as well, so that even with log rotation we can tell the current version. ",
        "label": 289
    },
    {
        "text": "hbase dropped updating list of store files on flush  here is how current updatereaders is implemented in storescanner:   // implementation of changedreadersobserver   public void updatereaders() throws ioexception {     if (this.closing.get()) {       return;     }     this.lock.writelock().lock();     try {       // could do this pretty nicely with keyvalueheap, but the existing       // implementation of this method only updated if no existing storefiles?       // lets discuss.       return;     } finally {       this.lock.writelock().unlock();     }   } fill in missing functionality. count of store files changes on flush and compaction. ",
        "label": 314
    },
    {
        "text": " testing  upgrade to junit x and use  beforeclass annotations to optimize tests  if we upgrade to junit 4.x we can get access to @beforeclass annotations... that allows us to start up dfs & hbase only once per test class. this should improve the speed of our unit tests substantially. we will also need an ability to reset the hbase state between tests however. drop all tables for example. ",
        "label": 314
    },
    {
        "text": "testiofencing occasionally fails  i can reproduce this using jdk 6 on ubuntu 13.10. running org.apache.hadoop.hbase.testiofencing tests run: 2, failures: 1, errors: 0, skipped: 0, time elapsed: 111.55 sec <<< failure! no failure trace captured yet. fix or disable. ",
        "label": 38
    },
    {
        "text": "hstore rowatorbeforefrommapfile  fails to locate the row if   of mapfiles    after hbase-528 committed, a misplaced return statement and } cause   rowatorbeforefrommapfile() never look into 2nd (and latter) mapfile  if candidatekeys.firstkey() <= map.finalkey(). ",
        "label": 86
    },
    {
        "text": "arrayindexoutofboundsexception when balance    2019-07-25 15:19:59,828 error [master/nna:16000.chore.1] hbase.scheduledchore: caught error java.lang.arrayindexoutofboundsexception: 3171 at org.apache.hadoop.hbase.master.balancer.baseloadbalancer$cluster.removeregion(baseloadbalancer.java:873) at org.apache.hadoop.hbase.master.balancer.baseloadbalancer$cluster.doaction(baseloadbalancer.java:716) at org.apache.hadoop.hbase.master.balancer.stochasticloadbalancer.balancecluster(stochasticloadbalancer.java:407) at org.apache.hadoop.hbase.master.balancer.stochasticloadbalancer.balancecluster(stochasticloadbalancer.java:318) at org.apache.hadoop.hbase.master.hmaster.balance(hmaster.java:1650) at org.apache.hadoop.hbase.master.hmaster.balance(hmaster.java:1567) at org.apache.hadoop.hbase.master.balancer.balancerchore.chore(balancerchore.java:49) at org.apache.hadoop.hbase.scheduledchore.run(scheduledchore.java:186) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.runandreset(futuretask.java:308) at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:180) at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:294) at org.apache.hadoop.hbase.jitterscheduledthreadpoolexecutorimpl$jitteredrunnablescheduledfuture.run(jitterscheduledthreadpoolexecutorimpl.java:111) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at java.lang.thread.run(thread.java:745) should check if the regionindex is valid when removeregion, java: hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/baseloadbalancer.java int[] removeregion(int[] regions, int regionindex) {     //todo: this maybe costly. consider using linked lists     int[] newregions = new int[regions.length - 1];     int i = 0;     for (i = 0; i < regions.length; i++) {         if (regions[i] == regionindex) {             break;         }         if (i == regions.length - 1) {             return arrays.copyof(regions, regions.length);         }         newregions[i] = regions[i];     }     system.arraycopy(regions, i+1, newregions, i, newregions.length - i);     return newregions; } ",
        "label": 292
    },
    {
        "text": "dropping table fails silently if table isn't disabled  rather than fail silently, hbase should throw an exception ",
        "label": 229
    },
    {
        "text": "test for  checkandput should properly read mvcc  checkandput should integrate with mvcc, similar to how hbase-4583 fixed appends and increments. also need a test, here's one we could use (originally proposed in hbase-7051):  the current value of some cell is 10.  i issue two concurrent requests:  a) a check and put where check value = 10, put value = 11  b) a put where put value = 50  the only result at the end of these operations that seems reasonable to me is the value of the cell being 50. if a occurred first (acid wise), then our values go 10->11->50. if b occurred first, then our values go 10->50 (and the checkandput fails) ",
        "label": 544
    },
    {
        "text": "outdated data can not be cleaned in time  compaction checker will send regions to the compact queue to do compact. but the priority of these regions is too low if these regions have only a few storefiles. when there is large through output, and the compact queue will aways have some regions with higher priority. this may causing the major compact be delayed for a long time(even a few days), and outdated data cleaning will also be delayed.  in our test case, we found some regions sent to the queue by major compact checker hunging in the queue for more than 2 days! some scanners on these regions cannot get availably data for a long time and lease expired. ",
        "label": 562
    },
    {
        "text": "  not running balancer because dead regionserver processing  is a lie  when running the balancer i see the message:  not running balancer because dead regionserver processing but that's not true, it does run, because the check is wrong: if (!this.servermanager.aredeadserversinprogress()) {   log.debug(\"not running balancer because dead regionserver processing\"); } also it doesn't return false like it should. ",
        "label": 229
    },
    {
        "text": "change the range block index scheme from  start end  to  start  end  and index range block by endkey  specially in hfile  from the code review of hfile (hbase-1818), we found the hfile allows duplicated key. but the old implementation would lead to missing of duplicated key when seek and scan, when the duplicated key span multiple blocks. we provide a patch (hbase-1841 is't step1) to resolve above issue. this patch modified hfile.writer to avoid generating a problem hfile with above cross-block duplicated key. it only start a new block when current appending key is different from the last appended key. but it still has a rish when the user of hfile.writer append many same duplicated key which lead to a very large block and need much memory or out-of-memory. the current hfile's block-index use startkey to index a block, i.e. the range/block index scheme is [startkey,endkey). as refering to the section 5.1 of the google bigtable paper. \"the metadata table stores the location of a tablet under a row key that is an encoding of the tablet's table identifer and its end row.\" the theory of bigtable's metadata is same as the blockindex in a sstable or hfile, so we should use endkey in hfile's blockindex. in my experiences of hypertable, the metadata is also \"tableid:endrow\". we would change the index scheme in hfile, from [startkey,endkey) to (startkey,endkey]. and change the binary search method to meet this index scheme. this change can resolve above duplicated-key issue. note:  the totally fix need to modify many modules in hbase, seems include hfile, meta schema, some internal code, etc. ",
        "label": 399
    },
    {
        "text": "fix testfilterset  broken up on hudson  hmm.. testfilterset works in ide but not up on hudson.... look into this. for now commenting out piece thats failing. ",
        "label": 247
    },
    {
        "text": "add to migration doc that meta should be healthy before upgrade  add a note to doc that operator ensures healthy meta before upgrade. ",
        "label": 314
    },
    {
        "text": "failing creating altering table with compression agrument from the hbase shell  hcolumndescriptor setcompressiontype takes compression.algorithm and not string hbase(main):007:0> create 't1', { name => 'f', compression => 'lzo'} error: cannot convert instance of class org.jruby.rubystring to class org.apache.hadoop.hbase.io.hfile.compression$algorithm ",
        "label": 210
    },
    {
        "text": "speed up tests by lowering some sleeps  while trying testadmin in the scope of hbase-3650, i saw that it takes a lot more time to run than it used to. upon inspection i see that there's 2 hardcoded 1 second sleeps in disabletablehandler and enabletablehandler in waituntildone (which is almost the same code in both cases too). setting that down to 50ms dropped the run time in half... and i'm sure there's a few other sleeps that we could get rid of. i think that at least those 1sec should be configurable so that we can tune them down in the tests, but i wonder if we need them at all. ",
        "label": 229
    },
    {
        "text": "speed up keyvalueheap next  a bit  see discussion on hbase-9969.  this was brought up by matt corgan. even though i didn't believe him first.       keyvaluescanner topscanner = this.heap.peek();       if (topscanner == null ||           this.comparator.compare(kvnext, topscanner.peek()) >= 0) {         this.heap.add(this.current);         this.current = pollrealkv();       } we already have the invariant everywhere this.current always has a real kv polled. so adding current back to the heap followed by pollrealkv() is a no-op if this.current is the only scanner anyway. this can be changed to:       ...       if (topscanner != null &&           this.comparator.compare(kvnext, topscanner.peek()) >= 0) {       ... with 20m rows, 5 cols each, everything in the cache, fully compacted - i.e. one hfile per store, a scan that filters everything at the server through all 100m kvs takes 15.1s without the change and 13.3s with it, so a 12% improvement. ",
        "label": 286
    },
    {
        "text": "report correct server hosting a table split for assignment to for mr jobs  currently we return a null string array to the mr framework to use a random node for mr job assignment. class: org.apache.hadoop.hbase.mapred.tablesplit  function getlocations() we should be able to query the meta now for the current host name of the server hosting the region in question.  this will help with scaling as there will be less cross server communication removing bandwidth as a bottleneck. the side effect of fixing this will help from overloading region servers with lots of mr clients all pulling from the same region server while theres work local for them to do. ",
        "label": 314
    },
    {
        "text": "region can't be opened for a long time  because the creating file failed   scenario:  ------------  1. file is created   2. but while writing data, all datanodes might have crashed. so writing data will fail.  3. now even if close is called in finally block, close also will fail and throw the exception because writing data failed.  4. after this if rs try to create the same file again, then alreadybeingcreatedexception will come. suggestion to handle this scenario.  ---------------------------  1. check for the existence of the file, if exists delete the file and create new file. here delete call for the file will not check whether the file is open or closed. overwrite option:  ----------------  1. overwrite option will be applicable if you are trying to overwrite a closed file.  2. if the file is not closed, then even with overwrite option same alreadybeingcreatedexception will be thrown.  this is the expected behaviour to avoid the multiple clients writing to same file. region server logs: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hdfs.protocol.alreadybeingcreatedexception: failed to create file /hbase/test1/12c01902324218d14b17a5880f24f64b/.tmp/.regioninfo for dfsclient_hb_rs_158-1-131-48,20020,1331107668635_1331107669061_-252463556_25 on client 158.1.132.19 because current leaseholder is trying to recreate file.  at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.recoverleaseinternal(fsnamesystem.java:1570)  at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfileinternal(fsnamesystem.java:1440)  at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfile(fsnamesystem.java:1382)  at org.apache.hadoop.hdfs.server.namenode.namenode.create(namenode.java:658)  at sun.reflect.generatedmethodaccessor9.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.ipc.rpc$server.call(rpc.java:547)  at org.apache.hadoop.ipc.server$handler$1.run(server.java:1137)  at org.apache.hadoop.ipc.server$handler$1.run(server.java:1133)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.ipc.server$handler.run(server.java:1131) at org.apache.hadoop.ipc.client.call(client.java:961)  at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:245)  at $proxy6.create(unknown source)  at sun.reflect.generatedmethodaccessor14.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at $proxy6.create(unknown source)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.<init>(dfsclient.java:3643)  at org.apache.hadoop.hdfs.dfsclient.create(dfsclient.java:778)  at org.apache.hadoop.hdfs.distributedfilesystem.create(distributedfilesystem.java:364)  at org.apache.hadoop.fs.filesystem.create(filesystem.java:630)  at org.apache.hadoop.fs.filesystem.create(filesystem.java:611)  at org.apache.hadoop.fs.filesystem.create(filesystem.java:518)  at org.apache.hadoop.hbase.regionserver.hregion.checkregioninfoonfilesystem(hregion.java:424)  at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:340)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2672)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2658)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.openregion(openregionhandler.java:330)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:116)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:158)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  [2012-03-07 20:51:45,858] [warn ] [rs_open_region-158-1-131-48,20020,1331107668635-23] [com.huawei.isap.ump.ha.client.rpcretryandswitchinvoker 131] retrying the method call: public abstract void org.apache.hadoop.hdfs.protocol.clientprotocol.create(java.lang.string,org.apache.hadoop.fs.permission.fspermission,java.lang.string,boolean,boolean,short,long) throws java.io.ioexception with arguments of length: 7. the exisiting activeserverconnection is:  activeserverconnectioninfo:  metadata:158-1-131-48/158.1.132.19:9000  version:145720623220907 [2012-03-07 20:51:45,872] [debug] [rs_open_region-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.zookeeper.zkassign 849] regionserver:20020-0x135ec32b39e0002-0x135ec32b39e0002 successfully transitioned node 91bf3e6f8adb2e4b335f061036353126 from m_zk_region_offline to rs_zk_region_opening  [2012-03-07 20:51:45,873] [debug] [rs_open_region-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.regionserver.hregion 2649] opening region: region => {name => 'test1,00088613810,1331112369360.91bf3e6f8adb2e4b335f061036353126.', startkey => '00088613810', endkey => '00088613815', encoded => 91bf3e6f8adb2e4b335f061036353126, table => {{name => 'test1', families => [ {name => 'value', bloomfilter => 'none', replication_scope => '0', versions => '3', compression => 'gz', ttl => '86400', blocksize => '655360', in_memory => 'false', blockcache => 'true'} ]}}  [2012-03-07 20:51:45,873] [debug] [rs_open_region-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.regionserver.hregion 316] instantiated test1,00088613810,1331112369360.91bf3e6f8adb2e4b335f061036353126.  [2012-03-07 20:51:45,874] [error] [rs_open_region-158-1-131-48,20020,1331107668635-20] [ ",
        "label": 544
    },
    {
        "text": "keyfilter for filtering based on comparsions of row  column  and qualifiers of keyvalues  from: angus he up on hbase-user: subject: is there any ways to retrieve values whose columns match specific regular expression?  is this functionality available in the current implementation? if not, is there any plans on this? see attached patch. low impact change, can go into 0.20.0. ",
        "label": 247
    },
    {
        "text": "javadoc issues on trunk   mdc replication  [javadoc] /opt/workspace/remote-ws/hbase/src/contrib/mdc_replication/src/java/org/apache/hadoop/hbase/regionserver/replication/replicationregionserver.java:145: warning - @return tag has no arguments.  [javadoc] /opt/workspace/remote-ws/hbase/src/contrib/mdc_replication/src/java/org/apache/hadoop/hbase/regionserver/replication/replicationregionserver.java:158: warning - @return tag has no arguments.  [javadoc] /opt/workspace/remote-ws/hbase/src/contrib/mdc_replication/src/java/org/apache/hadoop/hbase/regionserver/replication/replicationsink.java:277: warning - @return tag has no arguments.  [javadoc] /opt/workspace/remote-ws/hbase/src/contrib/mdc_replication/src/java/org/apache/hadoop/hbase/replication/replicationzookeeperhelper.java:144: warning - @return tag has no arguments. ",
        "label": 229
    },
    {
        "text": "extend cell usage through read path  umbrella issue for eliminating cell copying. the cell interface allows us to work with a reference to underlying bytes in the block cache without copying each cell into consecutive bytes in an array (keyvalue). ",
        "label": 544
    },
    {
        "text": "htable is not closed in loadtesttool loadtable   around line 476:       htable table = new htable(conf, tablename); table is not closed. ",
        "label": 441
    },
    {
        "text": "clean up our jmx view  its a bit of a mess  fix before we release 0.92.1 ",
        "label": 314
    },
    {
        "text": "custom tableinputformat in inittablemapperjob throws classnofoundexception on tablemapper  when using custom tableinputformat in tablemapreduceutil.inittablemapperjob in the following way  tablemapreduceutil.inittablemapperjob(\"mytable\",   myscan,   mymapper.class,  mykey.class,   myvalue.class,   myjob,true, mytableinputformat.class); i get error: java.lang.classnotfoundexception: org.apache.hadoop.hbase.mapreduce.tablemapper  at java.net.urlclassloader$1.run(urlclassloader.java:202)  at java.security.accesscontroller.doprivileged(native method)  at java.net.urlclassloader.findclass(urlclassloader.java:190)  at java.lang.classloader.loadclass(classloader.java:306)  at sun.misc.launcher$appclassloader.loadclass(launcher.java:301)  at java.lang.classloader.loadclass(classloader.java:247)  at java.lang.classloader.defineclass1(native method)  at java.lang.classloader.defineclasscond(classloader.java:631)  at java.lang.classloader.defineclass(classloader.java:615)  at java.security.secureclassloader.defineclass(secureclassloader.java:141)  at java.net.urlclassloader.defineclass(urlclassloader.java:283)  at java.net.urlclassloader.access$000(urlclassloader.java:58)  at java.net.urlclassloader$1.run(urlclassloader.java:197)  at java.security.accesscontroller.doprivileged(native method)  at java.net.urlclassloader.findclass(urlclassloader.java:190) if i do not use the last two parameters, there is no error.  what is going wrong here? thanks  regards ",
        "label": 339
    },
    {
        "text": "add configuration for global aggregate memcache size  currently, we have a configuration parameter for the size a memcache must reach before it is flushed. this leads to pretty even sized mapfiles when flushes run, which is nice. however, as noted in the parent issue, we can often get to a point where we run out of memory because too much data is hanging around in memcaches. i think that we should add a new configuration parameter that governs the total amount of memory that the region server should spend on memcaches. this would have to be some number less than the heap size - we'll have to discover the proper values through experimentation. then, when a put comes in, if the global aggregate size of all the memcaches for all the stores is at the threshold, then we should block the current and any subsequent put operations from completing until forced flushes cause the memory usage to go back down to a safe level. the existing strategy for triggering flushes will still be in play, just augmented with this blocking behavior. this approach has the advantage of helping us avoid oome situations by warning us well in advance of overflow. additionally, it becomes something of a performance tuning knob, allowing you to allocate more memory to improve write performance. this is superior to the previously suggested phantomreference approach because that would possibly causes us to bump into further oomes while we're trying to flush to avoid them. ",
        "label": 86
    },
    {
        "text": "fix licenses in   rat plugin won't pass  use the -drelease profile to see we are missing 30 or so license. fix. ",
        "label": 314
    },
    {
        "text": "wal logs get deleted before region server can fully flush  to replicate the problem do the following: 1. check /hbase/.logs/xxxx directory to see if you have wal logs for the region server you are shutting down.  2. executing kill <pid> (where pid is a regionserver pid)  3. watch the regionserver log to start flushing, you will see how many regions are left to flush: 09:36:54,665 info org.apache.hadoop.hbase.regionserver.hregionserver: waiting on 489 regions to close  09:56:35,779 info org.apache.hadoop.hbase.regionserver.hregionserver: waiting on 116 regions to close 4. check /hbase/.logs/xxxx \u2013 you will notice that it has dissapeared.  5. check namenode logs: 09:26:41,607 info org.apache.hadoop.hdfs.server.namenode.fsnamesystem.audit: ugi=root ip=/10.101.1.5 cmd=delete src=/hbase/.logs/rdaa5.prod.imageshack.com,60020,1319749 note that, if you kill -9 the rs now, and it crashes on flush, you won't have any wal logs to replay. we need to make sure that logs are deleted or moved out only when rs has fully flushed. otherwise its possible to lose data. ",
        "label": 529
    },
    {
        "text": "consider double checked locking for block cache lock  running a workload with a high query rate against a dataset that fits in cache, i saw a lot of cpu being used in idlock.getlockentry, being called by hfilereaderv2.readblock. even though it was all cache hits, it was wasting a lot of cpu doing lock management here. i wrote a quick patch to switch to a double-checked locking and it improved throughput substantially for this workload. ",
        "label": 453
    },
    {
        "text": "thrift testthriftservercmdline takes too much time  sec  on trunk, as of today.  3 minutes is a good target. ",
        "label": 340
    },
    {
        "text": "vm opts for shell only  over in hbase-2177 ryan goes on how enabling gc logging, it shows on stdout when you fire the shell: so one problem with this is the irb then logs all gc to stdout, which is ugly.  i do something like this in my scripts: export hbase_opts=\"\" export hbase_log_dir=<somewhere> export server_gc_opts=\"$hbase_opts -verbose:gc -xx:+printgcapplicationstoppedtime -xx:+printgcdatestamps -xx:+printgcdetails -xloggc:$hbase_home/logs/gc-hbase.log\" export jmx_opts=\"-dcom.sun.management.jmxremote.authenticate=true -dcom.sun.management.jmxremote.ssl=false -dcom.sun.management.jmxremote.password.file=$hbase_home/conf/jmxremote.password -dcom.sun.management.jmxremote\" export hbase_master_opts=\"$server_gc_opts -xloggc:$hbase_log_dir/logs/gc-master.log\" export hbase_regionserver_opts=\"$server_gc_opts -xloggc:$hbase_log_dir/gc-hbase.log -dcom.sun.management.jmxremote.port=10102 $jmx_opts\" export hbase_thrift_opts=\"-xmx1000m $server_gc_opts -xloggc:$hbase_log_dir/gc-hbase-thrift.log -dcom.sun.management.jmxremote.port=10103 $jmx_opts\" export hbase_zookeeper_opts=\"-xmx1000m $server_gc_opts -xloggc:$hbase_log_dir/gc-zk.log -dcom.sun.management.jmxremote.port=10104 $jmx_opts\" now you get remote jmx with logging to whatever directory (we have to log to our large data partition since logs... can be big).  also the shell doesnt log gc to stdout, and you can get separate gc logs for hmaster, hrs, thrift, zookeeper. need to make an opts for the shell to use.... or do the above. ",
        "label": 38
    },
    {
        "text": "hbase is very slow at determining table is not present  if i misspell a table name, it takes a very long time for hbase to determine that the table doesn't exist, because there are many levels of retries. this often causes timeouts, which then obscure the true cause of the problem. ",
        "label": 86
    },
    {
        "text": "document  becoming a committer   based on the mailing list discussion at https://lists.apache.org/thread.html/81c633cbe1f6f78421cbdad5b9549643c67803a723a9d86a513264c0@%3cdev.hbase.apache.org%3e it sounds like we should record some of the thoughts for future contributors to refer to. ",
        "label": 320
    },
    {
        "text": "create table with empty start row passed as splitkey causes the hmaster to abort  a coworker of mine just had this scenario. it does not make sense the empty_start_row as splitkey (since the region with the empty start key is implicit), but it should not cause the hmaster to abort.  the abort happens because it tries to bulk assign the same region twice and then runs into race conditions with zk. the same would (presumably) happen when two identical split keys are passed, but the client blocks that. the simplest solution here is to also block passed null or empty_start_row as split key by the client. ",
        "label": 544
    },
    {
        "text": "blockcache interface should be truly modular  currently, the if the blockcache that used isn't an lrublockcache, somewhere in metrics will try to cast it to an lrublockcache and cause an exception. the code should be modular enough to allow for the use of different block caches without throwing an exception. ",
        "label": 289
    },
    {
        "text": "result getvalue and result getcolumnlatest return the wrong column   in the following example result.getvalue returns the wrong column keyvalue kv = new keyvalue(bytes.tobytes(\"r\"), bytes.tobytes(\"24\"), bytes.tobytes(\"2\"), bytes.tobytes(7l));  result result = new result(new keyvalue[] { kv } );  system.out.println(bytes.tolong(result.getvalue(bytes.tobytes(\"2\"), bytes.tobytes(\"2\")))); //prints 7. ",
        "label": 182
    },
    {
        "text": " replication  refactor keeperexceptions thrown from replication state interfaces into replication specific exceptions  currently, the replication state interfaces (state, peers and queues) throw keeperexceptions from some of their methods. refactor these into replication specific exceptions to prevent the implementation details of zookeeper from leaking through. ",
        "label": 103
    },
    {
        "text": "minor reformatting of hbase default xml and removal of comments that say what the 'default' is  since rarely agrees w  actual setting   ",
        "label": 314
    },
    {
        "text": "can't move meta or root from shell  fails with unknownregionexception: error: java.lang.reflect.undeclaredthrowableexception: org.apache.hadoop.hbase.unknownregionexception: -root-,,0,70236052         at org.apache.hadoop.hbase.master.hmaster.move(hmaster.java:729)         at sun.reflect.generatedmethodaccessor5.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1039) ",
        "label": 314
    },
    {
        "text": "remove unused config  hbase hstore blockcache blocksize  spotted by lars. ",
        "label": 314
    },
    {
        "text": "ref guide section on release candidate generation refers to old doc files  currently it says to copy files from the master version of src/main/docbkx which is incorrect since the move to asciidoc. ",
        "label": 177
    },
    {
        "text": "commit batchupdate  method should return timestamp  the commit(batchupdate) and commit(list<batchupdate>) should return timestamp that batchupdate was committed with (in the case of commit(list<batchupdate> should return array of timestamps). this should reduce number of round trips and improve performance in update operations. ",
        "label": 247
    },
    {
        "text": "shell help page is too big  redo so just list commands and then you type 'help command name' to learn more about it  ",
        "label": 25
    },
    {
        "text": "import utility can't import an exported file from  basically we pbed org.apache.hadoop.hbase.client.result so a 0.96 cluster cannot import 0.94 exported files. this issue is annoying because a user can't import his old archive files after upgrade or archives from others who are using 0.94. the ideal way is to catch deserialization error and then fall back to 0.94 format for importing. ",
        "label": 233
    },
    {
        "text": "basic client pushback mechanism  the current blocking we do when we are close to some limits (memstores over the multiplier factor, too many store files, global memstore memory) is bad, too coarse and confusing. after hitting hbase-5161, it really becomes obvious that we need something better. i did a little brainstorm with stack, we came up quickly with two solutions: send some exception to the client, like overloadedexception, that's thrown when some situation happens like getting past the low memory barrier. it would be thrown when the client gets a handler and does some check while putting or deleting. the client would treat this a retryable exception but ideally wouldn't check .meta. for a new location. it could be fancy and have multiple levels of pushback, like send the exception to 25% of the clients, and then go up if the situation persists. should be \"easy\" to implement but we'll be using a lot more io to send the payload over and over again (but at least it wouldn't sit in the rs's memory). send a message alongside a successful put or delete to tell the client to slow down a little, this way we don't have to do back and forth with the payload between the client and the server. it's a cleaner (i think) but more involved solution. in every case the rs should do very obvious things to notify the operators of this situation, through logs, web ui, metrics, etc. other ideas? ",
        "label": 236
    },
    {
        "text": "resolve latest timestamp to current server time before scanning for acls  storing values with timestamps in the future is probably bad practice and can lead to surprises. if cells with timestamps in the future have acls, permissions from those acls will incorrectly be considered for authorizing the pending mutation. for sure that will be surprising. we should be able to avoid this case by resolving latest_timestamp to the current server time when creating the internal scanner for finding acls in the covered cell set. documenting a todo item from a discussion between anoop sam john and myself. ",
        "label": 38
    },
    {
        "text": "need a shaded hbase mapreduce module  ",
        "label": 48
    },
    {
        "text": "before running each shell command  check zk session and if not present  reestablish it  dmitriy was getting whack responses from his shell... nosuchmethodexception, etc., and it turned out that it was a long running shell that had run over a cluster restart. we should at least fail if we've lost our zk session or reconnect. ",
        "label": 286
    },
    {
        "text": "precommit hadoopqa is broke since  r1491656  fails like this: [info] ------------------------------------------------------------------------ [info] build failure [info] ------------------------------------------------------------------------ [info] total time: 42.547s [info] finished at: fri jun 21 02:43:29 utc 2013 [info] final memory: 48m/569m [info] ------------------------------------------------------------------------ [error] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testcompile (default-testcompile) on project hbase-it: compilation failure [error] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/integrationtestbulkload.java:[589,36] cannot access org.apache.hadoop.mapred.minimrcluster [error] class file for org.apache.hadoop.mapred.minimrcluster not found [error] util.startminimapreducecluster(); [error] -> [help 1] org.apache.maven.lifecycle.lifecycleexecutionexception: failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testcompile (default-testcompile) on project hbase-it: compilation failure /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/integrationtestbulkload.java:[589,36] cannot access org.apache.hadoop.mapred.minimrcluster class file for org.apache.hadoop.mapred.minimrcluster not found       util.startminimapreducecluster(); at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:213) at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:153) at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:145) at org.apache.maven.lifecycle.internal.lifecyclemodulebuilder.buildproject(lifecyclemodulebuilder.java:84) at org.apache.maven.lifecycle.internal.lifecyclemodulebuilder.buildproject(lifecyclemodulebuilder.java:59) at org.apache.maven.lifecycle.internal.lifecyclestarter.singlethreadedbuild(lifecyclestarter.java:183) at org.apache.maven.lifecycle.internal.lifecyclestarter.execute(lifecyclestarter.java:161) at org.apache.maven.defaultmaven.doexecute(defaultmaven.java:319) at org.apache.maven.defaultmaven.execute(defaultmaven.java:156) at org.apache.maven.cli.mavencli.execute(mavencli.java:537) at org.apache.maven.cli.mavencli.domain(mavencli.java:196) at org.apache.maven.cli.mavencli.main(mavencli.java:141) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.codehaus.plexus.classworlds.launcher.launcher.launchenhanced(launcher.java:290) at org.codehaus.plexus.classworlds.launcher.launcher.launch(launcher.java:230) at org.codehaus.plexus.classworlds.launcher.launcher.mainwithexitcode(launcher.java:409) at org.codehaus.plexus.classworlds.launcher.launcher.main(launcher.java:352) caused by: org.apache.maven.plugin.compilationfailureexception: compilation failure /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/integrationtestbulkload.java:[589,36] cannot access org.apache.hadoop.mapred.minimrcluster class file for org.apache.hadoop.mapred.minimrcluster not found       util.startminimapreducecluster(); at org.apache.maven.plugin.abstractcompilermojo.execute(abstractcompilermojo.java:729) at org.apache.maven.plugin.testcompilermojo.execute(testcompilermojo.java:161) at org.apache.maven.plugin.defaultbuildpluginmanager.executemojo(defaultbuildpluginmanager.java:101) at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:209) ... 19 more ",
        "label": 314
    },
    {
        "text": "htable getrow  for only a column family  i have a use case in which we want to access all the columns for a supplied column family while also avoiding the overhead of reading any other column families. there seems to be no method for doing this but doing a quick dive into the code it \"seems\" like this should be relatively easy to allow. this seems desirable since the architecture describes column families being stored together so reading only a single column family should be a lighter operation. ",
        "label": 229
    },
    {
        "text": " general  partitioner for  hbase  bulk  behind the api  write hfiles direct  uploader  for users to bulk upload by writing hfiles directly to the filesystem, they currently need to write a partitioner that is intimate with how their key schema works. this issue is about providing a general partitioner, one that could never be as fair as a custom-written partitioner but that might just work for many cases. the idea is that a user would supply the first and last keys in their dataset to upload. we'd then do bigdecimal on the range between start and end rowids dividing it by the number of reducers to come up with key ranges per reducer. (i thought jgray had done some bigdecimal work dividing keys already but i can't find it) ",
        "label": 314
    },
    {
        "text": "deprecate and replace hcd methods that have a 'should' prefix with a 'get' instead  hcolumndescriptor has a bunch of methods that have 'should' for a prefix. deprecate and give them a javabean 'get' or 'is' instead. ",
        "label": 53
    },
    {
        "text": "implement token based digest md5 authentication for mapreduce tasks  hbase security currently supports kerberos authentication for clients, but this isn't sufficient for map-reduce interoperability, where tasks execute without kerberos credentials. in order to fully interoperate with map-reduce clients, we will need to provide our own token authentication mechanism, mirroring the hadoop token authentication mechanisms. this will require obtaining an hbase authentication token for the user when the job is submitted, serializing it to a secure location, and then, at task execution, having the client or task code de-serialize the stored authentication token and use that in the hbase client authentication process. a detailed implementation proposal is sketched out on the wiki:  http://wiki.apache.org/hadoop/hbase/hbasetokenauthentication ",
        "label": 180
    },
    {
        "text": " stargate  deletes not working as expected  from private communication, an example: hbase(main):051:0> get 'books', 'fe80a1eb-2b3c-4995-8630-4131ef4b4eb7' column                       cell                                                                               attribute:author            timestamp=5410348784985219840, value=\\001richard matheson                          attribute:description       timestamp=5410348784985219840, value=\\001the most clever and riveting vampire nov                              el since dracula.                                                                  attribute:links             timestamp=5410348784985219840, value=--- []\\n\\n                                    attribute:title             timestamp=5410348784985219840, value=\\001i am legend                               log:change                  timestamp=5410348784985219840, value=--- []\\n\\n                                   5 row(s) in 0.0070 seconds   ==========   > curl http://localhost:8080/books/fe80a1eb-2b3c-4995-8630-4131ef4b4eb7 <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\"?><cellset><row key=\"zmu4mgexzwitmmizyy00otk1ltg2mzatndezmwvmngi0zwi3\"><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlomf1dghvcg==\">avjpy2hhcmqgtwf0agvzb24=</cell><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlomrlc2nyaxb0aw9u\">avrozsbtb3n0ignszxzlcibhbmqgcml2zxrpbmcgdmftcglyzsbub3zlbcbzaw5jzsbecmfjdwxhlg==</cell><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlomxpbmtz\">ls0tiftdcgo=</cell><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlonrpdgxl\">aukgqw0gtgvnzw5k</cell><cell timestamp=\"5410348784985219840\" column=\"bg9nomnoyw5nzq==\">ls0tiftdcgo=</cell></row></cellset>   > curl -x delete -v http://localhost:8080/books/fe80a1eb-2b3c-4995-8630-4131ef4b4eb7 * about to connect() to localhost port 8080 (#0) *   trying ::1... connected * connected to localhost (::1) port 8080 (#0) > delete /books/fe80a1eb-2b3c-4995-8630-4131ef4b4eb7 http/1.1 > user-agent: curl/7.18.2 (i486-pc-linux-gnu) libcurl/7.18.2 openssl/0.9.8g zlib/1.2.3.3 libidn/1.10 > host: localhost:8080 > accept: */* >  < http/1.1 200 ok < content-length: 0 <  * connection #0 to host localhost left intact * closing connection #0   > curl http://localhost:8080/books/fe80a1eb-2b3c-4995-8630-4131ef4b4eb7              <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\"?><cellset><row key=\"zmu4mgexzwitmmizyy00otk1ltg2mzatndezmwvmngi0zwi3\"><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlomf1dghvcg==\">avjpy2hhcmqgtwf0agvzb24=</cell><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlomrlc2nyaxb0aw9u\">avrozsbtb3n0ignszxzlcibhbmqgcml2zxrpbmcgdmftcglyzsbub3zlbcbzaw5jzsbecmfjdwxhlg==</cell><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlomxpbmtz\">ls0tiftdcgo=</cell><cell timestamp=\"5410348784985219840\" column=\"yxr0cmlidxrlonrpdgxl\">aukgqw0gtgvnzw5k</cell><cell timestamp=\"5410348784985219840\" column=\"bg9nomnoyw5nzq==\">ls0tiftdcgo=</cell></row></cellset> ",
        "label": 185
    },
    {
        "text": " shell  now does \\x20 for spaces  ",
        "label": 314
    },
    {
        "text": "fix the rcn correctness warning in memstoreflusher class  https://builds.apache.org/job/precommit-hbase-build/3057//artifact/trunk/patchprocess/newpatchfindbugswarningshbase-server.html#warnings_correctness  shows : bug type rcn_redundant_nullcheck_would_have_been_a_npe (click for details)  in class org.apache.hadoop.hbase.regionserver.memstoreflusher  in method org.apache.hadoop.hbase.regionserver.memstoreflusher.flushregion(memstoreflusher$flushregionentry)  value loaded from region  return value of org.apache.hadoop.hbase.regionserver.memstoreflusher$flushregionentry.access$000(memstoreflusher$flushregionentry)  at memstoreflusher.java:[line 346]  redundant null check at memstoreflusher.java:[line 363] ",
        "label": 290
    },
    {
        "text": "keyvalue keycomparator array overrun  09/06/08 22:58:47 info zookeeper.zookeeper: initiating client connection, host=b oa03:2181,boa02:2181,boa01:2181,boa04:2181 sessiontimeout=10000 watcher=org.apac he.hadoop.hbase.zookeeper.watcherwrapper@518bf072 09/06/08 22:58:47 info zookeeper.clientcnxn: zookeeper.disableautowatchreset is false 09/06/08 22:58:47 info zookeeper.clientcnxn: attempting connection to server boa 04/172.20.3.231:2181 09/06/08 22:58:47 info zookeeper.clientcnxn: priming connection to java.nio.chan nels.socketchannel[connected local=/172.20.3.232:40296 remote=boa04/172.20.3.231 :2181] 09/06/08 22:58:47 info zookeeper.clientcnxn: server connection successful 09/06/08 22:58:47 warn mapred.jobclient: use genericoptionsparser for parsing th e arguments. applications should implement tool for the same. 09/06/08 22:58:47 warn mapred.jobclient: no job jar file set.  user classes may not be found. see jobconf(class) or jobconf#setjar(string). 09/06/08 22:58:47 info zookeeper.zookeeper: initiating client connection, host=b oa03:2181,boa02:2181,boa01:2181,boa04:2181 sessiontimeout=10000 watcher=org.apac he.hadoop.hbase.zookeeper.watcherwrapper@362f0d54 09/06/08 22:58:47 info zookeeper.clientcnxn: attempting connection to server boa 03/172.20.3.230:2181 09/06/08 22:58:47 info zookeeper.clientcnxn: priming connection to java.nio.chan nels.socketchannel[connected local=/172.20.3.232:42792 remote=boa03/172.20.3.230 :2181] 09/06/08 22:58:47 info zookeeper.clientcnxn: server connection successful 09/06/08 22:58:48 info mapred.tableinputformatbase: split: 0->boa04.trendmicro.c om:,01e33c601a7a9dd0ddb5c8427438f2f1 exception in thread \"main\" java.lang.arrayindexoutofboundsexception: 32         at org.apache.hadoop.hbase.util.bytes.compareto(bytes.java:798)         at org.apache.hadoop.hbase.keyvalue$keycomparator.comparerows(keyvalue.j ava:1760)         at org.apache.hadoop.hbase.keyvalue$keycomparator.compare(keyvalue.java: 1696)         at org.apache.hadoop.hbase.keyvalue$keycomparator.compare(keyvalue.java: 1755)         at org.apache.hadoop.hbase.keyvalue$keycomparator.compare(keyvalue.java: 1687)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getcac hedlocation(hconnectionmanager.java:697)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locate regioninmeta(hconnectionmanager.java:541)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locate region(hconnectionmanager.java:525)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locate region(hconnectionmanager.java:488)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getreg ionlocation(hconnectionmanager.java:342)         at org.apache.hadoop.hbase.client.htable.getregionlocation(htable.java:1 91)         at org.apache.hadoop.hbase.mapred.tableinputformatbase.getsplits(tablein putformatbase.java:296)         at org.apache.hadoop.mapred.jobclient.submitjob(jobclient.java:742)         at org.apache.hadoop.mapred.jobclient.runjob(jobclient.java:1026)         at net.iridiant.crawler.mapred.documentparser.main(unknown source) ",
        "label": 547
    },
    {
        "text": "roll hlog if any datanode in the write pipeline dies  hdfs does not replicate the last block of a file that is being written to. this means that is datanodes in the write pipeline die, then the data blocks in the transaction log would be experiencing reduced redundancy. it would be good if the region server can detect datanode-death in the write pipeline while writing to the transaction log and if this happens, close the current log an open a new one. this depends on hdfs-826 ",
        "label": 341
    },
    {
        "text": "testmasterfailover testmasterfailoverwithmockedritondeadrs is failing  failing on hudson and locally ",
        "label": 180
    },
    {
        "text": "tmp directory should not be deleted when master restart used for user scan snapshot feature  when create table, table directories are firstly created in tmp directory and then move to data directory. so hdfs acls are set at the following tmp directories used for acls inherited: {hbase-rootdir}/.tmp/data {hbase-rootdir}/.tmp/data/{namespace} {hbase-rootdir}/.tmp/data/{namespace}/{table} when master restart, it will delete tmp directory and this will break this feature.   ",
        "label": 500
    },
    {
        "text": "test patch sh should be able to handle the case where  term is not defined  from https://builds.apache.org/job/precommit-hbase-build/9498/consolefull : tput: no value for $term and no -t specified tput: no value for $term and no -t specified test-patch.sh should be able to handle this case. ",
        "label": 191
    },
    {
        "text": "factor tableoperation and subclasses into separate files from hmaster  in hmaster, a lot of the things you do like table creation, deletion, etc, are abstracted into tableoperation subclasses. this is handy and elegant, but inflates the size of the hmaster file by a few hundred lines at least. i think we should move the tableoperation and subclasses out into their own files as opposed to being internal classes. this would make it easier to find them and reduce the size of hmaster. the new classes should be package-private. ",
        "label": 86
    },
    {
        "text": "rare failure in testratelimiter  maybe we aren't waiting long enough for a slow executor? or it is some kind of race. test usually passes. [error] tests run: 15, failures: 1, errors: 0, skipped: 0, time elapsed: 1.01 s <<< failure! - in org.apache.hadoop.hbase.quotas.testratelimiter  [error] testoverconsumptionfixedintervalrefillstrategy(org.apache.hadoop.hbase.quotas.testratelimiter) time elapsed: 0.028 s <<< failure!  java.lang.assertionerror: expected:<1000> but was:<999>  at org.apache.hadoop.hbase.quotas.testratelimiter.testoverconsumptionfixedintervalrefillstrategy(testratelimiter.java:122) ",
        "label": 98
    },
    {
        "text": "fix delayed rpc crash  delayed rpc crashes if return value is not delayed and enddelay is called before the actual function returns a value. ",
        "label": 477
    },
    {
        "text": "tool to handle finishing replication when the cluster is offline  we're having a discussion on the mailing list about replicating the data on a cluster that was shut down in an offline fashion. the motivation could be that you don't want to bring hbase back up but still need that data on the slave. so i have this idea of a tool that would be running on the master cluster while it is down, although it could also run at any time. basically it would be able to read the replication state of each master region server, finish replicating what's missing to all the slave, and then clear that state in zookeeper. the code that handles replication does most of that already, see replicationsourcemanager and replicationsource. basically when replicationsourcemanager.init() is called, it will check all the queues in zk and try to grab those that aren't attached to a region server. if the whole cluster is down, it will grab all of them. the beautiful thing here is that you could start that tool on all your machines and the load will be spread out, but that might not be a big concern if replication wasn't lagging since it would take a few seconds to finish replicating the missing data for each region server. i'm guessing when starting replicationsourcemanager you'd give it a fake region server id, and you'd tell it not to start its own source. fwiw the main difference in how replication is handled between apache's hbase and facebook's is that the latter is always done separately of hbase itself. this jira isn't about doing that. ",
        "label": 134
    },
    {
        "text": "regionstates getregionassignments  gets stuck on clone  this happens in processdeadserversandregionsintransition() on master startup. regionassigments is a tree map and treemap.clone() is expensive as it builds a tree from sorted data (order of n). there were a million entries in regionassigments and from jstack, thread was stuck in treemap.buildfromsorted() for couple of hours.  instead of this shadow clone, wrapping as an unmodifiable map should be enough. ",
        "label": 472
    },
    {
        "text": "hmaster throw nullpointerexception  nullpointerexception while hmaster starting.       java.lang.nullpointerexception         at java.util.treemap.getentry(treemap.java:324)         at java.util.treemap.get(treemap.java:255)         at org.apache.hadoop.hbase.master.assignmentmanager.addtoservers(assignmentmanager.java:1512)         at org.apache.hadoop.hbase.master.assignmentmanager.regiononline(assignmentmanager.java:606)         at org.apache.hadoop.hbase.master.assignmentmanager.processfailover(assignmentmanager.java:214)         at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:402)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:283) ",
        "label": 164
    },
    {
        "text": "do not retry exceptions such as unknown scanner or illegal argument  we moved a bunch of the client code inside callables. exceptions out of callable.call are caught and the method reinvoked up to the retry limit. this behavior is done even if the exception shouldn't be retried: e.g. unknownscannerexception. in the latter case, it doesn't matter how many times you retry, its never going to come right. ",
        "label": 314
    },
    {
        "text": "regexprowfilter does not work when there are columns from multiple families  if there are multiple column families, then creating a scanner with a regexprowfilter to match column values will mistakenly filter other columns. ",
        "label": 241
    },
    {
        "text": "testtableinputformatscan1 fail semi silently with the hadoop profile  it looks like testtableinputformatscan1 and testtableinputformatscan2 never complete and surefire doesn't complain about it. sure, you may see this: tests run: 6, failures: 5, errors: 1, skipped: 0, time elapsed: 269.036 sec <<< failure! org.apache.maven.surefire.util.surefirereflectionexception: ... org.apache.maven.surefire.booter.forkedbooter.main(forkedbooter.java:74) caused by: java.lang.outofmemoryerror: permgen space org.apache.maven.surefire.booter.surefirebooterforkexception: error occurred in starting fork, check output in log ...   testscanemptytobba(org.apache.hadoop.hbase.mapreduce.testtableinputformatscan1)   testscanemptytobbb(org.apache.hadoop.hbase.mapreduce.testtableinputformatscan1)   testscanemptytoopp(org.apache.hadoop.hbase.mapreduce.testtableinputformatscan1)   testscanemptytoempty(org.apache.hadoop.hbase.mapreduce.testtableinputformatscan1) but then: [info] hbase - server .................................... success [10:59.929s] this is on my machine. on our local jenkins it's leaking and the processes never die. and this is only with hadoop 2. it also looks like other tests are failing with permgen space. ",
        "label": 229
    },
    {
        "text": "hbase breaks testhregion testrecoverededitsreplaycompaction under hadoop2 profile  testhregion#testrecoverededitsreplycompaction and testhregionbusywait#testrecoverededitsreplycompaction fail against the hadoop2 profile due to hbase-2231. if you checkout at the patch on trunk, the error trace looks like this:    <error message=\"file /home/jon/proj/hbase/hbase-server/target/test-data/05b5be10-bc88-40b0-a274-d1ebffe24e85/testhregiontestrecoverededitsreplaycompaction/testrecoverededitsreplaycompaction/f1de1d311572557ca13c4cb810ebfc0b/.tmp does not exist\" type=\"java.io.filenotfoundexception\">java.io.filenotfoundexception: file /home/jon/proj/hbase/hbase-server/target/test-data/05b5be10-bc88-40b0-a274-d1ebffe24e85/testhregiontestrecoverededitsreplaycompaction/testrecoverededitsreplaycompaction/f1de1d311572557ca13c4cb810ebfc0b/.tmp does not exist         at org.apache.hadoop.fs.rawlocalfilesystem.liststatus(rawlocalfilesystem.java:340)         at org.apache.hadoop.fs.filesystem.liststatus(filesystem.java:1418)         at org.apache.hadoop.fs.filesystem.liststatus(filesystem.java:1458)         at org.apache.hadoop.fs.checksumfilesystem.liststatus(checksumfilesystem.java:569)         at org.apache.hadoop.hbase.regionserver.testhregion.testrecoverededitsreplaycompaction(testhregion.java:462)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at junit.framework.testcase.runtest(testcase.java:176)         at junit.framework.testcase.runbare(testcase.java:141)         at junit.framework.testresult$1.protect(testresult.java:122)         at junit.framework.testresult.runprotected(testresult.java:142)         at junit.framework.testresult.run(testresult.java:125)         at junit.framework.testcase.run(testcase.java:129)         at junit.framework.testsuite.runtest(testsuite.java:255)         at junit.framework.testsuite.run(testsuite.java:250)         at org.junit.internal.runners.junit38classrunner.run(junit38classrunner.java:84)         at org.apache.maven.surefire.junit4.junit4provider.execute(junit4provider.java:226)         at org.apache.maven.surefire.junit4.junit4provider.executetestset(junit4provider.java:133)         at org.apache.maven.surefire.junit4.junit4provider.invoke(junit4provider.java:114)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.maven.surefire.util.reflectionutils.invokemethodwitharray(reflectionutils.java:188)         at org.apache.maven.surefire.booter.providerfactory$providerproxy.invoke(providerfactory.java:166)         at org.apache.maven.surefire.booter.providerfactory.invokeprovider(providerfactory.java:86)         at org.apache.maven.surefire.booter.forkedbooter.runsuitesinprocess(forkedbooter.java:101) this was found via git bisect. git bisect run mvn clean install test -dskipits -dhadoop.profile=2.0 -dtest=testhregion#testrecoverededitsreplaycompaction ",
        "label": 155
    },
    {
        "text": "convert authenticationprotocol to protocol buffer service  with coprocessor endpoints now exposed as protobuf defined services, we should convert over all of our built-in endpoints to pb services. accesscontrollerprotocol was converted as part of hbase-5448, but the authentication token provider still needs to be changed. ",
        "label": 180
    },
    {
        "text": "pom cleaning  filter out jersey core in hadoop2 to match hadoop3 and remove redunant version specifications  working on hbase-22029, where we fail compile of hbase-it module four hours into an rc build, it looks like transitive include of jersey-core is the culprit. hadoop3 profile does a bunch of jersey-core exclusions. this issue is about having hadoop2 profile and hadoop3 profiles match around jersey-core treatment. some miscellaneous cleanups are also done. ",
        "label": 314
    },
    {
        "text": "locateregioninmeta should check the cache before doing the prefetch  locateregioninmeta uses a regionlockobject to synchronize all accesses to prefetch the regioncache.  synchronized (regionlockobject) {  // if the parent table is meta, we may want to pre-fetch some  // region info into the global region cache for this table.  if (bytes.equals(parenttable, hconstants.meta_table_name) &&  (getregioncacheprefetch(tablename)) ) { prefetchregioncache(tablename, row); } // check the cache again for a hit in case some other thread made the  // same query while we were waiting on the lock. if not supposed to  // be using the cache, delete any existing cached location so it won't  // interfere.  if (usecache) {  location = getcachedlocation(tablename, row);  if (location != null) { return location; } } else { deletecachedlocation(tablename, row); } however, for this to be effective, we need to check the cache as soon as we grab the lock; before doing the prefetch. checking the cache after doing the prefetch does not help the current thread, in case another thread has done the prefetch. ",
        "label": 199
    },
    {
        "text": "add means of getting the timestamps for all cell versions  e g  long   getversions row  column   should be means of asking hbase for list of all the timestamps associated with a particular cell. the brute force way would be adding a getversions method but perhaps we can come up w/ something more elegant than this? ",
        "label": 144
    },
    {
        "text": "setpreallocsize is different with comment in setuptestenv in minizookeepercluster java  setuptestenv() just sets 100 bytes not 100k bytes private static void setuptestenv() { // / xxx: from o.a.zk.t.clientbaseprivate static void setuptestenv() { // during the tests we run with 100k prealloc in the logs. // on windows systems prealloc of 64m was seen to take ~15seconds // resulting in test failure (client timeout on first session). // set env and directly in order to handle static init/gc issues   system.setproperty(\"zookeeper.preallocsize\", \"100\");   filetxnlog.setpreallocsize(100); } to reduce too many call padlogfile(), i think 100k is correct. ",
        "label": 114
    },
    {
        "text": " root  and  meta  are stale in table jsp if they moved  table.jsp does not use a lookup method on catalogtracker that does not force a refresh of the cache, thus it can get a stale location if root or .meta. moved and the master hasn't tried to access them yet. should just be a matter of using waitforroot/meta. ",
        "label": 286
    },
    {
        "text": "update our hadoop to alpha  ",
        "label": 314
    },
    {
        "text": "an offline splitparent region can be assigned breaking split references  an offline split parent region can be assigned by using 'assign' from shell. this may trigger a compaction on the split parent region that should be offline, breaking the references of the two daughters. the easy way to test this is: disable compactions create a table and insert some row split shutdown the master (otherwise there's a state checked inside the assignmentmanager) start the master assign the region marked as offline => true, split => true (the compaction on region startup will probably break the reference of the two daughters) ",
        "label": 242
    },
    {
        "text": "just remove deprecated methods in htable  is not backward compatible anyways  htable had deprecations introduce post 0.1 branch but 0.2 api is not going to be backward compatible because of other changes so i suggest we just remove the deprecated methods. users have to change their api anyways. the less noisey the api, the easier it'll be on users figuring what to use instead. added to 0.2 ",
        "label": 314
    },
    {
        "text": "fixup shell for hbase  shell was broken by hbase-1822. fix. ",
        "label": 38
    },
    {
        "text": "expose processlist in shell  per regionserver and perhaps by cluster   hbase-4057 adds processlist and it shows in the rs ui. this issue is about getting the processlist to show in the shell, like it does in mysql. labelling it noob; this is a pretty substantial issue but it shouldn't be too hard \u2013 it'd mostly be plumbing from rs into the shell. ",
        "label": 437
    },
    {
        "text": "fix coverage org apache hadoop hbase rest filter  ",
        "label": 40
    },
    {
        "text": "replicationsource thread is not always shutdown when a peer is removed   when a replication peer is removed the replicationsource's thread is not shutdown if it is still trying to get a connection to the peer. ",
        "label": 286
    },
    {
        "text": "update description of  root  in ref guide  since the resolution of hbase-3171, -root- is no longer used to store the location(s) of .meta. . unfortunately, not all of our documentation has been updated to reflect this change in architecture. ",
        "label": 330
    },
    {
        "text": "integrate with unit testing tools of hadoop's metrics2 framework  hadoop's metrics2 framework provides handy tools to write unit-tests for metrics sources. e.g. metricsasserts class. we want to use that too in hbase unit-tests. integration seems straightforward, wowever when integrating this piece we faced maven bug: http://jira.codehaus.org/browse/mrresources-53. hence we decided to extract this into separate issue (originally was done in one of the patches of hbase-6411). ",
        "label": 154
    },
    {
        "text": "ensure hbase is covered in thrift  hbase-4910 is a fix in thrift 1, make sure thrift 2 is not affected. ",
        "label": 285
    },
    {
        "text": "very wide rows   30m plus   cause us oome  from the list, see 'jvm oom' in http://mail-archives.apache.org/mod_mbox/hbase-user/201101.mbox/browser, it looks like wide rows \u2013 30m or so \u2013 causes oome during compaction. we should check it out. can the scanner used during compactions use the 'limit' when nexting? if so, this should save our oome'ing (or, we need to add to the next a max size rather than count of kvs). ",
        "label": 336
    },
    {
        "text": "add batchsize and filter to thrift2  attached patch will add batchsize and filter support to thrift2 ",
        "label": 193
    },
    {
        "text": "remove the is annotation from spacelimitingexception  @interfaceaudience.public @interfacestability.evolving public class spacelimitingexception extends quotaexceededexception { ",
        "label": 226
    },
    {
        "text": "bootstrapping a cluster leaves temporary wal directory laying around  when a new cluster is started, it creates a temporary wal as hbase:meta is created during bootstrapping the system. then this log is closed before properly opened on a region server. the temp wal file is scheduled for removal, moved to oldwals and eventually claimed. issue is that the wal directory with the temp region is not removed. for example: drwxr-xr-x   - hadoop hadoop          0 2015-05-28 10:21 /hbase/wals/hregion-65589555 the directory is empty and does not harm, but on the other hand it is not needed anymore and should be removed. cosmetic and good housekeeping. ",
        "label": 441
    },
    {
        "text": " hbase thirdparty  upgrade the dependencies for hbase thirdparty  ",
        "label": 149
    },
    {
        "text": "exclude  target  from rat checks  we need to exclude */target/* from rat checks in order to build releases now. ",
        "label": 38
    },
    {
        "text": "hbase for the new master  hbase-1921 was lost when writing the new master code. i guess it's going to be much harder to implement now, but i think it's a critical feature to have considering the reasons that brought me do it in the old master. there's already a test in testzookeeper which has been disabled a while ago. ",
        "label": 428
    },
    {
        "text": "clash between region unassign and splitting kills the master  i was running an online alter while regions were splitting, and suddenly the master died and left my table half-altered (haven't restarted the master yet). what killed the master: 2011-11-02 17:06:44,428 fatal org.apache.hadoop.hbase.master.hmaster: unexpected zk exception creating node closing  org.apache.zookeeper.keeperexception$nodeexistsexception: keepererrorcode = nodeexists for /hbase/unassigned/f7e1783e65ea8d621a4bc96ad310f101  at org.apache.zookeeper.keeperexception.create(keeperexception.java:110)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:42)  at org.apache.zookeeper.zookeeper.create(zookeeper.java:637)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.createnonsequential(recoverablezookeeper.java:459)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.create(recoverablezookeeper.java:441)  at org.apache.hadoop.hbase.zookeeper.zkutil.createandwatch(zkutil.java:769)  at org.apache.hadoop.hbase.zookeeper.zkassign.createnodeclosing(zkassign.java:568)  at org.apache.hadoop.hbase.master.assignmentmanager.unassign(assignmentmanager.java:1722)  at org.apache.hadoop.hbase.master.assignmentmanager.unassign(assignmentmanager.java:1661)  at org.apache.hadoop.hbase.master.bulkreopen$1.run(bulkreopen.java:69)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) a znode was created because the region server was splitting the region 4 seconds before: 2011-11-02 17:06:40,704 info org.apache.hadoop.hbase.regionserver.splittransaction: starting split of region testtable,0012469153,1320253135043.f7e1783e65ea8d621a4bc96ad310f101.  2011-11-02 17:06:40,704 debug org.apache.hadoop.hbase.regionserver.splittransaction: regionserver:62023-0x132f043bbde0710 creating ephemeral node for f7e1783e65ea8d621a4bc96ad310f101 in splitting state  2011-11-02 17:06:40,751 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:62023-0x132f043bbde0710 attempting to transition node f7e1783e65ea8d621a4bc96ad310f101 from rs_zk_region_splitting to rs_zk_region_splitting  ...  2011-11-02 17:06:44,061 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:62023-0x132f043bbde0710 successfully transitioned node f7e1783e65ea8d621a4bc96ad310f101 from rs_zk_region_splitting to rs_zk_region_split  2011-11-02 17:06:44,061 info org.apache.hadoop.hbase.regionserver.splittransaction: still waiting on the master to process the split for f7e1783e65ea8d621a4bc96ad310f101 now that the master is dead the region server is spewing those last two lines like mad. ",
        "label": 314
    },
    {
        "text": "clone operation from hbaseadmin can hang forever   sometimes the clone operation from the hbase shell can hang. the table has been created (it shows up in the web ui), but does not have any entries in meta. there don't seem to be any clone, snapshot, enable or disable found in the master's jstack. here's a trace from the hbaseadmin: \"main\" prio=10 tid=0x00007f782800d000 nid=0x25c waiting on condition [0x00007f782f9bf000]    java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.client.hbaseadmin.clonesnapshot(hbaseadmin.java:2413)         at org.apache.hadoop.hbase.client.hbaseadmin.clonesnapshot(hbaseadmin.java:2393)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.jruby.javasupport.javamethod.invokedirectwithexceptionhandling(javamethod.java:465)         at org.jruby.javasupport.javamethod.invokedirect(javamethod.java:323)         at org.jruby.java.invokers.instancemethodinvoker.call(instancemethodinvoker.java:69)         at org.jruby.runtime.callsite.cachingcallsite.call(cachingcallsite.java:201)         at org.jruby.ast.calltwoargnode.interpret(calltwoargnode.java:59)         at org.jruby.ast.newlinenode.interpret(newlinenode.java:104) ... (more jruby stack) ...  ",
        "label": 309
    },
    {
        "text": "loadtest tool no longer packaged after the modularization  ",
        "label": 236
    },
    {
        "text": "can not create table with the column family  count   this does not create a table create table tablex (count max_versions=1 compression=block in_memory); i found its the \"count\" column causing the problem ",
        "label": 86
    },
    {
        "text": "hserverload regionload breaks compatibility  this commit broke our 0.92/0.94 compatibility: ------------------------------------------------------------------------ r1136686 | stack | 2011-06-16 14:18:08 -0700 (thu, 16 jun 2011) | 1 line hbase-3927 display total uncompressed byte size of a region in web ui i just tried the new rc for 0.94. i brought up a 0.94 master on a 0.92 cluster and rather than just digest version 1 of the hserverload, i get this: 2012-04-14 22:47:59,752 warn org.apache.hadoop.ipc.hbaseserver: unable to read call parameters for client 10.4.14.38 java.io.ioexception: error in readfields         at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:684)         at org.apache.hadoop.hbase.ipc.invocation.readfields(invocation.java:125)         at org.apache.hadoop.hbase.ipc.hbaseserver$connection.processdata(hbaseserver.java:1269)         at org.apache.hadoop.hbase.ipc.hbaseserver$connection.readandprocess(hbaseserver.java:1184)         at org.apache.hadoop.hbase.ipc.hbaseserver$listener.doread(hbaseserver.java:722)         at org.apache.hadoop.hbase.ipc.hbaseserver$listener$reader.dorunloop(hbaseserver.java:513)         at org.apache.hadoop.hbase.ipc.hbaseserver$listener$reader.run(hbaseserver.java:488)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) caused by: a record version mismatch occured. expecting v2, found v1         at org.apache.hadoop.io.versionedwritable.readfields(versionedwritable.java:46)         at org.apache.hadoop.hbase.hserverload$regionload.readfields(hserverload.java:379)         at org.apache.hadoop.hbase.hserverload.readfields(hserverload.java:686)         at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:681)         ... 9 more ",
        "label": 441
    },
    {
        "text": "hudson running  test  target twice for trunk builds  this seems to be a recent occurrence (or at least i noticed it recently). for builds of trunk, hudson is invoking the \"test\" target twice in a row, see: https://hudson.apache.org/hudson/job/hbase-trunk/1643/console looks like it's running once for \"install\" and once for \"assembly:assembly\". on our internal hudson setup, we saw the same issue and fixed by changing the project targets from \"clean install assembly:assembly\" to just \"clean assembly:assembly\". would be good for someone with more mvn knowledge to confirm this is the right fix though. and if so we should make the same change in the asf hudson config. no reason for hudson builds to be slower than they need to. ",
        "label": 180
    },
    {
        "text": "reject read write from client but accept write from replication in state s  ",
        "label": 149
    },
    {
        "text": "testrpcmetrics fails on machine where region server is running  since whole test suite takes over an hour to run, i ran them on linux where region server is running. here is the consistent testrpcmetrics failure i saw: tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 0.196 sec <<< failure! testcustommetrics(org.apache.hadoop.hbase.regionserver.testrpcmetrics)  time elapsed: 0.079 sec  <<< error! java.net.bindexception: problem binding to /10.202.50.107:60020 : address already in use         at org.apache.hadoop.hbase.ipc.hbaseserver.bind(hbaseserver.java:216)         at org.apache.hadoop.hbase.ipc.hbaseserver$listener.<init>(hbaseserver.java:283)         at org.apache.hadoop.hbase.ipc.hbaseserver.<init>(hbaseserver.java:1189)         at org.apache.hadoop.hbase.ipc.writablerpcengine$server.<init>(writablerpcengine.java:266)         at org.apache.hadoop.hbase.ipc.writablerpcengine.getserver(writablerpcengine.java:233)         at org.apache.hadoop.hbase.ipc.writablerpcengine.getserver(writablerpcengine.java:46)         at org.apache.hadoop.hbase.ipc.hbaserpc.getserver(hbaserpc.java:379)         at org.apache.hadoop.hbase.ipc.hbaserpc.getserver(hbaserpc.java:368)         at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:336)         at org.apache.hadoop.hbase.regionserver.testrpcmetrics$testregionserver.<init>(testrpcmetrics.java:58)         at org.apache.hadoop.hbase.regionserver.testrpcmetrics.testcustommetrics(testrpcmetrics.java:119) ",
        "label": 21
    },
    {
        "text": "shell changes for setting consistency per request  we can add shell support to set consistency per request. ",
        "label": 499
    },
    {
        "text": "multiple test case failures  result-test.cc fails on estimatedsize assertion: the test assumes sizeof(result) will be the same as the estimated size of an empty result which is not the case because estimated size also accounts for current capacity of its row_ string member. when using docker:: classpath is not propagated properly within the docker container: if you follow the documentation and build the java project before starting docker the classpath file contains the wrong paths. also the code to read from a classpath env var doesn't quite work as expected minicluster is not writing an hbase-site.xml to disk as expected by some of the tests\u2014as a result the default zk port of 2181 is being used by clients ",
        "label": 208
    },
    {
        "text": "avoid uuid randomuuid in tests  we have a lot of places in our test code where we use uuid.randomuuid to generate table names or paths for uniqueness. unfortunately, this uses up a good chunk of system entropy, since sun chose that random uuid's should use the nativeprngblocking implementation. we don't need to block on entropy for random bits to pick a random table name in a test, so we can use something that doesn't strain the system too much - secure random can be a source of problems on some vm or containers. ",
        "label": 389
    },
    {
        "text": "allow incremental table alterations  as per the hbase shell help, the alter command will \"alter column family schema; pass table name and a dictionary specifying new column family schema.\" the assumption here seems to be that the new column family schema must be completely specified. in other words, if a certain attribute is not specified in the column family schema, then it is effectively defaulted. is this side-effect by design? i for one assumed (wrongly apparently) that i can alter a table in \"increments\". case in point, the following commands should've resulted in the final value of the versions attribute of my table to stay put at 1, but instead it got defaulted to 3. i guess there's no right or wrong answer here, but what should alter do by default? my expectation is that it only changes those attributes that were specified in the \"alter\" command, leaving the unspecified attributes untouched. hbase(main):003:0> create 't1', {name => 'f1', versions => 1} 0 row(s) in 1.7230 seconds  hbase(main):004:0> describe 't1'  description   {name => 't1', families => [ {name => 'f1', compression => 'none', versions => '1', ttl => '2147483647', blocksize => '65536', in_memory => ' false', blockcache => 'true'} ]}  1 row(s) in 0.2030 seconds  hbase(main):006:0> disable 't1'  0 row(s) in 0.1140 seconds  hbase(main):007:0> alter 't1', {name => 'f1', in_memory => 'true'} 0 row(s) in 0.0160 seconds  hbase(main):009:0> describe 't1'  description   {name => 't1', families => [ {name => 'f1', versions => '3', compression => 'none', ttl => '2147483647', blocksize => '65536', in_memory => ' true', blockcache => 'true'} ]}  1 row(s) in 0.1280 seconds ",
        "label": 265
    },
    {
        "text": "enable export snapshot tests that were disabled by proc v2 am in hbase  the proc-v2 am in hbase-14614 disabled the following tests: disabled testexportsnapshot hangs. disabled testsecureexportsnapshot disabled testmobsecureexportsnapshot and testmobexportsnapshot this jira tracks the work to enable them. if mob requires more work, we could split to 2 tickets. ",
        "label": 205
    },
    {
        "text": " replication  replicationsource won't close if failing to contact the sink  when trying to close a source, it will hang if it's already in shipedits() and has issues reaching the sink. the reason is that in that method the while loop only checks if the rs is going down but not if the source was asked to shutdown. ",
        "label": 229
    },
    {
        "text": "hbase shell disregards spaces at the end of a split key in a split file  when converting row keys to a printable string representation, bytes class considers space as a printable character, so it prints it out as it is. so, it's quite possible that a row key has a space at the end. when specifying split points in a file, the row keys are not quoted and the shell wrapper \"admin.rb\" strips any whitespace off the row keys:  file.foreach(splits_file) do |line|             arg[splits].push(line.strip())           end the correct approach is to use \"chomp()\" instead of \"strip()\" to just strip off carriage returns and newlines. we should assume that the hbase user is either using split points printed out by hbase itself (which will not have tabs) or is diligent enough to not use tabs at the end of a split point.  what's worse is that it goes undetected and will result in undesirable split points. ",
        "label": 97
    },
    {
        "text": "improve hbaseserver getremoteaddress by utilizing hbaseserver connection hostaddress  currently, hbaseserver#getremoteaddress would call getremoteip(), leading to call.connection.socket.getinetaddress(). the host address is actually stored in hbaseserver.connection.hostaddress field. we don't need to go through socket to get this information. without this patch it costs 4000ns, with this patch it costs 1600ns ",
        "label": 292
    },
    {
        "text": "hbaseobjectwritable readobject catches donotretryioexception and wraps it back in a regular ioexception  exception handling inside hbaseobjectwritable needs to be reworked, imho. for example:  at several places inside hbaseobjectwritable.readobject, exceptions are caught and rethrown as i/o exception (including classnotfoundexception!). so, if an implementation of readfields method throws a donotretryioexception, hbase still ends up retrying. this problem exists at least in 0.94.12 version of hbase. ",
        "label": 286
    },
    {
        "text": "testhbasefsck occasionally hung  from https://builds.apache.org/job/hbase-0.95-on-hadoop2/247/console : \"pool-1-thread-1\" prio=10 tid=0x73a2a400 nid=0x2f4d in object.wait() [0x73bdd000]    java.lang.thread.state: timed_waiting (on object monitor) at java.lang.object.wait(native method) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1412) - locked <0xccdd8898> (a org.apache.hadoop.hbase.ipc.rpcclient$call) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1630) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1687) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$blockingstub.createtable(masteradminprotos.java:29365) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$5.createtable(hconnectionmanager.java:1996) at org.apache.hadoop.hbase.client.hbaseadmin$2.call(hbaseadmin.java:590) at org.apache.hadoop.hbase.client.hbaseadmin$2.call(hbaseadmin.java:586) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:120) - locked <0x81c8abb0> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:98) - locked <0x81c8abb0> (a org.apache.hadoop.hbase.client.rpcretryingcaller) at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:3087) at org.apache.hadoop.hbase.client.hbaseadmin.createtableasync(hbaseadmin.java:586) at org.apache.hadoop.hbase.client.hbaseadmin.createtable(hbaseadmin.java:477) at org.apache.hadoop.hbase.util.testhbasefsck.setuptable(testhbasefsck.java:338) at org.apache.hadoop.hbase.util.testhbasefsck.testsplitdaughtersnotinmeta(testhbasefsck.java:1362) ...      {color:red}-1 core zombie tests{color}.  there are 1 zombie test(s):  at org.apache.hadoop.hbase.util.testhbasefsck.testsplitdaughtersnotinmeta(testhbasefsck.java:1362)' i looked at https://builds.apache.org/job/hbase-0.95-on-hadoop2/247/artifact/0.95-on-hadoop2/hbase-server/target/surefire-reports/,  there was no test output. ",
        "label": 441
    },
    {
        "text": "cell decoder scanner etc  should not hide exceptions  cell scanner, base decoder, etc., hide ioexception inside runtime exception. this can lead to unexpected behavior because a lot of code only expects ioexception. there's no logical justification behind this hiding so it should be removed before it's too late (the sooner we do it the less throws declarations need to be added) ",
        "label": 314
    },
    {
        "text": "backport hbase to  backport hbase-9605 which is about \"allow aggregationclient to skip specifying column family for row count aggregate\" ",
        "label": 520
    },
    {
        "text": "bad random read performance from synchronizing hfile fddatainputstream  deep in the hfile read path, there is this code:  synchronized (in) { in.seek(pos); ret = in.read(b, off, n); } this makes it so that only 1 read per file per thread is active. this prevents the os and hardware from being able to do io scheduling by optimizing lots of concurrent reads. we need to either use a reentrant api (pread may be partially reentrant according to todd) or use multiple stream objects, 1 per scanner/thread. ",
        "label": 314
    },
    {
        "text": "add a stress test tool for region based procedure store  ",
        "label": 149
    },
    {
        "text": "coprocessor loading and execution improvements  umbrella for current issues with coprocessor class loading and execution that should be sorted out for 0.98. ",
        "label": 38
    },
    {
        "text": "disabling table stuck on regions in transition  after stress testing with only icv and trying to disable table hbase run into regions-in-transition loop. i have logs and threads dumps. this region is on db2a regionserver, more logs as attachments 2010-10-26 09:32:00,838 info org.apache.hadoop.hbase.master.assignmentmanager: waiting on ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792 to clear regions-in-transition   2010-10-26 09:32:01,838 info org.apache.hadoop.hbase.master.assignmentmanager: waiting on ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792 to clear regions-in-transition   2010-10-26 09:32:02,838 info org.apache.hadoop.hbase.master.assignmentmanager: waiting on ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792 to clear regions-in-transition   2010-10-26 09:32:02,842 info org.apache.hadoop.hbase.master.servermanager: regionservers=2, averageload=1803   2010-10-26 09:32:03,104 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792   2010-10-26 09:32:03,104 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_close or closing for too long, running forced unassign again on region=ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0.  2010-10-26 09:32:03,104 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x12be24df47b0000 deleting existing unassigned node for 85b04e1097c676ef52b16d49305e0ab0 that is in expected state rs_zk_region_closing   2010-10-26 09:32:03,104 warn org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x12be24df47b0000 attempting to delete unassigned node in rs_zk_region_closing state but node is in rs_zk_region_closed state   2010-10-26 09:32:03,838 info org.apache.hadoop.hbase.master.assignmentmanager: waiting on ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792 to clear regions-in-transition   2010-10-26 09:32:04,838 info org.apache.hadoop.hbase.master.assignmentmanager: waiting on ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792 to clear regions-in-transition   2010-10-26 09:32:05,838 info org.apache.hadoop.hbase.master.assignmentmanager: waiting on ngolden_ac,,1287997478278.85b04e1097c676ef52b16d49305e0ab0. state=pending_close, ts=1288078080792 to clear regions-in-transition ",
        "label": 314
    },
    {
        "text": "hbaseobjectwritable should support arrays of any writable or serializable  not just writable   presently, an exception is actually thrown when deserializing the output of hbaseobjectwritable.writeobject(somesubclassofwritable[]). the issue is pretty difficult to debug. this patch adds support for arrays whose contents are subtypes of both serializable and writable. ",
        "label": 150
    },
    {
        "text": " shell  can't set table descriptor attributes when i alter a table  i wanted to set the compaction filesize attribute but no good: hbase(main):005:0> alter 'testtable', {max_filesize => 67108864}  typeerror: can't convert hash into string from /users/stack/documents/checkouts/hbase/trunk/bin/../bin/hbase.rb:138:in `hcd' from /users/stack/documents/checkouts/hbase/trunk/bin/../bin/hbase.rb:138:in `alter' from /users/stack/documents/checkouts/hbase/trunk/bin/../bin/hirb.rb:239:in `alter' from (hbase):6:in `binding' hbase(main):006:0> alter 'testtable', {max_filesize => 67108864} ",
        "label": 229
    },
    {
        "text": "keyvalue equals and compareto methods should match  keyvalue.kvcomparator includes the memstorets when comparing, however the keyvalue.equals() method ignores the memstorets. the comparator interface has always specified that comparator return 0 when equals would return true and vice versa. obeying that rule has been sort of optional in the past, but java 7 introduces a new default collection sorting algorithm called tim sort which relies on that behavior. http://bugs.sun.com/view_bug.do?bug_id=6804124 possible problem spots: there's a collections.sort(keyvalues) in redundantkvgenerator.generatetestkeyvalues(..) testcolumnseeking compares two collections of keyvalues using the containsall method. it is intentionally ignoring memstorets, so will need an alternative method for comparing the two collections. ",
        "label": 441
    },
    {
        "text": "remove keyvalue getbuffer   in many places this is simple task of just replacing the method name.  there, however, quite a few places where we assume that: 1. the entire kv is backed by a single byte array 2. the kvs key portion is backed by a single byte array some of those can easily be fixed, others will need their own jiras. ",
        "label": 314
    },
    {
        "text": "npe in replication when hdfs transparent encryption is enabled   we are seeing a npe when replication (or in this case async wal replay for region replicas) is run on top of an hdfs cluster with tde configured. this is the stack trace: java.lang.nullpointerexception         at org.apache.hadoop.hbase.cellutil.matchingrow(cellutil.java:370)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.countdistinctrowkeys(replicationsource.java:649)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.readallentriestoreplicateornextfile(replicationsource.java:450)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:346) this stack trace can only happen if waledit.getcells() returns an array containing null entries. i believe this happens due to keyvaluecodec.parsecell() uses keyvalueutil.iscreate() which returns null in case of eof at the beginning. however, the contract for the decoder.parsecell() is not clear whether returning null is acceptable or not. the other decoders (compressedkvdecoder, cellcodec, etc) do not return null while keyvaluecodec does. basedecoder has this code:   public boolean advance() throws ioexception {     if (!this.hasnext) return this.hasnext;     if (this.in.available() == 0) {       this.hasnext = false;       return this.hasnext;     }     try {       this.current = parsecell();     } catch (ioexception ioex) {       rethroweofexception(ioex);     }     return this.hasnext;   } which is not correct since it uses is.available() not according to the javadoc: (https://docs.oracle.com/javase/7/docs/api/java/io/inputstream.html#available()). dfsinputstream implements available() as the remaining bytes to read from the stream, so we do not see the issue there. cryptoinputstream.available() does a similar thing but see the issue. so two questions: what should be the interface for decoder.parsecell()? can it return null? how to properly fix basedecoder.advance() to not rely on available() call. ",
        "label": 155
    },
    {
        "text": "hbase can be stuck when closing regions concurrently  the attached test fails ~1% of the the time on 0.96. it seems it does not fail on 0.94.5. it's simple: a table creation and some puts. i attach the stack. logs says nothing it seems.  the suspicious part is: \"rs_close_region-localhost,57575,1361197489166-2\" prio=10 tid=0x00007fb0c8775800 nid=0x61ac runnable [0x00007fb09f272000]    java.lang.thread.state: runnable         at java.util.treemap.fixafterdeletion(treemap.java:2193)         at java.util.treemap.deleteentry(treemap.java:2151)         at java.util.treemap.remove(treemap.java:585)         at java.util.treeset.remove(treeset.java:259)         at org.apache.hadoop.hbase.regionserver.metricsregionaggregatesourceimpl.deregister(metricsregionaggregatesourceimpl.java:55)         at org.apache.hadoop.hbase.regionserver.metricsregionsourceimpl.close(metricsregionsourceimpl.java:86)         at org.apache.hadoop.hbase.regionserver.metricsregion.close(metricsregion.java:40)         at org.apache.hadoop.hbase.regionserver.hregion.doclose(hregion.java:1063)         at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:969)         - locked <0x00000006944e2558> (a java.lang.object)         at org.apache.hadoop.hbase.regionserver.handler.closeregionhandler.process(closeregionhandler.java:146)         at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:203)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) ",
        "label": 441
    },
    {
        "text": "use tcp keepalives  if a regionserver crashes while the client is engaged in ipc with it at a vulnerable point in the tcp fsm (established, no outstanding data to send), the ipc will be stuck waiting \"forever\" (> 12 hours, etc.). this hoses the client, especially if it is trying to look up a region in meta. worse, it is not possible to restart the regionserver if the hung client is colocated with it on the same host, because the os will consider port 60020 bound and in use, unless the client is forcibly killed. killing some types of applications \u2013 especially long running processes which can't redo work from a checkpoint but must start over from the beginning \u2013 can be very painful. investigate if tcp keepalives can be enabled at the ipc level. ",
        "label": 38
    },
    {
        "text": "scanner metrics are only reported if you get to the end of a scanner  when you turn on scanner metrics, the metrics are currently only made available if you run over all records available in the scanner. if you stop iterating before the end, the values are never flushed into the metrics object (in the scan attribute). will supply a patch with fix and test. ",
        "label": 209
    },
    {
        "text": "shaded artifacts are incorrect when built against hadoop  building master/branch-2 against the hadoop-3 profile results in check-invariants screaming about unrelocated dependencies. will list details in comment. ",
        "label": 320
    },
    {
        "text": "add storefile index size to hbase metrics  ",
        "label": 38
    },
    {
        "text": "rewrite testendtoendsplittransaction testcansplitjustafterasplit  the test itself is not stable enough, we just send a split request out and then use a dead loop to reference the store files of the split regions. ",
        "label": 149
    },
    {
        "text": "catch closedchannelexception and document it  closedchannelexception is a pretty obscure exception for the non-expert and doesn't tell you why you get it. we should instead catch it, print a warn, don't print a stack trace, and add a line in the book about this. ",
        "label": 229
    },
    {
        "text": "add precommit check for hbase native client  the ut and other checks for hbase cpp client haven't been added into the hadoop qa pipeline ",
        "label": 71
    },
    {
        "text": "remove unneeded api from encodedseeker  see parent. we do not need getkeyvaluebuffer. it's only used for tests, and parent patch fixes all tests to use getkeyvalue instead. ",
        "label": 198
    },
    {
        "text": "partitions file created in user's home directory by importtsv  i am using hbase 0.94 from cdh3u1.  after running importtsv, i find that a temporary partitions_* file is written to my user home directory in hdfs. this file should really be deleted automatically when it is no longer needed. ",
        "label": 339
    },
    {
        "text": "mean age of blocks in cache  seconds  on webui should be greater than zero  ",
        "label": 198
    },
    {
        "text": "book xml   fill out descriptions of metrics  i filled out the skeleton of the metrics in book.xml, but i'd like one of the committers to fill in the rest of the details on what these mean. thanks! for example.. ------- i'm assuming that these are referring to the lru block cache in memory. i'd like to docs to state this (for the sake of clarity) and also the units (e.g., mb). hbase.regionserver.blockcachecount  hbase.regionserver.blockcachefree  hbase.regionserver.blockcachehitratio  hbase.regionserver.blockcachesize ------- this is read latency from hdfs, i assume... hbase.regionserver.fsreadlatency_avg_time  hbase.regionserver.fsreadlatency_num_ops  hbase.regionserver.fssynclatency_avg_time  hbase.regionserver.fssynclatency_num_ops  hbase.regionserver.fswritelatency_avg_time  hbase.regionserver.fswritelatency_num_ops ---------------- point in time utilized (i.e., as opposed to max or trailing) memstore i assume, would be nice to document. hbase.regionserver.memstoresizemb --------------- obvious, but might as well document it hbase.regionserver.regions -------------- this is any put, get, delete, or scan operation, i assume? hbase.regionserver.requests -------------- detail on these would be nice, especially for tips on if there are any critical numbers/ratios to watch for. hbase.regionserver.storefileindexsizemb  hbase.regionserver.stores ",
        "label": 146
    },
    {
        "text": "address new license dependencies from hadoop3 alpha4  hadoop3-alpha4 adds at least one new dependency that requires an update to our supplemental-models.xml use this jira to track the changes for after that is released. ",
        "label": 320
    },
    {
        "text": "disable online altering by default  create a config for it  there's a whole class of bugs that we've been revealing from trying out online altering in conjunction with other operations like splitting. hbase-4729, hbase-4794, and hbase-4814 are examples. it's not so much that the online altering code is buggy, but that it wasn't tested in an environment that permits splitting. i think we should mark online altering as experimental in 0.92 and add a config to enable it (so it would be disabled by default, requiring people to enable for altering table schema). ",
        "label": 544
    },
    {
        "text": "increase timeouts in testreplication and testsplitlogworker  when i measure the times in testreplication.queuefailover, it takes about 15s on my (reasonably fast) laptop.  the timeout in queuefailover currently is 1500*2*15 = 45000ms.  for setup before each test (which truncates the table and waits for the changes to replicate) it is 1500*15 = 22500ms. interestingly i see queuefailover failures where the wait time is measured as 64260ms and some at 72316ms.  since these numbers are not even close to 45000ms, the machine or jvm must have been stuck for 15 or almost 30s (otherwise we'd get a timeout and the total time spent should be close to the timeout). so i would suggest that we increase the timeouts further.  we could set sleep_time to 2000 and retries to 20. would lead to 2000*2*20 = 80000ms. any objections? ",
        "label": 286
    },
    {
        "text": "add document about the ia private classes which appear in ia limitedprivate interfaces  ",
        "label": 149
    },
    {
        "text": " javadoc  add javadoc to delete explaining behavior when no timestamp provided  if delete family or delete row and no timestamp, we add a delete of 'now'. when delete of explicit cell, we add a delete of the most recent. add this fact to the delete javadoc. i was just stumped by this differing behavior. ",
        "label": 314
    },
    {
        "text": "fully enable servershutdownhandler after master joins the cluster  once root and meta are assigned, servershutdownhandler is enabled. so that we can handle meta/root region server failure before joincluster is completed. however, we can hold servershutdownhandler a little bit more for the user region assignments, i.e. doesn't assign user regions before joincluster is returned. if so, we can avoid some region assignments racing: same regions are trying to be assigned in both joincluster and servershutdownhandler. ",
        "label": 242
    },
    {
        "text": "unit tests won't run because of a typo  i had to modify line#101 in hbasetestcase.java in order to run the unit tests: (conf.get(\"fs.default.name\", \"file:///\").compareto(\"file::///\") == 0); i removed the second \":\" in compareto() so it becomes \"file:///\" ",
        "label": 314
    },
    {
        "text": "performanceevaluation generates 10x the number of expected mappers  with a command line like 'hbase org.apache.hadoop.hbase.performanceevaluation randomwrite 10' there are 100 mappers spawned, rather than the expected 10. the culprit appears to be the outer loop in writeinputfile which sets up 10 splits for every \"asked-for client\". i think the fix is just to remove that outer loop. ",
        "label": 499
    },
    {
        "text": "wrong sleep time when regionservercallable need retry  in rpcretryingcallerimpl, it get pause time by expectedsleep = callable.sleep(pause, tries + 1); and in regionservercallable, it get pasue time by sleep = connectionutils.getpausetime(pause, tries + 1). so tries will be bumped up twice. and the pasue time is 3 * hbase.client.pause when tries is 0.  retry_backoff = {1, 2, 3, 5, 10, 20, 40, 100, 100, 100, 100, 200, 200} ",
        "label": 187
    },
    {
        "text": "bin hbase script doesn't look for hadoop jars in the right place in trunk layout  running against an 0.24.0-snapshot hadoop:  ls: cannot access /home/todd/ha-demo/hadoop-0.24.0-snapshot/hadoop-common*.jar: no such file or directory  ls: cannot access /home/todd/ha-demo/hadoop-0.24.0-snapshot/hadoop-hdfs*.jar: no such file or directory  ls: cannot access /home/todd/ha-demo/hadoop-0.24.0-snapshot/hadoop-mapred*.jar: no such file or directory  the jars are rooted deeper in the heirarchy. ",
        "label": 242
    },
    {
        "text": "add missing security checks in rsrpcservices  the following rpc methods in rsrpcservices do not have acl check for admin rights. execregionserverservice ",
        "label": 48
    },
    {
        "text": "two css files raise release audit warning  from https://builds.apache.org/job/precommit-hbase-build/6869/artifact/trunk/patchprocess/patchreleaseauditproblems.txt :  !????? /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-server/src/main/resources/hbase-webapps/static/css/bootstrap-theme.css  !????? /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-server/src/main/resources/hbase-webapps/static/css/bootstrap-theme.min.css lines that start with ????? in the release audit report indicate files that do not have an apache license header. ",
        "label": 339
    },
    {
        "text": "profiler tab on master rs ui not working w o comprehensive message  as titled, when checking 1.5.0 rc3 binary package, clicking the \"profiler\" tab on hmaster/regionserver web ui, it complains page not found error like below: problem accessing /prof. reason:     not_found ",
        "label": 38
    },
    {
        "text": "control direct memory buffer consumption by hbaseclient  as jonathan explained here https://groups.google.com/group/asynchbase/browse_thread/thread/c45bc7ba788b2357?pli=1 , standard hbase client inadvertently consumes large amount of direct memory. we should consider using netty for nio-related tasks. ",
        "label": 75
    },
    {
        "text": "remove keylength cache from keyvalue  see discussion in hbase-9935. currently keyvalue caches the keylength in order to avoid decoding the key length in getkeylength() repeatedly.  benchmarking outside of a profiler revealed no improvement from that cache (and a sampling profiler also does not indicate getkeylength() as a performance issue).  along with this there's also a slight fix in scanquerymatcher. ",
        "label": 286
    },
    {
        "text": "fix saveversion sh's git awareness   many developers are using git to work with hbase. we should make the build scripts git aware (point to a git hash ) for revision. ",
        "label": 154
    },
    {
        "text": "testcacheconfig failing consistently in precommit builds  as stated in description ",
        "label": 314
    },
    {
        "text": "hconnectionimplementation does not connect to new active master  1) started hbase cluster with two masters   2) started shell.  3) master switch happened.  from now onward not able to perform list command without restarting the shell. its always pointing to old master. hbase(main):003:0> list table error: java.net.connectexception: connection refused here is some help for this command: list all tables in hbase. optional regular expression parameter could be used to filter the output. examples:   hbase> list   hbase> list 'abc.*' ",
        "label": 346
    },
    {
        "text": "fix broken tests  setwritebuffer now throws ioe   ",
        "label": 314
    },
    {
        "text": "deletion of splitting node on split rollback should clear the region from rit  if a failure happens in split before offlining_parent, we tend to rollback the split including deleting the znodes created.  on deletion of the rs_zk_splitting node we are getting a callback but not remvoving from rit. we need to remove it from rit, anyway ssh logic is well guarded in case the delete event comes due to rs down scenario. ",
        "label": 544
    },
    {
        "text": "testshell seems passed  but actually errors seen in test output file  when i was making test cases for 4554, i saw a weird issue that testshell seems to pass, but actually i saw error messages in the output file. -------------------------------------------------------  t e s t s ------------------------------------------------------- running org.apache.hadoop.hbase.client.testshell tests run: 1, failures: 0, errors: 0, skipped: 0, time elapsed: 39.252 sec results : tests run: 1, failures: 0, errors: 0, skipped: 0 error messages in org.apache.hadoop.hbase.client.testshell-output.txt: ...   6) error: test_alter_should_support_shortcut_delete_alter_specs(hbase::adminaltertabletest): argumenterror: there should be at least one argument but the table name     /home/mlai/git/hbase-private/src/test/ruby/../../main/ruby/hbase/admin.rb:307:in `alter'     ./src/test/ruby/hbase/admin_test.rb:271:in `test_alter_should_support_shortcut_delete_alter_specs'     org/jruby/rubyproc.java:268:in `call'     org/jruby/rubykernel.java:2038:in `send'     org/jruby/rubyarray.java:1572:in `each'     org/jruby/rubyarray.java:1572:in `each'        7) error: test_split_should_work(hbase::adminmethodstest): argumenterror: wrong number of arguments (1 for 2)     ./src/test/ruby/hbase/admin_test.rb:99:in `test_split_should_work'     org/jruby/rubyproc.java:268:in `call'     org/jruby/rubykernel.java:2038:in `send'     org/jruby/rubyarray.java:1572:in `each'     org/jruby/rubyarray.java:1572:in `each'      192 tests, 259 assertions, 1 failures, 6 errors done with tests! shutting down the cluster... 2011-10-07 16:46:14,760 info  [main] hbase.hbasetestingutility(551): shutting down minicluster 2011-10-07 16:46:14,760 debug [main] util.jvmclusterutil(214): shutting down hbase cluster ",
        "label": 327
    },
    {
        "text": "regionservers have a lock contention of configuration getprops  here's an extract from thread dump of the regionserver of my cluster: ... thread 267 (rw.default.readrpcserver.handler=184,queue=15,port=60020):   state: blocked   blocked count: 204028   waited count: 9702639   blocked on org.apache.hadoop.conf.configuration@5a5e3da   blocked by 250 (rw.default.readrpcserver.handler=167,queue=18,port=60020)   stack:     org.apache.hadoop.conf.configuration.getprops(configuration.java:2250)     org.apache.hadoop.conf.configuration.get(configuration.java:861)     org.apache.hadoop.conf.configuration.gettrimmed(configuration.java:880)     org.apache.hadoop.conf.configuration.getboolean(configuration.java:1281)     org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:138)     org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:157)     org.apache.hadoop.hbase.regionserver.hstore.createscanner(hstore.java:1804)     org.apache.hadoop.hbase.regionserver.hstore.getscanner(hstore.java:1794)     org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.<init>(hregion.java:3852)     org.apache.hadoop.hbase.regionserver.hregion.instantiateregionscanner(hregion.java:1952)     org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1938)     org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1915)     org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:4872)     org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:4847)     org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:2918)     org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:29921)     org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2031)     org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:108)     org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:116)     org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:96) ... there are such many threads in the thread dump.  i think that regionservers have a lock contention which causes performance issue. ",
        "label": 38
    },
    {
        "text": "encourage use of 'lzo' compression  add the wiki page to getting started  lzo makes life better and we should be actively encouraging folks to run with it more than we should have up to this. add it to 'getting started' and integrate the wiki doc. here. ",
        "label": 314
    },
    {
        "text": "testatomicoperation testmultirowmutationmultithreads is still failing occasionally  see run here: https://builds.apache.org/job/precommit-hbase-build/1318//testreport/org.apache.hadoop.hbase.regionserver/testatomicoperation/testmultirowmutationmultithreads/ 2012-03-27 04:36:12,627 debug [thread-118] regionserver.storescanner(499): storescanner.peek() is changed where before = rowb/colfamily11:qual1/7202/put/vlen=6/ts=7922,and after = rowb/colfamily11:qual1/7199/deletecolumn/vlen=0/ts=0  2012-03-27 04:36:12,629 info [thread-121] regionserver.hregion(1558): finished memstore flush of ~2.9k/2952, currentsize=1.6k/1640 for region testtable,,1332822963417.7cd30e219714cfc5e91f69def66e7f81. in 14ms, sequenceid=7927, compaction requested=true  2012-03-27 04:36:12,629 debug [thread-126] regionserver.testatomicoperation$2(362): flushing  2012-03-27 04:36:12,630 debug [thread-126] regionserver.hregion(1426): started memstore flush for testtable,,1332822963417.7cd30e219714cfc5e91f69def66e7f81., current region memstore size 1.9k  2012-03-27 04:36:12,630 debug [thread-126] regionserver.hregion(1474): finished snapshotting testtable,,1332822963417.7cd30e219714cfc5e91f69def66e7f81., commencing wait for mvcc, flushsize=1968  2012-03-27 04:36:12,630 debug [thread-126] regionserver.hregion(1484): finished snapshotting, commencing flushing stores  2012-03-27 04:36:12,630 debug [thread-126] util.fsutils(153): creating file=/home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/.tmp/61954619003e469baf1a34be5ff2ec57 with permission=rwxrwxrwx  2012-03-27 04:36:12,631 debug [thread-126] hfile.hfilewriterv2(143): initialized with cacheconfig:enabled [cachedataonread=true] [cachedataonwrite=false] [cacheindexesonwrite=false] [cachebloomsonwrite=false] [cacheevictonclose=false] [cachecompressed=false]  2012-03-27 04:36:12,631 info [thread-126] regionserver.storefile$writer(997): delete family bloom filter type for /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/.tmp/61954619003e469baf1a34be5ff2ec57: compoundbloomfilterwriter  2012-03-27 04:36:12,632 info [thread-126] regionserver.storefile$writer(1220): no general bloom and no deletefamily was added to hfile (/home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/.tmp/61954619003e469baf1a34be5ff2ec57)   2012-03-27 04:36:12,632 info [thread-126] regionserver.store(770): flushed , sequenceid=7934, memsize=1.9k, into tmp file /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/.tmp/61954619003e469baf1a34be5ff2ec57  2012-03-27 04:36:12,632 debug [thread-126] regionserver.store(795): renaming flushed file at /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/.tmp/61954619003e469baf1a34be5ff2ec57 to /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/colfamily11/61954619003e469baf1a34be5ff2ec57  2012-03-27 04:36:12,634 info [thread-126] regionserver.store(818): added /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/target/test-data/b9091c3c-961e-4035-850a-83ad14d517cc/testatomicoperationtestmultirowmutationmultithreads/testtable/7cd30e219714cfc5e91f69def66e7f81/colfamily11/61954619003e469baf1a34be5ff2ec57, entries=12, sequenceid=7934, filesize=1.3k  2012-03-27 04:36:12,642 debug [thread-118] regionserver.testatomicoperation$2(392): []  exception in thread \"thread-118\" junit.framework.assertionfailederror at junit.framework.assert.fail(assert.java:48)  at junit.framework.assert.fail(assert.java:56)  at org.apache.hadoop.hbase.regionserver.testatomicoperation$2.run(testatomicoperation.java:394)  2012-03-27 04:36:12,643 info [thread-126] regionserver.hregion(1558): finished memstore flush of ~1.9k/1968, currentsize=1.3k/1312 for region testtable,,1332822963417.7cd30e219714cfc5e91f69def66e7f81. in 14ms, sequenceid=7934, compaction requested=true i think this issue is different from parent. ",
        "label": 286
    },
    {
        "text": "revert 'instant schema alter' for now  hbase  see this discussion: http://search-hadoop.com/m/nxcqh1klsxr1/pull+instant+schema+updating+out%253f&subj=pull+instant+schema+updating+out+ pull out hbase-4213 for now. can add it back later. ",
        "label": 314
    },
    {
        "text": "need mainline to flush when 'blocking updates' goes up   look at the below. blocking goes up and flush doesn't happen for nearly ten minutes (other flushes happen in the meantime no problem). we need to run a flush immediately; schedule it before all others. 2008-08-12 01:45:54,175 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 9 on 60020' on region testtable,0024119972,1218505443888: memcache size 64.0m is >= than blocking 64.0m size ... 2008-08-12 01:54:36,695 debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region testtable,0024119972,1218505443888. current region memcache size 64.0m ",
        "label": 229
    },
    {
        "text": "nightly tests should verify src tgz generates and builds correctly  twice at release time i've bumped into broken src tgz packaging. this is somewhat expected as builds tend to have dark, hidden corners. let's add a check to our build bot so we find these issues earlier. ",
        "label": 402
    },
    {
        "text": "fix debug message in simpleregionnormalizer for small regions  the simpleregionnormalizer has this:      if ((smallestregion.getsecond() + smallestneighborofsmallestregion.getsecond()           < avgregionsize)) {         log.debug(\"table \" + table + \", smallest region size: \" + smallestregion.getsecond()           + \" and its smallest neighbor size: \" + smallestneighborofsmallestregion.getsecond()           + \", less than half the avg size, merging them\"); it does not check for \"less than half the avg size\" but only \"less than the avg size\", that is, drop the \"half\". fix message. ",
        "label": 284
    },
    {
        "text": "dividing fiterlist into two separate sub classes  filterlistwithor   filterlistwithand  ",
        "label": 514
    },
    {
        "text": "provide a common base abstract class for both regionobserver and masterobserver  some security coprocessors extend both regionobserver and masterobserver, unfortunately only one of the two can use the available base abstract class implementations. provide a common base abstract class for both the regionobserver and masterobserver interfaces. update current coprocessors that extend both interfaces to use the new common base abstract class. ",
        "label": 309
    },
    {
        "text": "remove the settabletimestamp and settablesequenceid  they are introduced by hbase-11777 and hbase-12082. both of them are ia.lp and denoted with deprecated. to my way of thinking, we should remove them before 2.0 release. ",
        "label": 98
    },
    {
        "text": "improve failed testmetascanner assert message so can see where why failure  i am looking at a failed testmetascanner from internal cluster and it asserts failure but says nought about why: stacktrace java.lang.assertionerror at org.junit.assert.fail(assert.java:92) at org.junit.assert.asserttrue(assert.java:43) at org.junit.assert.asserttrue(assert.java:54) at org.apache.hadoop.hbase.client.testmetascanner$1metascannerverifier.run(testmetascanner.java:208) at java.lang.thread.run(thread.java:662) totally useless. ",
        "label": 314
    },
    {
        "text": "document permissions that need to be set on importtsv output before completebulkload  i am using hbase 0.94 from cdh3u1.  after running importtsv using the -dimporttsv.bulk.output=<output dir> option, i find that completebulkload fails due to hbase not having write permissions on the contents of the output dir that importtsv wrote. i have to manually set write permissions on these contents before i can run completebulkload successfully.  ideally, i should not have to do that (set the permissions manually). given that i do, this should at least be documented as a limitation of the importtsv utility. ",
        "label": 339
    },
    {
        "text": "add a remote peer cluster wal directory config for synchronous replication  ",
        "label": 187
    },
    {
        "text": "add a chapter on hdfs in the troubleshooting section of the hbase reference guide   i looked mainly at the major failure case, but here is what i have: new sub chapter in the existing chapter \"troubleshooting and debugging hbase\": \"hdfs & hbase\" 1) hdfs & hbase  2) connection related settings  2.1) number of retries  2.2) timeouts  3) log samples 1) hdfs & hbase  hbase uses hdfs to store its hfile, i.e. the core hbase files and the write-ahead-logs, i.e. the files that will be used to restore the data after a crash.  in both cases, the reliability of hbase comes from the fact that hdfs writes the data to multiple locations. to be efficient, hbase needs the data to be available locally, hence it's highly recommended to have the hdfs datanode on the same machines as the hbase region servers. detailed information on how hdfs works can be found at [1]. important features are: hbase is a client application of hdfs, i.e. uses the hdfs dfsclient class. this class can appears in hbase logs with other hdfs client related logs. some hdfs settings are hdfs-server-side, i.e. must be set on the hdfs side, while some other are hdfs-client-side, i.e. must be set in hbase, while some other must be set in both places. the hdfs writes are pipelined from one datanode to another. when writing, there are communications between: hbase and hdfs namenode, through the hdfs client classes. hbase and hdfs datanodes, through the hdfs client classes. hdfs datanode between themselves: issues on these communications are in hdfs logs, not hbase. hdfs writes are always local when possible. as a consequence, there should not be much write error in hbase region servers: they write to the local datanode. if this datanode can't replicate the blocks, it will appear in its logs, not in the region servers logs. datanodes can be contacted through the ipc.client interface (once again this class can shows up in hbase logs) and the data transfer interface (usually shows up as the datanode class in the hbase logs). there are on different ports (defaults being: 50010 and 50020). to understand exactly what's going on, you must look that the hdfs log files as well: hbase logs represent the client side. with the default setting, hdfs needs 630s to mark a datanode as dead. for this reason, this node will still be tried by hbase or by other datanodes when writing and reading until hdfs definitively decides it's dead. this will add some extras lines in the logs. this monitoring is performed by the namenode. the hdfs clients (i.e. hbase using hdfs client code) don't fully rely on the namenode, but can mark temporally a node as dead if they had an error when they tried to use it. 2) settings for retries and timeouts  2.1) retries  ipc.client.connect.max.retries  default 10  indicates the number of retries a client will make to establish a server connection. not taken into account if the error is a sockettimeout. in this case the number of retries is 45 (fixed on branch, hadoop-7932 or in hadoop-7397). for sasl, the number of retries is hard-coded to 15. can be increased, especially if the socket timeouts have been lowered. ipc.client.connect.max.retries.on.timeouts  default 45  if you have hadoop-7932, max number of retries on timeout. counter is different than ipc.client.connect.max.retries so if you mix the socket errors you will get 55 retries with the default values. could be lowered, once it is available. with hadoop-7397 ipc.client.connect.max.retries is reused so there would be 10 tries. dfs.client.block.write.retries  default 3  number of tries for the client when writing a block. after a failure, will connect to the namenode a get a new location, sending the list of the datanodes already tried without success. could be increased, especially if the socket timeouts have been lowered. see hbase-6490. dfs.client.block.write.locatefollowingblock.retries  default 5  number of retries to the namenode when the client got notreplicatedyetexception, i.e. the existing nodes of the files are not yet replicated to dfs.replication.min. this should not impact hbase, as dfs.replication.min is defaulted to 1. dfs.client.max.block.acquire.failures  default 3  number of tries to read a block from the datanodes list. in other words, if 5 datanodes are supposed to hold a block (so dfs.replication equals to 5), the client will try all these datanodes, then check the value of dfs.client.max.block.acquire.failures to see if it should retry or not. if so, it will get a new list (likely the same), and will try to reconnect again to all these 5 datanodes. couldbe be increased, especially if the socket timeouts have been lowered. 2.2) timeouts  2.3.1) heatbeats  dfs.heartbeat.interval  default is 3s heartbeat.recheck.interval = 300s  defaults is 300s a datanode is considered as dead when there is no heartbeat for (2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval) seconds. that's 630s. so before the 10:30 minutes, the datanode is considered as fully available by the namenode. after this delay, hdfs is likely to start replicating the blocks contained in the dead node to get back to the right number of replica. as a consequence, if we're too aggressive we will have a side effect here, adding workload to an already damaged cluster. for this reason it's not recommended to change these settings. as there are communications between the datanodes, and as they share these settings, these settings are both hdfs-client-side and hdfs-server-side. 2.3.2) socket timeouts  3 timeouts are considered in hdfs: connect timeout: the timeout when we tried to establish the connection read timeout: the timeout when we read something on an already established connection write timeout: the timeout when we try to write something on an already established connection. they are managed by two settings: dfs.socket.timeout  default 60s dfs.datanode.socket.write.timeout  default is 480s. but these setting are used: between the dfsclient and the datanode between the ipc.client and the datanodes between the datanodes sometimes but not always with an extension (depending on the number of replica) for dfs.socket.timeout as a socket connect timeout but as well as a socket read timeout. for dfs.datanode.socket.write.timeout, when it's set to 0, a plain old java socket is created in some cases instead of a nio. final calculated connect timeout can be:  hard-coded to 20s for the the ipc.client in hadoop 1.0.3 (changed in hadoop-7397)  dfs.socket.timeout (ex: datanode#datatransfer, dataxceiver#replaceblock)  dfs.socket.timeout + 3s*#replica (ex: dataxceiver#write, dfsclient#getfilechecksum called from filechecksumservlet) final read timeouts can be:  dfs.socket.timeout (dataxceiver#replaceblock, ipc.client from dfsclient)  dfs.socket.timeout + 3s*#replica (ex: datanode#datatransfer, dataxceiver#write)  dfs.socket.timeout * #replica (ex: datanode#datatransfer) final calculated write timeouts can be:  dfs.datanode.socket.write.timeout (ex dataxceiver#copyblock/readblock/...)  dfs.datanode.socket.write.timeout + 5s*#replica) (ex dfsclient#createblockoutputstream, dataxceiver#writeblock)  dfs.datanode.socket.write.timeout + 5s*(#replica -1) (ex: datanode#datatransfer. see hadoop-5464). hence we will often see a 69000 timeout in the logs before the datanode is marked dead/excluded. also, setting \"dfs.socket.timeout\" to 0 does not make it wait forever, but likely 9 seconds instead of 69s for data transfer. 3) typical error logs.  3.1) typical logs when all datanode for a block are dead, making the hbase recovery impossible. hbase master logs will contain, with a 0.90 hbase:  info hdfs.dfsclient: failed to connect to /xxx50010, add to deadnodes and continue java.net.sockettimeoutexception: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.socketchannel[connection-pending remote=/region-server-1:50010]  => the client tries to connect to a dead datanode.  => it failed, so the client will try the next datanode in the list. usually the list size is 3 (dfs.replication).  => if the final list is empty, it means that all the datanodes proposed by the namenode are in our datanodes list.  => the hdfs client clears the dead nodes list and sleeps 3 seconds (hard-coded), shallowing interruptedexception, and asks again to the namenode. this is the log line:  info hdfs.dfsclient: could not obtain block blk_xxx from any node: java.io.ioexception: no live nodes contain current block. will get new block locations from namenode and retry...  => all the locations initially given by the namenode to this client are actually dead. the client asks for a new set of locations.  => we're very likely to have exactly the same datanode list as 3 seconds ago, except if a datanode came back to life or if a replication has just finished.  => after dfs.client.max.block.acquire.failures (default: 3), an exception is thrown, then logged, and we have in the logs:  warn hdfs.dfsclient: dfs read: java.io.ioexception: could not obtain block: blk_xxx file=/hbase/.logs/boxxxx,60020,xxx/xxx%3a60020.yyy  => there is another retry, hard-coded to 2, but this is logged only once, even if the second try fails.  => moreover, for the second try the errors counters are not reinitialized, including the dead nodes list, so this second attempt is unlikely to succeed. it should come again with an empty node list, and throw a new java.io.ioexception: could not obtain block: blk_xxx file=/hbase/.logs/boxxxx,60020,xxx/xxx%3a60020.yyy  => this exception will go to the final client (hbase). hbase will log it, and we will see  info wal.hlogsplitter: got while parsing hlog hdfs://namodenode:8020//hbase/.logs/boxxxx,60020,xxx/xxx60020.yyy. marking as corrupted java.io.ioexception: could not obtain block: blk_xxx file=/hbase/.logs/boxxxx,60020,xxx/xxx60020.yyy 3.2) typical log for write issues: the master reads the log, then wants to split it, hence writing a block:  info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.net.sockettimeoutexception: 69000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.socketchannel[connection-pending remote=/ xxx:50010]  => we tried to connect to the dead datanode to write. likely from the master (it does not have a datanode, so it connects to a remote datanode).  => a region server will not have this type of error, as it connects to a local datanode to write.  => it failed at the beginning, we cannot connect at all (i.e. not during the write itself)  info hdfs.dfsclient: abandoning block blk_xxx  => hbase (as a hdfs client) told to the namenode that the block is not written.  info hdfs.dfsclient: excluding datanode xxx:50010  => internally in hdfs client the stream puts it in the excludednodes list (the \"excluding datanode\" log line ).  => the hdfs client is going again to the namenode asking for another datanode set proposal, sending the excluded datanode list to be sure it's not trying on the same nodes again.  => there will be 3 retries by default. if you've lost 20% of your cluster 1% of the time the 3 attempts will fail. setting: \"dfs.client.block.write.retries\". if it's the case (i.e. all attempts failed), next log line is:  warn hdfs.dfsclient: datastreamer exception: java.io.ioexception: unable to create new block. and then, if it was for a split log:  fatal wal.hlogsplitter: writerthread-xxx got while writing log entry to log (various possible stacks here) ",
        "label": 330
    },
    {
        "text": "hbase shell's 'status 'detailed'' should escape the printed keys  currently the hbase shell's status command prints unescaped keys on the terminal causing the terminal to print garbage characters. we should escape the printed keys. ",
        "label": 468
    },
    {
        "text": "hbase should use a hadoop home environment variable if available   i have been burned a few times lately while developing code by having the make sure that the hadoop jar in hbase/lib is exactly correct. in my own deployment, there are actually 3 jars and a native library to keep in sync that hbase shouldn't have to know about explicitly. a similar problem arises when using stock hbase with cdh3 because of the security patches changing the wire protocol. all of these problems could be avoided by not assuming that the hadoop library is in the local directory. moreover, i think it might be possible to assemble the distribution such that the compile time hadoop dependency is in a cognate directory to lib and is referenced using a default value for hadoop_home. does anybody have any violent antipathies to such a change? ",
        "label": 14
    },
    {
        "text": "client hangs because the client is not notified  if the call first remove from the calls, when some exception happened in reading from the datainputstream, the call will not be notified, cause the client hangs. ",
        "label": 292
    },
    {
        "text": "region balancing does not bring newly added node within acceptable range  with a 10 node cluster, there were only 9 online nodes. with about 215 total regions, each of the 9 had around 24 regions (average load is 24). slop is 10% so 22 to 26 is the acceptable range. starting up the 10th node, master log showed: 2008-11-21 15:57:51,521 info org.apache.hadoop.hbase.master.servermanager: received start message from: 72.34.249.210:60020 2008-11-21 15:57:53,351 debug org.apache.hadoop.hbase.master.regionmanager: server 72.34.249.219:60020 is overloaded. server load: 25 avg: 22.0, slop: 0.1 2008-11-21 15:57:53,351 debug org.apache.hadoop.hbase.master.regionmanager: choosing to reassign 3 regions. mostloadedregions has 10 regions in it. 2008-11-21 15:57:53,351 debug org.apache.hadoop.hbase.master.regionmanager: going to close region streamitems,^@^@^@^@^ah\u00ef\u00bf\u00bd;,1225411051632 2008-11-21 15:57:53,351 debug org.apache.hadoop.hbase.master.regionmanager: going to close region streamitems,^@^@^@^@^@\u00ef\u00bf\u00bd\u00fd,1225411056686 2008-11-21 15:57:53,351 debug org.apache.hadoop.hbase.master.regionmanager: going to close region groups,,1222913580957 2008-11-21 15:57:53,975 debug org.apache.hadoop.hbase.master.regionmanager: server 72.34.249.213:60020 is overloaded. server load: 25 avg: 22.0, slop: 0.1 2008-11-21 15:57:53,975 debug org.apache.hadoop.hbase.master.regionmanager: choosing to reassign 3 regions. mostloadedregions has 10 regions in it. 2008-11-21 15:57:53,976 debug org.apache.hadoop.hbase.master.regionmanager: going to close region upgrade,,1226892014784 2008-11-21 15:57:53,976 debug org.apache.hadoop.hbase.master.regionmanager: going to close region streamitems,^@^@^@^@^@3^z\u00ef\u00bf\u00bd,1225411056701 2008-11-21 15:57:53,976 debug org.apache.hadoop.hbase.master.regionmanager: going to close region streamitems,^@^@^@^@^@         ^l,1225411049042 the new regionserver received only 6 regions. this happened because when the 10th came in, average load dropped to 22. this caused two servers with 25 regions (acceptable when avg was 24 but not now) to reassign 3 of their regions each to bring them back down to the average. unfortunately all other regions remained within the 10% slop (20 to 24) so they were not overloaded and thus did not reassign off any regions. it was only chance that made even 6 of the regions get reassigned as there could have been exactly 24 on each server, in which case none would have been assigned to the new node. this will behave worse on larger clusters when adding a new node has little impact on the avg load/server. ",
        "label": 167
    },
    {
        "text": "during import  single region blocks requests for  minutes  thread dumps  throws out pending requests  and continues  during a batch import, i have two processes importing into a single region. the behavior i saw was a regionserver with 2 regions of the table in question on it. the first region split, and the new regions were reassigned to another regionserver. following that, inserting into the region that was left over began to block client requests. i am attaching the regionserver log; below is the specific problem area: 2008-07-31 15:38:24,190 debug org.apache.hadoop.hbase.client.hconnectionmanager$tableservers: cache hit in table locations for row <> and tablename .meta.: location server 72.34.249.217:60020, location region name .meta.,,1  2008-07-31 15:38:24,194 info org.apache.hadoop.hbase.regionserver.compactsplitthread: region split, meta updated, and report to master all successful. old region=region => {name => 'items,01beddd6-813b-4f2b-ac48-a0cef395cb7e,12175434512  2008-07-31 15:38:34,052 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 7 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: memcache size 64.0m is >= than blocking  2008-07-31 15:39:00,270 info org.apache.hadoop.ipc.server: ipc server handler 8 on 60020, call batchupdate([b@17b4239f, row => 02c241b4-9d32-452d-8dab-247f4af693eb, {column => content:title, value => '...', column => content:content, va  org.apache.hadoop.hbase.notservingregionexception: items,01beddd6-813b-4f2b-ac48-a0cef395cb7e,1217543451296  at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:1436)  at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdate(hregionserver.java:1147)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:616)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:473)  at org.apache.hadoop.ipc.server$handler.run(server.java:896)  2008-07-31 15:39:09,547 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 8 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: memcache size 64.0m is >= than blocking  2008-07-31 15:39:44,079 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 9 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: memcache size 64.0m is >= than blocking  2008-07-31 15:40:19,574 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 1 on 60020' on region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296: memcache size 64.0m is >= than blocking  2008-07-31 15:49:09,130 info org.apache.hadoop.hbase.regionserver.logroller: rolling hlog. number of entries: 1  2008-07-31 15:49:09,144 debug org.apache.hadoop.hbase.regionserver.hlog: closing current log writer /hbase/log_72.34.249.212_1217535541159_60020/hlog.dat.1217543884691  2008-07-31 15:49:09,146 info org.apache.hadoop.hbase.regionserver.hlog: new log writer created at /hbase/log_72.34.249.212_1217535541159_60020/hlog.dat.1217544549145  2008-07-31 16:03:09,060 debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296. current region memcache size 64.0m  2008-07-31 16:03:09,467 info org.apache.hadoop.hbase.regionserver.hregion: unblocking updates for region items,8001eb31-98bb-4087-bd8d-e4b42805addb,1217543451296 'ipc server handler 5 on 60020'  2008-07-31 16:03:09,478 info org.apache.hadoop.ipc.server: process thread dump: discarding call batchupdate([b@4e727e0e, row => c08408b4-b68c-433c-ba3f-d46d3ba73288, {column => content:title, value => '...', column => content:content, v as you can see there was a 14 minute delay between updates being blocked, and the unblocking occurring. all the pending batchupdates were thrown out (too old) and then importing proceeded normally. the same behavior repeated itself later on a different regionserver, and again after a while it unfroze, kicked out pending updates, and continued. ",
        "label": 38
    },
    {
        "text": "add logging to schema change chaos actions   ",
        "label": 154
    },
    {
        "text": "fix findbugs complaint so yetus reports more green  umbrella issue for findbugs fixings. the red complaints in yetus report look ugly. lets fix. there ain't too many. this is umbrella issue. can do a component at a time. ",
        "label": 314
    },
    {
        "text": "npe close region  meta  in shell  ",
        "label": 314
    },
    {
        "text": "procedure v2  reimplement dispatchmergingregionhandler  use the proc-v2 state machine for dispatchmergingregionhandler. ",
        "label": 426
    },
    {
        "text": "maven hbase build broken on cygwin with copynativelib sh call   this is broken in both 0.92 as well as trunk pom.xml here's a sample maven log snippet from trunk (from mayuresh on user mailing list) [info] [antrun:run {execution: package} ]  [info] executing tasks main:  [mkdir] created dir: d:\\workspace\\mkshirsa\\hbase-trunk\\target\\hbase-0.93-snapshot\\hbase-0.93-snapshot\\lib\\native\\${build.platform}  [exec] ls: cannot access d:workspacemkshirsahbase-trunktarget/nativelib: no such file or directory  [exec] tar (child): cannot connect to d: resolve failed  [info] ------------------------------------------------------------------------  [error] build error  [info] ------------------------------------------------------------------------  [info] an ant buildexception has occured: exec returned: 3328 there are two issues:   1) the ant run task below doesn't resolve the windows file separator returned by the project.build.directory - this causes the above resolve failed.  <!-- using unix cp to preserve symlinks, using script to handle wildcards -->  <echo file=\"${project.build.directory}/copynativelibs.sh\">  if [ `ls ${project.build.directory}/nativelib | wc -l` -ne 0]; then 2) the tar argument value below also has a similar issue in that the path arg doesn't resolve right.  <!-- using unix tar to preserve symlinks -->  <exec executable=\"tar\" failonerror=\"yes\" dir=\"${project.build.directory}/${project.artifactid}-${project.version}\">  <arg value=\"czf\"/>  <arg value=\"/cygdrive/c/workspaces/hbase-0.92-svn/target/${project.artifactid}-${project.version}.tar.gz\"/>  <arg value=\".\"/>  </exec> in both cases, the fix would probably be to use a cross-platform way to handle the directory locations. ",
        "label": 431
    },
    {
        "text": "hbaseadmin istableavailable  name   returns true when the table does not exist  hbaseadmin::istableavailable( name ) returns true for a table in which hbaseadmin::tableexists( name ) returns false. it appears from the code that the default return value from istableavailable() is true and false is only returned in the case where the table is found and not all the region servers are online. ",
        "label": 38
    },
    {
        "text": "use collections emptylist  for empty list values  use collection.emptylist() for returning an empty list instead of return new arraylist<> (). the default constructor creates a buffer of size 10 for arraylist therefore, returning this static value saves on some memory and gc pressure and saves time not having to allocate a new internally buffer for each instantiation. ",
        "label": 130
    },
    {
        "text": " transactional  client testtransactions testputputscan fails sometimes  testcase: testputputscan took 15.822 sec failed  expected:<299> but was:<199> not sure exactly how the test is supposed to work but it seems that sometimes the two put are on the same timestamp so the value returned is 199. i will commit a temporary fix to branch in order to release 0.20.2 ",
        "label": 110
    },
    {
        "text": "copytable mr job named  copy table  in driver  the copytable mr job needs to change name, currently it requires passing quotes around it since it's \"copy table\". also all the other names are lower case and without and white spaces. ",
        "label": 314
    },
    {
        "text": "npe reading zk config in hbase  if zoo.cfg contains server.* (\"server.0=server0:2888:3888\\n\") and cluster.distributed property (in hbase-site.xml) is empty we get an npe in parsezoocfg(). the easy way to reproduce the bug is running org.apache.hbase.zookeeper.testhquorumpeer with hbase-site.xml containing: <property>   <name>hbase.cluster.distributed</name>   <value></value> </property> ",
        "label": 309
    },
    {
        "text": " fb  make hfilewriter use the bytesperchecksum obtained from config  we found a bug in the latest hbase code which \"hfile.io.bytes.per.checksum\" does not take effect. because we have never call hfile.getbytesperchecksum() when creating a hfile. fix this issue by calling the correct bytes/checksum (from hfile.getbytesperchecksum) for hfile creating ? ",
        "label": 378
    },
    {
        "text": "masteraddresstracker   zknamespacemanager zk listeners are missed after master recovery  testzookeeper#testregionassignmentaftermasterrecoveryduetozkexpiry always failed at the following verification for me in my dev env(you have to run the single test not the whole testzookeeper suite to reproduce) assertequals(\"number of rows should be equal to number of puts.\", numberofputs, numberofrows); we missed two zk listeners after master recovery masteraddresstracker & zknamespacemanager. my current patch is to fix the jira issue while i'm wondering if we should totally remove the master failover implementation when zk session expired because this causes reinitialize hmaster partially which is error prone and not a clean state to start from. ",
        "label": 233
    },
    {
        "text": "filter hfiles based on ttl  in scanwildcardcolumntracker we have     this.oldeststamp = environmentedgemanager.currenttimemillis() - ttl;   ...   private boolean isexpired(long timestamp) {     return timestamp < oldeststamp;   } but this time range filtering does not participate in hfile selection. in one real case this caused next() calls to time out because all kvs in a table got expired, but next() had to iterate over the whole table to find that out. we should be able to filter out those hfiles right away. i think a reasonable approach is to add a \"default timerange filter\" to every scan for a cf with a finite ttl and utilize existing filtering in storefile.reader.passestimerangefilter. ",
        "label": 324
    },
    {
        "text": "testhbasefsck testchecktablelocks broke  java lang assertionerror  expected  but was expired table lock   i've been looking into this test failure because i thought it particular to my rpc hackery. what i see is like the subject: java.lang.assertionerror: expected:<[]> but was:<[expired_table_lock]> and later in same unit test: java.lang.assertionerror: expected:<[expired_table_lock]> but was:<[expired_table_lock, expired_table_lock]> the test creates a write lock and then expires it. in subject failure, we are expiring the lock ahead of the time it should be. easier for me to reproduce is that the second write lock we put in place is not allowed to happen because of the presence of the first lock even though it has been judged expired: error: table lock acquire attempt found:[tablename=foo, lockowner=localhost,60000,1, threadid=387, purpose=testchecktablelocks, isshared=false, createtime=129898749] 2013-05-02 00:34:42,715 info  [thread-183] lock.zkinterprocesslockbase(431): lock is held by: write-testing utility0000000000 error: table lock acquire attempt found:[tablename=foo, lockowner=localhost,60000,1, threadid=349, purpose=testchecktablelocks, isshared=false, createtime=28506852] above, you see the expired lock and then our hbck lock visitor has it that the second lock is expired because it is held by the first lock. i can keep looking at this but input would be appreciated. it failed in recent trunk build https://builds.apache.org/view/h-l/view/hbase/job/hbase-trunk/4090/testreport/junit/org.apache.hadoop.hbase.util/testhbasefsck/testchecktablelocks/ ",
        "label": 314
    },
    {
        "text": "add cp hook before initialize variable set to true in master intialization  this hook helps in following cases.  1) when we are creating indexed table then there is a chance that master can go down after successful creation of user table but index table creation not yet started.  this hook helps to find such cases and create missing index table.  2) if any case there are mismatches in colocation of user and index regions we can run balancer. ",
        "label": 543
    },
    {
        "text": "provide a command or argument to startup  that formats znodes if provided  many a times i've had to, and have seen instructions being thrown, to stop cluster, clear out zk and restart. while this is only a quick (and painful to master) fix, it is certainly nifty to some smaller cluster users but the process is far too long, roughly: 1. stop hbase  2. start zkcli.sh and connect to the right quorum  3. find and ensure the hbase parent znode from the configs (/hbase only by default)  4. run an \"rmr /hbase\" in the zkcli.sh shell, or manually delete each znode if on a lower version of zk.  5. quit zkcli.sh and start hbase again perhaps it may be useful, if the start-hbase.sh itself accepted a formatzk parameter. such that, when you do a start-hbase.sh -formatzk, it does steps 2-4 automatically for you. for safety, we could make the formatter code ensure that no hbase instance is actually active, and skip the format process if it is. similar to a hdfs namenode's format, which would disallow if the name directories are locked. would this be a useful addition for administrators? bigtop too can provide a service subcommand that could do this. ",
        "label": 543
    },
    {
        "text": "fix type in documentation of pseudo distributed mode  see http://hbase.apache.org/pseudo-distributed.html, it has \"psuedo\" as opposed to \"pseudo\" - trivial fix. ",
        "label": 314
    },
    {
        "text": "nullpointerexception thrown when adding rows to a table from peer cluster  table with replication factor other than or  scenario:  =============  add_peer  create a table  alter table with replication_scope => '5'  enable table replication  login to peer cluster and try putting data to the table ",
        "label": 391
    },
    {
        "text": "hbase may lose edits after a crash if used with hdfs or older  this comes from a hdfs bug, fixed in some hdfs versions. i haven't found the hdfs jira for this. context: hbase write ahead log features. this is using hdfs append. if the node crashes, the file that was written is read by other processes to replay the action. so we have in hdfs one (dead) process writing with another process reading. but, despite the call to syncfs, we don't always see the data when we have a dead node. it seems to be because the call in dfsclient#updateblockinfo ignores the ipc errors and set the length to 0. so we may miss all the writes to the last block if we try to connect to the dead dn. hdfs 1.0.3, branch-1 or branch-1-win: we have the issue  http://svn.apache.org/viewvc/hadoop/common/branches/branch-1/src/hdfs/org/apache/hadoop/hdfs/dfsclient.java?revision=1359853&view=markup hdfs branch-2 or trunk: we should not have the issue (but not tested)  http://svn.apache.org/viewvc/hadoop/common/branches/branch-2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/dfsinputstream.java?view=markup the attached test will fail ~50 of the time. ",
        "label": 340
    },
    {
        "text": "deprecate admin's methods which used string regex  ",
        "label": 187
    },
    {
        "text": "the result from the postappend is ignored  regioncoprocessorhost.java   /**    * @param append append object    * @param result the result returned by the append    * @throws ioexception if an error occurred on the coprocessor    */   public void postappend(final append append, final result result) throws ioexception {     execoperation(coprocessors.isempty() ? null : new regionoperation() {       @override       public void call(regionobserver oserver, observercontext<regioncoprocessorenvironment> ctx)           throws ioexception {         oserver.postappend(ctx, append, result);       }     });   }   /**    * @param increment increment object    * @param result the result returned by postincrement    * @throws ioexception if an error occurred on the coprocessor    */   public result postincrement(final increment increment, result result) throws ioexception {     return execoperationwithresult(result,         coprocessors.isempty() ? null : new regionoperationwithresult<result>() {       @override       public void call(regionobserver oserver, observercontext<regioncoprocessorenvironment> ctx)           throws ioexception {         setresult(oserver.postincrement(ctx, increment, getresult()));       }     });   } ",
        "label": 98
    },
    {
        "text": "precommit on branch isn't supposed to run against hadoop  hadoop 3 doesn't work with hbase 1.y and we haven't done any work to backport the efforts to make hbase 2.y work with it. precommit shouldn't tell contributors otherwise. see hbase-18923 for an example of a branch-1 patch that had hadoop 3 compilation checked. ",
        "label": 402
    },
    {
        "text": "update filter test to ue columnvaluefilter instead of regexprowfilter  filtering by column value in regexprowfilter has been deprecated in favor of columnvaluefilter. filter tests should be modified to use columnvaluefilter. ",
        "label": 110
    },
    {
        "text": "asyncadmin will not refresh master address  this is a big problem... if we stop the active master and promote the backup master, all methods in asyncadmin, which need to connect the master will fail as they try to connect to the old active master forever... ",
        "label": 149
    },
    {
        "text": "currently the tail of hfiles with cellcomparator  classname makes it so hbase1 can't open hbase2 written hfiles  fix  see tail of hbase-19052 for discussion which concludes we should try and make it so operators do not have to go to latest hbase version before they upgrade, at least if we can avoid it. the necessary change of our default comparator from kv to cell naming has hfiles with tails that have the classname cellcomparator in them in place of keyvaluecomparator. if an hbase1 tries to open them, it will fail not having a cellcomparator in its classpath (we have name of comparator in tail because different files require different comparators... perhaps we write an alias instead of a class one day... todo). hbase-16189 and hbase-19052 are about trying to carry knowledge of hbase2 back to hbase1, a brittle approach making it so operators will have to upgrade to the latest branch-1 before they can go to hbase2. this issue is about undoing our writing of an incompatible (to hbase1) tail, not unless we really have to (and it sounds like we could do without writing an incompatible tail) to see if we can avoid requiring operators go to lastest branch-1 (we may end up needing this but lets a have a really good reason for it if we do). oh, let this filing be an answer to our anoop sam john's old high-level question over in hbase-16189: ...means when rolling upgrade done to 2.0, first users have to upgrade to some 1.x versions which is having this fix and then to 2.0.. what do you guys think whether we should avoid this kind of indirection? cc enis soztutar, stack, ted yu, matteo bertozzi yeah, lets try to avoid this if we can... ",
        "label": 314
    },
    {
        "text": " fb  hbaseadmin has ambiguous varargs invocations  ambiguous invocations of varargs methods with non-varargs arguments relied on the compiler to implicitly cast the arguments to object[]. some compilers apparently do not make this implicit cast, but instead wrap the arguments in another object[] causing them to be interpreted incorrectly. ",
        "label": 105
    },
    {
        "text": "change default ports  move them out of linux ephemeral port range  our defaults clash w/ the range linux assigns itself for creating come-and-go ephemeral ports; likely in our history we've clashed w/ a random, short-lived process. while easy to change the defaults, we should just ship w/ defaults that make sense. we could host ourselves up into the 7 or 8k range. see http://www.ncftp.com/ncftpd/doc/misc/ephemeral_ports.html ",
        "label": 248
    },
    {
        "text": "potential race condition between fsutils renameandsetmodifytime  and hfile logcleaner  we recently saw the following running 0.95.1 on hadoop 2.0: 2013-06-11 21:51:22,199 error [ipc server handler 18 on 60000] master.hmaster: region server ip-10-138-2-28.ec2.internal,60020,1370887927492 reported a fatal error: aborting region server ip-10-138-2-28.ec2.internal,60020,1370887927492: ioe in log roller cause: java.io.filenotfoundexception: file/directory /hbase/.oldlogs/ip-10-138-2-28.ec2.internal%2c60020%2c1370887927492.1370986265468 does not exist. at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.settimesint(fsnamesystem.java:1488) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.settimes(fsnamesystem.java:1453) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.settimes(namenoderpcserver.java:798) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.settimes(clientnamenodeprotocolserversidetranslatorpb.java:704) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java:43194) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:454) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:910) at org.apache.hadoop.ipc.server$handler$1.run(server.java:1694) at org.apache.hadoop.ipc.server$handler$1.run(server.java:1690) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:415) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1367) at org.apache.hadoop.ipc.server$handler.run(server.java:1688) at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:525) at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:90) at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:57) at org.apache.hadoop.hdfs.dfsclient.settimes(dfsclient.java:2097) at org.apache.hadoop.hdfs.distributedfilesystem.settimes(distributedfilesystem.java:813) at org.apache.hadoop.hbase.util.fsutils.renameandsetmodifytime(fsutils.java:1596) at org.apache.hadoop.hbase.regionserver.wal.fshlog.archivelogfile(fshlog.java:705) at org.apache.hadoop.hbase.regionserver.wal.fshlog.cleanoldlogs(fshlog.java:595) at org.apache.hadoop.hbase.regionserver.wal.fshlog.rollwriter(fshlog.java:536) at org.apache.hadoop.hbase.regionserver.logroller.run(logroller.java:96) at java.lang.thread.run(thread.java:722) caused by: org.apache.hadoop.ipc.remoteexception(java.io.filenotfoundexception): file/directory /hbase/.oldlogs/ip-10-138-2-28.ec2.internal%2c60020%2c1370887927492.1370986265468 does not exist. at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.settimesint(fsnamesystem.java:1488) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.settimes(fsnamesystem.java:1453) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.settimes(namenoderpcserver.java:798) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.settimes(clientnamenodeprotocolserversidetranslatorpb.java:704) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java:43194) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:454) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:910) at org.apache.hadoop.ipc.server$handler$1.run(server.java:1694) at org.apache.hadoop.ipc.server$handler$1.run(server.java:1690) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:415) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1367) at org.apache.hadoop.ipc.server$handler.run(server.java:1688) at org.apache.hadoop.ipc.client.call(client.java:1164) at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:202) at com.sun.proxy.$proxy11.settimes(unknown source) at sun.reflect.generatedmethodaccessor27.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:601) at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:164) at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:83) at com.sun.proxy.$proxy11.settimes(unknown source) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocoltranslatorpb.settimes(clientnamenodeprotocoltranslatorpb.java:685) at sun.reflect.generatedmethodaccessor26.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:601) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:266) at com.sun.proxy.$proxy12.settimes(unknown source) at org.apache.hadoop.hdfs.dfsclient.settimes(dfsclient.java:2095) ... 7 more 2013-06-11 21:51:32,976 info  [main-eventthread] zookeeper.regionservertracker: regionserver ephemeral node deleted, processing expiration [ip-10-138-2-28.ec2.internal,60020,1370887927492] one scenario that could have led to the above exception was that the logcleaner thread deleted the log file which had just been moved to .oldlogs dir before the call to fs.settimes() executed.  when fs.settimes() ran, the file was no longer there. ",
        "label": 309
    },
    {
        "text": "quick edit of 'getting started' for development release x  hack on the javadoc overview to remove egregious stuff like recommended hdfs patches and to make mention of difference between plain hadoop 0.20 and 0.20-append. ",
        "label": 314
    },
    {
        "text": "backup master does not become active master after zk exception  -> active master gets zk expiry exception.  -> backup master becomes active.  -> the previous active master retries and becomes the back up master.  now when the new active master goes down and the current back up master comes up, it goes down again with the zk expiry exception it got in the first step. if (abortnow(msg, t)) {       if (t != null) log.fatal(msg, t);       else log.fatal(msg);       this.abort = true;       stop(\"aborting\");     } in activemastermanager.blockuntilbecomingactivemaster we try to wait till the back up master becomes active.     synchronized (this.clusterhasactivemaster) {       while (this.clusterhasactivemaster.get() && !this.master.isstopped()) {         try {           this.clusterhasactivemaster.wait();         } catch (interruptedexception e) {           // we expect to be interrupted when a master dies, will fall out if so           log.debug(\"interrupted waiting for master to die\", e);         }       }       if (!clusterstatustracker.isclusterup()) {         this.master.stop(\"cluster went down before this master became active\");       }       if (this.master.isstopped()) {         return cleansetofactivemaster;       }       // try to become active master again now that there is no active master       blockuntilbecomingactivemaster(startupstatus,clusterstatustracker);     }     return cleansetofactivemaster; when the back up master (it is in back up mode as he got zk exception), once again tries to come to active we don't get the return value that comes out from // try to become active master again now that there is no active master       blockuntilbecomingactivemaster(startupstatus,clusterstatustracker); we tend to return the 'cleansetofactivemaster' which was previously false.  now because of this instead of again becoming active the back up master goes down in the abort() code. thanks to gopi,my colleague for reporting this issue. ",
        "label": 544
    },
    {
        "text": "add a filter that randomly included rows based on a configured chance   we needed a filter that allows us to randomly include rows. i will attach a patch with a simple filter that is able to do this. for example, if you wish to do a scan but only want 1/4 of all values (randomly distributed), you can use: new randomrowfilter(0.25f) i just wanted to share this filter with you. it's up to you if you want to include it in the codebase. ",
        "label": 171
    },
    {
        "text": "move the unsupported procedure type check before migrating to regionprocedurestore  per discussion on the list, let's figure out how to make the upgrade from a v2.1 procedure store less error-prone. could be a simple as documenting runbook steps to execute during the rolling upgrade, but it would be nice if the software could roll over the data versions gracefully. ",
        "label": 149
    },
    {
        "text": "fix documentation around minor compaction and ttl  minor compactions should be able to delete keyvalues outside of ttl. the docs currently suggest otherwise. we should bring them in line. ",
        "label": 305
    },
    {
        "text": "move the hregion addregiontometa to testdefaultmemstore  the usages of addregiontometa are almost removed by hbase-17470. hregion#addregiontometa is used only by testdefaultmemstore now. let us reduce the file size of hregion. ",
        "label": 106
    },
    {
        "text": "document bandwidth consumption limit feature for exportsnapshot  http://hbase.apache.org/book.html#ops.snapshots.export should document bandwidth consumption limit feature which is implemented by hbase-11083 and hbase-11090 ",
        "label": 330
    },
    {
        "text": "set version as snapshot in branch  ",
        "label": 149
    },
    {
        "text": "make filterlist accept var arg filters in its constructor as a convenience  when using a small number of filters for a filterlist, it's cleaner to use var args rather than forcing a list on the client. compare: scan.setfilter(new filterlist(filterlist.operator.must_pass_all, new firstkeyonlyfilter(), new keyonlyfilter())); vs: list<filter> filters = new arraylist<filter>(2);  filters.add(new filrstkeyonlyfilter());  filters.add(new keyonlyfilter());  scan.setfilter(new filterlist(filterlist.operator.must_pass_all, filters); ",
        "label": 162
    },
    {
        "text": "htable doput list  should check the writebuffer length every so often  this came up on a dist-list conversation between andy p., ted yu, and myself. andy noted that extremely large lists passed into put(list) can cause issues. ted suggested that having doput check the write-buffer length every so often (5-10 records?) so the flush doesn't happen only at the end, and i think that's good idea.  public void put(final list<put> puts) throws ioexception { doput(puts); } private void doput(final list<put> puts) throws ioexception {  for (put put : puts) { validateput(put); writebuffer.add(put); currentwritebuffersize += put.heapsize(); } if (autoflush || currentwritebuffersize > writebuffersize) { flushcommits(); } } once this change is made, remove the comment in hbase-4142 about large lists being a performance problem. ",
        "label": 146
    },
    {
        "text": "rest interface  enable to get numbers of rows from scanner interface  the scanner request will return one row for each request defaultly. but due to reduce the network bandwidth also the usability of the api, it should allow return multiple rows at once. ",
        "label": 549
    },
    {
        "text": "zk tick time bounds maximum zk session time  default session is set to 60 seconds but ticktime is 2 seconds. http://hadoop.apache.org/zookeeper/docs/current/zookeeperprogrammers.html#ch_zksessions \"one of the parameters to the zookeeper client library call to create a zookeeper session is the session timeout in milliseconds. the client sends a requested timeout, the server responds with the timeout that it can give the client. the current implementation requires that the timeout be a minimum of 2 times the ticktime (as set in the server configuration) and a maximum of 20 times the ticktime.\" so, max is actually 40 seconds. ",
        "label": 314
    },
    {
        "text": "update documentation on unit tests  points to address: we don't have anymore junit rules in the tests we should document how to run the test faster. some stuff is not used (run only a category) and should be removed from the doc imho. below the proposal: \u2013 15.6.2. unit tests hbase unit tests are subdivided into three categories: small, medium and large, with corresponding junit categories: smalltests, mediumtests, largetests. junit categories are denoted using java annotations and look like this in your unit test code. ...  @category(smalltests.class)  public class testhregioninfo {  @test  public void testcreatehregioninfoname() throws exception { // ... } } the above example shows how to mark a test as belonging to the small category. hbase uses a patched maven surefire plugin and maven profiles to implement its unit test characterizations. 15.6.2.4. running tests below we describe how to run the hbase junit categories.  15.6.2.4.1. default: small and medium category tests running mvn test will execute all small tests in a single jvm (no fork) and then medium tests in a separate jvm for each test instance. medium tests are not executed if there is an error in a small test. large tests are not executed. there is one report for small tests, and one report for medium tests if they are executed.  15.6.2.4.2. running all tests running mvn test -p runalltests will execute small tests in a single jvm then medium and large tests in a separate jvm for each test. medium and large tests are not executed if there is an error in a small test. large tests are not executed if there is an error in a small or medium test. there is one report for small tests, and one report for medium and large tests if they are executed 15.6.2.4.3. running a single test or all tests in a package to run an individual test, e.g. mytest, do mvn test -p localtests -dtest=mytest you can also pass multiple, individual tests as a comma-delimited list: mvn test -p localtests -dtest=mytest1,mytest2,mytest3 you can also pass a package, which will run all tests under the package: mvn test -p localtests -dtest=org.apache.hadoop.hbase.client.* the -p localtests will remove the junit category effect (without this specific profile, the categories are taken into account). each junit tests is executed in a separate jvm (a fork per test class). there is no parallelization when localtests profile is set. you will see a new message at the end of the report: \"[info] tests are skipped\". it's harmless. 15.6.2.4.4. running test faster  [replace previous chapter] by default, mvn test -p runalltests runs 5 tests in parallel. it can be increased for many developper machine. consider that you can have 2 tests in parallel per core, and you need about 2gb of memory per test. hence, if you have a 8 cores and 24gb box, you can have 16 tests in parallel. the setting is:  mvn test -p runalltests -dsurefire.secondpartthreadcount=12 to increase the speed, you can as well use a ramdisk. you will need 2gb of memory to run all the test. you will also need to delete the files between two test run.  the typical way to configure a ramdisk on linux is: sudo mkdir /ram2g  sudo mount -t tmpfs -o size=2048m tmpfs /ram2g you can then use it to run all hbase tests with the command: mvn test -p runalltests -dsurefire.secondpartthreadcount=8 -dtest.build.data.basedirectory=/ram2g ",
        "label": 340
    },
    {
        "text": "reimplement getmasterinfoport for admin  current implementation.   public int getmasterinfoport() throws ioexception {     // todo: fix!  reaching into internal implementation!!!!     connectionimplementation connection = (connectionimplementation)this.connection;     zookeeperkeepaliveconnection zkw = connection.getkeepalivezookeeperwatcher();     try {       return masteraddresstracker.getmasterinfoport(zkw);     } catch (keeperexception e) {       throw new ioexception(\"failed to get master info port from masteraddresstracker\", e);     }   } open this issue to fix todo. ",
        "label": 187
    },
    {
        "text": "memcachescanner didn't return the first row if it exists  cause hscannerinterface's output incorrect  htable.obtainscanner methods should return the start row if it exists, although htable's javadoc didn't clearly desc. but i found the result of htable scanners sometimes contain the start row, sometimes not. after more testing and code review, i found it should be a bug in hstore.memcache.memcachescanner. in the constructor it set this.currentrow = firstrow, but when doing next(), there's a this.currentrow = getnextrow(this.currentrow) before fetch result. ",
        "label": 241
    },
    {
        "text": "failed to start hmaster due to infinite retrying on meta assign  this is what i got at first, an exception when trying to write something to meta when meta has not been onlined yet. 2018-01-07,21:03:14,389 info org.apache.hadoop.hbase.master.hmaster: running recovermetaprocedure to ensure proper hbase:meta deploy. 2018-01-07,21:03:14,637 info org.apache.hadoop.hbase.master.procedure.recovermetaprocedure: start pid=1, state=runnable:recover_meta_split_logs; recovermetaprocedure failedmetaserver=null, splitwal=true 2018-01-07,21:03:14,645 info org.apache.hadoop.hbase.master.masterwalmanager: log folder hdfs://c402tst-community/hbase/c402tst-community/wals/c4-hadoop-tst-st27.bj,38900,1515330173896 belongs to an existing region server 2018-01-07,21:03:14,646 info org.apache.hadoop.hbase.master.masterwalmanager: log folder hdfs://c402tst-community/hbase/c402tst-community/wals/c4-hadoop-tst-st29.bj,38900,1515330177232 belongs to an existing region server 2018-01-07,21:03:14,648 info org.apache.hadoop.hbase.master.procedure.recovermetaprocedure: pid=1, state=runnable:recover_meta_assign_regions; recovermetaprocedure failedmetaserver=null, splitwal=true; retaining meta assignment to server=null 2018-01-07,21:03:14,653 info org.apache.hadoop.hbase.procedure2.procedureexecutor: initialized subprocedures=[{pid=2, ppid=1, state=runnable:region_transition_queue; assignprocedure table=hbase:meta, region=1588230740}] 2018-01-07,21:03:14,660 info org.apache.hadoop.hbase.master.procedure.masterprocedurescheduler: pid=2, ppid=1, state=runnable:region_transition_queue; assignprocedure table=hbase:meta, region=1588230740 hbase:meta hbase:meta,,1.1588230740 2018-01-07,21:03:14,663 info org.apache.hadoop.hbase.master.assignment.assignprocedure: start pid=2, ppid=1, state=runnable:region_transition_queue; assignprocedure table=hbase:meta, region=1588230740; rit=offline, location=null; forcenewplan=false, retain=false 2018-01-07,21:03:14,831 info org.apache.hadoop.hbase.zookeeper.metatablelocator: setting hbase:meta (replicaid=0) location in zookeeper as c4-hadoop-tst-st27.bj,38900,1515330173896 2018-01-07,21:03:14,841 info org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure: dispatch pid=2, ppid=1, state=runnable:region_transition_dispatch; assignprocedure table=hbase:meta, region=1588230740; rit=opening, location=c4-hadoop-tst-st27.bj,38900,1515330173896 2018-01-07,21:03:14,992 info org.apache.hadoop.hbase.master.procedure.rsproceduredispatcher: using procedure batch rpc execution for servername=c4-hadoop-tst-st27.bj,38900,1515330173896 version=3145728 2018-01-07,21:03:15,593 error org.apache.hadoop.hbase.client.asyncrequestfutureimpl: cannot get replica 0 location for {\"totalcolumns\":1,\"row\":\"hbase:meta\",\"families\":{\"table\":[{\"qualifier\":\"state\",\"vlen\":2,\"tag\":[],\"timestamp\":1515330195514}]},\"ts\":1515330195514} 2018-01-07,21:03:15,594 warn org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure: retryable error trying to transition: pid=2, ppid=1, state=runnable:region_transition_finish; assignprocedure table=hbase:meta, region=1588230740; rit=open, location=c4-hadoop-tst-st27.bj,38900,1515330173896 org.apache.hadoop.hbase.client.retriesexhaustedwithdetailsexception: failed 1 action: ioexception: 1 time, servers with issues: null         at org.apache.hadoop.hbase.client.batcherrors.makeexception(batcherrors.java:54)         at org.apache.hadoop.hbase.client.asyncrequestfutureimpl.geterrors(asyncrequestfutureimpl.java:1250)         at org.apache.hadoop.hbase.client.htable.batch(htable.java:457)         at org.apache.hadoop.hbase.client.htable.put(htable.java:570)         at org.apache.hadoop.hbase.metatableaccessor.put(metatableaccessor.java:1450)         at org.apache.hadoop.hbase.metatableaccessor.puttometatable(metatableaccessor.java:1439)         at org.apache.hadoop.hbase.metatableaccessor.updatetablestate(metatableaccessor.java:1785)         at org.apache.hadoop.hbase.metatableaccessor.updatetablestate(metatableaccessor.java:1151)         at org.apache.hadoop.hbase.master.tablestatemanager.udpatemetastate(tablestatemanager.java:183)         at org.apache.hadoop.hbase.master.tablestatemanager.settablestate(tablestatemanager.java:69)         at org.apache.hadoop.hbase.master.assignment.assignmentmanager.markregionasopened(assignmentmanager.java:1515)         at org.apache.hadoop.hbase.master.assignment.assignprocedure.finishtransition(assignprocedure.java:271)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:320)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:86)         at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:845)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1456)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1225)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$800(procedureexecutor.java:78)         at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1735) and then i got repeated exception like this infinitely 2018-01-07,21:03:15,596 warn org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure: retryable error trying to transition: pid=2, ppid=1, state=runnable:region_transition_finish; assignprocedure table=hbase:meta, region=1588230740; rit=open, location=c4-hadoop-tst-st27.bj,38900,1515330173896 org.apache.hadoop.hbase.exceptions.unexpectedstateexception: expected [offline, closed, splitting, split, opening, failed_open] so could move to open but current state=open         at org.apache.hadoop.hbase.master.assignment.regionstates$regionstatenode.transitionstate(regionstates.java:155)         at org.apache.hadoop.hbase.master.assignment.assignmentmanager.markregionasopened(assignmentmanager.java:1513)         at org.apache.hadoop.hbase.master.assignment.assignprocedure.finishtransition(assignprocedure.java:271)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:320)         at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:86)         at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:845)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1456)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1225)         at org.apache.hadoop.hbase.procedure2.procedureexecutor.access$800(procedureexecutor.java:78)         at org.apache.hadoop.hbase.procedure2.procedureexecutor$workerthread.run(procedureexecutor.java:1735) this is a bit strange. since we are assigning meta, why we need to write the state to meta table? i checked the code a bit.  in assignprocedure.finishtransition, we will do this    env.getassignmentmanager().markregionasopened(regionnode); and in assignmentmanager.markregionasopened, we will do this       if (ismetaregion(hri)) {         master.gettablestatemanager().settablestate(tablename.meta_table_name,             tablestate.state.enabled);         setmetainitialized(hri, true);       } and in tablestatemanager.settablestate, we will call udpatemetastate(a typo...) to write something to meta. i think this will lead to a dead lock? i do not think we need to put the state of meta table to meta table? it is always enabled... but i do not know why it worked when i tried to restart the cluster... maybe we do not enter this code path for a non-fresh cluster? ",
        "label": 314
    },
    {
        "text": "when a rs has hostname with uppercase letter  there are two rs entries in master  when a rs has uppercase letter in its hostname, e.g. harpertown08-15.sh.intel.com. then, there will be two rs entries in master report, they are  harpertown08-15.sh.intel.com  harpertown08-15.sh.intel.com this leads to wrong region allocation. this problem is caused by the implementation of java.net.inetsocketaddress. the logic is:  1. rs harpertown08-15.sh.intel.com sends its dns resolved hostname harpertown08-15.sh.intel.com to master for registration in hregionserver.reportforduty().  2. master handles it and returns harpertown08-15.sh.intel.com via hserveraddress object to rs to notify it this new name (actually the same as the one reported by rs).  3. hserveraddress deserialize this object by first read out hostname as string (\"harpertown08-15.sh.intel.com\") and construct a inetsocketaddress object.  4. rs get the new name by call this inetsocketaddress.gethostname() method which returns harpertown08-15.sh.intel.com instead.  5. in latter hearbeat communication (hregionserver.tryregionserverreport()), rs uses this new name (harpertown08-15.sh.intel.com) to report to master and master regard it as a new rs. thus, two rs entries exist. ",
        "label": 484
    },
    {
        "text": "upgrade jetty  jetty can be updated to 9.4.6 and thrift can be updated to 0.10.0. i tried to update them in hbase-17898 but some unit tests failed, so created a sub-task for them. ",
        "label": 314
    },
    {
        "text": "make clustermanager in integrationtestingutility pluggable  after the patch hbase-7009, we can use chaosmonkey to test the hbase cluster.  the clustermanager use ssh to stop/start the rs or master without passwd. to support other cluster manager tool, we need to make clustermanager in integrationtestingutility pluggable. ",
        "label": 411
    },
    {
        "text": "assign region doesn't check if the region is already assigned  tried to assign a region already assigned somewhere from hbase shell, the region is assigned to a different place but the previous assignment is not closed. so it causes double assignments. in such a case, it's better to issue a warning instead. ",
        "label": 242
    },
    {
        "text": "a couple of callqueue related improvements  in one of my in-memory read only testing(100% get requests), one of the top scalibility bottleneck came from the single callqueue. a tentative sharing this callqueue according to the rpc handler number showed a big throughput improvement(the original get() qps is around 60k, after this one and other hotspot tunning, i got 220k get() qps in the same single region server) in a ycsb read only scenario.  another stuff we can do is seperating the queue into read call queue and write call queue, we had done it in our internal branch, it would helpful in some outages, to avoid all read or all write requests ran out of all handler threads.  one more stuff is changing the current blocking behevior once the callqueue is full, considering the full callqueue almost means the backend processing is slow somehow, so a fail-fast here should be more reasonable if we using hbase as a low latency processing system. see \"callqueue.put(call)\" ",
        "label": 309
    },
    {
        "text": "update hbase default xml and general recommendations to better suit current hw  h2  experience  etc   this is a critical task we need to do before we release; review our defaults. on cursory review, there are configs in hbase-default.xml that no longer have matching code; there are some that should be changed because we know better now and others that should be amended because hardware and deploys are bigger than they used to be. we could also move stuff around so that the must-edit stuff is near the top (zk quorum config. is mid-way down the page) and beef up the descriptions \u2013 especially since these descriptions shine through in the refguide. lastly, i notice that our tgz does not \"include\" an hbase-default.xml other than the one bundled up in the jar. maybe we should make it more accessible. ",
        "label": 314
    },
    {
        "text": "wal corruption due to early dbbs re use when durability async wal is used  summary we had been chasing a wal corruption issue reported on one of our customers deployments running release 2.1.1 (cdh 6.1.0). after providing a custom modified jar with the extra sanity checks implemented by hbase-21401 applied on some code points, plus additional debugging messages, we believe it is related to directbytebuffer usage, and unsafe copy from offheap memory to on-heap array triggered here, such as when writing into a non bytebufferwriter type, as done here. more details on the following comment.   ",
        "label": 149
    },
    {
        "text": "cleanup the docs saying  htable use write buffer   cleanup the docs saying \"htable use write buffer\" default size of the htable client write buffer in bytes. a bigger buffer takes more memory \u2014 on both the client and server side since server instantiates the passed write buffer to process it \u2014 but a larger buffer size reduces the number of rpcs made. for an estimate of server-side memory-used, evaluate hbase.client.write.buffer * hbase.regionserver.handler.count put either adds new rows to a table (if the key is new) or can update existing rows (if the key already exists). puts are executed via table.put (writebuffer) or table.batch (non-writebuffer). ",
        "label": 98
    },
    {
        "text": "site build fails after hbase thirdparty upgrade  after hbase-thirdparty upgrade the hbase_generate_website job is failing in mvn site target on javadoc.   [error] failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.7.1:site (default-site) on project hbase: error generating maven-javadoc-plugin:3.0.1:aggregate report: [error] exit code: 1 - /home/jenkins/jenkins-slave/workspace/hbase_generate_website/hbase/hbase-common/src/main/java/org/apache/hadoop/hbase/cellutil.java:1034: warning - tag @link: can't find tagsiterator(cell) in org.apache.hadoop.hbase.cellutil [error] javadoc: error - class file for org.apache.hbase.thirdparty.com.google.errorprone.annotations.immutable not found after reverting thirdparty upgrade locally the site build passed.  ",
        "label": 149
    },
    {
        "text": "small improvement in kvheap seek  api  currently in seek/reseek() apis we tend to do lot of priorityqueue related operations. we initially add the current scanner to the heap, then poll and again add the scanner back if the seekkey is greater than the topkey in that scanner. since the kvs are always going to be in increasing order and in ideal scan flow every seek/reseek is followed by a next() call it should be ok if we start with checking the current scanner and then do a poll to get the next scanner. just avoid the initial pq.add(current) call. this could save some comparisons. ",
        "label": 544
    },
    {
        "text": "include set up instructions for building with ides besides eclipse  we have pretty detailed instructions for setting up eclipse to build hbase at http://hbase.apache.org/book/ides.html#eclipse. it would be nice to have this for other popular ides such as intellij. ",
        "label": 330
    },
    {
        "text": "npe from lrudictionary when size reaches the max init value  this happened while testing tags with compress_tag=true/false. i was trying to change this attribute of compressing tags by altering the hcd. the dbe used is fast_diff.   in one particular case i got this 2014-01-29 16:20:03,023 error [regionserver60020-smallcompactions-1390983591688] regionserver.compactsplitthread: compaction failed request = regionname=usertable,user5146961419203824653,1390979618897.2dd477d0aed888c615a29356c0bbb19d., storename=f1, filecount=4, filesize=498.6 m (226.0 m, 163.7 m, 67.0 m, 41.8 m), priority=6, time=1994941280334574 java.lang.nullpointerexception         at org.apache.hadoop.hbase.io.util.lrudictionary$bidirectionallrumap.put(lrudictionary.java:109)         at org.apache.hadoop.hbase.io.util.lrudictionary$bidirectionallrumap.access$200(lrudictionary.java:76)         at org.apache.hadoop.hbase.io.util.lrudictionary.addentry(lrudictionary.java:62)         at org.apache.hadoop.hbase.io.tagcompressioncontext.uncompresstags(tagcompressioncontext.java:147)         at org.apache.hadoop.hbase.io.encoding.buffereddatablockencoder$bufferedencodedseeker.decodetags(buffereddatablockencoder.java:270)         at org.apache.hadoop.hbase.io.encoding.fastdiffdeltaencoder$1.decode(fastdiffdeltaencoder.java:522)         at org.apache.hadoop.hbase.io.encoding.fastdiffdeltaencoder$1.decodefirst(fastdiffdeltaencoder.java:535)         at org.apache.hadoop.hbase.io.encoding.buffereddatablockencoder$bufferedencodedseeker.setcurrentbuffer(buffereddatablockencoder.java:188)         at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$encodedscannerv2.updatecurrentblock(hfilereaderv2.java:1017)         at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$encodedscannerv2.next(hfilereaderv2.java:1068)         at org.apache.hadoop.hbase.regionserver.storefilescanner.next(storefilescanner.java:137)         at org.apache.hadoop.hbase.regionserver.keyvalueheap.next(keyvalueheap.java:108)         at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:509)         at org.apache.hadoop.hbase.regionserver.compactions.compactor.performcompaction(compactor.java:217)         at org.apache.hadoop.hbase.regionserver.compactions.defaultcompactor.compact(defaultcompactor.java:76)         at org.apache.hadoop.hbase.regionserver.defaultstoreengine$defaultcompactioncontext.compact(defaultstoreengine.java:109)         at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:1074)         at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1382)         at org.apache.hadoop.hbase.regionserver.compactsplitthread$compactionrunner.run(compactsplitthread.java:475)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) i am not able to reproduce this repeatedly. one thing is i altered the table to use compress_tags here before that it was false.   my feeling is this is not due to the compress_tags because we try to handle this per file by adding it in file_info.   in the above stack trace the problem has occured while compaction and so the flushed file should have this property set. i think the problem could be with lrudicitonary.  the reason for npe is  if (currsize < initsize) {         // there is space to add without evicting.         indextonode[currsize].setcontents(stored, 0, stored.length);         sethead(indextonode[currsize]);         short ret = (short) currsize++;         nodetoindex.put(indextonode[ret], ret);         system.out.println(currsize);         return ret;       } else {         short s = nodetoindex.remove(tail);         tail.setcontents(stored, 0, stored.length);         // we need to rehash this.         nodetoindex.put(tail, s);         movetohead(tail);         return s;       } here short s = nodetoindex.remove(tail); is giving a null value and the typecasting to short primitive is throwing npe. am digging this further to see if am able to reproduce this. ",
        "label": 46
    },
    {
        "text": "testwalprocedurestore testload is flakey  i see it twice recently,   see.  https://builds.apache.org/job/precommit-hbase-build/16589//testreport/org.apache.hadoop.hbase.procedure2.store.wal/testwalprocedurestore/testload/ https://builds.apache.org/job/precommit-hbase-build/16532/testreport/org.apache.hadoop.hbase.procedure2.store.wal/testwalprocedurestore/testload/ let's see what's happening. update.  it failed once again today,   https://builds.apache.org/job/precommit-hbase-build/16602/testreport/junit/org.apache.hadoop.hbase.procedure2.store.wal/testwalprocedurestore/testload/ ",
        "label": 309
    },
    {
        "text": "two situations where we could open a region with smaller sequence number  recently i happened to run into code where we potentially could open region with smaller sequence number: 1) inside function: hregion#internalflushcache. this is due to we change the way wal sync where we use late binding(assign sequence number right before wal sync).  the flushseqid may less than the change sequence number included in the flush which may cause later region opening code to use a smaller than expected sequence number when we reopen the region. flushseqid = this.sequenceid.incrementandget(); ... mvcc.waitforread(w); 2) hregion#replayrecoverededits where we have following code: ...           if (coprocessorhost != null) {             status.setstatus(\"running pre-wal-restore hook in coprocessors\");             if (coprocessorhost.prewalrestore(this.getregioninfo(), key, val)) {               // if bypass this log entry, ignore it ...               continue;             }           } ...           currenteditseqid = key.getlogseqnum(); if coprocessor skip some tail waledits, then the function will return smaller currenteditseqid. in the end, a region may also open with a smaller sequence number. this may cause data loss because master may record a larger flushed sequence id and some waledits maybe skipped during recovery if the region fail again. ",
        "label": 426
    },
    {
        "text": "move classes into hbase client  ",
        "label": 154
    },
    {
        "text": "remove testvisibilitylabelswithdistributedlogreplay  a test for an unsupported feature  remove tests that do dlr. i saw one hang over on branch 0.98 test just now. ",
        "label": 314
    },
    {
        "text": "zkutil createwithparents won't set data during znode creation when parent folder doesn't exit  as you can see below, we don't pass data down when parent folder doesn't exists.     } catch(keeperexception.nonodeexception nne) {       createwithparents(zkw, getparent(znode));       createwithparents(zkw, znode); ",
        "label": 233
    },
    {
        "text": "testschemaresource broken on trunk up on hudson  ",
        "label": 314
    },
    {
        "text": "add getqualifierbuffershallowcopy to cell utility   right now if your qualifier is large and you need to access it, copying is your only option. we should allow a shallow buffer copy. ",
        "label": 154
    },
    {
        "text": "pooledhtable may be returned multiple times to the same pool  i have recently observed a very strange issue with an application using hbase and htablepool. after an investigation i have found that the root cause was the piece of code that was calling close() twice on the same htableinterface instance retrieved from htablepool (created with default policy). a closer look at the code revealed that pooledhtable.close() calls returntable(), which, in turn, places the table back into the queue of the pooled tables. no checking of any kind is done so it is possible to call it multiple times and place multiple references to the same htable into the same pool. this creates a number of negative effects: pool grows on each close() call and eventually gets filled up with the references to the same htable. from this moment the pool stops working as pool. multiple callers will get the same instance of htable while expecting to have unique instances once the pool is full, next call to close() will result to the call to the real close() method of htable. this will make htable unusable as close() call may shutdown() the internal thread pool. from this moment other attempts to use this htable will fail with rejectedexecutionexception. and since the htablepool will have additional references to that htable, other users of the pool will just start failing on any call that leads to flushcommits() the problem was, obviously, triggered by bad code on our side. but i think the pool has to be protected. probably the best way to fix it would be to implement a flag in pooledhtable that represent its state (leased/returned) and once close() is called, it would be \"returned\". from this moment any operations on this pooledhtable would result in something like illegalstateexception. ",
        "label": 548
    },
    {
        "text": "thrift2 can not parse values when using framed transport  thrifthbaseservicehandler.java use .array() on table names , and values (family , qualifier in checkanddelete , etc) which resulted in incorrect values with framed transport. replacing .array() with getbytes() fixed this problem. i've attached the patch edit: updated the patch to cover checkandput(), checkanddelete()  update: updated the patch to use bytebuffertobytearray() instead of getbytes() , also removed unused imports. ",
        "label": 193
    },
    {
        "text": "retire non distributed log splitting related code  i think we only use distributed log splitting now and the legacy code before distributed log splitting should be retired. any objections? thanks,  -jeffrey ",
        "label": 233
    },
    {
        "text": "hbase config sh needs to be updated so it can auto detects the sun jdk provided by rhel6  rhel6 will install its sun jdk in /usr/lib/jvm/java-1.6.0-sun-1.6.0.<update_version>.<arch>.  so this ticket is about adding this path to the jdk autodetection mechanism used in hbase-config.sh ",
        "label": 84
    },
    {
        "text": "remove zktablestateclientsidereader class  on client side this is now only used by zookeeperregistry. shouldn't be needed once we have registry implementation not using zk. on server side we should be using tablestatemanager instead. ",
        "label": 41
    },
    {
        "text": "rename  hbase skip errors  in hregion as it is too general sounding   we should rename \"hbase.skip.errors\", used in hregion.java for skipping errors when replaying edits. it should probably be something more like \"hbase.hregion.edits.replay.skip.errors\" or so. ",
        "label": 194
    },
    {
        "text": "testdatetieredcompactionpolicy negativeformajor is flaky  https://builds.apache.org/job/precommit-hbase-build/1365/artifact/patchprocess/patch-unit-hbase-server.txt : negativeformajor(org.apache.hadoop.hbase.regionserver.testdatetieredcompactionpolicy)  time elapsed: 0.48 sec  <<< failure! java.lang.assertionerror: expected:<true> but was:<false> at org.junit.assert.fail(assert.java:88) at org.junit.assert.failnotequals(assert.java:834) at org.junit.assert.assertequals(assert.java:118) at org.junit.assert.assertequals(assert.java:144) at org.apache.hadoop.hbase.regionserver.testdatetieredcompactionpolicy.compactequals(testdatetieredcompactionpolicy.java:94) at org.apache.hadoop.hbase.regionserver.testdatetieredcompactionpolicy.negativeformajor(testdatetieredcompactionpolicy.java:301) similar test failure occurred in master jdk 8 build as well (https://builds.apache.org/job/hbase-trunk_matrix/839/jdk=latest1.8,label=yahoo-not-h2/testreport/org.apache.hadoop.hbase.regionserver/testdatetieredcompactionpolicy/negativeformajor/). since testdatetieredcompactionpolicy is a small test, its failure prevented large tests from running. ",
        "label": 149
    },
    {
        "text": "enhance htable javadoc  better document htable is not thread-safe. ",
        "label": 158
    },
    {
        "text": "client public api should not have pb objects in  some more cleanup for the parent jira.   we have leaked some pb structs in admin (and possible other places). we should clean up these api before 2.0. examples include:   adminprotos.getregioninforesponse.compactionstate getcompactionstate(final tablename tablename)     throws ioexception;         ....   void snapshot(final string snapshotname,       final tablename tablename,       hbaseprotos.snapshotdescription.type type) throws ioexception, snapshotcreationexception,       illegalargumentexception;    ....   masterprotos.snapshotresponse takesnapshotasync(hbaseprotos.snapshotdescription snapshot)       throws ioexception, snapshotcreationexception; ",
        "label": 544
    },
    {
        "text": "incorrect dependency on log class from jetty  there are a few dependencies on the class org.mortbay.log.log which is a bug: 21:05 < dj_ryan> we should not depend on that logging framework  21:05 < dj_ryan> at all  21:05 < dj_ryan> ever i could find it in use in these classes: org.apache.hadoop.hbase.client.scannercallable org.apache.hadoop.hbase.client.testhcm org.apache.hadoop.hbase.mapreduce.export org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader ",
        "label": 314
    },
    {
        "text": "allow regions to be load balanced by table  from our experience, cluster can be well balanced and yet, one table's regions may be badly concentrated on few region servers.  for example, one table has 839 regions (380 regions at time of table creation) out of which 202 are on one server. it would be desirable for load balancer to distribute regions for specified tables evenly across the cluster. each of such tables has number of regions many times the cluster size. ",
        "label": 441
    },
    {
        "text": "up heap used by hadoopqa  ",
        "label": 314
    },
    {
        "text": "hlog preparation and cleanup are done under the updatelock  major slowdown  something i've seen quite often in our production environment: 2010-08-16 16:17:27,104 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000385321 whose highest sequence/edit id is 64837079950  2010-08-16 16:17:27,286 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000392770 whose highest sequence/edit id is 64837088260  2010-08-16 16:17:27,452 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000399300 whose highest sequence/edit id is 64837096566  2010-08-16 16:17:27,635 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000406997 whose highest sequence/edit id is 64837104865  2010-08-16 16:17:27,827 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000413803 whose highest sequence/edit id is 64837113153  2010-08-16 16:17:27,993 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000421709 whose highest sequence/edit id is 64837121467  2010-08-16 16:17:28,160 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000427333 whose highest sequence/edit id is 64837129775  2010-08-16 16:17:28,432 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000434365 whose highest sequence/edit id is 64837138074  2010-08-16 16:17:28,518 info org.apache.hadoop.hbase.regionserver.hlog: removing old hlog file /hbase/.logs/rs22,60020,1280909840873/hlog.dat.1282000440347 whose highest sequence/edit id is 64837146376  2010-08-16 16:17:28,612 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 39 on 60020 took 1801ms appending an edit to hlog; editcount=0  2010-08-16 16:17:28,615 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 37 on 60020 took 1804ms appending an edit to hlog; editcount=1  2010-08-16 16:17:28,615 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 25 on 60020 took 1805ms appending an edit to hlog; editcount=2  ...  2010-08-16 16:17:28,619 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 41 on 60020 took 1875ms appending an edit to hlog; editcount=50  2010-08-16 16:17:28,619 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 24 on 60020 took 1876ms appending an edit to hlog; editcount=51  2010-08-16 16:17:28,619 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 48 on 60020 took 1881ms appending an edit to hlog; editcount=54 and looking at hlog.rollwriter, we roll then cleanup those unused hlog files under updatelock, which blocks all the appenders (as shown). we should only do the first part under that lock ",
        "label": 229
    },
    {
        "text": "port hbase filter out edits at log split time  hbase-6508 is for 0.89-fb branch. this jira ports the feature to trunk. ",
        "label": 441
    },
    {
        "text": "testlogrolling testlogrollondatanodedeath test failure  this is a demanding unit test, which fails fairly often as software versions (jvm, hadoop) and system load change. currently when testing 0.98 branch i see this failure: failed tests:   testlogrollondatanodedeath(org.apache.hadoop.hbase.regionserver.wal.testlogrolling): lowreplication roller should've been disabled, current replication=1 could be a timing issue after the recent switch to hadoop 2 as default build/test profile. let's see if more leniency makes sense and if it can stabilize the test before disabling it. ",
        "label": 441
    },
    {
        "text": "define an interface that will apply to any possible store file implementation  step 1: define what methods we need from a concrete implementation. once we have figured out what the interface should look like, it will be easier to imagine structures that fulfill the requirements. ",
        "label": 314
    },
    {
        "text": "increase wait in testsplittransactiononcluster split  just failed a few time. in 0.94 we wait for 10s in trunk we wait for 30s. let's make it the same in 0.94. ",
        "label": 286
    },
    {
        "text": "enhance delete snapshot rb to call snapshot deletion api with regex  hbase-8461 added the api to hbaseadmin which allows user to specify regular expression for deleting snapshots. this jira would allow delete_snapshot.rb to utilize this functionality. ",
        "label": 53
    },
    {
        "text": "increment data will be lost when the memstore is flushed  there are two problems in increment() now:  first:  i see that the timestamp(the variable now) in hregion's increment() is generated before got the rowlock, so when there are multi-thread increment the same row, although it generate earlier, it may got the lock later. because increment just store one version, so till now, the result will still be right. when the region is flushing, these increment will read the kv from snapshot and memstore with whose timestamp is larger, and write it back to memstore. if the snapshot's timestamp larger than the memstore, the increment will got the old data and then do the increment, it's wrong. secondly:  also there is a risk in increment. because it writes the memstore first and then hlog, so if it writes hlog failed, the client will also read the incremented value. ",
        "label": 414
    },
    {
        "text": "stuck fshlog  bad disk  hdfs  and can't roll wal  hbase-1.1.1 and hadoop-2.7.1 we try to roll logs because can't append (see hdfs-8960) but we get stuck. see attached thread dump and associated log. what is interesting is that syncers are waiting to take syncs to run and at same time we want to flush so we are waiting on a safe point but there seems to be nothing in our ring buffer; did we go to roll log and not add safe point sync to clear out ringbuffer? needs a bit of study. try to reproduce. ",
        "label": 314
    },
    {
        "text": "in standalone mode with local filesystem hbase logs warning message failed to invoke 'unbuffer' method in class class org apache hadoop fs fsdatainputstream  new users may get nervous after seeing following warning level log messages (considering new users will most likely run hbase in standalone mode first): warn  [memstoreflusher.1] io.fsdatainputstreamwrapper: failed to invoke 'unbuffer' method in class class org.apache.hadoop.fs.fsdatainputstream . so there may be a tcp socket connection left open in close_wait state. ",
        "label": 53
    },
    {
        "text": "multihfileoutputformat  like multitableoutputformat, but outputting hfiles. key is tablename as an ibw. creates sub-writers (code cut and pasted from hfileoutputformat) on demand that produce hfiles in per-table subdirectories of the configured output path. does not currently support partitioning for existing tables / incremental update. ",
        "label": 499
    },
    {
        "text": "splittableregionprocedure and mergetableregionsprocedure should skip store files for unknown column families  hit this problem in our internal staging cluster. not sure why, but probably, there was a partial successful 'alter table' call that removed a family. as it is 'partial successful', there are still some stale store files of the removed family left under the region directory. and in splittableregionprocedure and mergetableregionsprocedure, we will get all the store files by listing the file system, so we will also get the stale store files for the family which should have been removed already, and then causes npe when we want to access the columnfamilydescriptor. although it is not the common case that there are store files for removed families, but fwiw, i think we can do something to make our procedures more robust... ",
        "label": 149
    },
    {
        "text": "table creation  though using  async  call to master  can actually run for a while and cause rpc timeout  our create table methods in hbaseadmin are synchronous from client pov. however, underneath, we're using an \"async\" create and then looping waiting for table availability. because the create is async and we loop instead of block on rpc, we don't expect rpc timeouts. however, when creating a table with lots of initial regions, the \"async\" create can actually take a long time (more than 30 seconds in this case) which causes the client to timeout and gives impression something failed. we should make the create truly async so that this can't happen. and rather than doing one-off, inline assignment as it is today, we should reuse the fancy enable/disable code stack just added to make this faster and more optimal. ",
        "label": 326
    },
    {
        "text": "convert ad hoc writable versioning to uses of versionedwritable  there are a number of instances of ad hoc versioning of writables: htd, hcd, etc. we should convert our classes to inherit versionedwritable. see http://hadoop.apache.org/common/docs/r0.19.1/api/org/apache/hadoop/io/versionedwritable.html ",
        "label": 38
    },
    {
        "text": "avoid byte buffer allocations when reading a value from a result object  when calling result.getvalue(), an extra dummy keyvalue and its associated underlying byte array are allocated, as well as a persistent buffer that will contain the returned value. these can be avoided by reusing a static array for the dummy object and by passing a bytebuffer object as a value destination buffer to the read method. the current functionality is maintained, and we have added a separate method call stack that employs the described changes. i will provide more details with the patch. running tests with a profiler, the reduction of read time seems to be of up to 40%. ",
        "label": 456
    },
    {
        "text": "documentation and release notes  document all the changes: algorithms, new configuration options, obsolete configurations, upgrade procedure and possibility of downgrade. ",
        "label": 402
    },
    {
        "text": "add on meta cache   we need to cache this stuff, and it needs to be fast. ",
        "label": 323
    },
    {
        "text": "atomicincrement doesnt increase hregion memcachesize  this prevents flushing! ",
        "label": 547
    },
    {
        "text": "timerange's factory functions do not support ranges  only  alltime  and  at   the org.apache.hadoop.hbase.io.timerange is used in functions like org.apache.hadoop.hbase.client.table.checkandmutatebuilder#timerange. the current ways to create a timerange are: factory functions: at (a single instant), alltime (all valid timestamps) deprecated and @interfaceaudience.private constructors, which support more ranges, like [minstamp, maxstamp), and [minstamp, max) this is insufficient for all but the simplest use of the checkandmutatebuilder#timerange function. on user@hbase.apache.org, it was suggested that more factory functions could be added: https://lists.apache.org/thread.html/0ffc5e57c396873d56e49d7b02e823432b053fb98037ee6778d7c2ce@%3cuser.hbase.apache.org%3e however, timerange's documentation currently says:  * can be returned and read by clients.  should not be directly created by clients.  * thus, all constructors are purposely @interfaceaudience.private. so another approach to making checkandmutatebuilder#timerange useful may be required. ",
        "label": 207
    },
    {
        "text": "hbckservercrashprocedure for 'unknown servers'  with an overdriving, sustained load, i can fairly easily manufacture an hbase:meta table that references servers that are no longer in the live list nor are members of deadservers; i.e. 'unknown servers'. the new 'hbck report' ui in master has a section where it lists 'unknown servers' if any in hbase:meta. once in this state, the repair is awkward. our assign/unassign procedure is particularly dogged about insisting that we confirm close/open of regions when it is going about its business which is well and good if server is in live/dead sets but when an 'unknown server', we invariably end up trying to confirm against a non-longer present server (more on this in follow-on issues). what is wanted is queuing of a servercrashprocedure for each 'unknown server'. it would split any wals (there shouldn't be any if server was restarted) and ideally it would cancel out any assigns and reassign regions off the 'unknown server'. but the 'normal' scp consults the in-memory cluster state figuring what regions were on the crashed server... and 'unknown servers' don't have state in in-master memory maps of servers to regions or in deadservers list which works fine for the usual case. suggestion here is that hbck2 be able to drive in a special scp, one which would get list of regions by scanning hbase:meta rather than asking master memory; an hbckscp. ",
        "label": 314
    },
    {
        "text": "upgrade to zk  currently we are shipping 0.92 with 3.4.1rc2 which is what became the release but change the pom to get the release; it looks better. ",
        "label": 38
    },
    {
        "text": "duplicate assignment of a region after region server recovery  after a region server recovery, some regions may get assigned to duplicate region servers. note: i am based on a slightly older trunk (prior to the hbase-2694). nevertheless, i think hbase-2694 doesn't address this case. scenario: three region server setup (store285,286,287), with about 500 regions in the table overall. kill -9 and restart one of the region servers (store286). the 170 odd regions in the failed region server got assigned out. but two of the regions got assigned to multiple region servers. looking at the log entries for one such region, it appears that there is some race condition that happens between the processregionopen (a regionserveroperation) and basescanner which causes the basescanner to think this region needs to be reassigned. relevant logs: master detects that the server start message (from the restarted rs) is from a server it already knows about, but startcode is different. so, it triggers server recovery. alternatively, the recovery will be triggered by znode expiry in some cases depending on which ever event (restart of rs or znode expiry) happens first. after that it does logs splits etc. for the failed rs; it then also removes the old region server/startcode from the deadservers map. 2010-06-17 10:26:06,420 info org.apache.hadoop.hbase.master.servermanager: server start rejected; we already have 10.138.95.182:60020 registered; existingserver=servername=store286.xyz.com,60020,1276629467680, load=(requests=22, regions=171, usedheap=6549, maxheap=11993), newserver=servername=store286.xyz.com,60020,1276795566511, load=(requests=0, regions=0, usedheap=0, maxheap=0) 2010-06-17 10:26:06,420 info org.apache.hadoop.hbase.master.servermanager: triggering server recovery; existingserver looks stale 2010-06-17 10:26:06,420 debug org.apache.hadoop.hbase.master.servermanager: added=store286.xyz.com,60020,1276629467680 to dead servers, added shutdown processing operation ... split log processing... 2010-06-17 10:29:51,317 debug org.apache.hadoop.hbase.master.regionserveroperation: removed store286.xyz.com,60020,1276629467680 from deadservers map what follows is the relevant log snippet for one of the regions that gets double assigned. master tries to assign the region to store285. at 10:30:20,006, in processregionopen, we update meta with information about the new assignment. however, just around the same time, basescanner processes this entry (at 10:30:20,009), but finds that the region is still assigned to the old region server. there have been some fixes for double assignment in basescanner because basescanner might be doing a stale read depending on when it started. but looks like there is still another hole left. 2010-06-17 10:30:10,186 info org.apache.hadoop.hbase.master.regionmanager: assigning region test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. to store285.xyz.com,60020,1276629468460 2010-06-17 10:30:11,701 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_process_open: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 8 of 2010-06-17 10:30:12,800 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_process_open: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 7 of 2010-06-17 10:30:13,905 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_process_open: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 6 of ... 2010-06-17 10:30:20,001 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store285.xyz.com,60020,1276629468460; 1 of 3 2010-06-17 10:30:20,001 info org.apache.hadoop.hbase.master.regionserveroperation: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. open on store285.xyz.com,60020,1276629468460 2010-06-17 10:30:20,006 info org.apache.hadoop.hbase.master.regionserveroperation: updated row test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. in region .meta.,,1 with startcode=1276629468460, server=store285.xyz.com:60020 2010-06-17 10:30:20,009 debug org.apache.hadoop.hbase.master.basescanner: current assignment of test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. is not valid;  serveraddress=store286.xyz.com:60020, startcode=1276629467680 unknown. at this point basescanner calls \"this.master.getregionmanager().setunassigned(info, true)\" to set the region to be unassigned (even though it is assigned to store285). and later, this region is given to another region server (store287). 2010-06-17 10:30:20,581 info org.apache.hadoop.hbase.master.regionmanager: assigning region test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. to store287.xyz.com,60020,1276629468678 2010-06-17 10:30:25,525 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_open: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. from store287.xyz.com,60020,1276629468678; 6 of 6 2010-06-17 10:30:25,531 info org.apache.hadoop.hbase.master.regionserveroperation: test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. open on store287.xyz.com,60020,1276629468678 2010-06-17 10:30:25,534 info org.apache.hadoop.hbase.master.regionserveroperation: updated row test1,9301760000,1276657884012.acff724037d739bab9af61e3edef0cc9. in region .meta.,,1 with startcode=1276629468678, server=store287.xyz.com:60020 ",
        "label": 247
    },
    {
        "text": "test errors when building on macos  results : failed tests: testbackgroundevictionthread[0](org.apache.hadoop.hbase.io.hfile.testlrublockcache): expected:<2> but was:<1>  testbackgroundevictionthread[1](org.apache.hadoop.hbase.io.hfile.testlrublockcache): expected:<2> but was:<1>  testsplitcalculatoreq(org.apache.hadoop.hbase.util.testregionsplitcalculator): expected:<2> but was:<1> ",
        "label": 284
    },
    {
        "text": "hbasetestingutility getminihbasecluster should be able to return null  right now we have this logic:   public minihbasecluster getminihbasecluster() {     if (this.hbasecluster instanceof minihbasecluster) {       return (minihbasecluster)this.hbasecluster;     }     throw new runtimeexception(hbasecluster + \" not an instance of \" +                                minihbasecluster.class.getname());   } since null is not actually an instance of minihbasecluster we'll throw the runtimeexception rather than returning null.  some tests call this method and check for null, which is pointless. ",
        "label": 286
    },
    {
        "text": "add missing equals or hashcode method s  to stock filter implementations  in hbase-15410, mike drob reminded me that filter implementations may not write equals or hashcode method(s). this issue is to add missing equals or hashcode method(s) to stock filter implementations such as keyonlyfilter. ",
        "label": 536
    },
    {
        "text": "asyncprotobuflogwriter persists protobuflogwriter as class name for backward compatibility  for hlog generated by 2.x, log splitting from hbase1 would result in: 1134720 2018-02-13 10:43:57,590 warn  [rs_log_replay_ops-ve0530:16020-0] regionserver.splitlogworker: log splitting of wals/ve0534.halxg.cloudera.com,16020,1518546984742-splitting/ve0534.halxg.cloudera.com%2c16020%2c1518546984742.meta.1518546993545.meta failed, returning error 1134721 java.io.ioexception: got unknown writer class: asyncprotobuflogwriter 1134722   at org.apache.hadoop.hbase.regionserver.wal.protobuflogreader.initinternal(protobuflogreader.java:220) 1134723   at org.apache.hadoop.hbase.regionserver.wal.protobuflogreader.initreader(protobuflogreader.java:169) 1134724   at org.apache.hadoop.hbase.regionserver.wal.readerbase.init(readerbase.java:66) 1134725   at org.apache.hadoop.hbase.regionserver.wal.protobuflogreader.init(protobuflogreader.java:164) 1134726   at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:303) 1134727   at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:267) 1134728   at org.apache.hadoop.hbase.wal.walsplitter.getreader(walsplitter.java:853) 1134729   at org.apache.hadoop.hbase.wal.walsplitter.getreader(walsplitter.java:777) 1134730   at org.apache.hadoop.hbase.wal.walsplitter.splitlogfile(walsplitter.java:298) 1134731   at org.apache.hadoop.hbase.wal.walsplitter.splitlogfile(walsplitter.java:236) asyncprotobuflogwriter didn't change wal format and hence can use protobuflogwriter as the persisted class name so that we avoid the above during rolling upgrade. ",
        "label": 441
    },
    {
        "text": "revision to hbase book  more examples in data model  more metrics  more performance  this is a large-ish change to the hbase book moving the 'data model' section forward in the book (before schema design) adding some short api example snippets in 'data model' that illustrate the points discussed correcting a minor formatting issue in my last patch with pre-creating regions. included a listing of the region-server metrics however, i would like a one of the commiters to comment on what some of these are. expanded mapreduce example added a few short example snippets expanded the performance section some of which was simply referencing other configuration topics brought up elsewhere that are certain to cause people performance problems if mis-configured. a few client examples of things that can cause performance problems if not used properly. ",
        "label": 146
    },
    {
        "text": "allow more than one backing file in bucketcache  allow bucketcache use more than just one backing file: e.g. chassis has more than one ssd in it. usage (setting the following configurations in hbase-site.xml): <property>  <name>hbase.bucketcache.ioengine</name>  <value>files:/mnt/disk1/bucketcache,/mnt/disk2/bucketcache,/mnt/disk3/bucketcache,/mnt/disk4/bucketcache</value>  </property>  <property>  <name>hbase.bucketcache.size</name>  <value>1048576</value>  </property> the above setting means the total capacity of cache is 1048576mb(1tb), each file length will be set to 0.25tb. ",
        "label": 107
    },
    {
        "text": "dfs exception and regionserver stuck during heavy write load  it's a 3 node setup, each runs datanode and regionserver. one runs as hbase master and hadoop namenode. after some heavy write load via java client, the client is stuck. stack trace on the regionserver shows: \"ipc server handler 46 on 60020\" daemon prio=10 tid=0x4dd3f000 nid=0x4eb3 waiting for monitor entry [0x4cc82000..0x4cc83130]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 43 on 60020\" daemon prio=10 tid=0x4dd3bc00 nid=0x4eb0 waiting for monitor entry [0x4cd75000..0x4cd75fb0]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 40 on 60020\" daemon prio=10 tid=0x4dd38400 nid=0x4ead runnable [0x4ce68000..0x4ce68e30]  java.lang.thread.state: runnable  at sun.nio.ch.epollarraywrapper.epollwait(native method)  at sun.nio.ch.epollarraywrapper.poll(epollarraywrapper.java:215)  at sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:65)  at sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:69) locked <0x6a557580> (a sun.nio.ch.util$1) locked <0x6a557570> (a java.util.collections$unmodifiableset) locked <0x5cdcec18> (a sun.nio.ch.epollselectorimpl)  at sun.nio.ch.selectorimpl.select(selectorimpl.java:80)  at org.apache.hadoop.net.socketiowithtimeout$selectorpool.select(socketiowithtimeout.java:237)  at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:155)  at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:149)  at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:122)  at java.io.bufferedinputstream.fill(bufferedinputstream.java:218)  at java.io.bufferedinputstream.read(bufferedinputstream.java:237) locked <0x552ffb60> (a java.io.bufferedinputstream)  at java.io.datainputstream.readint(datainputstream.java:370)  at org.apache.hadoop.dfs.dfsclient$blockreader.readchunk(dfsclient.java:928) locked <0x55300f78> (a org.apache.hadoop.dfs.dfsclient$blockreader)  at org.apache.hadoop.fs.fsinputchecker.readchecksumchunk(fsinputchecker.java:236)  at org.apache.hadoop.fs.fsinputchecker.fill(fsinputchecker.java:178)  at org.apache.hadoop.fs.fsinputchecker.read1(fsinputchecker.java:195)  at org.apache.hadoop.fs.fsinputchecker.read(fsinputchecker.java:159) locked <0x55300f78> (a org.apache.hadoop.dfs.dfsclient$blockreader)  at org.apache.hadoop.dfs.dfsclient$blockreader.read(dfsclient.java:823) locked <0x55300f78> (a org.apache.hadoop.dfs.dfsclient$blockreader)  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.readbuffer(dfsclient.java:1352) locked <0x59a70e40> (a org.apache.hadoop.dfs.dfsclient$dfsinputstream)  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.read(dfsclient.java:1388) locked <0x59a70e40> (a org.apache.hadoop.dfs.dfsclient$dfsinputstream)  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.read(dfsclient.java:1337) locked <0x59a70e40> (a org.apache.hadoop.dfs.dfsclient$dfsinputstream)  at java.io.datainputstream.readint(datainputstream.java:370)  at org.apache.hadoop.io.sequencefile$reader.readrecordlength(sequencefile.java:1847) locked <0x651f77b0> (a org.apache.hadoop.io.sequencefile$reader)  at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1877) locked <0x651f77b0> (a org.apache.hadoop.io.sequencefile$reader)  at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1782) locked <0x651f77b0> (a org.apache.hadoop.io.sequencefile$reader)  at org.apache.hadoop.io.mapfile$reader.seekinternal(mapfile.java:476) locked <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.io.mapfile$reader.getclosest(mapfile.java:558) locked <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.rowkeyfrommapfileemptykeys(hstore.java:1463)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1434) locked <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 38 on 60020\" daemon prio=10 tid=0x4dd36000 nid=0x4eab waiting for monitor entry [0x4cf0a000..0x4cf0b130]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 37 on 60020\" daemon prio=10 tid=0x4dd35000 nid=0x4eaa waiting for monitor entry [0x4cf5b000..0x4cf5c0b0]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 35 on 60020\" daemon prio=10 tid=0x4dd32c00 nid=0x4ea8 waiting for monitor entry [0x4cffd000..0x4cffdfb0]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 30 on 60020\" daemon prio=10 tid=0x4dd2d400 nid=0x4ea3 waiting for monitor entry [0x4d192000..0x4d193130]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 29 on 60020\" daemon prio=10 tid=0x4dd2c000 nid=0x4ea2 waiting for monitor entry [0x4d1e3000..0x4d1e40b0]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 26 on 60020\" daemon prio=10 tid=0x4dd29800 nid=0x4e9f waiting for monitor entry [0x4d2d6000..0x4d2d6f30]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 17 on 60020\" daemon prio=10 tid=0x4dd1f800 nid=0x4e96 waiting for monitor entry [0x4d5af000..0x4d5afeb0]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 14 on 60020\" daemon prio=10 tid=0x4dd1c400 nid=0x4e93 waiting for monitor entry [0x4d6a2000..0x4d6a3130]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 10 on 60020\" daemon prio=10 tid=0x4dd17c00 nid=0x4e8f waiting for monitor entry [0x4d7e6000..0x4d7e6f30]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 7 on 60020\" daemon prio=10 tid=0x4dd14800 nid=0x4e8c waiting for monitor entry [0x4d8d9000..0x4d8da1b0]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) \"ipc server handler 0 on 60020\" daemon prio=10 tid=0x4e2c0c00 nid=0x4e85 waiting for monitor entry [0x4db10000..0x4db10e30]  java.lang.thread.state: blocked (on object monitor)  at org.apache.hadoop.hbase.regionserver.hstore.rowatorbeforefrommapfile(hstore.java:1424) waiting to lock <0x59d27ba0> (a org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$reader)  at org.apache.hadoop.hbase.regionserver.hstore.getrowkeyatorbefore(hstore.java:1399)  at org.apache.hadoop.hbase.regionserver.hregion.getclosestrowbefore(hregion.java:1210)  at org.apache.hadoop.hbase.regionserver.hregionserver.getclosestrowbefore(hregionserver.java:1099)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:438)  at org.apache.hadoop.ipc.server$handler.run(server.java:896) in regionserver log, i see the following right before the client stuck (there are few other similar logs, but the client keeps going at those time points): 2008-07-17 22:31:49,404 info org.apache.hadoop.hbase.regionserver.hregion: region aaa,bbb,1216304670433/1145836031 available  2008-07-17 22:31:49,404 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region aaa,bbb,1216304670433  2008-07-17 22:32:07,653 warn org.apache.hadoop.hbase.regionserver.hstore: exception closing reader for 1145836031/ccc  java.io.ioexception: stream closed  at org.apache.hadoop.dfs.dfsclient$dfsinputstream.close(dfsclient.java:1319)  at java.io.filterinputstream.close(filterinputstream.java:155)  at org.apache.hadoop.io.sequencefile$reader.close(sequencefile.java:1581)  at org.apache.hadoop.io.mapfile$reader.close(mapfile.java:577)  at org.apache.hadoop.hbase.regionserver.hstore.closecompactionreaders(hstore.java:917)  at org.apache.hadoop.hbase.regionserver.hstore.compacthstorefiles(hstore.java:910)  at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:787)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:887)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:847)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:84)  (and two of the same exception, since i have 3 hstorefile to compact)  2008-07-17 22:32:07,912 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region aaa,bbb,1216304670433 in 18sec  [after this point, i only see regionserver rotates hlog, no other activities) at 22:32, no suspicious log in datanode, but 8mins later, i see this 2008-07-17 22:40:07,928 warn org.apache.hadoop.dfs.datanode: 192.168.1.5650010:got exception while serving blk_-38731635936101350 to /192.168.1.56  java.net.sockettimeoutexception: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.socketchannel[connected local=/192.168.1.56:50010 remote=/192.168.1.56:40691]  at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:170)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:144)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:105)  at java.io.bufferedoutputstream.write(bufferedoutputstream.java:105)  at java.io.dataoutputstream.write(dataoutputstream.java:90)  at org.apache.hadoop.dfs.datanode$blocksender.sendchunks(datanode.java:1784)  at org.apache.hadoop.dfs.datanode$blocksender.sendblock(datanode.java:1840)  at org.apache.hadoop.dfs.datanode$dataxceiver.readblock(datanode.java:1055)  at org.apache.hadoop.dfs.datanode$dataxceiver.run(datanode.java:984)  at java.lang.thread.run(thread.java:619) for this particular block in question, i found around the region available time: 2008-07-17 22:31:49,642 info org.apache.hadoop.dfs.datanode: receiving block blk_-38731635936101350 src: /192.168.1.56:37878 dest: /192.168.1.56:50010  2008-07-17 22:31:56,856 info org.apache.hadoop.dfs.datanode: received block blk_-38731635936101350 of size 67108864 from /192.168.1.56  2008-07-17 22:31:56,857 info org.apache.hadoop.dfs.datanode: packetresponder 1 for block blk_-38731635936101350 terminating and after the hbase client stuck, i found one datanode keeps sending the same block to the regionserver, which is blocked as shown above. ===== for the record, i did not see this \"stream closed\" error on another small 4-node cluster with trunk r675659 (same hadoop version with the 3-node cluster above). for hbase trunk r677011, i got java.lang.nullpointerexception  at org.apache.hadoop.hbase.client.servercallable.getservername(servercallable.java:63)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getregionserverwithretries(hconnectionmanager.java:886  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1161) then, the region server stucks 08/07/18 05:29:29 info ipc.rpc: problem connecting to server: /192.168.1.56:60020 stack dump shows similar as the above one, and i'm also seeing the dfs exception. ",
        "label": 314
    },
    {
        "text": "timestamp anchored scanning fails to find all records  if i add 3 versions of a cell and then scan across the first set of added cells using a timestamp that should only get values from the first upload, a bunch are missing (i added 100k on each of the three uploads). i thought it the fact that we set the number of cells found back to 1 in hstore when we move off current row/column but that doesn't seem to be it. i also tried upping the max_versions on my table and that seemed to have no effect. need to look closer. build a unit test because replicating on cluster takes too much time. ",
        "label": 241
    },
    {
        "text": "problem while stopping hbase  stop-hbase.sh stops the server successfully if and only if the server is instantiated properly. when u run start-hbase.sh; sleep 10; stop-hbase.sh; ( this works totally fine and has no issues ) whereas when u run start-hbase.sh; stop-hbase.sh; ( this never stops the server and neither the server gets initialized and starts properly ) ",
        "label": 11
    },
    {
        "text": "remove   todo  put back    hadoop home bin hadoop dfsadmin  safemode wait   in start-hbase.sh there is the above comment. was done as part of the below: ------------------------------------------------------------------------ r618560 | stack | 2008-02-04 22:15:44 -0800 (mon, 04 feb 2008) | 14 lines hbase-403 fix build after move of hbase in svn make it so scripts basically work again.  make up a log4j basic properites file (currently broke). m    trunk/conf/hbase-env.sh     put a java_home in here.  need to set it here now instead of up in hadoop-env a    trunk/conf/log4j.properties     basic log4j for hbase. m    trunk/bin/hbase     made pointers up into hadoop instead point to hbase substitutions. m    trunk/bin/start-hbase.sh     remove wait on dfs for moment.  need to figure how to do it now     we're detached (besides there is talk that we'd acually  the comment doesn't seem to apply any more because we wait up in java. this issue is about cleaning up this remark. ",
        "label": 314
    },
    {
        "text": "naming and documenting of the hadoop metrics2 properties file  hadoop-metrics2.properties is currently where metrics2 loads it's sinks.  this file could be better named, hadoop-hbase-metrics2.properties  in addition it needs examples like the current hadoop-metrics.properties has. ",
        "label": 154
    },
    {
        "text": "resolve name conflict when splitting if there are duplicated wal entries  the asyncfshlog introduced in hbase-14790 may write same wal entries to different wal files. wal entry itself is idempotent so replay is not a problem but the intermediate file name and final name when splitting is constructed using the lowest or highest sequence id of the wal entries written, so it is possible that different wal files will have same intermediate or final file name when splitting. in the currentm implementation, this will cause split fail or data loss. we need to solve this. ",
        "label": 149
    },
    {
        "text": "testheapsize broke in trunk  this commit added map to hregion commit 888d73a9f5fe907f7c616211322fff339eeaa446 author: zhihong yu <tedyu@apache.org> date:   fri dec 9 06:01:58 2011 +0000     hbase-4946  htable.coprocessorexec (and possibly coprocessorproxy) does not work with                    dynamically loaded coprocessors (andrei dragomir) ",
        "label": 314
    },
    {
        "text": "hfiles in use by a table which has the same name and namespace with a default table cloned from snapshot may be deleted when that snapshot and default table are deleted  we recently had a critical production issue in which hfiles that were still in use by a table were deleted.  this appears to have been caused by conditions in which table have the same namespace and name with a default table cloned from snapshot.when snapshot and default table be deleted,hfiles that are still in use may be deleted.  for example:  table with default namespace is: \"t1\"  the namespace of the new table is the same as the name of the default table, and is generated by snapshot cloned : \"t1: t1\"  when the snapshot and the default namespace table are deleted, the new table is also deleted in the used hfiles  this is because the creation of the backreferencefile get the table name is not normal, resulting in can not find the reference file, hfilecleaner to delete the hfiles in used, when the table has not been major compact   public static boolean create(final configuration conf, final filesystem fs,       final path dstfamilypath, final tablename linkedtable, final string linkedregion,       final string hfilename, final boolean createbackref) throws ioexception {     string familyname = dstfamilypath.getname();     string regionname = dstfamilypath.getparent().getname();     string tablename = fsutils.gettablename(dstfamilypath.getparent().getparent())         .getnameasstring();   public static tablename gettablename(path tablepath) {     return tablename.valueof(tablepath.getparent().getname(), tablepath.getname());   } ",
        "label": 554
    },
    {
        "text": "provide new delete flag which can delete all cells under a column family which have designated timestamp  in one of our production scenario (xiaomi message search), multiple cells will be put in batch using a same timestamp with different column names under a specific column-family. and after some time these cells also need to be deleted in batch by given a specific timestamp. but the column names are parsed tokens which can be arbitrary words , so such batch delete is impossible without first retrieving all kvs from that cf and get the column name list which has kv with that given timestamp, and then issuing individual deletecolumn for each column in that column-list. though it's possible to do such batch delete, its performance is poor, and customers also find their code is quite clumsy by first retrieving and populating the column list and then issuing a deletecolumn for each column in that column-list. this feature resolves this problem by introducing a new delete flag: deletefamilyversion.  1). when you need to delete all kvs under a column-family with a given timestamp, just call delete.deletefamilyversion(cfname, timestamp); only a deletefamilyversion type kv is put to hbase (like deletefamily / deletecolumn / delete) without read operation;  2). like other delete types, deletefamilyversion takes effect in get/scan/flush/compact operations, the scandeletetracker now parses out and uses deletefamilyversion to prevent all kvs under the specific cf which has the same timestamp as the deletefamilyversion kv to pop-up as part of a get/scan result (also in flush/compact). our customers find this feature efficient, clean and easy-to-use since it does its work without knowing the exact column names list that needs to be deleted. this feature has been running smoothly for a couple of months in our production clusters. ",
        "label": 203
    },
    {
        "text": "assignmentmanager should account for unsuccessful split correctly which initially passes quota check  when region split doesn't pass quota check, we would see exception similar to the following: 2015-12-29 16:07:33,653 info  [rs:0;10.21.128.189:57449-splits-1451434041585] regionserver.splitrequest(97): running rollback/cleanup of failed split of np2:                     testregionnormalizationsplitoncluster,zzzzz,1451434045065.27cccb3fae03002b8058beef61cb7c20.; failed to get ok from master to split np2:testregionnormalizationsplitoncluster,     zzzzz,1451434045065.27cccb3fae03002b8058beef61cb7c20. java.io.ioexception: failed to get ok from master to split np2:testregionnormalizationsplitoncluster,zzzzz,1451434045065.27cccb3fae03002b8058beef61cb7c20.   at org.apache.hadoop.hbase.regionserver.splittransactionimpl.stepsbeforeponr(splittransactionimpl.java:345)   at org.apache.hadoop.hbase.regionserver.splittransactionimpl.createdaughters(splittransactionimpl.java:262)   at org.apache.hadoop.hbase.regionserver.splittransactionimpl.execute(splittransactionimpl.java:502)   at org.apache.hadoop.hbase.regionserver.splitrequest.dosplitting(splitrequest.java:82)   at org.apache.hadoop.hbase.regionserver.splitrequest.run(splitrequest.java:155)   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) however, region split may fail for subsequent splittransactionphase's in stepsbeforeponr().  currently in branch-1, the distinction among the following transitioncode's is not clear in assignmentmanager#onregiontransition():     case split_ponr:     case split:     case split_reverted:       errormsg =           onregionsplit(servername, code, hri, hregioninfo.convert(transition.getregioninfo(1)),             hregioninfo.convert(transition.getregioninfo(2)));       if (org.apache.commons.lang.stringutils.isempty(errormsg)) {         try {           regionstatelistener.onregionsplitreverted(hri); onregionsplit() handles the above 3 transitioncode's. however, errormsg is normally null (onregionsplit returns null at the end).  this would result in onregionsplitreverted() being called for cases of split_ponr and split. when region split fails, assignmentmanager#onregiontransition() should account for the failure properly so that quota bookkeeping is consistent. ",
        "label": 441
    },
    {
        "text": "canary in regionserver mode might not enumerate all regionservers  when running in regionserver mode the canary is expected to probe for service health one time per regionserver during a probe interval. each time the canary chore fires, we create a regionservermonitor, which uses filterregionserverbyname (via getallregionserverbyname) to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. the list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. to ensure we have the complete list of live regionservers i think it would be better to use admin#getclusterstatus and enumerate the live server list returned in the result. ",
        "label": 396
    },
    {
        "text": "root stuck in assigning forever  after restart a cluster, all region servers checked into master but the master stuck in assigning forever. master log shows it keeps trying connect to one region server for root table, while that region server's log shows it keeps printing out notservingregionexception. after restart the master, things are ok now. ",
        "label": 242
    },
    {
        "text": "dynamic filter   not using dynamicclassloader when using filterlist  i've tried to use dynamic jar load (https://issues.apache.org/jira/browse/hbase-1936) but seems to have an issue with filterlist.   here is some log from my app where i send a get with a filterlist containing afilter and other with bfilter. 2013-12-02 13:55:42,564 debug org.apache.hadoop.hbase.util.dynamicclassloader: class d.p.afilter not found - using dynamical class loader 2013-12-02 13:55:42,564 debug org.apache.hadoop.hbase.util.dynamicclassloader: finding class: d.p.afilter 2013-12-02 13:55:42,564 debug org.apache.hadoop.hbase.util.dynamicclassloader: loading new jar files, if any 2013-12-02 13:55:42,677 debug org.apache.hadoop.hbase.util.dynamicclassloader: finding class again: d.p.afilter 2013-12-02 13:55:43,004 error org.apache.hadoop.hbase.io.hbaseobjectwritable: can't find class d.p.bfilter java.lang.classnotfoundexception: d.p.bfilter at java.net.urlclassloader$1.run(urlclassloader.java:202) at java.security.accesscontroller.doprivileged(native method) at java.net.urlclassloader.findclass(urlclassloader.java:190) at java.lang.classloader.loadclass(classloader.java:306) at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) at java.lang.classloader.loadclass(classloader.java:247) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:247) at org.apache.hadoop.conf.configuration.getclassbyname(configuration.java:820) at org.apache.hadoop.hbase.io.hbaseobjectwritable.getclassbyname(hbaseobjectwritable.java:792) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:679) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:594) at org.apache.hadoop.hbase.filter.filterlist.readfields(filterlist.java:324) at org.apache.hadoop.hbase.client.get.readfields(get.java:405) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:690) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:594) at org.apache.hadoop.hbase.client.action.readfields(action.java:101) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:690) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:594) at org.apache.hadoop.hbase.client.multiaction.readfields(multiaction.java:116) at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:690) at org.apache.hadoop.hbase.ipc.invocation.readfields(invocation.java:126) at org.apache.hadoop.hbase.ipc.hbaseserver$connection.processdata(hbaseserver.java:1311) at org.apache.hadoop.hbase.ipc.hbaseserver$connection.readandprocess(hbaseserver.java:1226) at org.apache.hadoop.hbase.ipc.hbaseserver$listener.doread(hbaseserver.java:748) at org.apache.hadoop.hbase.ipc.hbaseserver$listener$reader.dorunloop(hbaseserver.java:539) at org.apache.hadoop.hbase.ipc.hbaseserver$listener$reader.run(hbaseserver.java:514) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) afilter is not found so it tries with dynamicclassloader, but when it tries to load afilter, it uses urlclassloader and fails without checking out for dynamic jars. i think the issue is releated to filterlist#readfields filterlist.java    public void readfields(final datainput in) throws ioexception {     byte opbyte = in.readbyte();     operator = operator.values()[opbyte];     int size = in.readint();     if (size > 0) {       filters = new arraylist<filter>(size);       for (int i = 0; i < size; i++) {         filter filter = (filter)hbaseobjectwritable.readobject(in, conf);         filters.add(filter);       }     }   } hbaseobjectwritable#readobject uses a conf (created by calling hbaseconfiguration.create()) which i suppose doesn't include a dynamicclassloader instance. ",
        "label": 242
    },
    {
        "text": "add ut to confirm the race in masterrpcservices getprocedureresult  ",
        "label": 149
    },
    {
        "text": "race in start and terminate of a replication source after we async start replicatione endpoint  ",
        "label": 149
    },
    {
        "text": "adding missing import statement from example and removing some  override attributes   some left over @override tags and adding 2 import statements for the mapred sampleuploader. ",
        "label": 38
    },
    {
        "text": "backport hbase to  hbase-6059 seems to be an important issue. chunhui has already given a patch for 94. need to rebase if it does not apply cleanly.  raising a new one as the old issue is already closed. ",
        "label": 441
    },
    {
        "text": "retiring regions is not used  exploit or remove  there is a little dance around region close where a region first gets moved into the retiring queue. the idea, iirc, was that regions in retiring could serve reads while close was going about its business. meant that region was online that bit longer. this feature is not used any more \u2013 regions are added to retiring but gets do not bother to look in retiring. we should either remove retiring cocept altogether or else make use of it again. ",
        "label": 241
    },
    {
        "text": "in shell  provide alias of 'desc' for 'describe'  it would be nice if you could type desc <tablename> in hbase shell as shorthand for describe <tablename> similar to how you can in traditional databases. ",
        "label": 402
    },
    {
        "text": "independent timeout configuration for rpc channel between cluster nodes  default of \"hbase.rpc.timeout\" is 60000 ms (1 min). user sometimes  increase them to a bigger value such as 600000 ms (10 mins) for many  concurrent loading application from client. some user share the same  hbase-site.xml for both client and server. hregionserver  #tryregionserverreport via rpc channel to report to live master, but  there was a window for master failover scenario. that region server  attempting to connect to master, which was just killed, backup master  took the active role immediately and put to /hbase/master, but region  server was still waiting for the rpc timeout from connecting to the dead  master. if \"hbase.rpc.timeout\" is too long, this master failover process  will be long due to long rpc timeout from dead master. if so, could we separate with 2 options, \"hbase.rpc.timeout\" is still  for hbase client, while \"hbase.rpc.internal.timeout\" was for this  regionserver/master rpc channel, which could be set shorted value  without affect real client rpc timeout value? ",
        "label": 255
    },
    {
        "text": "hmaster does not go down while splitting logs even if explicit shutdown is called   when master starts up and tries to do splitlog, in case of any error we try to do that infinitely in a loop until it succeeds.  but now if we get a shutdown call, inside splitlogmanager           if (stopper.isstopped()) {             log.warn(\"stopped while waiting for log splits to be completed\");             return;           } here we know that the master has stopped. as the task may not be completed now  if (batch.done != batch.installed) {       batch.isdead = true;       tot_mgr_log_split_batch_err.incrementandget();       log.warn(\"error while splitting logs in \" + logdirs +       \" installed = \" + batch.installed + \" but only \" + batch.done + \" done\");       throw new ioexception(\"error or interrupt while splitting logs in \"           + logdirs + \" task = \" + batch);     } we throw an exception. in masterfilesystem.splitlogafterstartup() we don't check if the master is stopped and we try continously. ",
        "label": 286
    },
    {
        "text": "the dataframe datasource filter is wrong  and will result in data loss or unexpected behavior  following condition will result in the same filter. it will have data loss with the current filter construction.  col1 > 4 && col2 < 3  col1 > 4 || col2 < 3 ",
        "label": 444
    },
    {
        "text": "javadoc in some filters ambiguous  the javadoc on some of the filter is somewhat confusing.  the main filter interface has methods that behave like a sieve; when filterrowkey returns true, that means that the row is filtered out (not included). many of the filter implementations work the other way around. when the condition is met the value passes (ie, the row is returned). most filters make it clear when a values passes (passing through the filter meaning the values are returned from the scan).  some are less clear in light of how the filter interface works: whilematchfilter and singlecolumnvaluefilter are examples. ",
        "label": 330
    },
    {
        "text": "update hadoop version to ga  we're still building against hadoop 3.0-beta1, while ga is recently released. we should update, hopefully no surprises. ",
        "label": 48
    },
    {
        "text": "add more counters to htablemultiplexer  add counters for retried puts, and succeeded puts in htablemultiplexer. ",
        "label": 154
    },
    {
        "text": "hfilesystem   no filesystem for scheme  hdfs   i've been seeing this with hadoop 2.0.0-alpha-snapshot and hbase 0.94.0-snapshot: 2012-05-08 15:18:00,692 fatal [regionserver:0;acer.localdomain,48307,1336515479011] regionserver.hregionserver(1674): aborting region server acer.localdomain,48307,1336515479011: unhandled exception: no filesystem for scheme: hdfs java.io.ioexception: no filesystem for scheme: hdfs at org.apache.hadoop.hbase.fs.hfilesystem.newinstancefilesystem(hfilesystem.java:146) at org.apache.hadoop.hbase.fs.hfilesystem.<init>(hfilesystem.java:75) at org.apache.hadoop.hbase.regionserver.hregionserver.handlereportfordutyresponse(hregionserver.java:973) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.handlereportfordutyresponse(minihbasecluster.java:110) at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:671) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.runregionserver(minihbasecluster.java:136) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.access$000(minihbasecluster.java:89) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver$1.run(minihbasecluster.java:120) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:357) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1212) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:616) at org.apache.hadoop.hbase.util.methods.call(methods.java:37) at org.apache.hadoop.hbase.security.user.call(user.java:586) at org.apache.hadoop.hbase.security.user.access$700(user.java:50) at org.apache.hadoop.hbase.security.user$securehadoopuser.runas(user.java:426) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.run(minihbasecluster.java:118) at java.lang.thread.run(thread.java:679) not sure precisely when it started. first i thought it might be due to hbase-5955 but i cherry picked that change over from trunk. then i got hbase-5963 out of the way. ",
        "label": 38
    },
    {
        "text": "add maven scala plugin for scaladoc  added scala-tools.org to repository (as a side effect, all common artifacts get downloaded twice now, once from apache repo and once from scala-tools)  this fixes scala:doc precommit.  the patch 'bogus-scala-change' adds a blank line to a scala file to trigger scala:doc precommit. as expected, the target failed for master and passed for the patch. ",
        "label": 48
    },
    {
        "text": "hregionserver multi  no longer calls hregion put list  when possible  this should result in a reduce performance of puts in batched mode ",
        "label": 547
    },
    {
        "text": "server vs  client side keyvalue  do we need a server-side and client-side version of keyvalue? one of the lads here was trying to write client-side code and got put off by javadoc that said \"do not use this\" (he wanted to getvalue and didn't know ahead of time what columns the result contained). this would seem to say we need javadoc for server-side and then different javadoc for client-side. holstad wants kv to be comparable. we can't make kv comparable because at least on server-side, comparator to use changes with context. if there was a server-side kv, then we could make that comparable and leave the server-side kv alone. ",
        "label": 314
    },
    {
        "text": "the default values for many configuration items in the code are not consistent with hbase default xml  such as hbase.ipc.server.callqueue.handler.factor hbase.regionserver.logroll.errors.tolerated hbase.regionserver.region.split.policy zookeeper.session.timeout hbase.client.retries.number hbase.client.max.perserver.tasks hbase.client.keyvalue.maxsize hbase.normalizer.period hbase.hstore.blockingstorefiles hbase.snapshot.restore.take.failsafe.snapshot hbase.lease.recovery.dfs.timeout hbase.rest.filter.classes hbase.http.max.threads ",
        "label": 507
    },
    {
        "text": "murmurhash does not yield the same results as the reference c  implementation when size      last rounds of murmurhash are done in reverse order. data[length - 3], data[length - 2] and data[length - 1] in the block processing the remaining bytes should be data[len_m +2], data[len_m + 1], data[len_m]. ",
        "label": 541
    },
    {
        "text": "deleted cf are not cleared if memstore contain entries  while deleting the cf dynamically (without disabling the table), cf dirs are not cleared from fs when region memstore contain entries for that cf.  since we delete the cf from fs first and then reopen the region, during reopen rs will flush the memstore content to fs. so deleted cf store will contain the memstore content for the deleted cf.  so adding back same cf will have old entries. ",
        "label": 346
    },
    {
        "text": "shell warns about already initialized constants at startup  fix these warnings printed by the shell at startup: hbase.rb:82 warning: already initialized constant endpoint_classname hbase.rb:83 warning: already initialized constant cluster_key hbase.rb:84 warning: already initialized constant table_cfs hbase.rb:86 warning: already initialized constant config hbase.rb:87 warning: already initialized constant data ",
        "label": 188
    },
    {
        "text": "upgrade zookeeper to release  zookeeper 3.4.0 has been released.  we should upgade. ",
        "label": 441
    },
    {
        "text": "print the delta between phases in the split merge compact flush transaction journals  we print the start timestamp for each phase when logging the split/merge/compact/flush transaction journals and so when debugging an operator must do the math by hand. it would be trivial to also print the delta from the start timestamp of the previous phase and helpful to do so. ",
        "label": 38
    },
    {
        "text": "testsplittransactiononcluster tests are flaky  there's a variety of tests in this class that fail occasionally.  i think this is caused by incorrect waiting for the split to finish. the local split method in the test does not wait until both daughters are online, and in some tests there's an assert following immediately that the two daughters exist. ",
        "label": 286
    },
    {
        "text": "remove deprecation annotations from mapred namespace  i guess mapred api is here to stay. we should remove these annotations and clean it up. ",
        "label": 339
    },
    {
        "text": "coprocessor classloader is replicated for all regions in the hregionserver  hbase-6308 introduced a new custom coprocessorclassloader to load the coprocessor classes and a new instance of this cl is created for each single hregion opened. this leads to oome-permgen when the number of regions go above hundres / region server.   having the table coprocessor jailed in a separate classloader is good however we should create only one for all regions of a table in each hrs. ",
        "label": 441
    },
    {
        "text": "'counter' may overflow in boundedgroupingstrategy  groupname = groupnames[counter.getandincrement() % groupnames.length]; theoretically, counter can overflow and becomes negative then causes an arrayindexoutofboundsexception. but in practice, we need 2 billions different identifiers to make this happen, and before the overflow we will run into oom because of a huge groupnamecache... so not sure if it is worth to fix ",
        "label": 308
    },
    {
        "text": "regionalreadyintransitionexception needs to give more info to avoid assignment inconsistencies  seeing some of the recent issues in region assignment, regionalreadyintransitionexception is one reason after which the region assignment may or may not happen(in the sense we need to wait for the tm to assign).  in hbase-6317 we got one problem due to regionalreadyintransitionexception on master restart.  consider the following case, due to some reason like master restart or external assign call, we try to assign a region that is already getting opened in a rs.  now the next call to assign has already changed the state of the znode and so the current assign that is going on the rs is affected and it fails. the second assignment that started also fails getting raite exception. finally both assignments not carrying on. idea is to find whether any such raite exception can be retried or not.  here again we have following cases like where  -> the znode is yet to transitioned from offline to opening in rs  -> rs may be in the step of openregion.  -> rs may be trying to transition opening to opened.  -> rs is yet to add to online regions in the rs side. here in openregion() and updatemeta() any failures we are moving the znode to failed_open. so in these cases getting an raite should be ok. but in other cases the assignment is stopped.  the idea is to just add the current state of the region assignment in the rit map in the rs side and using that info we can determine whether the assignment can be retried or not on getting an raite. considering the current work going on in am, pls do share if this is needed atleast in the 0.92/0.94 versions? ",
        "label": 543
    },
    {
        "text": "replicationsource dies reading the peer's id  this is what i saw: 2012-07-01 05:04:01,638 error org.apache.hadoop.hbase.replication.regionserver.replicationsource: closing source 8 because an error occurred: could not read peer's cluster id org.apache.zookeeper.keeperexception$sessionexpiredexception: keepererrorcode = session expired for /va1-backup/hbaseid         at org.apache.zookeeper.keeperexception.create(keeperexception.java:127)         at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)         at org.apache.zookeeper.zookeeper.exists(zookeeper.java:1021)         at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:154)         at org.apache.hadoop.hbase.zookeeper.zkutil.checkexists(zkutil.java:259)         at org.apache.hadoop.hbase.zookeeper.clusterid.readclusteridznode(clusterid.java:61)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:253) the session should just be reopened. ",
        "label": 229
    },
    {
        "text": "struct and structiterator should properly handle trailing nulls  for a composite row key, phoenix strips off trailing null columns values in the row key. the reason this is important is that then new nullable row key columns can be added to a schema without requiring any data upgrade to existing rows. otherwise, adding new row key columns to the end of a schema becomes extremely cumbersome, as you'd need to delete all existing rows and add them back with a row key that includes a null value. rather than phoenix needing to modify the iteration code everywhere (as nick dimiduk outlined here: https://issues.apache.org/jira/browse/hbase-8693?focusedcommentid=13744499&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13744499), it'd be better if structiterator handled this out-of-the-box. otherwise, if phoenix has to specialize this, we'd lose the interop piece which is the justification for switching our type system to this new one in the first place. ",
        "label": 339
    },
    {
        "text": " windows  provide hbase scripts in order to start hbase on windows in a single user mode  provide .cmd scripts in order to start hbase on windows in a single user mode ",
        "label": 421
    },
    {
        "text": "add optional log roll interval so that log files are garbage collected  while optional cache flushes will increase the sequence id in the region server's log file, if the region server is basically idle, the log will not get rolled and consequently old log files will not get garbage collected. ",
        "label": 241
    },
    {
        "text": "hostname returned by tableinputformatbase reversedns contains trailing period  tableinputformatbase.reversedns makes a call to org.apache.hadoop.net.dns.reversedns which (at least in hadoop 1.0.4) returns a ptr record. ptr records contain a trailing period, which then shows up in the input split location causing the jobtracker to incorrectly match map jobs to data-local map slots. ",
        "label": 379
    },
    {
        "text": " amv2  if master gives open to another  if original eventually succeeds  master will kill it  if a rs is slow to open a region, the master will give the region to another to open it (in this case, was a massive set of edits to process and a load of storefiles to open...). should the original rs succeed with its open eventually, on reporting the master the successful open, the master currently kills the rs because the region is supposed to be elsewhere. this is an easy fix. the rs does not fully open a region until master gives it the go so just close the region if master rejects the open.... see '6.1.1 if master gives region to another to open, old rs will be kill itself on reject by master; easy fix!' in https://docs.google.com/document/d/1evka7fhdeoj1-9o8yzcotaqbv0u0bblblcczvsin69g/edit#heading=h.qtfojp9774h ",
        "label": 459
    },
    {
        "text": "entire row deletes not stored in row col bloom  if the user issues a row delete on an family with row+col blooms, that information is not currently detected by shouldseek(). possible known solutions are: 1. adding row as bloom filter key on row delete, shouldseek() should do both a row & row+col query for row+col filters.  2. keep delete information in a separate storage element. #1 seems like the best solution, but need to investigate further and fix this problem. ",
        "label": 23
    },
    {
        "text": "fix issue about rsgroupbasedloadbalancer roundrobinassignment where bogus server name is involved in two groups  if two groups both include bogus_server_name, assignments.putall will overwrite the previous data ",
        "label": 521
    },
    {
        "text": "minizookeepercluster should close zkdatabase when shutdown zookeeperservers  hbase-6820 points out the problem but not fix completely. killcurrentactivezookeeperserver() and killonebackupzookeeperserver() will shutdown the zookeeperserver and need to close zkdatabase as well. ",
        "label": 520
    },
    {
        "text": "multitableinputformatbase getsplits creates a connection per table  doesn't have to. connection setup costs. shared connection could save on hbase:meta hits (perhaps). ",
        "label": 314
    },
    {
        "text": "port zk multi support from hbase to  hbase-6775 adds support for zk.multi zkutil and uses it for the 0.92/0.94 compatibility fix implemented in hbase-6710. zk.multi support is most likely useful in 0.96, but since hbase-6710 is not relevant for 0.96, perhaps we should find another use case first before we port. ",
        "label": 199
    },
    {
        "text": "remove unneeded synchronized block from hfilev2 warning in branch  the below code block starts at line 277 of hfilewriterv2.java. class-level synchronization in a heavily used code path has a demonstrably significant negative effect on performance. i tested forcing a major compaction with 18 compaction threads per node; removing the synchronization resulted in an order of magnitude performance increase, with the bottleneck then being at the disks (where i want it to be).     synchronized (hfilewriterv2.class) {       if (warn_cell_with_tags && getfilecontext().isincludestags()) {         log.warn(\"a minimum hfile version of \" + hfile.min_format_version_with_tags           + \" is required to support cell attributes/tags. consider setting \"           + hfile.format_version_key + \" accordingly.\");         warn_cell_with_tags = false;       }     } ",
        "label": 373
    },
    {
        "text": "refactor coprocessor compaction api  after hbase-3797, the compaction logic flow has been significantly altered. because of this, the current compaction coprocessor api is insufficient for gaining full insight into compaction requests/results. refactor coprocessor api after hbase-3797 is committed to be more extensible and increase visibility. ",
        "label": 180
    },
    {
        "text": "remove methods deprecated in from trunk and  remove methods deprecated in 0.90 from codebase. i took a quick look. the messy bit is thrift referring to old stuff; will take a little work to do the convertion over to the new methods. ",
        "label": 248
    },
    {
        "text": "shaded dependencies for hbase testing util  folks that make use of our shaded client but then want to test things using the hbase-testing-util end up getting all of our dependencies again in the test scope. ",
        "label": 60
    },
    {
        "text": "compaction fails if bloomfilters are enabled  from thibaut up on the list. as soon as hbase tries to compact the table, the following exception appears in the logfile: (other compactations also work fine without any errors) 2008-11-30 00:55:57,769 error  org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region mytable,,1228002541526  java.lang.illegalargumentexception: maxvalue must be > 0  at org.onelab.filter.hashfunction.<init>(hashfunction.java:84)  at org.onelab.filter.filter.<init>(filter.java:97)  at org.onelab.filter.bloomfilter.<init>(bloomfilter.java:102)  at org.apache.hadoop.hbase.regionserver.hstorefile$bloomfiltermapfile$writer.<init>(hstorefile.java:829)  at org.apache.hadoop.hbase.regionserver.hstorefile.getwriter(hstorefile.java:436)  at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:889)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:902)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:860)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:83) because the region cannot compact and/or split, it is soon dead after (re)assignment. ",
        "label": 314
    },
    {
        "text": "basescanner results can't be trusted at all  related to hbase   gario this morning got a few double-assignments doing rolling restart.    1.       sep 29 15:18:24 hnn0.int.meetup.com [hbase] 2009-09-29 15:18:24,010 [ipc server handler 177 on 60000] info  org.apache.hadoop.hbase.master.servermanager  - processing msg_report_close: memberfeedindex,8828808,1252839447949 from hdn8.int.meetup.com,60020,1253572119095    2.       sep 29 15:18:39 hnn0.int.meetup.com [hbase] 2009-09-29 15:18:39,369 [ipc server handler 4 on 60000] info  org.apache.hadoop.hbase.master.regionmanager  - assigning region memberfeedindex,8828808,1252839447949 to hdn7.int.meetup.com,60020,1254251818253    3.       sep 29 15:18:42 hnn0.int.meetup.com [hbase] 2009-09-29 15:18:42,399 [ipc server handler 32 on 60000] info  org.apache.hadoop.hbase.master.servermanager  - processing msg_report_process_open: memberfeedindex,8828808,1252839447949 from hdn7.int.meetup.com,60020,1254251818253; 3 of 19    4.       sep 29 15:18:42 hnn0.int.meetup.com [hbase] 2009-09-29 15:18:42,400 [ipc server handler 32 on 60000] info  org.apache.hadoop.hbase.master.servermanager  - processing msg_report_open: memberfeedindex,8828808,1252839447949 from hdn7.int.meetup.com,60020,1254251818253; 13 of 19    5.       sep 29 15:19:01 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:01,195 [hmaster] info  org.apache.hadoop.hbase.master.regionserveroperation  - memberfeedindex,8828808,1252839447949 open on 192.168.60.213:60020    6.       sep 29 15:19:01 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:01,196 [hmaster] info  org.apache.hadoop.hbase.master.regionserveroperation  - updated row memberfeedindex,8828808,1252839447949 in region .meta.,,1 with startcode=1254251818253, server=192.168.60.213:60020    7.       sep 29 15:19:01 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:01,230 [regionmanager.metascanner] debug org.apache.hadoop.hbase.master.basescanner  - current assignment of memberfeedindex,8828808,1252839447949 is not valid;  serveraddress=192.168.60.214:60020, startcode=1253572119095 unknown.    8.       sep 29 15:19:03 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:03,059 [ipc server handler 88 on 60000] info  org.apache.hadoop.hbase.master.regionmanager  - assigning region memberfeedindex,8828808,1252839447949 to hdn8.int.meetup.com,60020,1254251940990    9.       sep 29 15:19:06 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:06,089 [ipc server handler 97 on 60000] info  org.apache.hadoop.hbase.master.servermanager  - processing msg_report_open: memberfeedindex,8828808,1252839447949 from hdn8.int.meetup.com,60020,1254251940990; 10 of 19   10.       sep 29 15:19:06 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:06,091 [hmaster] info  org.apache.hadoop.hbase.master.regionserveroperation  - memberfeedindex,8828808,1252839447949 open on 192.168.60.214:60020   11.       sep 29 15:19:06 hnn0.int.meetup.com [hbase] 2009-09-29 15:19:06,092 [hmaster] info  org.apache.hadoop.hbase.master.regionserveroperation  - updated row memberfeedindex,8828808,1252839447949 in region .meta.,,1 with startcode=1254251940990, server=192.168.60.214:60020 there's 35ms between update of row and ruling of region being unassigned. hbase-1784 added a get on .meta. row before ruling a region unassigned iff the server cell was empty. need to do this get every time. basescanner can't be trusted. its info is stale. need to hurry up with 0.21 rewrite. ",
        "label": 314
    },
    {
        "text": "process rit and master restart may remove an online server considering it as a dead server  if on master restart it finds the root/meta to be in rit state, master tries to assign the root region through processrit. master will trigger the assignment and next will try to verify the root region location.  root region location verification is done seeing if the rs has the region in its online list.  if the master triggered assignment has not yet been completed in rs then the verify root region location will fail.  because it failed splitlogandexpireifonline(currentrootserver); we do split log and also remove the server from online server list. ideally here there is nothing to do in splitlog as no region server was restarted. so master, though the server is online, master just invalidates the region server.  in a special case, if i have only one rs then my cluster will become non operative. ",
        "label": 544
    },
    {
        "text": "addendum to pluggable rpcscheduler  this patch fixes the review comments from michael stack and a small fix: make rpcscheduler fully pluggable. one can write his/her own implementation and add it to classpath and specify it by config \"hbase.region.server.rpc.scheduler.factory.class\". add unit tests and fix that rpcscheduler.stop is not called (discovered by tests) ",
        "label": 92
    },
    {
        "text": "sparksql scan operation doesn't work on kerberos cluster  i was using the hbase spark module at a client with kerberos and i ran into an issue with the scan. i made a fix for the client but we need to put it back into hbase. i will attach my solution, but it has a major problem. i had to over ride a protected class in spark. i will need help to decover a better approach ",
        "label": 444
    },
    {
        "text": "createtable splitkeys  should be synchronous  currently, there is a bug where createtable will return once the first region in the table has gone online, but this is supposed to be a synchronized operation. ",
        "label": 441
    },
    {
        "text": "nullpointerexception in secureclient when call is cleaned up due to rpc timeout  we find nullpointexception when using secureclient to access hbase. from the source code, we find that receiveresponse() will not check call != null before invoke call.setvalue(...)(line 378 and 380 in secureclient.java). however, as explained in receiveresponse() of hbaseclient, the call may have been cleaned up due to rpc timeout; therefore, it should be better to check call != null before invoke call.setvalue(...) in secureclient.  the exception stack trace is :   2013-01-28 12:11:20,060 [request-queuepool-10-thread-1] warn  org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation - failed all from region=sms_message,98,1358995149987.0c18f72df3f3f398f9f6dc83fe65afad., hostname=10.20.2.72, port=11600     2013-01-28 12:11:20,060 [request-queuepool-10-thread-1] warn  org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation - failed all from region=sms_message,98,1358995149987.0c18f72df3f3f398f9f6dc83fe65afad., hostname=10.20.2.72, port=11600     java.util.concurrent.executionexception: java.io.ioexception: call to 10.20.2.72/10.20.2.72:11600 failed on local exception: java.io.ioexception: unexpected exception receiving call responses       at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)       at java.util.concurrent.futuretask.get(futuretask.java:83)       at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.processbatchcallback(hconnectionmanager.java:1544)       at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.processbatch(hconnectionmanager.java:1396)       at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:937)       at org.apache.hadoop.hbase.client.htable.doput(htable.java:777)       at org.apache.hadoop.hbase.client.htable.put(htable.java:752)       at org.apache.hadoop.hbase.client.htablepool$pooledhtable.put(htablepool.java:397)       at com.xiaomi.infra.hbase.business.dao.sms.smsmessage.updatestatus(smsmessage.java:245)       at com.xiaomi.infra.hbase.business.client.sms.hmessagedao.doupdatemessage(hmessagedao.java:165)       at sun.reflect.generatedmethodaccessor6.invoke(unknown source)       at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)       at java.lang.reflect.method.invoke(method.java:597)       at com.xiaomi.infra.hbase.business.client.asyncdaowrapper.wrapperrunmethod(asyncdaowrapper.java:97)       at com.xiaomi.infra.hbase.business.client.hbaserequest.run(asyncdaowrapper.java:135)       at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)       at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)       at java.lang.thread.run(thread.java:662)     caused by: java.io.ioexception: call to 10.20.2.72/10.20.2.72:11600 failed on local exception: java.io.ioexception: unexpected exception receiving call responses       at org.apache.hadoop.hbase.ipc.hbaseclient.wrapexception(hbaseclient.java:1056)       at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:1025)       at org.apache.hadoop.hbase.ipc.securerpcengine$invoker.invoke(securerpcengine.java:165)       at $proxy7.multi(unknown source)       at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3$1.call(hconnectionmanager.java:1373)       at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3$1.call(hconnectionmanager.java:1)       at org.apache.hadoop.hbase.client.servercallable.withoutretries(servercallable.java:210)       at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3.call(hconnectionmanager.java:1380)       at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$3.call(hconnectionmanager.java:1)       at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)       at java.util.concurrent.futuretask.run(futuretask.java:138)       ... 3 more     caused by: java.io.ioexception: unexpected exception receiving call responses       at org.apache.hadoop.hbase.ipc.hbaseclient$connection.run(hbaseclient.java:610)     caused by: java.lang.nullpointerexception       at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.receiveresponse(secureclient.java:378)       at org.apache.hadoop.hbase.ipc.hbaseclient$connection.run(hbaseclient.java:606) ",
        "label": 238
    },
    {
        "text": "droppedsnapshotexception when flushing memstore after a datanode dies  a dead datanode in the cluster can lead to multiple hregionserver failures and corrupted data. the hregionserver failures can be reproduced consistently on a 7 machines cluster with approx 2000 regions. steps to reproduce the easiest and safest way is to reproduce it for the .meta. table, however it will work with any table. locate a datanode that stores the .meta. files and kill -9 it.   in order to get multiple writes to the .meta. table bring up or shut down a region server this will eventually cause a flush on the memstore 2009-09-25 09:26:17,775 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on .meta.,demo__assets,asset_283132172,1252898166036,1253265069920  2009-09-25 09:26:17,775 debug org.apache.hadoop.hbase.regionserver.hregion: started memstore flush for region .meta.,demo__assets,asset_283132172,1252898166036,1253265069920. current region memstore si  ze 16.3k  2009-09-25 09:26:17,791 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.io.ioexception: bad connect ack with firstbadlink 10.72.79.108:50010  2009-09-25 09:26:17,791 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_-8767099282771605606_176852 the dfsclient will retry for 3 times, but there's a high chance it will try on the same failed datanode (it takes around 10 minutes for dead datanode to be removed from cluster) 2009-09-25 09:26:41,810 warn org.apache.hadoop.hdfs.dfsclient: datastreamer exception: java.io.ioexception: unable to create new block.  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2814)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:2078)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2264) 2009-09-25 09:26:41,810 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_5317304716016587434_176852 bad datanode[2] nodes == null  2009-09-25 09:26:41,810 warn org.apache.hadoop.hdfs.dfsclient: could not get block locations. source file \"/hbase/.meta./225980069/info/5573114819456511457\" - aborting...  2009-09-25 09:26:41,810 fatal org.apache.hadoop.hbase.regionserver.memstoreflusher: replay of hlog required. forcing server shutdown  org.apache.hadoop.hbase.droppedsnapshotexception: region: .meta.,demo__assets,asset_283132172,1252898166036,1253265069920  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:942)  at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:835)  at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushregion(memstoreflusher.java:241)  at org.apache.hadoop.hbase.regionserver.memstoreflusher.run(memstoreflusher.java:149)  caused by: java.io.ioexception: bad connect ack with firstbadlink 10.72.79.108:50010  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2872)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2795)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:2078)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2264) after the hregionserver shuts down itself the regions will be reassigned however you might hit this 2009-09-26 08:04:23,646 info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: .meta.,demo__assets,asset_283132172,1252898166036,1253265069920  2009-09-26 08:04:23,684 warn org.apache.hadoop.hbase.regionserver.store: skipping hdfs://b0:9000/hbase/.meta./225980069/historian/1432202951743803786 because its empty. hbase-646 data loss?  ...  2009-09-26 08:04:23,776 info org.apache.hadoop.hbase.regionserver.hregion: region .meta.,demo__assets,asset_283132172,1252898166036,1253265069920/225980069 available; sequence id is 1331458484 we ended up with corrupted data in .meta. \"info:server\" after master got confirmation that it was updated from the hregionserver that got droppedsnapshotexception since after a cluster restart server:info will be correct, .meta. is safer to test with. also to detect data corruption you can just scan .meta. get the start key for each region and attempt to retrieve it from the corresponding table. if .meta. is corrupted you get a notservingregionexception. this issue is related to https://issues.apache.org/jira/browse/hdfs-630 i attached a patch for hdfs-630 https://issues.apache.org/jira/secure/attachment/12420919/hdfs-630.patch that fixes this problem. ",
        "label": 113
    },
    {
        "text": " visibilitycontroller  integration test for labeled data set mixing and filtered excise  create an integration test for the visibilitycontroller that:  1. create several tables of test data  2. assign a set of auths to each table. label all entries in the table with appropriate visibility expressions. insure that some data in every table overlaps with data in other tables at common row/family/qualifier coordinates. generate data like itbll so we can verify all data present later.  3. mix the data from the different tables into a new common table  4. verify for each set of auths defined in step #2 that all entries found in the source table can be found in the common table. like the itbll verification step but done n times for each set of auths defined in step #2.  5. choose one of the source tables. get its set of auths. perform a deletion with visibility expression from the common table using those auths.  6. verify that no data in the common table with the auth set chosen in #5 remains. a simple row count with the set of auths chosen in #5 that should return 0. ",
        "label": 544
    },
    {
        "text": "remove superfluous methods from string class  remove isempty method remove repeat use the apache commons implementations instead. ",
        "label": 130
    },
    {
        "text": "compaction at the granularity of a column family  currently, when compactions are requested, they are being done at a granularity of a region. so, when a particular column-family store  exceeds the threshold, for example, we go and perform the compaction for the entire region \u2013 consisting of all the column-families. would like to add support for performing compactions per column-family. ",
        "label": 341
    },
    {
        "text": "accept encoded region name in compacting spliting region from shell  sometimes, the region name has binary characters. when compacting/splitting it from shell, the region name is not recognized. if we can support encoded region name, it will make things easier. ",
        "label": 242
    },
    {
        "text": "enhance chaosmonkeyrunner with interruptibility  currently chaosmonkeyrunner performs looping unconditionally:     while (true) {// loop here until got killed       thread.sleep(10000);     } when chaosmonkeyrunner is invoked programmatically, it is desirable to add interruptibility to the runner so that the caller can manage its lifetime. another enhancement is to allow passing the path to hbase-site.xml where chaos monkey parameters are specified.  this is useful when the underlying hbase-site.xml is not on classpath. ",
        "label": 441
    },
    {
        "text": "statistics per column family per region  originating from this discussion on the dev list: http://search-hadoop.com/m/codku1urovs/simple+stastics+per+region/v=plain essentially, we should have built-in statistics gathering for hbase tables. this allows clients to have a better understanding of the distribution of keys within a table and a given region. we could also surface this information via the ui. there are a couple different proposals from the email, the overview is this:  we add in something on compactions that gathers stats about the keys that are written and then we surface them to a table. the possible proposals include: how to implement it? 1. coprocessors - advantage - it easily plugs in and people could pretty easily add their own statistics. disadvantage - ui elements would also require this, we get into dependent loading, which leads down the osgi path. also, these cps need to be installed after all the other cps on compaction to ensure they see exactly what gets written (doable, but a pain) 2. built into hbase as a custom scanner advantage - always goes in the right place and no need to muck about with loading cps etc. disadvantage - less pluggable, at least for the initial cut where do we store data? 1. .meta. advantage - its an existing table, so we can jam it into another cf there disadvantage - this would make meta much larger, possibly leading to splits and will make it much harder for other processes to read the info 2. a new stats table advantage - cleanly separates out the information from meta disadvantage - should use a 'system table' idea to prevent accidental deletion, manipulation by arbitrary clients, but still allow clients to read it. once we have this framework, we can then move to an actual implementation of various statistics. ",
        "label": 236
    },
    {
        "text": "npe when trying to read regioninfo from  meta   this is an old issue perhaps in a new guise. from the list, sebastien bauer reports: > 2010-10-25 08:13:01,690 error > org.apache.hadoop.hbase.master.catalogjanitor: caught exception > java.lang.nullpointerexception > 2010-10-25 08:13:24,385 info > org.apache.hadoop.hbase.master.servermanager: regionservers=2, > averageload=2538 > 2010-10-23 20:16:17,890 debug >  org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation: >  cached location for .meta.,,1.1028785192 is >  db2a.goldenline.pl:60020 >  2010-10-23 20:16:18,432 fatal org.apache.hadoop.hbase.master.hmaster: >  unhandled exception. starting >  shutdown. > >  java.lang.nullpointerexception > >        at >  org.apache.hadoop.hbase.util.writables.getwritable(writables.java:75) > >        at >  org.apache.hadoop.hbase.util.writables.gethregioninfo(writables.java:119) > >        at >  org.apache.hadoop.hbase.client.metascanner$1.processrow(metascanner.java:188) > >        at >  org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:157) > >        at >  org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:69) > >        at >  org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:54) > >        at >  org.apache.hadoop.hbase.client.metascanner.listallregions(metascanner.java:195) > >       at >  org.apache.hadoop.hbase.master.assignmentmanager.assignalluserregions(assignmentmanager.java:1048) > >        at >  org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:379) > >        at >  org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:265) > >  2010-10-23 20:16:18,433 info org.apache.hadoop.hbase.master.hmaster: >  aborting > >  2010-10-23 20:16:18,433 debug org.apache.hadoop.hbase.master.hmaster: >  stopping service threads i think he has an old master... checking. ",
        "label": 314
    },
    {
        "text": "table jsp doesn't handle entries in meta without server info  i have a table where one of the rows in meta is missing server info. table.jsp doesn't check for this case, so it throws an npe, which is then dumped to the .out log rather than anywhere where someone might find it. we should catch this case. ",
        "label": 453
    },
    {
        "text": "rowlock fails when used with indextable  the following exception is thrown when using rowlock to update a row in an indexedtable:  [junit] java.io.ioexception: java.io.ioexception: invalid row lock  [junit] at org.apache.hadoop.hbase.regionserver.hregion.getlock(hregion.java:1640)  [junit] at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1244)  [junit] at org.apache.hadoop.hbase.regionserver.tableindexed.indexedregion.put(indexedregion.java:97)  [junit] at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1216)  [junit] at org.apache.hadoop.hbase.regionserver.hregionserver.put(hregionserver.java:1818)  [junit] at sun.reflect.generatedmethodaccessor23.invoke(unknown source)  [junit] at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  [junit] at java.lang.reflect.method.invoke(method.java:597)  [junit] at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:650)  [junit] at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:915) note #1: line numbers in stacktrace may not make sense because i've been hacking in loads of debug info. note #2: i attaching a fix which includes unit tests ",
        "label": 272
    },
    {
        "text": "get rid of tabledescriptor getconfiguration  we should substitute td#value for the td#configuration. the td#configuration is rarely used in code base. ",
        "label": 98
    },
    {
        "text": "compactionrequest tostring  may throw nullpointerexception  i found the following in hbase-server/target/surefire-reports/org.apache.hadoop.hbase.util.testmergetable-output.txt : 2014-03-08 01:22:35,311 info  [ipc server handler 0 on 39151] blockmanagement.blockmanager(1009): block* addtoinvalidates: blk_1073741852_1028 127.0.0.1:58684 2014-03-08 01:22:35,312 info  [rs:0;kiyo:45971-shortcompactions-1394241753752] regionserver.hregion(1393): compaction interrupted java.io.interruptedioexception: aborting compaction of store contents in region test,,1394241738901.edbcdf3be9dd27c52b1fca1b09a5a582. because it was interrupted.         at org.apache.hadoop.hbase.regionserver.compactions.defaultcompactor.compact(defaultcompactor.java:81)         at org.apache.hadoop.hbase.regionserver.defaultstoreengine$defaultcompactioncontext.compact(defaultstoreengine.java:109)         at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:1131)         at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1390)         at org.apache.hadoop.hbase.regionserver.compactsplitthread$compactionrunner.run(compactsplitthread.java:475)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:724) 2014-03-08 01:22:35,314 debug [rs_close_region-kiyo:45971-0] regionserver.hregion(1069): updates disabled for region test,,1394241738901.edbcdf3be9dd27c52b1fca1b09a5a582. 2014-03-08 01:22:35,316 info  [storecloserthread-test,,1394241738901.edbcdf3be9dd27c52b1fca1b09a5a582.-1] regionserver.hstore(793): closed contents 2014-03-08 01:22:35,316 error [rs:0;kiyo:45971-shortcompactions-1394241753752] regionserver.compactsplitthread$compactionrunner(496): compaction failed request = regionname=test,,1394241738901.edbcdf3be9dd27c52b1fca1b09a5a582., storename=contents, filecount=7, filesize=71.3 m (10.2 m, 10.2 m, 10.2 m, 10.2 m, 10.2 m), priority=3, time=8144240699213330 java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.compactions.compactionrequest$2.apply(compactionrequest.java:213)         at org.apache.hadoop.hbase.regionserver.compactions.compactionrequest$2.apply(compactionrequest.java:211)         at com.google.common.collect.iterators$9.transform(iterators.java:845)         at com.google.common.collect.transformediterator.next(transformediterator.java:48)         at com.google.common.base.joiner.appendto(joiner.java:125)         at com.google.common.base.joiner.appendto(joiner.java:186)         at com.google.common.base.joiner.join(joiner.java:243)         at com.google.common.base.joiner.join(joiner.java:232)         at org.apache.hadoop.hbase.regionserver.compactions.compactionrequest.tostring(compactionrequest.java:204)         at java.lang.string.valueof(string.java:2854)         at java.lang.stringbuilder.append(stringbuilder.java:128)         at org.apache.hadoop.hbase.regionserver.compactsplitthread$compactionrunner.tostring(compactsplitthread.java:425)         at java.lang.string.valueof(string.java:2854)         at java.lang.stringbuilder.append(stringbuilder.java:128)         at org.apache.hadoop.hbase.regionserver.compactsplitthread$compactionrunner.run(compactsplitthread.java:477) the exception came from apply() method:           }), new function<storefile, string>() {             public string apply(storefile sf) {               return stringutils.humanreadableint(sf.getreader().length());             }           })); looks like sf.getreader() might become null when stringutils.humanreadableint() is called ",
        "label": 371
    },
    {
        "text": "hbase backup restore based on hbase snapshot  mfinally, we completed the implementation of our backup/restore solution, and would like to share with community through this jira. we are leveraging existing hbase snapshot feature, and provide a general solution to common users. our full backup is using snapshot to capture metadata locally and using exportsnapshot to move data to another cluster; the incremental backup is using offline-walplayer to backup hlogs; we also leverage global distribution rolllog and flush to improve performance; other added-on values such as convert, merge, progress report, and cli commands. so that a common user can backup hbase data without in-depth knowledge of hbase. our solution also contains some usability features for enterprise users. the detail design document and cli command will be attached in this jira. we plan to use 10~12 subtasks to share each of the following features, and document the detail implement in the subtasks: full backup : provide local and remote back/restore for a list of tables offline-walplayer to convert hlog to hfiles offline (for incremental backup) distributed logroll and distributed flush backup manifest and history incremental backup: to build on top of full backup as daily/weekly backup convert incremental backup wal files into hfiles merge several backup images into one(like merge weekly into monthly) add and remove table to and from backup image cancel a backup process backup progress status full backup based on existing snapshot -------------------------------------------------------------------------------------------------------------  below is the original description, to keep here as the history for the design and discussion back in 2013 there have been attempts in the past to come up with a viable hbase backup/restore solution (e.g., hbase-4618). recently, there are many advancements and new features in hbase, for example, filelink, snapshot, and distributed barrier procedure. this is a proposal for a backup/restore solution that utilizes these new features to achieve better performance and consistency. a common practice of backup and restore in database is to first take full baseline backup, and then periodically take incremental backup that capture the changes since the full baseline backup. hbase cluster can store massive amount data. combination of full backups with incremental backups has tremendous benefit for hbase as well. the following is a typical scenario for full and incremental backup. 1. the user takes a full backup of a table or a set of tables in hbase. 2. the user schedules periodical incremental backups to capture the changes from the full backup, or from last incremental backup. 3. the user needs to restore table data to a past point of time. 4. the full backup is restored to the table(s) or to different table name(s). then the incremental backups that are up to the desired point in time are applied on top of the full backup. we would support the following key features and capabilities. full backup uses hbase snapshot to capture hfiles. use hbase wals to capture incremental changes, but we use bulk load of hfiles for fast incremental restore. support single table or a set of tables, and column family level backup and restore. restore to different table names. support adding additional tables or cf to backup set without interruption of incremental backup schedule. support rollup/combining of incremental backups into longer period and bigger incremental backups. unified command line interface for all the above. the solution will support hbase backup to filesystem, either on the same cluster or across clusters. it has the flexibility to support backup to other devices and servers in the future. ",
        "label": 478
    },
    {
        "text": "update thrift to  the new version of thrift is 0.7.0 and it has features and bug fixes that could be useful to include in the next release of hbase. ",
        "label": 331
    },
    {
        "text": "flaky exclusion doesn't appear to work in precommit  yesterday we started defaulting the precommit parameter for the flaky test list to point to the job on builds.a.o. looks like the personality is ignoring it. example build that's marked to keep: https://builds.apache.org/job/precommit-hbase-build/7646/ (search for 'running unit tests' to skip to the right part of the console') should add some more debug output in there too. ",
        "label": 48
    },
    {
        "text": "add ability for tests to override server side timestamp setting  currenttimemillis   many of our tests use client apis which do not set explicit stamps. this creates weird timing issues with tests running on different systems because sometimes a set of operations happens in the same millisecond and other times they do not. we should have a way for a test to specify it's own way of generating the timestamps (for example, could always increment by 1 ensuring forward progression in time). ",
        "label": 122
    },
    {
        "text": " stargate  columns not handle by scan  there is an issue with scannermodel only adding the column families to the scan model, not actual columns. easy fix. ",
        "label": 285
    },
    {
        "text": "log splitting status should always be closed  with distributed log replay enabled by default, i ran into an issue that log splitting hasn't completed after 13 hours. it seems to hang somewhere. ",
        "label": 233
    },
    {
        "text": "hbase is not deleting the cell when a put with a keyvalue  keyvalue type delete is submitted  code executed:     @test     public void testhbaseputdeletecell() throws exception {         tablename tablename = tablename.valueof(\"my_test\");         configuration configuration = hbaseconfiguration.create();         htableinterface table = new htable(configuration, tablename);         final string rowkey = \"12345\";         final byte[] familly = bytes.tobytes(\"default\");         // put one row         put put = new put(bytes.tobytes(rowkey));         put.add(familly, bytes.tobytes(\"a\"), bytes.tobytes(\"a\"));         put.add(familly, bytes.tobytes(\"b\"), bytes.tobytes(\"b\"));         put.add(familly, bytes.tobytes(\"c\"), bytes.tobytes(\"c\"));         table.put(put);         // get row back and assert the values         get get = new get(bytes.tobytes(rowkey));         result result = table.get(get);         assert.istrue(bytes.tostring(result.getvalue(familly, bytes.tobytes(\"a\"))).equals(\"a\"), \"column a value should be a\");         assert.istrue(bytes.tostring(result.getvalue(familly, bytes.tobytes(\"b\"))).equals(\"b\"), \"column b value should be b\");         assert.istrue(bytes.tostring(result.getvalue(familly, bytes.tobytes(\"c\"))).equals(\"c\"), \"column c value should be c\");         // put the same row again with c column deleted         put = new put(bytes.tobytes(rowkey));         put.add(familly, bytes.tobytes(\"a\"), bytes.tobytes(\"a\"));         put.add(familly, bytes.tobytes(\"b\"), bytes.tobytes(\"b\"));         put.add(new keyvalue(bytes.tobytes(rowkey), familly, bytes.tobytes(\"c\"), hconstants.latest_timestamp, keyvalue.type.deletecolumn));         table.put(put);         // get row back and assert the values         get = new get(bytes.tobytes(rowkey));         result = table.get(get);         assert.istrue(bytes.tostring(result.getvalue(familly, bytes.tobytes(\"a\"))).equals(\"a\"), \"column a value should be a\");         assert.istrue(bytes.tostring(result.getvalue(familly, bytes.tobytes(\"b\"))).equals(\"b\"), \"column a value should be b\");         assert.istrue(result.getvalue(familly, bytes.tobytes(\"c\")) == null, \"column c should not exists\");     } this assertion fails, the cell is not deleted but rather the value is empty: hbase(main):029:0> scan 'my_test' row                                                   column+cell                                                                                                                                                   12345                                                column=default:a, timestamp=1408473082290, value=a                                                                                                            12345                                                column=default:b, timestamp=1408473082290, value=b                                                                                                            12345                                                column=default:c, timestamp=1408473082290, value=       this behavior is different than previous 4.8.x cloudera version and is currently corrupting all hive queries involving is null or is not null operators on the columns mapped to hbase ",
        "label": 423
    },
    {
        "text": "ensure artifacts in project dist area include required md5 file  from the 0.98.19rc0 thread: sean busbey [1]: asf policy requires that each file hosted in the project dist  space have a file with just the md5 sum in a file named after the  original with \".md5\" as a suffix. (having an additional file with all  the checksums is a good practice, imo.) i brought this up in our last  round of rcs as well. i don't want to hold up this vote, but i plan to  start voting -1 on future rcs that don't include md5 files. relevant policy:  http://www.apache.org/dev/release-distribution.html#sigs-and-sums andrew kyle purtell our release documentation (https://hbase.apache.org/book.html#releasing)  says we should generate sums like so: for i in *.tar.gz; do echo $i; gpg --print-mds $i > $i.mds ; done the make_rc.sh script also encodes the same. let's fix. ",
        "label": 339
    },
    {
        "text": "fix tests that carry meta in master that were disabled by proc v2 am in hbase  the following tests were disabled as part of core proc-v2 am in hbase-14614 testregionrebalancing is disabled because doesn't consider the fact that master carries system tables only (fix of average in regionstates brought out the issue). disabled testmetaaddresschange in testmetawithreplicas because presumes can move meta... you can't testasynctablegetmultithreaded wants to move hbase:meta...balancer does npes. amv2 won't let you move hbase:meta off master. testmasterfailover needs to be rewritten for amv2. it uses tricks not ordained when up on amv2. the test is also hobbled by fact that we religiously enforce that only master can carry meta, something we are lose about in old am this jira is tracking the work to enable/modify them. ",
        "label": 478
    },
    {
        "text": "there are a large number of java warnings in hbase  there are a large number of java warnings in the current hbase code base including: exceptions that do not define serialversionuid classes that use the raw type writablecomparable instead of writablecomparable<t> classes or interfaces that declare public members that are not a part of the public api. in this case they should be moved to a place where their visibility needs not be public. additionally, there are a number of classes that declare public members that need not be. make them protected or private or default as needed methods that have unnecessary else clauses potential null pointer access inner classes that are public that should be default or protected (e.g. regionhistoryinformation) assignment to an input parameter ",
        "label": 167
    },
    {
        "text": "shell  alter  should do a single modifytable operation  when performing an \"alter\" on multiple column families in a table, then shell will perform a separate admin.modifycolumn() call for each column family being modified, with all of the table regions being bulk-reopened each time. it would be much better to simply apply all the changes to the table descriptor, then do a single call to admin.modifytable(). ",
        "label": 308
    },
    {
        "text": "fix testmigrate up on hudson  its hanging on hudson again. caught a threaddump. its that old waiting on a vanished unix process... no hbase threads hanging out. i tried adding relocateregion just before taking out scan in verify. that was good for fixing the first region in the table. we hung when we tried to get second region. it was trying to go to old address. ",
        "label": 314
    },
    {
        "text": "remove the thread dump link on info pages  the debug dump page has the thread dump. fewer links on the page would make things a little clearer for new users. ",
        "label": 154
    },
    {
        "text": "the batchupdate class provides  put col  cell  and delete col  but no get col   batchupdate has no read access to the cell data that it holds currently. it would be nice to add a get() method to compliment the put() method. i was thinking something like: public byte[] get(byte[] column);  public byte[] get(string column); should be fairly easy to patch. ill try to get one in here soon. if not by today, then tomorrow. ",
        "label": 241
    },
    {
        "text": "add debug output for when balancer makes bad balance  balancer had assertions at end of the balancecluster method. these assertions trigger on occasion \u2013 just did for me and did previously for j-d \u2013 only there's no data to analyze when it fails. add logging data on balancer input and summary. ",
        "label": 314
    },
    {
        "text": "testhregiononcluster and testsplittransactiononcluster are racy with hbaseadmin move   seen in testhregiononcluster and testsplittransactiononcluster.  stack in both cases it: java.lang.reflect.undeclaredthrowableexception at $proxy20.move(unknown source) at org.apache.hadoop.hbase.client.hbaseadmin.move(hbaseadmin.java:1426) at org.apache.hadoop.hbase.regionserver.testhregiononcluster.testdatacorrectnessreplayingrecoverededits(testhregiononcluster.java:94) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:62) caused by: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hbase.unknownregionexception: cc372cfe78f796e467f6d383da1fecba at org.apache.hadoop.hbase.master.hmaster.move(hmaster.java:1141) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:1021) at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:150) ... 12 more ",
        "label": 286
    },
    {
        "text": "disable and drop of table is flakey still  just now, cheddar up on irc had table of 2k regions. a disable and drop gave him 2k rows in meta of historian info. he couldn't progress. had to make below script for him: meta = htable.new(\".meta.\") historian = \"historian:\" scanner = meta.getscanner([historian].to_java(java.lang.string)) while (result = scanner.next())       meta.deleteall(result.getrow()) end exit 0 this flakey disable/enable/drop is frustrating users. need to fix. ",
        "label": 229
    },
    {
        "text": "spark's createtable method throws an exception while the table is being split  hbaserelation.createtable checks table existence with hbaseadmin.istableavailable method here. this is unfortunate, because it can return false while splitting, so createtable will fail. it should use tableexists. ",
        "label": 363
    },
    {
        "text": "convert hstore to use only new interface methods  ",
        "label": 314
    },
    {
        "text": "change filters and getclosestrowbeforetracker to work with cells  ",
        "label": 544
    },
    {
        "text": "region server fails to properly close socket resulting in many close wait to data nodes  hbase dose not close a dead connection with the datanode.  this resulting in over 60k close_wait and at some point hbase can not connect to the datanode because too many mapped sockets from one host to another on the same port. the example below is with low close_wait count because we had to restart hbase to solve the porblem, later in time it will incease to 60-100k sockets on close_wait [root@hd2-region3 ~]# netstat -nap |grep close_wait |grep 21592 |wc -l  13156  [root@hd2-region3 ~]# ps -ef |grep 21592  root 17255 17219 0 12:26 pts/0 00:00:00 grep 21592  hbase 21592 1 17 aug29 ? 03:29:06 /usr/java/jdk1.6.0_26/bin/java -xx:onoutofmemoryerror=kill -9 %p -xmx8000m -ea -xx:+useconcmarksweepgc -xx:+cmsincrementalmode -dhbase.log.dir=/var/log/hbase -dhbase.log.file=hbase-hbase-regionserver-hd2-region3.swnet.corp.log ... ",
        "label": 53
    },
    {
        "text": "zkpermissionwatcher blocks all zk notifications  buckle up folks, we're going for a ride here. i've seeing this on a branch-2 based build, but i think the problem will affect branch-1 as well. i'm not able to easily reproduce the issue, but it will usually come up within an hour on a given cluster that i have, at which point the problem persists until an rs restart. i've been seeing the problem and paying attention for maybe two months, but i suspect it's been happening much longer than that. problem when running in a secure cluster, sometimes the zk eventthread will get stuck on a permissions update and not be able to process new notifications. this happens to also block flush and snapshot, which is how we found it. analysis the main smoking gun is seeing this in repeated jstacks: \"main-eventthread\" #43 daemon prio=5 os_prio=0 tid=0x00007f0b92644000 nid=0x6e69 waiting on condition [0x00007f0b6730f000]    java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.security.access.zkpermissionwatcher.nodechildrenchanged(zkpermissionwatcher.java:191)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:503)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:522)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:498) that sleep is a 20ms sleep in an atomicreference.compareandset loop - but it never gets past the condition.         while (!nodes.compareandset(null, nodelist)) {           try {             thread.sleep(20);           } catch (interruptedexception e) {             log.warn(\"interrupted while setting node list\", e);             thread.currentthread().interrupt();           }         } the warning never shows up in the logs, it just keeps looping and looping. the last relevant line from the watcher in logs is: 2017-08-17 21:25:12,379 debug org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: regionserver:22101-0x15df38884c80024, quorum=zk1:2181,zk2:2181,zk3:2181, baseznode=/hbase received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/acl which makes sense, because the code snippet is from permission watcher's nodechildrenchanged handler. the separate thread introduced in hbase-14370 is present, but not doing anything. and this event hasn't gotten to the part where it splits off into a thread: \"zk-permission-watcher4-thread-1\" #160 daemon prio=5 os_prio=0 tid=0x0000000001750800 nid=0x6fd9 waiting on condition [0x00007f0b5dce5000]    java.lang.thread.state: waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x00000007436ecea0> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)         at java.util.concurrent.locks.locksupport.park(locksupport.java:175)         at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:2039)         at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:442)         at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1074)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1134)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)         at java.lang.thread.run(thread.java:748) solutions there's a few approaches we can take to fix this, i think they are all complimentary. it might be useful to file subtasks or new issues for some of the solutions if they are longer term. 1. move flush and snapshot to procedurev2. this makes my proximate problem go away, but it's only relevant to branch-2 and master, and doesn't fix anything on branch-1. also, permissions updates would still get stuck, preventing future permissions updates. i think this is important long term for the robustness of the system, but not a viable short term fix. 2. add an executor to zookeeperwatcher and launch threads from there. maybe we'd want to pull the executor out of zkpw, but that's not strictly necessary and can be optimized later \u2013 if we're already threading, then adding another layer isn't a huge cost. 3. figure out the race condition or logic problem that causes nodes to be non-null above. i've tried looking at this and visual inspection isn't getting me anywhere. ",
        "label": 320
    },
    {
        "text": "testhregion should clean up test data directory upon completion  testhregion leaves some files behind in hbase-server/target/test-data directory after tests complete.  e.g. at the end of testregioninfofilecreation:     // verify that the .regioninfo file is still there     asserttrue(hregionfilesystem.region_info_file + \" should be present in the region dir\",         fs.exists(new path(regiondir, hregionfilesystem.region_info_file))); test-data directory should be cleaned upon completion. i noticed this when looping testhregion in order to reproduce test failure. ",
        "label": 191
    },
    {
        "text": "throw rowtoobigexception only for user scan get  when config hbase.table.max.rowsize, rowtoobigexception may be thrown by storescanner. but region flush/compact should catch it or throw it only for user scan. exceptions:  org.apache.hadoop.hbase.regionserver.rowtoobigexception: max row size allowed: 10485760, but row is bigger than that  at org.apache.hadoop.hbase.regionserver.storescanner.seekscanners(storescanner.java:355)  at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:276)  at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:238)  at org.apache.hadoop.hbase.regionserver.compactions.compactor.createscanner(compactor.java:403)  at org.apache.hadoop.hbase.regionserver.compactions.defaultcompactor.compact(defaultcompactor.java:95)  at org.apache.hadoop.hbase.regionserver.defaultstoreengine$defaultcompactioncontext.compact(defaultstoreengine.java:131)  at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:1211)  at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1952)  at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1774) or org.apache.hadoop.hbase.regionserver.rowtoobigexception: max row size allowed: 10485760, but the row is bigger than that.  at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:576)  at org.apache.hadoop.hbase.regionserver.storeflusher.performflush(storeflusher.java:132)  at org.apache.hadoop.hbase.regionserver.defaultstoreflusher.flushsnapshot(defaultstoreflusher.java:75)  at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:880)  at org.apache.hadoop.hbase.regionserver.hstore$storeflusherimpl.flushcache(hstore.java:2155)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcacheandcommit(hregion.java:2454)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:2193)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:2162)  at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:2053)  at org.apache.hadoop.hbase.regionserver.hregion.flush(hregion.java:1979)  at org.apache.hadoop.hbase.regionserver.testrowtoobig.testscannersseekonfewlargecells(testrowtoobig.java:101) ",
        "label": 187
    },
    {
        "text": "add missing name mark  added missing name mark ",
        "label": 86
    },
    {
        "text": " migration  addcolumn deletecolumn functionality in metautils  needed by hbase-533 ",
        "label": 314
    },
    {
        "text": "hfileprettyprinter scanned kv count always  the \"count\" variable used to print the \"scanned kv count\" is never incremented.  a local \"count\" variable in scankeysvalues() method is updated instead. ",
        "label": 309
    },
    {
        "text": "move exceptions to subpackages  some exceptions are only master-side or regionserver-side or client-side. those that are, move to subpackages. ",
        "label": 229
    },
    {
        "text": "maven remote resources plugin failure processing notice vm in hbase assembly  only seen when building 0.98 with -dhadoop.profile=1.1. happens with both jdk 6 and 7. [error] failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hbase-assembly: error rendering velocity resource. error invoking method 'get(java.lang.integer)' in java.util.arraylist at meta-inf/notice.vm[line 275, column 22]: invocationtargetexception: index: 0, size: 0 -> [help 1] from later comment by busbey:  \"use -dlicense.debug.print.included=true and then to examine the generated license file to see what transitive dependency doesn't have license information.\"\" ",
        "label": 38
    },
    {
        "text": "on split  parent region is sticking around in oldest sequenceid to region map though not online  we don't cleanup wals   here is log for a particular region: 2011-11-15 05:46:31,382 info org.apache.hadoop.hbase.regionserver.splittransaction: still waiting on the master to process the split for 8bbd7388262dc8cb1ce2cf4f04a7281d 2011-11-15 05:46:31,483 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:7003-0x1337b0b92cd000a-0x1337b0b92cd000a attempting to transition node 8bbd7388262dc8cb1ce2cf4f04a7281d from rs_zk_region_split to rs_zk_reg ion_split 2011-11-15 05:46:31,484 info org.apache.hadoop.hbase.regionserver.splitrequest: region split, meta updated, and report to master. parent=testtable,0862220095,1321335865649.8bbd7388262dc8cb1ce2cf4f04a7281d., new regions: testtab le,0862220095,1321335989689.f00c683df3182d8ef33e315f77ca539c., testtable,0892568091,1321335989689.a56ca1eff5b4401432fcba04b4e851f8.. split took 1sec 2011-11-15 05:46:37,705 debug org.apache.hadoop.hbase.regionserver.store: compacting hdfs://sv4r11s38:7000/hbase/testtable/a56ca1eff5b4401432fcba04b4e851f8/info/9ce16d8fa94e4938964c04775a6fa1a7.8bbd7388262dc8cb1ce2cf4f04a7281d- hdfs://sv4r11s38:7000/hbase/testtable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9ce16d8fa94e4938964c04775a6fa1a7-top, keycount=717559, bloomtype=none, size=711.1m 2011-11-15 05:46:37,705 debug org.apache.hadoop.hbase.regionserver.store: compacting hdfs://sv4r11s38:7000/hbase/testtable/a56ca1eff5b4401432fcba04b4e851f8/info/9213f4d7ee9b4fda857a97603a001f9e.8bbd7388262dc8cb1ce2cf4f04a7281d- hdfs://sv4r11s38:7000/hbase/testtable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9213f4d7ee9b4fda857a97603a001f9e-top, keycount=416691, bloomtype=none, size=412.9m 2011-11-15 05:46:53,090 debug org.apache.hadoop.hbase.regionserver.store: compacting hdfs://sv4r11s38:7000/hbase/testtable/f00c683df3182d8ef33e315f77ca539c/info/9ce16d8fa94e4938964c04775a6fa1a7.8bbd7388262dc8cb1ce2cf4f04a7281d- hdfs://sv4r11s38:7000/hbase/testtable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9ce16d8fa94e4938964c04775a6fa1a7-bottom, keycount=717559, bloomtype=none, size=711.1m 2011-11-15 05:46:53,090 debug org.apache.hadoop.hbase.regionserver.store: compacting hdfs://sv4r11s38:7000/hbase/testtable/f00c683df3182d8ef33e315f77ca539c/info/9213f4d7ee9b4fda857a97603a001f9e.8bbd7388262dc8cb1ce2cf4f04a7281d- hdfs://sv4r11s38:7000/hbase/testtable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9213f4d7ee9b4fda857a97603a001f9e-bottom, keycount=416691, bloomtype=none, size=412.9m 2011-11-15 05:48:00,690 debug org.apache.hadoop.hbase.regionserver.wal.hlog: found 3 hlogs to remove out of total 12; oldest outstanding sequenceid is 5699 from region 8bbd7388262dc8cb1ce2cf4f04a7281d 2011-11-15 05:57:54,083 info org.apache.hadoop.hbase.regionserver.wal.hlog: too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 8bbd7388262dc8cb1ce2cf4f04a7281d 2011-11-15 05:57:54,083 warn org.apache.hadoop.hbase.regionserver.logroller: failed to schedule flush of 8bbd7388262dc8cb1ce2cf4f04a7281dr=null, requester=null 2011-11-15 05:58:01,358 info org.apache.hadoop.hbase.regionserver.wal.hlog: too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 8bbd7388262dc8cb1ce2cf4f04a7281d 2011-11-15 05:58:01,359 warn org.apache.hadoop.hbase.regionserver.logroller: failed to schedule flush of 8bbd7388262dc8cb1ce2cf4f04a7281dr=null, requester=null ",
        "label": 314
    },
    {
        "text": "hql exceptions when no cluster to connect to  here's a couple of exceptions thrown by hql that should be fixed as we go: 08/01/24 10:39:42 info hbase.hconnectionmanager$tableservers: attempt 3 of 5 failed with <java.net.connectexception: connection refused>. retrying after sleep of 10000  08/01/24 10:39:52 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 1 time(s).08/01/24 10:39:53 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 2 time(s). 08/01/24 10:39:54 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 3 time(s).08/01/24 10:39:55 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 4 time(s). 08/01/24 10:39:56 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 5 time(s).08/01/24 10:39:57 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 6 time(s). 08/01/24 10:39:58 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 7 time(s).08/01/24 10:39:59 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 8 time(s). 08/01/24 10:40:00 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 9 time(s). 08/01/24 10:40:02 info ipc.client: retrying connect to server: localhost/127.0.0.1:60000. already tried 10 time(s). 08/01/24 10:40:03 warn hbase.hconnectionmanager$tableservers: testing for table existence threw exception org.apache.hadoop.hbase.masternotrunningexception         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.getmaster(hconnectionmanager.java:202)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locaterootregion(hconnectionmanager.java:691)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:329)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.relocateregion(hconnectionmanager.java:311)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:476)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:339)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.relocateregion(hconnectionmanager.java:311)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.listtables(hconnectionmanager.java:291)         at org.apache.hadoop.hbase.hconnectionmanager$tableservers.tableexists(hconnectionmanager.java:227)         at org.apache.hadoop.hbase.hql.createcommand.execute(createcommand.java:49)         at org.apache.hadoop.hbase.hql.hqlclient.executequery(hqlclient.java:50)         at org.apache.hadoop.hbase.shell.main(shell.java:114) exception in thread \"main\" java.lang.nullpointerexception         at org.apache.hadoop.hbase.hql.basiccommand.extracterrmsg(basiccommand.java:62)         at org.apache.hadoop.hbase.hql.basiccommand.extracterrmsg(basiccommand.java:68)         at org.apache.hadoop.hbase.hql.createcommand.execute(createcommand.java:67)         at org.apache.hadoop.hbase.hql.hqlclient.executequery(hqlclient.java:50)         at org.apache.hadoop.hbase.shell.main(shell.java:114) or hbase> create table log(uname, uid); creating table... please wait. exception in thread \"main\" java.lang.stringindexoutofboundsexception: string index out of range: -1         at java.lang.string.substring(string.java:1938)         at org.apache.hadoop.hbase.shell.basiccommand.extracterrmsg(basiccommand.java:60)         at org.apache.hadoop.hbase.shell.basiccommand.extracterrmsg(basiccommand.java:64)         at org.apache.hadoop.hbase.shell.createcommand.execute(createcommand.java:61)         at org.apache.hadoop.hbase.shell.main(shell.java:96) [hadoop@latewhatgrow-lx bin]$  ",
        "label": 314
    },
    {
        "text": " test  run mr branch jobs against hbase2 cluster  ensure this works still. run all our bundled mr tools at least. find some custom if time. ",
        "label": 252
    },
    {
        "text": "thrift idl needs update edit to match new api  and to fix bugs   talking w/ bryan, moving this out of the way of the 0.2.0 release. ",
        "label": 451
    },
    {
        "text": "fix failed shell uts  ",
        "label": 149
    },
    {
        "text": "range of delay in periodicmemstoreflusher should be configurable   when rss have a lot of regions and cfs, flushing everything within 5 minutes is not always doable. it might be interesting to be able to increase the range_of_delay. ",
        "label": 368
    },
    {
        "text": "testdefaultcompactselection test failed  hadoop.hbase.regionserver.testdefaultcompactselection  [error] failures:   [error] testdefaultcompactselection.testcompactionratio:74->testcompactionpolicy.compactequals:182->testcompactionpolicy.compactequals:201 expected:<[[4, 2, 1]]> but was:<[[]]>  [error] testdefaultcompactselection.teststuckstorecompaction:145->testcompactionpolicy.compactequals:182->testcompactionpolicy.compactequals:201 expected:<[[]30, 30, 30]> but was:<[[99, 30, ]30, 30, 30]> ",
        "label": 60
    },
    {
        "text": " append not enabled  warning should not show if hbase root dir isn't on dfs  hbase-2762 added a warning on the master ui if append isn't enabled. however, when hbase is in standalone mode and using a file:/// rootdir, it shows the warning, which doesn't make sense. ",
        "label": 314
    },
    {
        "text": "backport  hbase hbase meta's table jsp ref to wrong rs address  to branch  ",
        "label": 292
    },
    {
        "text": "typo in block caching docs  to turn off block cache for a scan, setcacheblocks should be used, but the hbase book says \"setcaching\", which is not relevant. ",
        "label": 418
    },
    {
        "text": "port fix for hbase 'intermittent testregionobserverscanneropenhook testregionobservercompactiontimestacking failure' to  according to this thread: http://search-hadoop.com/m/3czc31bqsdd , testregionobserverscanneropenhook#testregionobservercompactiontimestacking sometimes failed. this issue is to port the fix from hbase-9836 to 0.94 ",
        "label": 551
    },
    {
        "text": "move all the code in hbase rsgroup to hbase server and remove hbase rsgroup module  let's do this first and then try to refactor it. ",
        "label": 149
    },
    {
        "text": "multi family support for bulk upload tools  hfileoutputformat   loadtable rb   add multi-family support to bulk upload tools from hbase-48. ",
        "label": 341
    },
    {
        "text": "automatically migrate the rs group config for table after hbase  it used to be stored in the rsgroup table, so we need to migrate it to the new place. ",
        "label": 149
    },
    {
        "text": "opening a region failed on  metrics source regionserver sub regions already exists   i restarted a cluster on 0.95 (1ecd4c7e0b22bba75c76f2fc2ce369541502b6df) and some regions failed to open on their first assignment on an exception like: caused by: org.apache.hadoop.metrics2.metricsexception: metrics source regionserver,sub=regions already exists! at org.apache.hadoop.metrics2.lib.defaultmetricssystem.newsourcename(defaultmetricssystem.java:126) at org.apache.hadoop.metrics2.lib.defaultmetricssystem.sourcename(defaultmetricssystem.java:107) at org.apache.hadoop.metrics2.impl.metricssystemimpl.register(metricssystemimpl.java:217) at org.apache.hadoop.hbase.metrics.basesourceimpl.<init>(basesourceimpl.java:75) at org.apache.hadoop.hbase.regionserver.metricsregionaggregatesourceimpl.<init>(metricsregionaggregatesourceimpl.java:49) at org.apache.hadoop.hbase.regionserver.metricsregionaggregatesourceimpl.<init>(metricsregionaggregatesourceimpl.java:41) at org.apache.hadoop.hbase.regionserver.metricsregionserversourcefactoryimpl.getaggregate(metricsregionserversourcefactoryimpl.java:33) at org.apache.hadoop.hbase.regionserver.metricsregionserversourcefactoryimpl.createregion(metricsregionserversourcefactoryimpl.java:50) at org.apache.hadoop.hbase.regionserver.metricsregion.<init>(metricsregion.java:35) at org.apache.hadoop.hbase.regionserver.hregion.<init>(hregion.java:488) at org.apache.hadoop.hbase.regionserver.hregion.<init>(hregion.java:400) i'm attaching a bigger log. ",
        "label": 154
    },
    {
        "text": "revamp tableinputformat  needs updating to match hadoop x and remove bit where we can make   maps than regions  update tif to match new mr. remove the bit of logic where we will use number of configured maps as splits count rather than regions. ",
        "label": 285
    },
    {
        "text": "org apache hadoop hbase replication testreplicationpeer failed with testresetzookeepersession unit test  org.apache.hadoop.hbase.replication.testreplicationpeer running org.apache.hadoop.hbase.replication.testreplicationpeer  tests run: 1, failures: 1, errors: 0, skipped: 0, time elapsed: 25.89 sec <<< failure!  \u2014 stable failures, new for hbase 0.92.0, need to be fixed firstly. --------------------------------   target/surefire-reports/org.apache.hadoop.hbase.replication.testreplicationpeer.txt output: tests run: 1, failures: 1, errors: 0, skipped: 0, time elapsed: 28.245 sec <<< failure!  testresetzookeepersession(org.apache.hadoop.hbase.replication.testreplicationpeer) time elapsed: 25.247 sec <<< failure!  junit.framework.assertionfailederror: replicationpeer zookeeper session was not properly expired.  at junit.framework.assert.fail(assert.java:50)  at org.apache.hadoop.hbase.replication.testreplicationpeer.testresetzookeepersession(testreplicationpeer.java:73)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:60)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:37)  at java.lang.reflect.method.invoke(method.java:611)  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45)  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42)  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)  at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:62) target/surefire-reports/org.apache.hadoop.hbase.replication.testreplicationpeer-output.txt content: 2012-03-25 20:52:42,979 info [main] zookeeper.minizookeepercluster(174): started minizk cluster and connect 1 zk server on client port: 21818 2012-03-25 20:52:43,023 debug [main] zookeeper.zkutil(96): connection to cluster: clusterid opening connection to zookeeper with ensemble (localhost:21818) 2012-03-25 20:52:43,082 info [main] zookeeper.recoverablezookeeper(89): the identifier of this process is 4095@svltest116.svl.ibm.com 2012-03-25 20:52:43,166 debug [main-eventthread] zookeeper.zookeeperwatcher(257): connection to cluster: clusterid received zookeeper event, type=none, state=syncconnected, path=null 2012-03-25 20:52:43,175 info [thread-9] replication.testreplicationpeer(53): expiring replicationpeer zookeeper session. 2012-03-25 20:52:43,196 debug [main-eventthread] zookeeper.zookeeperwatcher(334): connection to cluster: clusterid-0x1364d226a3d0000 connected 2012-03-25 20:52:43,308 info [thread-9] hbase.hbasetestingutility(1234): zk closed session 0x1364d226a3d0000; sleeping=25000 2012-03-25 20:53:08,323 info [thread-9] replication.testreplicationpeer(57): attempting to use expired replicationpeer zookeeper session. ",
        "label": 318
    },
    {
        "text": "enable online schema update by default  after we get hbase-7305 and hbase-7546, things will become stable enough to enable online schema update to be enabled by default.   <property>     <name>hbase.online.schema.update.enable</name>     <value>false</value>     <description>     set true to enable online schema changes.  this is an experimental feature.\u00b7\u00b7     there are known issues modifying table schemas at the same time a region     split is happening so your table needs to be quiescent or else you have to     be running with splits disabled.     </description>   </property> ",
        "label": 154
    },
    {
        "text": "stringbuffer   stringbuilder   conversion of references as necessary  some references in tostring() converted from stringbuffer to stringbuilder as concurrency is probably not needed in those contexts as the references do not get out of scope. ",
        "label": 266
    },
    {
        "text": "hbase common doesn't compile with jdk1  with jdk1.7, mvn clean test give me this error: [error] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:compile (default-compile) on project hbase-common: compilation failure: compilation failure:  [error] could not parse error message: warning: [options] bootstrap class path not set in conjunction with -source 1.6  [error] /home/jxiang/git-repos/apache/tmp/t2/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:44: warning: unsafe is internal proprietary api and may be removed in a future release  [error] import sun.misc.unsafe;  [error] ^  [error]   [error] /home/jxiang/git-repos/apache/tmp/t2/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[1040,19] unsafe is internal proprietary api and may be removed in a future release  [error]   [error] /home/jxiang/git-repos/apache/tmp/t2/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[1046,21] unsafe is internal proprietary api and may be removed in a future release  [error]   [error] /home/jxiang/git-repos/apache/tmp/t2/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[1051,28] unsafe is internal proprietary api and may be removed in a future releas ",
        "label": 242
    },
    {
        "text": "move the enum rs zk region failed open to the last of the enum list in eventhandler  the new state that was added rs_zk_region_failed_open was failing the rolling restart. so move the new enum to the end of the list. ",
        "label": 544
    },
    {
        "text": "introduce delete add as a replacement for delete adddeletemarker  public delete adddeletemarker(cell kv) throws ioexception {  // todo: deprecate and rename 'add' so it matches how we add kvs to puts. ",
        "label": 493
    },
    {
        "text": "isreturnresult false on fast path in branch and branch is not respected  we don't pay attention to the isreturnresult when we go the fast path increment. fix. ",
        "label": 198
    },
    {
        "text": "remove thrift2 from and trunk  thrift2 is what our thrift interface should be. it is unfinished though and without an owner. while in place, it prompts why a thrift2 and a thrift1 questions. meantime, thrift1 is what folks use and it is getting bug fixes. suggest we remove thrift2 till it gets carried beyond thrift1. ",
        "label": 285
    },
    {
        "text": "space quota  if table inside namespace having space quota is dropped  data size usage is still considered for the drop table    steps to follow: 1.create a quota at namespace level 2.create 2 tables t1 and t2 inside namespace and write data. 3.write 5 mb of data each in both t1 and t2. 4. drop t1. 5. data usage for t2 will be shown 10 mb for 10 minutes. if table is dropped inside namespace, data size  usage is still shown for 10 minutes because of the configuration \"hbase.master.quotas.region.report.retention.millis\". this conf maintains the region report in cache(regionsizes) and old entry is used for 10 minutes. we can remove the entry from cache during the drop command as part of mastercp hook only, so that correct usage is show instantaneously after the drop command. ",
        "label": 412
    },
    {
        "text": "compress tables during to migration  current migration has no support for compression. you can always enable compression after migration and it should take effect after the next major compaction. for anyone else who may still have a significant amount of compressed data in 0.19, this might prove useful. we have a large amount of data stored in 0.19 under gzip compression, at an effective 10x rate. out of concern for migration speed and the risk of using more capacity than our cluster has, we didn't want to first write out all the data uncompressed before enabling lzo compression. i'm attaching a small patch that allowed us to migrate the data directly to a compressed table format. to use it, add a migrate.compression property to hbase-site.xml with a value of lzo or gz, and it should compress all tables during migration. ",
        "label": 125
    },
    {
        "text": "testtableshell is broken  error: test_append_should_work_with_value(hbase::tablesimplemethodstest): argumenterror: wrong number of arguments (3 for 1) /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/hbase/table.rb:291:in `_append_internal' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands/append.rb:45:in `append' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands.rb:49:in `block in command_safe' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands.rb:122:in `translate_hbase_exceptions' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands.rb:49:in `command_safe' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell.rb:148:in `internal_command' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/hbase/table.rb:51:in `block in add_command' src/test/ruby/hbase/table_test.rb:186:in `block in test_append_should_work_with_value'      183:     end      184:       185:     define_test \"append should work with value\" do   => 186:       @test_table.append(\"123\", 'x:cnt2', '123')      187:       assert_equal(\"123123\", @test_table._append_internal(\"123\", 'x:cnt2', '123'))      188:     end      189:  error: test_append_should_work_without_qualifier(hbase::tablesimplemethodstest): argumenterror: wrong number of arguments (3 for 1) /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/hbase/table.rb:291:in `_append_internal' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands/append.rb:45:in `append' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands.rb:49:in `block in command_safe' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands.rb:122:in `translate_hbase_exceptions' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell/commands.rb:49:in `command_safe' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/shell.rb:148:in `internal_command' /home/zhangduo/hbase/code/hbase-shell/src/main/ruby/hbase/table.rb:51:in `block in add_command' src/test/ruby/hbase/table_test.rb:191:in `block in test_append_should_work_without_qualifier'      188:     end      189:       190:     define_test 'append should work without qualifier' do   => 191:       @test_table.append('1001', 'x', '123')      192:       assert_equal('123321', @test_table._append_internal('1001', 'x', '321'))      193:     end      194:  ",
        "label": 257
    },
    {
        "text": "improve fix support excluding tests via maven  d property  currently the surefire plugin configuration defines the following exclusion: .       <plugin>         <groupid>org.apache.maven.plugins</groupid>         <artifactid>maven-surefire-plugin</artifactid>         <configuration>           <forkmode>always</forkmode>           <includes>             <include>**/test*.java</include>           </includes>           <excludes>             <exclude>**/*$*</exclude>           </excludes>         </configuration>       </plugin> afaict the '**/*$*' does not resolve to anything meaningful. adding support to exclude one or more tests via maven property, i.e. '-dtest.exclude=<testclass>' would be useful. ",
        "label": 14
    },
    {
        "text": "make secure bulk load work across remote secure clusters  two secure clusters, both with kerberos enabled.  run bulk load on one cluster to load files from another cluster. biadmin@hdtest249:~> hbase org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles hdfs://bdvm197.svl.ibm.com:9000/user/biadmin/mybackups/testtable/0709e79bb131af13ed088bf1afd5649c testtable_rr bulk load failed. in the region server log: 2014-04-02 20:04:56,361 error org.apache.hadoop.hbase.security.access.securebulkloadendpoint: failed to complete bulk load java.lang.illegalargumentexception: wrong fs: hdfs://bdvm197.svl.ibm.com:9000/user/biadmin/mybackups/testtable/0709e79bb131af13ed088bf1afd5649c/info/6b44ca48aebf48d98cb3491f512c41a7, expected: hdfs://hdtest249.svl.ibm.com:9000         at org.apache.hadoop.fs.filesystem.checkpath(filesystem.java:651)         at org.apache.hadoop.hdfs.distributedfilesystem.getpathname(distributedfilesystem.java:181)         at org.apache.hadoop.hdfs.distributedfilesystem.access$000(distributedfilesystem.java:92)         at org.apache.hadoop.hdfs.distributedfilesystem$22.docall(distributedfilesystem.java:1248)         at org.apache.hadoop.hdfs.distributedfilesystem$22.docall(distributedfilesystem.java:1244)         at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81)         at org.apache.hadoop.hdfs.distributedfilesystem.setpermission(distributedfilesystem.java:1244)         at org.apache.hadoop.hbase.security.access.securebulkloadendpoint$1.run(securebulkloadendpoint.java:233)         at org.apache.hadoop.hbase.security.access.securebulkloadendpoint$1.run(securebulkloadendpoint.java:223)         at java.security.accesscontroller.doprivileged(accesscontroller.java:300)         at javax.security.auth.subject.doas(subject.java:494)         at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1482)         at org.apache.hadoop.hbase.security.access.securebulkloadendpoint.securebulkloadhfiles(securebulkloadendpoint.java:223)         at org.apache.hadoop.hbase.protobuf.generated.securebulkloadprotos$securebulkloadservice.callmethod(securebulkloadprotos.java:4631)         at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:5088)         at org.apache.hadoop.hbase.regionserver.hregionserver.execservice(hregionserver.java:3219)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26933)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2150)         at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1854) ",
        "label": 234
    },
    {
        "text": "src tgz no longer builds after hbase  encountered on branch-1.1 while spinning rc. presumably this applies to all branches where hbase-14085 was applied. ",
        "label": 339
    },
    {
        "text": "expose rs work queue contents on web ui  would be nice to be able to see the contents of the various work queues - eg to know what regions are pending compaction/split/flush/etc. this is handy for debugging why a region might be blocked, etc. ",
        "label": 239
    },
    {
        "text": "clone snapshots on secure cluster should provide option to apply retained user permissions  currently, sudo su - test_user create 't1', 'f1' sudo su - hbase snapshot 't1', 'snap_one' clone_snapshot 'snap_one', 't2' in this scenario the user - test_user would not have permissions for the clone table t2.  we need to add improvement feature such that the permissions of the original table are recorded in snapshot metadata and an option is provided for applying them to the new table as part of the clone process. ",
        "label": 514
    },
    {
        "text": "testiofencing testfencingaroundcompactionafterwalsync fails  got several test failure on the latest build: [tianq@bdvm101 surefire-reports]$ ls -1t|grep \"tests run\" * |grep \"<<< failure\"   org.apache.hadoop.hbase.client.testreplicasclient.txt:tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 38.706 sec <<< failure!  org.apache.hadoop.hbase.master.testmasteroperationsforregionreplicas.txt:tests run: 2, failures: 1, errors: 0, skipped: 0, time elapsed: 30.669 sec <<< failure!  org.apache.hadoop.hbase.regionserver.testregionreplicas.txt:tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 39.113 sec <<< failure!  org.apache.hadoop.hbase.testiofencing.txt:tests run: 2, failures: 1, errors: 0, skipped: 0, time elapsed: 177.071 sec <<< failure! the first one: <failure message=\"timed out waiting for the region to flush\" type=\"java.lang.assertionerror\">java.lang.assertionerror: timed out waiting for the region to flush  >-at org.junit.assert.fail(assert.java:88)  >-at org.junit.assert.asserttrue(assert.java:41)  >-at org.apache.hadoop.hbase.testiofencing.dotest(testiofencing.java:291)  >-at org.apache.hadoop.hbase.testiofencing.testfencingaroundcompactionafterwalsync(testiofencing.java:236)  >-at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  >-at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  >-at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  >-at java.lang.reflect.method.invoke(method.java:606) ",
        "label": 407
    },
    {
        "text": "testlockprocedure testremotenamespacelockrecovery fails in master  this can be reproduced locally: testremotenamespacelockrecovery(org.apache.hadoop.hbase.master.locking.testlockprocedure)  time elapsed: 8.722 sec  <<< failure! java.lang.assertionerror: expected:<unlocked> but was:<locked> at org.apache.hadoop.hbase.master.locking.testlockprocedure.sendheartbeatandchecklocked(testlockprocedure.java:219) at org.apache.hadoop.hbase.master.locking.testlockprocedure.testremotelockrecovery(testlockprocedure.java:401) at org.apache.hadoop.hbase.master.locking.testlockprocedure.testremotenamespacelockrecovery(testlockprocedure.java:415) see also https://builds.apache.org/job/hbase-trunk_matrix/3902/jdk=jdk%201.8%20(latest),label=hadoop/testreport/org.apache.hadoop.hbase.master.locking/testlockprocedure/testremotenamespacelockrecovery/ ",
        "label": 441
    },
    {
        "text": "testzookeepermainserver fails with keeperexception connectionlossexception  i'm trying to run test suite on a local machine. i never get to the second part because i fail on below test with below exception near every time (and an ipv6 test... will do that next). 1 -------------------------------------------------------------------------------  2 test set: org.apache.hadoop.hbase.zookeeper.testzookeepermainserver  3 -------------------------------------------------------------------------------  4 tests run: 2, failures: 0, errors: 1, skipped: 0, time elapsed: 16.161 s <<< failure! - in org.apache.hadoop.hbase.zookeeper.testzookeepermainserver  5 testcommandlineworks(org.apache.hadoop.hbase.zookeeper.testzookeepermainserver) time elapsed: 15.848 s <<< error!  6 org.apache.zookeeper.keeperexception$connectionlossexception: keepererrorcode = connectionloss for /testcommandlineworks  7 at org.apache.hadoop.hbase.zookeeper.testzookeepermainserver.testcommandlineworks(testzookeepermainserver.java:81) looks like running the command before we are connected causes the above \u2013 we pause 15 seconds and then throw the above. if i wait until connected before proceding, stuff seems to work reliably. i don't have access to the watcher on connections since we override the zk main class... so this seems only avenue available at mo (this zk main thing is all a hack around zk main because it had bugs ... but i think we have to keep the hack because folks use different versions of zk. my $workplace defaults to something that is years old, 3.4.5 for instance). ",
        "label": 314
    },
    {
        "text": "row locks may deadlock with themselves  row locks in hregion are keyed by a int-sized hash of the row key. it's perfectly possible for two rows to hash to the same key. so, if any client tries to lock both rows, it will deadlock with itself. switching to a 64-bit hash is an improvement but still sketchy. ",
        "label": 140
    },
    {
        "text": "testcatalogtrackeroncluster testbadoriginalrootlocation fails occasionally  failure: java.io.ioexception: shutting down at org.apache.hadoop.hbase.minihbasecluster.init(minihbasecluster.java:223) at org.apache.hadoop.hbase.minihbasecluster.<init>(minihbasecluster.java:86) at org.apache.hadoop.hbase.minihbasecluster.<init>(minihbasecluster.java:77) at org.apache.hadoop.hbase.hbasetestingutility.startminihbasecluster(hbasetestingutility.java:650) at org.apache.hadoop.hbase.catalog.testcatalogtrackeroncluster.testbadoriginalrootlocation(testcatalogtrackeroncluster.java:68) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) at org.junit.rules.testwatcher$1.evaluate(testwatcher.java:47) at org.junit.rules.runrules.evaluate(runrules.java:18) at org.junit.runners.parentrunner.runleaf(parentrunner.java:263) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:68) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:47) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:60) at org.junit.runners.parentrunner.runchildren(parentrunner.java:229) at org.junit.runners.parentrunner.access$000(parentrunner.java:50) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:222) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.junit.runners.suite.runchild(suite.java:128) at org.junit.runners.suite.runchild(suite.java:24) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.lang.runtimeexception: master not initialized after 200 seconds at org.apache.hadoop.hbase.util.jvmclusterutil.startup(jvmclusterutil.java:206) at org.apache.hadoop.hbase.localhbasecluster.startup(localhbasecluster.java:420) at org.apache.hadoop.hbase.minihbasecluster.init(minihbasecluster.java:216) ... 32 more likely caused by this: 2013-01-31 04:52:23,064 fatal [master:0;hemera.apache.org,52696,1359607882775] master.hmaster(1493): unhandled exception. starting shutdown. org.apache.hadoop.hbase.ipc.hbaseclient$failedserverexception: this server is in the failed servers list: example.org/192.0.43.10:1234 at org.apache.hadoop.hbase.ipc.hbaseclient$connection.setupiostreams(hbaseclient.java:425) at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:1124) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:974) at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:86) at $proxy19.getprotocolversion(unknown source) at org.apache.hadoop.hbase.ipc.writablerpcengine.getproxy(writablerpcengine.java:138) at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:208) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:1335) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:1291) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:1278) at org.apache.hadoop.hbase.catalog.catalogtracker.getcachedconnection(catalogtracker.java:506) at org.apache.hadoop.hbase.catalog.catalogtracker.getrootserverconnection(catalogtracker.java:343) at org.apache.hadoop.hbase.catalog.catalogtracker.waitforrootserverconnection(catalogtracker.java:327) at org.apache.hadoop.hbase.catalog.catalogtracker.verifyrootregionlocation(catalogtracker.java:599) at org.apache.hadoop.hbase.master.hmaster.assignrootandmeta(hmaster.java:659) at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:560) at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:376) at java.lang.thread.run(thread.java:662) 2013-01-31 04:52:23,064 info  [master:0;hemera.apache.org,52696,1359607882775] master.hmaster(1652): aborting ",
        "label": 286
    },
    {
        "text": "race in hcm getmaster stalls clients  i found this issue trying to run ycsb on 0.94, i don't think it exists on any other branch. i believe that this was introduced in hbase-5058 \"allow hbaseadmin to use an existing connection\". the issue is that in hcm.getmaster it does this recipe: 1. check if the master is null and runs (if so, return) 2. grab a lock on masterlock 3. nullify this.master 4. try to get a new master the issue happens at 3, it should re-run 1 since while you're waiting on the lock someone else could have already fixed it for you. what happens right now is that the threads are all able to set the master to null before others are able to get out of getmaster and it's a complete mess. figuring it out took me some time because it doesn't manifest itself right away, silent retries are done in the background. basically the first clue was this: error doing get: org.apache.hadoop.hbase.client.retriesexhaustedexception: failed after attempts=10, exceptions: tue jun 19 23:40:46 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:40:47 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:40:48 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:40:49 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:40:51 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:40:53 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:40:57 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:41:01 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:41:09 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed tue jun 19 23:41:25 utc 2012, org.apache.hadoop.hbase.client.htable$3@571a4bd4, java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@2eb0a3f5 closed this was caused by the little dance up in hbaseadmin where it deletes \"stale\" connections... which are not stale at all. ",
        "label": 544
    },
    {
        "text": "testrsgroupsadmin2 is flaky  https://builds.apache.org/job/hbase-flaky-tests/job/master/4450/testreport/org.apache.hadoop.hbase.rsgroup/testrsgroupsadmin2/testfailedmoveserverstablesandrepair/ java.lang.nullpointerexception at org.apache.hadoop.hbase.rsgroup.testrsgroupsadmin2.randomlysetregionstate(testrsgroupsadmin2.java:627) at org.apache.hadoop.hbase.rsgroup.testrsgroupsadmin2.testfailedmoveserverstablesandrepair(testrsgroupsadmin2.java:754) ",
        "label": 149
    },
    {
        "text": "testnodehealthcheckchore testrshealthchore  stoppable must have been stopped  tests run: 4, failures: 1, errors: 0, skipped: 0, time elapsed: 623.639 sec <<< failure! testrshealthchore(org.apache.hadoop.hbase.testnodehealthcheckchore)  time elapsed: 0.001 sec  <<< failure! java.lang.assertionerror: stoppable must have been stopped.         at org.junit.assert.fail(assert.java:88)         at org.junit.assert.asserttrue(assert.java:41)         at org.apache.hadoop.hbase.testnodehealthcheckchore.testrshealthchore(testnodehealthcheckchore.java:108) ",
        "label": 38
    },
    {
        "text": "move regionserver metrics to metrics2  move regionserver metrics to metrics2 ",
        "label": 154
    },
    {
        "text": "bunch of log lines from qosfunction  marking normal priority after  in tests, rs logs contain a lot of the following because of regionopeningexception or regionmovedexception. 2013-05-23 20:40:08,891 debug [ipc reader 3 on port 60020] org.apache.hadoop.hbase.regionserver.qosfunction: marking normal priority after getting exception=org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: f8665bfb57c728262807429efc41fa49 2013-05-23 20:40:08,891 debug [ipc reader 3 on port 60020] org.apache.hadoop.hbase.regionserver.qosfunction: marking normal priority after getting exception=org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: f8665bfb57c728262807429efc41fa49 2013-05-23 20:40:10,385 debug [ipc reader 4 on port 60020] org.apache.hadoop.hbase.regionserver.qosfunction: marking normal priority after getting exception=org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: f8665bfb57c728262807429efc41fa49 2013-05-23 20:40:10,385 debug [ipc reader 4 on port 60020] org.apache.hadoop.hbase.regionserver.qosfunction: marking normal priority after getting exception=org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: f8665bfb57c728262807429efc41fa49 2013-05-23 20:40:10,786 debug [ipc reader 5 on port 60020] org.apache.hadoop.hbase.regionserver.qosfunction: marking normal priority after getting exception=org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: f8665bfb57c728262807429efc41fa49 2 ",
        "label": 314
    },
    {
        "text": "turn off the jdk8 javadoc linter     there's a new javadoc warning that causes warnings on all of our new patches and (i believe) breaks us on jdk8. thanks to stack for chasing it down on hbase-14849. 1 warning [warning] javadoc warnings [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/loadincrementalhfiles.java:430: warning - @param argument \"hfilesdir\" is not a parameter name. [info] ",
        "label": 402
    },
    {
        "text": "add scanner batching to export job  when a single row is too large for the rs heap then an oome can take out the entire rs. setting scanner batching in custom scans helps avoiding this scenario, but for the supplied export job this is not set. similar to hbase-3421 we can set the batching to a low number - or if needed make it a command line option. ",
        "label": 408
    },
    {
        "text": "fswritelatency metric may be incorrectly reported  fswritelatency metric is computed by maintaining writetime & writeops in hlog. if an hlog.append() carries multiple edits, then \"writetime\" is computed incorrectly for the subsequent edits because dowrite() is called for each of the edits with the same start time argument (\"now\"). this also causes a lot of false warn spews to the log. only one of the edits might have taken a long time, but every edit after that in a given hlog.append() operation will also raise these warning messages. 2010-03-03 11:00:42,247 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302227 2010-03-03 11:00:42,247 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302228 2010-03-03 11:00:42,247 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302229 2010-03-03 11:00:42,247 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302230 2010-03-03 11:00:42,247 warn org.apache.hadoop.hbase.regionserver.hlog: ipc server handler 51 on 60020 took 1814ms appending an edit to hlog; editcount=302231 will submit a patch shortly. ",
        "label": 263
    },
    {
        "text": "mvn site fails without install first  mvn site fails on hadoopqa runs, and also on fresh checkouts. the problem seems to be that mvn site somehow does not trigger a correct reactor ordering. hbase-server is built before other components, and thus throws dependency errors because the other modules are not build yet. an example from https://builds.apache.org/job/precommit-hbase-build/7491//consolefull: /home/jenkins/tools/maven/latest/bin/mvn compile site -dskiptests -dhbasepatchprocess > /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/patchprocess/patchsiteoutput.txt 2>&1 ",
        "label": 330
    },
    {
        "text": "scanner from regioncoprocessorenvironment gettable tablename  returns no local data  might be fixed as a byproduct of the fix for hbase-11766. here's what i'm seeing: from an endpoint coprocessor on system.catalog table rows exist in the table, but are all in a single region scan is a new, empty scan created like this: new scan() the following scanner does return data:     regionscanner scanner = region.getscanner(scan); the following scanner does not return any data: htableinterface htable = env.gettable(region.gettabledesc().gettablename()); the following scanner does return data:         htablepool pool = new htablepool (env.getconfiguration(),1);         htableinterface htable = pool.gettable(\"system.catalog\"); jeffrey zhong, andrew kyle purtell ",
        "label": 233
    },
    {
        "text": "globally barriered procedure mechanism  this is a simplified version of what was proposed in hbase-6573. instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a procedure. users need only to implement a methods to acquirebarrier, to act when insidebarrier, and to releasebarrier that use the externalexception cooperative error checking mechanism. globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. also if any node fails, it needs to be able to notify them so that they abort. the first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. this version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition. ",
        "label": 248
    },
    {
        "text": "hbase remote copytable not working when security enabled  when security is enabled, hbase copytable fails with kerberos exception: fatal org.apache.hadoop.ipc.secureclient: sasl authentication failed. the most likely cause is missing or invalid credentials. consider 'kinit'. javax.security.sasl.saslexception: gss initiate failed [caused by gssexception: no valid credentials provided (mechanism level: failed to find any kerberos tgt)] this is only when copying to remote hbase cluster (using either mrv1 or yarn), local copy works fine. ",
        "label": 224
    },
    {
        "text": "make hlog block size configurable  make hlog blocksize configurable to mitigate possible data loss. the ideal size is (size of record (avg key length + avg value length)) * (number of records between sync()) ",
        "label": 241
    },
    {
        "text": " jdk8  replace use of high scale lib counters with intrinsic facilities  jep155 introduces a few new classes (doubleaccumulator, doubleadder, longaccumulator, longadder) that \"internally employ contention-reduction techniques that provide huge throughput improvements as compared to atomic variables\". there are applications of these where we are currently using cliff click's high-scale-lib and for metrics. see http://openjdk.java.net/jeps/155 ",
        "label": 149
    },
    {
        "text": "revisit checkstyle rules  for the shaded imports, i prefer we put it at the bottom with a separated block. and also, for uts which use junit rule, we usually declare the field as public but checkstyle will report a visibilitymodifier problem. ",
        "label": 149
    },
    {
        "text": "hbase clients cannot recover when their zookeeper session becomes invalid  someone made mention of this loop last week but i don't think i filed an issue. here is another instance, again from a secret hbase admirer: \"it seems that when zookeeper dies and restarts, all client applications need to be restarted too. i just restarted hbase in non-distributed mode (which includes a zk) and now my application can't reconnect to zk unless i restart it too. i'm stuck in this loop: 2010-07-19 00:13:05,725 info org.apache.zookeeper.server.nioservercnxn:   closed socket connection for client /127.0.0.1:55153 (no session established for client) 2010-07-19 00:13:07,052 info org.apache.zookeeper.server.nioservercnxn:   accepted socket connection from /127.0.0.1:55154 2010-07-19 00:13:07,053 info org.apache.zookeeper.server.nioservercnxn:   refusing session request for client /127.0.0.1:55154 as it has seen zxid 0xf5 our last zxid is 0xd7   client must try another server \" ",
        "label": 70
    },
    {
        "text": "hlog closed should be checked inside of updatelock  concurrency issue: hlog.closed is set inside the updatelock but not checked inside the lock. ",
        "label": 356
    },
    {
        "text": "limit the number of regions in transitions displayed on master webpage   hbase-3837 added a table to the master web page that displays regions in transitions. there is a possibility that a massive number of rit's could be present which would make displaying this page slow. this should be limited to a reasonable number (100?) on the main page and provide a link to another page that displays all information. ",
        "label": 453
    },
    {
        "text": "meta region stuck in rs2zk region opened state  in cluster testing trunk, i ended up with a situation where meta was unassigned and no amount of restarting various pieces would fix it. on master startup, i see: 2010-06-20 21:08:05,431 debug org.apache.hadoop.hbase.master.basescanner: current assignment of .meta.,,1.1028785192 is not valid; serveraddress=, startcode=0 unknown.  2010-06-20 21:08:05,436 debug org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: while creating unassigned region 1028785192 exists, state = rs2zk_region_opened  2010-06-20 21:08:05,438 warn org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: <monster01.sf.cloudera.com:/hbase,org.apache.hadoop.hbase.master.hmaster>failed to create znode /hbase/unassigned/1028785192 in zookeeper  org.apache.zookeeper.keeperexception$nodeexistsexception: keepererrorcode = nodeexists for /hbase/unassigned/1028785192  2010-06-20 21:08:05,438 debug org.apache.hadoop.hbase.master.regionmanager: created unassigned znode .meta.,,1.1028785192 in state m2zk_region_offline then on the rs:  2010-06-20 21:08:05,899 error org.apache.hadoop.hbase.regionserver.rszookeeperupdater: znode /hbase/unassigned/1028785192 is not in closed/offline state (state = rs2zk_region_opened), will not open region.  2010-06-20 21:08:05,899 error org.apache.hadoop.hbase.regionserver.hregionserver: error opening .meta.,,1.1028785192  java.io.ioexception: znode /hbase/unassigned/1028785192 is not in closed/offline state (state = rs2zk_region_opened), will not open region. and the region never opens ",
        "label": 268
    },
    {
        "text": "backport hbase  block cache contents report ui  to  backport hbase-4089 + hbase-11329. block caching is an active area of investigation and development. this is a nice facility that provides convenient introspection capabilities in ui. ",
        "label": 314
    },
    {
        "text": "splitting hlog and opening region concurrently may cause data loss  case description:  1.split hlog thread creat writer for the file region a/recoverd.edits/123456 and is appending log entry  2.regionserver is opening region a now, and in the process replayrecoverededitsifany() ,it will delete the file region a/recoverd.edits/123456   3.split hlog thread catches the io exception, and stop parse this log file   and if skiperror = true , add it to the corrupt logs....however, data in other regions in this log file will loss   4.or if skiperror = false, it will check filesystem.of course, the file system is ok , and it only prints a error log, continue assigning regions. therefore, data in other log files will also loss!! the case may happen in the following:  1.move region from server a to server b  2.kill server a and server b  3.restart server a and server b we could prevent this exception throuth forbiding deleting recover.edits file   which is appending by split hlog thread ",
        "label": 107
    },
    {
        "text": " rest stargate  improve error response when trying to create a scanner on a nonexistant table  since 0.20.4, an attempt to create a scanner for a nonexistant table receives a \"400 bad request\" response with no furthur information. prior to 0.20.4 it would receive a \"500 org.apache.hadoop.hbase.tablenotfoundexception: <table>\" response with a stack trace in the body. neither of these is ideal - the 400 fails to identify what aspect of the request was bad, and the 500 incorrectly suggests that the error was internal. ideally the error should be a 400 error with information in the body identifying the nature of the problem. ",
        "label": 38
    },
    {
        "text": "improve file descriptor usage  currently  there are two file descriptors per storefile  this is because there are two open calls in the hfile: one with checksum and another for without checksum support in v2:  see the method in hfile:createreaderwithencoding() fsdatainputstream fsdis = fs.open(path); fsdatainputstream fsdisnofschecksum = fsdis; // if the fs is not an instance of hfilesystem, then create an // instance of hfilesystem that wraps over the specified fs. // in this case, we will not be able to avoid checksumming inside // the filesystem. if (!(fs instanceof hfilesystem)) {   hfs = new hfilesystem(fs); } else {   hfs = (hfilesystem)fs;   // open a stream to read data without checksum verification in   // the filesystem   fsdisnofschecksum = hfs.getnochecksumfs().open(path); } ",
        "label": 406
    },
    {
        "text": "intra row scanning  to continue scaling numbers of columns or versions in a single row, we need a mechanism to scan within a row so we can return some columns at a time. currently, an entire row must come back as one piece. ",
        "label": 38
    },
    {
        "text": "avoid admin balance during master initialize  in hbase-5850 many of the admin operations have been blocked till the master initializes. but the balancer is not. so this jira is to extend the pleaseholdexception in case of admin.balance() call before master is initialized. ",
        "label": 543
    },
    {
        "text": "use generics appropriately in rpcengine and reduce casts  with fixing a related bug of breaking thread safety in hconnectionmanager  in rpcengine,   versionedprotocol getproxy(class<? extends versionedprotocol> protocol, ...) should be   <t extends versionedprotocol> t getproxy(class<t> protocol, ...) also, while removing casts i encountered a bug of the method hconnectionmanager.hconnectionimplementation.getprotocol() using broken logic just like double-checked locking for hashmap. ",
        "label": 200
    },
    {
        "text": "most of the regions were added into assignmentmanager servers twice  here's the scenario of how did the problem happened: 1. when hmaster start, all regionservers checkin ok, and count of regions out on cluster is 10083, which is the actual region number count.  2. then openedregionhandler#process received zookeeper's events, and added 9923 regions to the hris list.  but the 9923 regions already exists, force added.  3. the loadbalancer get the wrong region numbers of 20006 (10083 + 9923). assignmentmanager#addtoservers method:  private void addtoservers(final hserverinfo hsi, final hregioninfo hri) {  list<hregioninfo> hris = servers.get(hsi);  if (hris == null) { hris = new arraylist<hregioninfo>(); servers.put(hsi, hris); } hris.add(hri); // same region was double added here  } logs:  2011-06-27 16:13:06,845 info org.apache.hadoop.hbase.master.servermanager: exiting wait on regionserver(s) to checkin; count=3, stopped=false, count of regions out on cluster=10083  2011-06-27 16:13:17,334 info org.apache.hadoop.hbase.master.assignmentmanager: failed-over master needs to process 9923 regions in transition  2011-06-27 16:21:45,135 debug org.apache.hadoop.hbase.master.loadbalancer: balance parameter: numregions=20006, numservers=3, max=6669, min=6668 ",
        "label": 441
    },
    {
        "text": "fix link in developer guide to  code review checklist   the destination of the link \"code review checklist\" in https://hbase.apache.org/book.html#_reject has been moved. ",
        "label": 339
    },
    {
        "text": "printstacktrace in fsutils  this is bad...     public boolean accept(path p) {       boolean isvalid = false;       try {         if (hconstants.hbase_non_user_table_dirs.contains(p.tostring())) {           isvalid = false;         } else {             isvalid = this.fs.getfilestatus(p).isdir();         }       } catch (ioexception e) {         e.printstacktrace();          <================        }       return isvalid;     }   } ",
        "label": 191
    },
    {
        "text": "poor performance of htable getscanner in multithreaded environment due to dns getdefaulthost  being called in scannercallable prepare   hi, i am running a app on top of phoenix which will fork say around 100+ thread to call htable.getscanner(scan) to do parallel scan ( say each scan is actually targeting one region), and each scan will only match a few result and return thus will be very fast. under this case, i found that the htable.getscanner(scan) op itself runs pretty slow. by profiling with jvisualvm. i found 90% of app time is cost on org.apache.hadoop.net.dns.getdefaulthost. which been invoked by scannnercallable.checkifregionserverisremote. the root cause is that dns.getdefaulthost involves synchronized methods in java.net.inet4addressimpl which have the 100+ thread to lock and wait upon each other. each call to dns.getdefaulthost cost around 30ms, while in another case, i run single thread to call 100k times dns.getdefaulthost , each cost leas than 0.06ms. by hacking the code and remove the call to checkifregionserverisremote, my app runs 5 times faster, say, 50k op in my app cost 200+ seconds instead of 1000+ seconds. by check the code further, i found this checkifregionserverisremote seems just for use of metrics collection. ( or maybe retry logic?) i am wondering that could this been removed or switch to some other implementation? so that cases like mine which run large number of small scan with multi threads could performance way better? ",
        "label": 441
    },
    {
        "text": "update hbaserpc to match hadoop rpc  hadoop rpc has had some improvements done. copied them down to our version of hadoop rpc, hbaserpc in the hbase.ipc package. ",
        "label": 314
    },
    {
        "text": "fix non varargs compile warnings  in hbase-hbck2 submodule there are a few compile warnings with \"non-varargs call of varargs method with inexact argument type for last parameter\". warnings can be removed by casting null value. [warning] /users/peter.somogyi/cloudera/repos/hbase-operator-tools/hbase-hbck2/src/test/java/org/apache/hbase/testhbck2.java:[206,46] non-varargs call of varargs method with inexact argument type for last parameter;   cast to java.lang.string for a varargs call   cast to java.lang.string[] for a non-varargs call and to suppress this warning [warning] /users/peter.somogyi/cloudera/repos/hbase-operator-tools/hbase-hbck2/src/test/java/org/apache/hbase/testhbck2.java:[313,43] non-varargs call of varargs method with inexact argument type for last parameter;   cast to java.lang.string for a varargs call   cast to java.lang.string[] for a non-varargs call and to suppress this warning [warning] /users/peter.somogyi/cloudera/repos/hbase-operator-tools/hbase-hbck2/src/test/java/org/apache/hbase/testfsregionsmetarecoverer.java:[117,53] non-varargs call of varargs method with inexact argument type for last parameter;   cast to org.apache.hadoop.hbase.client.result for a varargs call   cast to org.apache.hadoop.hbase.client.result[] for a non-varargs call and to suppress this warning [warning] /users/peter.somogyi/cloudera/repos/hbase-operator-tools/hbase-hbck2/src/test/java/org/apache/hbase/testfsregionsmetarecoverer.java:[132,53] non-varargs call of varargs method with inexact argument type for last parameter;   cast to org.apache.hadoop.hbase.client.result for a varargs call   cast to org.apache.hadoop.hbase.client.result[] for a non-varargs call and to suppress this warning [warning] /users/peter.somogyi/cloudera/repos/hbase-operator-tools/hbase-hbck2/src/test/java/org/apache/hbase/testfsregionsmetarecoverer.java:[162,53] non-varargs call of varargs method with inexact argument type for last parameter;   cast to org.apache.hadoop.hbase.client.result for a varargs call   cast to org.apache.hadoop.hbase.client.result[] for a non-varargs call and to suppress this warning   ",
        "label": 486
    },
    {
        "text": "in zookeeper sh  ' zookeeper' is not defined  cmd line shows     starting zookeeper  i e  a leading '  '  ",
        "label": 229
    },
    {
        "text": "new ui elements may request external resources  after hbase-6135, the ui may, depending on browser version, attempt to pull in external resources, e.g. from masterstatustmpl.jamon:     <!--[if lt ie 9]>       <script src=\"http://html5shim.googlecode.com/svn/trunk/html5.js\"></script>     <![endif]--> this won't work if the ui is viewed in a restricted environment. also, pulling external resources from googlecode / svn like this seems not a good practice, those can change at any given third party checkin. rather, we should pull in any resources needed into our /static/ ? ",
        "label": 154
    },
    {
        "text": "replace reflection in fshlog with hdfsdataoutputstream getcurrentblockreplication   as comment todo said, we use hdfsdataoutputstream#getcurrentblockreplication and dfsoutputstream.getpipeline to replace reflection in fshlog ",
        "label": 198
    },
    {
        "text": "the deletefamily cell is skipped when storescanner seeks to next column  the qualifier of a deleted row (with keep deleted cells true) re-appears after re-inserting the same row multiple times (with different timestamp) with an empty qualifier. scenario: 1. put row with family and qualifier (timestamp 1). 2. delete entire row (timestamp 2). 3. put same row again with family without qualifier (timestamp 3).  a scan (latest version) returns the row with family without qualifier, version 3 (which is correct). 4. put the same row again with family without qualifier (timestamp 4).  a scan (latest version) returns multiple rows: the row with family without qualifier, version 4 (which is correct). the row with family with qualifier, version 1 (which is wrong). there is a test scenario attached.  output:  <log> 13:42:53,952 [main] client.hbaseadmin - started disable of test_dml  <log> 13:42:55,801 [main] client.hbaseadmin - disabled test_dml  <log> 13:42:57,256 [main] client.hbaseadmin - deleted test_dml  <log> 13:42:58,592 [main] client.hbaseadmin - created test_dml  put row: 'myrow' with family: 'myfamily' with qualifier: 'myqualifier' with timestamp: '1'  scan printout =>  row: 'myrow', timestamp: '1', family: 'myfamily', qualifier: 'myqualifier', value: 'myvalue'  delete row: 'myrow'  scan printout =>  put row: 'myrow' with family: 'myfamily' with qualifier: 'null' with timestamp: '3'  scan printout =>  row: 'myrow', timestamp: '3', family: 'myfamily', qualifier: '', value: 'myvalue'  put row: 'myrow' with family: 'myfamily' with qualifier: 'null' with timestamp: '4'  scan printout =>  row: 'myrow', timestamp: '4', family: 'myfamily', qualifier: '', value: 'myvalue'  row: 'myrow', timestamp: '1', family: 'myfamily', qualifier: 'myqualifier', value: 'myvalue' ",
        "label": 98
    },
    {
        "text": "update contents about tracing in the reference guide  adding explanation about client side settings and shell command for tracing. ",
        "label": 305
    },
    {
        "text": "java6 as a requirement  make java6 a requirement running hbase. our hand will proably be forced by hadoop making java6 a requirement (0.19?). ",
        "label": 314
    },
    {
        "text": "hbase crash when network card has a ipv6 address  i've met a problem startup hbase. i setup hbase with hdfs,  my server's network card has a ipv4 address and also a ipv6 address. when i first startup hbase with default configuration file,  i found that the region server can't   register to master. and i found lots of 127.0.0.1 in log. so i suppose interface \"default\" would not work and add following: <property>  <name>dfs.datanode.dns.interface</name>  <value>eth0</value>  <description>the name of the network interface from which a data node should   report its ip address.  </description>  </property> however, when this is done. hbase master crashes;  and i see ipv6 addresses in the log. so i dig into the source code,  found that hbase fails to deal with ipv6 address. details is in following:  in class org.apache.hadoop.hbase.hregionserver  the method getthisip() invoke the method of class belongs to hadoop-core package  the class is: org.apache.hadoop.net.dns  the method is: getdefaultip(string strinterface)  this method invokes another method in the same class: getips(string strinterface)  method getips always returns the first ip address no matter it is ipv4 or ipv6 i have fixed it by modifying method of org.apache.hadoop.net.dns.getips(string  strinterface)  such that it always returns ipv4 address  it is working now for me.  but when hadoop upgrades, i have to modify again. in order to avoid the problem,  i modify a method in class: org.apache.hadoop.net.dns the following is the modified code of this method, it would not return ipv6  address now. /** returns all the ips associated with the provided interface, if any, in textual form. @param strinterface the name of the network interface to query (e.g. eth0) @return a string vector of all the ips associated with the provided interface @throws unknownhostexception if an unknownhostexception is encountered in querying the default interface  */  public static string[] getips(string strinterface)  throws unknownhostexception {  try {  networkinterface netif = networkinterface.getbyname(strinterface);  if (netif == null)  return new string[] { inetaddress.getlocalhost() .gethostaddress() } ;  else {  vector<string> ips = new vector<string>();  enumeration e = netif.getinetaddresses();  while (e.hasmoreelements()) { string addr=((inetaddress) e.nextelement()).gethostaddress(); if(addr.length()<=15)//only when it is a ipv4 address ips.add(addr); //ips.add(((inetaddress) e.nextelement()).gethostaddress()); } return ips.toarray(new string[] {});  }  } catch (socketexception e) unknown macro: { return new string[] { inetaddress.getlocalhost().gethostaddress() }; }  } ",
        "label": 241
    },
    {
        "text": "purge startupdate usage from internal code and test cases  we have batch updates now. nothing internal should be using the deprecated startupdate method. ",
        "label": 86
    },
    {
        "text": "backport hbase  hbase rpc aspires to grow an infinite tree of trace scopes  some other places are also unsafe  intent to branch  check the exposure of branch-1 code to the problems described on hbase-22115 and apply the fix approach there. ",
        "label": 473
    },
    {
        "text": "testrollingrestart fails intermittently  i got the following when running test suite on trunk: testbasicrollingrestart(org.apache.hadoop.hbase.master.testrollingrestart)  time elapsed: 300.28 sec  <<< error! java.lang.exception: test timed out after 300000 milliseconds         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.master.testrollingrestart.waitforrsshutdowntostartandfinish(testrollingrestart.java:313)         at org.apache.hadoop.hbase.master.testrollingrestart.testbasicrollingrestart(testrollingrestart.java:210) i ran testrollingrestart#testbasicrollingrestart manually afterwards which wiped out test output file for the failed test. similar failure can be found on jenkins:  https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.92/19/testreport/junit/org.apache.hadoop.hbase.master/testrollingrestart/testbasicrollingrestart/ ",
        "label": 544
    },
    {
        "text": "implement modifytable and enable disabletablereplication for asyncadmin  add 3 methods to asyncadmin.  modifytable()  enabletablereplication()  disabletablereplication() ",
        "label": 187
    },
    {
        "text": " uberhbck  add options for how to handle offline split parents   in a recent case, we attempted to repair a cluster that suffered from hbase-4238 that had about 6-7 generations of \"leftover\" split data. the hbck repair options in an development version of hbase-5128 treat hdfs as ground truth but didn't check split and offline flags only found in meta. the net effect was that it essentially attempted to merge many regions back into its eldest geneneration's parent's range. more safe guards to prevent \"mega-merges\" are being added on hbase-5128. this issue would automate the handling of the \"mega-merge\" avoiding cases such as \"lingering grandparents\". the strategy here would be to add more checks against .meta., and perform part of the catalog janitor's responsibilities for lingering grandparents. this would potentially include options to sideline regions, deleting grandparent regions, min size for sidelining, and mechanisms for cleaning .meta.. note: there already exists an mechanism to reload these regions \u2013 the bulk loaded mechanisms in loadincrementalhfiles can be used to re-add grandparents (automatically splitting them if necessary) to hbase. ",
        "label": 242
    },
    {
        "text": "illegalargumentexception  server cannot be null  i got a couple of these just now: 2008-10-25 00:05:25,192 warn org.apache.hadoop.hbase.regionserver.hregionserver: processing message (retry: 0) java.io.ioexception: java.io.ioexception: java.lang.illegalargumentexception: server cannot be null         at org.apache.hadoop.hbase.master.metaregion.<init>(metaregion.java:42)         at org.apache.hadoop.hbase.master.processregionstatuschange.<init>(processregionstatuschange.java:45)         at org.apache.hadoop.hbase.master.processregionopen.<init>(processregionopen.java:50)         at org.apache.hadoop.hbase.master.servermanager.processregionopen(servermanager.java:467)         at org.apache.hadoop.hbase.master.servermanager.processmsgs(servermanager.java:340)         at org.apache.hadoop.hbase.master.servermanager.processregionserverallswell(servermanager.java:314)         at org.apache.hadoop.hbase.master.servermanager.regionserverreport(servermanager.java:233)         at org.apache.hadoop.hbase.master.hmaster.regionserverreport(hmaster.java:569)         at sun.reflect.generatedmethodaccessor3.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:623)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:622)         at org.apache.hadoop.ipc.server$handler.run(server.java:888)         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)         at java.lang.reflect.constructor.newinstance(constructor.java:539)         at org.apache.hadoop.hbase.remoteexceptionhandler.decoderemoteexception(remoteexceptionhandler.java:82)         at org.apache.hadoop.hbase.remoteexceptionhandler.checkioexception(remoteexceptionhandler.java:48)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:404)         at java.lang.thread.run(thread.java:674) ",
        "label": 314
    },
    {
        "text": "reenable cleanup in test teardown in testaccesscontroller3 disabled by hbase  we'd commented out cleanup in the test because test was stalling on it. this issue is about restoring it. ",
        "label": 314
    },
    {
        "text": "setting cache blocks to false in an hbase shell scan doesn't work  i was attempting to prevent blocks from being cached by setting cache_blocks => false in the hbase shell when doing a scan but i kept seeing tons of evictions when i ran it. after inspecting \"table.rb\" i found this line: cache = args[\"cache_blocks\"] || true the problem then is that if cache_blocks is false then this expression will always return true. therefore, it's impossible to turn off block caching. ",
        "label": 314
    },
    {
        "text": "minor fixes in row lock implementation  in reviewing the hregion.getrowlockinternal() implementation, i noticed a couple of possible issues that i wanted to pass along: make sure rowlockcontext.cleanup() always called if row lock could not be acquired do not attempt to remove unusable row lock from map multiple times. i'll post a patch for consideration. ",
        "label": 225
    },
    {
        "text": "testget2 testgetclosestbefore fails with hadoop  testget2.testgetclosestbefore fails with hadoop-0.17.1 after the rows are flushed to a mapfile, we get no result when we try to find the closest row before 038. we find 035, but that is deleted. so we advance, the next record is 040 which is after 038 and we give up. this results in a null result being passed back to the test which then dies with an npe because it expects that getclosestrowbefore should find row 030. it appears that there is no logic to back up from a candidate row if the candidate came before the desired key but is deleted. we should find the row before. i'm guessing that this is failing because hadoop-0.17.1 incorporates hadoop-3472 (mapfile.reader getclosest() function returns incorrect results when before is true) ",
        "label": 241
    },
    {
        "text": "web ui framable page  the web uis do not include the \"x-frame-options\" header to prevent the pages from being framed from another site. reference:  https://www.owasp.org/index.php/clickjacking  https://www.owasp.org/index.php/clickjacking_defense_cheat_sheet  https://developer.mozilla.org/en-us/docs/web/http/x-frame-options ",
        "label": 177
    },
    {
        "text": "testmetareadereditor is missing call to catalogtracker stop   i noticed that testmetareadereditor hung on 0.92 jenkins builds - see https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.92/249/console it turns out that catalogtracker.stop() is missing. ",
        "label": 441
    },
    {
        "text": "deadlock between hregion put and hregion close  hbase-2037 added a bunch of fixes but also a deadlock: hregion.put: splitsandcloseslock.readlock().lock(); newscannerlock.writelock().lock(); hregion.close newscannerlock.writelock().lock(); try {   splitsandcloseslock.writelock().lock(); to recreate, start a performanceevaluation on standalone and it happens roughly 75% of the time. ",
        "label": 314
    },
    {
        "text": "addition to supporting projects page  at flipkart, we are using a light-weight orm wrapper on top of hbase-client: hbase-orm (open source, apache 2.0 license) it helps java applications interact with hbase in an object-oriented manner, by encapsulating get, put, delete, append, increment and other classes in hbase-client.     it's published on maven central:  https://search.maven.org/search?q=g:com.flipkart%20and%20a:hbase-object-mapper&core=gav kindly list this as supporting project. pr to follow.    ",
        "label": 301
    },
    {
        "text": "response size calculated in rpcserver for warning toolarge responses does not count cellscanner payload  after hbase-13158 where we respond back to rpcs with cells in the payload , the protobuf response will just have the count the cells to read from payload, but there are set of features where we log warn in rpcserver whenever the response is toolarge, but this size now is not considering the sizes of the cells in the payloadcellscanner. code form rpcserver       long responsesize = result.getserializedsize();       // log any rpc responses that are slower than the configured warn       // response time or larger than configured warning size       boolean tooslow = (processingtime > warnresponsetime && warnresponsetime > -1);       boolean toolarge = (responsesize > warnresponsesize && warnresponsesize > -1);       if (tooslow || toolarge) {         // when tagging, we let toolarge trump toosmall to keep output simple         // note that large responses will often also be slow.         logresponse(new object[]{param},             md.getname(), md.getname() + \"(\" + param.getclass().getname() + \")\",             (toolarge ? \"toolarge\" : \"tooslow\"),             status.getclient(), starttime, processingtime, qtime,             responsesize);       } should this feature be not supported any more or should we add a method to cellscanner or a new interface which returns the serialized size (but this might not include the compression codecs which might be used during response ?) any other idea this could be fixed ? ",
        "label": 234
    },
    {
        "text": "metascanner and metareader are very similar  purge one  metascanner in client package is a little more involved but metareader does similar. both allow you specify a visitor on .meta. and root. we should dump one of them. ",
        "label": 41
    },
    {
        "text": "hbase should be based on hadoop  hbase 0.2.0 release should be based on hadoop 0.17.2 (or -dev it not available when we release 0.2.0) for the following two dfs client issues.  hadoop-3760. fix a bug with hdfs file close() mistakenly introduced  by hadoop-3681. (lohit vijayarenu via rangadi)  hadoop-3681. dfsclient can get into an infinite loop while closing  a file if there are some errors. (lohit vijayarenu via rangadi) should we set it as blocker? ",
        "label": 241
    },
    {
        "text": "polish the migration to  currently, migration works but there's still a couple of rough edges: hbase-8045 finished the .meta. migration but didn't remove root, so it's still on the filesystem. data in zk needs to be removed manually. either we fix up the data in zk or we delete it ourselves. testmetamigrationremovinghtd has a testmetaupdatedflaginroot method, but root is gone now. elliott was also mentioning that we could have \"hbase migrate\" do the hfilev1 checks, clear zk, remove root, etc. ",
        "label": 543
    },
    {
        "text": "hconnection hmasterinterface should allow for way to get hostname of currently active master in multi master hbase setup  i have a multi-master hbase set up, and i'm trying to programmatically determine which of the masters is currently active. but the api does not allow me to do this. there is a getmaster() method in the hconnection class, but it returns an hmasterinterface, whose methods do not allow me to find out which master won the last race. the api should have a getactivemasterhostname() or something to that effect. ",
        "label": 131
    },
    {
        "text": "when an exception bubbles out of getregionserverwithretries  wrap the exception with a retriesexhaustedexception  there's always a lot of confusion about what notservingregionexceptions and wrongregionexceptions mean when they come out of the client side. to help alleviate this, i propose that we create a new exception type called retriesexhaustedexception that wraps the actual thrown exception and presents it clearly as a retry issue. this will be very helpful, i think. perhaps the exception can even take a list of the exceptions that led up to this point. ",
        "label": 86
    },
    {
        "text": "metatableaccessor shouldn't use zookeeeper  after committing patch for hbase-4495, there's an further improvement which can be made (discussed originally on review board to that jira). we have metatableaccessor and metatablelocator classes. first one is used to access information stored in hbase:meta table. second one is used to deal with zookeeper state to find out region server hosting hbase:meta, wait for it to become available and so on. metatableaccessor, in turn, should only operate on the meta table content, so shouldn't need zk. the only reason why metatableaccessor is using zk - when callers request assignment information, they can request location of meta table itself, which we can't read from meta, so in that case metatableaccessor relays the call to metatablelocator. may be the solution here is to declare that clients of metatableaccessor shall not use it to work with meta table itself (not it's content). ",
        "label": 323
    },
    {
        "text": " hbck2 hbase operator tools  release making scripts  make some scripts for creating hbase-operator-tools releases so its easy to do and not subject to vagaries of environment or rm's attention-to-detail (or not). ",
        "label": 314
    },
    {
        "text": "hrs npe on way out if no master to connect to  08/10/14 18:13:17 fatal regionserver.hregionserver: unhandled exception. aborting... java.io.ioexception: call failed on local exception         at org.apache.hadoop.ipc.client.call(client.java:718)         at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:245)         at $proxy0.getprotocolversion(unknown source)         at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:388)         at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:364)         at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:412)         at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:328)         at org.apache.hadoop.hbase.regionserver.hregionserver.reportforduty(hregionserver.java:709)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:290)         at java.lang.thread.run(thread.java:674) caused by: java.net.connectexception: connection refused         at sun.nio.ch.socketchannelimpl.checkconnect(native method)         at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:592)         at sun.nio.ch.socketadaptor.connect(socketadaptor.java:118)         at org.apache.hadoop.ipc.client$connection.setupiostreams(client.java:300)         at org.apache.hadoop.ipc.client$connection.access$1700(client.java:177)         at org.apache.hadoop.ipc.client.getconnection(client.java:789)         at org.apache.hadoop.ipc.client.call(client.java:704)         ... 9 more 08/10/14 18:13:17 debug hbase.regionhistorian: offlined 08/10/14 18:13:17 info ipc.server: stopping server on 60020 exception in thread \"regionserver/0:0:0:0:0:0:0:0:60020\" java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:449)         at java.lang.thread.run(thread.java:674) ",
        "label": 314
    },
    {
        "text": "add a new admin method and shell cmd to trigger the hbck chore to run  a follow-on issue after hbase-22709. oh, also as a follow-on, should make it so user can trigger this chore to run... or perhaps a new shell command which is called hbck_report that runs this chore and the catalogjanitor, etc., to produce all hbck. ",
        "label": 187
    },
    {
        "text": "allow regions to split around scanners  we have a number of scanners iterating over a table that also sees a lot of constant write activity. if the scans are too frequent we will suppress splitting. at a lull then a number of splits happen all at once, occasionally overwhelming dfs and causing file corruption. i wonder how much work it would be to split regions around scanners. rather than wait for scanner leases to expire, suspend/block the scanner, split the table, and then negotiate with the client to continue. ",
        "label": 314
    },
    {
        "text": "jruby shell for replication  we need a shell to easily issue administration commands for replication. it should be easy to merge with existing core shell. ",
        "label": 229
    },
    {
        "text": "fix races in slab cache  a few races are still lingering in the slab cache. here are some tests and proposed fixes. ",
        "label": 289
    },
    {
        "text": "user permission command encounters nullpointerexception  as user hbase, user_permission command gave: java.io.ioexception: java.io.ioexception   at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2185)   at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1854) caused by: java.lang.nullpointerexception   at org.apache.hadoop.hbase.security.access.accesscontrollists.getusertablepermissions(accesscontrollists.java:484)   at org.apache.hadoop.hbase.security.access.accesscontroller.getuserpermissions(accesscontroller.java:1341)   at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice$1.getuserpermissions(accesscontrolprotos.java:9949)   at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice.callmethod(accesscontrolprotos.java:10107)   at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:5121)   at org.apache.hadoop.hbase.regionserver.hregionserver.execservice(hregionserver.java:3211)   at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26851)   at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2147)   ... 1 more   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)   at java.lang.reflect.constructor.newinstance(constructor.java:513)   at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:106)   at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:95)   at org.apache.hadoop.hbase.protobuf.protobufutil.getremoteexception(protobufutil.java:235)   at org.apache.hadoop.hbase.protobuf.protobufutil.execservice(protobufutil.java:1304)   at org.apache.hadoop.hbase.ipc.regioncoprocessorrpcchannel$1.call(regioncoprocessorrpcchannel.java:87)   at org.apache.hadoop.hbase.ipc.regioncoprocessorrpcchannel$1.call(regioncoprocessorrpcchannel.java:84)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:120)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:98)   at org.apache.hadoop.hbase.ipc.regioncoprocessorrpcchannel.callexecservice(regioncoprocessorrpcchannel.java:90)   at org.apache.hadoop.hbase.ipc.coprocessorrpcchannel.callblockingmethod(coprocessorrpcchannel.java:67)   at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice$blockingstub.getuserpermissions(accesscontrolprotos.java:10304)   at org.apache.hadoop.hbase.protobuf.protobufutil.getuserpermissions(protobufutil.java:1974)   at sun.reflect.nativemethodaccessorimpl.invoke0(native method)   at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)   at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)   at java.lang.reflect.method.invoke(method.java:597) ... caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception(java.io.ioexception): java.io.ioexception   at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2185)   at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1854) caused by: java.lang.nullpointerexception   at org.apache.hadoop.hbase.security.access.accesscontrollists.getusertablepermissions(accesscontrollists.java:484)   at org.apache.hadoop.hbase.security.access.accesscontroller.getuserpermissions(accesscontroller.java:1341)   at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice$1.getuserpermissions(accesscontrolprotos.java:9949)   at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice.callmethod(accesscontrolprotos.java:10107)   at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:5121)   at org.apache.hadoop.hbase.regionserver.hregionserver.execservice(hregionserver.java:3211)   at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26851)   at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2147)   ... 1 more   at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1387)   at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1591)   at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1648)   at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$blockingstub.execservice(clientprotos.java:27327)   at org.apache.hadoop.hbase.protobuf.protobufutil.execservice(protobufutil.java:1300)   ... 234 more according to help for user_permission command: for example:     hbase> user_permission     hbase> user_permission 'table1' not specifying table name should be accepted. ",
        "label": 441
    },
    {
        "text": "hfilereaderv2 prefetch overreaches  runs off the end of the data  small bug found while investigating another issue related to prefetch ",
        "label": 314
    },
    {
        "text": "balancer kicks in way too early  balancer kicks in before all regions are assigned out. causes confusion. master won't accept opens from \"overloaded\" hrs. master is slow to respond to ui and hrs during. master sometimes takes too long to respond to a hrs heartbeat and so the hrs will reinit. this causes more confusion. ",
        "label": 314
    },
    {
        "text": "enable testmasteroperationsforregionreplicas testcreatetablewithmultiplereplicas disabled by proc v2 am in hbase  the following replica tests were disabled by core proc-v2 am in hbase-14614: disabled parts of...testcreatetablewithmultiplereplicas in testmasteroperationsforregionreplicas there is an issue w/ assigning more replicas if number of replicas is changed on us. see '/* disabled!!!!! for now!!!!'. note we moved fixing of the below two tests out to hbase-19268 disabled testregionreplicasonmidclusterhighreplication in teststochasticloadbalancer2 disabled testflushandcompactionsinprimary in testregionreplicas this jira tracks the work to enable them (or modify/remove if not applicable). ",
        "label": 205
    },
    {
        "text": "testmetamigrationconvertingtopb fails on hadoop due to filenotfoundexception  namespaceupgrade is called by testmetamigrationconvertingtopb  testmetamigrationconverttopb.tgz doesn't contain .archive directory.  namespaceupgrade#migratetables() calls liststatus() on .archive directory (hardcoded)  due to change of behavior in hadoop 2.0, filenotfoundexception gets thrown and makes the test fail. ",
        "label": 544
    },
    {
        "text": "provide  editorconfig based on checkstyle configuration  i don't have an exhaustive analysis of the issue, but there's at least one case where check style plugin configuration disagrees with our settings in dev-support/hbase_eclipse_formatter.xml. formatter settings produce this code chunk       uncaughtexceptionhandler =           (t, e) -> abort(\"uncaught exception in executorservice thread \" + t.getname(), e); but check style wants       uncaughtexceptionhandler =         (t, e) -> abort(\"uncaught exception in executorservice thread \" + t.getname(), e); ",
        "label": 339
    },
    {
        "text": "remove unused test class from testhlogsplit  with the changes introduced by hbase-8962, we no longer need the class zombienewlogwriterregionserver in testhlogsplit. it should be removed. ",
        "label": 323
    },
    {
        "text": "fix javadoc warning in importtsv tsvparser ctor  from https://builds.apache.org/job/precommit-hbase-build/7623/artifact/trunk/patchprocess/patchjavadocwarnings.txt : [warning] javadoc warnings [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/importtsv.java:123: warning - @param argument \"tagseperatorstr\" is not a parameter name. ",
        "label": 544
    },
    {
        "text": "masterprocwals never clean up  masterprocwals directory grows pretty much un-bounded. because of that when master failover happens the nn is flooded with connections and everything grinds to a halt. ",
        "label": 309
    },
    {
        "text": "better readability of   hbase regionserver lease period  property - \"hbase.regionserver.lease.period\" hardcode would be ideally moved to hconstants. also - default value seems to be listed all over the place, (thankfully consistent though). that would be nice to be in hconstants too .. $ find . -name *.java | xargs grep -nr hbase.regionserver.lease.period  ./src/contrib/mdc_replication/src/java/org/apache/hadoop/hbase/replication/replicationconnectionmanager.java:59: this.rpctimeout = conf.getlong(\"hbase.regionserver.lease.period\", 60000);  ./src/java/org/apache/hadoop/hbase/client/hconnectionmanager.java:298: this.rpctimeout = conf.getlong(\"hbase.regionserver.lease.period\", 60000);  ./src/java/org/apache/hadoop/hbase/client/htable.java:121: conf.getint(\"hbase.regionserver.lease.period\", 60 * 1000);  ./src/java/org/apache/hadoop/hbase/regionserver/hregionserver.java:271: this.rpctimeout = conf.getlong(\"hbase.regionserver.lease.period\", 60000);  ./src/java/org/apache/hadoop/hbase/regionserver/hregionserver.java:335: conf.getint(\"hbase.regionserver.lease.period\", 60 * 1000), ./src/test/org/apache/hadoop/hbase/regionserver/testhregion.java:1890: conf.setint(\"hbase.regionserver.lease.period\", 10 * 1000); more of a nit, but provides better readability across. ",
        "label": 266
    },
    {
        "text": "fully qualified hbase rootdir doesn't work  jim was setting up cluster w/ new hbase. setting fully qualified hbase.rootdir was failing. complaint was that the filesystems didn't match \u2013 i.e. the hdfs of the fully qualified hbase.rootdir didn't jibe w/ the default hadoop file:///. fix needs to be backported. the problem was that because the hadoop config files were not found (because they are in a different directory and not on the classpath) then fs.get(conf) returns file:/// ",
        "label": 241
    },
    {
        "text": "turn of logging of every catalog table row entry on every scan  just log edits. in an install with many tables and thousands of regions per table, the debug log is clogged with statements of the obvious. ",
        "label": 314
    },
    {
        "text": "ensure that scanners that read from the storefiles respect mvcc  currently, the key-values written to the disk do not include the mvcc (rwcc) version information. once we add that  information, and make it persistent to disk; let us make the scanners respect the mvcc mechanism by ignoring   \"newer\" writes. ",
        "label": 34
    },
    {
        "text": "sending random data crashes thrift service  upstream thrift library has a know issue (thrift-601) causing the thrift server to crash with an out-of-memory error when bogus requests are sent. this reproduces when a very large request size is sent in the request header, making the thrift server to allocate a large memory segment leading to oom. loadbalancer health checks are the first \"candidate\" for bogus requests  thrift developers admit this is a known issue with tbinaryprotocol and their recommandation is to use tcompactprotocol/tframedtransport but this requires all thrift clients to be updated (might not be feasible atm) so we need a fix similar to cassandra-475. ",
        "label": 9
    },
    {
        "text": "master jsp   table jsp do not uri encode table or region names in links  \"uazaaaaaznagneki+gc\" is a key in my \"userdata\" table which happens to be the start key for a region named \"userdata,uazaaaaaznagneki+gc,1238170268268\" \"/table.jsp?name=userdata\" lists a link to \"/regionhistorian.jsp?regionname=userdata,uazaaaaaznagneki+gc,1238170268268\" which is incorrect because the \" + \" character is not properly uri encoded. this results in a misleading user error message: \"this region is no longer available. it may be due to a split, a merge or the name changed. \" manually escaping the \" + \" character as \"%2b\" produces the correct output. a quick skim of master.jsp suggests it has a similar problem: it doesn't uri encode table names when constructing links to table.jsp ",
        "label": 285
    },
    {
        "text": "metric for blocked updates  when the disc subsystem cannot keep up with a sustained high write load, a region will eventually block updates to throttle clients.  (hregion.checkresources). it would be nice to have a metric for this, so that these occurrences can be tracked. ",
        "label": 312
    },
    {
        "text": "add initialize method to load balancer interface  the load balancer has two methods setmasterservices and setconf that needs to be called prior to it being functional. some balancers will need to go through an initialization procedure once these methods have been called. an initialize() method would be helpful in this regard. ",
        "label": 174
    },
    {
        "text": "when set blocksize option in performance evaluation tool  error occurs error  unrecognized option command   blocksize  i believe \"blocksize\" is an new options for pe in hbase2.0, when i try to set the blocksize, error occurs:error: unrecognized option/command: --blocksize=131072. the error occurs because of missing a \"continue;\" when we match the option \"blocksize\". if there isn't a \"continue\" the program will execute the last \"printusageandexit branch\". ",
        "label": 526
    },
    {
        "text": "race condition in testzkbasedopencloseregion  testcloseregion is called before testreopenregion. here's the sequence of events: 2014-04-18 20:58:05,645 info  [thread-380] master.testzkbasedopencloseregion(313): running testcloseregion 2014-04-18 20:58:05,645 info  [thread-380] master.testzkbasedopencloseregion(315): number of region servers = 2 2014-04-18 20:58:05,645 info  [thread-380] master.testzkbasedopencloseregion(164): -root-,,0.70236052 2014-04-18 20:58:05,646 debug [thread-380] master.testzkbasedopencloseregion(320): asking rs to close region -root-,,0.70236052 ... 2014-04-18 20:58:06,237 info  [rs_close_root-hemera.apache.org,46533,1397854669633-0] regionserver.hregion(1148): closed -root-,,0.70236052 ... 2014-04-18 20:58:06,404 info  [thread-380] master.testzkbasedopencloseregion(333): done with testcloseregion then 2014-04-18 20:58:06,431 info  [pool-1-thread-1] hbase.resourcechecker(157): before master.testzkbasedopencloseregion#testreopenregion: 234 threads, 388 file descriptors 4 connections,  ... 2014-04-18 20:58:06,466 debug [master_open_region-hemera.apache.org,52650,1397854669138-3] zookeeper.zkutil(1597): master:52650-0x14576a1835d0000 retrieved 62 byte(s) of data from znode /hbase/unassigned/70236052; data=region=-root-,,0, origin=hemera.apache.org,46533,1397854669633, state=rs_zk_region_opened 2014-04-18 20:58:06,473 debug [pool-1-thread-1] client.clientscanner(191): finished with scanning at {name => '.meta.,,1', startkey => '', endkey => '', encoded => 1028785192,} 2014-04-18 20:58:06,473 info  [thread-396] master.testzkbasedopencloseregion(123): number of region servers = 2 2014-04-18 20:58:06,474 info  [thread-396] master.testzkbasedopencloseregion(164): -root-,,0.70236052 2014-04-18 20:58:06,474 debug [thread-396] master.testzkbasedopencloseregion(130): asking rs to close region -root-,,0.70236052 2014-04-18 20:58:06,474 info  [thread-396] master.testzkbasedopencloseregion(147): unassign -root-,,0.70236052 2014-04-18 20:58:06,474 debug [thread-396] master.assignmentmanager(2126): starting unassignment of region -root-,,0.70236052 (offlining) 2014-04-18 20:58:06,475 debug [thread-396] master.assignmentmanager(2132): attempted to unassign region -root-,,0.70236052 but it is not currently assigned anywhere 2014-04-18 20:58:06,478 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(294): master:52650-0x14576a1835d0000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/unassigned/70236052 2014-04-18 20:58:06,478 debug [pool-1-thread-1-eventthread] master.assignmentmanager(1176): the znode of region -root-,,0.70236052 has been deleted. 2014-04-18 20:58:06,478 info  [pool-1-thread-1-eventthread] master.assignmentmanager(1188): the master has opened the region -root-,,0.70236052 that was online on hemera.apache.org,46533,1397854669633 2014-04-18 20:58:06,478 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(294): master:52650-0x14576a1835d0000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/unassigned then nothing happens. so testcloseregion unassigns the root region and testreopenregion starts before root is reassigned. hence it waits forever for the close event, since it never happens. this is the key \"master.assignmentmanager(2132): attempted to unassign region root,,0.70236052 but it is not currently assigned anywhere\" the easiest fix is to just run testcloseregion last (as it was before we switched junit). ",
        "label": 286
    },
    {
        "text": "cme in zkw introduced in hbase  saw this while tail'ing a log for something else: 2010-06-15 17:30:03,769 error [main-eventthread] zookeeper.clientcnxn$eventthread(490): error while calling watcher java.util.concurrentmodificationexception         at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)         at java.util.abstractlist$itr.next(abstractlist.java:343)         at org.apache.hadoop.hbase.zookeeper.zookeeperwrapper.process(zookeeperwrapper.java:235) looks like the listeners list's iterator is used in an unprotected manner. ",
        "label": 268
    },
    {
        "text": "exportsnapshot does not honor  chmod option  create a snapshot of an existing hbase table, export the snapshot using the -chuser, -chgroup, -chmod options.  look in hdfs filesystem for export. the files do not have the correct ownership, group, permissions thanks to ian roberts who first reported the issue. ",
        "label": 441
    },
    {
        "text": "not able to run the test suite in background because testshell gets suspended on tty output  can't run the test suite in background. problem seems to be due to testshell. this works fine: % mvn test -dtest=testshell -dtest.output=true but: % mvn test -dtest=testshell -dtest.output=true &  or, % mvn test -dtest=testshell -dtest.output=true >& test.log & causes test to hang, and eventually timeout after 3600 seconds. the process is reported as being suspended on tty output. [3] + suspended (tty output) mvn test -dtest=testshell -dtest.output=true ",
        "label": 25
    },
    {
        "text": "bufferedmutatoroverasyncbufferedmutator trycompletefuture consume too much cpu time  ",
        "label": 149
    },
    {
        "text": "prefixfilter doesn't filter all remaining rows if the prefix is longer than rowkey being compared  the prefixfilter will filter rowkey as:   public boolean filterrowkey(cell firstrowcell) {     ...     int length = firstrowcell.getrowlength();     if (length < prefix.length) return true; // ===> return directly if the prefix is longer     ....     if ((!isreversed() && cmp > 0) || (isreversed() && cmp < 0)) {       passedprefix = true;     }     filterrow = (cmp != 0);     return filterrow;   } if the prefix is longer than the current rowkey, prefixfilter#filterrowkey will filter the rowkey directly without comparing, so that won't set 'passedprefix' flag even the current row is larger than the prefix.  for example, if there are three rows 'a', 'b' and 'c' in the table, and we issue a scan request as: hbase(main):001:0> scan 'test_table', {startrow => 'a', filter => \"(prefixfilter ('aa'))\"} the region server will check the three rows before returning. in our production, the user issue a scan with a prefixfilter. the prefix is longer than the rowkeys of following millions of rows, so the region server will continue to check rows until hit a rowkey longer than the prefix. this make the client easily timeout. to fix this case, it seems we need to compare the prefix with the rowkey every serveral rows even when the prefix is longer. ",
        "label": 238
    },
    {
        "text": "try to remove all minimapreducecluster in unit tests  as discussion in dev list, we will try to do mr job without minimapreducecluster. testcases will run faster and more reliable. ",
        "label": 198
    },
    {
        "text": "global authorization may lose efficacy  it depends on the order of which region be opened first.   suppose we have one 1 regionserver and only 1 user region region-a on this server, acl region was on another regionserver. acl was opened a few seconds before region-a.  the global authorization data read from zookeeper was overwritten by the data read from configuration.   private tableauthmanager(zookeeperwatcher watcher, configuration conf)       throws ioexception {     this.conf = conf;     this.zkperms = new zkpermissionwatcher(watcher, this, conf);     try {   // read global authorization data from zookeeper.        this.zkperms.start();     } catch (keeperexception ke) {       log.error(\"zookeeper initialization failed\", ke);     }     // it will overwrite globalcache.     // initialize global permissions based on configuration     globalcache = initglobal(conf);   } this issue can be easily reproduced by below steps:  1. start a cluster with 3 regionservers.  2. create a new table t1.  3. grant a new user user-a with global authorization.  4. kill 1 regionserver rs3 and switch balance off.  5. start regionserver rs3.  6. assign region t1 to rs3.  7. put data with user user-a. ",
        "label": 240
    },
    {
        "text": "nightly job needs to run all stages and then comment on jira  follow on from hbase-18147, need a post action that pings all newly-committed jiras with result of the branch build ",
        "label": 402
    },
    {
        "text": "hmaster is unable to start of hfile v1 is used  this was reported by hh zhu (zhh200910@gmail.com)  if the following is specified in hbase-site.xml\uff1a     <property>         <name>hfile.format.version</name>         <value>1</value>     </property> clear the hdfs directory \"hbase.rootdir\" so that masterfilesystem.bootstrap() is executed.  you would see: java.lang.nullpointerexception     at org.apache.hadoop.hbase.io.hfile.hfilereaderv1.close(hfilereaderv1.java:358)     at org.apache.hadoop.hbase.regionserver.storefile$reader.close(storefile.java:1083)     at org.apache.hadoop.hbase.regionserver.storefile.closereader(storefile.java:570)     at org.apache.hadoop.hbase.regionserver.store.close(store.java:441)     at org.apache.hadoop.hbase.regionserver.hregion.doclose(hregion.java:782)     at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:717)     at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:688)     at org.apache.hadoop.hbase.master.masterfilesystem.bootstrap(masterfilesystem.java:390)     at org.apache.hadoop.hbase.master.masterfilesystem.checkrootdir(masterfilesystem.java:356)     at org.apache.hadoop.hbase.master.masterfilesystem.createinitialfilesystemlayout(masterfilesystem.java:128)     at org.apache.hadoop.hbase.master.masterfilesystem.<init>(masterfilesystem.java:113)     at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:435)     at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:314)     at java.lang.thread.run(thread.java:619) the above exception would lead to: java.lang.runtimeexception: hmaster aborted     at org.apache.hadoop.hbase.master.hmastercommandline.startmaster(hmastercommandline.java:152)     at org.apache.hadoop.hbase.master.hmastercommandline.run(hmastercommandline.java:103)     at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65)     at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:76)     at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:1512) in org.apache.hadoop.hbase.master.hmaster.hmaster(configuration conf), we have: this.conf.setfloat(cacheconfig.hfile_block_cache_size_key, 0.0f); when cacheconfig is instantiated, the following is called: org.apache.hadoop.hbase.io.hfile.cacheconfig.instantiateblockcache(configuration conf) since \"hfile.block.cache.size\" is 0.0, instantiateblockcache() would return null, resulting in blockcache field of cacheconfig to be null.  when master closes root region, org.apache.hadoop.hbase.io.hfile.hfilereaderv1.close(boolean evictonclose) would be called. cacheconf.getblockcache() returns null, leading to master abort. the following should be called in hfilereaderv1.close(), similar to the code in hfilereaderv2.close(): if (evictonclose && cacheconf.isblockcacheenabled()) ",
        "label": 204
    },
    {
        "text": "adding a flush file of zero entries  saw this in log in trunk:     [junit] 2008-04-04 20:22:40,943 debug [regionserver:0.cacheflusher] regionserver.hstore(676): added 1403560700/text/8075392345773720818 with 0 entries, sequence id 537, data size 0.0, file size 110.0 for 1403560700/text i thought that we'd fixed flushing zero-entry files ",
        "label": 241
    },
    {
        "text": "prefetching  meta  rows in case only when usecache is set to true  while doing a .meta. lookup (hcm#locateregioninmeta), we also prefetch some other region's info for that table. the usual call to the meta lookup has usecache variable set to true.   currently, it calls prefetch irrespective of the value usecache flag:             if (bytes.equals(parenttable, hconstants.meta_table_name) &&                 (getregioncacheprefetch(tablename)))  {               prefetchregioncache(tablename, row);             } later on, if usecache flag is set to false, it deletes the entry for that row from the cache with a forcedeletecachedlocation() call. this always results in two calls to the .meta. table in this case. the usecache variable is set to false in case we are retrying to find a region (regionserver failover). it can be verified from the log statements of a client while having a regionserver failover. in the below example, the client was connected to a1217, when a1217 got killed. the region in question is moved to a1215. client got this info from meta scan, where as client cache this info from meta, but then delete it from cache as it want the latest info.   the result is even the meta provides the latest info, it is still deleted this causes even the latest info to be deleted. thus, client deletes a1215.abc.com even though it is correct info. 13/04/15 09:49:12 debug client.hconnectionmanager$hconnectionimplementation: cached location for t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. is a1217.abc.com:40020 13/04/15 09:49:12 warn client.servercallable: received exception, tries=1, numretries=30 message=connection refused 13/04/15 09:49:12 debug client.hconnectionmanager$hconnectionimplementation: removed all cached region locations that map to a1217.abc.com,40020,1365621947381 13/04/15 09:49:13 debug client.metascanner: current info from scan results = {name => 't,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c.', startkey => 'user7225973201630273569', endkey => '', encoded => 40382355b8c45e1338d620c018f8ff6c,} 13/04/15 09:49:13 debug client.metascanner: scanning .meta. starting at row=t,user7225973201630273569,00000000000000 for max=10 rows using hconnection-0x7786df0f 13/04/15 09:49:13 debug client.metascanner: current info from scan results = {name => 't,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c.', startkey => 'user7225973201630273569', endkey => '', encoded => 40382355b8c45e1338d620c018f8ff6c,} 13/04/15 09:49:13 debug client.hconnectionmanager$hconnectionimplementation: cached location for t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. is a1215.abc.com:40020 13/04/15 09:49:13 debug client.hconnectionmanager$hconnectionimplementation: removed a1215.abc.com:40020 as a location of t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. for tablename=t from cache 13/04/15 09:49:13 debug client.metascanner: current info from scan results = {name => 't,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c.', startkey => 'user7225973201630273569', endkey => '', encoded => 40382355b8c45e1338d620c018f8ff6c,} 13/04/15 09:49:13 debug client.hconnectionmanager$hconnectionimplementation: cached location for t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. is a1215.abc.com:40020 13/04/15 09:49:13 warn client.servercallable: received exception, tries=2, numretries=30 message=org.apache.hadoop.hbase.exceptions.unknownscannerexception: name: -6313340536390503703, already closed? 13/04/15 09:49:13 debug client.clientscanner: advancing internal scanner to startkey at 'user760712450403198900' ",
        "label": 199
    },
    {
        "text": "hbase needs a 'safe mode'  internally we have a cluster of thousands of regions. we just did a hbase restart w/ master on new node. just so happened that one of the regionservers was running extra slow (was downloaded by other processes). meant that its portion of the assigments was taking a long time to come up... while these regions were stuck in deploy mode, the cluster is not useable. we need a sort of 'safe-mode' in hbase where clients fail if they try to attach to a cluster not yet fully up. ui should show when all assignments have been successfully made so admin can at least see when they have a problematic regionserver in their midst. ",
        "label": 241
    },
    {
        "text": "block cache can be mistakenly instantiated on master  after changes in the block cache instantiation over in hbase-4422, it looks like the hmaster can now end up with a block cache instantiated. not a huge deal but prevents the process from shutting down properly. ",
        "label": 247
    },
    {
        "text": "fix balancer in not moving regions off overloaded regionserver  we pushed 1.3 to a couple of clusters. in some cases the regions were assigned very un-evenly and the regions would not move after that. we ended up with one rs getting thousands of regions and most servers getting 0. running balancer would do nothing. the balancer would say that it couldn't find a solution with less than the current cost. ",
        "label": 154
    },
    {
        "text": "tablemapreduceutil findorcreatejar calls updatemap null    resulting in thrown npe  tablemapreduceutil.findorcreatejar line 596:  jar = getjar(my_class);  updatemap(jar, packagedclasses);  in case getjar returns null, updatemap will throw npe.  should check null==jar before calling updatemap. ",
        "label": 33
    },
    {
        "text": "compactions too slow  know there was some changes to compaction a few days back i just got back   to where i could run imports with the new client api  and noticing that compactions are vary slow i meand 100x+ slower then they   used to be i thought one of the servers compaction thread had died  but after retesting and ruleing that out.   i tested with and without compression and with and with out   blockcache and all are running about the same i got one small compaction to complete it took 1977 secs to compact 48.56mb  thats about 25kb sec ",
        "label": 547
    },
    {
        "text": "testsplittransactiononcluster testsshcleanupdaugtherregionsofabortedsplit is flakey  this one fails a bit on master at least: https://builds.apache.org/view/h-l/view/hbase/job/hbase-trunk_matrix/496/jdk=latest1.8,label=hadoop/testreport/junit/org.apache.hadoop.hbase.regionserver/testsplittransactiononcluster/testsshcleanupdaugtherregionsofabortedsplit/history/ ... about 15% of the time. ",
        "label": 198
    },
    {
        "text": "if hbase directory exists but version file is inexistent  still proceed with bootstrapping  on the dev list i suggested we change the way we manage the empty hbase directory case. stack answered: yes. in fact, its probably safe-to-do now we've left far behind the  pre-history versions of hbase where there was no hbase.version file in the  hbase.rootdir. if absent, lets proceed and just write it rather than treat  it as a non-migrated instance ",
        "label": 167
    },
    {
        "text": "add note to ref guide about snapshots and ec2 reverse dns requirements   from irc from mighty jeremy carroll. 17:10 <jeremy_carroll> jmhsieh: i think i found the root cuase. all my region servers reach the barrier, but it does not continue. 17:11 <jeremy_carroll> jmhsieh: all rs have this in their logs: 2013-05-01 00:04:56,356 debug org.apache.hadoop.hbase.procedure.subprocedure: subprocedure 'backup1' coordinator notified of 'acquire', waiting on 'reached' or 'abort' from coordinator. 17:11 <jeremy_carroll> jmhsieh: then the coordinator (master) never sends anything. they just sit until the timeout. 17:12 <jeremy_carroll> jmhsieh: so basically 'reached' is never obtained. then abort it set, and it fails. ... 17:24 <jeremy_carroll> jmhsieh: found the bug. the hostnames dont match the master due to dns resolution 17:25 <jeremy_carroll> jmhsieh: the barrier aquired is putting in the local hostname from the regionservers. in ec2 (where reverse dns does not work well), the master hands the internal name to the client. 17:25 <jeremy_carroll> jmhsieh: https://s3.amazonaws.com/uploads.hipchat.com/23947/185789/au94meik0h3y5ii/screen%20shot%202013-04-30%20at%2017.25.50.png  17:26 <jeremy_carroll> jmhsieh: so it's waiting for something like 'ip-10-155-208-202.ec2.internal,60020,1367366580066' znode to show up, but instead 'hbasemetaclustera-d1b0a484,60020,1367366580066,' is being inserted. barrier is not reached 17:27 <jeremy_carroll> jmhsieh: reason being in our environment the master does not have a reverse dns entry. so we get stuff like this on regionserver startup in our logs. 17:27 <jeremy_carroll> jmhsieh: 2013-05-01 00:03:00,614 info org.apache.hadoop.hbase.regionserver.hregionserver: master passed us hostname to use. was=hbasemetaclustera-d1b0a484, now=ip-10-155-208-202.ec2.internal 17:54 <jeremy_carroll> jmhsieh: that was it. verified. now that reverse dns is working, snapshots are working. now how to figure out how to get reverse dns working on route53. i wished there was something like 'slave.host.name' inside of hadoop for this. looking at source code. ",
        "label": 330
    },
    {
        "text": "classnotfoundexception while running it tests in trunk using 'mvn verify'  trying to run  mvn verify -dit.test=integrationtestbiglinkedlist -dtest.output.tofile=false causes this classnotfoundexception issue testcontinuousingest(org.apache.hadoop.hbase.test.integrationtestbiglinkedlist): org/jboss/netty/channel/channelfactory ",
        "label": 544
    },
    {
        "text": "schemametrics updateoncachehit costs too much while full scanning a table with all of its fields  the schemametrics.updateoncachehit costs too much while i am doing the full table scanning.  here is the top 5 hotspots within regionserver while full scanning a table: (sorry for the less-well-format) cpu: intel westmere microarchitecture, speed 2.262e+06 mhz (estimated)  counted cpu_clk_unhalted events (clock cycles when not halted) with a unit mask of 0x00 (no unit mask) count 5000000  samples % image name symbol name  -------------------------------------------------------------------------------  98447 13.4324 14033.jo void org.apache.hadoop.hbase.regionserver.metrics.schemametrics.updateoncachehit(org.apache.hadoop.hbase.io.hfile.blocktype$blockcategory, boolean)  98447 100.000 14033.jo void org.apache.hadoop.hbase.regionserver.metrics.schemametrics.updateoncachehit(org.apache.hadoop.hbase.io.hfile.blocktype$blockcategory, boolean) [self]  -------------------------------------------------------------------------------  45814 6.2510 14033.jo int org.apache.hadoop.hbase.keyvalue$keycomparator.comparerows(byte[], int, int, byte[], int, int)  45814 100.000 14033.jo int org.apache.hadoop.hbase.keyvalue$keycomparator.comparerows(byte[], int, int, byte[], int, int) [self]  -------------------------------------------------------------------------------  43523 5.9384 14033.jo boolean org.apache.hadoop.hbase.regionserver.storefilescanner.reseek(org.apache.hadoop.hbase.keyvalue)  43523 100.000 14033.jo boolean org.apache.hadoop.hbase.regionserver.storefilescanner.reseek(org.apache.hadoop.hbase.keyvalue) [self]  -------------------------------------------------------------------------------  42548 5.8054 14033.jo int org.apache.hadoop.hbase.keyvalue$keycomparator.compare(byte[], int, int, byte[], int, int)  42548 100.000 14033.jo int org.apache.hadoop.hbase.keyvalue$keycomparator.compare(byte[], int, int, byte[], int, int) [self]  -------------------------------------------------------------------------------  40572 5.5358 14033.jo int org.apache.hadoop.hbase.io.hfile.hfileblockindex$blockindexreader.binarysearchnonrootindex(byte[], int, int, java.nio.bytebuffer, org.apache.hadoop.io.rawcomparator)~1  40572 100.000 14033.jo int org.apache.hadoop.hbase.io.hfile.hfileblockindex$blockindexreader.binarysearchnonrootindex(byte[], int, int, java.nio.bytebuffer, org.apache.hadoop.io.rawcomparator)~1 [self] ",
        "label": 95
    },
    {
        "text": "replicationzookeeper copyqueuesfromrsusingmulti should not return any queues if it failed to execute   we just ran into an interesting scenario. we restarted a cluster that was setup as a replication source.  the stop went cleanly. upon restart all regionservers aborted within a few seconds with variations of these errors:  http://pastebin.com/3iqvubqs ",
        "label": 199
    },
    {
        "text": "testcompaction and testcompactionwithcoprocessor run too long  584 seconds each testcompaction: forking command line: /bin/sh -c cd /data/src/hbase/hbase-server && /usr/lib/jvm/java-1.7.0.45-oracle-amd64/jre/bin/java -enableassertions -xmx1900m -xx:maxpermsize=100m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true -jar /data/src/hbase/hbase-server/target/surefire/surefirebooter5980733570856201818.jar /data/src/hbase/hbase-server/target/surefire/surefire4520171250819563114tmp /data/src/hbase/hbase-server/target/surefire/surefire_2794381603824180144412tmp running org.apache.hadoop.hbase.regionserver.testcompaction tests run: 18, failures: 0, errors: 0, skipped: 0, time elapsed: 584.609 sec testcompactionwithcoprocessor: forking command line: /bin/sh -c cd /data/src/hbase/hbase-server && /usr/lib/jvm/java-1.7.0.45-oracle-amd64/jre/bin/java -enableassertions -xmx1900m -xx:maxpermsize=100m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true -jar /data/src/hbase/hbase-server/target/surefire/surefirebooter7194368346045889527.jar /data/src/hbase/hbase-server/target/surefire/surefire9025480282422315585tmp /data/src/hbase/hbase-server/target/surefire/surefire_2815590620956840351617tmp running org.apache.hadoop.hbase.regionserver.testcompactionwithcoprocessor tests run: 18, failures: 0, errors: 0, skipped: 0, time elapsed: 584.399 sec slim down or split up. ",
        "label": 38
    },
    {
        "text": "make testadmin testenabletableroundrobinassignment friendly to concurrent tests  from https://builds.apache.org/view/g-l/view/hbase/job/hbase-trunk/2410/artifact/trunk/target/surefire-reports/org.apache.hadoop.hbase.client.testadmin.txt : testenabletableroundrobinassignment(org.apache.hadoop.hbase.client.testadmin)  time elapsed: 4.345 sec  <<< error! java.lang.illegalargumentexception: check the value configured in 'zookeeper.znode.parent'. there could be a mismatch with the one configured in the master. at org.apache.hadoop.hbase.zookeeper.rootregiontracker.waitrootregionlocation(rootregiontracker.java:81) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:753) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:733) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregioninmeta(hconnectionmanager.java:866) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:765) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:733) at org.apache.hadoop.hbase.client.htable.<init>(htable.java:202) at org.apache.hadoop.hbase.client.htable.<init>(htable.java:157) at org.apache.hadoop.hbase.client.testadmin.testenabletableroundrobinassignment(testadmin.java:604) this was due to:     htable metatable = new htable(hconstants.meta_table_name); a few lines above, we have the correct usage:     htable ht = new htable(test_util.getconfiguration(), tablename); ",
        "label": 240
    },
    {
        "text": "piped commands to hbase shell should return non zero if shell command failed   see hbase-11655. we would like the shell to return non-zero when a command that has been piped to it fails in \"scripting mode\". this could be due to invalid commands or commands issued with invalid arguments. this would lower the barrier to entry for hbase admins so they could effectively script some operations. ",
        "label": 402
    },
    {
        "text": "integrationtestbiglinkedlistwithchaosmonkey uses the wrong table name  integrationtestbiglinkedlistwithchaosmonkey creates a table named integrationtestbiglinkedlistwithchaosmonkey but when inserting data it doesn't insert any data into it. ",
        "label": 233
    },
    {
        "text": "unify code for major minor compactions  today minor compactions do not process deletes, purge old versions, etc. only major compactions do. the rationale was probably to save cpu . we should evaluate if major compaction logic indeed runs significantly slower. unifying minor compactions to do the same thing as major compactions has other advantages: if the same keys are deleted/updated repeatedly, the fact that deletes/overwrites are not processed during minor compaction makes each subsequent minor compaction more expensive as the total amount of data keeps growing. we'll have fewer bugs if the logic is as symmetric as possible. any bugs in ttl enforcement, version enforcement, etc. could cause behavior to be different after a major compaction. keeping the same logic means these bugs will get caught earlier. - note: there will still need to be one difference in the two schemes, and that has to do with delete markers. any compaction which doesn't compact all files will still need to leave delete markers. ",
        "label": 34
    },
    {
        "text": "guava drops mapevictionlistener and hadoop alpha requires it  hadoop 2.0.0-alpha depends on guava 11.0.2. updating hbase dependencies to match produces the following compilation errors: [error] singlesizecache.java:[41,32] cannot find symbol [error] symbol  : class mapevictionlistener [error] location: package com.google.common.collect [error]  [error] singlesizecache.java:[94,4] cannot find symbol [error] symbol  : class mapevictionlistener [error] location: class org.apache.hadoop.hbase.io.hfile.slab.singlesizecache [error]  [error] singlesizecache.java:[94,69] cannot find symbol [error] symbol  : class mapevictionlistener [error] location: class org.apache.hadoop.hbase.io.hfile.slab.singlesizecache ",
        "label": 286
    },
    {
        "text": "hregioninfo changes for adding replicaid and metaeditor metareader changes for region replicas  as per parent jira, the cleanest way to add region replicas we think is to actually create one more region per replica per primary region. so for example, if a table has 10 regions with replication = 3, the table would indeed be created with 30 regions. these regions will be handled and assigned individually for am purposes.   we can add replicaid to hregioninfo to indicate the replicaid, and use this to differentiate different replicas of the same region. so, primary replica would have replicaid = 0, and the others will have replicaid > 0. these replicas will share the same regionid prefix, but differ in an appended replicaid. the primary will not contain the replicaid so that no changes would be needed for existing tables. in meta, the replica regions are kept in the same row as the primary ( so for above example, there will be 10 rows in meta). the servers for the replicas are kept in columns like \"server+replicaid\". ",
        "label": 155
    },
    {
        "text": "document acl matrix in the book  we have an excellent matrix at https://issues.apache.org/jira/secure/attachment/12531252/security-acl%20matrix.pdf for acl. once the changes are done, we can adapt that and put it in the book, also add some more documentation about the new authorization features. ",
        "label": 330
    },
    {
        "text": "shell create table script cannot handle split key which is expressed in raw bytes  ",
        "label": 450
    },
    {
        "text": "add a ut to address the hfileblock heapsize  in testheapsize  in hbase-22005, i added a bytebuffallocator reference in hfileblock, but no increase relative heapsize in hfileblock#heapsize(). so i guess we have no ut for the hfileblock#heapsize(). will add a ut for this, also fix the heapsize change issue. other classes also need an ut:  1. hfileblockindex;  2. hfilecontext ",
        "label": 514
    },
    {
        "text": "upgrading lucene to lucene  hbase has an utility to export columns as lucene indices. (o.a.h.hbase.mapreduce.buildtableindex ) . this patch increases the version in libraries.properties and addresses some deprecations towards moving it. rationale for upgradation:  ==================== a lot has been happening in the lucene since 2.2, with improved performance and focus on nrt (near real time search) happening recently. hence - we need to keep up with the same and make the utility publish indices for the new version. caveats:  =======  index created by lucene 3.0 is not backward-compatible with lucene 2.2 code. in other words - as part of this upgradation - indices need to be created all over again and the library interacting with the index ( readers / searchers ) need to be upgraded to the new version as well. ",
        "label": 266
    },
    {
        "text": "historian deadlocks if regionserver is at global memory boundary and is hosting  meta   the global memory unit test was deadlocking because historian was trying to update .meta. with flush info \u2013 only the single regionserver was the one hosting the .meta. and the regionserver global lock was in place while memory is full ",
        "label": 314
    },
    {
        "text": "npe in master after upgrading to  i have upgraded my environment from 0.90.4 to 0.92.0 after the table migration i get the following error in the master (permanent) 2012-01-25 18:23:48,648 fatal master-namenode,60000,1327512209588 org.apache.hadoop.hbase.master.hmaster - unhandled exception. starting shutdown. java.lang.nullpointerexception         at org.apache.hadoop.hbase.master.assignmentmanager.rebuilduserregions(assignmentmanager.java:2190)         at org.apache.hadoop.hbase.master.assignmentmanager.joincluster(assignmentmanager.java:323)         at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:501)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:326)         at java.lang.thread.run(thread.java:662) 2012-01-25 18:23:48,650 info namenode,60000,1327512209588 org.apache.hadoop.hbase.master.hmaster - aborting i think that's because i had a hard crash in the cluster a while ago - and the following warn since then 2012-01-25 21:20:47,121 warn namenode,60000,1327513078123-catalogjanitor org.apache.hadoop.hbase.master.catalogjanitor - regioninfo_qualifier is empty in keyvalues={emails,,xxx./info:server/1314336400471/put/vlen=38, emails,,1314189353300.xxx./info:serverstartcode/1314336400471/put/vlen=8} my patch was simple to go around the npe (as the other code around the lines)  but i don't know if that's correct ",
        "label": 452
    },
    {
        "text": "new client server implementation of how gets and puts are handled   creating an issue where the implementation of the new client and server will go. leaving hbase-1249 as a discussion forum and will put code and patches here. ",
        "label": 247
    },
    {
        "text": "align services interfaces in master and regionserver  hbase-18183 adds a coprocessorregionserverservice to give a view on regionserviceservices that is safe to expose to coprocessors. on the master-side, masterservices becomes an interface for exposing to coprocessors. we need to align the two. for background, see https://issues.apache.org/jira/browse/hbase-12260?focusedcommentid=16203820&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16203820 ",
        "label": 314
    },
    {
        "text": "storefilescanner seek swallows ioes  if a storefilescanner fails to seek, it silently swallows the ioe and returns false as if it were the end of the scanner. this means that we can silently lose data if an ioe occurs for one of the store files during a compaction. ",
        "label": 453
    },
    {
        "text": " testing  speed up the test suite  the test suite takes a long time to run, and a lot of the time spent running is really wasted on repeated startup and shutdown, waiting for asynchronous events to occur, and production-tuned timeouts to occur. additionally, we use a minidfs instead of the local filesystem. we should: make whatever changes are needed to the local dfs so that it can run on windows and use that as the basis of all of our tests minimize redoing expensive setup where possible by combining tests into groups or suites that can share common setup create a way of running all the parts (master, regionserver, client) in a single thread and explicitly advancing through necessary states so that we can reliably and quickly get what we need tested accomplished use smaller test datasets where it would make a difference (testtableindex and testtablemapreduce, i'm looking at you!) a faster test suite means faster turnaround on new patches, faster hudson, and a shorter patch queue. not to mention less annoyance on the part of the developers. ",
        "label": 314
    },
    {
        "text": "remove slop for stochastic load balancer  the new load balancer already has the idea of some slop built in. we shouldn't have two layers of it. ",
        "label": 154
    },
    {
        "text": "should treat null consistency as consistency strong in connectionutils timelineconsistentread  in our pe we may set the consistency to null and in connectionutils.timelineconsistentread, we will treat it as timeline. ",
        "label": 149
    },
    {
        "text": "fold special cased mob apis into existing apis  there are a number of apis that came in with mob that are not new actions for hbase, simply new actions for a mob implementation: compactmob compactmobs majorcompactmob majorcompactmobs getmobcompactionstate and in hbaseadmin: validatemobcolumnfamily remove these special cases from the admin api where possible by folding them into existing apis. we definitely don't need one method for a singleton and another for collections. ideally we will not have any apis named *mob when finished, whether mobs are in use on a table or not should be largely an internal detail. exposing as schema option would be fine, this conforms to existing practice for other features. marking critical because i think removing the *mob special cased apis should be a precondition for release of this feature either in 2.0 or as a backport. ",
        "label": 198
    },
    {
        "text": "testdistributedlogsplitting creates a minicluster rooted at  hbase  testdistributedlogsplitting creates a minicluster rooted at ~/hbase. i have a machine configured such that my $home is nfs mounted, and this combination makes for a rather flakey test. test cluster(s) instantiated by this test should instead root out of target (like all the rest). ",
        "label": 233
    },
    {
        "text": "decouple region merging from zookeeper  region merge should be decoupled from zk. ",
        "label": 407
    },
    {
        "text": "metaservershutdownhandler stucks if  meta  assignment fails in previous attempt  while running log replay on a one node setup, i killed meta regionserver. the metassh tries to assign the meta table, but it failed as there was no other regionservers to assign to. but the meta server znode was already updated to null. when the assignment fails, the metassh is retried. but from the next iteration, it will not try to assign the meta region, but keeps on waiting for meta region. this keeps on going even after regionserver is brought up again. ",
        "label": 199
    },
    {
        "text": "remove testreplicationkillrs  tests temporarily  removing this suite of tests for now. jean-daniel cryans is working on fixing these over in hbase-8615 but currently he is off in \"exotic location\". removing meantime to get in some clean builds. will put back in hbase-9061 which is critical for 0.95.2 ",
        "label": 314
    },
    {
        "text": "disable error prone for hbase protocol shaded  this is all generated code that we shouldn't be running extra analysis on because it adds a lot of noise to the build, and also takes a very long time (15 minutes on my machine). let's make it fast and simple. even when we run with error-prone enabled for the rest of the build, it should not apply here. ",
        "label": 320
    },
    {
        "text": "caching set on scan object gets lost when using tablemapreduceutil in   0.94 and before, if one sets caching on the scan object in the job by calling scan.setcaching(int) and passes it to tablemapreduceutil, it is correctly read and used by the mappers during a mapreduce job. this is because scan.write respects and serializes caching, which is used internally by tablemapreduceutil to serialize and transfer the scan object to the mappers. 0.95+, after the move to protobuf, protobufutil.toscan does not respect caching anymore as clientprotos.scan does not have the field caching. caching is passed via the scanrequest object to the server and so is not needed in the scan object. however, this breaks application code that relies on the earlier behavior. this will lead to sudden degradation in scan performance 0.96+ for users relying on the old behavior. there are 2 options here:  1. add caching to scan object, adding an extra int to the payload for the scan object which is really not needed in the general case.  2. document and preach that tablemapreduceutil.setscannercaching must be called by the client. ",
        "label": 213
    },
    {
        "text": "skip checksum is broke  are we double checksumming by default   the hfile contains checksums for decrease the iops, so when hbase read hfile , that dont't need to read the checksum from meta file of hdfs. but hlog file of hbase don't contain the checksum, so when hbase read the hlog, that must read checksum from meta file of hdfs. we could add setskipchecksum per file to hdfs or we could write checksums into wal if this skip checksum facility is enabled ",
        "label": 286
    },
    {
        "text": "assignments are not retained on a cluster start  when a cluster is fully shutdown and then started up again with hbase.master.startup.retainassign set to true, i noticed that the assignments are not retained. upon digging, it seems like hbase-10101 made a change due to which the server holding the meta previously is added to dead-servers (in hmaster.assignmeta). later on, this makes the assignmentmanager think that the master recovered from a failure as opposed to a fresh cluster start (the servermanager.deadservers list is not empty in the check within   assignmentmanager.processdeadserversandregionsintransition) ",
        "label": 242
    },
    {
        "text": "npe in hfilereaderv2 close during major compaction when hfile block cache size is set to  on a test system got this exception when hfile.block.cache.size is set to 0: java.lang.nullpointerexception  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2.close(hfilereaderv2.java:321)  at org.apache.hadoop.hbase.regionserver.storefile$reader.close(storefile.java:1065)  at org.apache.hadoop.hbase.regionserver.storefile.closereader(storefile.java:539)  at org.apache.hadoop.hbase.regionserver.storefile.deletereader(storefile.java:549)  at org.apache.hadoop.hbase.regionserver.store.completecompaction(store.java:1314)  at org.apache.hadoop.hbase.regionserver.store.compact(store.java:686)  at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1016)  at org.apache.hadoop.hbase.regionserver.compactions.compactionrequest.run(compactionrequest.java:178)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619) minor issue as nobody in their right mind with have hfile.block.cache.size=0 looks like this is due to hbase-4422 ",
        "label": 286
    },
    {
        "text": "zkutil create and watch methods don't set watch in some cases  while using the zkutil methods during testing, i found that watch was not set when it should be set based on the methods and method comments:  createnodeifnotexistsandwatch  createephemeralnodeandwatch for example, in createnodeifnotexistsandwatch():  public static boolean createnodeifnotexistsandwatch(       zookeeperwatcher zkw, string znode, byte [] data)   throws keeperexception {     try {       zkw.getrecoverablezookeeper().create(znode, data, createacl(zkw, znode),           createmode.persistent);     } catch (keeperexception.nodeexistsexception nee) {       try {         zkw.getrecoverablezookeeper().exists(znode, zkw);       } catch (interruptedexception e) {         zkw.interruptedexception(e);         return false;       }       return false;     } catch (interruptedexception e) {       zkw.interruptedexception(e);       return false;     }     return true;   } the watch is only set via exists() call when the node already exists.  similarly in createephemeralnodeandwatch():   public static boolean createephemeralnodeandwatch(zookeeperwatcher zkw,       string znode, byte [] data)   throws keeperexception {     try {       zkw.getrecoverablezookeeper().create(znode, data, createacl(zkw, znode),           createmode.ephemeral);     } catch (keeperexception.nodeexistsexception nee) {       if(!watchandcheckexists(zkw, znode)) {         // it did exist but now it doesn't, try again         return createephemeralnodeandwatch(zkw, znode, data);       }       return false;     } catch (interruptedexception e) {       log.info(\"interrupted\", e);       thread.currentthread().interrupt();     }     return true;   } ",
        "label": 234
    },
    {
        "text": "updating hbase version to  due to a shading issue in hbase-shaded-testing-util, whereby jackson is omitted from the fat jar, the hbase-connectors tests fail to start mini clusters which fail with the below error. *** run aborted ***  java.lang.noclassdeffounderror: could not initialize class org.apache.hadoop.hdfs.web.webhdfsfilesystem  at org.apache.hadoop.hdfs.server.namenode.namenodehttpserver.initwebhdfs(namenodehttpserver.java:78)  at org.apache.hadoop.hdfs.server.namenode.namenodehttpserver.start(namenodehttpserver.java:166)  at org.apache.hadoop.hdfs.server.namenode.namenode.starthttpserver(namenode.java:842)  at org.apache.hadoop.hdfs.server.namenode.namenode.initialize(namenode.java:693)  at org.apache.hadoop.hdfs.server.namenode.namenode.<init>(namenode.java:906)  at org.apache.hadoop.hdfs.server.namenode.namenode.<init>(namenode.java:885)  at org.apache.hadoop.hdfs.server.namenode.namenode.createnamenode(namenode.java:1626)  at org.apache.hadoop.hdfs.minidfscluster.createnamenode(minidfscluster.java:1162)  at org.apache.hadoop.hdfs.minidfscluster.createnamenodesandsetconf(minidfscluster.java:1037)  at org.apache.hadoop.hdfs.minidfscluster.initminidfscluster(minidfscluster.java:830 this issue is fixed in hbase 2.2.2 with hbase-23007. ",
        "label": 215
    },
    {
        "text": "splitlogworker fails to let go of a task  kills the rs  i hope i didn't break spacetime continuum, i got this while testing 0.92.0: 2011-12-20 03:06:19,838 fatal org.apache.hadoop.hbase.regionserver.splitlogworker: logic error - end task /hbase/splitlog/hdfs%3a%2f%2fsv4r11s38%3a9100%2fhbase%2f.logs%2fsv4r14s38%2c62023%2c1324345935047-splitting%2fsv4r14s38%252c62023%252c1324345935047.1324349363814 done failed because task doesn't exist  org.apache.zookeeper.keeperexception$nonodeexception: keepererrorcode = nonode for /hbase/splitlog/hdfs%3a%2f%2fsv4r11s38%3a9100%2fhbase%2f.logs%2fsv4r14s38%2c62023%2c1324345935047-splitting%2fsv4r14s38%252c62023%252c1324345935047.1324349363814  at org.apache.zookeeper.keeperexception.create(keeperexception.java:111)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)  at org.apache.zookeeper.zookeeper.setdata(zookeeper.java:1228)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.setdata(recoverablezookeeper.java:372)  at org.apache.hadoop.hbase.zookeeper.zkutil.setdata(zkutil.java:654)  at org.apache.hadoop.hbase.regionserver.splitlogworker.endtask(splitlogworker.java:372)  at org.apache.hadoop.hbase.regionserver.splitlogworker.grabtask(splitlogworker.java:280)  at org.apache.hadoop.hbase.regionserver.splitlogworker.taskloop(splitlogworker.java:197)  at org.apache.hadoop.hbase.regionserver.splitlogworker.run(splitlogworker.java:165)  at java.lang.thread.run(thread.java:662) i'll post more logs in a moment. what i can see is that the master shuffled that task around a bit and one of the region servers died on this stack trace while the others were able to interrupt themselves. ",
        "label": 229
    },
    {
        "text": "avoid random port usage by default jmx server  create custome jmx server  if we enable jmx mbean server for hmaster or region server through vm arguments, the process will use one random which we cannot configure.  this can be a problem if that random port is configured for some other service. this issue can be avoided by supporting a custom jmx server. the ports can be configured. if there is no ports configured, it will continue the same way as now. ",
        "label": 361
    },
    {
        "text": "get the testacidguarantee unit test to fail consistently  we know that testacidguarantee is broken in the current trunk. however, the unit-test passes more often than not. in order to test out the solution we need to get it to fail consistently. this patch may not be committed/turned in. but,  required to test/accept the fix to 2856. ",
        "label": 34
    },
    {
        "text": "zk settings for initlimit synclimit shouldn't have been removed from hbase default xml  the initlimit/synclimit zk settings were removed from hbase-default.xml as part of hbase-2392 (upgrade to zk 3.3). but these settings are needed. without them, if you try to start a zk quorum of more than 1 server, you'll get the following error: java.lang.illegalargumentexception: initlimit is not set at org.apache.zookeeper.server.quorum.quorumpeerconfig.parseproperties(quorumpeerconfig.java:246) at org.apache.hadoop.hbase.zookeeper.hquorumpeer.main(hquorumpeer.java:76) ",
        "label": 38
    },
    {
        "text": "need to handle assertion error while splitting log through servershutdownhandler by shutting down the master  we know that while parsing the hlog we expect the proper length from hdfs.  in walreaderfsdatainputstream               assert(reallength >= this.length); we are trying to come out if the above condition is not satisfied. but if ssh.splitlog() gets this problem then it lands in the run method of eventhandler. this kills the ssh thread and so further assignment does not happen. if root and meta are to be assigned they cannot be.  i think in this condition we abort the master by catching such exceptions.  please do suggest. ",
        "label": 544
    },
    {
        "text": "release scripts should not need to write out a copy of gpg key material  we shouldn't need to export a copy of the gpg key. either we should give the container access to the gpg-agent on the host or we should do signing steps outside of the container. i don't think the latter is feasible, since the build with -p apache-release needs access to keys to sign as a part of the build. ",
        "label": 402
    },
    {
        "text": "add to hbase book  'schema' chapter   pre creating regions and key types  in the hbase book for the \"schema\" chapter. expanded the section on why monotonically increasing keys are bad, and specifically why the opentsdb key approach works, which is a common question on the hbase dist list. also, adding section on pre-creating regions (how to do it). ",
        "label": 314
    },
    {
        "text": "encode storefile path urls in the ui to handle scenarios where cf contains special characters  like   etc   \u3010test step\u3011\uff1a 1. create 'specialchar' ,'#' 2.put 'specialchar','r1','#:cq','1000' 3.flush 'specialchar' 4.put 'specialchar','r2','#:cq','1000' 5.flush 'specialchar'   once hfile is created, click the hfile link in ui. the following error is throwing. java.io.filenotfoundexception: path is not a file: /hbase/data/default/specialchar/df9d19830c562c4eeb3f8b396211d52d  at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:90)  at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:76)  at org.apache.hadoop.hdfs.server.namenode.fsdirstatandlistingop.getblocklocations(fsdirstatandlistingop.java:153)  at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1942)  at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.getblocklocations(namenoderpcserver.java:739)  at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.getblocklocations(clientnamenodeprotocolserversidetranslatorpb.java:432)  at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java)  at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:524)  at org.apache.hadoop.ipc.rpc$server.call(rpc.java:991)  at org.apache.hadoop.ipc.server$rpccall.run(server.java:878)  at org.apache.hadoop.ipc.server$rpccall.run(server.java:824)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:422)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1729)  at org.apache.hadoop.ipc.server$handler.run(server.java:2684) ",
        "label": 496
    },
    {
        "text": "splittableregionprocedure should hold the lock of its daughter regions  currently splittableregionprocedure only hold the region of parent region. but during processing of this procedure, after the daughter regions are updated to meta, other procedures can grab the lock of them, which is the situation we don't want to see.  so i think splittableregionprocedure should hold the lock of parent region and its daughter regions like mergetableregionsprocedure holding the lock of merged region. ",
        "label": 244
    },
    {
        "text": "error in quick start shell exercises  the shell exercises in the quick start (http://hbase.apache.org/book/quickstart.html) starts hbase(main):003:0> create 'test', 'cf' 0 row(s) in 1.2200 seconds hbase(main):003:0> list 'table' test 1 row(s) in 0.0550 seconds it looks like the second command is wrong. running it, the actual output is hbase(main):001:0> create 'test', 'cf' 0 row(s) in 0.3630 seconds hbase(main):002:0> list 'table' table                                                                                                                                                                                                                  0 row(s) in 0.0100 seconds the argument to list should be 'test', not 'table', and the output in the example is missing the table line. ",
        "label": 314
    },
    {
        "text": "add check for licenses before rolling an rc  add to how to release doc  and check for inlining a tool that does this for us  ",
        "label": 314
    },
    {
        "text": "sleeper sleep does not go back to sleep when interrupted and no stop flag given   when interrupted, the sleeper.sleep method should exit if the stop flag was given, otherwise it should continue sleeping. currently it seems to exits regardless, which means the stop flag is meaningless. here is the relevant code, from src/java/org/apache/hadoop/hbase/util/sleeper.java:   public void sleep(final long starttime) {     if (this.stop.get()) {       return;     }     long now = system.currenttimemillis();     long waittime = this.period - (now - starttime);     if (waittime > this.period) {       log.warn(\"calculated wait time > \" + this.period +         \"; setting to this.period: \" + system.currenttimemillis() + \", \" +         starttime);     }     if (waittime > 0) {       try {         thread.sleep(waittime);         long slept = system.currenttimemillis() - now;         if (slept > (10 * this.period)) {           log.warn(\"we slept \" + slept + \"ms, ten times longer than scheduled: \" +             this.period);         }       } catch(interruptedexception iex) {         // we we interrupted because we're meant to stop?  if not, just         // continue ignoring the interruption         if (this.stop.get()) {           return;         }       }     }   } essentially, the 'if (waittime > 0)' portion needs to change to a while loop so that the sleeping will continue after an interruption occurs. i'll attach a patch when i get around to fixing it. ",
        "label": 314
    },
    {
        "text": "cached hregioninterface connections crash when getting unknownhost exceptions  this isssue is unlikely to come up in a cluster test case. however, for development, the following thing happens: 1. start the hbase cluster locally, on network a (dns a, etc)  2. the region locations are cached using the hostname (mycomputer.company.com, 211.x.y.z - real ip)  3. change network location (go home)  4. start the hbase cluster locally. my hostname / ips are not different (mycomputer, 192.168.0.130 - new ip) if the region locations have been cached using the hostname, there is an unknownhostexception in catalogtracker.getcachedconnection(servername sn), uncaught in the catch statements. the server will crash constantly. the error should be caught and not rethrown, so that the cached connection expires normally. ",
        "label": 37
    },
    {
        "text": "intermittently testzookeeper testregionassignmentaftermasterrecoveryduetozkexpiry hangs in admin createtable  call  testzookeeper#testregionassignmentaftermasterrecoveryduetozkexpiry sometimes hung.  here were two recent occurrences:  https://builds.apache.org/job/precommit-hbase-build/7676/console  https://builds.apache.org/job/precommit-hbase-build/7671/console there were 9 occurrences of the following in both stack traces: \"fiforpcscheduler.handler1-thread-5\" daemon prio=10 tid=0x09df8800 nid=0xc17 waiting for monitor entry [0x6fdf8000]    java.lang.thread.state: blocked (on object monitor)   at org.apache.hadoop.hbase.master.tablenamespacemanager.istableavailableandinitialized(tablenamespacemanager.java:250)   - waiting to lock <0x7f69b5f0> (a org.apache.hadoop.hbase.master.tablenamespacemanager)   at org.apache.hadoop.hbase.master.hmaster.istablenamespacemanagerready(hmaster.java:3146)   at org.apache.hadoop.hbase.master.hmaster.getnamespacedescriptor(hmaster.java:3105)   at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1743)   at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1782)   at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$2.callblockingmethod(masterprotos.java:38221)   at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:1983)   at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:92) the test hung here: \"pool-1-thread-1\" prio=10 tid=0x74f7b800 nid=0x5aa5 in object.wait() [0x74efe000]    java.lang.thread.state: timed_waiting (on object monitor)   at java.lang.object.wait(native method)   - waiting on <0xcc848348> (a org.apache.hadoop.hbase.ipc.rpcclient$call)   at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1436)   - locked <0xcc848348> (a org.apache.hadoop.hbase.ipc.rpcclient$call)   at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1654)   at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1712)   at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$blockingstub.createtable(masterprotos.java:40372)   at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$5.createtable(hconnectionmanager.java:1931)   at org.apache.hadoop.hbase.client.hbaseadmin$2.call(hbaseadmin.java:598)   at org.apache.hadoop.hbase.client.hbaseadmin$2.call(hbaseadmin.java:594)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:116)   - locked <0x7faa26d0> (a org.apache.hadoop.hbase.client.rpcretryingcaller)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:94)   - locked <0x7faa26d0> (a org.apache.hadoop.hbase.client.rpcretryingcaller)   at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:3124)   at org.apache.hadoop.hbase.client.hbaseadmin.createtableasync(hbaseadmin.java:594)   at org.apache.hadoop.hbase.client.hbaseadmin.createtable(hbaseadmin.java:485)   at org.apache.hadoop.hbase.testzookeeper.testregionassignmentaftermasterrecoveryduetozkexpiry(testzookeeper.java:486) ",
        "label": 441
    },
    {
        "text": "do not run errorprone on jdk7  nightly jdk7 build fails for branch-1 and branch-1.4 when errorprone profile is used. [error] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project hbase-common: execution default-compile of goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile failed: an api incompatibility was encountered while executing org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile: java.lang.unsupportedclassversionerror: javax/tools/diagnosticlistener : unsupported major.minor version 52.0 https://builds.apache.org/job/hbase%20nightly/job/branch-1/929/ ",
        "label": 352
    },
    {
        "text": "testmetamigrationremovinghtd fails when used with hadoop x  testmetamigrationremovinghtd fails when build is done with \"-dhadoop.profile=23\" option. the reason is the changes of defaults in \"-mkdir\" cli call. in 0.23/2.x, it doesn't create parent directories by default anymore. the patch will be submitted shortly. ",
        "label": 40
    },
    {
        "text": "two versions of netty with hadoop profile  and  i don't know why, but when you do a mvn dependency:tree, everything looks fine. when you look at the generated target/cached_classpath.txt you see 2 versions of netty: netty-3.2.4.final.jar and netty-3.5.9.final.jar. this is bad and can lead to unpredictable behavior. i haven't looked at the other dependencies. ",
        "label": 340
    },
    {
        "text": "hlog shouldn't create a new hbc when rolling  hbase-2059 added this change in hlog.rollwriter: this.writer = createwriter(fs, newpath, new hbaseconfiguration(conf)); which has since become: hlog.writer nextwriter = this.createwriterinstance(fs, newpath,   hbaseconfiguration.create(conf)); it's unclear to me why it needs to do that, but it bite us today because we swapped jars under a running hbase with: 2011-05-05 12:06:12,876 fatal org.apache.hadoop.conf.configuration: error parsing conf file: java.util.zip.zipexception: invalid stored block lengths  2011-05-05 12:06:12,877 error org.apache.hadoop.hbase.regionserver.logroller: log rolling failed  java.lang.runtimeexception: java.util.zip.zipexception: invalid stored block lengths  at org.apache.hadoop.conf.configuration.loadresource(configuration.java:1352)  at org.apache.hadoop.conf.configuration.loadresources(configuration.java:1227)  at org.apache.hadoop.conf.configuration.getprops(configuration.java:1156)  at org.apache.hadoop.conf.configuration.get(configuration.java:427)  at org.apache.hadoop.hbase.hbaseconfiguration.checkdefaultsversion(hbaseconfiguration.java:63)  at org.apache.hadoop.hbase.hbaseconfiguration.addhbaseresources(hbaseconfiguration.java:89)  at org.apache.hadoop.hbase.hbaseconfiguration.create(hbaseconfiguration.java:100)  at org.apache.hadoop.hbase.hbaseconfiguration.create(hbaseconfiguration.java:110)  at org.apache.hadoop.hbase.regionserver.wal.hlog.rollwriter(hlog.java:485)  at org.apache.hadoop.hbase.regionserver.logroller.run(logroller.java:94)  caused by: java.util.zip.zipexception: invalid stored block lengths  at java.util.zip.inflaterinputstream.read(inflaterinputstream.java:147)  at java.util.zip.inflaterinputstream.read(inflaterinputstream.java:105)  at java.io.filterinputstream.read(filterinputstream.java:66)  at com.sun.org.apache.xerces.internal.impl.xmlentitymanager$rewindableinputstream.read(xmlentitymanager.java:2932)  at com.sun.org.apache.xerces.internal.impl.xmlentitymanager.setupcurrententity(xmlentitymanager.java:704)  at com.sun.org.apache.xerces.internal.impl.xmlversiondetector.determinedocversion(xmlversiondetector.java:186)  at com.sun.org.apache.xerces.internal.parsers.xml11configuration.parse(xml11configuration.java:772)  at com.sun.org.apache.xerces.internal.parsers.xml11configuration.parse(xml11configuration.java:737)  at com.sun.org.apache.xerces.internal.parsers.xmlparser.parse(xmlparser.java:119)  at com.sun.org.apache.xerces.internal.parsers.domparser.parse(domparser.java:235)  at com.sun.org.apache.xerces.internal.jaxp.documentbuilderimpl.parse(documentbuilderimpl.java:284)  at javax.xml.parsers.documentbuilder.parse(documentbuilder.java:180)  at org.apache.hadoop.conf.configuration.loadresource(configuration.java:1266)  ... 9 more ",
        "label": 229
    },
    {
        "text": "filters generate stackoverflowexception  below is from list. you're doing nothing wrong. the filters as written recurse until they find a match. if long stretches between matching rows, then you will get a stackoverflowerror. filters need to be changed. thanks for pointing this out. can you do without them for the moment until we get a chance to fix it? st.ack david alves wrote:  > hi st.ack and all  >   > the error always occurs when trying to see if there are more rows to  > process.  > yes i'm using a filter(regexprowfilter) to select only the rows (any  > row key) that match a specific value in one of the columns.  > then i obtain the scanner just test the hasnext method, close the  > scanner and return.  > am i doing something wrong?  > still stackoverflowerror is not supposed to happen right?  >  > regards  > david alves  > on thu, 2008-03-27 at 12:36 -0700, stack wrote:  >> you are using a filter? if so, tell us more about it.  >> st.ack  >>  >> david alves wrote:  >>> hi guys   >>>  >>> i 'm using hbase to keep data that is later indexed.  >>> the data is indexed in chunks so the cycle is get xxxx records index  >>> them check for more records etc...  >>> when i tryed the candidate-2 instead of the old 0.16.0 (which i  >>> switched to do to the regionservers becoming unresponsive) i got the  >>> error in the end of this email well into an indexing job.  >>> so you have any idea why? am i doing something wrong?  >>>  >>> david alves  >>>  >>> java.lang.runtimeexception: org.apache.hadoop.ipc.remoteexception:  >>> java.io.ioexception: java.lang.stackoverflowerror  >>> at java.io.datainputstream.readfully(datainputstream.java:178)  >>> at java.io.datainputstream.readlong(datainputstream.java:399)  >>> at org.apache.hadoop.dfs.dfsclient  >>> $blockreader.readchunk(dfsclient.java:735)  >>> at  >>> org.apache.hadoop.fs.fsinputchecker.readchecksumchunk(fsinputchecker.java:234)  >>> at  >>> org.apache.hadoop.fs.fsinputchecker.fill(fsinputchecker.java:176)  >>> at  >>> org.apache.hadoop.fs.fsinputchecker.read1(fsinputchecker.java:193)  >>> at  >>> org.apache.hadoop.fs.fsinputchecker.read(fsinputchecker.java:157)  >>> at org.apache.hadoop.dfs.dfsclient  >>> $blockreader.read(dfsclient.java:658)  >>> at org.apache.hadoop.dfs.dfsclient  >>> $dfsinputstream.readbuffer(dfsclient.java:1130)  >>> at org.apache.hadoop.dfs.dfsclient  >>> $dfsinputstream.read(dfsclient.java:1166)  >>> at java.io.datainputstream.readfully(datainputstream.java:178)  >>> at org.apache.hadoop.io.dataoutputbuffer  >>> $buffer.write(dataoutputbuffer.java:56)  >>> at  >>> org.apache.hadoop.io.dataoutputbuffer.write(dataoutputbuffer.java:90)  >>> at org.apache.hadoop.io.sequencefile  >>> $reader.next(sequencefile.java:1829)  >>> at org.apache.hadoop.io.sequencefile  >>> $reader.next(sequencefile.java:1729)  >>> at org.apache.hadoop.io.sequencefile  >>> $reader.next(sequencefile.java:1775)  >>> at org.apache.hadoop.io.mapfile$reader.next(mapfile.java:461)  >>> at org.apache.hadoop.hbase.hstore  >>> $storefilescanner.getnext(hstore.java:2350)  >>> at  >>> org.apache.hadoop.hbase.habstractscanner.next(habstractscanner.java:256)  >>> at org.apache.hadoop.hbase.hstore  >>> $hstorescanner.next(hstore.java:2561)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1807)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> at org.apache.hadoop.hbase.hregion  >>> $hscanner.next(hregion.java:1843)  >>> ... ",
        "label": 241
    },
    {
        "text": "log recovery doesnt reset the max sequence id  new logfiles can get tossed as 'duplicates'  during log recovery, we dont reset store.maxseqid, thus new log entries are stamped starting off from the old files. this can cause a problem if we fail and recover again, since the new mutations are deemed \"old\" and shouldnt be applied in a subsequent recovery scenario. ",
        "label": 547
    },
    {
        "text": "fix hlog compression's incompatibilities  i ran some tests to verify if wal compression should be turned on by default. for a use case where it's not very useful (values two order of magnitude bigger than the keys), the insert time wasn't different and the cpu usage 15% higher (150% cpu usage vs 130% when not compressing the wal). when values are smaller than the keys, i saw a 38% improvement for the insert run time and cpu usage was 33% higher (600% cpu usage vs 450%). i'm not sure wal compression accounts for all the additional cpu usage, it might just be that we're able to insert faster and we spend more time in the memstore per second (because our memstores are bad when they contain tens of thousands of values). those are two extremes, but it shows that for the price of some cpu we can save a lot. my machines have 2 quads with ht, so i still had a lot of idle cpus. ",
        "label": 229
    },
    {
        "text": "hbase shell user permission should list super users defined on hbase site xml  on hbase shell, user_permission command without specifying any user lists all users granted global permissions (grant 'user', 'rwxca'): hbase(main):008:0* user_permission user                                                   namespace,table,family,qualifier:permission                                                                                                                    @hbase-admin                                          hbase,hbase:acl,,: [permission: actions=read,write,exec,create,admin]                                                                                         1 row(s) in 3.1710 seconds users defined as super users on hbase-site.xml configuration file are not listed by this same command, even though a super user has the same permissions/privileges as those defined through \"grant 'user', 'rwxca' command on the shell: <property>     <name>hbase.superuser</name>     <value>hbase-user</value> </property> ",
        "label": 423
    },
    {
        "text": "update trunk and branch zk to just release   ",
        "label": 229
    },
    {
        "text": "startupbulkassigner would cause a lot of timeout on rit when assigning large numbers of regions  timeout   mins   in our produce environment  we find a lot of timeout on rit when cluster up, there are about 7w regions in the cluster( 25 regionservers ). first, we could see the following log:(see the region 33cf229845b1009aa8a3f7b0f85c9bd0)  master's log  2012-02-13 18:07:41,409 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x348f4a94723da5 async create of unassigned node for 33cf229845b1009aa8a3f7b0f85c9bd0 with offline state   2012-02-13 18:07:42,560 debug org.apache.hadoop.hbase.master.assignmentmanager$createunassignedasynccallback: rs=item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=offline, ts=1329127661409, server=r03f11025.yh.aliyun.com,60020,1329127549907   2012-02-13 18:07:42,996 debug org.apache.hadoop.hbase.master.assignmentmanager$existsunassignedasynccallback: rs=item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=offline, ts=1329127661409   2012-02-13 18:10:48,072 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=pending_open, ts=1329127662996  2012-02-13 18:10:48,072 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_open for too long, reassigning region=item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0.   2012-02-13 18:11:16,744 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_opened, server=r03f11025.yh.aliyun.com,60020,1329127549907, region=33cf229845b1009aa8a3f7b0f85c9bd0   2012-02-13 18:38:07,310 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: handling opened event for 33cf229845b1009aa8a3f7b0f85c9bd0; deleting unassigned node   2012-02-13 18:38:07,310 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x348f4a94723da5 deleting existing unassigned node for 33cf229845b1009aa8a3f7b0f85c9bd0 that is in expected state rs_zk_region_opened   2012-02-13 18:38:07,314 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x348f4a94723da5 successfully deleted unassigned node for region 33cf229845b1009aa8a3f7b0f85c9bd0 in expected state rs_zk_region_opened   2012-02-13 18:38:07,573 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: opened region item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. on r03f11025.yh.aliyun.com,60020,1329127549907   2012-02-13 18:50:54,428 debug org.apache.hadoop.hbase.master.assignmentmanager: no previous transition plan was found (or we are ignoring an existing plan) for item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. so generated a random one; hri=item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0., src=, dest=r01b05043.yh.aliyun.com,60020,1329127549041; 29 (online=29, exclude=null) available servers   2012-02-13 18:50:54,428 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. to r01b05043.yh.aliyun.com,60020,1329127549041   2012-02-13 19:31:50,514 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=pending_open, ts=1329132528086   2012-02-13 19:31:50,514 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_open for too long, reassigning region=item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. regionserver's log  2012-02-13 18:07:43,537 info org.apache.hadoop.hbase.regionserver.hregionserver: received request to open region: item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0.   2012-02-13 18:11:16,560 debug org.apache.hadoop.hbase.regionserver.handler.openregionhandler: processing open of item_20120208,\\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. through the rs's log, we could find it is larger than 3mins from receive openregion request to start processing openregion, causing timeout on rit in master for the region. let's see the code of startupbulkassigner, we could find regionplans are not added when assigning regions, therefore, when one region opened, it will not updatetimers of other regions whose destination is the same. ",
        "label": 107
    },
    {
        "text": " hbck  does not complain about tables with no end region  z   hbck does not detect or have an error condition when the last region of a table is missing (end key != ''). ",
        "label": 46
    },
    {
        "text": "filenotfoundexception trying to load hstorefile 'data'  from renaud delbru up on the list: hi, after our issues (\"replay of hlog required\", in a precious thread) with hbase, it seems that hbase has corrupted regions. we have, on the three region servers, errors stating that hbase cannot open certain regions because some map files on hdfs are missing (see the log attached). do you have any ideas how to fix this ? thanks. java.io.filenotfoundexception: file does not exist: hdfs://hadoop1.sindice.net:54310/hbase/page-repository/1105668475/field/mapfiles/5122893264992435570/data   at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:369)   at org.apache.hadoop.hbase.regionserver.hstorefile.length(hstorefile.java:464)   at org.apache.hadoop.hbase.regionserver.hstore.loadhstorefiles(hstore.java:409)   at org.apache.hadoop.hbase.regionserver.hstore.<init>(hstore.java:236)   at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:1575)   at org.apache.hadoop.hbase.regionserver.hregion.<init>(hregion.java:451)   at org.apache.hadoop.hbase.regionserver.hregionserver.instantiateregion(hregionserver.java:901)   at org.apache.hadoop.hbase.regionserver.hregionserver.openregion(hregionserver.java:876)   at org.apache.hadoop.hbase.regionserver.hregionserver$worker.run(hregionserver.java:816)   at java.lang.thread.run(thread.java:619) i'd thought we'd added handling of this kind of event post hdfs crashings but looking in code, it seems that trunk may not have been fixed up properly. ",
        "label": 314
    },
    {
        "text": "odd interaction between hregion get  hregion deleteall and compactions  if you apply the patch for hbase-483 to the 0.1 branch and comment out lines 309 and 315 of metautils.java (which force compactions of the root and meta regions respectively), testmergetool fails. why forcing compactions makes the test succeed is a mystery to me. ",
        "label": 314
    },
    {
        "text": "add missing security checks in masterrpcservices  the following rpc methods in masterrpcservices do not have acl check for admin rights. normalize setnormalizerrunning runcatalogscan enablecatalogjanitor runcleanerchore setcleanerchorerunning execmasterservice execprocedure execprocedurewithret ",
        "label": 48
    },
    {
        "text": "test scanner batching in export job feature hbase and report on improvement hbase adds  from tail of hbase-6372, jon had raised issue that test added did not actually test the feature. this issue is about adding a test of hbase-6372. we should also have numbers for the improvement that hbase-6372 brings. ",
        "label": 22
    },
    {
        "text": "the multi tablemap job don't support the security hbase cluster  hbase-3996 adds the support of multiple tables and scanners as input to the mapper in map/reduce jobs. but it don't support the security hbase cluster. eran kutner bryan baugher ps: hbase-3996 only support multiple tables from the same hbase cluster. should we support multiple tables from the different clusters? ",
        "label": 411
    },
    {
        "text": "missing sources jar for several modules when building hbase  introduced by hbase-14085. the problem is, for example, in hbase-common/pom.xml, we have pom.xml <plugin>           <groupid>org.apache.maven.plugins</groupid>           <artifactid>maven-source-plugin</artifactid>           <configuration>             <excluderesources>true</excluderesources>             <includes>               <include>src/main/java</include>               <include>${project.build.outputdirectory}/meta-inf</include>             </includes>           </configuration>         </plugin> but in fact, the path inside <include> tag is relative to source directories, not the project directory. so the maven-source-plugin always end with no sources in project. archive not created. ",
        "label": 149
    },
    {
        "text": "hbase hadoop2 compat module ignores hadoop profile  in hbase-hadoop2-compat/pom.xml :     <dependency>       <groupid>org.apache.hadoop</groupid>       <artifactid>hadoop-common</artifactid>       <version>${hadoop-two.version}</version>     </dependency> the version for hadoop-common defaults to hadoop-two. this means that when hadoop-3.0 profile is used, hadoop-three.version would be ignored. the following error would be observed: caused by: java.util.serviceconfigurationerror: org.apache.hadoop.hbase.zookeeper.metricszookeepersource: provider org.apache.hadoop.hbase.zookeeper.metricszookeepersourceimpl could not be instantiated   at java.util.serviceloader.fail(serviceloader.java:232)   at java.util.serviceloader.access$100(serviceloader.java:185)   at java.util.serviceloader$lazyiterator.nextservice(serviceloader.java:384)   at java.util.serviceloader$lazyiterator.next(serviceloader.java:404)   at java.util.serviceloader$1.next(serviceloader.java:480)   at org.apache.hadoop.hbase.compatibilitysingletonfactory.getinstance(compatibilitysingletonfactory.java:59)   ... 34 more caused by: java.lang.noclassdeffounderror: could not initialize class com.sun.proxy.$proxy17   at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)   at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)   at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)   at java.lang.reflect.constructor.newinstance(constructor.java:423)   at java.lang.reflect.proxy.newproxyinstance(proxy.java:739)   at org.apache.commons.configuration2.builder.fluent.parameters.createparametersproxy(parameters.java:294)   at org.apache.commons.configuration2.builder.fluent.parameters.filebased(parameters.java:185)   at org.apache.commons.configuration2.builder.fluent.configurations.fileparams(configurations.java:602)   at org.apache.commons.configuration2.builder.fluent.configurations.fileparams(configurations.java:638)   at org.apache.commons.configuration2.builder.fluent.configurations.filebasedbuilder(configurations.java:164)   at org.apache.commons.configuration2.builder.fluent.configurations.propertiesbuilder(configurations.java:264)   at org.apache.hadoop.metrics2.impl.metricsconfig.loadfirst(metricsconfig.java:115)   at org.apache.hadoop.metrics2.impl.metricsconfig.create(metricsconfig.java:98)   at org.apache.hadoop.metrics2.impl.metricssystemimpl.configure(metricssystemimpl.java:478)   at org.apache.hadoop.metrics2.impl.metricssystemimpl.start(metricssystemimpl.java:188)   at org.apache.hadoop.metrics2.impl.metricssystemimpl.init(metricssystemimpl.java:163)   at org.apache.hadoop.metrics2.lib.defaultmetricssystem.init(defaultmetricssystem.java:62)   at org.apache.hadoop.metrics2.lib.defaultmetricssystem.initialize(defaultmetricssystem.java:58)   at org.apache.hadoop.hbase.metrics.basesourceimpl$defaultmetricssysteminitializer.init(basesourceimpl.java:51)   at org.apache.hadoop.hbase.metrics.basesourceimpl.<init>(basesourceimpl.java:112)   at org.apache.hadoop.hbase.zookeeper.metricszookeepersourceimpl.<init>(metricszookeepersourceimpl.java:56)   at org.apache.hadoop.hbase.zookeeper.metricszookeepersourceimpl.<init>(metricszookeepersourceimpl.java:51) ",
        "label": 48
    },
    {
        "text": "per request profiling  add ability to get hbase metrics back from an individual request. apps can use this to track fine-grained metrics (such as server-side latency, block cache access/hit count; sync latency; append latency; etc.) on a per-api call basis. also, enhance hbase shell to support this feature. [for example, if in the shell we turn profiling on, for all subsequent requests the profiling stats will be pretty printed, or optionally written to a file.] ",
        "label": 57
    },
    {
        "text": "enumerate acid properties of hbase in a well defined spec  it's not written down anywhere what the guarantees are for each operation in hbase with regard to the various acid properties. i think the developers know the answers to these questions, but we need a clear spec for people building systems on top of hbase. here are a few sample questions we should endeavor to answer: for a multicell put within a cf, is the update made durable atomically? for a put across cfs, is the update made durable atomically? can a read see a row that hasn't been sync()ed to the hlog? what isolation do scanners have? somewhere between snapshot isolation and no isolation? after a client receives a \"success\" for a write operation, is that operation guaranteed to be visible to all other clients?  etc i see this jira as having several points of discussion: evaluation of what the current state of affairs is evaluate whether we currently provide any guarantees that aren't useful to users of the system (perhaps we can drop in exchange for performance) evaluate whether we are missing any guarantees that would be useful to users of the system ",
        "label": 453
    },
    {
        "text": "move hmaster and related classes into master package  ",
        "label": 86
    },
    {
        "text": "fix potential null pointer dereference in hregionserver and tablelockchecker  from https://builds.apache.org/job/precommit-hbase-build/5852//artifact/trunk/patchprocess/newpatchfindbugswarningshbase-server.html : <td>possible null pointer dereference of rpcc in org.apache.hadoop.hbase.regionserver.hregionserver.multi(rpccontroller, clientprotos$multirequest)</td> </tr> <tr class=\"detailrow0\"> <td/> <td> <p id=\"n69637\" style=\"display: none;\"> <a href=\"#np_null_on_some_path\">bug type np_null_on_some_path (click for details)</a> <br/>in class org.apache.hadoop.hbase.regionserver.hregionserver<br/>in method org.apache.hadoop.hbase.regionserver.hregionserver.multi(rpccontroller, clientprotos$multirequest)<br/>value loaded from rpcc<br/>dereferenced at hregionserver.java:[line 3176]<br/>known null at hregionserver.java:[line 3174]</p> ... <td>possible null pointer dereference of rpcc in org.apache.hadoop.hbase.regionserver.hregionserver.mutate(rpccontroller, clientprotos$mutaterequest)</td> </tr> <tr class=\"detailrow1\"> <td/> <td> <p id=\"n69712\" style=\"display: none;\"> <a href=\"#np_null_on_some_path\">bug type np_null_on_some_path (click for details)</a> <br/>in class org.apache.hadoop.hbase.regionserver.hregionserver<br/>in method org.apache.hadoop.hbase.regionserver.hregionserver.mutate(rpccontroller, clientprotos$mutaterequest)<br/>value loaded from rpcc<br/>dereferenced at hregionserver.java:[line 2832]<br/>known null at hregionserver.java:[line 2830]</p> ... <td>possible null pointer dereference of rpcc in org.apache.hadoop.hbase.regionserver.hregionserver.replay(rpccontroller, clientprotos$multirequest)</td> </tr> <tr class=\"detailrow0\"> <td/> <td> <p id=\"n69787\" style=\"display: none;\"> <a href=\"#np_null_on_some_path\">bug type np_null_on_some_path (click for details)</a> <br/>in class org.apache.hadoop.hbase.regionserver.hregionserver<br/>in method org.apache.hadoop.hbase.regionserver.hregionserver.replay(rpccontroller, clientprotos$multirequest)<br/>value loaded from rpcc<br/>dereferenced at hregionserver.java:[line 3742]<br/>known null at hregionserver.java:[line 3740]</p> ... <td>possible null pointer dereference of data in org.apache.hadoop.hbase.util.hbck.tablelockchecker$1.handlemetadata(byte[])</td> </tr> <tr class=\"detailrow1\"> <td/> <td> <p id=\"n73108\" style=\"display: none;\"> <a href=\"#np_null_on_some_path\">bug type np_null_on_some_path (click for details)</a> <br/>in class org.apache.hadoop.hbase.util.hbck.tablelockchecker$1<br/>in method org.apache.hadoop.hbase.util.hbck.tablelockchecker$1.handlemetadata(byte[])<br/>value loaded from data<br/>dereferenced at tablelockchecker.java:[line 68]<br/>known null at tablelockchecker.java:[line 60]</p> ",
        "label": 441
    },
    {
        "text": "mr example job to count table rows  the lars' import is a little messy; he's not sure how many records were imported. running a select takes a couple of hours. he happens to have an idle mr cluster standing by. an example mr job that just did a count of records would be generally useful. could even output row keys so you'd have a list of what made it in. later, if this tool becomes popular with derivatives and similiars, we can bundle a jar of mr jobs to run against your tables that can answer common queries and that are amenable to subclassing/modification. ",
        "label": 314
    },
    {
        "text": "testimportexport has been failing against hadoop profile  see hbase-5876. i'm going to commit the v3 patches under this name since there has been two months (my bad) since the first half was committed and found to be incomplte.  \u2014 4/9/13 updated - this will take the patch from hbase-8258 to fix this specific problem. the umbrella that used to be hbase-8258 is now handled with hbase-6891. ",
        "label": 248
    },
    {
        "text": "results returned by preappend hook in a coprocessor are replaced with null from other coprocessor even on bypass  phoenix adding multiple coprocessors for a table and one of them has preappend and preincrement implementation and bypass the operations by returning the results. but the other coprocessors which doesn't have any implementation returning null and the results returned by previous coprocessor are override by null and always going with default implementation of append and increment operations. but it's not the case with old versions and works fine on bypass. ",
        "label": 314
    },
    {
        "text": "stopping a hregionserver with unflushed cache causes data loss from org apache hadoop hbase droppedsnapshotexception  1. start a hbase cluster  2. create a table t1: create 't1', {name => 'f1'} 3. put a cell in the table: put 't1', 'r1', 'f1:', 'value'  4. scan it, see it's fine  5. stop the hregionsever hosting the t1 region: hbase/bin/hbase-daemon.sh stop regionserver.  6. watch the region being reassigned from the original hregionserver  7. scan the t1 table again. it's empty now. if between step 4 and step 5 the cache is flushed (e.g. hbase cluster restart) no data is loss. however it means that if you stop a region server with dirty cache you will loose some data. hregionserver log after issuing hbase-daemon.sh stop regionserver: 2008-12-09 06:37:46,873 info org.apache.hadoop.ipc.server: ipc server handler 7 on 60020: exiting  2008-12-09 06:37:46,873 info org.apache.hadoop.ipc.server: ipc server handler 8 on 60020: exiting  2008-12-09 06:37:46,873 info org.apache.hadoop.ipc.server: ipc server handler 9 on 60020: exiting  2008-12-09 06:37:46,873 info org.apache.hadoop.ipc.server: ipc server handler 0 on 60020: exiting  2008-12-09 06:37:46,874 info org.apache.hadoop.hbase.regionserver.hregionserver: stopping infoserver  2008-12-09 06:37:46,874 info org.apache.hadoop.ipc.server: stopping ipc server responder  2008-12-09 06:37:46,874 info org.mortbay.util.threadedserver: stopping acceptor serversocket[addr=0.0.0.0/0.0.0.0,port=0,localport=60030]  2008-12-09 06:37:46,886 info org.mortbay.http.socketlistener: stopped socketlistener on 0.0.0.0:60030  2008-12-09 06:37:46,948 info org.mortbay.util.container: stopped httpcontext[/static,/static]  2008-12-09 06:37:47,007 info org.mortbay.util.container: stopped httpcontext[/logs,/logs]  2008-12-09 06:37:47,007 info org.mortbay.util.container: stopped org.mortbay.jetty.servlet.webapplicationhandler@60ded0f0  2008-12-09 06:37:47,094 info org.mortbay.util.container: stopped webapplicationcontext[/,/]  2008-12-09 06:37:47,094 info org.mortbay.util.container: stopped org.mortbay.jetty.server@6490832e  2008-12-09 06:37:47,094 debug org.apache.hadoop.hbase.regionserver.hregionserver: closing region t1,,1228833363456  2008-12-09 06:37:47,094 debug org.apache.hadoop.hbase.regionserver.hregion: compactions and cache flushes disabled for region t1,,1228833363456  2008-12-09 06:37:47,094 debug org.apache.hadoop.hbase.regionserver.hregion: scanners disabled for region t1,,1228833363456  2008-12-09 06:37:47,094 debug org.apache.hadoop.hbase.regionserver.hregion: no more active scanners for region t1,,1228833363456  2008-12-09 06:37:47,095 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region t1,,1228833363456  2008-12-09 06:37:47,095 debug org.apache.hadoop.hbase.regionserver.hregion: no more row locks outstanding on region t1,,1228833363456  2008-12-09 06:37:47,095 debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region t1,,1228833363456. current region memcache size 18.0  2008-12-09 06:37:47,095 info org.apache.hadoop.hbase.regionserver.flusher: regionserver/0:0:0:0:0:0:0:0:60020.cacheflusher exiting  2008-12-09 06:37:47,096 info org.apache.hadoop.hbase.regionserver.logroller: logroller exiting.  2008-12-09 06:37:47,096 info org.apache.hadoop.hbase.regionserver.compactsplitthread: regionserver/0:0:0:0:0:0:0:0:60020.compactor exiting  2008-12-09 06:37:47,099 error org.apache.hadoop.hbase.regionserver.hregionserver: error closing region t1,,1228833363456  org.apache.hadoop.hbase.droppedsnapshotexception: region: t1,,1228833363456  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1071)  at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:619)  at org.apache.hadoop.hbase.regionserver.hregionserver.closeallregions(hregionserver.java:951)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:459)  at java.lang.thread.run(thread.java:619)  caused by: java.io.ioexception: filesystem closed  at org.apache.hadoop.dfs.dfsclient.checkopen(dfsclient.java:196)  at org.apache.hadoop.dfs.dfsclient.getfileinfo(dfsclient.java:564)  at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:390)  at org.apache.hadoop.fs.filesystem.exists(filesystem.java:667)  at org.apache.hadoop.hbase.regionserver.hstorefile.<init>(hstorefile.java:152)  at org.apache.hadoop.hbase.regionserver.hstore.internalflushcache(hstore.java:599)  at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:577)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1058)  ... 4 more  2008-12-09 06:37:47,100 debug org.apache.hadoop.hbase.regionserver.hlog: closing log writer in hdfs://h1:54310/hbase/log_10.131.237.51_1228833326838_60020  2008-12-09 06:37:47,101 error org.apache.hadoop.hbase.regionserver.hregionserver: close and delete failed  java.io.ioexception: filesystem closed  at org.apache.hadoop.dfs.dfsclient.checkopen(dfsclient.java:196)  at org.apache.hadoop.dfs.dfsclient.access$600(dfsclient.java:59)  at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.closeinternal(dfsclient.java:2689)  at org.apache.hadoop.dfs.dfsclient$dfsoutputstream.close(dfsclient.java:2655)  at org.apache.hadoop.fs.fsdataoutputstream$positioncache.close(fsdataoutputstream.java:59)  at org.apache.hadoop.fs.fsdataoutputstream.close(fsdataoutputstream.java:79)  at org.apache.hadoop.io.sequencefile$writer.close(sequencefile.java:962)  at org.apache.hadoop.hbase.regionserver.hlog.close(hlog.java:349)  at org.apache.hadoop.hbase.regionserver.hlog.closeanddelete(hlog.java:333)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:461)  at java.lang.thread.run(thread.java:619)  2008-12-09 06:37:47,102 info org.apache.hadoop.hbase.regionserver.hregionserver: telling master that region server is shutting down at: 10.131.237.51:60020  2008-12-09 06:37:47,104 info org.apache.hadoop.hbase.regionserver.hregionserver: stopping server at: 10.131.237.51:60020  2008-12-09 06:37:47,882 info org.apache.hadoop.hbase.leases: regionserver/0:0:0:0:0:0:0:0:60020.leasechecker closing leases  2008-12-09 06:37:47,882 info org.apache.hadoop.hbase.leases: regionserver/0:0:0:0:0:0:0:0:60020.leasechecker closed leases  2008-12-09 06:37:54,919 info org.apache.hadoop.hbase.regionserver.hregionserver: worker thread exiting  2008-12-09 06:37:54,920 info org.apache.hadoop.hbase.regionserver.hregionserver: regionserver/0:0:0:0:0:0:0:0:60020 exiting  2008-12-09 06:37:54,920 info org.apache.hadoop.hbase.regionserver.hregionserver: shutdown thread complete ",
        "label": 241
    },
    {
        "text": "scanner prefetching leaks scanners   running ycsb workload-e on 0.95 basically hangs the whole cluster. ycsb opens a scanner for 100 rows. 1. ycsb calls next 2. pre-fetching starts 3. ycsb closes scanner 4. pre-fetching re-adds the scanner. so the end result is: \"scan-prefetch-2-thread-45\" daemon prio=10 tid=0x00007f7e406ec800 nid=0x40bc runnable [0x00007f75ffefd000]    java.lang.thread.state: runnable         at org.apache.hadoop.hbase.regionserver.leases$lease.equals(leases.java:272)         at java.util.priorityqueue.indexof(priorityqueue.java:342)         at java.util.priorityqueue.remove(priorityqueue.java:360)         at java.util.concurrent.delayqueue.remove(delayqueue.java:476)         at org.apache.hadoop.hbase.regionserver.leases.removelease(leases.java:232)         - locked <0x00007f774455a660> (a java.util.concurrent.delayqueue)         at org.apache.hadoop.hbase.regionserver.regionscannerholder$scanprefetcher.call(regionscannerholder.java:269)         at org.apache.hadoop.hbase.regionserver.regionscannerholder$scanprefetcher.call(regionscannerholder.java:260)         at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334)         at java.util.concurrent.futuretask.run(futuretask.java:166)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:722) \"scan-prefetch-2-thread-44\" daemon prio=10 tid=0x00007f7e4c1ba800 nid=0x40bb waiting on condition [0x00007f7605b79000]    java.lang.thread.state: timed_waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x00007f774455ad78> (a java.util.concurrent.synchronousqueue$transferstack)         at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:226)         at java.util.concurrent.synchronousqueue$transferstack.awaitfulfill(synchronousqueue.java:460)         at java.util.concurrent.synchronousqueue$transferstack.transfer(synchronousqueue.java:359)         at java.util.concurrent.synchronousqueue.poll(synchronousqueue.java:942)         at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1068)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1130)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:722) \"scan-prefetch-2-thread-43\" daemon prio=10 tid=0x00007f7e38cbc800 nid=0x40ba waiting on condition [0x00007f7609ab8000]    java.lang.thread.state: timed_waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x00007f774455ad78> (a java.util.concurrent.synchronousqueue$transferstack)         at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:226)         at java.util.concurrent.synchronousqueue$transferstack.awaitfulfill(synchronousqueue.java:460)         at java.util.concurrent.synchronousqueue$transferstack.transfer(synchronousqueue.java:359)         at java.util.concurrent.synchronousqueue.poll(synchronousqueue.java:942)         at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1068)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1130)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:722) ",
        "label": 154
    },
    {
        "text": "avoid move api to take the destination server same as the source server   in move currently we take any destination specified and if the destination is same as the source we still do unassign and assign. here we can have problems due to regionalreadyintransitionexception and thus hanging the region in rit for long time. we can avoid this scenario by not allowing the move to happen in this scenario. ",
        "label": 543
    },
    {
        "text": "fix heap size reporting in hregion  the size reported by fixed_overhead variable in hregion misses out on a boolean variable. testheapsize doesn't report it because of the alignment we do to make it multiple of 8. the equation for hregion heap usage from fixed_overhead is:   fixed_overhead = align(100 + 42*ref + 1 arr) = align(284) = 288 on a 32 bit vm  (on a 32 bit vm, 1 ref = 4bytes, 1 arr = 16 bytes) the equation formed using reflection (in classsize) is:  expected_overehead = align(101 + 42*ref + 1arr) = align(285) = 288.  so, the testheapsize doesn't fail currently. but if i add a reference (did in last patch in 8741), it starts failing because, now the equations are:  fixed_overhead = align(100 + 43*ref + 1 arr) = align(288) = 288  expected_overhead = align(101 + 43*ref + 1 arr) = align(289) = 296. ",
        "label": 199
    },
    {
        "text": "optimize reference guide to use cell acl conveniently  when i use cell acl,i found it didn't work ,and without related settings explain in hbase reference guide. after i read the code , i found we need to change two option value in hbase-site.xml.it is user unfriendly to use cell acl function.so, i think we need to explain how to enable this function in reference guide. ",
        "label": 415
    },
    {
        "text": "put up 6rc0  ",
        "label": 149
    },
    {
        "text": "filenotfoundexception trying to get index file size for metrics  this is an odd one. we open a region and load up its store files. part of loading store files is confirming presence of mapfile index files (reconstituting them even if missing). the below log is of region open and then seconds later, failing to find the index files when we go to look at them for sake of metrics: 2008-12-28 00:06:19,330 info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: content,10a1c144cf729885001e71a5ff5108dc,1230416158498 2008-12-28 00:06:19,330 debug org.apache.hadoop.hbase.regionserver.hregion: opening region content,10a1c144cf729885001e71a5ff5108dc,1230416158498/2030495720 2008-12-28 00:06:19,337 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/url/info/4139998553412763261, isreference=false, sequence id=13310628, length=54275, majorcompaction=false 2008-12-28 00:06:19,368 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/url/info/4467064002967944271, isreference=false, sequence id=9760762, length=432827, majorcompaction=false 2008-12-28 00:06:19,373 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/url/info/5563124412728188459, isreference=false, sequence id=12406630, length=22596, majorcompaction=false 2008-12-28 00:06:19,379 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/url/info/7040855870376599550, isreference=false, sequence id=12795530, length=4163, majorcompaction=false 2008-12-28 00:06:19,379 debug org.apache.hadoop.hbase.regionserver.hstore: loaded 4 file(s) in hstore 2030495720/url, max sequence id 13310628 2008-12-28 00:06:19,496 debug org.apache.hadoop.hbase.regionserver.hstore: applied 0, skipped 2078 because sequence id <= 13310628 2008-12-28 00:06:19,687 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/info/info/4076087643455354411, isreference=false, sequence id=9760762, length=1491212, majorcompaction=false 2008-12-28 00:06:19,691 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/info/info/4178444212265859440, isreference=false, sequence id=13310628, length=156148, majorcompaction=false 2008-12-28 00:06:19,697 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/info/info/7223110203696352566, isreference=false, sequence id=12406630, length=84614, majorcompaction=false 2008-12-28 00:06:19,703 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/info/info/8629305049543986640, isreference=false, sequence id=12795530, length=9293, majorcompaction=false 2008-12-28 00:06:19,704 debug org.apache.hadoop.hbase.regionserver.hstore: loaded 4 file(s) in hstore 2030495720/info, max sequence id 13310628 2008-12-28 00:06:19,773 debug org.apache.hadoop.hbase.regionserver.hstore: moving /data/hbase/content/compaction.dir/888098363/content/mapfiles/3159110292991346789 to /data/hbase/content/888098363/content/mapfiles/6086595812879433437 2008-12-28 00:06:19,787 debug org.apache.hadoop.hbase.regionserver.hstore: completed  compaction of 888098363/content store size is 172.4m 2008-12-28 00:06:19,791 debug org.apache.hadoop.hbase.regionserver.hstore: compaction size of 888098363/info: 1.4m; skipped 2 file(s), size: 1221094 2008-12-28 00:06:19,801 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 2 file(s)  into /data/hbase/content/compaction.dir/888098363/info/mapfiles/2317624608256855622 2008-12-28 00:06:19,821 debug org.apache.hadoop.hbase.regionserver.hstore: applied 0, skipped 2078 because sequence id <= 13310628 2008-12-28 00:06:19,879 debug org.apache.hadoop.hbase.regionserver.hstore: moving /data/hbase/content/compaction.dir/888098363/info/mapfiles/2317624608256855622 to /data/hbase/content/888098363/info/mapfiles/2738033219360665217 2008-12-28 00:06:19,883 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/content/info/2253403598042963153, isreference=false, sequence id=13310628, length=12491312, majorcompaction=false 2008-12-28 00:06:19,887 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/content/info/4596284995168293361, isreference=false, sequence id=9760762, length=158138153, majorcompaction=false 2008-12-28 00:06:19,896 debug org.apache.hadoop.hbase.regionserver.hstore: completed  compaction of 888098363/info store size is 1.4m 2008-12-28 00:06:19,898 debug org.apache.hadoop.hbase.regionserver.hstore: compaction size of 888098363/url: 405.9k; skipped 2 file(s), size: 339984 2008-12-28 00:06:19,904 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 2 file(s)  into /data/hbase/content/compaction.dir/888098363/url/mapfiles/1108952767537472093 2008-12-28 00:06:19,924 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/content/info/7464468628166271570, isreference=false, sequence id=12406630, length=11058492, majorcompaction=false 2008-12-28 00:06:19,930 debug org.apache.hadoop.hbase.regionserver.hstore: loaded /data/hbase/content/2030495720/content/info/7703689073557380324, isreference=false, sequence id=12795530, length=183695, majorcompaction=false 2008-12-28 00:06:19,931 debug org.apache.hadoop.hbase.regionserver.hstore: loaded 4 file(s) in hstore 2030495720/content, max sequence id 13310628 2008-12-28 00:06:19,958 debug org.apache.hadoop.hbase.regionserver.hstore: moving /data/hbase/content/compaction.dir/888098363/url/mapfiles/1108952767537472093 to /data/hbase/content/888098363/url/mapfiles/4113677425818108069 2008-12-28 00:06:19,976 debug org.apache.hadoop.hbase.regionserver.hstore: completed  compaction of 888098363/url store size is 403.6k 2008-12-28 00:06:19,978 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region content,846510f382e9a0ae5655f03cb772830d,1230384248550 in 0sec 2008-12-28 00:06:19,979 info org.apache.hadoop.hbase.regionserver.hregion: starting  compaction on region content,e2f10daf46269ad3cc25766aa3bf48c4,1230416165744 2008-12-28 00:06:19,981 debug org.apache.hadoop.hbase.regionserver.hstore: compaction size of 2100409777/content: 170.2m; skipped 1 file(s), size: 144757669 2008-12-28 00:06:19,996 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 2 file(s)  into /data/hbase/content/compaction.dir/2100409777/content/mapfiles/7166480659109830957 2008-12-28 00:06:20,033 debug org.apache.hadoop.hbase.regionserver.hstore: applied 0, skipped 2078 because sequence id <= 13310628 2008-12-28 00:06:20,127 debug org.apache.hadoop.hbase.regionserver.hregion: deleting old log file: hdfs://sjdc-atr-dc-1.atr.trendmicro.com:50000/data/hbase/content/2030495720/oldlogfile.log 2008-12-28 00:06:20,137 debug org.apache.hadoop.hbase.regionserver.hregion: next sequence id for region content,10a1c144cf729885001e71a5ff5108dc,1230416158498 is 13310629 2008-12-28 00:06:20,138 info org.apache.hadoop.hbase.regionserver.hregion: region content,10a1c144cf729885001e71a5ff5108dc,1230416158498/2030495720 available .... 2008-12-28 00:06:21,008 debug org.apache.hadoop.hbase.regionserver.hstore: started compaction of 2 file(s)  into /data/hbase/content/compaction.dir/2100409777/url/mapfiles/744229794307693860 2008-12-28 00:06:21,030 warn org.apache.hadoop.hbase.regionserver.hregionserver: error getting store file index size for 2030495720/content: java.io.filenotfoundexception: file does not exist: hdfs://sjdc-atr-dc-1.atr.trendmicro.com:50000/data/hbase/content/2030495720/content/mapfiles/7703689073557380324/index         at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:394)         at org.apache.hadoop.hbase.regionserver.hstorefile.indexlength(hstorefile.java:488)         at org.apache.hadoop.hbase.regionserver.hstore.getstorefilesindexsize(hstore.java:2174)         at org.apache.hadoop.hbase.regionserver.hregionserver.dometrics(hregionserver.java:936)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:333)         at java.lang.thread.run(thread.java:619) 2008-12-28 00:06:21,032 warn org.apache.hadoop.hbase.regionserver.hregionserver: error getting store file index size for 2030495720/info: java.io.filenotfoundexception: file does not exist: hdfs://sjdc-atr-dc-1.atr.trendmicro.com:50000/data/hbase/content/2030495720/info/mapfiles/8629305049543986640/index         at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:394)         at org.apache.hadoop.hbase.regionserver.hstorefile.indexlength(hstorefile.java:488)         at org.apache.hadoop.hbase.regionserver.hstore.getstorefilesindexsize(hstore.java:2174)         at org.apache.hadoop.hbase.regionserver.hregionserver.dometrics(hregionserver.java:936)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:333)          at java.lang.thread.run(thread.java:619) 2008-12-28 00:06:21,053 warn org.apache.hadoop.hbase.regionserver.hregionserver: error getting store file index size for 2030495720/url: java.io.filenotfoundexception: file does not exist: hdfs://sjdc-atr-dc-1.atr.trendmicro.com:50000/data/hbase/content/2030495720/url/mapfiles/7040855870376599550/index         at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:394)         at org.apache.hadoop.hbase.regionserver.hstorefile.indexlength(hstorefile.java:488)         at org.apache.hadoop.hbase.regionserver.hstore.getstorefilesindexsize(hstore.java:2174)         at org.apache.hadoop.hbase.regionserver.hregionserver.dometrics(hregionserver.java:936)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:333)         at java.lang.thread.run(thread.java:619) ... ",
        "label": 314
    },
    {
        "text": "port hbase 'testlogrolling testlogrollondatanodedeath test failure' to branch  testlogrolling#testlogrollondatanodedeath used to fail quite often. this is fixed by hbase-10142 in 0.98 and above. toward the end of hbase-10142, jonathan and enis proposed porting the fix to 0.96 and stack ack'ed. this issue ports the fix to 0.96 branch ",
        "label": 441
    },
    {
        "text": "incorrect offlinemetarepair log class name  at the beginning of offlinemetarepair.java, we can observe:  \"private static final log log = logfactory.getlog(hbasefsck.class.getname());\" it would be better change to :  \"private static final log log = logfactory.getlog(offlinemetarepair.class.getname());\" ",
        "label": 290
    },
    {
        "text": "hbase broke splitting  only split three logs no matter what n was   there's a hard-coding in hlog#splitlog that presumes we always read in batches of ten logs: index: src/java/org/apache/hadoop/hbase/regionserver/hlog.java =================================================================== --- src/java/org/apache/hadoop/hbase/regionserver/hlog.java     (revision 799653) +++ src/java/org/apache/hadoop/hbase/regionserver/hlog.java     (working copy) @@ -860,7 +860,7 @@          // stop at logfiles.length when it's the last step          int endindex = step == maxsteps - 1? logfiles.length:             step * concurrentlogreads + concurrentlogreads; -        for (int i = (step * 10); i < endindex; i++) { +        for (int i = (step * concurrentlogreads); i < endindex; i++) {            // check for possibly empty file. with appends, currently hadoop             // reports a zero length even if the file has been sync'd. revisit if            // hadoop-4751 is committed. when i changed it so we default to reading 3 files at a time rather than 10 over in hbase-1683, the hard-coding made it so we didn't read all logs. ",
        "label": 314
    },
    {
        "text": "cell dbb end to end on the read path  umbrella jira to make sure we can have blocks cached in offheap backed cache. in the entire read path, we can refer to this offheap buffer and avoid onheap copying.  the high level items i can identify as of now are  1. avoid the array() call on bb in read path.. (this is there in many classes. we can handle class by class)  2. support buffer based getter apis in cell. in read path we will create a new cell with backed by bb. will need in cellcomparator, filter (like scvf), cps etc.  3. avoid keyvalue.ensurekeyvalue() calls in read path - this make byte copy.  4. remove all cp hooks (which are already deprecated) which deal with kvs. (in read path) will add subtasks under this. ",
        "label": 46
    },
    {
        "text": "procedure v2   masterprocedurequeue fix concurrency issue on table queue deletion  stephen yuan jiang found a concurrecy issue in the procedure queue delete where we don't have an exclusive lock before deleting the table thread 1: create table is running - the queue is empty and wlock is false  thread 2: marktableasdeleted see the queue empty and wlock= false thread 1: trywrite() set wlock=true; too late thread 2: delete the queue thread 1: never able to release the lock - npe when trying to get the queue ",
        "label": 309
    },
    {
        "text": "bulk load cleanup may falsely deem file deletion successful  toward the cleanupbulkload() method:     fs.delete(new path(request.getbulktoken()), true); the return value from delete() call is ignore, potentially leading to file lying around after the cleanup. this applies to all branches.  discovered when investigating bulk load test failure. ",
        "label": 370
    },
    {
        "text": "performanceevaluation  decouple data size from client concurrency  perfeval tool provides a --rows=r for specifying the number of records to work with and requires the user provide a value of n, used as the concurrency level. from what i can tell, every concurrent process will interact with r rows. in order to perform an apples-to-apples test, the user must re-calculate the value r for every new value of n. instead, i propose accepting a --size=s for the amount of data to interact with and let perfeval divide that amongst the n clients on the user's behalf. ",
        "label": 339
    },
    {
        "text": "increment does not extend mutation but probably should  increment is the only operation in the class of mutators that does not extend mutation. it mostly duplicates what mutation provides, but not quite. the signatures for setwritetowal and getfamilymap are slightly different. this can be inconvenient because it requires special case code and therefore could be considered an api design nit. unfortunately it is not a simple change: the interface is marked stable and the internals of the family map are different from other mutation types. the latter is why i suspect this was not addressed when mutation was introduced. ",
        "label": 314
    },
    {
        "text": "master failover can split logs of live servers  the reason why testmasterfailover fails is that when it does the master failover, the new master doesn't wait long enough for all region servers to checkin so it goes ahead and split logs... which doesn't work because of the way lease timeouts work: 2010-12-21 07:30:36,977 debug [master:0;vesta.apache.org:33170] wal.hlogsplitter(256): splitting hlog 1 of 1:  hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3a38743.1292916617204, length=0 2010-12-21 07:30:36,977 debug [writerthread-1] wal.hlogsplitter$writerthread(619): writer thread thread[writerthread-1,5,main]: starting 2010-12-21 07:30:36,977 debug [writerthread-2] wal.hlogsplitter$writerthread(619): writer thread thread[writerthread-2,5,main]: starting 2010-12-21 07:30:36,977 info  [master:0;vesta.apache.org:33170] util.fsutils(625): recovering file  hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3a38743.1292916617204 2010-12-21 07:30:36,979 warn  [ipc server handler 8 on 49187] namenode.fsnamesystem(1122): dir* namesystem.startfile:  failed to create file /user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3a38743.1292916617204 for  dfsclient_hb_m_vesta.apache.org:33170_1292916630791 on client 127.0.0.1, because this file is already being created by  dfsclient_hb_rs_vesta.apache.org,38743,1292916616340_1292916617166 on 127.0.0.1 ... 2010-12-21 07:33:44,332 warn  [master:0;vesta.apache.org:33170] util.fsutils(644): waited 187354ms for lease recovery on  hdfs://localhost:49187/user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3a38743.1292916617204:  org.apache.hadoop.hdfs.protocol.alreadybeingcreatedexception: failed to create file  /user/hudson/.logs/vesta.apache.org,38743,1292916616340/vesta.apache.org%3a38743.1292916617204  for dfsclient_hb_m_vesta.apache.org:33170_1292916630791 on client 127.0.0.1, because this file is already  being created by dfsclient_hb_rs_vesta.apache.org,38743,1292916616340_1292916617166 on 127.0.0.1 i think that we should always check in zk the number of live region servers before waiting for them to check in, this way we know how many we should expect during failover. there's also a case where we still want to timeout, since rs can die during that time, but we should wait a bit longer. ",
        "label": 247
    },
    {
        "text": "hbck   add offline create fix hbase version and hbase id  one of our clients run into a problem, in which they have the hbase.root.dir, and cluster data, but their hbase.id and hbase.version files are corrupted. hmaster creates those on start, but not if there is already existing data. we can add smt like --fixidfile, and ability for hbck to do some offline repairs for the version file. ",
        "label": 557
    },
    {
        "text": "remove multirowmutationprocessormessages proto  small cleanup in pbs. see parent task for discussion. the content of this proto file belong elsewhere. ",
        "label": 117
    },
    {
        "text": "refactor testavroserver into an integration test  testavroserver is a beefy test, spins up a mini cluster, does a large series of manipulations and then spins it down. it take about 2 mins to run on a local machine, which on the high side for a 'unit' test. this is part of the implentation discussed in http://search-hadoop.com/m/l9ozbneojk1 ",
        "label": 236
    },
    {
        "text": "refactor pe to create htable the correct way  multithreaded clients that directly create htables are out of style and will be crushed under thousands of threads. our own performanceevaluation is now suffering from this too, so it needs to keep an hconnection around in order to create tables. ",
        "label": 229
    },
    {
        "text": "testdrainingserver testdrainingserverwithabort sometimes fails in trunk  testdrainingserver#testdrainingserverwithabort failed in trunk build #3348: error message test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24> stacktrace junit.framework.assertionfailederror: test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24> at junit.framework.assert.fail(assert.java:50) at junit.framework.assert.failnotequals(assert.java:287) at junit.framework.assert.assertequals(assert.java:67) at junit.framework.assert.assertequals(assert.java:134) at org.apache.hadoop.hbase.testdrainingserver.testdrainingserverwithabort(testdrainingserver.java:241) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:62) standard output shutting down the mini hdfs cluster shutting down datanode 4 shutting down datanode 3 shutting down datanode 2 shutting down datanode 1 shutting down datanode 0 standard error 2012-09-18 20:18:30,026 info  [pool-1-thread-1] hbase.resourcechecker(144): before testdrainingserver#testdrainingserverwithabort: 441 threads, 700 file descriptors 7 connections,  2012-09-18 20:18:30,044 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodecreated, state=syncconnected, path=/hbase/balancer 2012-09-18 20:18:30,044 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(1141): master:35050-0x139db080e690000 retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; pbuf\\x08\\x00 2012-09-18 20:18:30,045 debug [ipc server handler 2 on 35050] zookeeper.zkutil(1141): master:35050-0x139db080e690000 retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; pbuf\\x08\\x00 2012-09-18 20:18:30,045 info  [ipc server handler 2 on 35050] master.hmaster(1363): balanceswitch=false 2012-09-18 20:18:30,047 info  [thread-604] hbase.testdrainingserver(211): regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.] 2012-09-18 20:18:30,047 info  [thread-604] hbase.testdrainingserver(112): making hemera.apache.org,33334,1347999502311 the draining server; it has 1 online regions 2012-09-18 20:18:30,048 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/draining 2012-09-18 20:18:30,049 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/draining/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,049 info  [pool-1-thread-1-eventthread] zookeeper.drainingservertracker(83): draining rs node created, adding to list [hemera.apache.org,33334,1347999502311] 2012-09-18 20:18:30,049 info  [thread-604] hbase.testdrainingserver(220): the available servers are: [hemera.apache.org,38814,1347999502374, hemera.apache.org,58959,1347999502361, hemera.apache.org,43875,1347999502347, hemera.apache.org,51601,1347999502333] 2012-09-18 20:18:30,049 fatal [thread-604] regionserver.hregionserver(1811): aborting region server hemera.apache.org,51601,1347999502333: aborting 2012-09-18 20:18:30,050 fatal [thread-604] regionserver.hregionserver(1818): regionserver abort: loaded coprocessors are: [] 2012-09-18 20:18:30,054 info  [thread-604] regionserver.hregionserver(1821): dump of metrics: requestspersecond=0, numberofonlineregions=3, numberofstores=3, numberofstorefiles=0, storefileindexsizemb=0, rootindexsizekb=0, totalstaticindexsizekb=0, totalstaticbloomsizekb=0, memstoresizemb=0, mbinmemorywithoutwal=0, numberofputswithoutwal=0, readrequestscount=0, writerequestscount=0, compactionqueuesize=0, flushqueuesize=0, usedheapmb=167, maxheapmb=1688, blockcachesizemb=1.98, blockcachefreemb=420.25, blockcachecount=1, blockcachehitcount=41, blockcachemisscount=1, blockcacheevictedcount=0, blockcachehitratio=97%, blockcachehitcachingratio=97%, hdfsblockslocalityindex=0, slowhlogappendcount=0, fsreadlatencyhistogrammean=0, fsreadlatencyhistogramcount=0, fsreadlatencyhistogrammedian=0, fsreadlatencyhistogram75th=0, fsreadlatencyhistogram95th=0, fsreadlatencyhistogram99th=0, fsreadlatencyhistogram999th=0, fspreadlatencyhistogrammean=0, fspreadlatencyhistogramcount=0, fspreadlatencyhistogrammedian=0, fspreadlatencyhistogram75th=0, fspreadlatencyhistogram95th=0, fspreadlatencyhistogram99th=0, fspreadlatencyhistogram999th=0, fswritelatencyhistogrammean=0, fswritelatencyhistogramcount=0, fswritelatencyhistogrammedian=0, fswritelatencyhistogram75th=0, fswritelatencyhistogram95th=0, fswritelatencyhistogram99th=0, fswritelatencyhistogram999th=0 2012-09-18 20:18:30,056 error [ipc server handler 0 on 35050] master.hmaster(1193): region server &#0;&#0;hemera.apache.org,51601,1347999502333 reported a fatal error: aborting region server hemera.apache.org,51601,1347999502333: aborting 2012-09-18 20:18:30,058 info  [thread-604] regionserver.hregionserver(1737): stopped: aborting 2012-09-18 20:18:30,058 fatal [thread-604] regionserver.hregionserver(1811): aborting region server hemera.apache.org,43875,1347999502347: aborting 2012-09-18 20:18:30,058 fatal [thread-604] regionserver.hregionserver(1818): regionserver abort: loaded coprocessors are: [] 2012-09-18 20:18:30,062 info  [thread-604] regionserver.hregionserver(1821): dump of metrics: requestspersecond=0, numberofonlineregions=7, numberofstores=7, numberofstorefiles=0, storefileindexsizemb=0, rootindexsizekb=0, totalstaticindexsizekb=0, totalstaticbloomsizekb=0, memstoresizemb=0, mbinmemorywithoutwal=0, numberofputswithoutwal=0, readrequestscount=0, writerequestscount=0, compactionqueuesize=0, flushqueuesize=0, usedheapmb=167, maxheapmb=1688, blockcachesizemb=1.98, blockcachefreemb=420.25, blockcachecount=1, blockcachehitcount=41, blockcachemisscount=1, blockcacheevictedcount=0, blockcachehitratio=97%, blockcachehitcachingratio=97%, hdfsblockslocalityindex=0, slowhlogappendcount=0, fsreadlatencyhistogrammean=0, fsreadlatencyhistogramcount=0, fsreadlatencyhistogrammedian=0, fsreadlatencyhistogram75th=0, fsreadlatencyhistogram95th=0, fsreadlatencyhistogram99th=0, fsreadlatencyhistogram999th=0, fspreadlatencyhistogrammean=0, fspreadlatencyhistogramcount=0, fspreadlatencyhistogrammedian=0, fspreadlatencyhistogram75th=0, fspreadlatencyhistogram95th=0, fspreadlatencyhistogram99th=0, fspreadlatencyhistogram999th=0, fswritelatencyhistogrammean=0, fswritelatencyhistogramcount=0, fswritelatencyhistogrammedian=0, fswritelatencyhistogram75th=0, fswritelatencyhistogram95th=0, fswritelatencyhistogram99th=0, fswritelatencyhistogram999th=0 2012-09-18 20:18:30,062 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.splitlogworker(522): sending interrupt to stop the worker thread 2012-09-18 20:18:30,063 info  [splitlogworker-hemera.apache.org,51601,1347999502333] regionserver.splitlogworker(206): splitlogworker interrupted while waiting for task, exiting: java.lang.interruptedexception 2012-09-18 20:18:30,063 info  [splitlogworker-hemera.apache.org,51601,1347999502333] regionserver.splitlogworker(170): splitlogworker hemera.apache.org,51601,1347999502333 exiting 2012-09-18 20:18:30,063 error [ipc server handler 1 on 35050] master.hmaster(1193): region server &#0;&#0;hemera.apache.org,43875,1347999502347 reported a fatal error: aborting region server hemera.apache.org,43875,1347999502347: aborting 2012-09-18 20:18:30,063 info  [thread-604] regionserver.hregionserver(1737): stopped: aborting 2012-09-18 20:18:30,063 info  [regionserver:1;hemera.apache.org,51601,1347999502333.compactionchecker] hbase.chore(81): regionserver:1;hemera.apache.org,51601,1347999502333.compactionchecker exiting 2012-09-18 20:18:30,063 fatal [thread-604] regionserver.hregionserver(1811): aborting region server hemera.apache.org,58959,1347999502361: aborting 2012-09-18 20:18:30,063 debug [pool-1-thread-1.lrublockcache.evictionthread] hfile.lrublockcache(418): block cache lru eviction started; attempting to free -408721.95 kb of total=1.98 mb 2012-09-18 20:18:30,063 info  [regionserver:1;hemera.apache.org,51601,1347999502333.cacheflusher] regionserver.memstoreflusher(264): regionserver:1;hemera.apache.org,51601,1347999502333.cacheflusher exiting 2012-09-18 20:18:30,064 debug [rs_close_region-hemera.apache.org,51601,1347999502333-0] handler.closeregionhandler(124): processing close of t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118. 2012-09-18 20:18:30,063 info  [regionserver:1;hemera.apache.org,51601,1347999502333.logroller] regionserver.logroller(118): logroller exiting. 2012-09-18 20:18:30,064 debug [rs_close_region-hemera.apache.org,51601,1347999502333-0] regionserver.hregion(954): closing t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.: disabling compactions & flushes 2012-09-18 20:18:30,064 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.hregionserver(943): aborting server hemera.apache.org,51601,1347999502333 2012-09-18 20:18:30,064 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.splitlogworker(522): sending interrupt to stop the worker thread 2012-09-18 20:18:30,064 debug [rs_close_region-hemera.apache.org,51601,1347999502333-1] handler.closeregionhandler(124): processing close of t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc. 2012-09-18 20:18:30,065 info  [regionserver:2;hemera.apache.org,43875,1347999502347.cacheflusher] regionserver.memstoreflusher(264): regionserver:2;hemera.apache.org,43875,1347999502347.cacheflusher exiting 2012-09-18 20:18:30,065 info  [regionserver:2;hemera.apache.org,43875,1347999502347.logroller] regionserver.logroller(118): logroller exiting. 2012-09-18 20:18:30,065 info  [regionserver:2;hemera.apache.org,43875,1347999502347.compactionchecker] hbase.chore(81): regionserver:2;hemera.apache.org,43875,1347999502347.compactionchecker exiting 2012-09-18 20:18:30,063 fatal [thread-604] regionserver.hregionserver(1818): regionserver abort: loaded coprocessors are: [] 2012-09-18 20:18:30,065 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] handler.closeregionhandler(124): processing close of t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289. 2012-09-18 20:18:30,065 debug [rs_close_region-hemera.apache.org,51601,1347999502333-1] regionserver.hregion(954): closing t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.: disabling compactions & flushes 2012-09-18 20:18:30,067 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(954): closing t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.: disabling compactions & flushes 2012-09-18 20:18:30,065 info  [splitlogworker-hemera.apache.org,43875,1347999502347] regionserver.splitlogworker(206): splitlogworker interrupted while waiting for task, exiting: java.lang.interruptedexception 2012-09-18 20:18:30,067 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.hregionserver(943): aborting server hemera.apache.org,43875,1347999502347 2012-09-18 20:18:30,067 debug [regionserver:2;hemera.apache.org,43875,1347999502347] catalog.catalogtracker(257): stopping catalog tracker org.apache.hadoop.hbase.catalog.catalogtracker@11a59ce 2012-09-18 20:18:30,064 debug [regionserver:1;hemera.apache.org,51601,1347999502333] catalog.catalogtracker(257): stopping catalog tracker org.apache.hadoop.hbase.catalog.catalogtracker@f7bf2d 2012-09-18 20:18:30,064 debug [rs_close_region-hemera.apache.org,51601,1347999502333-0] regionserver.hregion(975): updates disabled for region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118. 2012-09-18 20:18:30,064 debug [rs_close_region-hemera.apache.org,51601,1347999502333-2] handler.closeregionhandler(124): processing close of t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d. 2012-09-18 20:18:30,067 info  [regionserver:2;hemera.apache.org,43875,1347999502347] client.hconnectionmanager$hconnectionimplementation(1523): closing zookeeper sessionid=0x139db080e690009 2012-09-18 20:18:30,067 info  [thread-604] regionserver.hregionserver(1821): dump of metrics: requestspersecond=0, numberofonlineregions=4, numberofstores=4, numberofstorefiles=0, storefileindexsizemb=0, rootindexsizekb=0, totalstaticindexsizekb=0, totalstaticbloomsizekb=0, memstoresizemb=0, mbinmemorywithoutwal=0, numberofputswithoutwal=0, readrequestscount=0, writerequestscount=0, compactionqueuesize=0, flushqueuesize=0, usedheapmb=168, maxheapmb=1688, blockcachesizemb=1.98, blockcachefreemb=420.25, blockcachecount=1, blockcachehitcount=41, blockcachemisscount=1, blockcacheevictedcount=0, blockcachehitratio=97%, blockcachehitcachingratio=97%, hdfsblockslocalityindex=0, slowhlogappendcount=0, fsreadlatencyhistogrammean=0, fsreadlatencyhistogramcount=0, fsreadlatencyhistogrammedian=0, fsreadlatencyhistogram75th=0, fsreadlatencyhistogram95th=0, fsreadlatencyhistogram99th=0, fsreadlatencyhistogram999th=0, fspreadlatencyhistogrammean=0, fspreadlatencyhistogramcount=0, fspreadlatencyhistogrammedian=0, fspreadlatencyhistogram75th=0, fspreadlatencyhistogram95th=0, fspreadlatencyhistogram99th=0, fspreadlatencyhistogram999th=0, fswritelatencyhistogrammean=0, fswritelatencyhistogramcount=0, fswritelatencyhistogrammedian=0, fswritelatencyhistogram75th=0, fswritelatencyhistogram95th=0, fswritelatencyhistogram99th=0, fswritelatencyhistogram999th=0 2012-09-18 20:18:30,067 info  [splitlogworker-hemera.apache.org,43875,1347999502347] regionserver.splitlogworker(170): splitlogworker hemera.apache.org,43875,1347999502347 exiting 2012-09-18 20:18:30,067 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(975): updates disabled for region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289. 2012-09-18 20:18:30,067 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] handler.closeregionhandler(124): processing close of t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa. 2012-09-18 20:18:30,069 error [ipc server handler 3 on 35050] master.hmaster(1193): region server &#0;&#0;hemera.apache.org,58959,1347999502361 reported a fatal error: aborting region server hemera.apache.org,58959,1347999502361: aborting 2012-09-18 20:18:30,069 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(954): closing t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.: disabling compactions & flushes 2012-09-18 20:18:30,067 debug [rs_close_region-hemera.apache.org,51601,1347999502333-1] regionserver.hregion(975): updates disabled for region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc. 2012-09-18 20:18:30,066 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] handler.closeregionhandler(124): processing close of t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961. 2012-09-18 20:18:30,070 info  [regionserver:1;hemera.apache.org,51601,1347999502333] client.hconnectionmanager$hconnectionimplementation(1523): closing zookeeper sessionid=0x139db080e690007 2012-09-18 20:18:30,070 info  [thread-604] regionserver.hregionserver(1737): stopped: aborting 2012-09-18 20:18:30,070 info  [storecloserthread-t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,070 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(975): updates disabled for region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa. 2012-09-18 20:18:30,070 info  [storecloserthread-t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,071 info  [rs_close_region-hemera.apache.org,51601,1347999502333-1] regionserver.hregion(1023): closed t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc. 2012-09-18 20:18:30,068 info  [storecloserthread-t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,071 debug [rs_close_region-hemera.apache.org,51601,1347999502333-1] handler.closeregionhandler(168): closed region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc. 2012-09-18 20:18:30,071 info  [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(1023): closed t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289. 2012-09-18 20:18:30,068 debug [rs_close_region-hemera.apache.org,51601,1347999502333-2] regionserver.hregion(954): closing t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.: disabling compactions & flushes 2012-09-18 20:18:30,071 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] handler.closeregionhandler(168): closed region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289. 2012-09-18 20:18:30,071 info  [rs_close_region-hemera.apache.org,51601,1347999502333-0] regionserver.hregion(1023): closed t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118. 2012-09-18 20:18:30,071 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.hregionserver(1091): waiting on 2 regions to close 2012-09-18 20:18:30,071 debug [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.hregionserver(1095): {73c7f7079688e986da1850e53fb74f9d=t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.} 2012-09-18 20:18:30,071 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.splitlogworker(522): sending interrupt to stop the worker thread 2012-09-18 20:18:30,070 info  [thread-604] hbase.testdrainingserver(239): regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.] 2012-09-18 20:18:30,072 info  [regionserver:3;hemera.apache.org,58959,1347999502361.cacheflusher] regionserver.memstoreflusher(264): regionserver:3;hemera.apache.org,58959,1347999502361.cacheflusher exiting 2012-09-18 20:18:30,070 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] regionserver.hregion(954): closing t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.: disabling compactions & flushes 2012-09-18 20:18:30,072 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] handler.closeregionhandler(124): processing close of t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c. 2012-09-18 20:18:30,070 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.hregionserver(1091): waiting on 8 regions to close 2012-09-18 20:18:30,072 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(954): closing t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.: disabling compactions & flushes 2012-09-18 20:18:30,072 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] handler.closeregionhandler(124): processing close of t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824. 2012-09-18 20:18:30,072 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(975): updates disabled for region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c. 2012-09-18 20:18:30,073 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] regionserver.hregion(954): closing t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.: disabling compactions & flushes 2012-09-18 20:18:30,072 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] handler.closeregionhandler(124): processing close of t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d. 2012-09-18 20:18:30,073 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/draining/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,072 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] regionserver.hregion(975): updates disabled for region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961. 2012-09-18 20:18:30,073 info  [storecloserthread-t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,072 info  [regionserver:3;hemera.apache.org,58959,1347999502361.compactionchecker] hbase.chore(81): regionserver:3;hemera.apache.org,58959,1347999502361.compactionchecker exiting 2012-09-18 20:18:30,073 info  [storecloserthread-t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,073 info  [rs_close_region-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(1023): closed t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c. 2012-09-18 20:18:30,075 info  [rs_close_region-hemera.apache.org,43875,1347999502347-1] regionserver.hregion(1023): closed t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961. 2012-09-18 20:18:30,075 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] handler.closeregionhandler(168): closed region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c. 2012-09-18 20:18:30,072 info  [regionserver:3;hemera.apache.org,58959,1347999502361.logroller] regionserver.logroller(118): logroller exiting. 2012-09-18 20:18:30,072 info  [splitlogworker-hemera.apache.org,58959,1347999502361] regionserver.splitlogworker(206): splitlogworker interrupted while waiting for task, exiting: java.lang.interruptedexception 2012-09-18 20:18:30,071 info  [storecloserthread-t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,076 info  [splitlogworker-hemera.apache.org,58959,1347999502361] regionserver.splitlogworker(170): splitlogworker hemera.apache.org,58959,1347999502361 exiting 2012-09-18 20:18:30,076 info  [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(1023): closed t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa. 2012-09-18 20:18:30,071 debug [rs_close_region-hemera.apache.org,51601,1347999502333-0] handler.closeregionhandler(168): closed region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118. 2012-09-18 20:18:30,071 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] handler.closeregionhandler(124): processing close of t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e. 2012-09-18 20:18:30,071 debug [rs_close_region-hemera.apache.org,51601,1347999502333-2] regionserver.hregion(975): updates disabled for region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d. 2012-09-18 20:18:30,079 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] handler.closeregionhandler(168): closed region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa. 2012-09-18 20:18:30,079 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] handler.closeregionhandler(124): processing close of t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2. 2012-09-18 20:18:30,078 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(954): closing t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.: disabling compactions & flushes 2012-09-18 20:18:30,076 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] handler.closeregionhandler(124): processing close of t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03. 2012-09-18 20:18:30,075 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] handler.closeregionhandler(168): closed region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961. 2012-09-18 20:18:30,079 info  [storecloserthread-t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,073 info  [pool-1-thread-1-eventthread] zookeeper.drainingservertracker(101): draining rs node deleted, removing from list [hemera.apache.org,33334,1347999502311] 2012-09-18 20:18:30,084 info  [rs_close_region-hemera.apache.org,51601,1347999502333-2] regionserver.hregion(1023): closed t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d. 2012-09-18 20:18:30,084 info  [pool-1-thread-1] hbase.resourcechecker(144): after testdrainingserver#testdrainingserverwithabort: 348 threads (was 441), 584 file descriptors (was 700). 5 connections (was 7),  2012-09-18 20:18:30,073 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] regionserver.hregion(954): closing t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.: disabling compactions & flushes 2012-09-18 20:18:30,073 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] regionserver.hregion(975): updates disabled for region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824. 2012-09-18 20:18:30,072 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.hregionserver(943): aborting server hemera.apache.org,58959,1347999502361 2012-09-18 20:18:30,086 info  [storecloserthread-t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,072 debug [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.hregionserver(1095): {c1e5810326004add2ab532ccc9e8c24e=t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e., 6b45157a30794a82fc5cbdb3589032e2=t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2., 95255bd4b3285804a2c818d1bf7f459f=t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f., cb765c4343d7b8c707c88b4d4b79a165=t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165., 78622cfb2af5eca037e2375d7566cac6=t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.} 2012-09-18 20:18:30,086 info  [rs_close_region-hemera.apache.org,58959,1347999502361-2] regionserver.hregion(1023): closed t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824. 2012-09-18 20:18:30,086 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] handler.closeregionhandler(168): closed region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824. 2012-09-18 20:18:30,086 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] handler.closeregionhandler(124): processing close of t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194. 2012-09-18 20:18:30,086 debug [regionserver:3;hemera.apache.org,58959,1347999502361] catalog.catalogtracker(257): stopping catalog tracker org.apache.hadoop.hbase.catalog.catalogtracker@1880b02 2012-09-18 20:18:30,086 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] regionserver.hregion(975): updates disabled for region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d. 2012-09-18 20:18:30,084 debug [rs_close_region-hemera.apache.org,51601,1347999502333-2] handler.closeregionhandler(168): closed region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d. 2012-09-18 20:18:30,084 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/draining 2012-09-18 20:18:30,084 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(954): closing t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.: disabling compactions & flushes 2012-09-18 20:18:30,081 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] handler.closeregionhandler(124): processing close of t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f. 2012-09-18 20:18:30,090 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] regionserver.hregion(954): closing t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.: disabling compactions & flushes 2012-09-18 20:18:30,090 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] regionserver.hregion(975): updates disabled for region t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f. 2012-09-18 20:18:30,079 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(975): updates disabled for region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e. 2012-09-18 20:18:30,079 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(954): closing t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.: disabling compactions & flushes 2012-09-18 20:18:30,090 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(975): updates disabled for region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03. 2012-09-18 20:18:30,090 info  [storecloserthread-t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,090 info  [storecloserthread-t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,089 info  [regionserver:3;hemera.apache.org,58959,1347999502361] client.hconnectionmanager$hconnectionimplementation(1523): closing zookeeper sessionid=0x139db080e690008 2012-09-18 20:18:30,091 info  [rs_close_region-hemera.apache.org,58959,1347999502361-1] regionserver.hregion(1023): closed t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d. 2012-09-18 20:18:30,091 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] handler.closeregionhandler(168): closed region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d. 2012-09-18 20:18:30,091 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] handler.closeregionhandler(124): processing close of t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da. 2012-09-18 20:18:30,089 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] regionserver.hregion(954): closing t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.: disabling compactions & flushes 2012-09-18 20:18:30,091 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] regionserver.hregion(975): updates disabled for region t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194. 2012-09-18 20:18:30,091 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] regionserver.hregion(954): closing t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.: disabling compactions & flushes 2012-09-18 20:18:30,091 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] regionserver.hregion(975): updates disabled for region t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da. 2012-09-18 20:18:30,091 info  [storecloserthread-t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,092 info  [storecloserthread-t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,092 info  [rs_close_region-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(1023): closed t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03. 2012-09-18 20:18:30,092 info  [rs_close_region-hemera.apache.org,58959,1347999502361-1] regionserver.hregion(1023): closed t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da. 2012-09-18 20:18:30,092 debug [rs_close_region-hemera.apache.org,58959,1347999502361-0] handler.closeregionhandler(168): closed region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03. 2012-09-18 20:18:30,091 info  [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(1023): closed t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e. 2012-09-18 20:18:30,092 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] handler.closeregionhandler(168): closed region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e. 2012-09-18 20:18:30,092 info  [pool-1-thread-1] hbase.hbasetestingutility(747): shutting down minicluster 2012-09-18 20:18:30,092 debug [pool-1-thread-1] util.jvmclusterutil(223): shutting down hbase cluster 2012-09-18 20:18:30,090 info  [storecloserthread-t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,090 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(975): updates disabled for region t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2. 2012-09-18 20:18:30,093 info  [rs_close_region-hemera.apache.org,43875,1347999502347-1] regionserver.hregion(1023): closed t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f. 2012-09-18 20:18:30,092 info  [pool-1-thread-1] master.hmaster(2049): cluster shutdown requested 2012-09-18 20:18:30,092 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] handler.closeregionhandler(124): processing close of t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165. 2012-09-18 20:18:30,093 info  [storecloserthread-t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,092 debug [rs_close_region-hemera.apache.org,58959,1347999502361-1] handler.closeregionhandler(168): closed region t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da. 2012-09-18 20:18:30,093 info  [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(1023): closed t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2. 2012-09-18 20:18:30,092 debug [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.compactsplitthread(253): waiting for split thread to finish... 2012-09-18 20:18:30,093 debug [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.compactsplitthread(253): waiting for large compaction thread to finish... 2012-09-18 20:18:30,091 info  [storecloserthread-t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,093 debug [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.compactsplitthread(253): waiting for small compaction thread to finish... 2012-09-18 20:18:30,094 debug [regionserver:0;hemera.apache.org,33334,1347999502311-eventthread] zookeeper.zookeeperwatcher(265): regionserver:33334-0x139db080e690002 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/shutdown 2012-09-18 20:18:30,093 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] handler.closeregionhandler(168): closed region t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2. 2012-09-18 20:18:30,093 info  [hemera.apache.org,35050,1347999502111-catalogjanitor] hbase.chore(81): hemera.apache.org,35050,1347999502111-catalogjanitor exiting 2012-09-18 20:18:30,093 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(954): closing t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.: disabling compactions & flushes 2012-09-18 20:18:30,093 info  [master:0;hemera.apache.org,35050,1347999502111] master.servermanager(398): waiting on regionserver(s) to go down hemera.apache.org,38814,1347999502374, hemera.apache.org,58959,1347999502361, hemera.apache.org,43875,1347999502347, hemera.apache.org,33334,1347999502311, hemera.apache.org,51601,1347999502333 2012-09-18 20:18:30,093 info  [hemera.apache.org,35050,1347999502111-balancerchore] hbase.chore(81): hemera.apache.org,35050,1347999502111-balancerchore exiting 2012-09-18 20:18:30,093 debug [rs_close_region-hemera.apache.org,43875,1347999502347-1] handler.closeregionhandler(168): closed region t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f. 2012-09-18 20:18:30,094 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(975): updates disabled for region t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165. 2012-09-18 20:18:30,094 debug [rs_close_root-hemera.apache.org,58959,1347999502361-0] handler.closeregionhandler(124): processing close of -root-,,0.70236052 2012-09-18 20:18:30,094 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.hregionserver(1091): waiting on 2 regions to close 2012-09-18 20:18:30,094 info  [storecloserthread-t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,094 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] handler.closeregionhandler(124): processing close of t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6. 2012-09-18 20:18:30,095 info  [rs_close_region-hemera.apache.org,43875,1347999502347-0] regionserver.hregion(1023): closed t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165. 2012-09-18 20:18:30,094 debug [regionserver:3;hemera.apache.org,58959,1347999502361-eventthread] zookeeper.zookeeperwatcher(265): regionserver:58959-0x139db080e690005 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/shutdown 2012-09-18 20:18:30,094 info  [pool-1-thread-1] regionserver.hregionserver(1737): stopped: shutdown requested 2012-09-18 20:18:30,094 info  [rs_close_region-hemera.apache.org,58959,1347999502361-2] regionserver.hregion(1023): closed t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194. 2012-09-18 20:18:30,094 debug [regionserver:1;hemera.apache.org,51601,1347999502333-eventthread] zookeeper.zookeeperwatcher(265): regionserver:51601-0x139db080e690001 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/shutdown 2012-09-18 20:18:30,093 debug [regionserver:4;hemera.apache.org,38814,1347999502374-eventthread] zookeeper.zookeeperwatcher(265): regionserver:38814-0x139db080e690003 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/shutdown 2012-09-18 20:18:30,093 debug [regionserver:2;hemera.apache.org,43875,1347999502347-eventthread] zookeeper.zookeeperwatcher(265): regionserver:43875-0x139db080e690004 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/shutdown 2012-09-18 20:18:30,093 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/shutdown 2012-09-18 20:18:30,095 debug [regionserver:3;hemera.apache.org,58959,1347999502361-eventthread] zookeeper.zkutil(237): regionserver:58959-0x139db080e690005 /hbase/shutdown does not exist. watcher is set. 2012-09-18 20:18:30,095 debug [regionserver:4;hemera.apache.org,38814,1347999502374-eventthread] zookeeper.zkutil(237): regionserver:38814-0x139db080e690003 /hbase/shutdown does not exist. watcher is set. 2012-09-18 20:18:30,095 debug [rs_close_region-hemera.apache.org,58959,1347999502361-2] handler.closeregionhandler(168): closed region t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194. 2012-09-18 20:18:30,095 debug [rs_close_region-hemera.apache.org,43875,1347999502347-0] handler.closeregionhandler(168): closed region t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165. 2012-09-18 20:18:30,096 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(237): master:35050-0x139db080e690000 /hbase/shutdown does not exist. watcher is set. 2012-09-18 20:18:30,095 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(954): closing t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.: disabling compactions & flushes 2012-09-18 20:18:30,094 debug [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.hregionserver(1095): {70236052=-root-,,0.70236052} 2012-09-18 20:18:30,094 debug [rs_close_root-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(954): closing -root-,,0.70236052: disabling compactions & flushes 2012-09-18 20:18:30,094 debug [regionserver:0;hemera.apache.org,33334,1347999502311-eventthread] zookeeper.zkutil(237): regionserver:33334-0x139db080e690002 /hbase/shutdown does not exist. watcher is set. 2012-09-18 20:18:30,096 debug [rs_close_root-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(975): updates disabled for region -root-,,0.70236052 2012-09-18 20:18:30,096 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(975): updates disabled for region t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6. 2012-09-18 20:18:30,096 debug [regionserver:2;hemera.apache.org,43875,1347999502347-eventthread] zookeeper.zkutil(237): regionserver:43875-0x139db080e690004 /hbase/shutdown does not exist. watcher is set. 2012-09-18 20:18:30,095 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.splitlogworker(522): sending interrupt to stop the worker thread 2012-09-18 20:18:30,096 info  [storecloserthread-t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,098 info  [regionserver:0;hemera.apache.org,33334,1347999502311.cacheflusher] regionserver.memstoreflusher(264): regionserver:0;hemera.apache.org,33334,1347999502311.cacheflusher exiting 2012-09-18 20:18:30,098 info  [rs_close_region-hemera.apache.org,43875,1347999502347-2] regionserver.hregion(1023): closed t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6. 2012-09-18 20:18:30,098 info  [storecloserthread--root-,,0.70236052-1] regionserver.hstore(635): closed info 2012-09-18 20:18:30,095 debug [regionserver:1;hemera.apache.org,51601,1347999502333-eventthread] zookeeper.zkutil(237): regionserver:51601-0x139db080e690001 /hbase/shutdown does not exist. watcher is set. 2012-09-18 20:18:30,098 info  [rs_close_root-hemera.apache.org,58959,1347999502361-0] regionserver.hregion(1023): closed -root-,,0.70236052 2012-09-18 20:18:30,098 debug [rs_close_region-hemera.apache.org,43875,1347999502347-2] handler.closeregionhandler(168): closed region t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6. 2012-09-18 20:18:30,098 debug [rs_close_region-hemera.apache.org,33334,1347999502311-0] handler.closeregionhandler(124): processing close of t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564. 2012-09-18 20:18:30,098 info  [regionserver:0;hemera.apache.org,33334,1347999502311.compactionchecker] hbase.chore(81): regionserver:0;hemera.apache.org,33334,1347999502311.compactionchecker exiting 2012-09-18 20:18:30,098 info  [regionserver:0;hemera.apache.org,33334,1347999502311.logroller] regionserver.logroller(118): logroller exiting. 2012-09-18 20:18:30,098 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.hregionserver(947): stopping server hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,098 info  [splitlogworker-hemera.apache.org,33334,1347999502311] regionserver.splitlogworker(206): splitlogworker interrupted while waiting for task, exiting: java.lang.interruptedexception 2012-09-18 20:18:30,098 debug [regionserver:0;hemera.apache.org,33334,1347999502311] catalog.catalogtracker(257): stopping catalog tracker org.apache.hadoop.hbase.catalog.catalogtracker@5afcb1 2012-09-18 20:18:30,098 debug [rs_close_region-hemera.apache.org,33334,1347999502311-0] regionserver.hregion(954): closing t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.: disabling compactions & flushes 2012-09-18 20:18:30,098 debug [rs_close_root-hemera.apache.org,58959,1347999502361-0] handler.closeregionhandler(168): closed region -root-,,0.70236052 2012-09-18 20:18:30,099 debug [rs_close_region-hemera.apache.org,33334,1347999502311-0] regionserver.hregion(975): updates disabled for region t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564. 2012-09-18 20:18:30,099 info  [regionserver:0;hemera.apache.org,33334,1347999502311] client.hconnectionmanager$hconnectionimplementation(1523): closing zookeeper sessionid=0x139db080e69000b 2012-09-18 20:18:30,099 info  [splitlogworker-hemera.apache.org,33334,1347999502311] regionserver.splitlogworker(170): splitlogworker hemera.apache.org,33334,1347999502311 exiting 2012-09-18 20:18:30,099 info  [storecloserthread-t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,099 info  [rs_close_region-hemera.apache.org,33334,1347999502311-0] regionserver.hregion(1023): closed t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564. 2012-09-18 20:18:30,100 debug [rs_close_region-hemera.apache.org,33334,1347999502311-0] handler.closeregionhandler(168): closed region t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564. 2012-09-18 20:18:30,100 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.hregionserver(966): stopping server hemera.apache.org,33334,1347999502311; all regions closed. 2012-09-18 20:18:30,100 info  [regionserver:0;hemera.apache.org,33334,1347999502311.logsyncer] wal.hlog$logsyncer(1245): regionserver:0;hemera.apache.org,33334,1347999502311.logsyncer exiting 2012-09-18 20:18:30,100 debug [regionserver:0;hemera.apache.org,33334,1347999502311] wal.hlog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,272 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.hregionserver(966): stopping server hemera.apache.org,51601,1347999502333; all regions closed. 2012-09-18 20:18:30,272 info  [regionserver:1;hemera.apache.org,51601,1347999502333.logsyncer] wal.hlog$logsyncer(1245): regionserver:1;hemera.apache.org,51601,1347999502333.logsyncer exiting 2012-09-18 20:18:30,272 debug [regionserver:1;hemera.apache.org,51601,1347999502333] wal.hlog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,51601,1347999502333 2012-09-18 20:18:30,284 info  [regionserver:0;hemera.apache.org,33334,1347999502311.leasechecker] regionserver.leases(123): regionserver:0;hemera.apache.org,33334,1347999502311.leasechecker closing leases 2012-09-18 20:18:30,284 info  [regionserver:0;hemera.apache.org,33334,1347999502311.leasechecker] regionserver.leases(130): regionserver:0;hemera.apache.org,33334,1347999502311.leasechecker closed leases 2012-09-18 20:18:30,285 info  [regionserver:1;hemera.apache.org,51601,1347999502333.leasechecker] regionserver.leases(123): regionserver:1;hemera.apache.org,51601,1347999502333.leasechecker closing leases 2012-09-18 20:18:30,285 info  [regionserver:2;hemera.apache.org,43875,1347999502347.leasechecker] regionserver.leases(123): regionserver:2;hemera.apache.org,43875,1347999502347.leasechecker closing leases 2012-09-18 20:18:30,285 info  [regionserver:2;hemera.apache.org,43875,1347999502347.leasechecker] regionserver.leases(130): regionserver:2;hemera.apache.org,43875,1347999502347.leasechecker closed leases 2012-09-18 20:18:30,286 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.leases(123): regionserver:1;hemera.apache.org,51601,1347999502333 closing leases 2012-09-18 20:18:30,286 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.leases(130): regionserver:1;hemera.apache.org,51601,1347999502333 closed leases 2012-09-18 20:18:30,285 info  [regionserver:1;hemera.apache.org,51601,1347999502333.leasechecker] regionserver.leases(130): regionserver:1;hemera.apache.org,51601,1347999502333.leasechecker closed leases 2012-09-18 20:18:30,286 debug [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.compactsplitthread(253): waiting for split thread to finish... 2012-09-18 20:18:30,286 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.hregionserver(966): stopping server hemera.apache.org,43875,1347999502347; all regions closed. 2012-09-18 20:18:30,286 debug [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.compactsplitthread(253): waiting for large compaction thread to finish... 2012-09-18 20:18:30,287 debug [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.compactsplitthread(253): waiting for small compaction thread to finish... 2012-09-18 20:18:30,287 info  [regionserver:2;hemera.apache.org,43875,1347999502347.logsyncer] wal.hlog$logsyncer(1245): regionserver:2;hemera.apache.org,43875,1347999502347.logsyncer exiting 2012-09-18 20:18:30,287 debug [regionserver:2;hemera.apache.org,43875,1347999502347] wal.hlog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,43875,1347999502347 2012-09-18 20:18:30,287 info  [regionserver:3;hemera.apache.org,58959,1347999502361.leasechecker] regionserver.leases(123): regionserver:3;hemera.apache.org,58959,1347999502361.leasechecker closing leases 2012-09-18 20:18:30,288 info  [regionserver:3;hemera.apache.org,58959,1347999502361.leasechecker] regionserver.leases(130): regionserver:3;hemera.apache.org,58959,1347999502361.leasechecker closed leases 2012-09-18 20:18:30,289 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/rs/hemera.apache.org,51601,1347999502333 2012-09-18 20:18:30,289 info  [pool-1-thread-1-eventthread] zookeeper.regionservertracker(94): regionserver ephemeral node deleted, processing expiration [hemera.apache.org,51601,1347999502333] 2012-09-18 20:18:30,289 info  [pool-1-thread-1-eventthread] master.servermanager(446): cluster shutdown set; hemera.apache.org,51601,1347999502333 expired; onlineservers=4 2012-09-18 20:18:30,289 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/rs 2012-09-18 20:18:30,290 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.hregionserver(995): stopping server hemera.apache.org,51601,1347999502333; zookeeper connection closed. 2012-09-18 20:18:30,290 info  [regionserver:1;hemera.apache.org,51601,1347999502333] regionserver.hregionserver(998): regionserver:1;hemera.apache.org,51601,1347999502333 exiting 2012-09-18 20:18:30,290 info  [shutdown of org.apache.hadoop.hbase.fs.hfilesystem@21e115] hbase.minihbasecluster$singlefilesystemshutdownthread(186): hook closing fs=org.apache.hadoop.hbase.fs.hfilesystem@21e115 2012-09-18 20:18:30,291 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374 2012-09-18 20:18:30,291 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,43875,1347999502347 2012-09-18 20:18:30,292 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,58959,1347999502361 2012-09-18 20:18:30,293 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,296 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.hregionserver(966): stopping server hemera.apache.org,58959,1347999502361; all regions closed. 2012-09-18 20:18:30,296 info  [regionserver:3;hemera.apache.org,58959,1347999502361.logsyncer] wal.hlog$logsyncer(1245): regionserver:3;hemera.apache.org,58959,1347999502361.logsyncer exiting 2012-09-18 20:18:30,297 debug [regionserver:3;hemera.apache.org,58959,1347999502361] wal.hlog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,58959,1347999502361 2012-09-18 20:18:30,298 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.leases(123): regionserver:2;hemera.apache.org,43875,1347999502347 closing leases 2012-09-18 20:18:30,298 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.leases(130): regionserver:2;hemera.apache.org,43875,1347999502347 closed leases 2012-09-18 20:18:30,298 debug [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.compactsplitthread(253): waiting for split thread to finish... 2012-09-18 20:18:30,298 debug [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.compactsplitthread(253): waiting for large compaction thread to finish... 2012-09-18 20:18:30,298 debug [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.compactsplitthread(253): waiting for small compaction thread to finish... 2012-09-18 20:18:30,300 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/rs/hemera.apache.org,43875,1347999502347 2012-09-18 20:18:30,300 info  [pool-1-thread-1-eventthread] zookeeper.regionservertracker(94): regionserver ephemeral node deleted, processing expiration [hemera.apache.org,43875,1347999502347] 2012-09-18 20:18:30,300 info  [pool-1-thread-1-eventthread] master.servermanager(446): cluster shutdown set; hemera.apache.org,43875,1347999502347 expired; onlineservers=3 2012-09-18 20:18:30,301 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/rs 2012-09-18 20:18:30,301 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.hregionserver(995): stopping server hemera.apache.org,43875,1347999502347; zookeeper connection closed. 2012-09-18 20:18:30,301 info  [regionserver:2;hemera.apache.org,43875,1347999502347] regionserver.hregionserver(998): regionserver:2;hemera.apache.org,43875,1347999502347 exiting 2012-09-18 20:18:30,302 info  [shutdown of org.apache.hadoop.hbase.fs.hfilesystem@147e54e] hbase.minihbasecluster$singlefilesystemshutdownthread(186): hook closing fs=org.apache.hadoop.hbase.fs.hfilesystem@147e54e 2012-09-18 20:18:30,302 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374 2012-09-18 20:18:30,303 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,58959,1347999502361 2012-09-18 20:18:30,303 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,307 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.leases(123): regionserver:3;hemera.apache.org,58959,1347999502361 closing leases 2012-09-18 20:18:30,308 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.leases(130): regionserver:3;hemera.apache.org,58959,1347999502361 closed leases 2012-09-18 20:18:30,309 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/rs/hemera.apache.org,58959,1347999502361 2012-09-18 20:18:30,309 info  [pool-1-thread-1-eventthread] zookeeper.regionservertracker(94): regionserver ephemeral node deleted, processing expiration [hemera.apache.org,58959,1347999502361] 2012-09-18 20:18:30,309 info  [pool-1-thread-1-eventthread] master.servermanager(446): cluster shutdown set; hemera.apache.org,58959,1347999502361 expired; onlineservers=2 2012-09-18 20:18:30,309 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/rs 2012-09-18 20:18:30,310 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.hregionserver(995): stopping server hemera.apache.org,58959,1347999502361; zookeeper connection closed. 2012-09-18 20:18:30,310 info  [regionserver:3;hemera.apache.org,58959,1347999502361] regionserver.hregionserver(998): regionserver:3;hemera.apache.org,58959,1347999502361 exiting 2012-09-18 20:18:30,310 info  [shutdown of org.apache.hadoop.hbase.fs.hfilesystem@1c7510d] hbase.minihbasecluster$singlefilesystemshutdownthread(186): hook closing fs=org.apache.hadoop.hbase.fs.hfilesystem@1c7510d 2012-09-18 20:18:30,310 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374 2012-09-18 20:18:30,311 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(869): closing user regions 2012-09-18 20:18:30,311 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,311 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] handler.closeregionhandler(124): processing close of t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4. 2012-09-18 20:18:30,311 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] regionserver.hregion(954): closing t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.: disabling compactions & flushes 2012-09-18 20:18:30,311 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(124): processing close of t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23. 2012-09-18 20:18:30,311 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] regionserver.hregion(975): updates disabled for region t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4. 2012-09-18 20:18:30,312 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(954): closing t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.: disabling compactions & flushes 2012-09-18 20:18:30,312 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(975): updates disabled for region t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23. 2012-09-18 20:18:30,312 info  [storecloserthread-t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,311 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] handler.closeregionhandler(124): processing close of t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203. 2012-09-18 20:18:30,312 info  [storecloserthread-t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,312 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] regionserver.hregion(954): closing t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.: disabling compactions & flushes 2012-09-18 20:18:30,313 info  [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1023): closed t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23. 2012-09-18 20:18:30,313 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] regionserver.hregion(975): updates disabled for region t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203. 2012-09-18 20:18:30,313 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(168): closed region t,vvv,1347999506708.df029f1344d578333d45fbd32dbbba23. 2012-09-18 20:18:30,312 info  [rs_close_region-hemera.apache.org,38814,1347999502374-1] regionserver.hregion(1023): closed t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4. 2012-09-18 20:18:30,313 info  [storecloserthread-t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,313 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(124): processing close of t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd. 2012-09-18 20:18:30,313 info  [rs_close_region-hemera.apache.org,38814,1347999502374-2] regionserver.hregion(1023): closed t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203. 2012-09-18 20:18:30,313 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(954): closing t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.: disabling compactions & flushes 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] handler.closeregionhandler(168): closed region t,ppp,1347999506690.815416a53df0b3b68fc865a0edf53203. 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(975): updates disabled for region t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd. 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] handler.closeregionhandler(124): processing close of t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b. 2012-09-18 20:18:30,313 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] handler.closeregionhandler(168): closed region t,bbb,1347999506644.8a6358bbd5596deaad1f278da7ca32b4. 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] regionserver.hregion(954): closing t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.: disabling compactions & flushes 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] handler.closeregionhandler(124): processing close of t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9. 2012-09-18 20:18:30,314 info  [storecloserthread-t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] regionserver.hregion(954): closing t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.: disabling compactions & flushes 2012-09-18 20:18:30,315 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] regionserver.hregion(975): updates disabled for region t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9. 2012-09-18 20:18:30,314 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] regionserver.hregion(975): updates disabled for region t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b. 2012-09-18 20:18:30,315 info  [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1023): closed t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd. 2012-09-18 20:18:30,315 info  [storecloserthread-t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,315 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(168): closed region t,jjj,1347999506672.393b3166f0a447101593a6e538c0a2fd. 2012-09-18 20:18:30,315 info  [rs_close_region-hemera.apache.org,38814,1347999502374-1] regionserver.hregion(1023): closed t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9. 2012-09-18 20:18:30,316 debug [rs_close_region-hemera.apache.org,38814,1347999502374-1] handler.closeregionhandler(168): closed region t,ccc,1347999506650.8568717cae604e1dab3b78bfeff881e9. 2012-09-18 20:18:30,315 info  [storecloserthread-t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,316 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(124): processing close of t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f. 2012-09-18 20:18:30,316 info  [rs_close_region-hemera.apache.org,38814,1347999502374-2] regionserver.hregion(1023): closed t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b. 2012-09-18 20:18:30,316 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(954): closing t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.: disabling compactions & flushes 2012-09-18 20:18:30,316 debug [rs_close_region-hemera.apache.org,38814,1347999502374-2] handler.closeregionhandler(168): closed region t,ddd,1347999506653.d94a101bf8bc56ad5a83b8b626f8189b. 2012-09-18 20:18:30,316 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(975): updates disabled for region t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f. 2012-09-18 20:18:30,317 info  [storecloserthread-t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f.-1] regionserver.hstore(635): closed f 2012-09-18 20:18:30,317 info  [rs_close_region-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1023): closed t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f. 2012-09-18 20:18:30,317 debug [rs_close_region-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(168): closed region t,uuu,1347999506706.bf4ceeff334c5a35fa3a54272c2f395f. 2012-09-18 20:18:30,392 info  [hemera.apache.org,35050,1347999502111.splitlogmanagertimeoutmonitor] hbase.chore(81): hemera.apache.org,35050,1347999502111.splitlogmanagertimeoutmonitor exiting 2012-09-18 20:18:30,507 debug [regionserver:0;hemera.apache.org,33334,1347999502311] wal.hlog(975): moved 1 log files to /user/jenkins/hbase/.oldlogs 2012-09-18 20:18:30,509 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.leases(123): regionserver:0;hemera.apache.org,33334,1347999502311 closing leases 2012-09-18 20:18:30,509 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.leases(130): regionserver:0;hemera.apache.org,33334,1347999502311 closed leases 2012-09-18 20:18:30,509 debug [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.compactsplitthread(253): waiting for split thread to finish... 2012-09-18 20:18:30,509 debug [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.compactsplitthread(253): waiting for large compaction thread to finish... 2012-09-18 20:18:30,509 debug [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.compactsplitthread(253): waiting for small compaction thread to finish... 2012-09-18 20:18:30,510 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/rs/hemera.apache.org,33334,1347999502311 2012-09-18 20:18:30,511 info  [pool-1-thread-1-eventthread] zookeeper.regionservertracker(94): regionserver ephemeral node deleted, processing expiration [hemera.apache.org,33334,1347999502311] 2012-09-18 20:18:30,511 info  [pool-1-thread-1-eventthread] master.servermanager(446): cluster shutdown set; hemera.apache.org,33334,1347999502311 expired; onlineservers=1 2012-09-18 20:18:30,511 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/rs 2012-09-18 20:18:30,512 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.hregionserver(995): stopping server hemera.apache.org,33334,1347999502311; zookeeper connection closed. 2012-09-18 20:18:30,512 info  [regionserver:0;hemera.apache.org,33334,1347999502311] regionserver.hregionserver(998): regionserver:0;hemera.apache.org,33334,1347999502311 exiting 2012-09-18 20:18:30,512 info  [shutdown of org.apache.hadoop.hbase.fs.hfilesystem@1ee9cc3] hbase.minihbasecluster$singlefilesystemshutdownthread(186): hook closing fs=org.apache.hadoop.hbase.fs.hfilesystem@1ee9cc3 2012-09-18 20:18:30,513 debug [pool-1-thread-1-eventthread] zookeeper.zkutil(235): master:35050-0x139db080e690000 set watcher on existing znode /hbase/rs/hemera.apache.org,38814,1347999502374 2012-09-18 20:18:30,513 info  [pool-1-thread-1] regionserver.hregionserver(1737): stopped: shutdown requested 2012-09-18 20:18:30,514 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.splitlogworker(522): sending interrupt to stop the worker thread 2012-09-18 20:18:30,514 info  [splitlogworker-hemera.apache.org,38814,1347999502374] regionserver.splitlogworker(206): splitlogworker interrupted while waiting for task, exiting: java.lang.interruptedexception 2012-09-18 20:18:30,514 info  [regionserver:4;hemera.apache.org,38814,1347999502374.compactionchecker] hbase.chore(81): regionserver:4;hemera.apache.org,38814,1347999502374.compactionchecker exiting 2012-09-18 20:18:30,514 info  [splitlogworker-hemera.apache.org,38814,1347999502374] regionserver.splitlogworker(170): splitlogworker hemera.apache.org,38814,1347999502374 exiting 2012-09-18 20:18:30,514 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(947): stopping server hemera.apache.org,38814,1347999502374 2012-09-18 20:18:30,514 info  [regionserver:4;hemera.apache.org,38814,1347999502374.logroller] regionserver.logroller(118): logroller exiting. 2012-09-18 20:18:30,514 info  [regionserver:4;hemera.apache.org,38814,1347999502374.cacheflusher] regionserver.memstoreflusher(264): regionserver:4;hemera.apache.org,38814,1347999502374.cacheflusher exiting 2012-09-18 20:18:30,514 debug [regionserver:4;hemera.apache.org,38814,1347999502374] catalog.catalogtracker(257): stopping catalog tracker org.apache.hadoop.hbase.catalog.catalogtracker@df39bc 2012-09-18 20:18:30,515 info  [regionserver:4;hemera.apache.org,38814,1347999502374] client.hconnectionmanager$hconnectionimplementation(1523): closing zookeeper sessionid=0x139db080e69000a 2012-09-18 20:18:30,516 debug [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.compactsplitthread(253): waiting for split thread to finish... 2012-09-18 20:18:30,516 debug [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.compactsplitthread(253): waiting for large compaction thread to finish... 2012-09-18 20:18:30,516 debug [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.compactsplitthread(253): waiting for small compaction thread to finish... 2012-09-18 20:18:30,517 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(1091): waiting on 1 regions to close 2012-09-18 20:18:30,517 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(124): processing close of .meta.,,1.1028785192 2012-09-18 20:18:30,517 debug [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(1095): {1028785192=.meta.,,1.1028785192} 2012-09-18 20:18:30,517 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(954): closing .meta.,,1.1028785192: disabling compactions & flushes 2012-09-18 20:18:30,518 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(975): updates disabled for region .meta.,,1.1028785192 2012-09-18 20:18:30,518 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1481): started memstore flush for .meta.,,1.1028785192, current region memstore size 20.1k 2012-09-18 20:18:30,518 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1528): finished snapshotting .meta.,,1.1028785192, commencing wait for mvcc, flushsize=20560 2012-09-18 20:18:30,518 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1538): finished snapshotting, commencing flushing stores 2012-09-18 20:18:30,521 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] util.fsutils(167): creating file=hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/.tmp/6c449086188648c08a990537dba22ca4 with permission=rwxrwxrwx 2012-09-18 20:18:30,523 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] hfile.hfilewriterv2(142): initialized with cacheconfig:enabled [cachedataonread=true] [cachedataonwrite=false] [cacheindexesonwrite=false] [cachebloomsonwrite=false] [cacheevictonclose=false] [cachecompressed=false] 2012-09-18 20:18:30,524 info  [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.storefile$writer(1021): delete family bloom filter type for hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/.tmp/6c449086188648c08a990537dba22ca4: compoundbloomfilterwriter 2012-09-18 20:18:30,939 info  [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.storefile$writer(1241): no general bloom and no deletefamily was added to hfile (hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/.tmp/6c449086188648c08a990537dba22ca4)  2012-09-18 20:18:30,939 info  [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hstore(767): flushed , sequenceid=59, memsize=20.1k, into tmp file hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/.tmp/6c449086188648c08a990537dba22ca4 2012-09-18 20:18:30,948 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hstore(792): renaming flushed file at hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/.tmp/6c449086188648c08a990537dba22ca4 to hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/info/6c449086188648c08a990537dba22ca4 2012-09-18 20:18:30,954 info  [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hstore(815): added hdfs://localhost:52077/user/jenkins/hbase/.meta./1028785192/info/6c449086188648c08a990537dba22ca4, entries=89, sequenceid=59, filesize=10.2k 2012-09-18 20:18:30,955 info  [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1617): finished memstore flush of ~20.1k/20560, currentsize=0.0/0 for region .meta.,,1.1028785192 in 437ms, sequenceid=59, compaction requested=false 2012-09-18 20:18:30,956 info  [storecloserthread-.meta.,,1.1028785192-1] regionserver.hstore(635): closed info 2012-09-18 20:18:30,956 info  [rs_close_meta-hemera.apache.org,38814,1347999502374-0] regionserver.hregion(1023): closed .meta.,,1.1028785192 2012-09-18 20:18:30,956 debug [rs_close_meta-hemera.apache.org,38814,1347999502374-0] handler.closeregionhandler(168): closed region .meta.,,1.1028785192 2012-09-18 20:18:31,112 info  [master:0;hemera.apache.org,35050,1347999502111] master.servermanager(398): waiting on regionserver(s) to go down hemera.apache.org,38814,1347999502374 2012-09-18 20:18:31,118 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(966): stopping server hemera.apache.org,38814,1347999502374; all regions closed. 2012-09-18 20:18:31,118 info  [regionserver:4;hemera.apache.org,38814,1347999502374.logsyncer] wal.hlog$logsyncer(1245): regionserver:4;hemera.apache.org,38814,1347999502374.logsyncer exiting 2012-09-18 20:18:31,118 debug [regionserver:4;hemera.apache.org,38814,1347999502374] wal.hlog(1007): closing hlog writer in hdfs://localhost:52077/user/jenkins/hbase/.logs/hemera.apache.org,38814,1347999502374 2012-09-18 20:18:31,124 debug [regionserver:4;hemera.apache.org,38814,1347999502374] wal.hlog(975): moved 1 log files to /user/jenkins/hbase/.oldlogs 2012-09-18 20:18:31,125 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.leases(123): regionserver:4;hemera.apache.org,38814,1347999502374 closing leases 2012-09-18 20:18:31,125 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.leases(130): regionserver:4;hemera.apache.org,38814,1347999502374 closed leases 2012-09-18 20:18:31,126 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodedeleted, state=syncconnected, path=/hbase/rs/hemera.apache.org,38814,1347999502374 2012-09-18 20:18:31,126 info  [pool-1-thread-1-eventthread] zookeeper.regionservertracker(94): regionserver ephemeral node deleted, processing expiration [hemera.apache.org,38814,1347999502374] 2012-09-18 20:18:31,126 info  [pool-1-thread-1-eventthread] master.servermanager(446): cluster shutdown set; hemera.apache.org,38814,1347999502374 expired; onlineservers=0 2012-09-18 20:18:31,126 debug [master:0;hemera.apache.org,35050,1347999502111] master.hmaster(1067): stopping service threads 2012-09-18 20:18:31,126 info  [pool-1-thread-1-eventthread] master.hmaster(2049): cluster shutdown set; onlineserver=0 2012-09-18 20:18:31,127 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(995): stopping server hemera.apache.org,38814,1347999502374; zookeeper connection closed. 2012-09-18 20:18:31,127 info  [regionserver:4;hemera.apache.org,38814,1347999502374] regionserver.hregionserver(998): regionserver:4;hemera.apache.org,38814,1347999502374 exiting 2012-09-18 20:18:31,127 info  [master:0;hemera.apache.org,35050,1347999502111.oldlogcleaner] hbase.chore(81): master:0;hemera.apache.org,35050,1347999502111.oldlogcleaner exiting 2012-09-18 20:18:31,127 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): master:35050-0x139db080e690000 received zookeeper event, type=nodechildrenchanged, state=syncconnected, path=/hbase/rs 2012-09-18 20:18:31,127 info  [master:0;hemera.apache.org,35050,1347999502111.archivedhfilecleaner] hbase.chore(81): master:0;hemera.apache.org,35050,1347999502111.archivedhfilecleaner exiting 2012-09-18 20:18:31,128 info  [shutdown of org.apache.hadoop.hbase.fs.hfilesystem@194f467] hbase.minihbasecluster$singlefilesystemshutdownthread(186): hook closing fs=org.apache.hadoop.hbase.fs.hfilesystem@194f467 2012-09-18 20:18:31,128 info  [pool-1-thread-1] util.jvmclusterutil(263): shutdown of 1 master(s) and 5 regionserver(s) complete 2012-09-18 20:18:31,128 info  [pool-1-thread-1] client.hconnectionmanager$hconnectionimplementation(1523): closing zookeeper sessionid=0x139db080e69000d 2012-09-18 20:18:31,129 debug [master:0;hemera.apache.org,35050,1347999502111] zookeeper.zkutil(1141): master:35050-0x139db080e690000 retrieved 36 byte(s) of data from znode /hbase/master; data=pbuf\\x0a\\x1e\\x0a\\x11hemera.ap... 2012-09-18 20:18:31,130 debug [master:0;hemera.apache.org,35050,1347999502111] catalog.catalogtracker(257): stopping catalog tracker org.apache.hadoop.hbase.catalog.catalogtracker@1152c0e 2012-09-18 20:18:31,130 info  [hemera.apache.org,35050,1347999502111.timeoutmonitor] hbase.chore(81): hemera.apache.org,35050,1347999502111.timeoutmonitor exiting 2012-09-18 20:18:31,130 info  [hemera.apache.org,35050,1347999502111.timerupdater] hbase.chore(81): hemera.apache.org,35050,1347999502111.timerupdater exiting 2012-09-18 20:18:31,130 info  [master:0;hemera.apache.org,35050,1347999502111] master.hmaster(481): hmaster main thread exiting 2012-09-18 20:18:31,132 info  [pool-1-thread-1] zookeeper.minizookeepercluster(238): shutdown minizk cluster with all zk servers 2012-09-18 20:18:31,132 info  [pool-1-thread-1] log.slf4jlog(67): stopped selectchannelconnector@localhost:0 2012-09-18 20:18:31,231 debug [master:0;hemera.apache.org,35050,1347999502111-eventthread] zookeeper.zookeeperwatcher(265): hconnection 0x1c1eceb-0x139db080e690006 received zookeeper event, type=none, state=disconnected, path=null 2012-09-18 20:18:31,231 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(265): unittest-0x139db080e69000e received zookeeper event, type=none, state=disconnected, path=null 2012-09-18 20:18:31,231 debug [master:0;hemera.apache.org,35050,1347999502111-eventthread] zookeeper.zookeeperwatcher(363): hconnection 0x1c1eceb-0x139db080e690006 received disconnected from zookeeper, ignoring 2012-09-18 20:18:31,231 debug [pool-1-thread-1-eventthread] zookeeper.zookeeperwatcher(363): unittest-0x139db080e69000e received disconnected from zookeeper, ignoring 2012-09-18 20:18:31,237 warn  [org.apache.hadoop.hdfs.server.datanode.dataxceiverserver@1f09a31] datanode.dataxceiverserver(138): datanoderegistration(127.0.0.1:34950, storageid=ds-1597293543-140.211.11.27-34950-1347999501880, infoport=40910, ipcport=42169):dataxceiveserver:java.nio.channels.asynchronouscloseexception at java.nio.channels.spi.abstractinterruptiblechannel.end(abstractinterruptiblechannel.java:185) at sun.nio.ch.serversocketchannelimpl.accept(serversocketchannelimpl.java:159) at sun.nio.ch.serversocketadaptor.accept(serversocketadaptor.java:84) at org.apache.hadoop.hdfs.server.datanode.dataxceiverserver.run(dataxceiverserver.java:131) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:31,285 info  [regionserver:4;hemera.apache.org,38814,1347999502374.leasechecker] regionserver.leases(123): regionserver:4;hemera.apache.org,38814,1347999502374.leasechecker closing leases 2012-09-18 20:18:31,285 info  [regionserver:4;hemera.apache.org,38814,1347999502374.leasechecker] regionserver.leases(130): regionserver:4;hemera.apache.org,38814,1347999502374.leasechecker closed leases 2012-09-18 20:18:32,237 warn  [pool-1-thread-1] util.mbeans(73): hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid-809266774 javax.management.instancenotfoundexception: hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid-809266774 at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.exclusiveunregistermbean(defaultmbeanserverinterceptor.java:415) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.unregistermbean(defaultmbeanserverinterceptor.java:403) at com.sun.jmx.mbeanserver.jmxmbeanserver.unregistermbean(jmxmbeanserver.java:506) at org.apache.hadoop.metrics2.util.mbeans.unregister(mbeans.java:71) at org.apache.hadoop.hdfs.server.datanode.fsdataset.shutdown(fsdataset.java:2067) at org.apache.hadoop.hdfs.server.datanode.datanode.shutdown(datanode.java:799) at org.apache.hadoop.hdfs.minidfscluster.shutdowndatanodes(minidfscluster.java:566) at org.apache.hadoop.hdfs.minidfscluster.shutdown(minidfscluster.java:550) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminidfscluster(hbasetestingutility.java:503) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminicluster(hbasetestingutility.java:752) at org.apache.hadoop.hbase.testdrainingserver.teardownafterclass(testdrainingserver.java:132) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:36) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.junit.runners.suite.runchild(suite.java:128) at org.junit.runners.suite.runchild(suite.java:24) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:32,238 warn  [pool-1-thread-1] datanode.fsdatasetasyncdiskservice(121): asyncdiskservice has already shut down. 2012-09-18 20:18:32,238 info  [pool-1-thread-1] log.slf4jlog(67): stopped selectchannelconnector@localhost:0 2012-09-18 20:18:32,341 warn  [org.apache.hadoop.hdfs.server.datanode.dataxceiverserver@2a5ab9] datanode.dataxceiverserver(138): datanoderegistration(127.0.0.1:46013, storageid=ds-1826840059-140.211.11.27-46013-1347999501721, infoport=33986, ipcport=42754):dataxceiveserver:java.nio.channels.asynchronouscloseexception at java.nio.channels.spi.abstractinterruptiblechannel.end(abstractinterruptiblechannel.java:185) at sun.nio.ch.serversocketchannelimpl.accept(serversocketchannelimpl.java:159) at sun.nio.ch.serversocketadaptor.accept(serversocketadaptor.java:84) at org.apache.hadoop.hdfs.server.datanode.dataxceiverserver.run(dataxceiverserver.java:131) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:33,341 warn  [pool-1-thread-1] util.mbeans(73): hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid1641970170 javax.management.instancenotfoundexception: hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid1641970170 at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.exclusiveunregistermbean(defaultmbeanserverinterceptor.java:415) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.unregistermbean(defaultmbeanserverinterceptor.java:403) at com.sun.jmx.mbeanserver.jmxmbeanserver.unregistermbean(jmxmbeanserver.java:506) at org.apache.hadoop.metrics2.util.mbeans.unregister(mbeans.java:71) at org.apache.hadoop.hdfs.server.datanode.fsdataset.shutdown(fsdataset.java:2067) at org.apache.hadoop.hdfs.server.datanode.datanode.shutdown(datanode.java:799) at org.apache.hadoop.hdfs.minidfscluster.shutdowndatanodes(minidfscluster.java:566) at org.apache.hadoop.hdfs.minidfscluster.shutdown(minidfscluster.java:550) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminidfscluster(hbasetestingutility.java:503) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminicluster(hbasetestingutility.java:752) at org.apache.hadoop.hbase.testdrainingserver.teardownafterclass(testdrainingserver.java:132) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:36) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.junit.runners.suite.runchild(suite.java:128) at org.junit.runners.suite.runchild(suite.java:24) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:33,342 warn  [pool-1-thread-1] datanode.fsdatasetasyncdiskservice(121): asyncdiskservice has already shut down. 2012-09-18 20:18:33,342 info  [pool-1-thread-1] log.slf4jlog(67): stopped selectchannelconnector@localhost:0 2012-09-18 20:18:33,444 warn  [org.apache.hadoop.hdfs.server.datanode.dataxceiverserver@18c458] datanode.dataxceiverserver(138): datanoderegistration(127.0.0.1:39857, storageid=ds-635958744-140.211.11.27-39857-1347999501558, infoport=49006, ipcport=39278):dataxceiveserver:java.nio.channels.asynchronouscloseexception at java.nio.channels.spi.abstractinterruptiblechannel.end(abstractinterruptiblechannel.java:185) at sun.nio.ch.serversocketchannelimpl.accept(serversocketchannelimpl.java:159) at sun.nio.ch.serversocketadaptor.accept(serversocketadaptor.java:84) at org.apache.hadoop.hdfs.server.datanode.dataxceiverserver.run(dataxceiverserver.java:131) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:34,444 warn  [pool-1-thread-1] util.mbeans(73): hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid389747850 javax.management.instancenotfoundexception: hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid389747850 at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.exclusiveunregistermbean(defaultmbeanserverinterceptor.java:415) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.unregistermbean(defaultmbeanserverinterceptor.java:403) at com.sun.jmx.mbeanserver.jmxmbeanserver.unregistermbean(jmxmbeanserver.java:506) at org.apache.hadoop.metrics2.util.mbeans.unregister(mbeans.java:71) at org.apache.hadoop.hdfs.server.datanode.fsdataset.shutdown(fsdataset.java:2067) at org.apache.hadoop.hdfs.server.datanode.datanode.shutdown(datanode.java:799) at org.apache.hadoop.hdfs.minidfscluster.shutdowndatanodes(minidfscluster.java:566) at org.apache.hadoop.hdfs.minidfscluster.shutdown(minidfscluster.java:550) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminidfscluster(hbasetestingutility.java:503) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminicluster(hbasetestingutility.java:752) at org.apache.hadoop.hbase.testdrainingserver.teardownafterclass(testdrainingserver.java:132) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:36) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.junit.runners.suite.runchild(suite.java:128) at org.junit.runners.suite.runchild(suite.java:24) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:34,445 warn  [pool-1-thread-1] datanode.fsdatasetasyncdiskservice(121): asyncdiskservice has already shut down. 2012-09-18 20:18:34,445 info  [pool-1-thread-1] log.slf4jlog(67): stopped selectchannelconnector@localhost:0 2012-09-18 20:18:34,547 warn  [org.apache.hadoop.hdfs.server.datanode.dataxceiverserver@11c55bb] datanode.dataxceiverserver(138): datanoderegistration(127.0.0.1:47094, storageid=ds-1080581881-140.211.11.27-47094-1347999501341, infoport=51461, ipcport=36418):dataxceiveserver:java.nio.channels.asynchronouscloseexception at java.nio.channels.spi.abstractinterruptiblechannel.end(abstractinterruptiblechannel.java:185) at sun.nio.ch.serversocketchannelimpl.accept(serversocketchannelimpl.java:159) at sun.nio.ch.serversocketadaptor.accept(serversocketadaptor.java:84) at org.apache.hadoop.hdfs.server.datanode.dataxceiverserver.run(dataxceiverserver.java:131) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:35,547 warn  [pool-1-thread-1] util.mbeans(73): hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid-1857067045 javax.management.instancenotfoundexception: hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid-1857067045 at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.exclusiveunregistermbean(defaultmbeanserverinterceptor.java:415) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.unregistermbean(defaultmbeanserverinterceptor.java:403) at com.sun.jmx.mbeanserver.jmxmbeanserver.unregistermbean(jmxmbeanserver.java:506) at org.apache.hadoop.metrics2.util.mbeans.unregister(mbeans.java:71) at org.apache.hadoop.hdfs.server.datanode.fsdataset.shutdown(fsdataset.java:2067) at org.apache.hadoop.hdfs.server.datanode.datanode.shutdown(datanode.java:799) at org.apache.hadoop.hdfs.minidfscluster.shutdowndatanodes(minidfscluster.java:566) at org.apache.hadoop.hdfs.minidfscluster.shutdown(minidfscluster.java:550) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminidfscluster(hbasetestingutility.java:503) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminicluster(hbasetestingutility.java:752) at org.apache.hadoop.hbase.testdrainingserver.teardownafterclass(testdrainingserver.java:132) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:36) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.junit.runners.suite.runchild(suite.java:128) at org.junit.runners.suite.runchild(suite.java:24) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:35,548 warn  [pool-1-thread-1] datanode.fsdatasetasyncdiskservice(121): asyncdiskservice has already shut down. 2012-09-18 20:18:35,548 info  [pool-1-thread-1] log.slf4jlog(67): stopped selectchannelconnector@localhost:0 2012-09-18 20:18:35,650 warn  [org.apache.hadoop.hdfs.server.datanode.dataxceiverserver@b3b6a6] datanode.dataxceiverserver(138): datanoderegistration(127.0.0.1:58813, storageid=ds-1129259921-140.211.11.27-58813-1347999501165, infoport=47167, ipcport=40188):dataxceiveserver:java.nio.channels.asynchronouscloseexception at java.nio.channels.spi.abstractinterruptiblechannel.end(abstractinterruptiblechannel.java:185) at sun.nio.ch.serversocketchannelimpl.accept(serversocketchannelimpl.java:159) at sun.nio.ch.serversocketadaptor.accept(serversocketadaptor.java:84) at org.apache.hadoop.hdfs.server.datanode.dataxceiverserver.run(dataxceiverserver.java:131) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:36,179 warn  [datanode: [/x1/jenkins/jenkins-slave/workspace/hbase-trunk/trunk/hbase-server/target/test-data/d1b07c00-61dd-47e9-ac9b-95a060d6795d/dfscluster_6ef910b6-2888-4dda-84f2-de311d45c6a2/dfs/data/data1,/x1/jenkins/jenkins-slave/workspace/hbase-trunk/trunk/hbase-server/target/test-data/d1b07c00-61dd-47e9-ac9b-95a060d6795d/dfscluster_6ef910b6-2888-4dda-84f2-de311d45c6a2/dfs/data/data2]] util.mbeans(73): hadoop:service=datanode,name=datanodeinfo javax.management.instancenotfoundexception: hadoop:service=datanode,name=datanodeinfo at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.exclusiveunregistermbean(defaultmbeanserverinterceptor.java:415) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.unregistermbean(defaultmbeanserverinterceptor.java:403) at com.sun.jmx.mbeanserver.jmxmbeanserver.unregistermbean(jmxmbeanserver.java:506) at org.apache.hadoop.metrics2.util.mbeans.unregister(mbeans.java:71) at org.apache.hadoop.hdfs.server.datanode.datanode.unregistermxbean(datanode.java:522) at org.apache.hadoop.hdfs.server.datanode.datanode.shutdown(datanode.java:737) at org.apache.hadoop.hdfs.server.datanode.datanode.run(datanode.java:1471) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:36,650 warn  [pool-1-thread-1] util.mbeans(73): hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid-245364923 javax.management.instancenotfoundexception: hadoop:service=datanode,name=fsdatasetstate-undefinedstorageid-245364923 at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.exclusiveunregistermbean(defaultmbeanserverinterceptor.java:415) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.unregistermbean(defaultmbeanserverinterceptor.java:403) at com.sun.jmx.mbeanserver.jmxmbeanserver.unregistermbean(jmxmbeanserver.java:506) at org.apache.hadoop.metrics2.util.mbeans.unregister(mbeans.java:71) at org.apache.hadoop.hdfs.server.datanode.fsdataset.shutdown(fsdataset.java:2067) at org.apache.hadoop.hdfs.server.datanode.datanode.shutdown(datanode.java:799) at org.apache.hadoop.hdfs.minidfscluster.shutdowndatanodes(minidfscluster.java:566) at org.apache.hadoop.hdfs.minidfscluster.shutdown(minidfscluster.java:550) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminidfscluster(hbasetestingutility.java:503) at org.apache.hadoop.hbase.hbasetestingutility.shutdownminicluster(hbasetestingutility.java:752) at org.apache.hadoop.hbase.testdrainingserver.teardownafterclass(testdrainingserver.java:132) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:36) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.junit.runners.suite.runchild(suite.java:128) at org.junit.runners.suite.runchild(suite.java:24) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) 2012-09-18 20:18:36,651 warn  [pool-1-thread-1] datanode.fsdatasetasyncdiskservice(121): asyncdiskservice has already shut down. 2012-09-18 20:18:36,651 info  [pool-1-thread-1] log.slf4jlog(67): stopped selectchannelconnector@localhost:0 2012-09-18 20:18:36,753 warn  [org.apache.hadoop.hdfs.server.namenode.fsnamesystem$replicationmonitor@1827d1] namenode.fsnamesystem$replicationmonitor(2718): replicationmonitor thread received interruptedexception.java.lang.interruptedexception: sleep interrupted 2012-09-18 20:18:36,774 info  [pool-1-thread-1] hbase.hbasetestingutility(756): minicluster is down ",
        "label": 340
    },
    {
        "text": " findbugs  fix correctness warnings  see https://builds.apache.org/job/precommit-hbase-build/1313//artifact/trunk/patchprocess/newpatchfindbugswarnings.html fix the warnings in the correctness section. ",
        "label": 131
    },
    {
        "text": "missing  override annotation for rawasynctableimpl scan  ",
        "label": 377
    },
    {
        "text": "document rc voting guidelines in ref guide  document all necessary and suggested steps to vote on a release.   there are only a handful necessary checks a pmc member must do on every  release, and all of them relate to packaging, license and notice files, and  license auditing, which can be accomplished by running the rat tool, by  attempting to compile from source (unit tests optional), and through manual  inspection of license and notice files in the source distribution and  embedded in a sample of the binaries. this entire process should take you  less than 15 minutes, from my experience. this is the baseline. any individual pmcer may opt to do more than the baseline, but it is  optional. personally i would also read the compatibility report, and then  run the unit test suite in the background and come back to it when finished  to complete the voting task. in my opinion now that is the baseline tasks  any hbase pmc voter should take. beyond that, at least for my releases, you  can read the vote email to find the additional functional and performance  checks i might have done and factor that in to your voting confidence. you  can also run them yourselves, but is totally optional. ",
        "label": 435
    },
    {
        "text": "its and actions modify immutable tabledescriptors  ",
        "label": 187
    },
    {
        "text": "hlog compression may fail due to hadoop fs input stream returning partial bytes  in a recent test run, i noticed the following in test output: 2013-05-24 22:01:02,424 debug [regionserver:0;kiyo.gq1.ygridcore.net,42690,1369432806911.replicationsource,2] fs.hfilesystem$reorderwalblocks(327): /user/hortonzy/hbase/.logs/kiyo.gq1.ygridcore.net,42690,1369432806911/kiyo.gq1.ygridcore.net%2c42690%2c1369432806911.1369432840428 is an hlog file, so reordering blocks, last hostname will be:kiyo.gq1.ygridcore.net 2013-05-24 22:01:02,429 debug [regionserver:0;kiyo.gq1.ygridcore.net,42690,1369432806911.replicationsource,2] wal.protobuflogreader(118): after reading the trailer: waleditsstopoffset: 132235, filelength: 132243, trailerpresent: true 2013-05-24 22:01:02,438 error [regionserver:0;kiyo.gq1.ygridcore.net,42690,1369432806911.replicationsource,2] wal.protobuflogreader(236): error  while reading 691 wal kvs; started reading at 53272 and read up to 65538 2013-05-24 22:01:02,438 warn  [regionserver:0;kiyo.gq1.ygridcore.net,42690,1369432806911.replicationsource,2] regionserver.replicationsource(324): 2 got: java.io.ioexception: error  while reading 691 wal kvs; started reading at 53272 and read up to 65538         at org.apache.hadoop.hbase.regionserver.wal.protobuflogreader.readnext(protobuflogreader.java:237)         at org.apache.hadoop.hbase.regionserver.wal.readerbase.next(readerbase.java:96)         at org.apache.hadoop.hbase.replication.regionserver.replicationhlogreadermanager.readnextandsetposition(replicationhlogreadermanager.java:89)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.readallentriestoreplicateornextfile(replicationsource.java:404)         at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:320) caused by: java.lang.indexoutofboundsexception: index (30062) must be less than size (1)         at com.google.common.base.preconditions.checkelementindex(preconditions.java:305)         at com.google.common.base.preconditions.checkelementindex(preconditions.java:284)         at org.apache.hadoop.hbase.regionserver.wal.lrudictionary$bidirectionallrumap.get(lrudictionary.java:124)         at org.apache.hadoop.hbase.regionserver.wal.lrudictionary$bidirectionallrumap.access$000(lrudictionary.java:71)         at org.apache.hadoop.hbase.regionserver.wal.lrudictionary.getentry(lrudictionary.java:42)         at org.apache.hadoop.hbase.regionserver.wal.walcellcodec$compressedkvdecoder.readintoarray(walcellcodec.java:210)         at org.apache.hadoop.hbase.regionserver.wal.walcellcodec$compressedkvdecoder.parsecell(walcellcodec.java:184)         at org.apache.hadoop.hbase.codec.basedecoder.advance(basedecoder.java:46)         at org.apache.hadoop.hbase.regionserver.wal.waledit.readfromcells(waledit.java:213)         at org.apache.hadoop.hbase.regionserver.wal.protobuflogreader.readnext(protobuflogreader.java:217)         ... 4 more 2013-05-24 22:01:02,439 debug [regionserver:0;kiyo.gq1.ygridcore.net,42690,1369432806911.replicationsource,2] regionserver.replicationsource(583): nothing to replicate, sleeping 100 times 10 will attach test output. ",
        "label": 441
    },
    {
        "text": "add scan settimerange  support in shell  the shell does not support scans by time range. this is trivial, simply add two more optional values mintimestamp and maxtimestamp and if both are set call scan.settimerange(minstamp, maxstamp). ",
        "label": 194
    },
    {
        "text": "attach memcached as secondary block cache to regionserver  currently, block caches are limited by heap size, which is limited by garbage collection times in java. we can get around this by using memcached w/jni as a secondary block cache. this should be faster than the linux file system's caching, and allow us to very quickly gain access to a high quality slab allocated cache. ",
        "label": 289
    },
    {
        "text": "when using ant  projecthelp to build hbase not all the important options show up   ",
        "label": 161
    },
    {
        "text": "grant revoke global permissions  hbase-3025 introduced simple acls based on coprocessors. it defines global/table/cf/cq level permissions. however, there is no way to grant/revoke global level permissions, other than the hbase.superuser conf setting. ",
        "label": 309
    },
    {
        "text": "retaining locality after restart broken  in defaultloadbalancer, we implement the \"retain assignment\" function like so:       if (sn != null && servers.contains(sn)) {         assignments.get(sn).add(region.getkey()); but this will never work since after a cluster restart, all servers have a new servername with a new startcode. ",
        "label": 453
    },
    {
        "text": "hbase protocol compile failed for name conflict of regiontransition  the compile of hbase-protocol failed for there are two message named regiontransition in zookeeper.proto and regionserverstatus.proto $mvn clean package -pcompile-protobuf -x  ....  [debug] regionserverstatus.proto:81:9: \"regiontransition\" is already defined in file \"zookeeper.proto\".  [debug] regionserverstatus.proto:114:12: \"regiontransition\" seems to be defined in \"zookeeper.proto\", which is not imported by \"regionserverstatus.proto\". to use it here, please add the necessary import.  [error] protoc compiler error through it will be ok if we compile the the zookeeper.proto and regionserverstatus.proto seperately. it is not very convenient. the new regiontransition is regionserverstatus.proto and introduced in hbase-11059.  jimmy xiang  what's your suggestion about this issue? ",
        "label": 242
    },
    {
        "text": "remove unused imports from hbase thrift module  currently there are three warnings regarding unused imports in the hbase-thrift module (see the attached xml). the unused imports should be removed. ",
        "label": 188
    },
    {
        "text": "deprecate classes that depend on  old  metrics framework  org.apache.hadoop.metrics.* classes have been deprecated. we should deprecate hbase classes that depend on them. ",
        "label": 154
    },
    {
        "text": " hbck2  add filesystem fixup to hbck2  poking around, was going to add a 'filesystem' command to hbck2. it would take nothing or a list of one or more table names and optionally a path to a region. if nothing passed, it would review the fs for corrupt hfiles and bad links reporting the bad as it went. if --fix is passed, it will sideline corrupt files and mis-links. would advise reopen/rolling-restart if change made. for starters would make use of the hbck1 fs tools added by hbase-22680. above would exploit the following facillty from hbck1.  -checkcorrupthfiles  -sidelinecorrupthfiles  -fixversionfile  -fixreferencefiles  -fixhfilelinks this would address the following from parent issue at least: fix hfile link problems... enumerate store files to determine file level corruption... i noticed that the 'read-only' hbck1 tool hbasefsck which is present in the hbase2 codebase actually allows fixes in the filesystem; i.e. you'd run hbasefsck and pass say -fixversionfile. this is mixed in w/ all the other hbck1 noise but an operator should be able to make progress with what is there regards hfile corruption, bad links, etc. in other words, could just doc this facility in the hbck2 tool point folks to the latent hbck1 hbasefsck if they need to do fixup of the fs (would be confusing but it exists). ",
        "label": 314
    },
    {
        "text": "replication admin get peer state id  in ruby script pointing to undefined method  getpeerstate'  list_peers is failing because replication_admin#get_peer_state(id) pointing to the method removed from replicationadmin. we need to read peer state from replicationpeer. hbase(main):003:0> list_peers  peer_id cluster_key state error: undefined method `getpeerstate' for #<java::orgapachehadoophbaseclientreplication::replicationadmin:0x7e299629> here is some help for this command: list all replication peer clusters.   hbase> list_peers     # get peer cluster state     def get_peer_state(id)       @replication_admin.getpeerstate(id)     end ",
        "label": 229
    },
    {
        "text": "asyncwriter of fshlog might throw arrayindexoutofboundsexception  asyncwriter of fshlog might throw arrayindexoutofboundsexception because of the following code in asyncwriter#run():              }            }            if (!hasidlesyncer) {             int idx = (int)this.lastwrittentxid % asyncsyncers.length;              asyncsyncers[idx].setwrittentxid(this.lastwrittentxid);            }          } in obove code, \"this.lastwrittentxid % asyncsyncers.length\" might become negative when this.lastwrittentxid is bigger than interger.max_value where this.lastwrittentxid is a long. the attachment gives a quick fix. ",
        "label": 238
    },
    {
        "text": "fix rest tests on trunk  most of the rest tests do not pass on trunk. most likely because configuration is being generated internally within rest classes rather than being passed in, so when tests override configs they are not getting picked up. there was a similar issue already fixed with thrift and avro. ",
        "label": 247
    },
    {
        "text": "for single row reads of specific columns  seek to the first column in hfiles rather than start of row  currently we will always seek to the start of a row. if we are getting specific columns, we should seek to the first column in that row. ",
        "label": 357
    },
    {
        "text": "builtingzipdecompressor is only released during full gc  that seems to be bug in hadoop, actually. builtingzipdecompressor.end() needs to be called to release it's resource, but it is not called anywhere in codecpool.  instead the end() is called by finalize(), which is only called during a full gc (or never, depending on jvm). this is only an issue in test. in real life most folks will have the native gzipdecompressor ",
        "label": 286
    },
    {
        "text": "hbaseconfiguration  ctor  deprecated  hbaseconfiguration() deprecated and replaced by hbaseconfiguration.create() ",
        "label": 266
    },
    {
        "text": " part of hbase  we wait on leases to expire before regionserver goes down  rather  just let client fail  addressing this issue will help hbase-1583. we should do for 0.20.0 and perhaps for 0.19.x even. currently, if outstanding leases, in hregion close, we'll hang until lease expires. could be a minute. could be worse, the client might come in and renew the lease a few times at least till it finishes out the region. this gets in way of regionserver shutting down fast. j-d suggests that regionserver should just go down and outstanding clients should fail rather than try and be nice to outstanding clients (in his case, his mr job had failed so no clients... but we insist on lease expiring). ",
        "label": 314
    },
    {
        "text": "acknowledged writes may get lost if regionserver clock is set backwards  we experience a small amount of lost acknowledged writes in production on july 1st (~700 identified so far). what happened was that we had ntp turned off since june 29th to prevent issues due to the leap second on june 30th. ntp was turned back on july 1st. the next day, we noticed we were missing writes to a few of our higher throughput aggregation tables. we found that this is caused by hbase taking the current time using system.currenttimemillis, which may be set backwards by ntp, and using this without any checks to populate the timestamp of rows for which the client didn't supply a timestamp. our application uses a read-modify-write pattern using get+checkandput to perform aggregation as follows: 1. read version 1  2. mutate  3. write version 2  4. read version 2  5. mutate  6. write version 3 the application retries the full read-modify-write if the checkandput fails. what must have happened on july 1st, after we started ntp back up, was this (timestamps added): 1. read version 1 (timestamp 10)  2. mutate  3. write version 2 (hbase-assigned timestamp 11)  4. read version 2 (timestamp 11)  5. mutate  6. write version 3 (hbase-assigned timestamp 10) hence, the last write was eclipsed by the first write, and hence, an acknowledged write was lost. while this seems to match documented behavior (paraphrasing: \"if timestamp is not specified hbase will assign a timestamp using system.currenttimemillis\" \"the row with the highest timestamp will be returned by get\"), i think it is very unintuitive and needs at least a big warning in the documentation, along the lines of \"acknowledged writes may not be visible unless the timestamp is explicitly specified and equal to or larger than the highest timestamp for that row\". i would also like to use this ticket to start a discussion on if we can make the behavior better: could hbase assign a timestamp of max(max timestamp for the row, system.currenttimemillis()) in the checkandput write path, instead of blindly taking system.currenttimemillis(), similar to what has been done in hbase-12449 for increment and append? thoughts? ",
        "label": 155
    },
    {
        "text": "fix locking in memcache flush  memcache flushing holds a write lock while it reopens storefilescanners. i had a case where this process timed out and caused an exception to be thrown, which made the region server believe it had been unable to flush it's cache and shut itself down. stack trace is: #  \"regionserver/0:0:0:0:0:0:0:0:60020.cacheflusher\" daemon prio=10 tid=0x00000000562df400 nid=0x15d1 runnable [0x000000004108b000..0x000000004108bd90]  #  java.lang.thread.state: runnable  #  at java.util.zip.crc32.updatebytes(native method)  #  at java.util.zip.crc32.update(crc32.java:45)  #  at org.apache.hadoop.util.datachecksum.update(datachecksum.java:223)  #  at org.apache.hadoop.fs.fsinputchecker.readchecksumchunk(fsinputchecker.java:241)  #  at org.apache.hadoop.fs.fsinputchecker.fill(fsinputchecker.java:177)  #  at org.apache.hadoop.fs.fsinputchecker.read1(fsinputchecker.java:194)  #  at org.apache.hadoop.fs.fsinputchecker.read(fsinputchecker.java:159)  # locked <0x00002aaaec1bd2d8> (a org.apache.hadoop.hdfs.dfsclient$blockreader)  #  at org.apache.hadoop.hdfs.dfsclient$blockreader.read(dfsclient.java:1061)  # locked <0x00002aaaec1bd2d8> (a org.apache.hadoop.hdfs.dfsclient$blockreader)  #  at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.readbuffer(dfsclient.java:1616)  # locked <0x00002aaad1239000> (a org.apache.hadoop.hdfs.dfsclient$dfsinputstream)  #  at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.read(dfsclient.java:1666)  # locked <0x00002aaad1239000> (a org.apache.hadoop.hdfs.dfsclient$dfsinputstream)  #  at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.read(dfsclient.java:1593)  # locked <0x00002aaad1239000> (a org.apache.hadoop.hdfs.dfsclient$dfsinputstream)  #  at java.io.datainputstream.readint(datainputstream.java:371)  #  at org.apache.hadoop.hbase.io.sequencefile$reader.next(sequencefile.java:1943)  # locked <0x00002aaad1238c38> (a org.apache.hadoop.hbase.io.sequencefile$reader)  #  at org.apache.hadoop.hbase.io.sequencefile$reader.next(sequencefile.java:1844)  # locked <0x00002aaad1238c38> (a org.apache.hadoop.hbase.io.sequencefile$reader)  #  at org.apache.hadoop.hbase.io.sequencefile$reader.next(sequencefile.java:1890)  # locked <0x00002aaad1238c38> (a org.apache.hadoop.hbase.io.sequencefile$reader)  #  at org.apache.hadoop.hbase.io.mapfile$reader.next(mapfile.java:525)  # locked <0x00002aaad1238b80> (a org.apache.hadoop.hbase.io.halfmapfilereader)  #  at org.apache.hadoop.hbase.io.halfmapfilereader.next(halfmapfilereader.java:192)  # locked <0x00002aaad1238b80> (a org.apache.hadoop.hbase.io.halfmapfilereader)  #  at org.apache.hadoop.hbase.regionserver.storefilescanner.getnext(storefilescanner.java:312)  #  at org.apache.hadoop.hbase.regionserver.storefilescanner.openreaders(storefilescanner.java:110)  #  at org.apache.hadoop.hbase.regionserver.storefilescanner.updatereaders(storefilescanner.java:378)  #  at org.apache.hadoop.hbase.regionserver.hstore.notifychangedreadersobservers(hstore.java:737)  #  at org.apache.hadoop.hbase.regionserver.hstore.updatereaders(hstore.java:725)  #  at org.apache.hadoop.hbase.regionserver.hstore.internalflushcache(hstore.java:694)  # locked <0x00002aaab7b41d30> (a java.lang.integer)  #  at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:630)  #  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:881)  #  at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:789)  #  at org.apache.hadoop.hbase.regionserver.memcacheflusher.flushregion(memcacheflusher.java:227)  #  at org.apache.hadoop.hbase.regionserver.memcacheflusher.run(memcacheflusher.java:137) ",
        "label": 247
    },
    {
        "text": "use sleep multilier when choosing sinks in replicationsource  currently we see this every second. filling up the log: 2013-05-10 18:36:00,766 info org.apache.zookeeper.clientcnxn: opening socket connection to server ist6-mnds1-2-sfm.ops.sfdc.net/10.224.156.197:2181. will not attempt to authenticate using sasl (unable to locate a login configuration) 2013-05-10 18:36:00,767 warn org.apache.zookeeper.clientcnxn: session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect java.net.connectexception: connection refused at sun.nio.ch.socketchannelimpl.checkconnect(native method) at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:599) at org.apache.zookeeper.clientcnxnsocketnio.dotransport(clientcnxnsocketnio.java:350) at org.apache.zookeeper.clientcnxn$sendthread.run(clientcnxn.java:1068) 2013-05-10 18:36:01,868 info org.apache.zookeeper.clientcnxn: opening socket connection to server ist6-mnds1-4-sfm.ops.sfdc.net/10.224.156.199:2181. will not attempt to authenticate using sasl (unable to locate a login configuration) 2013-05-10 18:36:01,870 warn org.apache.zookeeper.clientcnxn: session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect java.net.connectexception: connection refused at sun.nio.ch.socketchannelimpl.checkconnect(native method) at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:599) at org.apache.zookeeper.clientcnxnsocketnio.dotransport(clientcnxnsocketnio.java:350) at org.apache.zookeeper.clientcnxn$sendthread.run(clientcnxn.java:1068) 2013-05-10 18:36:01,971 info org.apache.zookeeper.clientcnxn: opening socket connection to server ist6-mnds1-3-sfm.ops.sfdc.net/10.224.156.198:2181. will not attempt to authenticate using sasl (unable to locate a login configuration) patch is trivial. ",
        "label": 286
    },
    {
        "text": "verify protobuf serialization is correctly chunking upon read to avoid direct memory ooms  result.readfields() used to read from the input stream in 8k chunks to avoid oom issues with direct memory.  (reading variable sized chunks into direct memory prevent the jvm from reusing the allocated direct memory and direct memory is only collected during full gcs) this is just to verify protobufs parsefrom type methods do the right thing as well so that we do not reintroduce this problem. ",
        "label": 139
    },
    {
        "text": "add timerange support into increment to optimize for counters that are partitioned on time  in many use cases of increments, a given counter is only incremented during a specific window of time (ie. the counters are partitioned/sharded by time). with this kind of schema, you are constantly creating new counters. when a new counter is \"created\" (incremented the first time) you will always end up looking at a block from every file in the region because no previous value will exist. however, with the new timerange optimizations that skip files if they don't contain values of the timerange you're interested in, we could utilize that information to optimize the get within the increment. this would be optional and an addition to the increment class. ",
        "label": 247
    },
    {
        "text": "near instantaneous online schema and table state updates  we should not need to take a table offline to update hcd or htd. one option for that is putting htds and hcds up into zk, with mirror on disk catalog tables to be used only for cold init scenarios, as discussed on irc. in this scheme, regionservers hosting regions of a table would watch permanent nodes in zk associated with that table for schema updates and take appropriate actions out of the watcher. in effect, schema updates become another item in the todo list. /hbase/tables/<table-name>/schema must be associated with a write locking scheme also handled with zk primitives to avoid situations where one concurrent update clobbers another. ",
        "label": 341
    },
    {
        "text": "publish hbase build against h1 and h2 adding ' hadoop1' or ' hadoop2' to version string  so we can publish both the hadoop1 and the hadoop2 jars to a maven repository, and so we can publish two packages, one for hadoop1 and one for hadoop2, given how maven works, our only alternative (to the best of my knowledge and after consulting others) is by amending the version string to include hadoop1 or hadoop2. ",
        "label": 314
    },
    {
        "text": "default hbase dynamic jars dir to hbase rootdir jars  a reasonable default for hbase.dynamic.jars.dir would be hbase.rootdir/jars so that folks aren't forced to edit their hbase-sites.xml to take advantage of the new, cool feature to load coprocessor/custom filter jars out of hdfs. ",
        "label": 464
    },
    {
        "text": "stop hbase sh clean up  backup master  in stop-hbase.sh:   # todo: store backup masters in zookeeper and have the primary send them a shutdown message   # stop any backup masters   \"$bin\"/hbase-daemons.sh --config \"${hbase_conf_dir}\" \\     --hosts \"${hbase_backup_masters}\" stop master-backup after hbase-5213, stop-hbase.sh -> hbase master stop will bring down the backup master too via the cluster status znode. we should not need the above code anymore. another issue happens when the current master died and the backup master became the active master. nohup nice -n ${hbase_niceness:-0} \"$hbase_home\"/bin/hbase \\    --config \"${hbase_conf_dir}\" \\    master stop \"$@\" > \"$logout\" 2>&1 < /dev/null & waitforprocessend `cat $pid` 'stop-master-command' we can still issue 'hbase-stop.sh' from the old master.  stop-hbase.sh -> hbase master stop -> look for active master -> request shutdown  this process still works.  but the waitforprocessend statement will not work since the local master pid is not relevant anymore.  what is the best way in the this case? ",
        "label": 234
    },
    {
        "text": "add prefetch support for async region locator  it will be useful for large tables. ",
        "label": 149
    },
    {
        "text": "include the cause when constructing restoresnapshotexception in restoresnapshot  when implementing hbase-21718, a snapshot related ut fails because of the incorrect cause of restoresnapshotexception. finally i found that we just a create a restoresnapshotexception without providing the cause in restoresnapshot. we should fix this. ",
        "label": 149
    },
    {
        "text": "add option to set block cache to false on sparksql executions  i was working at a client with a ported down version of the spark module for hbase and realized we didn't add an option to turn of block cache for the scans. at the client i just disabled all caching with spark sql, this is an easy but very impactful fix. the fix for this patch will make this configurable ",
        "label": 512
    },
    {
        "text": "wrong or indeterminate behavior when there are duplicate versions of a column  as of now, both gets and scanners will end up returning all duplicate versions of a column. the ordering of them is indeterminate. we need to decide what the desired/expected behavior should be and make it happen. note: it's nearly impossible for this to work with gets as they are now implemented in 1304 so this is really a scanner issue. to implement this correctly with gets, we would have to undo basically all the optimizations that gets do and making them far slower than a scanner. ",
        "label": 357
    },
    {
        "text": "hbck should check lingering reference hfile and have option to sideline them automatically  sometimes, some lingering reference hfile points to some region doesn't exist any more. this will prevent the region to open. hbck should be able to find these files and sideline them automatically if requested. ",
        "label": 242
    },
    {
        "text": "port hbase fast forwarding fuzzyrowfilter to  this is to port hbase-6509: 'implement fast-forwarding fuzzyrowfilter to allow filtering rows e.g. by \"???alex?b\"' to 0.94 ",
        "label": 46
    },
    {
        "text": " rsgroup  forward port hbase to master branch  ",
        "label": 370
    },
    {
        "text": "jenkins build should compare trunk vs patch for javadoc warnings  the javadoc check should look for an increase in the number of warnings. it can do so by running javadoc against trunk before running it for the patch. this will increase build times. ",
        "label": 339
    },
    {
        "text": "add hadoop x  hbase 0.2.0 will run on hadoop 0.17. we need to add released 0.17 hadoop when available. also need to update the libs under lib/native to match the committed hadoop. ",
        "label": 241
    },
    {
        "text": "update ref guide about the async client to reflect the change in hbase  ",
        "label": 149
    },
    {
        "text": "backport hbase to branch  ",
        "label": 352
    },
    {
        "text": "secure thriftserver needs to login before calling hbasehandler  in thriftserver.java the following call is made serverrunner = new thriftserverrunner(conf); which invokes public thriftserverrunner(configuration conf) throws ioexception {     this(conf, new thriftserverrunner.hbasehandler(conf));   } all of this is happening before the user has logged in and fails. ",
        "label": 50
    },
    {
        "text": "the current thrift api does not allow a new scanner to be created without supplying a column list unlike the other apis   the current thrift api does not allow a new scanner to be created without supplying a column list, unlike the rest api. i posted this on the hbase-users mailing list. others concurred that it appears to have been an oversight in the thrift api. its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner. ",
        "label": 451
    },
    {
        "text": "testsplitlogworker occasionally fails  testsplitlogworker failed in 10% of 50 runs of the 0.98 branch test suite, but only when using jdk 6 on ubuntu 12. ",
        "label": 38
    },
    {
        "text": "fix findbugs complaint in hbase server  ",
        "label": 314
    },
    {
        "text": "backport of to branch  backport the hbase-2707 fix to the 0.20 branch. if 2707 happens, it hoses cluster... ",
        "label": 314
    },
    {
        "text": " performance  the replay of logs on server crash takes way too long  watching recovery from a crash on streamy.com where there were 1048 logs and repay is running at rate of about 20 seconds each. meantime these regions are not online. this is way too long to wait on recovery for a live site. marking critical. performance related so priority and in 0.20.0. ",
        "label": 229
    },
    {
        "text": "dfs errors during a read operation  get scan  may cause write outliers  this is a similar issue as discussed in hbase-8228 1) a scanner holds the store.readlock() while opening the store files ... encounters errors. thus, takes a long time to finish. 2) a flush is completed, in the mean while. it needs the write lock to commit(), and update scanners. hence ends up waiting. 3+) all puts (and also gets) to the cf, which will need a read lock, will have to wait for 1) and 2) to complete. thus blocking updates to the system for the dfs timeout. fix:  open store files outside the read lock. getscanners() already tries to do this optimisation. however, store.getscanner() which calls this functions through the storescanner constructor, redundantly tries to grab the readlock. causing the readlock to be held while the storefiles are being opened, and seeked.  we should get rid of the readlock() in store.getscanner(). this is not required. the constructor for storescanner calls getscanners(xxx, xxx, xxx). this has the required locking already. ",
        "label": 154
    },
    {
        "text": "tif and tof use log4j directly rather than apache commons logging  ",
        "label": 314
    },
    {
        "text": "update testenvironmentedgemanager to fix error  fixes a small issue with the test. fixing the unit tests false assumption that the delegate  starts out being the default delegate. this assumption is violated  if another part of the code calls injectedge with something  other than the defaultenvironmentedge. ",
        "label": 154
    },
    {
        "text": " nightly  make xml test non voting in branch  hbase-19354 moved branch-1.2 to use azul openjdk. turns out zulu-7 does does not have a js script engine so jrunscript fails which causes the xml check to fail with 19:55:59 | -1 | xml | 0m 7s | the source tree has 60 ill-formed xml   19:55:59 | | | | file(s). the xml.txt file has this in it: script engine for language js can not be found if i run jrunscript from zulu-7 on cmd-line, it does this: $ $java_home/bin/jrunscript  script engine for language js can not be found for now, make the xml check non-voting in branch-1.2 so can see if we can get the nightly running. ",
        "label": 314
    },
    {
        "text": "package level javadoc should have example client or at least point at the faq  package-level javadoc should have example client or at least point at the faq that ties to package release.  for example, the batchupdate example code appears in the faq sample code, but it is missing in the javadoc.  it will be better to tie the example code snippet with the new methods that replaces the deprecated methods which   will also be tied to release in the javadoc. ",
        "label": 86
    },
    {
        "text": "metareader fullscan doesn't stop scanning when vistor returns false in version  in current 0.90 code,  public static void fullscan(catalogtracker catalogtracker,       final visitor visitor, final byte [] startrow)   throws ioexception {     hregioninterface metaserver =       catalogtracker.waitformetaserverconnectiondefault();     scan scan = new scan();     if (startrow != null) scan.setstartrow(startrow);     scan.addfamily(hconstants.catalog_family);     long scannerid = metaserver.openscanner(         hregioninfo.first_meta_regioninfo.getregionname(), scan);     try {       result data;       while((data = metaserver.next(scannerid)) != null) {         if (!data.isempty()) visitor.visit(data);       }     } finally {       metaserver.close(scannerid);     }     return;   } if visitor.visit(data) return false, the scan will not stop;  however, it is not the same as the description of visitor public interface visitor {     /**      * visit the catalog table row.      * @param r a row from catalog table      * @return true if we are to proceed scanning the table, else false if      * we are to stop now.      */     public boolean visit(final result r) throws ioexception;   } i think it is a miss, and trunk doesn't exist this hole. ",
        "label": 107
    },
    {
        "text": "clusterstatus should be able to return responses by scope  the current clusterstatus response returns too much information about the load per region and replication cluster wide. sometimes that response can be quite large (10s or 100s of mbs) and methods like getserversize() or getregionscount() don't really need the full response. one possibility is to provide a scope (or filter) for the clusterstatus requests to limit the response back to the client. ",
        "label": 370
    },
    {
        "text": "findbugs issues  findbugs issues/ fixes for a subset of them. ",
        "label": 266
    },
    {
        "text": "region is on service before completing initialization when doing rollback of split  it will affect read correctness  ",
        "label": 107
    },
    {
        "text": "list namespace tables seems to fail  i was trying the rc0 of 0.95.2. and was on the shell trying to list tables in a namespace. i tried listing tables in the 'hbase' namespace and in a namespace i created. the listing fails: hbase(main):008:0> list_namespace_tables 'hbase' table error: org.apache.hadoop.hbase.client.retriesexhaustedexception: failed after attempts=7, exceptions: fri aug 16 17:47:17 utc 2013, org.apache.hadoop.hbase.client.rpcretryingcaller@2313b44d, org.apache.hadoop.hbase.ipc.remotewithextrasexception(java.io.ioexception): java.io.ioexception: illegal character <58> at 5. user-space table qualifiers can only contain 'alphanumeric characters': i.e. [a-za-z_0-9-.]: hbase:meta         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2194)         at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1861) caused by: java.lang.illegalargumentexception: illegal character <58> at 5. user-space table qualifiers can only contain 'alphanumeric characters': i.e. [a-za-z_0-9-.]: hbase:meta         at org.apache.hadoop.hbase.tablename.islegaltablequalifiername(tablename.java:155)         at org.apache.hadoop.hbase.tablename.islegaltablequalifiername(tablename.java:125)         at org.apache.hadoop.hbase.tablename.finishvalueof(tablename.java:249)         at org.apache.hadoop.hbase.tablename.valueof(tablename.java:242)         at org.apache.hadoop.hbase.master.hmaster.listtablenamesbynamespace(hmaster.java:3130)         at org.apache.hadoop.hbase.master.hmaster.listtablenamesbynamespace(hmaster.java:3063)         at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:27764)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2156)         ... 1 more ",
        "label": 38
    },
    {
        "text": "add convenience methods to rowfilterset  this patch adds getoperator and addfilter to rowfilterset. i found this useful when constructing filters from higher-level queries. ",
        "label": 110
    },
    {
        "text": "htable setwritebuffersize does not flush the writebuffer when its size is set to a value lower than its current size   when setting the size of the write buffer to a value lower than the current size of data in the write buffer, the content of the write buffer should be flushed so it does not occupy in memory more than its new size for an extended period of time. ",
        "label": 306
    },
    {
        "text": "testsnapshotexceptionsnare and testwalreferencetask missing test annotation failing testchecktestclasses  trivial fix. ",
        "label": 248
    },
    {
        "text": "createtable java doc needs to be improved  hbaseadmin.createtable() java doc says public void createtable(htabledescriptor desc,  byte[][] splitkeys)  throws ioexception  creates a new table with an initial set of empty regions defined by the specified split keys. the total number of regions created will be the number of split keys plus one (the first region has a null start key and the last region has a null end key). synchronous operation. if we specify null values for first region start key and last region end key, geting nullpointerexception as arrays.sort compares each element.  i guess the documentation should not talk about null values and explain about splitkeys[][] length as n-1, where n is number of regions. splitkeys[][] would look like splitkeys[0] = \"key value 1\" .. splitkeys[n-1] = \"key value n-1\" ",
        "label": 333
    },
    {
        "text": "deadlock in testnamespaceauditor testregionoperations on  this was left as a zombie after one of my test runs this weekend. \"walprocedurestoresyncthread\" daemon prio=10 tid=0x00007f3ccc209000 nid=0x3960 in object.wait() [0x00007f3c6b6b5000]    java.lang.thread.state: blocked (on object monitor) at java.lang.object.wait(native method) at java.lang.object.wait(object.java:503) at org.apache.hadoop.ipc.client.call(client.java:1397) - locked <0x00000007f2813390> (a org.apache.hadoop.ipc.client$call) at org.apache.hadoop.ipc.client.call(client.java:1364) at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:206) at com.sun.proxy.$proxy23.create(unknown source) at sun.reflect.generatedmethodaccessor25.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:187) at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:102) at com.sun.proxy.$proxy23.create(unknown source) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocoltranslatorpb.create(clientnamenodeprotocoltranslatorpb.java:264) at sun.reflect.generatedmethodaccessor20.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:279) at com.sun.proxy.$proxy24.create(unknown source) at sun.reflect.generatedmethodaccessor20.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:279) at com.sun.proxy.$proxy24.create(unknown source) at org.apache.hadoop.hdfs.dfsoutputstream.newstreamforcreate(dfsoutputstream.java:1612) at org.apache.hadoop.hdfs.dfsclient.create(dfsclient.java:1488) at org.apache.hadoop.hdfs.dfsclient.create(dfsclient.java:1413) at org.apache.hadoop.hdfs.distributedfilesystem$6.docall(distributedfilesystem.java:387) at org.apache.hadoop.hdfs.distributedfilesystem$6.docall(distributedfilesystem.java:383) at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81) at org.apache.hadoop.hdfs.distributedfilesystem.create(distributedfilesystem.java:383) at org.apache.hadoop.hdfs.distributedfilesystem.create(distributedfilesystem.java:327) at org.apache.hadoop.fs.filesystem.create(filesystem.java:906) at org.apache.hadoop.fs.filesystem.create(filesystem.java:887) at org.apache.hadoop.fs.filesystem.create(filesystem.java:784) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.rollwriter(walprocedurestore.java:766) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.rollwriter(walprocedurestore.java:733) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.tryrollwriter(walprocedurestore.java:668) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.periodicroll(walprocedurestore.java:711) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.syncloop(walprocedurestore.java:531) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore.access$000(walprocedurestore.java:66) at org.apache.hadoop.hbase.procedure2.store.wal.walprocedurestore$1.run(walprocedurestore.java:180) ",
        "label": 198
    },
    {
        "text": "improve bytes to accept byte buffers which don't allow us to directly access their backing arrays  inside hbase, it seems that there is the implicit assumption that byte buffers have backed arrays and are not read-only, and we can freely call bytebuffer.array() and arrayoffset() without runtime exceptions. but some classes, including bytes, are supposed to be used by users from outside of hbase, and we should think the possibility that methods receive byte buffers which don't hold the assumption. ",
        "label": 200
    },
    {
        "text": "enable peer cluster to choose change the columnfamilies tables it really want to replicate from a source cluster  consider scenarios (all cf are with replication-scope=1): 1) cluster s has 3 tables, table a has cfa,cfb, table b has cfx,cfy, table c has cf1,cf2. 2) cluster x wants to replicate table a : cfa, table b : cfx and table c from cluster s. 3) cluster y wants to replicate table b : cfy, table c : cf2 from cluster s. current replication implementation can't achieve this since it'll push the data of all the replicatable column-families from cluster s to all its peers, x/y in this scenario. this improvement provides a fine-grained replication theme which enable peer cluster to choose the column-families/tables they really want from the source cluster: a). set the table:cf-list for a peer when addpeer:  hbase-shell> add_peer '3', \"zk:1100:/hbase\", \"table1; table2:cf1,cf2; table3:cf2\" b). view the table:cf-list config for a peer using show_peer_tablecfs:  hbase-shell> show_peer_tablecfs \"1\" c). change/set the table:cf-list for a peer using set_peer_tablecfs:  hbase-shell> set_peer_tablecfs '2', \"table1:cfx; table2:cf1; table3:cf1,cf2\" in this theme, replication-scope=1 only means a column-family can be replicated to other clusters, but only the 'table:cf-list list' determines which cf/table will actually be replicated to a specific peer. to provide back-compatibility, empty 'table:cf-list list' will replicate all replicatable cf/table. (this means we don't allow a peer which replicates nothing from a source cluster, we think it's reasonable: if replicating nothing why bother adding a peer?) this improvement addresses the exact problem raised by the first faq in \"http://hbase.apache.org/replication.html\":  \"global means replicate? any provision to replicate only to cluster x and not to cluster y? or is that for later?  yes, this is for much later.\" i also noticed somebody mentioned \"replication-scope\" as integer rather than a boolean is for such fine-grained replication purpose, but i think extending \"replication-scope\" can't achieve the same replication granularity flexibility as providing above per-peer replication configurations. this improvement has been running smoothly in our production clusters (xiaomi) for several months. ",
        "label": 203
    },
    {
        "text": "jenkins build failing  failsafe npe'ing  builds up on jenkins have been failing over the last few days. looking at it w/ nkeyway, its kinda odd. i ran exact command locally as did n and it works fine. i removed all of my repo and still works. n looked at surefire source. its the includes that is coming back empty causing the npe we see up on jenkins. extra odd is that it does not seem like it a checkin of ours that brought this on. see here where its 'working' on 0.94 branch: https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.94/76/ then a little later ted triggers a build w/ no changes made: https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.94/77/console its failing running the integration test phase. let me mess around and try and get it going again. ",
        "label": 314
    },
    {
        "text": "regions by region server table in master's table view needs styling  need to add class=\"table\" to the table for region counts per region server on table display page. ",
        "label": 154
    },
    {
        "text": "when hbase regionserver restarts  it says  impossible state for createlease   i restarted a regionserver, and got this error in its logs: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: java.lang.assertionerror: impossible state for createlease(): lease -435227488/-435227488 is still held.  at org.apache.hadoop.hbase.leases.createlease(leases.java:145)  at org.apache.hadoop.hbase.hmaster.regionserverstartup(hmaster.java:1278  )  at sun.reflect.generatedmethodaccessor11.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at org.apache.hadoop.ipc.rpc$server.call(rpc.java:379)  at org.apache.hadoop.ipc.server$handler.run(server.java:596)  at org.apache.hadoop.ipc.client.call(client.java:482)  at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:184)  at $proxy0.regionserverstartup(unknown source)  at org.apache.hadoop.hbase.hregionserver.reportforduty(hregionserver.jav  a:1025)  at org.apache.hadoop.hbase.hregionserver.run(hregionserver.java:659)  at java.lang.thread.run(unknown source) ",
        "label": 314
    },
    {
        "text": "fix flaky test testsnapshotcloneindependence  so the main bug fix here is that if online is set to false, createsnapshotandvalidate disables the original table. so instead of current code doing if (!online) {     trydisable(admin, localtablename); } we should be doing if (!online) {   admin.enabletable(originaltablename);   util.waittableavailable(originaltablename); } ",
        "label": 48
    },
    {
        "text": "fixedfiletrailer should recognize cellcomparatorimpl class in branch x  hbase-18945 has gone into branch-2 .  let's consider rolling upgrade scenario from 1.x to 2.0 where there're three servers: s1, s2, s3 s1 is upgraded to 2.0 first. it flushes to hfile in region r1 with cellcomparatorimpl written in the hfile trailer.  somehow s1 crashes and master assigns r1 to s2 which is still running 1.x  the following code in fixedfiletrailer would be triggered:       try {         comparatorklass = (class<? extends cellcomparator>) class.forname(comparatorclassname);       } catch (classnotfoundexception e) {         throw new ioexception(e);       } since s2 is not aware of cellcomparatorimpl. this issue is to backport cellcomparatorimpl related change to branch-1.x  note: cellcomparatorimpl wouldn't be used in write path, only in read path. ",
        "label": 441
    },
    {
        "text": "port hbase 'race between balancer and disable table can lead to inconsistent cluster' to  ",
        "label": 543
    },
    {
        "text": "make hadoop the default precommit for trunk  here is discussion thread: http://search-hadoop.com/m/ggc1019wdva/making+hadoop+2+the+default+precommit&subj=re+discuss+making+hadoop+2+the+default+precommit+for+trunk+ones+we+get+green+builds jenkins builds have been stable recently: https://builds.apache.org/job/hbase-trunk/  https://builds.apache.org/job/hbase-trunk-on-hadoop-2.0.0/  http://54.241.6.143/job/hbase-trunk-hadoop-2/ we should run test suite against hadoop 2 in precommit build ",
        "label": 441
    },
    {
        "text": "split request accepted   but currently a noop  the \"split\" button from the web ui displays this message and indeed seems to do nothing. ",
        "label": 229
    },
    {
        "text": " region offline  should throw ioexception  not illegalstateexception  it would be nice if i could wrap my htable.get calls in try {} catch (ioexception e). but that doesn't work, since i also need to catch illegalstateexception. i think that any time there is something wrong with hbase, hbase calls should throw an ioexception (or subclass thereof). things like illegalstateexception should be reserved for programmer error. ",
        "label": 86
    },
    {
        "text": "generate changes md and releasenotes md for  ",
        "label": 149
    },
    {
        "text": "region stayed in transition   in closing state  got the following during testing, 1. on a given machine, kill \"rs process id\". then kill \"hmaster process id\".  2. start rs first via \"bin/hbase-daemon.sh --config ./conf start regionserver.\". start hmaster via \"bin/hbase-daemon.sh --config ./conf start master\". one region of a table stayed in closing state. according to zookeeper,  794a6ff17a4de0dd0a19b984ba18eea9 miweng_500region,h\\xb49x\\x10bm\\xb1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. state=closing, ts=wed sep 07 17:21:44 pdt 2011 (75701s ago), server=sea-esxi-0,60000,1315428682281 according to .meta. table, the region has been assigned to from sea-esxi-0 to sea-esxi-4. miweng_500region,h\\xb49x\\x10bm\\xb1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. sea-esxi-4:60030 h\\xb49x\\x10bm\\xb1 i7k\\xc6\\xa7\\xef\\x9d\\x90 0 ",
        "label": 326
    },
    {
        "text": " error  undefined method  message' for nil nilclass  in the shell on error  not sure where this is coming from but since today if i try to create a table that already exists in the shell i get: error: undefined method `message' for nil:nilclass instead of the normal exception. ",
        "label": 441
    },
    {
        "text": "mapreduce fixup  ",
        "label": 38
    },
    {
        "text": "limits the amount of time an edit can live in the memstore   a colleague of mine ran into an interesting issue.  he inserted some data with the wal disabled, which happened to fit in the aggregate memstores memory. two weeks later he a had problem with the hdfs cluster, which caused the region servers to abort. he found that his data was lost. looking at the log we found that the memstores were not flushed at all during these two weeks. should we have an option to flush memstores periodically. there are obvious downsides to this, like many small storefiles, etc. ",
        "label": 139
    },
    {
        "text": "we are calling checkrow  twice in dominibatchmutation   in multi() -> dominibatchmutation() code path, we end up calling checkrow() twice, once from checkbatchop() and once from getrowlock(). see anoop sam john's comments at https://issues.apache.org/jira/browse/hbase-15600?focusedcommentid=15257636&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15257636. ",
        "label": 198
    },
    {
        "text": "hbase needs to have the create permission on the parent of its zookeeper parent znode  upon startup, hbase attempts to create its zookeeper.parent.znode in zookeeper, it does so using zkutil.createandfailsilent which as its name seems to imply will fail silent if the znode exists. but if hbase does not have the create permission on its zookeeper.parent.znode parent znode then the create attempt will fail with a org.apache.zookeeper.keeperexception$noauthexception and will terminate the process. in a production environment where zookeeper has a managed namespace it is not possible to give hbase create permission on the parent of its parent znode. zkutil.createandfailsilent should therefore be modified to check that the znode exists using zookeeper.exist prior to attempting to create it. ",
        "label": 21
    },
    {
        "text": "stuck in regionsintransition because rebalance came in at same time as a split  saw this doing cluster tests: 2010-09-25 21:31:48,212 debug org.apache.hadoop.hbase.master.hmaster: not running balancer because regions in transition: {73781e505e452221c9cd0e03585eb5d1=usertable,user800184056,  128... here's the problem: 2010-09-25 08:16:48,186 info org.apache.hadoop.hbase.master.hmaster: balance hri=usertable,user800184056,1285397376525.73781e505e452221c9cd0e03585eb5d1., src=su184,60020,       1285371621579, dest=sv2borg189,60020,1285371621577 2010-09-25 08:16:48,186 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region usertable,user800184056,1285397376525.                                73781e505e452221c9cd0e03585eb5d1. (offlining) 2010-09-25 08:16:52,656 info org.apache.hadoop.hbase.master.servermanager: received region_split: usertable,user800184056,1285397376525.73781e505e452221c9cd0e03585eb5d1.:            daughters; usertable,user800184056,1285402609029.c05825561e7ea3cc6507c70bfb21541a., usertable,user804024623,1285402609029.28f64903a7875bdafc1e7ee344b225b0. 2010-09-25 08:17:11,414 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out:  usertable,user800184056,1285397376525.                               73781e505e452221c9cd0e03585eb5d1. state=pending_close, ts=1285402608186 ....just as we were doing a balance, the region split. over on rs, i see the split starting up and then in comes the balance 'close' message. by the time the close handler runs on regionserver the split is well underway and close handler actually doesn't find an online region to split. ",
        "label": 314
    },
    {
        "text": "update third party deps and add zoopeeker  ",
        "label": 154
    },
    {
        "text": "htable getmetadata is very inefficient  the current implementation of htable.getmetadata is very inefficient. in order to find the htabledescriptor, it does a full meta scan for all table names. then, it selects the one that matches the table we're looking for. that's a huge number of operations to satisfy a very simple question. to make matters worse, it doesn't even cache the result it gets - subsequent calls will go right back out to a meta scan. at the very least, we should only look up the first meta row for a given take and do it as a random read. we should consider caching the result, or even just grabbing it during table creation (as we will check meta to make sure the table you're opening exists). of course, if we move table descriptors out of meta altogether (maybe into zookeeper?) then this will have to be revisited no matter what. ",
        "label": 241
    },
    {
        "text": "support custom gc options in hbase env sh  when running things like bin/start-hbase and bin/hbase-daemon.sh start [master|regionserver|etc] we end up setting hbase_opts property a couple times via calling hbase-env.sh. this is generally not a problem for most cases, but when you want to set your own gc log properties, one would think you should set hbase_gc_opts, which get added to hbase_opts. nope! that would make too much sense. running bin/hbase-daemons.sh will run bin/hbase-daemon.sh with the daemons it needs to start. each time through hbase-daemon.sh we also call bin/hbase. this isn't a big deal except for each call to hbase-daemon.sh, we also source hbase-env.sh twice (once in the script and once in bin/hbase). this is important for my next point. note that to turn on gc logging, you uncomment: # export hbase_opts=\"$hbase_opts -verbose:gc -xx:+printgcdetails -xx:+printgcdatestamps $hbase_gc_opts\"  and then to log to a gc file for each server, you then uncomment: # export hbase_use_gc_logfile=true in hbase-env.sh on the first pass through hbase-daemon.sh, hbase_gc_opts isn't set, so hbase_opts doesn't get anything funky, but we set hbase_use_gc_logfile, which then sets hbase_gc_opts to the log file (-xloggc:...). then in bin/hbase we again run hbase-env.sh, which now hs hbase_gc_opts set, adding the gc file. this isn't a general problem because hbase_opts is set without prefixing the existing hbase_opts (eg. hbase_opts=\"$hbase_opts ...\"), allowing easy updating. however, gc opts don't work the same and this is really odd behavior when you want to set your own gc opts, which can include turning on gc log rolling (yes, yes, they really are jvm opts, but they ought to support their own param, to help minimize clutter). the simple version of this patch will just add an idempotent gc option to hbase-env.sh and some comments that uncommenting # export hbase_use_gc_logfile=true will lead to a custom gc log file per server (along with an example name), so you don't need to set \"-xloggc\". the more complex solution does the above and also solves the multiple calls to hbase-env.sh so we can be sane about how all this works. note that to fix this, hbase-daemon.sh just needs to read in hbase_use_gc_logfile after sourcing hbase-env.sh and then update hbase_opts. oh and also not source hbase-env.sh in bin/hbase. even further, we might want to consider adding options just for cases where we don't need gc logging - i.e. the shell, the config reading tool, hcbk, etc. this is the hardest version to handle since the first couple will willy-nilly apply the gc options. ",
        "label": 236
    },
    {
        "text": "testhfileoutputformat testmrincrementalloadwithsplit failed due to too many splits and few retries  this one seems worth a dig. we seem to be making progress but here is what we are trying to load which seems weird: 2015-10-01 17:19:41,322 info  [main] mapreduce.loadincrementalhfiles(360): split occured while grouping hfiles, retry attempt 10 with 4 files remaining to group or split 2015-10-01 17:19:41,323 error [main] mapreduce.loadincrementalhfiles(402): ------------------------------------------------- bulk load aborted with some files not yet loaded: -------------------------------------------------   hdfs://localhost:39540/user/jenkins/test-data/720ae36a-2495-456b-ba68-19e260685a35/testlocalmrincrementalload/info-b/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/ce11cbe2490d444d8958264004286aff.bottom   hdfs://localhost:39540/user/jenkins/test-data/720ae36a-2495-456b-ba68-19e260685a35/testlocalmrincrementalload/info-b/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/ce11cbe2490d444d8958264004286aff.top   hdfs://localhost:39540/user/jenkins/test-data/720ae36a-2495-456b-ba68-19e260685a35/testlocalmrincrementalload/info-a/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/30c58eeb23a6464da21117e6e1bc565c.bottom   hdfs://localhost:39540/user/jenkins/test-data/720ae36a-2495-456b-ba68-19e260685a35/testlocalmrincrementalload/info-a/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/_tmp/30c58eeb23a6464da21117e6e1bc565c.top whats that about? making note here. will keep an eye on this one. ",
        "label": 309
    },
    {
        "text": "multiple commit logs per region server  for write dominated workloads, the fact that we have only 1 commit log per server artificially limits the aggregate throughput to that of one disk (this is especially true if we do true fs level syncs on each datanode, or even frequent intermittent fs level syncs as data is being written to each block). we could consider allowing a configurable number of commit logs per server (perhaps something close to or slightly less than the number of disks per server), and we could shard regions on the server, by maybe just a simple modulo scheme, into those commit logs. [a quick way to experiment with the same might be to run multiple region server instances per server; but might be better operationally to package the feature into a single region server.] ",
        "label": 294
    },
    {
        "text": "backport hbase  allow dropping caches behind compactions  to  hbase-14098 adds a new configuration toggle - \"hbase.hfile.drop.behind.compaction\" - which if set to \"true\" tells compactions to drop pages from the os blockcache after write. it's on by default where committed so far but a backport to 0.98 would default it to off. (the backport would also retain compat methods to limitedprivate interface storefilescanner.) what could make it a controversial change in 0.98 is it changes the default setting of 'hbase.regionserver.compaction.private.readers' from \"false\" to \"true\". i think it's fine, we use private readers in production. they're stable and do not present perf issues. ",
        "label": 411
    },
    {
        "text": "postscannerfilterrow consumes a lot of cpu  during an unrelated test i found that when scanning a tall table with cq only and filtering most results at the server, 50% of time is spend in postscannerfilterrow, even though the coprocessor does nothing in that hook. we need to find a way not to call this hook when not needed, or to question why we have this hook at all. i think ramkrishna.s.vasudevan added the hook (or maybe anoop sam john). i am also not sure whether phoenix uses this hook (james taylor?) ",
        "label": 286
    },
    {
        "text": " hbase  thrift put mutaterow methods need to throw illegalargument exceptions  some inputs to these methods can trigger an underlying illegalargumentexception which needs to bubble up to a thrift illegalargument exception ",
        "label": 314
    },
    {
        "text": "client is not getting unknownscannerexceptions  they are being eaten  this was reported by mudphone on irc and confirmed by myself in quick test. if the client takes too long going back to the rs, the rs will throw an unknownscannerexception but it doesn't get back to the client. instead, the client scan silently ends. marking this blocker. its actually in 0.20.4. thats what i was testing. mayhaps an rc sinker? ",
        "label": 229
    },
    {
        "text": "fix hbase unit tests running on hadoop  this is an umbrella issue for fixing unit tests and hbase builds form 0.92+ on top of hadoop 0.23 (currently 0.92/0.94) and hadoop 2.0.x (trunk/0.96). once these are up and passing properly, we'll close out the umbrella issue by adding hbase-trunk-on-hadoop-2 build to the hadoopqa bot. ",
        "label": 248
    },
    {
        "text": "modify tableinputformat splitting algorithm to allow any number of mappers  currently, the number of mappers specified when using tableinputformat is strictly followed if less than total regions on the input table. if greater, the number of regions is used. this will modify the splitting algorithm to do the following: specify 0 mappers when you want # mappers = # regions if you specify fewer mappers than regions, will use exactly the number you specify based on the current algorithm if you specify more mappers than regions, will divide regions up by determining [start,x) [x,end). the number of mappers will always be a multiple of number of regions. this is so we do not have scanners spanning multiple regions. there is an additional issue in that the default number of mappers in jobconf is set to 1. that means if a user does not explicitly set number of map tasks, a single mapper will be used. i'm going to deal with that in a separate jira as the issue currently exists, there are a number of ways to implement this, and it's not required to complete this issue. ",
        "label": 314
    },
    {
        "text": "since hbase  hbase opts cannot be set on the command line  discussed in hbase-7091.  it's not critical, but a little bit surprising, as the comments in bin/hbase doesn't say anything about this. if you create your own hbase-env then it's not an issue... ",
        "label": 411
    },
    {
        "text": "incremental bulk load support for increments  from http://hbase.apache.org/bulk-loads.html: \"the bulk load feature uses a mapreduce job to output table data in hbase's internal data format, and then directly loads the data files into a running cluster. using bulk load will use less cpu and network than going via the hbase api.\" i have been working with a specific implementation of, and can envision, a class of applications that reduce data into a large collection of counters, perhaps building projections of the data in many dimensions in the process. one can use hadoop mapreduce as the engine to accomplish this for a given data set and use loadincrementalhfiles to move the result into place for live serving. mr is natural for summation over very large counter sets: emit counter increments for the data set and projections thereof in mappers, use combiners for partial aggregation, use reducers to do final summation into hfiles. however, it is not possible to then merge in a set of updates to an existing table built in the manner above without either 1) joining the table data and the update set into a large mr temporary set, followed by a complete rewrite of the table; or 2) posting all of the updates as increments via the hbase api, impacting any other concurrent users of the hbase service, and perhaps taking 10-100 times longer than if updates could be computed directly into hfiles like the original import. both of these alternatives are expensive in terms of cpu and time; one is also expensive in terms of disk. i propose adding incremental bulk load support for increments. here is a sketch of a possible implementation: add a kv type for increment modify hfile main, loadincrementalhfiles, and others that work with hfiles directly to handle the new kv type bulk load api can move the files to be merged into the stores as before. implement an alternate compaction algorithm or modify the existing. need to identify increments and apply them to an existing most recent version of a value, or create the value if it does not exist. use keyvalueheap as is to merge value-sets by row as before. for each row, use a kv-keyed map for in memory update of values. if there is an existing value and it is not a serialized long, ignore the increment and log at info level. use the persistent hashmapwrapper from hive's commonjoinoperator, with an appropriate memory limit, so work for overlarge rows will spill to disk. can be local disk, not hdfs. never return an increment kv to a client doing a get or scan. before the merge is complete, if we find an increment kv when searching store files for a value, continue searching back in the store files until we find a put kv for the value, adding up increments as they are encountered, then applying them to the put value; or until search ends, in which case the increment is treated as a put. if there is an existing value and it is not a serialized long, ignore the increment and log at info level. as a beneficial side effect, with increments as just another kv type we can unify put and increment handling. because this is a core concern i'd prefer discussing this as a possible enhancement of core as opposed to a coprocessor-based extension. however it could be possible to implement all but the kv changes within the coprocessor framework. ",
        "label": 38
    },
    {
        "text": "umbrella  miscellaneous improvements from production usage  we use hbase to (mainly) build index for our search engine in alibaba. recently we are upgrading our online cluster from 0.98.12 to 1.x and i'd like to take the opportunity to contribute a bunch of our private patches to community (better late than never, i hope ). this is an umbrella to track this effort. ",
        "label": 504
    },
    {
        "text": "rest tests are broken locally and up in hudson  the restservlet is a singleton. in our rest tests the singleton is carried over between tests that start a new mini cluster each time. a while back we added cleanup of zk when we let go of a connection... whats happen is that the singleton is using a cleaned up zookeeperwatcher ",
        "label": 314
    },
    {
        "text": "document release announcement template  our release docs should include a release announcement template for folks to use (and suggested email lists to send to) ",
        "label": 402
    },
    {
        "text": "add metrics for the number of blocks in cache per cf and block type  we need to know the number of blocks in cache per cf/block type, not just the total size of all blocks of the given type, in order to measure data block encoding efficiency. ",
        "label": 324
    },
    {
        "text": "testmergetool failing in branch and trunk since hbase went in  the hbase-618 fix revealed that testmergetool depends on compactions running. ",
        "label": 314
    },
    {
        "text": "npe in processregionclose because meta is offline kills master and thus the cluster  this issue was born of study done in hbase-2413. the meta went offline and we were processing a region close at the same time. the close processing fell into a npe loop and wouldn't get out of it killing master and effectivly killing the cluster: 2010-03-31 17:50:57,004 info org.apache.hadoop.hbase.master.servermanager: hbasetest020.x.x.x,60020,1270077892989 znode expired 2010-03-31 17:50:57,004 info org.apache.hadoop.hbase.master.regionmanager: meta region removed from onlinemetaregions 2010-03-31 17:51:15,385 info org.apache.hadoop.hbase.master.servermanager: received start message from: hbasetest020.x.x.x,60020,1270083075377 2010-03-31 17:51:15,399 debug org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: updated znode /hbase/rs/1270083075377 with data 10.18.35.215:60020 2010-03-31 17:51:15,870 debug org.apache.hadoop.hbase.master.regionmanager: server is overloaded: load=5, avg=3.0, slop=0.3 2010-03-31 17:51:15,870 debug org.apache.hadoop.hbase.master.regionmanager: choosing to reassign 2 regions. mostloadedregions has 5 regions in it. 2010-03-31 17:51:15,870 debug org.apache.hadoop.hbase.master.regionmanager: going to close region test1,3147000000,1270081876965 2010-03-31 17:51:15,870 debug org.apache.hadoop.hbase.master.regionmanager: going to close region test1,9352000000,1270080893514 2010-03-31 17:51:15,870 info org.apache.hadoop.hbase.master.regionmanager: skipped 0 region(s) that are in transition states 2010-03-31 17:51:15,878 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: test1,3147000000,1270081876965 from hbasetest019.x.x.x,60020,1270082983630; 1 of 2 2010-03-31 17:51:15,879 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: test1,9352000000,1270080893514 from hbasetest019.x.x.x,60020,1270082983630; 2 of 2 2010-03-31 17:51:44,897 debug org.apache.hadoop.hbase.master.processregionclose$1: trying to contact region server for regionname '.meta.,,1', but failed after 10 attempts. exception 1: java.io.ioexception: call to /10.18.35.215:60020 failed on local exception: java.io.eofexceptionexception 1: java.net.connectexception: connection refusedexception 1: java.net.connectexception: connection refusedexception 1: java.net.connectexception: connection refusedexception 1: java.net.connectexception: connection refusedexception 1: java.net.connectexception: connection refusedexception 1: java.net.connectexception: connection refusedexception 1: org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: .meta.,,1         at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:2282)         at org.apache.hadoop.hbase.regionserver.hregionserver.delete(hregionserver.java:1989)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:577)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:915) exception 1: org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: .meta.,,1         at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:2282)         at org.apache.hadoop.hbase.regionserver.hregionserver.delete(hregionserver.java:1989)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:577)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:915) org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: .meta.,,1         at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:2282)         at org.apache.hadoop.hbase.regionserver.hregionserver.delete(hregionserver.java:1989)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:577)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:915)         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)         at java.lang.reflect.constructor.newinstance(constructor.java:513)         at org.apache.hadoop.hbase.remoteexceptionhandler.decoderemoteexception(remoteexceptionhandler.java:94)         at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:74)         at org.apache.hadoop.hbase.master.processregionclose.process(processregionclose.java:63)         at org.apache.hadoop.hbase.master.hmaster.processtodoqueue(hmaster.java:494)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:429) 2010-03-31 17:51:44,899 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of test1,1230000000,1270081673808, false, reassign: true 2010-03-31 17:51:44,899 debug org.apache.hadoop.hbase.master.processregionclose$1: exception in retryablemetaoperation: java.lang.nullpointerexception         at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:64)         at org.apache.hadoop.hbase.master.processregionclose.process(processregionclose.java:63)         at org.apache.hadoop.hbase.master.hmaster.processtodoqueue(hmaster.java:494)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:429) 2010-03-31 17:51:44,900 warn org.apache.hadoop.hbase.master.hmaster: processing pending operations: processregionclose of test1,1230000000,1270081673808, false, reassign: true java.lang.runtimeexception: java.lang.nullpointerexception         at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:96)         at org.apache.hadoop.hbase.master.processregionclose.process(processregionclose.java:63)         at org.apache.hadoop.hbase.master.hmaster.processtodoqueue(hmaster.java:494)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:429) caused by: java.lang.nullpointerexception         at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:64)         ... 3 more ... and so on. marking a blocker for 0.20.5. ",
        "label": 314
    },
    {
        "text": "gracefully shutdown logsyncer  currently, in closing a hlog, logsyncerthread is interrupted. logsyncer could be in the middle to sync the writer. we should avoid interrupting the sync. ",
        "label": 242
    },
    {
        "text": "compactionrequest should not be exposed to user directly  it is an implementation class. and we need to find another to let user know the compaction start and end. ",
        "label": 149
    },
    {
        "text": "javadocs in hba should be clear about which functions are asynchronous and which are synchronous  since we don't name all of our functions as sync/async, we should at least be explicit in javadoc about the behavior of each. ",
        "label": 167
    },
    {
        "text": " num compacting kvs  diverges from  num compacted kvs  over time  i have been running what amounts to an ingestion test for a day or so. this is an all-in-one cluster launched with './bin/hbase master start' from sources. in the rs stats on the master ui, the \"num compacting kvs\" has diverged from \"num compacted kvs\" even though compaction has been completed from perspective of selection, no compaction tasks are running on the rs. i think this could be confusing \u2013 is compaction happening or not? or maybe i'm misunderstanding what this is supposed to show? ",
        "label": 406
    },
    {
        "text": "add an option to presplit table to performanceevaluation  performanceevaluation a quick way to 'benchmark' a hbase cluster. the current 'write*' operations do not pre-split the table. pre splitting the table will really boost the insert performance. it would be nice to have an option to enable pre-splitting table before the inserts begin. it would look something like: (a) hbase ...performanceevaluation --presplit=10 <other options>  (b) hbase ...performanceevaluation --presplit <other options> (b) will try to presplit the table on some default value (say number of region servers) ",
        "label": 429
    },
    {
        "text": "fix and reenable testmasterprocedurewallease  fix and reenable flakey important test. ",
        "label": 433
    },
    {
        "text": "do not oome  throw rowtoobigexception instead  if 10m columns in a row, throw a rowtoobigexception rather than oome when get'ing or scanning w/o in-row scan flag set. ",
        "label": 323
    },
    {
        "text": "revisit handling of backupstate cancelled  during review of hbase-15411, enis made the following point: nobody puts the backup in cancelled state. setcancelled() is not used. so if i abort a backup, who writes to the system table the new state?  not sure whether this is a phase 1 patch issue or due to this patch. we can open a new jira and address it there if you do not want to do it in this patch.  also maybe this should be named aborted rather than cancelled. this issue is to decide whether this state should be kept (e.g. through notification from procedure v2 framework in response to abortion). if it is to be kept, the state should be renamed aborted. ",
        "label": 441
    },
    {
        "text": "parameters are incorrect in procedures jsp  in procedures jps, the parameters of table name, region start end keys are wrong, please see the first picture. this is because all bytes params are encoded in base64. it is confusing. ",
        "label": 500
    },
    {
        "text": "snapshot operation fails with filenotfoundexception  failing to take snapshot due to filenotfoundexception flushsnapshotsubprocedure.regionsnapshottask takes a region level read lock call to hregion#addregiontosnapshot. call to snapshotmanifest#addregion. this gets the current list of store files. race \u2192 file is marked as compacted away and hfilearchiver moves the file to archive under store level lock. snapshotmanifest#addregion visits the stale list of store files one by one. it does a file.getstatus() call to get length of each file. since the file object still points to the original file, file.getstatus() fails with filenotfoundexception. ",
        "label": 55
    },
    {
        "text": "wrong order of null check  in method org.apache.hadoop.hbase.mapreduce.tableinputformatbase.getsplits(jobcontext)  this.table is used before null-throw check. ",
        "label": 291
    },
    {
        "text": "can't use 'close region' when keys are binary  maybe we should allow you to use an encoded name to close a region, or use binary escaping. ",
        "label": 285
    },
    {
        "text": "remove regionmergerequest  /**  * handles processing region merges. put in a queue, owned by hregionserver.  */ // todo:unused: remove!!! @interfaceaudience.private class regionmergerequest implements runnable { hbase-14614 has discharged regionmergerequest from duty. ",
        "label": 226
    },
    {
        "text": "split up core in buck  core in buck is starting to get a little large. if we split it up some then buck cache can do some good things for us. ",
        "label": 154
    },
    {
        "text": "a region's state is kept in several places in the master opening the possibility for race conditions  a region's state exists in multiple maps in the regionmanager: unassignedregions, pendingregions, regionstoclose, closingregions, regionstodelete, etc. one of these race conditions was found in hbase-534. for hbase-0.1.x, we should just patch the holes we find. the ultimate solution (which requires a lot of changes in hmaster) should be applied to hbase trunk. proposed solution: create a class that encapsulates a region's state and provide synchronized access to the class that validates state changes.  there should be a single structure that holds regions in these transitional states and it should be a synchronized collection of some kind. ",
        "label": 241
    },
    {
        "text": "add passing of optional cell blocks over rpc  make it so we can pass cells/data w/o having to bury it all in protobuf to get it over the wire. ",
        "label": 314
    },
    {
        "text": "clarify  web api  in version and compatibility docs  per discussion on hbase-13861, update our version and compatibility section to clarify under operational compatibility that by \"web page api\" we mean the /jmx endpoint. ",
        "label": 330
    },
    {
        "text": "mismatch with config param name in xml and code  in hbase-default.xml there listed one param \"io.storefile.bloom.cacheonwrite\" <property>   <name>io.storefile.bloom.cacheonwrite</name>   <value>false</value>   <description>   enables cache-on-write for inline blocks of a compound bloom filter.   </description> </property> no place in the code this config is getting used.  instead in cacheconfig there is a config param name /**    * configuration key to cache compound bloom filter blocks on write.    */   public static final string cache_bloom_blocks_on_write_key =       \"hfile.block.bloom.cacheonwrite\"; seems issue with entry in the xml file.  we can correct the xml with the config name as hfile.block.bloom.cacheonwrite ? ",
        "label": 46
    },
    {
        "text": "stack overflow in assignmentmanager  my test cluster experienced a switch outage earlier this week which threw the master into a really bad state. in the catch clause of assignmentmanager.assign, we recurse, and if all of the region servers are inaccessible, we do so until we get a stack overflow. ",
        "label": 314
    },
    {
        "text": " compat  mark protected methods of quotasettings that touch protobuf internals as ia private  quotasettings is audience public. the param for setupsetquotarequest moved to package org.apache.hadoop.hbase.shaded.protobuf.generated.masterprotos.setquotarequest. quotasettings was added in 1.1. is quotasettings for use by anyone but our cpep? ",
        "label": 402
    },
    {
        "text": "in ui make host addresses all look the same   not ip sometimes and host at others  ",
        "label": 314
    },
    {
        "text": "typo in familyfilter  i think there's a typo. \"qualifier name\" should read \"column family name\" family filter  this filter takes a compare operator and a comparator. it compares each qualifier name with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that column. ",
        "label": 284
    },
    {
        "text": "performance fixes   profiler driven  while profiling hbase i found a number of slow pieces. here are fixes for them. ",
        "label": 547
    },
    {
        "text": "deploy new hbase logo  deploy new hbase logo on site. ",
        "label": 314
    },
    {
        "text": "mutaterowswithlocks might require updateslock readlock with waittime  mutaterowswithlocks will acquire updateslock.readlock by the following code: ... lock(this.updateslock.readlock(), acquiredrowlocks.size()); ... however, acquiredrowlocks might be empty, and then the waittime of hregion.lock(...) will be set to 0, which will make mutaterowswithlocks fail if can not acquire updateslock.readlock immediately.   in our environment, we implement a region coprocessor which need to hold row locks before invoke mutaterowswithlocks. then, the rowstolock(passed to mutaterowswithlocks) will be an empty set, and we get the following exception occasionally: org.apache.hadoop.hbase.regiontoobusyexception: failed to get a lock in 0ms                                                                                            582   at org.apache.hadoop.hbase.regionserver.hregion.lock(hregion.java:6191)  583   at org.apache.hadoop.hbase.regionserver.hregion.mutaterowswithlocks(hregion.java:5126)  584   at org.apache.hadoop.hbase.regionserver.hregion.mutaterowswithlocks(hregion.java:5034) ... is it reasonable that we use default waittime when rowstolock is empty? (as the following code) lock(this.updateslock.readlock(), acquiredrowlocks.size() == 0 ? 1 : acquiredrowlocks.size()); ",
        "label": 238
    },
    {
        "text": "secure bulk load  design doc: https://cwiki.apache.org/confluence/display/hcatalog/hbase+secure+bulk+load short summary: security as it stands does not cover the bulkloadhfiles() feature. users calling this method will bypass acls. also loading is made more cumbersome in a secure setting because of hdfs privileges. bulkloadhfiles() moves the data from user's directory to the hbase directory, which would require certain write access privileges set. our solution is to create a coprocessor which makes use of authmanager to verify if a user has write access to the table. if so, launches a mr job as the hbase user to do the importing (ie rewrite from text to hfiles). one tricky part this job will have to do is impersonate the calling user when reading the input files. we can do this by expecting the user to pass an hdfs delegation token as part of the securebulkload() coprocessor call and extend an inputformat to make use of that token. the output is written to a temporary directory accessible only by hbase and then bulkloadhfiles() is called. ",
        "label": 174
    },
    {
        "text": "backport hbase to  ",
        "label": 543
    },
    {
        "text": "oome running randomread pe  disable blanket enabling of blockcache   blockcache is misbehaving on trunk. something is broke. we oome about 20% into the randomread test. looking at heap, its all soft references. instrumenting the referencequeue, we're never clearing full gc'ing. something is off. ",
        "label": 314
    },
    {
        "text": "calculate the region servers in default group in foreground  as now we do not need to persist the default rs group, it is fine to calcuate in foreground as it is just some in memory computations. and maybe even we could calculate the server set when we want to use it? the advantage here is that there will be no lag. ",
        "label": 149
    },
    {
        "text": "scanmetrics depends on number of rpc calls to the server   currently, scan metrics is not published in case there is one trip to server. i was testing it on a small row range (200 rows) with a large cache value (1000). it doesn't look right as metrics should not depend on number of rpc calls (number of rpc call is just one metrics fwiw). ",
        "label": 199
    },
    {
        "text": "avoid nested retry loops in hconnectionmanager  while testing client timeouts when the hbase is not available we found that even with aggressive settings, it takes the client 10 minutes or more to finally receive an exception.  part of this is due to nested nested retry loops in locateregion. locateregion will first try to locate the table in meta (which is retried), then it will try to locate the meta table is root (which is also retried).  so for each retry of the meta lookup we retry the root lookup as well. i have have that avoids locateregion retrying if it is called from code that already has a retry loop. ",
        "label": 286
    },
    {
        "text": "accessed from older client throws undeclaredthrowableexception  frustrates rolling upgrade  from the list: after upgrading hbase from 0.20.0 to 0.20.2 i am getting following error message: java.lang.reflect.undeclaredthrowableexception at $proxy2.getregioninfo(unknown source) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locaterootregion(hconnectionmanager.java:931) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:573) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:549) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:623) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:582) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:549) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:623) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:586) at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:549) at org.apache.hadoop.hbase.client.htable.<init>(htable.java:125) at org.apache.hadoop.hbase.client.htable.<init>(htable.java:103) at com.xxxx.orderloader$customerreducer.setup(orderloader.java:81) at org.apache.hadoop.mapreduce.reducer.run(reducer.java:172) at org.apache.hadoop.mapred.reducetask.runnewreducer(reducetask.java:563) at org.apache.hadoop.mapred.reducetask.run(reducetask.java:408) at org.apache.hadoop.mapred.child.main(child.java:170) caused by: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: java.lang.nullpointerexception at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:723) at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:328) ... 17 more this is happening for the following line in my code: this.table = new htable(new hbaseconfiguration(), tablename); this used to work flawlessly before, so seems like something changed in hbaseconfiguration, may be?  i am looking into this, but any pointers would be greatly appreciated. we should figure what we broke. try fix for 0.20.3. in future, as part of release, try accessing cluster with an older client to ensure we've not done something that would frustrate rolling upgrade. ",
        "label": 229
    },
    {
        "text": "atomic increment operations  discussion was already started in hbase-798 first version will contain to additional htable methods: incrementcolumnvalue(table,row,family:column) -> inserts a new version of the specified column (which must already exist) with an incremented cell value. incrementfamilycolumn(table,row,family: ) -> inserts a new column under the specified family. this assumes the family always contains only one column. that column is read, incremented, deleted, and a new column is inserted with the incremented column name. the existing value is re-inserted unchanged to the new column. i'm going to be making even more specialized versions of these for internal use (taking apart byte[]'s, doing different bit/byte operations, and assorted munging...). so if anyone has any requests for other specialized versions, or more generic versions, please post them here and i'd be more than happy to take a stab at them. next week i will likely have a go at hbase-493 ",
        "label": 547
    },
    {
        "text": "make netty engine default in hbase2  hbase-17263 added netty rpc server. this issue is about making it default given it has seen good service across two singles-days at scale. netty handles the scenario seen in hbase-19320 (see tail of hbase-19320 for suggestion to netty the default) ",
        "label": 314
    },
    {
        "text": "small typo on acid documentation page  i noticed a couple of occurrences of the \"word\" wholely on the acid semantics doc page (https://hbase.apache.org/acid-semantics.html) this should be \"wholly\". ",
        "label": 112
    },
    {
        "text": "remove the unneccesary info cf deletion in deletetableprocedure deletefrommeta  i saw the deletefrommeta() method in deletetableprocedure & truncatetableprocedure: protected static void deletefrommeta(final masterprocedureenv env,     final tablename tablename, list<regioninfo> regions) throws ioexception {   metatableaccessor.deleteregions(env.getmasterservices().getconnection(), regions);   // clean any remaining rows for this table.   cleananyremainingrows(env, tablename);   // clean region references from the server manager   env.getmasterservices().getservermanager().removeregions(regions);   // clear favored nodes for this table   favorednodesmanager fnm = env.getmasterservices().getfavorednodesmanager();   if (fnm != null) {     fnm.deletefavorednodesforregions(regions);   } } 1. deleteregions : delete the info column family first;  2. cleananyremainingrows : delete the whole row for regions then. in fact, we can only change to delete the whole row, no need the extra info cf deletion any more. ",
        "label": 514
    },
    {
        "text": "decouple split transaction from zookeeper  as part of hbase-10296 splittransaction should be decoupled from zookeeper.   this is an initial patch for review. at the moment the consensus provider placed directly to splittransaction to minimize affected code. in the ideal world it should be done in hserver. ",
        "label": 407
    },
    {
        "text": "zkasyncregistry ctor would hang when zookeeper cluster is not available  from https://builds.apache.org/job/hbase-flaky-tests/23477/testreport/junit/org.apache.hadoop.hbase.client/testadmin2/testcheckhbaseavailablewithoutcluster/ : org.junit.runners.model.testtimedoutexception: test timed out after 300000 milliseconds at org.apache.hadoop.hbase.client.testadmin2.testcheckhbaseavailablewithoutcluster(testadmin2.java:573) it seems this started hanging after hbase-19313 ",
        "label": 190
    },
    {
        "text": "release  ",
        "label": 187
    },
    {
        "text": "npe in org apache hadoop hbase ipc rpcserver connection readandprocess  2015-07-02 09:49:32,028 warn  [.reader=4,port=60020] ipc.rpcserver - rpcserver.listener,port=60020: count of bytes read: 0 java.lang.nullpointerexception     at org.apache.hadoop.hbase.ipc.rpcserver$connection.readandprocess(rpcserver.java:1479)     at org.apache.hadoop.hbase.ipc.rpcserver$listener.doread(rpcserver.java:854)     at org.apache.hadoop.hbase.ipc.rpcserver$listener$reader.dorunloop(rpcserver.java:645)     at org.apache.hadoop.hbase.ipc.rpcserver$listener$reader.run(rpcserver.java:620)     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) ",
        "label": 38
    },
    {
        "text": "deadlock when rs tries to rpc to itself inside splittransaction  in testing the 0.89.20100830 rc, i ran into a deadlock with the following situation: all of the ipc handler threads are blocked on the region lock, which is held by compactsplitthread. compactsplitthread is in the process of trying to edit meta to create the offline parent. meta happens to be on the same server as is executing the split. therefore, the compactsplitthread is trying to connect back to itself, but all of the handler threads are blocked, so the ipc never happens. thus, the entire rs gets deadlocked. ",
        "label": 453
    },
    {
        "text": "blocked wal archive   logroller  failed to schedule flush of xxxx  because it is not online on us   on a heavily loaded cluster, wal count keeps rising and we can get into a state where we are not rolling the logs off fast enough. in particular, there is this interesting state at the extreme where we pick a region to flush because 'too many wals' but the region is actually not online. as the wal count rises, we keep picking a region-to-flush that is no longer on the server. this condition blocks our being able to clear wals; eventually wals climb into the hundreds and the rs goes zombie with a full call queue that starts throwing callqueuetoolargeexceptions (bad if this servers is the one carrying hbase:meta): i.e. clients fail to access the regionserver. one symptom is a fast spike in wal count for the rs. a restart of the rs will break the bind. here is how it looks in the log: # here is region closing.... 2019-10-16 23:10:55,897 info org.apache.hadoop.hbase.regionserver.handler.unassignregionhandler: closed 8ee433ad59526778c53cc85ed3762d0b .... # then soon after ... 2019-10-16 23:11:44,041 warn org.apache.hadoop.hbase.regionserver.logroller: failed to schedule flush of 8ee433ad59526778c53cc85ed3762d0b, because it is not online on us 2019-10-16 23:11:45,006 info org.apache.hadoop.hbase.regionserver.wal.abstractfswal: too many wals; count=45, max=32; forcing flush of 1 regions(s): 8ee433ad59526778c53cc85ed3762d0b ... # later... 2019-10-16 23:20:25,427 info org.apache.hadoop.hbase.regionserver.wal.abstractfswal: too many wals; count=542, max=32; forcing flush of 1 regions(s): 8ee433ad59526778c53cc85ed3762d0b 2019-10-16 23:20:25,427 warn org.apache.hadoop.hbase.regionserver.logroller: failed to schedule flush of 8ee433ad59526778c53cc85ed3762d0b, because it is not online on us i've seen this runaway wals 2.2.1. i've seen runaway wals in a 1.2.x version regularly that had hbase-16721 fix in it, but can't say yet if it was for same reason as above. ",
        "label": 149
    },
    {
        "text": " fb  fix race condition between append sync and hlog close  work flow   the regionserver was performing a shutdown as it got youaredeadexception. it closed the hlog and was waiting to close all the regions where it got stuck. the reason region close is stuck because it's trying to obtain a lock which is occupied by the put op, which in turn is waiting for append to hlog to complete. there is race condition here is between append/sync and hlog.close() work flow. scenario: thread 1 => doing the append  thread 2 => doing hregionserver shutdown timeline: t1> 1: verifies that logsyncer is not shutting down and hlog is not closed and calls sync()  t2> 2: hregionserver issued hlog.close()  t3> 2: in hlog.close(), it joins the logsyncer thread, which signals all the threads waiting on syncdone and exits.  t4> 1: in sync, it sees that the sync has not complete until its txd, hence adds itself to the syncdone.await queue. note: at t4, it does not check whether the logsyncer thread is alive or not, which caused this hang. ",
        "label": 378
    },
    {
        "text": "it's possible for regions to not be major compacted for more than hbase hregion majorcompaction  this is something i've seen here since we upgraded to 0.89 since gets are now scans, although i don't currently have any strong evidence that it wasn't happening in 0.20. i saw this when we began getting alerts on the frontend that some requests were taking more than 8 seconds to complete. even getting a value could take more than 3 minutes in the shell. the first thing i did was major compacting the table that was slow and the problem went away immediately. looking in the logs, it seems the compaction transformed 2 files of (total) 550mb into 5.2mb. looking back in the logs for september, it appears that that table was never major compacted and was slowly growing everyday. some more grepping around showed that quite a few regions were never major compacted. i'm still looking at the code, but the issue seems to be that the minor compactions are always happening on all store files more than once per day on certain regions, meaning that the oldest timestamp is always smaller than hbase.hregion.majorcompaction and major compactions are never triggered. ",
        "label": 247
    },
    {
        "text": "log more details when a scanner lease expires  the message logged by the regionserver when a scanner lease expires isn't as useful as it could be. scanner 4765412385779771089 lease expired - most clients don't log their scanner id, so it's really hard to figure out what was going on. i think it would be useful to at least log the name of the region on which the scanner was open, and it would be great to have the ip:port of the client that had that lease too. ",
        "label": 124
    },
    {
        "text": "tablesnapshotregionsplit should be public  this class extends writable and so should be public so it can be used outside of the existing code line we ship. this will be consistent with tablesplit, which is also public. ",
        "label": 339
    },
    {
        "text": "npe while splitting table with empty column family store  i did a simple test on trunk where i create a table (after wiping the local /tmp/hbase-<username>): hbase(main):001:0> create 'testtable', 'cf1', 'cf2'                                                                                                   then i inserted 17k rows: hbase(main):002:0> for i in 'a'..'z' do for j in 'a'..'z' do for k in 'a'..'z' do put 'testtable', \"row-#{i}#{j}#{k}\", \"cf1:#{k}\", \"#{k}\" end end end and called hbase(main):003:0> split 'testttable' and the logs gave this npe and no split was performed: 2011-01-31 10:06:38,534 debug org.apache.hadoop.hbase.regionserver.hregion: started memstore flush for testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c., current region memstore size 3.5m 2011-01-31 10:06:38,575 debug org.apache.hadoop.hbase.regionserver.hregion: finished snapshotting, commencing flushing stores 2011-01-31 10:06:38,856 info org.apache.hadoop.hbase.regionserver.store: renaming flushed file at file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/.tmp/5265602271926296451 to file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/cf1/5349044325262044918 2011-01-31 10:06:38,861 info org.apache.hadoop.hbase.regionserver.store: added file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/cf1/5349044325262044918, entries=17576, sequenceid=17588, memsize=3.5m, filesize=549.9k 2011-01-31 10:06:38,863 info org.apache.hadoop.hbase.regionserver.hregion: finished memstore flush of ~3.5m for region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. in 328ms, sequenceid=17588, compaction requested=false 2011-01-31 10:06:38,869 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. 2011-01-31 10:06:38,870 info org.apache.hadoop.hbase.regionserver.hregion: aborted compaction on region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. after 0sec 2011-01-31 10:06:38,872 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.store.checksplit(store.java:1367)         at org.apache.hadoop.hbase.regionserver.store.compact(store.java:633)         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:793)         at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:81) 2011-01-31 10:06:38,873 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. because user-triggered split; priority=1, compaction queue size=0 i added a row with data in cf2: hbase(main):005:0> put 'testtable', 'row1', 'cf2', 'test1'                                                                                            and the tried to split the table again like above and now it worked. ",
        "label": 285
    },
    {
        "text": "split hbase dev mailing list into hbase dev issues   hadoop did a split similar to this recently. i think splitting the mailing lists will make things much easier to manage for our developers. ",
        "label": 314
    },
    {
        "text": "implement regionserver group based balancer  re-purposing this jira after the discussion last week. ",
        "label": 462
    },
    {
        "text": "potential thread creation in metascanner metascan  metascanner.metascan() creates an instance of htable per call. the constructur used creates a new threadpoolexecutor. the executor itself will not create a thread unless it's pool is used. i am not sure if the htable instance in question ever uses it's pool. but if so, this could become a big performance issue. logging an issue at lars's request. mail list chain below. ------------------------------------- indeed. that is bad.  i cannot see a clean fix immediately, but we need to look at this. mind filing a ticket, kireet? \u2013 lars ________________________________  from: kireet <kireet-teh5dpvpl8nqt0dzr+alfa@public.gmane.org>  to: public-user-50pas4ewwpeyzmrdd/iqwq-wofgn7rls/m9smdsby/kfg@public.gmane.org   sent: friday, may 31, 2013 11:58 am  subject: re: hconnectionmanager$hconnectionimplementation.locateregioninmeta even if i initiate the call via a pooled htable, the metascanner seems   to use a concrete htable instance. the constructor invoked seems to   create a java threadpoolexecutor. i am not 100% sure but i think as long   as nothing is submitted to the threadpoolexecutor it won't create any   threads. i just wanted to confirm this was the case. i do see the   connection is shared. --kireet on 5/30/13 7:38 pm, ted yu wrote:  > htablepool$**pooledhtable is a wrapper around htable.  >  > here is how htable obtains a connection:  >  > public htable(configuration conf, final byte[] tablename, final  > executorservice pool)  > throws ioexception {  > this.connection = hconnectionmanager.getconnection(conf);  >  > meaning the connection is a shared one based on certain key/value pairs  > from conf.  >  > bq. so every call to batch will create a new thread?  >  > i don't think so.  >  > on thu, may 30, 2013 at 11:28 am, kireet <kireet-teh5dpvpl8nqt0dzr+alfa-xmd5yjdbdmrexy1tmh2ibg@public.gmane.org> wrote:  >  >>  >>  >> thanks, will give it a shot. so i should download 0.94.7 (latest stable)  >> and run the patch tool on top with the backport? this is a little new to me.  >>  >> also, i was looking at the stack below. from my reading of the code, the  >> htable.batch() call will always cause the prefetch call to occur, which  >> will cause a new htable object to get created. the constructor used in  >> creating a new thread pool. so every call to batch will create a new  >> thread? or the htable's thread pool never gets used as the pool is only  >> used for writes? i think i am missing something but just want to confirm.  >>  >> thanks  >> kireet ",
        "label": 286
    },
    {
        "text": "hbase broke testatomicincrement  fix and reenable  ",
        "label": 167
    },
    {
        "text": "hbase on hadoop with local short circuit reads  ssr  causes oom  we've run into an issue with hbase 0.94 on hadoop2, with ssr turned on that the memory usage of the hbase process grows to 7g, on an -xmx3g, after some time, this causes oom for the rss. upon further investigation, i've found out that we end up with 200 regions, each having 3-4 store files open. under hadoop2 ssr, blockreaderlocal allocates directbuffers, which is unlike hdfs 1 where there is no direct buffer allocation. it seems that there is no guards against the memory used by local buffers in hdfs 2, and having a large number of open files causes multiple gb of memory to be consumed from the rs process. this issue is to further investigate what is going on. whether we can limit the memory usage in hdfs, or hbase, and/or document the setup. possible mitigation scenarios are: turn off ssr for hadoop 2 ensure that there is enough unallocated memory for the rs based on expected # of store files ensure that there is lower number of regions per region server (hence number of open files) stack trace: org.apache.hadoop.hbase.droppedsnapshotexception: region: integrationtestloadandverify,yc^p\\xd7\\x945\\xd4,1363388517630.24655343d8d356ef708732f34cfe8946.         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1560)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1439)         at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:1380)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushregion(memstoreflusher.java:449)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.flushoneforglobalpressure(memstoreflusher.java:215)         at org.apache.hadoop.hbase.regionserver.memstoreflusher.access$500(memstoreflusher.java:63)         at org.apache.hadoop.hbase.regionserver.memstoreflusher$flushhandler.run(memstoreflusher.java:237)         at java.lang.thread.run(thread.java:662) caused by: java.lang.outofmemoryerror: direct buffer memory         at java.nio.bits.reservememory(bits.java:632)         at java.nio.directbytebuffer.<init>(directbytebuffer.java:97)         at java.nio.bytebuffer.allocatedirect(bytebuffer.java:288)         at org.apache.hadoop.hdfs.util.directbufferpool.getbuffer(directbufferpool.java:70)         at org.apache.hadoop.hdfs.blockreaderlocal.<init>(blockreaderlocal.java:315)         at org.apache.hadoop.hdfs.blockreaderlocal.newblockreader(blockreaderlocal.java:208)         at org.apache.hadoop.hdfs.dfsclient.getlocalblockreader(dfsclient.java:790)         at org.apache.hadoop.hdfs.dfsinputstream.getblockreader(dfsinputstream.java:888)         at org.apache.hadoop.hdfs.dfsinputstream.blockseekto(dfsinputstream.java:455)         at org.apache.hadoop.hdfs.dfsinputstream.readwithstrategy(dfsinputstream.java:645)         at org.apache.hadoop.hdfs.dfsinputstream.read(dfsinputstream.java:689)         at java.io.datainputstream.readfully(datainputstream.java:178)         at org.apache.hadoop.hbase.io.hfile.fixedfiletrailer.readfromstream(fixedfiletrailer.java:312)         at org.apache.hadoop.hbase.io.hfile.hfile.pickreaderversion(hfile.java:543)         at org.apache.hadoop.hbase.io.hfile.hfile.createreaderwithencoding(hfile.java:589)         at org.apache.hadoop.hbase.regionserver.storefile$reader.<init>(storefile.java:1261)         at org.apache.hadoop.hbase.regionserver.storefile.open(storefile.java:512)         at org.apache.hadoop.hbase.regionserver.storefile.createreader(storefile.java:603)         at org.apache.hadoop.hbase.regionserver.store.validatestorefile(store.java:1568)         at org.apache.hadoop.hbase.regionserver.store.commitfile(store.java:845)         at org.apache.hadoop.hbase.regionserver.store.access$500(store.java:109)         at org.apache.hadoop.hbase.regionserver.store$storeflusherimpl.commit(store.java:2209)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:1541) ",
        "label": 314
    },
    {
        "text": "improve thread safety of htablepool  there are some operations in htablepool accessing poolmap in multiple places without any explicit synchronization. for example htablepool.closetablepool() calls poolmap.values(), and calls poolmap.remove(). if other threads add new instances to the pool in the middle of the calls, the newly added instances might be dropped. (htablepool.closetablepool() also has another problem that calling it by multiple threads causes accessing htable by multiple threads.) moreover, poolmap is not thread safe for the same reason. for example poolmap.put() calles concurrentmap.get() and calles concurrentmap.put(). if other threads add a new instance to the concurent map in the middle of the calls, the new instance might be dropped. and also implementations of pool have the same problems. ",
        "label": 200
    },
    {
        "text": "pe is broken when other tables exist  the iteration in checktable is broken.       for (int i = 0; i < extanttables.length; i++) {         if (extanttables[0].equals(tabledescriptor)) {           log.warn(\"table \" + tabledescriptor + \" already exists\");           tableexists = true;           break;         }       } ",
        "label": 229
    },
    {
        "text": "make compaction policy pluggable  currently, the compaction selection is pluggable. it will be great to make the compaction algorithm pluggable too so that we can implement and play with other compaction algorithms. ",
        "label": 242
    },
    {
        "text": "htableinterface append operation may overwrites values  i use hbase java api and i try to append values bytes.tobytes(\"one two\") and bytes.tobytes(\" three\") in 3 columns.  only for 2 out of these 3 columns the result is \"one two three\". output from the hbase shell:   hbase(main):008:0* scan \"mytesttable\" row                                    column+cell                                                                                                      mytestrowkey                          column=testa:dlbytes, timestamp=1375436156140, value=one two three                                               mytestrowkey                          column=testa:tbytes, timestamp=1375436156140, value=one two three                                                mytestrowkey                          column=testa:ulbytes, timestamp=1375436156140, value= three                                                     1 row(s) in 0.0280 seconds my test code: database.java import static org.junit.assert.*; import java.io.ioexception;   import org.apache.hadoop.conf.configuration; import org.apache.hadoop.hbase.hbaseconfiguration; import org.apache.hadoop.hbase.hcolumndescriptor; import org.apache.hadoop.hbase.htabledescriptor; import org.apache.hadoop.hbase.client.hbaseadmin; import org.apache.hadoop.hbase.client.htableinterface; import org.apache.hadoop.hbase.client.htablepool; import org.apache.hadoop.hbase.client.append; import org.apache.hadoop.hbase.client.result; import org.apache.hadoop.hbase.util.bytes; import org.junit.test; ...     @test     public void testappend() throws ioexception {         byte [] rowkey = bytes.tobytes(\"mytestrowkey\");         byte [] column1 = bytes.tobytes(\"ulbytes\");         byte [] column2 = bytes.tobytes(\"dlbytes\");         byte [] column3 = bytes.tobytes(\"tbytes\");         string part11 = \"one two\";         string part12 = \" three\";         string cfamily = \"testa\";         string table = \"mytesttable\";         configuration conf = hbaseconfiguration.create();         htablepool pool = new htablepool(conf, 10);         hbaseadmin admin = new hbaseadmin(conf);                  if(admin.tableexists(table)){             admin.disabletable(table);             admin.deletetable(table);         }                  htabledescriptor tabledescriptor = new htabledescriptor(table);         hcolumndescriptor hcd = new hcolumndescriptor(cfamily);         hcd.setmaxversions(1);         tabledescriptor.addfamily(hcd);         admin.createtable(tabledescriptor);         htableinterface table = pool.gettable(table);                  append a = new append(rowkey);         a.setreturnresults(false);         a.add(bytes.tobytes(cfamily), column1, bytes.tobytes(part11));         a.add(bytes.tobytes(cfamily), column2, bytes.tobytes(part11));         a.add(bytes.tobytes(cfamily), column3, bytes.tobytes(part11));         table.append(a);         a = new append(rowkey);         a.add(bytes.tobytes(cfamily), column1, bytes.tobytes(part12));         a.add(bytes.tobytes(cfamily), column2, bytes.tobytes(part12));         a.add(bytes.tobytes(cfamily), column3, bytes.tobytes(part12));         result result = table.append(a);         byte [] resultforcolumn1 = result.getvalue(bytes.tobytes(cfamily), column1);         byte [] resultforcolumn2 = result.getvalue(bytes.tobytes(cfamily), column2);         byte [] resultforcolumn3 = result.getvalue(bytes.tobytes(cfamily), column3);         if (resultforcolumn1 == null || resultforcolumn2 == null || resultforcolumn3 == null)             system.out.println(\"the db table contains these values but they are never given back, strange...\");         else {             assertequals(0, bytes.compareto(bytes.tobytes(part11 + part12),                     resultforcolumn1));             assertequals(0, bytes.compareto(bytes.tobytes(part11 + part12),                     resultforcolumn2));             assertequals(0, bytes.compareto(bytes.tobytes(part11 + part12),                     resultforcolumn3));         }         htable t = new htable(conf, table);            get getoperation = new get(rowkey);         getoperation.addcolumn(bytes.tobytes(cfamily), column1);         result res = t.get(getoperation);                    assertequals(0, bytes.compareto(bytes.tobytes(part11 + part12), res.getvalue(bytes.tobytes(cfamily), column1)));       } ",
        "label": 441
    },
    {
        "text": "cleanerchore logs too much  so much so it obscures all else that is going on  testing 0.94.3rc0, i see loads of this in logs: 2012-11-23 13:39:40,488 debug org.apache.hadoop.hbase.master.cleaner.cleanerchore: file:/tmp/hbase-stack/hbase/.oldlogs/192.168.1.73%2c61033%2c1353705880078.1353706219757 is not deletable according t#  14 2012-11-23 13:39:40,488 debug org.apache.hadoop.hbase.master.cleaner.cleanerchore: file:/tmp/hbase-stack/hbase/.oldlogs/192.168.1.73%2c61033%2c1353705880078.1353706221210 is not deletable according t#  1 there is too much of it. can we cut back some? ",
        "label": 236
    },
    {
        "text": "update pom plugins  a bunch are old. lets update. [~balazs.meszaros] you want to have a go at this sir? ",
        "label": 352
    },
    {
        "text": "fix build break   ec2  because all contrib/** reuses build-contrib.xml -, internally they reuse ivy-retrieve.xml and hence need the presence of an ivy.xml in ec2 directory to succeed. temporary patch with no dependencies in . ideal patch should be to refactor build*.xml as appropriate. ",
        "label": 266
    },
    {
        "text": "multiget methods in thrift  the thrift api does not expose multi-get operations. this patch adds the meyhods getrows, getrowswithcolumns, getrowsts and getrowswithcolumnsts. ",
        "label": 304
    },
    {
        "text": "hbase needs to create baseznode recursively  in deploy env, multi small hbase clusters may share a same zk cluster. so, for hbase cluster1, its parent znode is /hbase/cluster1. but in hbase version 0.94.1, hbase use zkutil.createandfailsilent(this, baseznode) to create parent path and it will throw a nonode exception if znode /hbase donot exist.  we want to change it to zkutil.createwithparents(this, baseznode); to suport create baseznode recursivly. the nonode exception is: java.lang.runtimeexception: failed construction of master: class org.apache.hadoop.hbase.master.hmaster  at org.apache.hadoop.hbase.master.hmaster.constructmaster(hmaster.java:1792)  at org.apache.hadoop.hbase.master.hmastercommandline.startmaster(hmastercommandline.java:146)  at org.apache.hadoop.hbase.master.hmastercommandline.run(hmastercommandline.java:103)  at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)  at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:77)  at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:1806)  caused by: org.apache.zookeeper.keeperexception$nonodeexception: keepererrorcode = nonode for /hbase/cluster1  at org.apache.zookeeper.keeperexception.create(keeperexception.java:111)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)  at org.apache.zookeeper.zookeeper.create(zookeeper.java:778)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.createnonsequential(recoverablezookeeper.java:420)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.create(recoverablezookeeper.java:402)  at org.apache.hadoop.hbase.zookeeper.zkutil.createandfailsilent(zkutil.java:905)  at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.createbaseznodes(zookeeperwatcher.java:166)  at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.<init>(zookeeperwatcher.java:159)  at org.apache.hadoop.hbase.master.hmaster.<init>(hmaster.java:282)  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)  at java.lang.reflect.constructor.newinstance(constructor.java:513)  at org.apache.hadoop.hbase.master.h ",
        "label": 411
    },
    {
        "text": "creating table may throw nullpointerexception  it happened in latest branch 0.90. but i can't reproduce it.  >  > it seems using api gethregioninfoornull is better or check the input parameter before call gethregioninfo.  >  > code:  > public static writable getwritable(final byte [] bytes, final   > writable w)  > throws ioexception { > return getwritable(bytes, 0, bytes.length, w); > } > return getwritable(bytes, 0, bytes.length, w); // it seems input   > parameter bytes is null  >  > logs:  > 11/07/15 10:15:42 info zookeeper.clientcnxn: socket connection   > established to c4c3.site/157.5.100.3:2181, initiating session  > 11/07/15 10:15:42 info zookeeper.clientcnxn: session establishment   > complete on server c4c3.site/157.5.100.3:2181, sessionid = 0x2312b8e3f700002, negotiated timeout = 180000 [info] create : ufdr111 222!  > [info] create : ufdr111 start!  > java.lang.nullpointerexception  > at   > org.apache.hadoop.hbase.util.writables.getwritable(writables.java:75)  > at   > org.apache.hadoop.hbase.util.writables.gethregioninfo(writables.java:1  > 19)  > at   > org.apache.hadoop.hbase.client.hbaseadmin$1.processrow(hbaseadmin.java  > :306)  > at   > org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:1  > 90)  > at   > org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:9  > 5)  > at   > org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:7  > 3)  > at   > org.apache.hadoop.hbase.client.hbaseadmin.createtable(hbaseadmin.java:  > 325)  > at createtable.main(createtable.java:96)  > ",
        "label": 529
    },
    {
        "text": " ec2  support multivolume local instance storage  the (old) hadoop ec2 scripts we used as a basis for our scripts do not properly handle instance types that have their local storage attached as multiple volumes. the ami build script uses amazon's latest fedora 8 ami as a base, which does not automatically mount instance storage on multiple volumes either. we recommend use of two instance types: c1.medium for zookeeper, which has one vdisk of 340 gb as /dev/sdb mounted on /mnt by the base image c1.xlarge for master and slaves, which have four vdisks of 420 gb as /dev/sd[bcde], only one of which is mounted on /mnt by the base image additionally, the m1.xlarge instance type, which a user might use anyway, has two vdisks of 420 gb as /dev/sd[bc], only one of which is mounted on /mnt by the base image. the hbase-ec2-init-remote.sh script should probe for all available instance storage devices, mount them, and update the datanode configuration appropriately. ",
        "label": 38
    },
    {
        "text": "testregionserverexit broken  testregionserverexit has a couple of problems: 1. region server tries to start http server on a port already in use:  [junit] 2008-02-07 07:01:06,529 fatal [regionserver:2] hbase.hregionserver(867): failed init  [junit] java.io.ioexception: problem starting http server  [junit] at org.apache.hadoop.hbase.util.infoserver.start(infoserver.java:227)  [junit] at org.apache.hadoop.hbase.hregionserver.startservicethreads(hregionserver.java:928)  [junit] at org.apache.hadoop.hbase.hregionserver.init(hregionserver.java:863)  [junit] at org.apache.hadoop.hbase.hregionserver.run(hregionserver.java:633)  [junit] at java.lang.thread.run(thread.java:595)  [junit] caused by: org.mortbay.util.multiexception[java.net.bindexception: address already in use]  [junit] at org.mortbay.http.httpserver.dostart(httpserver.java:731)  [junit] at org.mortbay.util.container.start(container.java:72)  [junit] at org.apache.hadoop.hbase.util.infoserver.start(infoserver.java:205)  [junit] ... 4 more  [junit] 2008-02-07 07:01:06,530 fatal [regionserver:2] hbase.hregionserver(772): unhandled exception. aborting... the region server that died apparently was serving the root region. the test case apparently has a long timeout for finding the root region because you see a lot of  [junit] 2008-02-07 07:04:14,813 debug [thread-540] hbase.hconnectionmanager$tableservers(708): wake. retry finding root region.  [junit] 2008-02-07 07:04:14,814 debug [thread-540] hbase.hconnectionmanager$tableservers(704): sleeping. waiting for root region.  [junit] 2008-02-07 07:04:24,823 debug [thread-540] hbase.hconnectionmanager$tableservers(708): wake. retry finding root region.  [junit] 2008-02-07 07:04:24,827 debug [thread-540] hbase.hconnectionmanager$tableservers(704): sleeping. waiting for root region.  [junit] 2008-02-07 07:04:34,833 debug [thread-540] hbase.hconnectionmanager$tableservers(708): wake. retry finding root region.  [junit] 2008-02-07 07:04:34,836 debug [thread-540] hbase.hconnectionmanager$tableservers(704): sleeping. waiting for root region.  [junit] 2008-02-07 07:04:44,842 debug [thread-540] hbase.hconnectionmanager$tableservers(708): wake. retry finding root region. until finally the client gives up:  [junit] 2008-02-07 07:04:44,843 fatal [thread-540] hbase.testregionserverexit$1(161): could not re-open meta table because  [junit] org.apache.hadoop.hbase.noserverforregionexception: timed out trying to locate root region  [junit] at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locaterootregion(hconnectionmanager.java:718)  [junit] at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:329)  [junit] at org.apache.hadoop.hbase.hconnectionmanager$tableservers.relocateregion(hconnectionmanager.java:311)  [junit] at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:476)  [junit] at org.apache.hadoop.hbase.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:339)  [junit] at org.apache.hadoop.hbase.hconnectionmanager$tableservers.relocateregion(hconnectionmanager.java:311)  [junit] at org.apache.hadoop.hbase.htable.getregionlocation(htable.java:114)  [junit] at org.apache.hadoop.hbase.htable$clientscanner.nextscanner(htable.java:889)  [junit] at org.apache.hadoop.hbase.htable$clientscanner.<init>(htable.java:817)  [junit] at org.apache.hadoop.hbase.htable.obtainscanner(htable.java:522)  [junit] at org.apache.hadoop.hbase.htable.obtainscanner(htable.java:411)  [junit] at org.apache.hadoop.hbase.testregionserverexit$1.run(testregionserverexit.java:156)  [junit] at java.lang.thread.run(thread.java:595)  [junit] exception in thread \"thread-540\" junit.framework.assertionfailederror  [junit] at junit.framework.assert.fail(assert.java:47)  [junit] at junit.framework.assert.fail(assert.java:53)  [junit] at org.apache.hadoop.hbase.testregionserverexit$1.run(testregionserverexit.java:162)  [junit] at java.lang.thread.run(thread.java:595) which is not the way the test is supposed to run at all.  it appears that when we start multiple region servers in a minihbasecluster, they all try to start their http server on the same port. in the past i believe that the http server start failure was not fatal, so the test ran. we should either have some kind of setting for minihbasecluster that tells the master and region servers not to start their http servers, or some way of telling multiple servers not to start on the same port, or making http startup failure non-fatal. tests like these are good as they (eventually) point out a regression to us. ",
        "label": 241
    },
    {
        "text": "hbase rpc should send size of response  the rpc reply from server->client does not include the size of the payload, it is framed like so:  <i32> callid  <byte> errorflag  <byte[]> data the data segment would contain enough info about how big the response is so that it could be decoded by a writable reader. this makes it difficult to write buffering clients, who might read the entire 'data' then pass it to a decoder. while less memory efficient, if you want to easily write block read clients (eg: nio) it would be necessary to send the size along so that the client could snarf into a local buf. the new proposal is:  <i32> callid  <i32> size  <byte> errorflag  <byte[]> data the size being sizeof(data) + sizeof(errorflag). ",
        "label": 314
    },
    {
        "text": "multiincrement multiappend  multiget functionality for increments and appends   hbase-1845 introduced multiget and other cross-row/cross-region batch operations. we should add a way to do that with increments. ",
        "label": 286
    },
    {
        "text": "polish the wal interface after hbase  we have a closeregion flag which seems to be redundant with the marker waledit. ",
        "label": 314
    },
    {
        "text": "should a failure in creating an unassigned node abort the master   in assignmentmanager's createunassignedasynccallback, we have the following condition: if (rc != 0) {         // thisis resultcode.  if non-zero, need to resubmit.         log.warn(\"rc != 0 for \" + path + \" -- retryable connectionloss -- \" +           \"fix see http://wiki.apache.org/hadoop/zookeeper/faq#a2\");         this.zkw.abort(\"connectionloss writing unassigned at \" + path +           \", rc=\" + rc, null);         return; } while a similar structure inside existsunassignedasynccallback (which the above is linked to), does not have such a force abort. do we really require the abort statement here, or can we make do without? ",
        "label": 194
    },
    {
        "text": "softvaluesortedmap is broken  can generate npes  the way softvaluesortedmap is using softvalues, it looks like that it's able to get it's keys garbage collected along with the values themselves. we got this issue in production but i was also able to randomly generate it using ycsb with 300 threads. here's an example on 0.20 with jdk 1.6u14: java.lang.nullpointerexception         at org.apache.hadoop.hbase.util.bytes.compareto(bytes.java:1036)         at org.apache.hadoop.hbase.util.bytes$bytearraycomparator.compare(bytes.java:104)         at org.apache.hadoop.hbase.util.bytes$bytearraycomparator.compare(bytes.java:96)         at java.util.treemap.cmp(treemap.java:1911)         at java.util.treemap.get(treemap.java:1835)         at org.apache.hadoop.hbase.util.softvaluesortedmap.get(softvaluesortedmap.java:91)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getcachedlocation(hconnectionmanager.java:788)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:651)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:634)         at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:601)         at org.apache.hadoop.hbase.client.htable.<init>(htable.java:128)         at org.apache.hadoop.hbase.thrift.thriftserver$hbasehandler.gettable(thriftserver.java:262)         at org.apache.hadoop.hbase.thrift.thriftserver$hbasehandler.mutaterowts(thriftserver.java:585)         at org.apache.hadoop.hbase.thrift.thriftserver$hbasehandler.mutaterow(thriftserver.java:578)         at org.apache.hadoop.hbase.thrift.generated.hbase$processor$mutaterow.process(hbase.java:2345)         at org.apache.hadoop.hbase.thrift.generated.hbase$processor.process(hbase.java:1988)         at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:259)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:619) in this specific case, the null cannot be the passed key because it's coming from htable which uses hconstants.empty_start_row. it cannot be a null key that was inserted previously because we would have got the npe at insert time. this can only mean that some key became null. ",
        "label": 229
    },
    {
        "text": "group atomic put delete operation into a single waledit to handle region server failures   hbase-3584 does not not provide fully atomic operation in case of region server failures (see explanation there). what should happen is that either (1) all edits are applied via a single waledit, or (2) the waledits are applied in async mode and then sync'ed together. for #1 it is not clear whether it is advisable to manage multiple different operations (put/delete) via a single wal edit. a quick check reveals that wal replay on region startup would work, but that replication would need to be adapted. the refactoring needed would be non-trivial. #2 might actually not work, as another operation could request sync'ing a later edit and hence flush these entries out as well. addendum:  the attached patch implements #1 and fixes replication to be able to deal with different operations being grouped in one waledit. ",
        "label": 286
    },
    {
        "text": "compact can skip the security access control  when client sends compact command to rs, the rs just create a compactionrequest, and then put it into the thread pool to process the compactionrequest. and when the region do the compact, it uses the rs's ugi to process the compact, so the compact can successfully done. example: user \"mapred\" do not have permission \"admin\", hbase(main):001:0> user_permission 'security' user                                table,family,qualifier:permission                                                                        mapred                             security,f1,c1: [permission: actions=read,write]  hbase(main):004:0> put 'security', 'r6', 'f1:c1', 'v9' 0 row(s) in 0.0590 seconds hbase(main):005:0> put 'security', 'r6', 'f1:c1', 'v10' 0 row(s) in 0.0040 seconds hbase(main):006:0> compact 'security' 0 row(s) in 0.0260 seconds maybe we can add permission check in the precompactselection() ? ",
        "label": 414
    },
    {
        "text": "deduplicate copies of jquery files  in hbase-14093 we started using a webjar as a source of bootstrap.  based on that solution it would be very simple to use jquery from webjar too.  this way only the version number should be changed at later version updates and we would not store the js code of jquery. ",
        "label": 439
    },
    {
        "text": "testfsutils fails against hadoop  trunk: mvn clean test -dhadoop.profile=2.0 -dtest=testfsutils java.io.filenotfoundexception: file /home/jon/proj/hbase-trunk/hbase-server/target/test-data/02beb8c8-06c1-47ea-829b-6e7ce0570cf8/hbase.version does not exist         at org.apache.hadoop.fs.rawlocalfilesystem.liststatus(rawlocalfilesystem.java:315)         at org.apache.hadoop.fs.filesystem.liststatus(filesystem.java:1279)         at org.apache.hadoop.fs.filesystem.liststatus(filesystem.java:1319)         at org.apache.hadoop.fs.checksumfilesystem.liststatus(checksumfilesystem.java:557)         at org.apache.hadoop.fs.filterfilesystem.liststatus(filterfilesystem.java:213)         at org.apache.hadoop.hbase.util.fsutils.getversion(fsutils.java:270)         at org.apache.hadoop.hbase.util.testfsutils.testversion(testfsutils.java:58)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) ...      ",
        "label": 248
    },
    {
        "text": "move master metrics to metrics  move master metrics to metrics 2 ",
        "label": 18
    },
    {
        "text": " hadoop2  several tests break because of hdfs  several unit tests will break due to hdfs-4305 (which enforces a min block size) because apprently, we set our block size smaller in these tests. specified block size is less than configured minimum value (dfs.namenode.fs-limits.min-block-size): 1024 < 1048576   at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfileint(fsnamesystem.java:1816)   at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfile(fsnamesystem.java:1795)   at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.create(namenoderpcserver.java:418)   at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.create(clientnamenodeprotocolserversidetranslatorpb.java:205)   at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java:44134)   at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:453)   at org.apache.hadoop.ipc.rpc$server.call(rpc.java:1002)   at org.apache.hadoop.ipc.server$handler$1.run(server.java:1696)   at org.apache.hadoop.ipc.server$handler$1.run(server.java:1692)   at java.security.accesscontroller.doprivileged(native method)   at javax.security.auth.subject.doas(subject.java:396)   at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1408)   at org.apache.hadoop.ipc.server$handler.run(server.java:1690) org.apache.hadoop.hbase.regionserver.testhregion.testgethdfsblocksdistribution  org.apache.hadoop.hbase.regionserver.testhregionbusywait.testgethdfsblocksdistribution  org.apache.hadoop.hbase.replication.testmasterreplication.testcyclicreplication  org.apache.hadoop.hbase.replication.testmasterreplication.testsimpleputdelete  org.apache.hadoop.hbase.replication.testmultislavereplication.testmultislavereplication  org.apache.hadoop.hbase.util.testfsutils.testcomputehdfsblocksdistribution ",
        "label": 248
    },
    {
        "text": "disable concurrent nightly builds  multiple nightly jobs for the same branch can run concurrently. there isn't much benefit from it and can easily exhaust the jenkins workers. ",
        "label": 352
    },
    {
        "text": "reopen a region if store reader references may have leaked  we can leak store reader references if a coprocessor or core function somehow opens a scanner, or wraps one, and then does not take care to call close on the scanner or the wrapped instance. a reasonable mitigation for a reader reference leak would be a fast reopen of the region on the same server (initiated by the rs) this will release all resources, like the refcount, leases, etc. the clients should gracefully ride over this like any other region transition. this reopen would be like what is done during schema change application and ideally would reuse the relevant code. if the refcount is over some ridiculous threshold this mitigation could be triggered along with a fat warn in the logs. ",
        "label": 473
    },
    {
        "text": "fix nosuchmethodexception in when running on local filesystem  fix this ugly exception that shows when running 0.92.1 when on local filesystem: 2012-03-16 10:54:48,351 info org.apache.hadoop.hbase.regionserver.wal.hlog: getnumcurrentreplicas--hdfs-826 not available; hdfs_out=org.apache.hadoop.fs.fsdataoutputstream@301abf87 java.lang.nosuchmethodexception: org.apache.hadoop.fs.checksumfilesystem$checksumfsoutputsummer.getnumcurrentreplicas()         at java.lang.class.getdeclaredmethod(class.java:1937)         at org.apache.hadoop.hbase.regionserver.wal.hlog.getgetnumcurrentreplicas(hlog.java:425)         at org.apache.hadoop.hbase.regionserver.wal.hlog.<init>(hlog.java:408)         at org.apache.hadoop.hbase.regionserver.wal.hlog.<init>(hlog.java:331)         at org.apache.hadoop.hbase.regionserver.hregionserver.instantiatehlog(hregionserver.java:1229)         at org.apache.hadoop.hbase.regionserver.hregionserver.setupwalandreplication(hregionserver.java:1218)         at org.apache.hadoop.hbase.regionserver.hregionserver.handlereportfordutyresponse(hregionserver.java:937)         at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:648)         at java.lang.thread.run(thread.java:680) ",
        "label": 458
    },
    {
        "text": "unhandled exception at regionserver  while starting hbase i get following exception: java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:459)  at java.lang.thread.run(thread.java:619) in region server's log on the second machine whereas at first machine all going well.  we've discussed with larsgeorge this problem at irc channel and seems problem is in hregionserver implementation.  patch which fixies that problem attached to this message, but it should be not a final variant, because i cannot stop hbase with this fix. ",
        "label": 143
    },
    {
        "text": " c  install header files as well as library  currently \"make install\" only installs libhbaseclient[_d].[so|a], but in order for the library to be useful for building applications, we'll need to install the header files also. this, of course, brings up another problem: that the headers aren't in their own directory structure, so that application includes could look something like: #include <hbase/...> i suggest that we create an hbase sub-directory (under hbase-native-client), move all the [public] headers into there (we can keep the current directory structure under that, i.e. hbase/core/cell.h, etc.) ",
        "label": 155
    },
    {
        "text": "hbaseserver logs the full query   we log the query when we have an error. as a results, the logs are not readable when using stuff like multi. as a side note, this is as well a security issue (no need to encrypt the network and the storage if the logs contain everything). i'm not removing the full log line here; but just ask and i do it . ",
        "label": 314
    },
    {
        "text": "several modules missing in nexus for apache hbase  the latest version of hbase-shaded-client is currently 2.2.1. it has been a while since 2.2.2 release (2019/10/25). see: https://search.maven.org/search?q=hbase-shaded-client. ",
        "label": 402
    },
    {
        "text": "allow client side connection pooling  by design, the hbase rpc client multiplexes calls to a given region server (or the master for that matter) over a single socket, access to which is managed by a connection thread defined in the hbaseclient class. while this approach may suffice for most cases, it tends to break down in the context of a real-time, multi-threaded server, where latencies need to be lower and throughputs higher. in brief, the problem is that we dedicate one thread to handle all client-side reads and writes for a given server, which in turn forces them to share the same socket. as load increases, this is bound to serialize calls on the client-side. in particular, when the rate at which calls are submitted to the connection thread is greater than that at which the server responds, then some of those calls will inevitably end up sitting idle, just waiting their turn to go over the wire. in general, sharing sockets across multiple client threads is a good idea, but limiting the number of such sockets to one may be overly restrictive for certain cases. here, we propose a way of defining multiple sockets per server endpoint, access to which may be managed through either a load-balancing or thread-local pool. to that end, we define the notion of a sharedmap, which maps a key to a resource pool, and supports both of those pool types. specifically, we will apply that map in the hbaseclient, to associate multiple connection threads with each server endpoint (denoted by a connection id).  currently, the sharedmap supports the following types of pools: a threadlocalpool, which represents a pool that builds on the threadlocal class. it essentially binds the resource to the thread from which it is accessed. a reusablepool, which represents a pool that builds on the linkedlist class. it essentially allows resources to be checked out, at which point it is (temporarily) removed from the pool. when the resource is no longer required, it should be returned to the pool in order to be reused. a roundrobinpool, which represents a pool that stores its resources in an arraylist. it load-balances access to its resources by returning a different resource every time a given key is looked up. to control the type and size of the connection pools, we give the user a couple of parameters (viz. \"hbase.client.ipc.pool.type\" and \"hbase.client.ipc.pool.size\"). in case the size of the pool is set to a non-zero positive number, that is used to cap the number of resources that a pool may contain for any given key. a size of integer#max_value is interpreted to mean an unbounded pool. ",
        "label": 265
    },
    {
        "text": "cleanup checkandxxx logic  1. the checkand {put|delete} method that takes a compareop is not exposed via htable[interface].  2. there is unnecessary duplicate code in the check{put|delete} code in hregionserver. ",
        "label": 203
    },
    {
        "text": "cleanup arrays vs lists of scanners  arrays and lists are used inconsistently for working with sets of scanners throughout keyvalueheap, memstore, storescanner, etc. we should pick one and use it consistently. ",
        "label": 547
    },
    {
        "text": "cannot scan all families in a row with a limit  startrow  etc   suggest moving specification of columns inside the hash of optional arguments rather than require it proceed the hash of optional limit, startrow, etc. ",
        "label": 218
    },
    {
        "text": "master crash when splitting hlog may cause data loss  let's see the code of hlogsplitter#splitlog(final filestatus[] logfiles) private list<path> splitlog(final filestatus[] logfiles) throws ioexception {  try {   for (filestatus log : logfiles) {   parsehlog(in, logpath, entrybuffers, fs, conf, skiperrors);  }  archivelogs(srcdir, corruptedlogs, processedlogs, oldlogdir, fs, conf);  } finally {       status.setstatus(\"finishing writing output logs and closing down.\");       splits = outputsink.finishwritingandclose();     } } if master is killed, after finishing archivelogs(srcdir, corruptedlogs, processedlogs, oldlogdir, fs, conf),   but before finishing splits = outputsink.finishwritingandclose();  log date would loss! ",
        "label": 107
    },
    {
        "text": "removing a peer does not properly clean up the replicationsourcemanager state and metrics  removing a peer does not clean up the associated metrics and state from walsbyid map in the replicationsourcemanager. ",
        "label": 55
    },
    {
        "text": " replication  replication will block if wal compress set differently in master and slave configuration  as we know in hbase 0.94.0 we have a configuration below  <property>  <name>hbase.regionserver.wal.enablecompression</name>  <value>true</value>  </property>  if we enable it in master cluster and disable it in slave cluster . then replication will not work. it will throw unwrapremoteexception again and again in master cluster. 2012-08-09 12:49:55,892 warn org.apache.hadoop.hbase.replication.regionserver.replicationsource: can't replicate because of an error  on the remote cluster:   java.io.ioexception: ipc server unable to read call parameters: error in readfields  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)  at java.lang.reflect.constructor.newinstance(constructor.java:513)  at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:95)  at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:79)  at org.apache.hadoop.hbase.replication.regionserver.replicationsource.shipedits(replicationsource.java:635)  at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:365)  caused by: org.apache.hadoop.ipc.remoteexception: ipc server unable to read call parameters: error in readfields  at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:921)  at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:151)  at $proxy13.replicatelogentries(unknown source)  at org.apache.hadoop.hbase.replication.regionserver.replicationsource.shipedits(replicationsource.java:616)  ... 1 more this is because slave cluster can not parse the hlog entry . 2012-08-09 14:46:05,891 warn org.apache.hadoop.ipc.hbaseserver: unable to read call parameters for client 10.232.98.89  java.io.ioexception: error in readfields  at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:685)  at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:586)  at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:635)  at org.apache.hadoop.hbase.ipc.invocation.readfields(invocation.java:125)  at org.apache.hadoop.hbase.ipc.hbaseserver$connection.processdata(hbaseserver.java:1292)  at org.apache.hadoop.hbase.ipc.hbaseserver$connection.readandprocess(hbaseserver.java:1207)  at org.apache.hadoop.hbase.ipc.hbaseserver$listener.doread(hbaseserver.java:735)  at org.apache.hadoop.hbase.ipc.hbaseserver$listener$reader.dorunloop(hbaseserver.java:524)  at org.apache.hadoop.hbase.ipc.hbaseserver$listener$reader.run(hbaseserver.java:499)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.eofexception  at java.io.datainputstream.readfully(datainputstream.java:180)  at org.apache.hadoop.hbase.keyvalue.readfields(keyvalue.java:2254)  at org.apache.hadoop.hbase.regionserver.wal.waledit.readfields(waledit.java:146)  at org.apache.hadoop.hbase.regionserver.wal.hlog$entry.readfields(hlog.java:1767)  at org.apache.hadoop.hbase.io.hbaseobjectwritable.readobject(hbaseobjectwritable.java:682)  ... 11 more ",
        "label": 552
    },
    {
        "text": "add command for exporting snapshot in hbase command script  users of the hbase command script can now use the 'hbase snapshot export\u2019 command. this replaces the need to type the full class name of 'org.apache.hadoop.hbase.snapshot.exportsnapshot'  in addition to adding command 'snapshot export', we are also grouping snapshot related commands together as subcommands under 'hbase snapshot'. ",
        "label": 506
    },
    {
        "text": "connect to regionserver  ",
        "label": 154
    },
    {
        "text": "add support for phabricator differential as an alternative code review tool  from http://phabricator.org/ : \"phabricator is a open source collection of web applications which make it easier to write, review, and share source code. it is currently available as an early release. phabricator was developed at facebook.\" it's open source so pretty much anyone could host an instance of this software. to begin with, there will be a public-facing instance located at http://reviews.facebook.net (sponsored by facebook and hosted by the osuosl http://osuosl.org). we will use this jira to deal with adding (and ensuring) apache-friendly support that will allow us to do code reviews with phabricator for hbase. ",
        "label": 341
    },
    {
        "text": "tableinputformat should support more confs for the scanner  tableinputformat currently only supports hbase.mapreduce.scan and hbase.mapreduce.inputtable confs. the scan conf is a base64 encoded scanner object, which makes it difficult to use tif from hadoop pipes. i propose that the following confs be supported iff hbase.mapreduce.scan is not set: hbase.mapreduce.scan.columns hbase.mapreduce.scan.timestamp hbase.mapreduce.scan.timerange.start hbase.mapreduce.scan.timerange.end hbase.mapreduce.scan.maxversions hbase.mapreduce.scan.cacheblocks hbase.mapreduce.scan.cachedrows ideally we would also support filters but i think that would be harder to implement. ",
        "label": 61
    },
    {
        "text": "improve the copy table doc to include information about versions  current copytable documentation [1] isn't clear about the default number of versions that are copied per cell. this patch just adds it as a note to that doc. [1] http://hbase.apache.org/book/ops_mgt.html#copytable ",
        "label": 71
    },
    {
        "text": "testdistributedlogsplitting testmasterstartsupwithlogsplittingwork fails frequently  3 out of last four run of my rc jenkins build failed in this test.  might be related to hbase-11041 too. ",
        "label": 286
    },
    {
        "text": "filesystem not closed in secure bulkload  filesystem not closed in secure bulkload after bulkload finish, it will cause memory used more and more if too many bulkload . ",
        "label": 53
    },
    {
        "text": "document metrics  explain why this work was done. explain how the data flows from the core classes into hadoop metrics2, and on to jmx. explain naming metrics. ",
        "label": 154
    },
    {
        "text": "hbase autorestart will overwrite the gc log  while using hbase autorestart , the gc log will be overwirited after the process(hmaster or hregionserver) retarting. this is because the autorestart loop is in internal_autostart function ( in hbase-daemon.sh), but we only rotate the gc log in autorestart function. (internal_autostart)  one_hour_in_secs=3600  autostartwindowstartdate=`date +%s`  autostartcount=0  touch \"$hbase_autostart_file\" 1. keep starting the command until asked to stop. reloop on software crash  while true  do  code # restart the hbase process util the cluster is shut down  fi (autorestart)  echo running $command, logging to $hbase_logout 1. stop the command  $thiscmd --config \"${hbase_conf_dir}\" stop $command $args &  wait_until_done $! 2. wait a user-specified sleep period  sp=${hbase_restart_sleep:-3}  if [ $sp -gt 0 ]; then  sleep $sp  fi  check_before_start  hbase_rotate_log $hbase_logout  hbase_rotate_log $hbase_loggc  nohup $thiscmd --config \"${hbase_conf_dir}\" --autostart-window-size ${autostart_window_size} --autostart-window-retry-limit ${autostart_window_retry_limit} \\  internal_autostart $command $args < /dev/null > ${hbase_logout} 2>&1 & ",
        "label": 169
    },
    {
        "text": "backport hbase include issue servers information in retriesexhaustedwithdetailsexception message  to branch  ",
        "label": 534
    },
    {
        "text": "improvements and refactor for hbasefsck tool  refactored the way the hbasefsck tool works \u2013 it now loads all the info it can find into ram first, and then looks over the in-memory structures for inconsistencies. it's still a work in progress, but detects more kinds of problems now (eg multiple assignment, etc) and has some very basic functional tests. ",
        "label": 247
    },
    {
        "text": "add debugging around commit log cleanup  yesterday, streamy replayed 1000 logs. this seems too many for any one regionserver to be carrying. its hard to tell if the 1000 logs were legit though because our logging isn't detailed enough. this issue is about adding detail around log cleaning so we can see how many logs are to be cleaned and which region the last edit belongs to. ",
        "label": 314
    },
    {
        "text": "add per table metrics back  we used to have per-table metrics, but it was removed in some restructuring. we have per-region metrics, and per-regionserver metrics, but nothing in between. for majority of users, per-region is too granular, they are mostly interested in table level aggregates. this is especially useful in multi-tenant cases where a table's disk usage, number of requests, etc can be made much more visible. in this jira, we'll add the basic infrastructure to add a single (or a few) per-table metrics. than we can improve on that by adding remaining metrics from the region server level. the plan is to not aggregate per-table metrics at master for now. just aggregation of per-region metrics at the per-table level for every regionserver. ",
        "label": 28
    },
    {
        "text": "redundant monitoredtask instances in case of distributed log splitting retry  in case of log splitting retry, the following code would be executed multiple times:   public long splitlogdistributed(final list<path> logdirs) throws ioexception {     monitoredtask status = taskmonitor.get().createstatus(           \"doing distributed log split in \" + logdirs); leading to multiple monitoredtask instances. user may get confused by multiple distributed log splitting entries for the same region server on master ui ",
        "label": 441
    },
    {
        "text": "make call queue too big exception use servername  if the rpc server is listening to something other than the hostname ( 0.0.0.0 for example ) then the exception thrown isn't very helpful. we should make that more helpful. ",
        "label": 323
    },
    {
        "text": "testhfilecleaner testhfilecleaning sometimes fails in trunk  in build #3334, i saw: java.lang.assertionerror: expected:<1> but was:<0> at org.junit.assert.fail(assert.java:93) at org.junit.assert.failnotequals(assert.java:647) at org.junit.assert.assertequals(assert.java:128) at org.junit.assert.assertequals(assert.java:472) at org.junit.assert.assertequals(assert.java:456) at org.apache.hadoop.hbase.master.cleaner.testhfilecleaner.testhfilecleaning(testhfilecleaner.java:88) ",
        "label": 236
    },
    {
        "text": "consolidate snapshot related classes into fewer packages  the snapshot branch seems to have more packages with fewer classes present in each. we should consolidate them down to a core set. i'm suggesting limiting it to: o.a.h.h.errorhandling (move o.a.h.h.server.errorhandling.** to this package)   o.a.h.h.procedure (eliminate procedure.member, procedure.coordinator, possibly add zk for the zk implementation)  o.a.h.h.snapshot (move o.a.h.h.server.snapshots.** to this package)  o.a.h.h.master.snapshot (fold subpackages in)  o.a.h.h.regionserver.snapshot (fold subpackges in) likely move all testsnapshotfrom* to o.a.h.h.snapshot. ",
        "label": 248
    },
    {
        "text": "remove the compactionrequest parameter in precompactselection  as we do not have a compactionrequest yet when pre compaction selection so we always pass a null when calling which is useless...           override = getcoprocessorhost().precompactselection(this, candidatesforcoproc,             tracker, null, user); ",
        "label": 352
    },
    {
        "text": "subprocedure initialization fails with invalid znode data   sometimes snapshots subprocedures fail to start on rs because data read from zk is bad. 2012-12-13 07:22:55,238 error org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs: illegal argument exception java.lang.illegalargumentexception: could not read snapshot information from request.         at org.apache.hadoop.hbase.regionserver.snapshot.regionserversnapshotmanager$snapshotsubprocedurebuilder.buildsubprocedure(regionserversnapsh otmanager.java:284)         at org.apache.hadoop.hbase.procedure.proceduremember.createsubprocedure(proceduremember.java:98)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:199)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.waitfornewprocedures(zkprocedurememberrpcs.java:167)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.access$1(zkprocedurememberrpcs.java:150)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs$1.nodechildrenchanged(zkprocedurememberrpcs.java:106)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:303)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) 2012-12-13 07:22:55,239 error org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs: failed due to null subprocedure local foreignthreadexception from null         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:203)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.waitfornewprocedures(zkprocedurememberrpcs.java:167)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.access$1(zkprocedurememberrpcs.java:150)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs$1.nodechildrenchanged(zkprocedurememberrpcs.java:106)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:303)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) caused by: java.lang.illegalargumentexception: could not read snapshot information from request.         at org.apache.hadoop.hbase.regionserver.snapshot.regionserversnapshotmanager$snapshotsubprocedurebuilder.buildsubprocedure(regionserversnapshotmanager.java:284)         at org.apache.hadoop.hbase.procedure.proceduremember.createsubprocedure(proceduremember.java:98)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:199)         ... 6 more 2012-12-13 07:22:55,239 error org.apache.zookeeper.clientcnxn: error while calling watcher  java.lang.nullpointerexception         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.sendmemberaborted(zkprocedurememberrpcs.java:266)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.startnewsubprocedure(zkprocedurememberrpcs.java:203)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.waitfornewprocedures(zkprocedurememberrpcs.java:167)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs.access$1(zkprocedurememberrpcs.java:150)         at org.apache.hadoop.hbase.procedure.zkprocedurememberrpcs$1.nodechildrenchanged(zkprocedurememberrpcs.java:106)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:303)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) ",
        "label": 248
    },
    {
        "text": "fstabledescriptors get  can return null reference  in some cases  it is not checked  in one of our cases, 1.2.0 based master could not start because the null reference is not checked. master crashed because of the following exception. 2017-11-20 08:30:20,178 fatal org.apache.hadoop.hbase.master.hmaster: failed to become active master java.lang.nullpointerexception at org.apache.hadoop.hbase.master.assignmentmanager.rebuilduserregions(assignmentmanager.java:2993) at org.apache.hadoop.hbase.master.assignmentmanager.joincluster(assignmentmanager.java:494) at org.apache.hadoop.hbase.master.hmaster.finishactivemasterinitialization(hmaster.java:821) at org.apache.hadoop.hbase.master.hmaster.access$500(hmaster.java:192) at org.apache.hadoop.hbase.master.hmaster$1.run(hmaster.java:1827) at java.lang.thread.run(thread.java:745) ",
        "label": 252
    },
    {
        "text": "fix findbug warning in visibilitycontroller  there're 3 findbugs warnings for visibilitycontroller: rcn nullcheck of user at line 959 of value previously dereferenced in org.apache.hadoop.hbase.security.visibility.visibilitycontroller.getsystemandsuperusers() test unknown bug pattern vo_volatile_increment in org.apache.hadoop.hbase.security.visibility.visibilitycontroller.addlabels(rpccontroller, visibilitylabelsprotos$visibilitylabelsrequest, rpccallback) is inconsistent synchronization of org.apache.hadoop.hbase.security.visibility.visibilitycontroller.regionenv; locked 62% of time ",
        "label": 441
    },
    {
        "text": "create async admin methods for clear block cache  as part of the review for hbase-18624, reviewers suggested to add the clear_block_cache to the asyncadmin as well. since the issue was very large, we decided to split this into a follow-up jira. the purpose of this jira will be to finish the work on the asyncadmin. ",
        "label": 187
    },
    {
        "text": "incorrect sequence number for cache flush  an hregion asks each hstore to flush its cache with a sequence number x. the assumption is that all the updates before x will be flushed. so during the startup reconstruction, the updates before x are skipped. the use of updateslock should guarantee that all the updates before x will be flushed when hstore flushes with x - snapshots are taken after the write lock on updateslock is acquired, while all the updates are written to the log and to the cache with the read lock on updateslock is acquired. however, because the sequence number x is obtained without the write lock on updateslock, some updates with sequence number <= x may not have been written to the cache which will be flushed. ",
        "label": 241
    },
    {
        "text": "rs hangs waiting on region to close on shutdown  has to timeout before can go down  i am seeing cluster sometimes fails to go down hanging out waiting on close - it looks like it is waiting on hbase:meta that is the issue. i am running 0.98.0rc3 and hadoop-2.4.0-snapshot. might be my setup. filing this issue to keep an eye on this as i go. it looks like we are not calling close the region that is holding us up. log is full of this: 2014-02-14 16:07:21,095 debug [regionserver60020] regionserver.hregionserver: waiting on 1588230740 ",
        "label": 314
    },
    {
        "text": "throw exception with job getstatus getfailureinfo  when exportsnapshot fails      // run the mr job     if (!job.waitforcompletion(true)) {       // todo: replace the fixed string with job.getstatus().getfailureinfo()       // when it will be available on all the supported versions.       throw new exportsnapshotexception(\"copy files map-reduce job failed\");     } ",
        "label": 106
    },
    {
        "text": "enable direct byte buffers lrublockcache  java offers the creation of direct byte buffers which are allocated outside of the heap. they need to be manually free'd, which can be accomplished using an documented clean method. the feature will be optional. after implementing, we can benchmark for differences in speed and garbage collection observances. ",
        "label": 289
    },
    {
        "text": "loadincrementalhfiles fails to load from remote cluster in hadoop  running on hadoop 2, loadincrementalhfiles gives the following exception when loading from a remote cluster. org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles$3@455e455e, java.io.ioexception: j ava.io.ioexception: java.lang.unsupportedoperationexception: immutable configuration         at org.apache.hadoop.hbase.regionserver.compoundconfiguration.setclass(compoundconfiguration.java:516)         at org.apache.hadoop.ipc.rpc.setprotocolengine(rpc.java:195)         at org.apache.hadoop.hdfs.namenodeproxies.creatennproxywithclientprotocol(namenodeproxies.java:250)         at org.apache.hadoop.hdfs.namenodeproxies.createnonhaproxy(namenodeproxies.java:169)         at org.apache.hadoop.hdfs.namenodeproxies.createproxy(namenodeproxies.java:130)         at org.apache.hadoop.hdfs.dfsclient.<init>(dfsclient.java:482)         at org.apache.hadoop.hdfs.dfsclient.<init>(dfsclient.java:445)         at org.apache.hadoop.hdfs.distributedfilesystem.initialize(distributedfilesystem.java:136)         at org.apache.hadoop.fs.filesystem.createfilesystem(filesystem.java:2429)         at org.apache.hadoop.fs.filesystem.access$200(filesystem.java:88)         at org.apache.hadoop.fs.filesystem$cache.getinternal(filesystem.java:2463)         at org.apache.hadoop.fs.filesystem$cache.get(filesystem.java:2445)         at org.apache.hadoop.fs.filesystem.get(filesystem.java:363)         at org.apache.hadoop.fs.path.getfilesystem(path.java:283)         at org.apache.hadoop.hbase.regionserver.store.assertbulkloadhfileok(store.java:571)         at org.apache.hadoop.hbase.regionserver.hregion.bulkloadhfiles(hregion.java:3689)         at org.apache.hadoop.hbase.regionserver.hregion.bulkloadhfiles(hregion.java:3637)         at org.apache.hadoop.hbase.regionserver.hregionserver.bulkloadhfiles(hregionserver.java:2939)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:60)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:37)         at java.lang.reflect.method.invoke(method.java:611)         at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:320)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426)         at org.apache.hadoop.hbase.client.servercallable.withretries(servercallable.java:186)         at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles.tryatomicregionload(loadincrementalhfiles.java:567)         at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles$1.call(loadincrementalhfiles.java:317)         at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles$1.call(loadincrementalhfiles.java:315) this does not happen when loading from the same filesystem. ",
        "label": 441
    },
    {
        "text": "testreplicationsmalltests testdisableenable fails intermittently  from https://builds.apache.org/job/hbase-0.95/444/testreport/junit/org.apache.hadoop.hbase.replication/testreplicationsmalltests/testdisableenable/ : java.lang.assertionerror: waited too much time for put replication at org.junit.assert.fail(assert.java:88) at org.apache.hadoop.hbase.replication.testreplicationsmalltests.testdisableenable(testreplicationsmalltests.java:313) ... 2013-08-14 08:06:47,228 debug [rs:1;vesta:39079-eventthread.replicationsource,2] wal.protobuflogreader(118): after reading the trailer: waleditsstopoffset: 0, filelength: 0, trailerpresent: false 2013-08-14 08:06:47,228 warn  [rs:1;vesta:39079-eventthread.replicationsource,2] regionserver.replicationsource(301): 2 got:  java.io.ioexception: cannot seek after eof at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.seek(dfsclient.java:2593) at org.apache.hadoop.fs.fsdatainputstream.seek(fsdatainputstream.java:37) at org.apache.hadoop.hbase.regionserver.wal.protobuflogreader.seekonfs(protobuflogreader.java:275) at org.apache.hadoop.hbase.regionserver.wal.readerbase.seek(readerbase.java:115) at org.apache.hadoop.hbase.replication.regionserver.replicationhlogreadermanager.seek(replicationhlogreadermanager.java:108) at org.apache.hadoop.hbase.replication.regionserver.replicationsource.readallentriestoreplicateornextfile(replicationsource.java:388) at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:297) 2013-08-14 08:06:47,228 debug [rs:1;vesta:39079-eventthread.replicationsource,2] regionserver.replicationsource(578): nothing to replicate, sleeping 100 times 1 2013-08-14 08:06:47,329 debug [rs:1;vesta:39079-eventthread.replicationsource,2] fs.hfilesystem$reorderwalblocks(327): /user/jenkins/hbase/wals/vesta.apache.org,39079,1376467506138/vesta.apache.org%2c39079%2c1376467506138.1376467603252 is an hlog file, so reordering blocks, last hostname will be:vesta.apache.org looking at test output from successful builds, i didn't see the above exception. ",
        "label": 441
    },
    {
        "text": "can not get svn revision   at build time if locale is not english  my locale is zh_tw.utf-8, so 'svn info' shows messages in chinese. but  src/saveversion.sh expects english from output. i suggest that we add clear lang, lc_* in saveversion.sh before calling svn. ",
        "label": 385
    },
    {
        "text": "make hregionserver aware of the regions it's opening closing  this is a serious issue about a race between regions being opened and closed in region servers. we had this situation where the master tried to unassign a region for balancing, failed, force unassigned it, force assigned it somewhere else, failed to open it on another region server (took too long), and then reassigned it back to the original region server. a few seconds later, the region server processed the first closed and the region was left unassigned. this is from the master log: 11-04-05 15:11:17,758 debug org.apache.hadoop.hbase.master.assignmentmanager: sent close to servername=sv4borg42,60020,1300920459477, load=(requests=187, regions=574, usedheap=3918, maxheap=6973) for region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:12:10,021 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 state=pending_close, ts=1302041477758  2011-04-05 15:12:10,021 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_close for too long, running forced unassign again on region=stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  ...  2011-04-05 15:14:45,783 debug org.apache.hadoop.hbase.master.assignmentmanager: forcing offline; was=stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 state=closed, ts=1302041685733  2011-04-05 15:14:45,783 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x42ec2cece810b68 creating (or updating) unassigned node for 1470298961 with offline state  ...  2011-04-05 15:14:45,885 debug org.apache.hadoop.hbase.master.assignmentmanager: using pre-existing plan for region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961; plan=hri=stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961, src=sv4borg42,60020,1300920459477, dest=sv4borg40,60020,1302041218196  2011-04-05 15:14:45,885 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 to sv4borg40,60020,1302041218196  2011-04-05 15:15:39,410 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out: stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 state=pending_open, ts=1302041700944  2011-04-05 15:15:39,410 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_open for too long, reassigning region=stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:39,410 debug org.apache.hadoop.hbase.master.assignmentmanager: forcing offline; was=stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 state=pending_open, ts=1302041700944  ...  2011-04-05 15:15:39,410 debug org.apache.hadoop.hbase.master.assignmentmanager: no previous transition plan was found (or we are ignoring an existing plan) for stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 so generated a random one; hri=stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961, src=, dest=sv4borg42,60020,1300920459477; 19 (online=19, exclude=null) available servers  2011-04-05 15:15:39,410 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 to sv4borg42,60020,1300920459477  2011-04-05 15:15:40,951 debug org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: master:60000-0x42ec2cece810b68 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/prodjobs/unassigned/1470298961  2011-04-05 15:15:40,952 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:60000-0x42ec2cece810b68 retrieved 93 byte(s) of data from znode /prodjobs/unassigned/1470298961 and set watcher; region=stumbles_by_userid2,'\ufffd\ufffd\ufffd\ufffd\u7a57\ufffd\ufffd\ufffd6,1266566087256, server=sv4borg42,60020,1300920459477, state=rs_zk_region_opened  2011-04-05 15:15:40,952 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_opened, server=sv4borg42,60020,1300920459477, region=1470298961  2011-04-05 15:15:42,222 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: handling opened event for 1470298961; deleting unassigned node  ...  2011-04-05 15:15:55,812 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:60000-0x42ec2cece810b68 retrieved 93 byte(s) of data from znode /prodjobs/unassigned/1470298961 and set watcher; region=stumbles_by_userid2,'\ufffd\ufffd\ufffd\ufffd\u7a57\ufffd\ufffd\ufffd6,1266566087256, server=sv4borg42,60020,1300920459477, state=rs_zk_region_closing  2011-04-05 15:15:55,812 debug org.apache.hadoop.hbase.master.assignmentmanager: handling new unassigned node: /prodjobs/unassigned/1470298961 (region=stumbles_by_userid2,'\ufffd\ufffd\ufffd\ufffd\u7a57\ufffd\ufffd\ufffd6,1266566087256, server=sv4borg42,60020,1300920459477, state=rs_zk_region_closing)  2011-04-05 15:15:55,812 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_closing, server=sv4borg42,60020,1300920459477, region=1470298961  2011-04-05 15:15:55,812 warn org.apache.hadoop.hbase.master.assignmentmanager: received closing for region 1470298961 from server sv4borg42,60020,1300920459477 but region was in the state null and not in expected pending_close or closing states and from sv4borg42: 2011-04-05 15:09:58,755 info org.apache.hadoop.hbase.regionserver.hregionserver: received close region: stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:11:17,757 info org.apache.hadoop.hbase.regionserver.hregionserver: received close region: stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:12:10,021 info org.apache.hadoop.hbase.regionserver.hregionserver: received close region: stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:14:45,675 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: processing close of stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:14:45,700 debug org.apache.hadoop.hbase.regionserver.hregion: closing stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961: disabling compactions & flushes  2011-04-05 15:14:45,701 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:14:45,701 info org.apache.hadoop.hbase.regionserver.hregion: closed stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:14:45,758 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: closed region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:39,410 info org.apache.hadoop.hbase.regionserver.hregionserver: received request to open region: stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:39,410 debug org.apache.hadoop.hbase.regionserver.handler.openregionhandler: processing open of stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:39,486 debug org.apache.hadoop.hbase.regionserver.hregion: opening region: region => {name => 'stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256', startkey => '\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6', endkey => '\\x00'\\x9au\\x7f\\xff\\xfe\\xebq\\xb0\\xc3\\xef\\x00jr\\xf2', encoded => 1470298961, table => ...  2011-04-05 15:15:39,487 debug org.apache.hadoop.hbase.regionserver.hregion: instantiated stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:40,399 info org.apache.hadoop.hbase.regionserver.hregion: onlined stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961; next sequenceid=37627407247  2011-04-05 15:15:40,488 info org.apache.hadoop.hbase.catalog.metaeditor: updated row stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 in region .meta.,,1 with server=sv4borg42:60020, startcode=1300920459477  2011-04-05 15:15:40,582 debug org.apache.hadoop.hbase.regionserver.handler.openregionhandler: opened stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:55,776 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: processing close of stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:55,809 debug org.apache.hadoop.hbase.regionserver.hregion: closing stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961: disabling compactions & flushes  2011-04-05 15:15:55,809 debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:55,809 info org.apache.hadoop.hbase.regionserver.hregion: closed stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:55,842 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: closed region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:55,943 debug org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: processing close of stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961  2011-04-05 15:15:55,943 warn org.apache.hadoop.hbase.regionserver.handler.closeregionhandler: received close for region stumbles_by_userid2,\\x00'\\x8e\\xe8\\x7f\\xff\\xfe\\xe7\\xa9\\x97\\xfc\\xdf\\x01\\x10\\xcc6,1266566087256.1470298961 but currently not serving ",
        "label": 229
    },
    {
        "text": "testimportexport tests failing due to jar manifest issues on hadoop  seen in recent hadoop 2 test failures: -------------------------------------------------------  t e s t s ------------------------------------------------------- running org.apache.hadoop.hbase.mapreduce.testimportexport 2012-09-27 15:57:27.192 java[62016:1903] unable to load realm info from scdynamicstore 2012-09-27 15:57:27.325 java[62016:1903] unable to load realm info from scdynamicstore tests run: 3, failures: 0, errors: 3, skipped: 0, time elapsed: 46.22 sec <<< failure! results : tests in error:    testsimplecase(org.apache.hadoop.hbase.mapreduce.testimportexport): java.lang.runtimeexception: java.util.zip.zipexception: duplicate entry: meta-inf/manifest.mf   testmetaexport(org.apache.hadoop.hbase.mapreduce.testimportexport): java.lang.runtimeexception: java.util.zip.zipexception: duplicate entry: meta-inf/manifest.mf moved over from hbase-6601. ",
        "label": 248
    },
    {
        "text": "rename cell datatype to cell type  align the name with keyvalue.type. also, it is the return type of cell.gettype() so the qualifier \"data\" is inappropriate. ",
        "label": 98
    },
    {
        "text": "clean up duplicate dependency management entries for hbase shaded miscellaneous  our root pom's dependency management section has two entries like: <dependency>         <groupid>org.apache.hbase.thirdparty</groupid>         <artifactid>hbase-shaded-miscellaneous</artifactid>         <version>${hbase-thirdparty.version}</version>       </dependency> which causes a bunch of warnings at build time. remove one. ",
        "label": 106
    },
    {
        "text": "accommodate the hbase indexer lily sep consumer deploy type  this is a follow-on from hbase-10504, define a replication interface. there we defined a new, flexible replication endpoint for others to implement but it did little to help the case of the lily hbase-indexer. this issue takes up the case of the hbase-indexer. the hbase-indexer poses to hbase as a 'fake' peer cluster (for why hbase-indexer is implemented so, the advantage to having the indexing done in a separate process set that can be independently scaled, can participate in the same security realm, etc., see discussion in hbase-10504). the hbase-indexer will start up a cut-down \"regionserver\" processes that are just an instance of hbase rpcserver hosting an adminprotos service. they make themselves 'appear' to the replication source by hoisting up an ephemeral znode 'registering' as a regionserver. the source cluster then streams waledits to the admin protos method:  public replicatewalentryresponse replicatewalentry(final rpccontroller controller,       final replicatewalentryrequest request) throws serviceexception { the hbase-indexer relies on other hbase internals like server so it can get a zookeeperwatcher instance and know the 'name' to use for this cut-down server. thoughts on how to proceed include: better formalize its current digestion of hbase internals; make it so rpcserver is allowed to be used by others, etc. this would be hard to do given they use basics like server, protobuf serdes for wal types, and adminprotos service. any change in this wide api breaks (again) hbase-indexer. we have made a 'channel' for coprocessor endpoints so they continue to work though they use 'internal' types. they can use protos in hbase-protocol. hbase-protocol protos are in a limbo currently where they are sort-of 'public'; a todo. perhaps the hbase-indexer could do similar relying on the hbase-protocol (pb2.5) content and we could do something to reveal rpcserver and zk for hbase-indexer safe use. start an actual regionserver only have it register the adminprotos service only \u2013 not clientprotos and the service that does master interaction, etc. [i checked, this is not as easy to do as i at first thought -- st.ack] then have the hbase-indexer implement an admincoprocessor to override the replicatewalentry method (the admin cp implementation may need work). this would narrow the hbase-indexer exposure to that of the admin coprocessor interface over in hbase-10504, enis soztutar suggested \"... if we want to provide isolation for the replication services in hbase, we can have a simple host as another daemon which hosts the replicationendpoint implementation. rs's will use a built-in re to send the edits to this layer, and the host will delegate it to the re implementation. the flow would be something like: rs --> re inside rs --> host daemon for re --> actual re implementation --> third party system...\" other crazy notions occur including the setup of an admin interface coprocessor endpoint. a new replicationendpoint would feed the replication stream to the remote cluster via the cpep registered channel. but time is short. hopefully we can figure something that will work in 2.0 timeframe w/o too much code movement. ",
        "label": 314
    },
    {
        "text": "downloads page is out of date for and release lines  the downloads page (http://hbase.apache.org/downloads.html) still shows 1.3.3 and 1.4.9 releases. the links are mostly dead since dist.a.o has been updated for later releases. ",
        "label": 38
    },
    {
        "text": "infoservers no longer put up a ui  infoservers do not work in an vanilla hadoop 0.20 and hbase 0.20-dev installation. directory listings instead of ui. ",
        "label": 314
    },
    {
        "text": "remove reservation blocks from region server  here is link to discussion on dev@hbase: http://search-hadoop.com/m/hsjjh1icjji1/resevoir+blocks+for+region+server&subj=re+resevoir+blocks+for+region+server from j-d:  since we default to onoutofmemoryerror kill -9 then we should just get  rid of it imo. ",
        "label": 441
    },
    {
        "text": "major compaction on non existing table does not throw error  following will not complain even if fubar does not exist echo \"major_compact 'fubar'\" | $hbase_home/bin/hbase shell the downside for this defect is that major compaction may be skipped due to  a typo by ops. ",
        "label": 417
    },
    {
        "text": "share some stuffs with the initial reader when new stream reader created  when switching pread to stream read, new hfilereaderimpl will be create, but the two different readers do not share informations with each other. maybe we can divide hfilereaderimpl into two different class, such as hfilepreadreader and hfilestreamreader. when constructing hfilestreamreader, it will copy some stuffs (fileinfo, index, etc) from an already existing reader, and no need to do prefetch operations. ",
        "label": 521
    },
    {
        "text": "copytable and verifyreplication   option to specify batch size  versions  need option to specify batch size for copytable and verifyreplication. we are working on patch for this. ",
        "label": 347
    },
    {
        "text": "org apache hadoop hbase integrationtestrebalanceandkillserverstargeted fails when i cd hbase it and mvn verify  trying to make up something to hand off to bigtop project, running the hbase it tests, this one fails. durruti:failsafe-reports stack$ more org.apache.hadoop.hbase.integrationtestrebalanceandkillserverstargeted.txt  ------------------------------------------------------------------------------- test set: org.apache.hadoop.hbase.integrationtestrebalanceandkillserverstargeted ------------------------------------------------------------------------------- tests run: 1, failures: 1, errors: 0, skipped: 0, time elapsed: 206.538 sec <<< failure! testdataingest(org.apache.hadoop.hbase.integrationtestrebalanceandkillserverstargeted)  time elapsed: 206.395 sec  <<< failure! junit.framework.assertionfailederror: load failed with error code 1         at junit.framework.assert.fail(assert.java:50)         at org.apache.hadoop.hbase.ingestintegrationtestbase.runingesttest(ingestintegrationtestbase.java:98)         at org.apache.hadoop.hbase.integrationtestrebalanceandkillserverstargeted.testdataingest(integrationtestrebalanceandkillserverstargeted.java:121)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) ... org.apache.hadoop.hbase.integrationtestrebalanceandkillserverstargeted-output.txt has nothing in it. ",
        "label": 406
    },
    {
        "text": "add fast row key only scanning  instead of requiring a user to set up a scanner with any column and scan the table to gather all row keys while ignoring the column value we should have a fast and lightweight scanner that for example takes a \"null\" for the column list and then simply returns only the matching keys of all non-empty or deleted rows. filters should still be applicable. ",
        "label": 247
    },
    {
        "text": "improve increasingtoupperboundregionsplitpolicy to avoid too many regions  during some (admittedly artificial) load testing we found a large amount split activity, which we tracked down the increasingtoupperboundregionsplitpolicy. the current logic is this (from the comments):  \"regions that are on this server that all are of the same table, squared, times the region flush size or the maximum region split size, whichever is smaller\" so with a flush size of 128mb and max file size of 20gb, we'd need 13 region of the same table on an rs to reach the max size.  with 10gb file sized it is still 9 regions of the same table.  considering that the number of regions that an rs can carry is limited and there might be multiple tables, this should be more configurable. i think the squaring is smart and we do not need to change it. we could make the start size configurable and default it to the flush size add multiplier for the initial size, i.e. start with n * flushsize also change the default to start with 2*flush size of course one can override the default split policy, but these seem like simple tweaks. or we could instead set the goal of how many regions of the same table would need to be present in order to reach the max size. in that case we'd start with maxsize/goal^2. so if max size is 20gb and the goal is three we'd start with 20g/9 = 2.2g for the initial region size. michael stack, i'm especially interested in your opinion. ",
        "label": 286
    },
    {
        "text": "initiate lease recovery for outstanding wal files at the very beginning of recovery  at the beginning of recovery, master can send lease recovery requests concurrently for outstanding wal files using a thread pool.  each split worker would first check whether the wal file it processes is closed. thanks to nicolas liochon and jeffery discussion with whom gave rise to this idea. ",
        "label": 441
    },
    {
        "text": "npe in  rs status during rs shutdown  while hitting reload to see when a kill-initiated rs shutdown would make the web ui go away, i got a stack trace from an npe ",
        "label": 371
    },
    {
        "text": " hbase  add a means of scanning over all versions  currently, scanner will get cells of the timestamp specified on construction, or the cell of the nearest vintage. there should be a means of getting a scanner to run over every cell in hbase if only to figure whats in there. ",
        "label": 247
    },
    {
        "text": "loadbalancer should log table name when balancing per table  the load balancer logs lines like these: 2019-10-02 23:18:47,664 info  [46493_choreservice_6] balancer.stochasticloadbalancer - skipping load balancing because balanced cluster; total cost is 46.68964334022376, sum multiplier is 1087.0 min cost which need balance is 0.05 when balancing per table it would be useful if the table name was also printed in the log line. ",
        "label": 71
    },
    {
        "text": "local master backup sh doesn't start hmaster correctly  ./bin/local-master-backup.sh cannot start a backup hmaster. the error in log is a port conflict. after reading the file it seems an additional option is required to make it work: @@ -40,6 +40,7 @@  dn=$2  export hbase_ident_string=\"$user-$dn\"  hbase_master_args=\"\\  + -d hbase.master.port=`expr 16000 + $dn` \\  -d hbase.master.info.port=`expr 16010 + $dn` \\  -d hbase.regionserver.port=`expr 16020 + $dn` \\  -d hbase.regionserver.info.port=`expr 16030 + $dn` \\ ",
        "label": 393
    },
    {
        "text": "hconnection istableavailable returns true even with not all regions available   this function as per the java doc is supposed to return true iff \"all the regions in the table are available\". but if the table is still being created this function may return inconsistent results (for example, when a table with a large number of split keys is created). ",
        "label": 441
    },
    {
        "text": "rest schema modification throw null pointer exception  2010-10-21 14:41:38,462 error org.mortbay.log: /node_table_modify/schema  java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.bytes.tobytes(bytes.java:400)  at org.apache.hadoop.hbase.client.hbaseadmin.addcolumn(hbaseadmin.java:597)  at org.apache.hadoop.hbase.rest.schemaresource.update(schemaresource.java:153)  at org.apache.hadoop.hbase.rest.schemaresource.update(schemaresource.java:177)  at org.apache.hadoop.hbase.rest.schemaresource.post(schemaresource.java:204)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jersey.server.impl.model.method.dispatch.abstractresourcemethoddispatchprovider$responseoutinvoker._dispatch(abstractresourcemethoddispatchprovider.java:187)  at com.sun.jersey.server.impl.model.method.dispatch.resourcejavamethoddispatcher.dispatch(resourcejavamethoddispatcher.java:70)  at com.sun.jersey.server.impl.uri.rules.httpmethodrule.accept(httpmethodrule.java:279)  at com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept(sublocatorrule.java:121)  at com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept(righthandpathrule.java:136)  at com.sun.jersey.server.impl.uri.rules.sublocatorrule.accept(sublocatorrule.java:121)  at com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept(righthandpathrule.java:136)  at com.sun.jersey.server.impl.uri.rules.resourceclassrule.accept(resourceclassrule.java:86)  at com.sun.jersey.server.impl.uri.rules.righthandpathrule.accept(righthandpathrule.java:136)  at com.sun.jersey.server.impl.uri.rules.rootresourceclassesrule.accept(rootresourceclassesrule.java:74)  at com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest(webapplicationimpl.java:1357)  at com.sun.jersey.server.impl.application.webapplicationimpl._handlerequest(webapplicationimpl.java:1289)  at com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest(webapplicationimpl.java:1239)  at com.sun.jersey.server.impl.application.webapplicationimpl.handlerequest(webapplicationimpl.java:1229)  at com.sun.jersey.spi.container.servlet.webcomponent.service(webcomponent.java:420)  at com.sun.jersey.spi.container.servlet.servletcontainer.service(servletcontainer.java:497)  at com.sun.jersey.spi.container.servlet.servletcontainer.service(servletcontainer.java:684)  at javax.servlet.http.httpservlet.service(httpservlet.java:820)  at org.mortbay.jetty.servlet.servletholder.handle(servletholder.java:511)  at org.mortbay.jetty.servlet.servlethandler.handle(servlethandler.java:390)  at org.mortbay.jetty.servlet.sessionhandler.handle(sessionhandler.java:182)  at org.mortbay.jetty.handler.contexthandler.handle(contexthandler.java:765)  at org.mortbay.jetty.handler.handlerwrapper.handle(handlerwrapper.java:152)  at org.mortbay.jetty.server.handle(server.java:326)  at org.mortbay.jetty.httpconnection.handlerequest(httpconnection.java:542)  at org.mortbay.jetty.httpconnection$requesthandler.content(httpconnection.java:943)  at org.mortbay.jetty.httpparser.parsenext(httpparser.java:843)  at org.mortbay.jetty.httpparser.parseavailable(httpparser.java:218)  at org.mortbay.jetty.httpconnection.handle(httpconnection.java:404)  at org.mortbay.jetty.bio.socketconnector$connection.run(socketconnector.java:228)  at org.mortbay.thread.queuedthreadpool$poolthread.run(queuedthreadpool.java:582) ",
        "label": 132
    },
    {
        "text": "hconnnectionmanager getregionserverwithretries doesn't call aftercall properly  hconnnectionmanager.getregionserverwithretries:           return callable.call();         } catch (throwable t) {           callable.shouldretry(t); shouldretry relies on the proper starttime and endtime to calculate the timeout value. however, callable.aftercall() is called in finally block. thus callable.calltimeout will be set to negative value in callable.shouldretry(). ",
        "label": 544
    },
    {
        "text": "if hbase starts stops often in less than hours  you end up with lots of store files  since we don't compact on open and close of regions, all regions that have edits will do a flush when the cluster is stopped so that if you do it a couple of times in a row it's easy to get more than 10 store files and it won't be compacted until the next day. on open, we should check that number and compact if needed. ",
        "label": 229
    },
    {
        "text": "high read write intensive regions may cause long crash recovery  compaction of high read loaded region may leave compacted files undeleted because of existing scan references: info org.apache.hadoop.hbase.regionserver.hstore - can't archive compacted file hdfs://hdfs-ha/hbase... because of either iscompactedaway=true or file has reference, isreferencedinreads=true, refcount=1, skipping for now if region is either high write loaded this happens quite often and region may have few storefiles and tons of undeleted compacted hdfs files. region keeps all that files (in my case thousands) untill graceful region closing procedure, which ignores existing references and drop obsolete files. it works fine unless consuming some extra hdfs space, but only in case of normal region closing. if region server crashes than new region server, responsible for that overfiling region, reads hdfs folder and try to deal with all undeleted files, producing tons of storefiles, compaction tasks and consuming abnormal amount of memory, wich may lead to outofmemory exception and further region servers crash. this stops writing to region because number of storefiles reach hbase.hstore.blockingstorefiles limit, forces high gc duty and may take hours to compact all files into working set of files. workaround is a periodically check hdfs folders files count and force region assign for ones with too many files. it could be nice if regionserver had a setting similar to hbase.hstore.blockingstorefiles and invoke attempt to drop undeleted compacted files if number of files reaches this setting. ",
        "label": 544
    },
    {
        "text": " hbck2  add a master web ui to show the problematic regions  on our cluster which based 2.2.0, we found one problem: there are some opened regions which had wrong regionserver in meta. the regionserver is not exist. we used hbck2 to fix them by the following steps. 1. disable table 2. bypass the stucked close region procedure (as the target regionserver is not exist) and disable table procedure. 3. setregionstate to closed. 4. settablestate to disabled. 5. enable table we found this problem by scan the hbase:meta. i thought we should add this feature to hbck2. the we can use hbck2 to find this problem. thanks. ",
        "label": 187
    },
    {
        "text": "put add methods should return this for ease of use  be consistant with get   makes for easier one-liners, and is similar to how get works. this patch also adds gettimestamp which is needed by transactional layer ",
        "label": 110
    },
    {
        "text": "drop the ' ' prefix from names under hbase rootdir after namespaces goes in  change it so instead of hbase.rootdir looking like this: drwxr-xr-x   5 stack  staff  170 aug  5 14:43 .data -rw-r--r--   1 stack  staff   12 aug  5 14:43 .hbase.id.crc -rw-r--r--   1 stack  staff   12 aug  5 14:43 .hbase.version.crc drwxr-xr-x   3 stack  staff  102 aug  5 14:43 .logs drwxr-xr-x   2 stack  staff   68 aug  5 14:43 .oldlogs drwxr-xr-x   3 stack  staff  102 aug  5 14:43 .tmp -rwxrwxrwx   1 stack  staff   42 aug  5 14:43 hbase.id -rwxrwxrwx   1 stack  staff    7 aug  5 14:43 hbase.version it looks like this: drwxr-xr-x   5 stack  staff  170 aug  5 14:43 data -rw-r--r--   1 stack  staff   12 aug  5 14:43 .hbase.id.crc -rw-r--r--   1 stack  staff   12 aug  5 14:43 .hbase.version.crc drwxr-xr-x   3 stack  staff  102 aug  5 14:43 logs drwxr-xr-x   2 stack  staff   68 aug  5 14:43 oldlogs drwxr-xr-x   3 stack  staff  102 aug  5 14:43 tmp -rwxrwxrwx   1 stack  staff   42 aug  5 14:43 hbase.id -rwxrwxrwx   1 stack  staff    7 aug  5 14:43 hbase.version also removes any root dir if it is present. ",
        "label": 314
    },
    {
        "text": "call blockuntilconnected when constructing zkasyncregistry temporary workaround   ",
        "label": 149
    },
    {
        "text": "a successful write to client write buffer may be lost or not visible  a client can do a write to a client side 'write buffer' if enabled via htable.setautoflush(false). now, assume a client puts value v under key k. two wrongs things can happen, violating the acid semantics of hbase given at: http://hbase.apache.org/acid-semantics.html 1) say the client fails immediately after the put succeeds. in this case, the put will be lost, violating the durability property: <quote> any operation that returns a \"success\" code (eg does not throw an exception) will be made durable. </quote> 2) say the client issues a read for k immediately after writing k. the put will be stored in the client side write buffer, while the read will go to the region server, returning an older value, instead of v, violating the visibility property: <quote>  when a client receives a \"success\" response for any mutation, that mutation  is immediately visible to both that client and any client with whom it later  communicates through side channels.  </quote> thanks,  tallat ",
        "label": 146
    },
    {
        "text": "master doesn't see regionserver edits because of clock skew  the streamy folks had a cluster where regionserver was 2 minutes in advance of the master. on split, regionserver would update .meta. with split info but scanners opened on the master wouldn't see the edits because they were being opened using current time. ",
        "label": 314
    },
    {
        "text": "catalogjanitor delete region info in meta during restore snapshot  today i meet a issue during restore snapshot. it can be reproduce in step below: 1. table t1 create a snapshot s1 successfully  2. region r1 in t1 split   3. catalogjanitor chore begin to work and found daughter do not have reference , so r1 can be deleted  4. restore snapshot s1 . restoresnapshothelper add region r1 to meta table  5. catalogjanitor delete r1 region info in meta which restoresnapshothelper just inserted .  6. restore snapshot finished. then we can found there is a hole in t1 after restore snapshot. data loss. ",
        "label": 309
    },
    {
        "text": "potential null object dereference in tablepermission equals   at line 326:         ((namespace == null && other.getnamespace() == null) ||          namespace.equals(other.getnamespace())) if namespace is null but other.getnamespace() is not null, we would deference null object. ",
        "label": 441
    },
    {
        "text": "delete the patches in hbase protocol shaded module  as now we will apply the patch in the hbase-thirdparty repo. ",
        "label": 277
    },
    {
        "text": "refactor testreplicationsyncuptool  especially that testreplicationsyncuptoolwithbulkloadeddata overrides a test method, which is a bit hard to change in the future. ",
        "label": 149
    },
    {
        "text": "compaction requests should be prioritized to prevent blocking  while testing the write capacity of a 4 machine hbase cluster we were getting long and frequent client pauses as we attempted to load the data. looking into the problem we'd get a relatively large compaction queue and when a region hit the \"hbase.hstore.blockingstorefiles\" limit it would get block the client and the compaction request would get put on the back of the queue waiting for many other less important compactions. the client is basically stuck at that point until a compaction is done. prioritizing the compaction requests and allowing the request that is blocking other actions go first would help solve the problem. you can see the problem by looking at our log files: you'll first see an event such as a too many hlog which will put a lot of requests on the compaction queue. 2010-05-25 10:53:26,570 info org.apache.hadoop.hbase.regionserver.hlog: too many hlogs: logs=33, maxlogs=32; forcing flush of 22 regions(s): responsecounts,rs_6ezzltdwhgitwhy,1274232223324, responses,rs_0qhkl5rumpcbx3k-1274213057242,1274513189592, responses,rs_1anyntegjzvishw-12742177419 21,1274511001873, responses,rs_1hq4ug5bdolayue-1274216757425,1274726323747, responses,rs_1y7sbqstszrye7a-1274328697838,1274478031930, responses,rs_1zh5tb5odw4bvlm-1274216239894,1274538267659, responses,rs_3bhc4kyom3q72yc-1274290546987,1274502062319, responses,rs_3ra9babmaxfavbk-127421457 9958,1274381552543, responses,rs_6sdrgnuyyld3or6-1274219941155,1274385453586, responses,rs_8agcemwbi6mzuoq-1274306857429,1274319602718, responses,rs_8c8t9dn47uwtg1s-1274215381765,1274289112817, responses,rs_8j5wmdmkmjxzk6g-1274299593861,1274494738952, responses,rs_8e5sz0hefpadb6c-1274288 641459,1274495868557, responses,rs_8rjcnmbxpkzi896-1274306981684,1274403047940, responses,rs_9fs3vedcyrf0kx2-1274245971331,1274754745013, responses,rs_9ozgptxo31npv3c-1274214027769,1274396489756, responses,rs_a3fdo2jhqwuy37c-1274209228660,1274399508186, responses,rs_a3ljvxwtj29mhva-12742 then you see the too many log files: 2010-05-25 10:53:31,364 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region responses-index,--1274799047787--r_cbkrgxx0fdwjpso,1274804575862/783020138 because: regionserver/192.168.0.81:60020.cacheflusher 2010-05-25 10:53:32,364 warn org.apache.hadoop.hbase.regionserver.memstoreflusher: region responses-index,--1274799047787--r_cbkrgxx0fdwjpso,1274804575862 has too many store files, putting it back at the end of the flush queue. which leads to this: 2010-05-25 10:53:27,061 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 60 on 60020' on region responses-index,--1274799047787--r_cbkrgxx0fdwjpso,1274804575862: memstore size 128.0m is >= than blocking 128.0m size 2010-05-25 10:53:27,061 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 84 on 60020' on region responses-index,--1274799047787--r_cbkrgxx0fdwjpso,1274804575862: memstore size 128.0m is >= than blocking 128.0m size 2010-05-25 10:53:27,065 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 1 on 60020' on region responses-index,--1274799047787--r_cbkrgxx0fdwjpso,1274804575862: memstore size 128.0m is >= than blocking 128.0m size once the compaction / split is done a flush is able to happen which unblocks the ipc allowing writes to continue. unfortunately this process can take upwards of 15+ minutes (the specific case shown here from our logs took about 4 minutes). ",
        "label": 232
    },
    {
        "text": "web ui shows two meta regions  running 0.90@r1052112 i see two regions for meta on the same server. both have start key '' and end key ''. things seem to work ok, but it's very strange. ",
        "label": 314
    },
    {
        "text": "usability improvement to htablepool  to improve the usability of the htablepool the implementation should not rely on the user returning the connection to the pool but rather do that transparently when user closes the htableimplementation it got. to do that a htableimplementation proxy implementation should be returned that wraps a htable object and holds a reference to the pool. when the client close the proxy it will actually automatically return the wrapped htable back in pool to be reused. in this case the method htablepool.puttable don't need to be public ",
        "label": 120
    },
    {
        "text": "if clocks are way off  then we can have daughter split come before rather than after its parent in  meta   on the jon gray cluster, his clocks are skewed badly. i see weird stuff in .meta. 2008-06-25 14:57:57,728 debug org.apache.hadoop.hbase.hmaster: hmaster.metascanner regioninfo: {regionname: items,823ce1e3-d414-474f-ac70-c4081cecef0f,1214416614697, startkey: <823ce1e3-d414-474f-ac70-c4081cecef0f>, endkey: <86f8df20-e237-4bb3-9748-88cef892bd70>, encodedname: 1157924217, offline: true, split: true, tabledesc: {name: items, families: {cfrecs:={name: cfrecs, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, clusters:={name: clusters, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, content:={name: content, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, readby:={name: readby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, receivedby:={name: receivedby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, savedby:={name: savedby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, sentby:={name: sentby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}}}}, server: 192.168.249.223:60020, startcode: 1214406344634 2008-06-25 14:57:57,732 debug org.apache.hadoop.hbase.hmaster: hmaster.metascanner regioninfo: {regionname: items,823ce1e3-d414-474f-ac70-c4081cecef0f,1214416641213, startkey: <823ce1e3-d414-474f-ac70-c4081cecef0f>, endkey: <83fca0e2-f324-4f9e-99c1-1fdbeff63b3d>, encodedname: 541300165, tabledesc: {name: items, families: {cfrecs:={name: cfrecs, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, clusters:={name: clusters, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, content:={name: content, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, readby:={name: readby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, receivedby:={name: receivedby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, savedby:={name: savedby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, sentby:={name: sentby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}}}}, server: 192.168.249.220:60020, startcode: 1214424347649 2008-06-25 14:57:57,738 debug org.apache.hadoop.hbase.hmaster: hmaster.metascanner regioninfo: {regionname: items,823ce1e3-d414-474f-ac70-c4081cecef0f,1214434560891, startkey: <823ce1e3-d414-474f-ac70-c4081cecef0f>, endkey: <9066d4f3-314b-4d9c-90e8-7aa08a52fdd4>, encodedname: 1673833201, offline: true, split: true, tabledesc: {name: items, families: {cfrecs:={name: cfrecs, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, clusters:={name: clusters, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, content:={name: content, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, readby:={name: readby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, receivedby:={name: receivedby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, savedby:={name: savedby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, sentby:={name: sentby, max versions: 2, compression: none, in memory: false, max length: 2147483647, bloom filter: none}}}}, server: 192.168.249.221:60020, startcode: 1214406358315 thats 3 regions with same start code; 2 are offline. looking at the regionids \u2013 these are timestamps \u2013 i see that they don't jibe with how they should be aligned. parents should come before daughters in timestamps. looking at clocks on cluster, they are badly skewed: {code <st^ack> [hbase@mb0 logs]$ for i in 0 1 2 3 4 5 6 7 8 9; do ssh hb$i \"hostname; date\"; done <st^ack> hb0.streamy.com <st^ack> wed jun 25 16:47:29 pdt 2008 <st^ack> hb1.streamy.com <st^ack> wed jun 25 11:47:39 pdt 2008 <st^ack> hb2.streamy.com <st^ack> wed jun 25 11:47:40 pdt 2008 <st^ack> hb3.streamy.com <st^ack> wed jun 25 11:47:26 pdt 2008 <st^ack> hb4.streamy.com <st^ack> wed jun 25 11:47:35 pdt 2008 <st^ack> hb5.streamy.com <st^ack> wed jun 25 16:47:29 pdt 2008 <st^ack> hb6.streamy.com <st^ack> wed jun 25 16:47:29 pdt 2008 <st^ack> hb7.streamy.com <st^ack> wed jun 25 16:47:29 pdt 2008 <st^ack> hb8.streamy.com <st^ack> wed jun 25 16:47:30 pdt 2008 <st^ack> hb9.streamy.com <st^ack> wed jun 25 16:47:30 pdt 2008 {code} looking at split code, looks like the regionserver sets the regionid/timestamp on the new daughter regions inside in the hregioninfo constructor that gets called when splitting:     this.regionid = system.currenttimemillis(); daughters update the .meta. table; they need to have a basic check that they are not inserting with a timestamp that is older than the parent they are splitting. this is a bit like hbase-609 ",
        "label": 314
    },
    {
        "text": "kv size metric went missing from storescanner   in trunk due to the metric refactor, at least the kv size metric went missing.  see this code in storescanner.java:     } finally {       if (cumulativemetric > 0 && metric != null) {       }     } just an empty if statement, where the metric used to be collected. ",
        "label": 154
    },
    {
        "text": "total number of regions in transition number on ui incorrect  total number of regions in transition shows 100 when there are 100 or more regions in transition. ",
        "label": 323
    },
    {
        "text": "javac hangs compiling hbase example module since namespaces went in  javac hangs loading hregion.class. if i remove the ns changes \u2013 the import of tablename \u2013 i can get it to compile again. digging. more detail to follow. ",
        "label": 314
    },
    {
        "text": "the meta can hold an entry for a region with a different server name from the one actually in the assignmentmanager thus making the region inaccessible   regionstate rit = this.services.getassignmentmanager().isregionintransition(e.getkey());             servername addressfromam = this.services.getassignmentmanager()                 .getregionserverofregion(e.getkey());             if (rit != null && !rit.isclosing() && !rit.ispendingclose()) {               // skip regions that were in transition unless closing or               // pending_close               log.info(\"skip assigning region \" + rit.tostring());             } else if (addressfromam != null                 && !addressfromam.equals(this.servername)) {               log.debug(\"skip assigning region \"                     + e.getkey().getregionnameasstring()                     + \" because it has been opened in \"                     + addressfromam.getservername());               } in servershutdownhandler we try to get the address in the am. this address is initially null because it is not yet updated after the region was opened .i.e. the call back after node deletion is not yet done in the master side.  but removal from rit is completed on the master side. so this will trigger a new assignment.  so there is a small window between the online region is actually added in to the online list and the servershutdownhandler where we check the existing address in am. ",
        "label": 544
    },
    {
        "text": "clearer warning message when connecting a non secure hbase client to a secure hbase server  when a connection from a non secure-rpc-engine  client is attempted the warning message you get is related to version  mismatch: mar 28, 3:27:13 pm warn org.apache.hadoop.ipc.secureserver incorrect  header or version mismatch from 172.29.82.121:43849 got version 3  expected version 4 while this is true, it isn't as useful as it could be. a more specific error message warning end users that they're connecting with a non-secure client may be more useful. ",
        "label": 409
    },
    {
        "text": "keyvalue methods throw nullpointerexception instead of illegalargumentexception during parameter sanity check  methods of org.apache.hadoop.hbase.keyvalue  public static int getdelimiter(final byte [] b, int offset, final int length, final int delimiter)  public static int getdelimiterinreverse(final byte [] b, final int offset, final int length, final int delimiter)  throw nullpointerexception instead of illegalargumentexception when byte array b is check for null - which is very bad practice!  please refactor this because this can be very misleading. ",
        "label": 314
    },
    {
        "text": "update javadoc builder in pom to exclude empty packages in user api javadoc  there are many packages that have no apis exposed but are present in the javadoc after the @interfaceaudience.public apis. make them go away. ",
        "label": 248
    },
    {
        "text": "add  performancetest for append  checkand   in a sibling issue we add incrementperformancetest which is handy checking increment performance. make a general tool to do all check-and-set type ops like increment, append, checkandput, checkanddelete... just to make sure no regression in future. ",
        "label": 314
    },
    {
        "text": "splitlogworker exited due to concurrentmodificationexception  in playing with 0.96 code on a live cluster, found this issue: 2012-07-03 12:13:32,572 error org.apache.hadoop.hbase.regionserver.splitlogworker: unexpected error  java.util.concurrentmodificationexception  at java.util.treemap$privateentryiterator.nextentry(treemap.java:1100)  at java.util.treemap$valueiterator.next(treemap.java:1145)  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter$outputsink.closelogwriters(hlogsplitter.java:1330)  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter$outputsink.finishwritingandclose(hlogsplitter.java:1221)  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlogfiletotemp(hlogsplitter.java:441)  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlogfiletotemp(hlogsplitter.java:369)  at org.apache.hadoop.hbase.regionserver.splitlogworker$1.exec(splitlogworker.java:113)  at org.apache.hadoop.hbase.regionserver.splitlogworker.grabtask(splitlogworker.java:276)  at org.apache.hadoop.hbase.regionserver.splitlogworker.taskloop(splitlogworker.java:197)  at org.apache.hadoop.hbase.regionserver.splitlogworker.run(splitlogworker.java:164)  at java.lang.thread.run(thread.java:662)  2012-07-03 12:13:32,575 info org.apache.hadoop.hbase.regionserver.splitlogworker: splitlogworker ****.cloudera.com,57020,1341335300238 exiting ",
        "label": 242
    },
    {
        "text": "add uts for the new lock type peer  ",
        "label": 187
    },
    {
        "text": "replace stringbuffer with stringbuilder for hbase common  replace stringbuffer with non-synchronized version stringbuilder ",
        "label": 130
    },
    {
        "text": "splitting a region does not support the hbase rs evictblocksonclose config when closing source region  i have a table with bucket cache turned on and hbase.rs.evictblocksonclose set to false. i split a region and watched that the closing of the source region did not complete until the bucketcache was flushed for that region. ",
        "label": 441
    },
    {
        "text": "opts arguments are duplicated  some of the command line arguments constructed in the bash scripts are getting duplicated here is what my hmaster process looks like with a ps aux | grep java. /library/java/home/bin/java  -xmx1000m  -ea -xx:+heapdumponoutofmemoryerror -xx:+useconcmarksweepgc -xx:+cmsincrementalmode  -ea -xx:+heapdumponoutofmemoryerror -xx:+useconcmarksweepgc -xx:+cmsincrementalmode  -ea -xx:+heapdumponoutofmemoryerror -xx:+useconcmarksweepgc -xx:+cmsincrementalmode  -dcom.sun.management.jmxremote.ssl=false  -dcom.sun.management.jmxremote.authenticate=false  -dcom.sun.management.jmxremote.port=10101  -dcom.sun.management.jmxremote.ssl=false  -dcom.sun.management.jmxremote.authenticate=false  -dcom.sun.management.jmxremote.port=10101  -dcom.sun.management.jmxremote.ssl=false  -dcom.sun.management.jmxremote.authenticate=false  -dcom.sun.management.jmxremote.port=10101  -dhbase.log.dir=/users/tims/workspace/hbase-trunk/bin/../logs  -dhbase.log.file=hbase-tims-master-grassmann.local.log  -dhbase.home.dir=/users/tims/workspace/hbase-trunk/bin/..  -dhbase.id.str=tims -dhbase.root.logger=info,drfa  -classpath <blablablablabla> org.apache.hadoop.hbase.master.hmaster start this wouldn't really be a problem except if you try to add a java agent in the hbase-env.sh like so: export hbase_master_opts=\"$hbase_master_opts $hbase_jmx_base -dcom.sun.management.jmxremote.port=10101 -javaagent:lib/helloworldagent.jar\" it adds the option 3 times and it starts three java agents. my example agent print hello world once per second, but i end up with three hello world lines per second. i attached my helloworldagent.jar to demonstrate. ",
        "label": 314
    },
    {
        "text": "splitlogmanager   log the exception when failed to finish split log file  we should log the exception itself too:         try {           hlogsplitter.moverecoverededitsfromtemp(tmpname, logfile, conf);         } catch (ioexception e) {           log.warn(\"could not finish splitting of log file \" + logfile);           return status.err;         } ",
        "label": 242
    },
    {
        "text": "hbase removed compactionqueuesize metric  per rs compactionqueuesize metric is no longer reported. hbase-2165 (r932137) seems to have reverted more stuff than just the fragmentation display alone. would be nice to have this added back. ",
        "label": 386
    },
    {
        "text": "rewrite testclientoperationtimeout so we do not timeout when creating table  this should be test issue. we set the timeout to 500ms and the retry number to 1, so on a slow machine, we may end up in the setup method where we want to create a table... i think we should separated the connection which is used for testing, and set the configs only for this connection. ",
        "label": 149
    },
    {
        "text": "regions unbalanced when adding new node  when adding a new regionserver to a cluster, the new rs will receive some regions but not enough to actually be considered balanced. to recreate, just take an rs offline, allow regions to be reassigned, and then bring it back up. master will get itself into a broken, stuck state where it continuously outputs a line like this: 2009-08-03 12:54:57,812 debug org.apache.hadoop.hbase.master.regionmanager: server dn4,60020,1249329081079 will be unloaded for balance. server load: 341 avg: 318.0, regions can be moved: 55 this line is output every 3 seconds and never stops until another rs joins/leaves the cluster. making this a blocker because when your new rs only gets some regions (in my case, about half as many as it should have), then all new regions will be assigned to that rs. this basically destroys any possibility for good load distribution with new data. ",
        "label": 247
    },
    {
        "text": "simple log roll snapshot  this snapshot will likely be the cheapest snapshot taking method \u2013 it rolls or drops a marker in the hlog when a snapshot request is made. this approach can be seen as an improvement upon copytable/export with similar consistency guarantees. specifically, there will be \"ragged edges\", and happens-before relation on clients may not be consistent from a client's point of view but it will be useable for many backup for disaster recovery use cases. for example: if client 1 writes row a to region a and then row b to region b, a client happens-before enforcing snapshot would only be allowed to have neither a nor b, only a, or both a and b. having only b should not be possible. this outcome would be possible with the log-roll snapshot. ",
        "label": 248
    },
    {
        "text": "names in the filter interface are confusing  i don't like the names of the filter methods in rowfilterinterface. they don't really tell how the methods are being used in the implementation of scanners. i'd like to change: filter(text) to filterrow(...) filter(text, text, byte[]) to filtercolumn(...)  and the worst one is filternotnull(sortedmap<text, byte[]>). this should be filterrow(text, sortedmap<text, byte[]>) (so we add the row key/). it may be nice to have timestamps in the methods as well? also the java doc could be cleaned and improved to tell how the filtering is implemented (check rows keys first, then check each individual columns, finally check the assembled row) upon positive feedback, and i'll create a patch. ",
        "label": 110
    },
    {
        "text": "address issues found by error prone in hbase common  we should address the new compilation errors found by running with -perrorprone. can convert this to a top-level task and add subtasks for modules if desired (in which case, link it back to parent issue hbase-12187, please) ",
        "label": 320
    },
    {
        "text": "parallelize load of  regioninfo files in diagnostic repair portion of hbck   on heavily loaded hdfs's some dfs nodes may not respond quickly and backs off for 60s before attempting to read data from another datanode. portions of the information gathered from hdfs (.regioninfo files) are loaded serially. with hbase with clusters with 100's, or 1000's, or 10000's regions encountering these 60s delay blocks progress and can be very painful. there is already some parallelization of portions of the hdfs information load operations and the goal here is move the reading of .regioninfos into the parallelized sections.. ",
        "label": 248
    },
    {
        "text": "support setting up two clusters with a and s state  ",
        "label": 149
    },
    {
        "text": "provide a put api that adds the provided family  qualifier  value without copying  in the put api, we have addimmutable()  /**    * see {@link #addcolumn(byte[], byte[], byte[])}. this version expects    * that the underlying arrays won't change. it's intended    * for usage internal hbase to and for advanced client applications.    */   public put addimmutable(byte [] family, byte [] qualifier, byte [] value) but in the implementation, the family, qualifier and value are still being copied locally to create kv. hopefully we should provide an api that truly uses immutable family, qualifier and value. ",
        "label": 490
    },
    {
        "text": " copytable  unexpected behavior if  starttime is not specifed but  endtime is   if one uses copytable and specifies only an endtime, i'd expect to include all rows from unix epoch time upto the specified endtime. instead, it copies all the rows. the workaround for copies with this kind of range is to specify --startime=1 (note not --starttime=0), which is also unintuitive. ",
        "label": 358
    },
    {
        "text": "metrics support for cluster load history  emissions and graphs  hbase should write loadings on a period in a format that is amenable to tools like ganglia (rrd). master can dump cluster loadings and averages. regionservers would report their own loadings. should exploit the work up in hadoop for doing this kinda thing (gangliacontext) where it makes sense. extra browning points if user can optionally enable display of graphs in the hbase ui (jrobin). ",
        "label": 314
    },
    {
        "text": "backport hbase  remove synchronization block from metatablemetrics and fix lossycounting algorithm  to branch  ",
        "label": 38
    },
    {
        "text": "new logo for hbase  0.90 had the old bass clef hbase logo. 0.92 had a purple version of our logo. 0.94 had the golden gate international engineering coloring. here is one for 0.96. unless objection, will check it in. it is the 0.94 logo \u2013 same colors, same font \u2013 just moved around some w/ the white space filled in in the h and the b. ",
        "label": 314
    },
    {
        "text": "intermittent testhbasefsck testmissingregioninfoqualifier failure due to nullpointerexception  from https://builds.apache.org/job/hbase-0.96-hadoop2/105/testreport/junit/org.apache.hadoop.hbase.util/testhbasefsck/testmissingregioninfoqualifier/ : java.lang.nullpointerexception at org.apache.hadoop.hbase.util.testhbasefsck$4.processrow(testhbasefsck.java:1891) at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:179) at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:105) at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:65) at org.apache.hadoop.hbase.util.testhbasefsck.testmissingregioninfoqualifier(testhbasefsck.java:1887) looks like metascanner.gethregioninfo() returned null below:         public boolean processrow(result rowresult) throws ioexception {          if(!metascanner.gethregioninfo(rowresult).gettable().issystemtable()) { ",
        "label": 441
    },
    {
        "text": "hcm locaterootregion fails hard on  connection refused   while running some tests on replication, i saw that our client does something dumb if it tries to contact a dead region server that held the root region in hcm.locaterootregion. will post stack trace in a comment. the problem here is that we don't retry at all, the exception will come straight out of hcm like it's the end of the world. ",
        "label": 229
    },
    {
        "text": "convert existing mapfile  or hstorefile  to conform to new interface  ",
        "label": 314
    },
    {
        "text": "add num calls in priority and general queue to rs ui  1.2 added the queue size. we should add the number of calls in the queue. ",
        "label": 251
    },
    {
        "text": "htablepool constructor not reading config files in certain cases  creating a htablepool can issue two behaviour depanding on the constructor called. case 1: loads the configs from hbase-site  public htablepool() { this(hbaseconfiguration.create(), integer.max_value); } calling this with null values for configuration:   public htablepool(final configuration config, final int maxsize) { this(config, maxsize, null, null); } will issue:  public htablepool(final configuration config, final int maxsize,  final htableinterfacefactory tablefactory, pooltype pooltype) {  // make a new configuration instance so i can safely cleanup when  // done with the pool.  this.config = config == null ? new configuration() : config; which does not read the hbase-site config files as hbaseconfiguration.create() does. i've tracked this problem to all versions of hbase. ",
        "label": 212
    },
    {
        "text": "write can be totally blocked temporarily by a write heavy region  write to a region can be blocked temporarily if the memstore of that region reaches the threshold(hbase.hregion.memstore.block.multiplier * hbase.hregion.flush.size) until the memstore of that region is flushed. for a write-heavy region, if its write requests saturates all the handler threads of that rs when write blocking for that region occurs, requests of other regions/tables to that rs also can't be served due to no available handler threads...until the pending writes of that write-heavy region are served after the flush is done. hence during this time period, from the rs perspective it can't serve any request from any table/region just due to a single write-heavy region. this sounds not very reasonable, right? maybe write requests from a region can only be served by a sub-set of the handler threads, and then write blocking of any single region can't lead to the scenario mentioned above? comment? ",
        "label": 203
    },
    {
        "text": "remove use of reflection for user getshortname  the time we spend looking up a user's short name with user#getshortname in hot security code paths is mostly spent in class#getmethod, ~70%. this is only ~1% of overall cpu time but is the bulk of time spent in accesscontroller#preput, for example. ",
        "label": 38
    },
    {
        "text": "npe in memstoreflusher  every now and again in a 0.90.1-2 load run i get a npe on this line:  if (bestanyregion.memstoresize.get() > 2 * bestflushableregion.memstoresize.get()) { ",
        "label": 547
    },
    {
        "text": "old  meta   tableinfo file kills hmaster  in pre-0.96, .meta. has .tableinfo files which refer to .meta. on startup, master tries to read it and aborts since the table name has changed. the .meta. .tableinfo files are not being created in 0.94.x (fixed for 96 in hbase-6971; but this can be reproduced when migrating from 0.92 -> 0.94 -> 0.96. our old users would be affected by this. java.lang.illegalargumentexception: .meta. no longer exists. the table has been renamed to hbase:meta at org.apache.hadoop.hbase.tablename.valueof(tablename.java:291) at org.apache.hadoop.hbase.tablename.valueof(tablename.java:283) at org.apache.hadoop.hbase.htabledescriptor.readfields(htabledescriptor.java:960) at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:131) at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:101) at org.apache.hadoop.hbase.htabledescriptor.parsefrom(htabledescriptor.java:1407) at org.apache.hadoop.hbase.util.fstabledescriptors.readtabledescriptor(fstabledescriptors.java:521) at org.apache.hadoop.hbase.util.fstabledescriptors.createtabledescriptorfortabledirectory(fstabledescriptors.java:707) at org.apache.hadoop.hbase.util.fstabledescriptors.createtabledescriptor(fstabledescriptors.java:683) at org.apache.hadoop.hbase.util.fstabledescriptors.createtabledescriptor(fstabledescriptors.java:670) at org.apache.hadoop.hbase.master.masterfilesystem.checkrootdir(masterfilesystem.java:485) at org.apache.hadoop.hbase.master.masterfilesystem.createinitialfilesystemlayout(masterfilesystem.java:145) at org.apache.hadoop.hbase.master.masterfilesystem.<init>(masterfilesystem.java:129) at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:761) ",
        "label": 199
    },
    {
        "text": "bad imports in testuser from hbase change  the hbase-4515 patch mistakenly added unused imports to testuser: import org.apache.hadoop.security.unixusergroupinformation; import org.apache.hadoop.security.usergroupinformation; both should be removed. unixusergroupinformation is specific to non-secure hadoop and importing it will break compilation against secure hadoop variants (0.20.205, 0.22, 0.23). ",
        "label": 180
    },
    {
        "text": "upgrade the jamon runtime dependency  current version of hbase uses mpl 1.1 which has legal restrictions. newer versions of jamon-runtime appear to be mpl 2.0. hbase should upgrade to a safer licensed version of jamon.  2.4.0 is mpl 1.1 : http://grepcode.com/snapshot/repo1.maven.org/maven2/org.jamon/jamon-runtime/2.4.0  2.4.1 is mpl 2.0 : http://grepcode.com/snapshot/repo1.maven.org/maven2/org.jamon/jamon-runtime/2.4.1 here\u2019s a comparison of the equivalent sections of the respective licenses dealing w/ termination: mpl 1.1 - section 8 (termination) subsection 2:  8.2. if you initiate litigation by asserting a patent infringement claim (excluding declatory judgment actions) against initial developer or a contributor (the initial developer or contributor against whom you file such action is referred to as \"participant\") alleging that:  such participant's contributor version directly or indirectly infringes any patent, then any and all rights granted by such participant to you under sections 2.1 and/or 2.2 of this license shall, upon 60 days notice from participant terminate prospectively, unless if within 60 days after receipt of notice you either: agree in writing to pay participant a mutually agreeable reasonable royalty for your past and future use of modifications made by such participant, or (ii) withdraw your litigation claim with respect to the contributor version against such participant. if within 60 days of notice, a reasonable royalty and payment arrangement are not mutually agreed upon in writing by the parties or the litigation claim is not withdrawn, the rights granted by participant to you under sections 2.1 and/or 2.2 automatically terminate at the expiration of the 60 day notice period specified above.  any software, hardware, or device, other than such participant's contributor version, directly or indirectly infringes any patent, then any rights granted to you by such participant under sections 2.1(b) and 2.2(b) are revoked effective as of the date you first made, used, sold, distributed, or had made, modifications made by that participant.  mpl 2.0 - section 5 (termination) subsection 2:  5.2. if you initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a contributor version directly or indirectly infringes any patent, then the rights granted to you by any and all contributors for the covered software under section 2.1 of this license shall terminate. ",
        "label": 38
    },
    {
        "text": "flush   compaction handling from secondary region replicas  we should be handling flushes and compactions from the primary region replica being replayed to the secondary region replica via hbase-11568. some initial thoughts for how can this be done is discussed in hbase-11183. more details will come together with the patch. ",
        "label": 155
    },
    {
        "text": "oobe in prefix key encoding  from hbase-10936. testzerobyte[1](org.apache.hadoop.hbase.io.encoding.testdatablockencoders)  time elapsed: 0.008 sec  <<< error! java.lang.indexoutofboundsexception         at java.io.bytearrayoutputstream.write(bytearrayoutputstream.java:138)         at java.io.dataoutputstream.write(dataoutputstream.java:107)         at org.apache.hadoop.hbase.util.bytebufferutils.copybuffertostream(bytebufferutils.java:163)         at org.apache.hadoop.hbase.util.bytebufferutils.movebuffertostream(bytebufferutils.java:147)         at org.apache.hadoop.hbase.io.encoding.prefixkeydeltaencoder.addkv(prefixkeydeltaencoder.java:70)         at org.apache.hadoop.hbase.io.encoding.prefixkeydeltaencoder.internalencodekeyvalues(prefixkeydeltaencoder.java:87)         at org.apache.hadoop.hbase.io.encoding.buffereddatablockencoder.encodekeyvalues(buffereddatablockencoder.java:591)         at org.apache.hadoop.hbase.io.encoding.prefixkeydeltaencoder.encodekeyvalues(prefixkeydeltaencoder.java:44)         at org.apache.hadoop.hbase.io.encoding.testdatablockencoders.encodebytes(testdatablockencoders.java:100)         at org.apache.hadoop.hbase.io.encoding.testdatablockencoders.testalgorithm(testdatablockencoders.java:112)         at org.apache.hadoop.hbase.io.encoding.testdatablockencoders.testencodersondataset(testdatablockencoders.java:423)         at org.apache.hadoop.hbase.io.encoding.testdatablockencoders.testzerobyte(testdatablockencoders.java:208) see attached test case. ",
        "label": 544
    },
    {
        "text": "reset loadbalancer back to stochasticloadbalancer  it seems like hbase-7296 changed the loadbalancer class by mistake, removing a good deal of functionality. ",
        "label": 314
    },
    {
        "text": "typo in book chapter architecture html  erroneous apostrophe in https://hbase.apache.org/book/architecture.html#arch.overview have created a patch, it's my first. any recommendations for improving the report or the patch are welcomed. thanks ",
        "label": 374
    },
    {
        "text": " webui  hmaster webui should display the number of regions a table has   pre-0.96/trunk hbase displayed the number of regions per table in the table listing. would be good to have this back. ",
        "label": 41
    },
    {
        "text": "change the comment in hbaseclasstestrule to reflect change in default test timeouts  the default timeout is 13 minutes. change the comment accordingly.         // all tests have a 10minute timeout. ",
        "label": 363
    },
    {
        "text": " migration  this message 'java io ioexception  install x of hbase and run its migration first' is useless  you'll see above message after you've committed to a new version of hadoop. you won't be able to go back. ",
        "label": 218
    },
    {
        "text": "implement table checkandput   this task is to implement table#checkandput() method ",
        "label": 441
    },
    {
        "text": " hbck2  reference file check fails if compiled with old version but check against new  the innocuous looking change hbase-22721 refactor hbasefsck: move the inner class out made it so some hbck2 filesystem checks fail if hbck2 was compiled with a version from before this change but run against a newer version. in name of making hbck2 able to run against broader swath of hbases, let me make some changes. here is the exception you'd see: 05:22:09.920 [main] info  org.apache.hadoop.hbase.client.connectionimplementation - closing master protocol: masterservice exception in thread \"main\" java.lang.nosuchmethoderror: org.apache.hadoop.hbase.util.fsutils.gettablestorefilepathmap(lorg/apache/hadoop/fs/filesystem;lorg/apache/hadoop/fs/path;lorg/apache/hadoop/fs/pathfilter;ljava/util/concurrent/executorservice;lorg/apache/hadoop/hbase/util/hbasefsck$errorreporter;)ljava/util/map;         at org.apache.hbase.hbck1.hbasefsck.offlinereferencefilerepair(hbasefsck.java:1191)         at org.apache.hbase.hbck1.hbasefsck.offlinehbck(hbasefsck.java:846)         at org.apache.hbase.filesystemfsck.fsck(filesystemfsck.java:103)         at org.apache.hbase.hbck2.docommandline(hbck2.java:560)         at org.apache.hbase.hbck2.run(hbck2.java:470)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:76)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:90)         at org.apache.hbase.hbck2.main(hbck2.java:620) while the 'fix' is for hbase-operator-tools, this area up in hbase needs commentary and deprecation. it was wonky from the get-go and we need to be explicit that this code is not to be enhanced and is going away. will file a companion issue for hbase changes. ",
        "label": 314
    },
    {
        "text": "get counter broken in shell  hbase(main):010:0> incr 't', 'r1', 'f1:c1'  counter value = 2 hbase(main):011:0> get_counter 't', 'r1', 'f1:c1' error: undefined method `first' for #<#<class:01x79f7abae>:0x73286b10> ",
        "label": 453
    },
    {
        "text": "integration test classes are not part of the default hbase classpath  expected behavior:  following the instructions here:  http://hbase.apache.org/book/hbase.tests.html#integration.tests after pulling down the latest bits from trunk and running,  > mvn clean compile test-compile package install assembly:assembly -dskiptests i should be able to run integration tests with the following command:  > bin/hbase --config /path/to/configs org.apache.hadoop.hbase.integrationtestsdriver according to the documentation, this should kick off the tests. observed behavior:  upon executing   > bin/hbase --config /path/to/configs org.apache.hadoop.hbase.integrationtestsdriver i see the following:  exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/hadoop/hbase/integrationtestsdriver  caused by: java.lang.classnotfoundexception: org.apache.hadoop.hbase.integrationtestsdriver  at java.net.urlclassloader$1.run(urlclassloader.java:202)  at java.security.accesscontroller.doprivileged(native method)  at java.net.urlclassloader.findclass(urlclassloader.java:190)  at java.lang.classloader.loadclass(classloader.java:306)  at sun.misc.launcher$appclassloader.loadclass(launcher.java:301)  at java.lang.classloader.loadclass(classloader.java:247) i can fix it by adding the following line to the hbase-env.sh file corresponding to the configs: export hbase_classpath=/path/to/project/hbase/hbase-it/target/test-classes/ this produces the correct output. ",
        "label": 155
    },
    {
        "text": "unexpected  when hlog sync  queue full  got the below exceptions log in case of a write heavy test 2014-05-07 11:29:56,417 error [main.append-pool1-t1] wal.fshlog$ringbuffereventhandler(1882): unexpected!!! java.lang.illegalstateexception: queue full  at java.util.abstractqueue.add(unknown source)  at org.apache.hadoop.hbase.regionserver.wal.fshlog$syncrunner.offer(fshlog.java:1227)  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1878)  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1)  at com.lmax.disruptor.batcheventprocessor.run(batcheventprocessor.java:133)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source) 2014-05-07 11:29:56,418 error [main.append-pool1-t1] wal.fshlog$ringbuffereventhandler(1882): unexpected!!! java.lang.arrayindexoutofboundsexception: 5  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1838)  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1)  at com.lmax.disruptor.batcheventprocessor.run(batcheventprocessor.java:133)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source) 2014-05-07 11:29:56,419 error [main.append-pool1-t1] wal.fshlog$ringbuffereventhandler(1882): unexpected!!! java.lang.arrayindexoutofboundsexception: 6  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1838)  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1)  at com.lmax.disruptor.batcheventprocessor.run(batcheventprocessor.java:133)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source) 2014-05-07 11:29:56,419 error [main.append-pool1-t1] wal.fshlog$ringbuffereventhandler(1882): unexpected!!! java.lang.arrayindexoutofboundsexception: 7  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1838)  at org.apache.hadoop.hbase.regionserver.wal.fshlog$ringbuffereventhandler.onevent(fshlog.java:1)  at com.lmax.disruptor.batcheventprocessor.run(batcheventprocessor.java:133)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)   in fshlog$syncrunner.offer we do blockingqueue.add() which throws exception as it is full. the problem is the below shown catch() we do not do any cleanup. this.syncrunners[index].offer(sequence, this.syncfutures, this.syncfuturescount);         attainsafepoint(sequence);         this.syncfuturescount = 0;       } catch (throwable t) {         log.error(\"unexpected!!!\", t);       } syncfuturescount is not getting reset to 0 and so the subsequent onevent() handling throws arrayindexoutofboundsexception. i think we should do the below   1. handle the exception and call cleanupoutstandingsyncsonexception() as in other cases of exception handling  2. instead of blockingqueue.add() use offer() ",
        "label": 314
    },
    {
        "text": "thrift server should have an option to bind to ip address  the thrift server can be bound to a port by calling ./bin/hbase thrift --port port as in all other server in hadoop everything is option to bind to a set ip address this will help with security issues example   ./bin/hbase rest --bind 0.0.0.0 as of now thrift is on all ip's on the server making it a security risk. so it would be nice to have the option to bind to a ip so we can limit access to the server. ",
        "label": 284
    },
    {
        "text": "testrskilledwhenmasterinitializing testcorrectnesswhenmasterfailover is flakey  failed here: https://builds.apache.org/job/hbase-0.95-on-hadoop2/169/testreport/junit/org.apache.hadoop.hbase.regionserver/testrskilledwhenmasterinitializing/testcorrectnesswhenmasterfailover/ and http://54.241.6.143/job/hbase-0.95-hadoop-2/579/org.apache.hbase$hbase-server/testreport/junit/org.apache.hadoop.hbase.regionserver/testrskilledwhenmasterinitializing/org_apache_hadoop_hbase_regionserver_testrskilledwhenmasterinitializing/ java.lang.exception: test timed out after 120000 milliseconds at java.lang.thread.sleep(native method) at org.apache.hadoop.hbase.zookeeper.zkassign.blockuntilnorit(zkassign.java:1002) at org.apache.hadoop.hbase.regionserver.testrskilledwhenmasterinitializing.testcorrectnesswhenmasterfailover(testrskilledwhenmasterinitializing.java:177) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:74) and with this: java.lang.nullpointerexception at org.apache.hadoop.hbase.regionserver.testrskilledwhenmasterinitializing.teardownafterclass(testrskilledwhenmasterinitializing.java:83) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:33) at org.junit.runners.parentrunner.run(parentrunner.java:309) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662) ",
        "label": 544
    },
    {
        "text": "servernotrunningexception coming out of assignrootandmeta kills the master  i'm surprised we still have issues like that and i didn't get a hit while googling so forgive me if there's already a jira about it. when the master starts it verifies the locations of root and meta before assigning them, if the server is started but not running you'll get this: 2011-09-23 04:47:44,859 warn org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation: remoteexception connecting to rs  org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hbase.ipc.servernotrunningexception: server is not running yet  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1038)  at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:771)  at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:257)  at $proxy6.getprotocolversion(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:419)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:393)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:444)  at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:349)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:969)  at org.apache.hadoop.hbase.catalog.catalogtracker.getcachedconnection(catalogtracker.java:388)  at org.apache.hadoop.hbase.catalog.catalogtracker.getmetaserverconnection(catalogtracker.java:287)  at org.apache.hadoop.hbase.catalog.catalogtracker.verifymetaregionlocation(catalogtracker.java:484)  at org.apache.hadoop.hbase.master.hmaster.assignrootandmeta(hmaster.java:441)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:388)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:282) i hit that 3-4 times this week while debugging something else. the worst is that when you restart the master it sees that as a failover, but none of the regions are assigned so it takes an eternity to get back fully online. ",
        "label": 186
    },
    {
        "text": "enable distributed log replay as default  enable 'distributed log replay' by default. depends on hfilev3 being enabled. ",
        "label": 314
    },
    {
        "text": "no basescanner means no gc'ing of split  offlined parent regions  i need to add back cleanup of split parents probably as a chore in master. can use new metareader code. ",
        "label": 314
    },
    {
        "text": "mark storefilescanner storefilereader as ia limitedprivate phoenix   do not see any reason why it is marked as ia.limitedprivate. it is not referenced in any cps. ",
        "label": 149
    },
    {
        "text": "add listtabledescriptors list tablename  method in asyncadmin  missed this one in asyncadmin. found this when implementing hbase-21718. ",
        "label": 540
    },
    {
        "text": " visibilitycontroller  stackable scanlabelgenerators  the scanlabelgenerator is used by the visibilitycontroller to assemble the effective label set for a user in the rpc context before processing any request. currently only one implementation of this interface can be installed, although which implementation to use can be specified in the site file. instead it should be possible to stack multiple implementations of this component the same way we do coprocessors, installed with explicit priority with ties broken by a counter, where those implementations installed later in the chain have an opportunity to modify the pending effective label set. ",
        "label": 46
    },
    {
        "text": "hbase backup restore phase  multiwal support  we need to support multiwal configurations.  to enable multi wal, use this code:     conf1.set(walfactory.wal_provider, \"multiwal\"); ",
        "label": 441
    },
    {
        "text": "experiment  temporarily disable balancer and a few others to see if root of crashed timedout jvms  looking at recent builds of 1.2, i see a few of the runs finishing with kills and notice that a jvm exited without reporting back state. running the hanging test finder, i can see at least that in one case that the balancer tests seem to be outstanding; looking in test output, seems to be still going on.... a few others are reported as hung but they look like they have just started running and are just killed by surefire. this issue is about trying to disable a few of the problematics like balancer tests to see if our overall stability improves. if so, i can concentrate on stabilizing these few tests. else will just undo the experiment and put the tests back on line. ",
        "label": 353
    },
    {
        "text": "will keep scheduling major compactions if last time one ran  we didn't   excerpt below has major compactions scheduled ever few hours but nothing to compact each time through \u2013 last run was a major compaction \u2013 and times are off. need to touch files if nothing to compact so major doesn't get scheduled again. 2008-12-20 02:41:03,933 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 249999 seconds 2008-12-20 05:27:43,933 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 259999 seconds 2008-12-20 08:14:23,932 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 269999 seconds 2008-12-20 11:01:03,932 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 279999 seconds 2008-12-20 13:47:43,932 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 289999 seconds 2008-12-20 16:34:23,932 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 299999 seconds 2008-12-20 19:21:03,933 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 309999 seconds 2008-12-20 22:07:43,932 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 319999 seconds then it says: 2008-12-20 13:47:43,935 debug org.apache.hadoop.hbase.regionserver.hstore: skipping major compaction because only one (major) compacted file only and elapsedtime 289999825 is < ttl=-1 i think whats happening is that we're not compacting because we just did but major compactions keep getting triggered because we are not updating the file timestamp. look into it. at least fix up logging so its clearer whats happening.. here is snippet: 2008-12-20 11:01:04,026 debug org.apache.hadoop.hbase.regionserver.hstore: skipping major compaction because only one (major) compacted file only and elapsedtime 149999850 is < ttl=-1 2008-12-20 11:01:04,026 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region assigners,,1229364037757 in 0sec 2008-12-20 13:47:43,932 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 170018656/content. time since last major compaction: 289999 seconds 2008-12-20 13:47:43,932 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region streamitems,^@^@^@^@^c\ufffdr\ufffd,1229427582771 because: regionserver/0:0:0:0:0:0:0:0:60020.majorcompactionchecker requests major compaction 2008-12-20 13:47:43,932 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region streamitems,^@^@^@^@^c\ufffdr\ufffd,1229427582771 2008-12-20 13:47:43,934 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 65151897/content. time since last major compaction: 259999 seconds 2008-12-20 13:47:43,934 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region streamitems,^@^@^@^@^c3=\ufffd,1229459727659 because: regionserver/0:0:0:0:0:0:0:0:60020.majorcompactionchecker requests major compaction 2008-12-20 13:47:43,935 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 1579534353/content. time since last major compaction: 259983 seconds 2008-12-20 13:47:43,935 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region streamitems,^@^@^@^@^av^ds,1229459695202 because: regionserver/0:0:0:0:0:0:0:0:60020.majorcompactionchecker requests major compaction 2008-12-20 13:47:43,935 debug org.apache.hadoop.hbase.regionserver.hstore: skipping major compaction because only one (major) compacted file only and elapsedtime 289999825 is < ttl=-1 2008-12-20 13:47:43,936 debug org.apache.hadoop.hbase.regionserver.hstore: major compaction triggered on store: 1277711984/content. time since last major compaction: 339999 seconds ... ",
        "label": 314
    },
    {
        "text": "npe in hstorescanner updatereaders  2009-01-01 23:55:41,629 fatal org.apache.hadoop.hbase.regionserver.memcacheflusher: replay of hlog required. forcing server shutdown  org.apache.hadoop.hbase.droppedsnapshotexception: region: content,cff13605e2ea6ce0b221ac864687bf08,1230777531253  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:880)  at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:773)  at org.apache.hadoop.hbase.regionserver.memcacheflusher.flushregion(memcacheflusher.java:227)  at org.apache.hadoop.hbase.regionserver.memcacheflusher.run(memcacheflusher.java:137)  caused by: java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.hstorescanner.updatereaders(hstorescanner.java:322)  at org.apache.hadoop.hbase.regionserver.hstore.notifychangedreadersobservers(hstore.java:737)  at org.apache.hadoop.hbase.regionserver.hstore.updatereaders(hstore.java:725)  at org.apache.hadoop.hbase.regionserver.hstore.internalflushcache(hstore.java:694)  at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:630)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:865)  ... 3 more ",
        "label": 38
    },
    {
        "text": "data loss while scanning using prefix tree data block encoding  in scan case, i prepare some data as beflow: table desc (using the prefix-tree encoding) :  'prefix_tree_test', {name => 'cf_1', data_block_encoding => 'prefix_tree', ttl => '15552000'} and i put 5 rows as:  (rowkey , qualifier, value)  'a-b-0-0', 'qf_1', 'c1-value'  'a-b-a-1', 'qf_1', 'c1-value'  'a-b-a-1-1402329600-1402396277', 'qf_2', 'c2-value'  'a-b-a-1-1402397227-1402415999', 'qf_2', 'c2-value-2'  'a-b-b-2-1402397300-1402416535', 'qf_2', 'c2-value-3' so i try to scan the rowkey between 'a-b-a-1' and 'a-b-a-1:' , i and got the corret result:  test 1:   scan scan = new scan(); scan.setstartrow(\"a-b-a-1\".getbytes());  scan.setstoprow(\"a-b-a-1:\".getbytes());  ------------------------------------------------------  'a-b-a-1', 'qf_1', 'c1-value'  'a-b-a-1-1402329600-1402396277', 'qf_2', 'c2-value'  'a-b-a-1-1402397227-1402415999', 'qf_2', 'c2-value-2' and then i try next , scan to addcolumn  test2:  scan scan = new scan();  scan.addcolumn(bytes.tobytes(\"cf_1\") , bytes.tobytes(\"qf_2\")); scan.setstartrow(\"a-b-a-1\".getbytes());  scan.setstoprow(\"a-b-a-1:\".getbytes());  ----------------------------------------------  except:  'a-b-a-1-1402329600-1402396277', 'qf_2', 'c2-value'  'a-b-a-1-1402397227-1402415999', 'qf_2', 'c2-value-2' but actually i got nonthing. then i update the addcolumn for scan.addcolumn(bytes.tobytes(\"cf_1\") , bytes.tobytes(\"qf_1\")); and i got the expected result 'a-b-a-1', 'qf_1', 'c1-value' as well. then i do more testing... i update the case to modify the startrow greater than the 'a-b-a-1'   test3:  scan scan = new scan(); scan.setstartrow(\"a-b-a-1-\".getbytes());  scan.setstoprow(\"a-b-a-1:\".getbytes());  ------------------------------------------------------  except:  'a-b-a-1-1402329600-1402396277', 'qf_2', 'c2-value'  'a-b-a-1-1402397227-1402415999', 'qf_2', 'c2-value-2' but actually i got nothing again. i modify the start row greater than 'a-b-a-1-1402329600-1402396277' scan scan = new scan();  scan.setstartrow(\"a-b-a-1-140239\".getbytes());  scan.setstoprow(\"a-b-a-1:\".getbytes()); and i got the expect row as well:  'a-b-a-1-1402397227-1402415999', 'qf_2', 'c2-value-2' so, i think it may be a bug in the prefix-tree encoding.it happens after the data flush to the storefile, and it's ok when the data in mem-store. ",
        "label": 544
    },
    {
        "text": "use hbaseqa in hbase precommit github pr job  ",
        "label": 149
    },
    {
        "text": "fix wal splitting when region has moved multiple times  currently wal splitting is broken when a region has been opened multiple times in recent minutes. region open and region close write event markers to the wal. these markers should have the sequence id in them. however it is currently getting 1. that means that if a region has moved multiple times in the last few mins then multiple split log workers will try and create the recovered edits file for sequence id 1. one of the workers will fail and on failing they will delete the recovered edits. causing all split wal attempts to fail. we need to: 1. make sure that close get the correct sequence id for open. 2. filter all region events from recovered edits it appears that the close event with a sequence id of one is coming from region warm up. ",
        "label": 154
    },
    {
        "text": "add copy method in bytes  having a \"copy\" method into bytes might be nice to reduce client code size and improve readability. ",
        "label": 230
    },
    {
        "text": "verifyreplication does not honour versions option  source: hbase(main):001:0> scan 't1', {raw => true, versions => 100}  row column+cell   r1 column=f1:, timestamp=1449030102091, value=value1112   r1 column=f1:, timestamp=1449029774173, value=value1001   r1 column=f1:, timestamp=1449029709974, value=value1002     target:  hbase(main):023:0> scan 't1', {raw => true, versions => 100} row column+cell   r1 column=f1:, timestamp=1449030102091, value=value1112   r1 column=f1:, timestamp=1449030090758, value=value1112   r1 column=f1:, timestamp=1449029984282, value=value1111   r1 column=f1:, timestamp=1449029774173, value=value1001   r1 column=f1:, timestamp=1449029709974, value=value1002 /bin/hbase org.apache.hadoop.hbase.mapreduce.replication.verifyreplication --versions=100 1 t1 org.apache.hadoop.hbase.mapreduce.replication.verifyreplication$verifier$counters  goodrows=1 does not show any mismatch. ideally it should show. this is because in   verifyreplication class maxversion is not correctly set. ",
        "label": 475
    },
    {
        "text": "testsplittransactiononcluster hangs frequently  this what i saw once in a local build. java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at org.apache.hadoop.hbase.client.hbaseadmin.disabletable(hbaseadmin.java:831)         at org.apache.hadoop.hbase.regionserver.testsplittransactiononcluster.testshouldclearritwhennodefoundinsplittingstate(testsplittransactiononcluster.java:650) ",
        "label": 544
    },
    {
        "text": "show block cache hit ratio for requests where cacheblocks true  we've been disabling block caching for mr jobs and other big scans. it seems to improve cache performance, but it's difficult to measure the hit ratio because even scans that don't cache blocks still request blocks from the cache (which is good), and those requests affect the hit/miss stats, which makes it difficult to see how the queries where cacheblocks=true are performing. ",
        "label": 247
    },
    {
        "text": "regionserver ubuntu's startup script return code always  hbase-regionserver startup script always returns 0 (exit 0 at the end of the script) this is wrong behaviour which causes issues when trying to recognise true status of the service.  replacing it with 'exit $?' seems to fix the problem, looking at hbase master return codes are assigned to retval variable which is used with exit. not sure if the problem exist in other versions. > /etc/init.d/hbase-regionserver.orig status  hbase-regionserver is not running.  > echo $? after fix: > /etc/init.d/hbase-regionserver status  hbase-regionserver is not running.  > echo $?  1 ",
        "label": 230
    },
    {
        "text": "enable core dump by default for docker  in recent debugging experience, i found that by default the ulimit value prohibits the generation of core dump. this makes debugging difficult. we should enable core dump generation by default. https://www.akadia.com/services/ora_enable_core.html ",
        "label": 441
    },
    {
        "text": " hbase  make cache flush triggering less simplistic  when flusher runs \u2013 its triggered when the sum of all stores in a region > a configurable max size \u2013 we flush all stores though a store memcache might have but a few bytes. i would think stores should only dump their memcache disk if they have some substance. the problem becomes more acute, the more families you have in a region. possible behaviors would be to dump the biggest store only, or only those stores > 50% of max memcache size. behavior would vary dependent on the prompt that provoked the flush. would also log why the flush is running: optional or > max size. this issue comes out of hadoop-2621. ",
        "label": 241
    },
    {
        "text": "provide an alternative to cellutil settimestamp  cellutil.settimestamp has been deprecated in 2.0 and is marked for removal in 3.0. phoenix currently uses this api to set the timestamp of cells in its indexing coprocessor for tables that have mutable indexes. we can't use the cellbuilder api since this involves creating a copy of the cell which will be expensive. fyi @stack ",
        "label": 402
    },
    {
        "text": "bindexception when setting up minikdc  root cause : port for kdc service gets selected in the constructor, but we bind to it later in minikdc.start()-->minikdc.initkdcserver() --> kdcserver.start(). in meantime, some other service can capture the port which results in bindexception. the solution here is to catch the exception and retry. from https://builds.apache.org/view/h-l/view/hbase/job/hbase-1.2/330/jdk=latest1.7,label=hadoop/testreport/junit/org.apache.hadoop.hbase.security.token/testgeneratedelegationtoken/org_apache_hadoop_hbase_security_token_testgeneratedelegationtoken/ error message address already in use  stacktrace java.net.bindexception: address already in use  at sun.nio.ch.net.bind0(native method)  at sun.nio.ch.net.bind(net.java:444)  at sun.nio.ch.net.bind(net.java:436)  at sun.nio.ch.serversocketchannelimpl.bind(serversocketchannelimpl.java:214)  at sun.nio.ch.serversocketadaptor.bind(serversocketadaptor.java:74)  at org.apache.mina.transport.socket.nio.niosocketacceptor.open(niosocketacceptor.java:198)  at org.apache.mina.transport.socket.nio.niosocketacceptor.open(niosocketacceptor.java:51)  at org.apache.mina.core.polling.abstractpollingioacceptor.registerhandles(abstractpollingioacceptor.java:547)  at org.apache.mina.core.polling.abstractpollingioacceptor.access$400(abstractpollingioacceptor.java:68)  at org.apache.mina.core.polling.abstractpollingioacceptor$acceptor.run(abstractpollingioacceptor.java:422)  at org.apache.mina.util.namepreservingrunnable.run(namepreservingrunnable.java:64)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:745) can this utility be made to not fail if address taken? try another? ",
        "label": 48
    },
    {
        "text": " drop non secure builds  make security the default  i would like to only create a single 0.94 tarball/release that contains the security code - and drop the non-secure tarballs and releases. let's discuss... ",
        "label": 286
    },
    {
        "text": "testhlogsplit testsplitwillnottouchlogsifnewhloggets occsionally fails with npe  i am working on an hdfs side patch to reduce the cost of file recovery in hlog#splitlog. when i run hbase unit test, i sometimes see testhlogsplit#testsplitwillnottouchlogsifnewhloggets fail with the following error.  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.wal.testhlogsplit.testsplitwillnottouchlogsifnewhloggetscreatedaftersplitstarted(testhlogsplit.java:462)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44)  at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)  at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41)  at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20)  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:76)  at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50)  at org.junit.runners.parentrunner$3.run(parentrunner.java:193)  at org.junit.runners.parentrunner$1.schedule(parentrunner.java:52)  at org.junit.runners.parentrunner.runchildren(parentrunner.java:191)  at org.junit.runners.parentrunner.access$000(parentrunner.java:42)  at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:184)  at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28)  at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31)  at org.junit.runners.parentrunner.run(parentrunner.java:236)  at org.eclipse.jdt.internal.junit4.runner.junit4testreference.run(junit4testreference.java:46)  at org.eclipse.jdt.internal.junit.runner.testexecution.run(testexecution.java:38)  at org.eclipse.jdt.internal.junit.runner.remotetestrunner.runtests(remotetestrunner.java:467)  at org.eclipse.jdt.internal.junit.runner.remotetestrunner.runtests(remotetestrunner.java:683)  at org.eclipse.jdt.internal.junit.runner.remotetestrunner.run(remotetestrunner.java:390)  at org.eclipse.jdt.internal.junit.runner.remotetestrunner.main(remotetestrunner.java:197) ",
        "label": 441
    },
    {
        "text": "thrift should support next nbrow  like functionality  currently the java hbase api support calling next(number_of_rows) where as the thrift interface doesn't. we have a patch to get this working internally. ",
        "label": 21
    },
    {
        "text": "security source code dirs missing from release tarballs   the release tarballs have a compiled version of the hbase jars and the security tarball seems to have the compiled security bits. however, the source code and resources for security implementation are missing from the release tarballs in both distributions. they should be included in both. ",
        "label": 248
    },
    {
        "text": "avoid splits from once again opening a closed reader for fetching the first and last key  currently split flow is such that we close the parent region and all its store file readers are also closed. after that inorder to split the reference files we need the first and last keys for which once again open the readers on those store files. this could be costlier operation considering the fact that it has to contact the hdfs for this close and open operation. this jira is to see if we can improve this. ",
        "label": 198
    },
    {
        "text": "checkjavacwarnings in test patch sh should bail out early if there is compilation error  currently checkjavacwarnings doesn't exit qa script in the presence of compilation error.  here is one example: https://builds.apache.org/job/precommit-hbase-build/9360/console . checkjavacwarnings should do the following so that it is clear what caused the qa run to fail:   if [[ $? != 0 ]] ; then     err=`$grep -a 5 'compilation failure' $patch_dir/trunkjavacwarnings.txt`     echo \"trunk compilation is broken?     \\{code\\}$err\\{code\\}\"     cleanupandexit 1   fi ",
        "label": 191
    },
    {
        "text": "implement a canary monitoring program  this jira is to implement a standalone program that can be used to do \"canary monitoring\" of a running hbase cluster. this program would gather a list of the regions in the cluster, then iterate over them doing lightweight operations (eg short scans) to provide metrics about latency as well as alert on availability issues. ",
        "label": 309
    },
    {
        "text": "get with closest row before on  hbase meta  can return empty cell during region merge split  during region split/merge there's a brief period of time where doing a \"get\" with \"closest_row_before=true\" on \"hbase:meta\" may return empty \"getresponse.result.cell\" field even though parent, splita and splitb regions are all in \"hbase:meta\". both gohbase (https://github.com/tsuna/gohbase) and asynchbase (https://github.com/opentsdb/asynchbase) interprets this as \"tabledoesnotexist\", which is returned to the client. here's a gist that reproduces this problem: https://gist.github.com/timoha/c7a236b768be9220e85e53e1ca53bf96. note that you have to use older htable client (i used 1.2.4) as current versions ignore `get.setclosestrowbefore(bool)` option. ",
        "label": 514
    },
    {
        "text": "add action in chaos monkey to restart active namenode  under hbase-it we have many actions related to datanode, zookeeper, hmaster which gets use with chaos monkey and they are useful in testing . having action which restart active namenode would be useful too. ",
        "label": 319
    },
    {
        "text": "eliminate threadlocal from mvcc code  brought up by vladimir rodionov and ted yu.  currently we use threadlocals to communicate the current readpoint between a regionscanner and the store{file}scanner's down the stack.  since threadlocals are not cheap we should consider whether it is possible to pass the readpoint through the call stack instead. ",
        "label": 441
    },
    {
        "text": "potential duplicate calls to log appendnosync  in hregion dominibatchmutation   in hregion#dominibatchmutation():         if (noncegroup != currentnoncegroup || nonce != currentnonce) {           if (waledit.size() > 0) {             assert isinreplay;             txid = this.log.appendnosync(this.getregioninfo(), htabledescriptor.gettablename(),                   waledit, m.getclusterids(), now, htabledescriptor, this.sequenceid, true,                   currentnoncegroup, currentnonce);             haswalappends = true;           }           currentnoncegroup = noncegroup;           currentnonce = nonce;         }         // add wal edits by cp         waledit fromcp = batchop.waleditsfromcoprocessors[i];         if (fromcp != null) {           for (keyvalue kv : fromcp.getkeyvalues()) {             waledit.add(kv);           }         } ...       mutation mutation = batchop.getmutation(firstindex);       if (waledit.size() > 0) {         txid = this.log.appendnosync(this.getregioninfo(), this.htabledescriptor.gettablename(),               waledit, mutation.getclusterids(), now, this.htabledescriptor, this.sequenceid,               true, currentnoncegroup, currentnonce);         haswalappends = true;       } if fromcp is null, there may not be new edits added to waledit.  but log#appendnosync() would be called one more time at line 2368. ",
        "label": 406
    },
    {
        "text": "update hbase for java  we need to make sure that hbase compiles and works with jdk 7. once we verify it is reasonably stable, we can explore utilizing the g1 garbage collector. when all deployments are ready to move to jdk 7, we can start using new language features, but in the transition period we will need to maintain a codebase that compiles both with jdk 6 and jdk 7. ",
        "label": 324
    },
    {
        "text": " hbase  add a method of getting multiple  but not all  cells for a row at once  we should have the ability to return some but not all cells from a row at once. there are likely to be a number of situations when getfull will return much more data than needed, but using individual get calls would likely be too small. this method should support returning a specific list of columns all at once. map<text, byte[]> results = table.getmulti(new text[]{cella, cellb, cellc}, timestamp); ",
        "label": 86
    },
    {
        "text": "rs restart just before master intialization we make the cluster non operative  consider a case where my master is getting restarted. rs that was alive when the master restart started, gets restarted before the master initializes the servershutdownhandler. servershutdownhandlerenabled = true; in this case when the rs tries to register with the master, the master will try to expire the server but the server cannot be expired as still the servershutdownhandler is not enabled. this case may happen when i have only one rs gets restarted or all the rs gets restarted at the same time.(before assignrootandmeta). log.info(message);       if (existingserver.getstartcode() < servername.getstartcode()) {         log.info(\"triggering server recovery; existingserver \" +           existingserver + \" looks stale, new server:\" + servername);         expireserver(existingserver);       } if another rs is brought up then the cluster comes back to normalcy. may be a very corner case. ",
        "label": 544
    },
    {
        "text": "hbase connector for kafka connect  implement an hbase connector with source and sink tasks for the connect framework (http://docs.confluent.io/2.0.0/connect/index.html) available in kafka 0.9 and later. see also: http://www.confluent.io/blog/announcing-kafka-connect-building-large-scale-low-latency-data-pipelines an hbase source (http://docs.confluent.io/2.0.0/connect/devguide.html#task-example-source-task) could be implemented as a replication endpoint or walobserver, publishing cluster wide change streams from the wal to one or more topics, with configurable mapping and partitioning of table changes to topics. an hbase sink task (http://docs.confluent.io/2.0.0/connect/devguide.html#sink-tasks) would persist, with optional transformation (json? avro?, map fields to native schema?), kafka sinkrecords into hbase tables. ",
        "label": 322
    },
    {
        "text": "fix javadoc for all public declarations  just as client, master and region server were refactored in subissues of hbase-75, create subtasks for fixing javadoc for all public declarations. some are missing, some are just incorrect. see the hadoop code review checklist: http://wiki.apache.org/hadoop/codereviewchecklist ",
        "label": 241
    },
    {
        "text": "backport portions of hbase to  marking critical so it gets in. ",
        "label": 180
    },
    {
        "text": "modularize maven structure for tests  there's a few reasons to break tests out into their own module:  1. allowing maven users to easily re-consume test utilities as part of a \"test\" package which doesn't pollute the runtime classpath  2. putting integration tests (tests that create or require a cluster) in their own module allows users to easily rebuild and test the core of hbase without running long-running tests, reducing the developer iteration loop after some discussions with stack on irc, it sounds like there was some historic investigation of this which was abandoned because the module system was becoming too complex. i'd suggest that rather than trying to break out components all at once into their modules, evaluate creation of modules on a case-by-case basis and only create them when there's a significant use case justification. i created a sample of what i'm thinking about (based on the current trunk) and posted it on github  git://github.com/ekohlwey/modularized-hbase.git ",
        "label": 248
    },
    {
        "text": "testsplitlogmanager testtaskresigned fails sometimes  the test failed in https://builds.apache.org/job/precommit-hbase-build/8131//testreport for testtaskresigned() :     int version = zkutil.checkexists(zkw, tasknode);     // could be small race here.     if (tot_mgr_resubmit.get() == 0) waitforcounter(tot_mgr_resubmit, 0, 1, to/2); there was no log similar to the following (corresponding to waitforcounter() call above): 2013-12-10 21:23:54,905 info  [main] hbase.waiter(174): waiting up to [3,200] milli-secs(wait.for.ratio=[1]) meaning, the version (2) retrieved corresponded to resubmitted task. version1 retrieved same value, leading to assertion failure. ",
        "label": 441
    },
    {
        "text": "integrationtestingestwithacl should automatically set the superuser when running on local minicluster  to reproduce: $ mvn -dskiptests clean install $ cd hbase-it $ mvn verify -dit.test=integrationtestingestwithacl  this should execute successfully according to http://hbase.apache.org/book/hbase.tests.html section 16.7.5.1. instead no tables can deploy because the superuser is not automatically set to the running user, as what used to happen once upon a time: 2014-06-03 20:15:10,067 warn  [htable-pool12-t1] client.asyncprocess(675): #7, table=hbase:meta, attempt=1/350 failed 1 ops, last exception: org.apache.hadoop.hbase.security.accessdeniedexception: org.apache.hadoop.hbase.security.accessdeniedexception: insufficient permissions (user=apurtell, scope=hbase:meta, family=info:regioninfo, action=write) at org.apache.hadoop.hbase.security.access.accesscontroller.preput(accesscontroller.java:1447) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.preput(regioncoprocessorhost.java:1122) at org.apache.hadoop.hbase.regionserver.hregion.dopremutationhook(hregion.java:2269) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2244) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2200) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2204) at org.apache.hadoop.hbase.regionserver.hregionserver.dobatchop(hregionserver.java:4263) at org.apache.hadoop.hbase.regionserver.hregionserver.dononatomicregionmutation(hregionserver.java:3479) at org.apache.hadoop.hbase.regionserver.hregionserver.multi(hregionserver.java:3369) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:29503) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2012) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:98) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:168) at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:39) at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:111) at java.lang.thread.run(thread.java:745)  on localhost,59092,1401826507071, tracking started tue jun 03 20:15:10 utc 2014 - failed, not retrying anymore ",
        "label": 544
    },
    {
        "text": "allow copytable to identify the source cluster  for replication scenarios   when i worked on hbase-2195 i added a mechanism for an edit to identify its source cluster, so that replication would not bounce it back to the source.  see: this.clusterid = zkhelper.getuuidforcluster(zkhelper.getzookeeperwatcher()); in replicationsource, and put.setclusterid(entry.getkey().getclusterid()); in replicationsink. in master-master replication scenarios, it would very useful if copytable would identify the source cluster (by tagging each put/delete with the source clusterid before applying it). ",
        "label": 229
    },
    {
        "text": "prevent runaway compactions  a rabid upload will easily outrun our compaction ability dropping flushes faster than we can compact them up. fix. ",
        "label": 38
    },
    {
        "text": "improve the efficiency of our mr jobs with a few configurations  this is a low hanging fruit, some of our mr jobs like rowcounter and copytable don't even setcacheblocks on the scan object which out of the box completely screws up a running system. another thing would be to disable speculative execution. ",
        "label": 286
    },
    {
        "text": " replication  shutting down a stream leaves recovered sources running  when removing a peer it will call replicationsourcemanager.removepeer which calls closerecoveredqueue which does this: log.info(\"done with the recovered queue \" + src.getpeerclusterznode()); this.oldsources.remove(src); this.zkhelper.deletesource(src.getpeerclusterznode(), false); this works in the case where the recovered source is done and is calling this method, but when removing a peer it never calls terminate on thus it leaving it running. ",
        "label": 229
    },
    {
        "text": "eliminate the warnings from the spotbugs  it is hard to get +1 from qa currently because spotbugs is always unhappy... ",
        "label": 98
    },
    {
        "text": "move regionserver and related classes into regionserver package  move: hregionserver hregioninterface hregion hstore hstorefile hinternalscannerinterface ? hstorekey others? ",
        "label": 86
    },
    {
        "text": "startup scripts create  out files   when start hbase with bin/start-hbase.sh, script creates 2 out files. -rw-r--r-- 1 jmspaggiari jmspaggiari     0 aug 31 15:38 hbase-jmspaggiari-master-t430s.out -rw-r--r-- 1 jmspaggiari jmspaggiari     0 aug 31 15:38 hbase-jmspaggiari-master-t430s.out.1 should create only one. ",
        "label": 314
    },
    {
        "text": "update supplemental models for new deps in hadoop trunk  did a test compile of hbase against latest hadoop trunk, there are some new dependencies that need to be added to the supplemental-models.xml file. ",
        "label": 320
    },
    {
        "text": "add support for zookeeper authentication  some users may run a zookeeper cluster in \"multi tenant mode\" meaning that more than one client service would  like to share a single zookeeper service instance (cluster). in this case the client services typically want to protect  their data (zk znodes) from access by other services (tenants) on the cluster. say you are running hbase and solr   and neo4j, or multiple hbase instances, etc... having authentication/authorization on the znodes is important for both   security and helping to ensure that services don't interact negatively (touch each other's data). today hbase does not have support for authentication or authorization. this should be added to the hbase clients  that are accessing the zk cluster. in general it means calling addauthinfo once after a session is established: http://hadoop.apache.org/zookeeper/docs/current/api/org/apache/zookeeper/zookeeper.html#addauthinfo(java.lang.string, byte[]) with a user specific credential, often times this is a shared secret or certificate. you may be able to statically configure this  in some cases (config string or file to read from), however in my case in particular you may need to access it programmatically,  which adds complexity as the end user may need to load code into hbase for accessing the credential. secondly you need to specify a non \"world\" acl when interacting with znodes (create primarily):  http://hadoop.apache.org/zookeeper/docs/current/api/org/apache/zookeeper/data/acl.html  http://hadoop.apache.org/zookeeper/docs/current/api/org/apache/zookeeper/zoodefs.html feel free to ping the zookeeper team if you have questions. it might also be good to discuss with some   potential end users - in particular regarding how the end user can specify the credential. ",
        "label": 164
    },
    {
        "text": "show per region request count in table jsp  table.jsp should display per region request count.  it should also display region count per region server. ",
        "label": 441
    },
    {
        "text": "bug in hlog we print array of byes for region name  i see lines in the debug logs like this 2008-08-12 16:13:20,638 debug org.apache.hadoop.hbase.regionserver.hlog: found 1 logs to remove using oldest outstanding seqnum of 265156192 from region [b@18a3257 ",
        "label": 73
    },
    {
        "text": " hbase master stop  does not bring down backup masters  typing \"hbase master stop\" produces the following message: \"stop start cluster shutdown; master signals regionserver shutdown\"  it seems like backup masters should be considered part of the cluster, but they are not brought down by \"hbase master stop\".  \"stop-hbase.sh\" does correctly bring down the backup masters. the same behavior is observed when a client app makes use of the client api hbaseadmin.shutdown() http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/hbaseadmin.html#shutdown() \u2013 this isn't too surprising since i think \"hbase master stop\" just calls this api. it seems like hbase-1448 address this; perhaps there was a regression? ",
        "label": 186
    },
    {
        "text": "deprecate and throw unsupported operation when admin closeregion is called   umesh agashe tripped over this today. admin#closeregion which we used to use in branch-1 will cause damage in amv2 cluster. instead you need to call unassign \u2013 i.e. all cluster ops must go via the master; no more going direct to regionserver closing regions behind the master's back. review all admin ops to see what else skirts master and deprecate and throw unsupported if called. ",
        "label": 48
    },
    {
        "text": "secure rest server should login before getting an instance of rest servlet  fails with exception avax.security.sasl.saslexception: gss initiate failed [caused by gssexception: no valid credentials provided (mechanism level: failed to find any kerberos tgt)]         at com.sun.security.sasl.gsskerb.gsskrb5client.evaluatechallenge(gsskrb5client.java:194)         at org.apache.hadoop.hbase.security.hbasesaslrpcclient.saslconnect(hbasesaslrpcclient.java:139)         at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.setupsaslconnection(secureclient.java:194)         at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.access$500(secureclient.java:92)         at org.apache.hadoop.hbase.ipc.secureclient$secureconnection$2.run(secureclient.java:302)         at org.apache.hadoop.hbase.ipc.secureclient$secureconnection$2.run(secureclient.java:299)         at java.security.accesscontroller.doprivileged(native method)         at javax.security.auth.subject.doas(subject.java:396)         at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1178)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.util.methods.call(methods.java:37)         at org.apache.hadoop.hbase.security.user.call(user.java:590)         at org.apache.hadoop.hbase.security.user.access$700(user.java:51)         at org.apache.hadoop.hbase.security.user$securehadoopuser.runas(user.java:444)         at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.setupiostreams(secureclient.java:298)         at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:1124)         at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:974)         at org.apache.hadoop.hbase.ipc.securerpcengine$invoker.invoke(securerpcengine.java:104)         at $proxy5.getprotocolversion(unknown source)         at org.apache.hadoop.hbase.ipc.securerpcengine.getproxy(securerpcengine.java:146)         at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getmaster(hconnectionmanager.java:711)         at org.apache.hadoop.hbase.client.hbaseadmin.<init>(hbaseadmin.java:116)         at org.apache.hadoop.hbase.rest.restservlet.<init>(restservlet.java:74)         at org.apache.hadoop.hbase.rest.restservlet.getinstance(restservlet.java:57)         at org.apache.hadoop.hbase.rest.main.main(main.java:81) caused by: gssexception: no valid credentials provided (mechanism level: failed to find any kerberos tgt)         at sun.security.jgss.krb5.krb5initcredential.getinstance(krb5initcredential.java:130)         at sun.security.jgss.krb5.krb5mechfactory.getcredentialelement(krb5mechfactory.java:106)         at sun.security.jgss.krb5.krb5mechfactory.getmechanismcontext(krb5mechfactory.java:172)         at sun.security.jgss.gssmanagerimpl.getmechanismcontext(gssmanagerimpl.java:209)         at sun.security.jgss.gsscontextimpl.initseccontext(gsscontextimpl.java:195)         at sun.security.jgss.gsscontextimpl.initseccontext(gsscontextimpl.java:162)         at com.sun.security.sasl.gsskerb.gsskrb5client.evaluatechallenge(gsskrb5client.java:175) ",
        "label": 50
    },
    {
        "text": "upgrade existing thrift binding using thrift compiler   ",
        "label": 252
    },
    {
        "text": "introduce a new log writer which can write to two hdfses  ",
        "label": 149
    },
    {
        "text": "replace the hrs leases with zookeeper  ",
        "label": 229
    },
    {
        "text": "prefix compression   trie data block encoding  the hbase data block format has room for 2 significant improvements for applications that have high block cache hit ratios. first, there is no prefix compression, and the current keyvalue format is somewhat metadata heavy, so there can be tremendous memory bloat for many common data layouts, specifically those with long keys and short values. second, there is no random access to keyvalues inside data blocks. this means that every time you double the datablock size, average seek time (or average cpu consumption) goes up by a factor of 2. the standard 64kb block size is ~10x slower for random seeks than a 4kb block size, but block sizes as small as 4kb cause problems elsewhere. using block sizes of 256kb or 1mb or more may be more efficient from a disk access and block-cache perspective in many big-data applications, but doing so is infeasible from a random seek perspective. the prefixtrie block encoding format attempts to solve both of these problems. some features: trie format for row key encoding completely eliminates duplicate row keys and encodes similar row keys into a standard trie structure which also saves a lot of space the column family is currently stored once at the beginning of each block. this could easily be modified to allow multiple family names per block all qualifiers in the block are stored in their own trie format which caters nicely to wide rows. duplicate qualifers between rows are eliminated. the size of this trie determines the width of the block's qualifier fixed-width-int the minimum timestamp is stored at the beginning of the block, and deltas are calculated from that. the maximum delta determines the width of the block's timestamp fixed-width-int the block is structured with metadata at the beginning, then a section for the row trie, then the column trie, then the timestamp deltas, and then then all the values. most work is done in the row trie, where every leaf node (corresponding to a row) contains a list of offsets/references corresponding to the cells in that row. each cell is fixed-width to enable binary searching and is represented by [1 byte operationtype, x bytes qualifier offset, x bytes timestamp delta offset]. if all operation types are the same for a block, there will be zero per-cell overhead. same for timestamps. same for qualifiers when i get a chance. so, the compression aspect is very strong, but makes a few small sacrifices on varint size to enable faster binary searches in trie fan-out nodes. a more compressed but slower version might build on this by also applying further (suffix, etc) compression on the trie nodes at the cost of slower write speed. even further compression could be obtained by using all vints instead of fints with a sacrifice on random seek speed (though not huge). one current drawback is the current write speed. while programmed with good constructs like treemaps, bytebuffers, binary searches, etc, it's not programmed with the same level of optimization as the read path. work will need to be done to optimize the data structures used for encoding and could probably show a 10x increase. it will still be slower than delta encoding, but with a much higher decode speed. i have not yet created a thorough benchmark for write speed nor sequential read speed. though the trie is reaching a point where it is internally very efficient (probably within half or a quarter of its max read speed) the way that hbase currently uses it is far from optimal. the keyvaluescanner and related classes that iterate through the trie will eventually need to be smarter and have methods to do things like skipping to the next row of results without scanning every cell in between. when that is accomplished it will also allow much faster compactions because the full row key will not have to be compared as often as it is now. current code is on github. the trie code is in a separate project than the slightly modified hbase. there is an hbase project there as well with the deltaencoding patch applied, and it builds on top of that. https://github.com/hotpads/hbase/tree/prefix-trie-1  https://github.com/hotpads/hbase-prefix-trie/tree/hcell-scanners i'll follow up later with more implementation ideas. ",
        "label": 307
    },
    {
        "text": "fix tests broken by recent metrics re work  counts are appoximate and go away. we should re-work the tests or test utils to make them work now. ",
        "label": 154
    },
    {
        "text": "more log pruning  this issue covers some tightening up of log messages; as is all of the zk noise tends to overwhelm. for example, zkwatcher logs a generic \"this event happened in zk with path x and event type y\" but just after, there will be a log from the handler of this zk event with this subsequent log more descriptive. this change would make zkwatcher log at info by default rather than debug cutting down on logging content (re-enabling debug is easy to do if needed). ",
        "label": 314
    },
    {
        "text": "sync bulk and regular assigment handling socket timeout exception  in regular assignment, in case of socket network timeout, it tries to call openregion again and again without change the region plan, zk offline node,  till the region is out of transition, in case the region server is still up. we may need to sync them up and make sure bulk assignment does the same in this case. ",
        "label": 242
    },
    {
        "text": "store last flushed sequence id for each store of region for distributed log replay  hbase-7006 stores last flushed sequence id of the region in zookeeper. to prevent deleted data from appearing again, we should store last flushed sequence id for each store of region in zookeeper. see discussion here:  https://issues.apache.org/jira/browse/hbase-7006?focusedcommentid=13660428&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13660428 ",
        "label": 233
    },
    {
        "text": "npe in eventhandler when region already reassigned  when a region takes too long to open, it will try to update the unassigned znode and will fail on an ugly npe like this: debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:60020-0x22dc571dde04ca7 attempting to transition node 0519dc3b62a569347526875048c37faa from rs_zk_region_opening to rs_zk_region_opening  debug org.apache.hadoop.hbase.zookeeper.zkutil: regionserver:60020-0x22dc571dde04ca7 unable to get data of znode /hbase/unassigned/0519dc3b62a569347526875048c37faa because node does not exist (not necessarily an error)  error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event m_rs_open_region  java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:75)  at org.apache.hadoop.hbase.executor.regiontransitiondata.frombytes(regiontransitiondata.java:198)  at org.apache.hadoop.hbase.zookeeper.zkassign.transitionnode(zkassign.java:672)  at org.apache.hadoop.hbase.zookeeper.zkassign.retransitionnodeopening(zkassign.java:585)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.tickleopening(openregionhandler.java:322)  at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:97)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:151)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) i think the region server in this case should be closing the region asap. ",
        "label": 314
    },
    {
        "text": "hfileblockindex write error in hfile v2 due to incorrect split into intermediate index blocks  after writing some data, compaction and scan operation both failure, the exception message is below:  2012-09-18 06:32:26,227 error org.apache.hadoop.hbase.regionserver.compactions.compactionrequest: compaction failed regionname=hfile_test,,1347778722498.d220df43fb9d8af4633bd7f547613f9e., storename=page_info, filecount=7, filesize=1.3m (188.0k, 188.0k, 188.0k, 188.0k, 188.0k, 185.8k, 223.3k), priority=9, time=45826250816757428java.io.ioexception: could not reseek storefilescanner[hfilescanner for reader reader=hdfs://hadoopdev1.cm6:9000/hbase/hfile_test/d220df43fb9d8af4633bd7f547613f9e/page_info/b0f6118f58de47ad9d87cac438ee0895, compression=lzo, cacheconf=cacheconfig:enabled [cachedataonread=true] [cachedataonwrite=false] [cacheindexesonwrite=false] [cachebloomsonwrite=false] [cacheevictonclose=false] [cachecompressed=false], firstkey=http://com.truereligionbrandjeans.www/womens_dresses/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_shirts/pl/c/womens_sweaters/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_shirts/pl/c/womens_shirts/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/womens_sweaters/pl/c/4010.html/page_info:anchor_sig/1347764439449/deletecolumn, lastkey=http://com.trura.www//page_info:page_type/1347763395089/put, avgkeylen=776, avgvaluelen=4, entries=12853, length=228611, cur=http://com.truereligionbrandjeans.www/womens_exclusive_details/pl/c/4970.html/page_info:is_deleted/1347764003865/put/vlen=1/ts=0] to key http://com.truereligionbrandjeans.www/womens_exclusive_details/pl/c/4970.html/page_info:is_deleted/oldest_timestamp/minimum/vlen=0/ts=0  at org.apache.hadoop.hbase.regionserver.storefilescanner.reseek(storefilescanner.java:178)   at org.apache.hadoop.hbase.regionserver.nonlazykeyvaluescanner.dorealseek(nonlazykeyvaluescanner.java:54)   at org.apache.hadoop.hbase.regionserver.keyvalueheap.generalizedseek(keyvalueheap.java:299)  at org.apache.hadoop.hbase.regionserver.keyvalueheap.reseek(keyvalueheap.java:244)   at org.apache.hadoop.hbase.regionserver.storescanner.reseek(storescanner.java:521)   at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:402)  at org.apache.hadoop.hbase.regionserver.store.compactstore(store.java:1570)   at org.apache.hadoop.hbase.regionserver.store.compact(store.java:997)   at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1216)  at org.apache.hadoop.hbase.regionserver.compactions.compactionrequest.run(compactionrequest.java:250)   at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.ioexception: expected block type leaf_index, but got intermediate_index: blocktype=intermediate_index, ondisksizewithoutheader=8514, uncompressedsizewithoutheader=131837, prevblockoffset=-1, databeginswith=\\x00\\x00\\x00\\x9b\\x00\\x00\\x00\\x00\\x00\\x00\\x03#\\x00\\x00\\x050\\x00\\x00\\x08\\xb7\\x00\\x00\\x0cr\\x00\\x00\\x0f\\xfa\\x00\\x00\\x120, fileoffset=218942 at org.apache.hadoop.hbase.io.hfile.hfilereaderv2.validateblocktype(hfilereaderv2.java:378)  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2.readblock(hfilereaderv2.java:331) at org.apache.hadoop.hbase.io.hfile.hfileblockindex$blockindexreader.seektodatablock(hfileblockindex.java:213)  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$abstractscannerv2.seekto(hfilereaderv2.java:455)  at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$abstractscannerv2.reseekto(hfilereaderv2.java:493)   at org.apache.hadoop.hbase.regionserver.storefilescanner.reseekatorafter(storefilescanner.java:242)   at org.apache.hadoop.hbase.regionserver.storefilescanner.reseek(storefilescanner.java:167) after some debug works\uff0ci found that when hfile closing, if the rootchunk is empty, the only one curinlinechunk will upgrade to root chunk. but if the last block flushing make curinlinechunk exceed max index block size, the root chunk(upgrade from curinlinechunk) will be splited into intermediate index blocks, and the index level is set to 2. so when blockindexreader read the root index, it expects the next level index block is leaf index(index level=2), but the on disk index block is intermediate block, the error happened. after i add some code to check curinlinechunk's size when rootchunk is empty in shouldwriteblock(boolean closing), this bug can be fixed. ",
        "label": 324
    },
    {
        "text": "looking for the surefire killer  builds being killed executionexception  java lang runtimeexception  the forked vm terminated without properly saying goodbye  vm crash or system exit called   i see this in a build that started at two hours ago... about 6:45... its build 15941 on ubuntu-6 warning: 2 rogue build processes detected, terminating. /bin/kill -9 18640  /bin/kill -9 22625  if i back up to build 15939, started about 3 1/2 hours ago, say, 5:15.... i see: running org.apache.hadoop.hbase.client.testshell  killed ... but it was running on ubuntu-1.... so it doesn't look like we are killing ourselves... when we do this in test-patch.sh 1. 1. 1. kill any rogue build processes from the last attempt  $ps auxwww | $grep ${project_name}patchprocess | $awk ' {print $2} ' | /usr/bin/xargs -t -i {} /bin/kill -9 {} > /dev/null the above code runs in a few places... in test-patch.sh. let me try and add some more info around what is being killed... ",
        "label": 314
    },
    {
        "text": "add rpc call queues to the web ui  the size of the call queue for the regionserver is a critical metric to see if things are going too slowly. we should add the call queue size to the ui under the queues tab. ",
        "label": 345
    },
    {
        "text": "illegalaccesserror  has not been initialized  getmaxsequenceid      1.       2009-10-21 08:58:21,003 debug org.apache.hadoop.hbase.master.regionmanager: doing for address: 72.34.249.213:60020, startcode: 1256140228290, load: (requests=0, regions=338, usedheap=169, maxheap=3974) nregions: 1 and nregionstoassign: 1    2.       2009-10-21 08:58:21,003 info org.apache.hadoop.hbase.master.regionmanager: assigning region items,\\x00\\x00\\x00\\x00\\x12\\x96\\x40\\xc9\\x00\\x00\\x00\\x00,1256050226044 to hb3,60020,1256140228290    3.       2009-10-21 08:58:21,004 debug org.apache.hadoop.hbase.master.hmaster: processing todo: regionassignmenthistorian from hb3,60020,1256140228290    4.       2009-10-21 08:58:24,030 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_process_open: items,\\x00\\x00\\x00\\x00\\x12\\x96\\x40\\xc9\\x00\\x00\\x00\\x00,1256050226044 from hb3,60020,1256140228290; 1 of 1    5.       2009-10-21 08:58:27,468 info org.apache.hadoop.hbase.master.basescanner: regionmanager.rootscanner scanning meta region {server: 72.34.249.218:60020, regionname: -root-,,0, startkey: <>}    6.       2009-10-21 08:58:27,470 info org.apache.hadoop.hbase.master.basescanner: regionmanager.metascanner scanning meta region {server: 72.34.249.210:60020, regionname: .meta.,,1, startkey: <>}    7.       2009-10-21 08:58:27,474 info org.apache.hadoop.hbase.master.basescanner: regionmanager.rootscanner scan of 1 row(s) of meta region {server: 72.34.249.218:60020, regionname: -root-,,0, startkey: <>} complete    8.       2009-10-21 08:58:28,777 info org.apache.hadoop.hbase.master.basescanner: regionmanager.metascanner scan of 3425 row(s) of meta region {server: 72.34.249.210:60020, regionname: .meta.,,1, startkey: <>} complete    9.       2009-10-21 08:58:28,777 info org.apache.hadoop.hbase.master.basescanner: all 1 .meta. region(s) scanned   10.       2009-10-21 08:58:30,091 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_close: items,\\x00\\x00\\x00\\x00\\x12\\x96\\x40\\xc9\\x00\\x00\\x00\\x00,1256050226044: java.lang.illegalaccesserror: has not been initialized   11.               at org.apache.hadoop.hbase.regionserver.storefile.getmaxsequenceid(storefile.java:216)   12.               at org.apache.hadoop.hbase.regionserver.store.loadstorefiles(store.java:388)   13.               at org.apache.hadoop.hbase.regionserver.store.<init>(store.java:217)   14.               at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:1507)   15.               at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:308)   16.               at org.apache.hadoop.hbase.regionserver.hregionserver.instantiateregion(hregionserver.java:1601)   17.               at org.apache.hadoop.hbase.regionserver.hregionserver.openregion(hregionserver.java:1570)   18.               at org.apache.hadoop.hbase.regionserver.hregionserver$worker.run(hregionserver.java:1485)   19.               at java.lang.thread.run(thread.java:619)   20.        from hb3,60020,1256140228290; 1 of 1   21.       2009-10-21 08:58:30,091 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processregionclose of items,\\x00\\x00\\x00\\x00\\x12\\x96\\x40\\xc9\\x00\\x00\\x00\\x00,1256050226044, false, reassign: true   22.       2009-10-21 08:58:30,091 info org.apache.hadoop.hbase.master.regionserveroperation: region set as unassigned: items,\\x00\\x00\\x00\\x00\\x12\\x96\\x40\\xc9\\x00\\x00\\x00\\x00,1256050226044 ",
        "label": 314
    },
    {
        "text": "replicationlogcleaner slow at large scale  at a large scale the replicationlogcleaner fails to clean up .oldlogs as fast as the cluster is producing them. for each old hlog file that has been replicated and should be deleted the replicationlogcleaner checks every replication queue in zookeeper before removing it. this means that as a cluster scales up the number of files to delete scales as well as the time to delete each file so the cleanup chore scales quadratically. in our case it reached the point where the oldlogs were growing faster than they were being cleaned up. we're now running with a patch that allows the replicationlogcleaner to refresh its list of files in the replication queues from zookeeper just once for each batch of files the cleanerchore wants to evaluate. i'd propose updating filecleanerdelegate to take a list<filestatus> rather than a single one at a time. this would allow file cleaners that check an external resource for references such as zookeeper (for replicationlogcleaner) or hdfs (for snapshotlogcleaner which looks like it may also have similar trouble at scale) to load those references once per batch rather than for every log. ",
        "label": 125
    },
    {
        "text": "add a decent heuristic for region size  a few of us were brainstorming this morning about what the default region size should be. there were a few general points made: in some ways it's better to be too-large than too-small, since you can always split a table further, but you can't merge regions currently with hfile v2 and multithreaded compactions there are fewer reasons to avoid very-large regions (10gb+) for small tables you may want a small region size just so you can distribute load better across a cluster for big tables, multi-gb is probably best ",
        "label": 314
    },
    {
        "text": "oome but we don't abort  on streamy cluster saw case where graceful shutdown had been triggered rather than an abort on oome. on graceful shutdown, we wait on leases to expire or be closed. server wouldn't go down because it was waiting on leases to expire only an oome in leases had killed the thread so it wasn't ever going to expire anything. node was stuck for four hours till someone noticed it. ",
        "label": 314
    },
    {
        "text": "list labels shouldn't raise argumenterror if no labels are defined  list_labels shouldn't raise argumenterror if no labels are defined. the empty list is a valid return case, and anyway argumenterror should be used when arguments to the command are erroneous. here the command is well formed: hbase(main):001:0> list_labels error: no auth label defined here is some help for this command: list the visibility labels defined in the system. optional regular expression parameter could be used to filter the labels being returned. syntax : list_labels for example:     hbase> list_labels 'secret.*'     hbase> list_labels ",
        "label": 46
    },
    {
        "text": "add permission check to roll wal writer  currently hbase provides hbaseadmin.rollhlogwriter() and shell command to roll wal on a region server. but no permission check is done on this operation in a secure cluster.  we need to add permission check to prevent un-authorized user from running this operation. ",
        "label": 234
    },
    {
        "text": "minimize a number of hbase client transitive dependencies  hbase-client has a number of transitive dependencies not needed for a client mode execution. in my test i've added the following exclusions: <exclusions>                 <exclusion>                 <groupid>com.sun.jersey</groupid>                 <artifactid>jersey-server</artifactid>                          </exclusion>                 <exclusion>                 <groupid>com.sun.jersey</groupid>                 <artifactid>jersey-core</artifactid>                          </exclusion>                 <exclusion>                 <groupid>com.sun.jersey</groupid>                 <artifactid>jersey-json</artifactid>                          </exclusion>                 <exclusion>                 <groupid>com.sun.jersey.contribs</groupid>                 <artifactid>jersey-guice</artifactid>                          </exclusion>                 <exclusion>                 <groupid>com.google.inject</groupid>                 <artifactid>guice</artifactid>                          </exclusion>                 <exclusion>                 <groupid>com.google.inject.extensions</groupid>                 <artifactid>guice-servlet</artifactid>                          </exclusion>                     <exclusion>                 <groupid>org.mortbay.jetty</groupid>                 <artifactid>jetty</artifactid>                          </exclusion>                 <exclusion>                 <groupid>org.mortbay.jetty</groupid>                 <artifactid>jetty-util</artifactid>                          </exclusion>                 <exclusion>                 <groupid>commons-httpclient</groupid>                 <artifactid>commons-httpclient</artifactid>                          </exclusion>            </exclusions> proposal: add related exclusions to some of the dependencies hbase-client depends upon. ",
        "label": 405
    },
    {
        "text": "user class should implement equals  and hashcode   the current implementation of the user class does not override equals() and hashcode(), so connection reuse in hbaseclient is completely broken. ",
        "label": 441
    },
    {
        "text": "document the meaning of  interfaceaudience in hbase ref guide  ",
        "label": 330
    },
    {
        "text": "add hbaseadmin gettabledescriptor function  currently, to get a htabledescriptor for a particular table, i have to call listtables() and then loop through the htabledescriptor[]. it would be nice if the api just had a function to give me a single htabledescriptor, since i already know the name of the table i'm interested in. alternately, listtables could return a map of string -> htabledescriptor, keyed off table name. ",
        "label": 241
    },
    {
        "text": "add backoff when region failed open too many times    public static final string assign_max_attempts =     \"hbase.assignment.maximum.attempts\"; private static final int default_assign_max_attempts = integer.max_value; now the default config is integer.max_value.    2019-04-09,10:50:44,921 info org.apache.hadoop.hbase.master.assignment.transitregionstateprocedure: retry=170813 of max=2147483647; pid=2849, ppid=2846, state=runnable:region_state_transition_confirm_opened, locked=true; transitregionstateprocedure table=integrationtestbiglinkedlist, region=634feb79a583480597e1843647d11228, reopen/move; rit=opening, location=c4-hadoop-tst-st26.bj,29100,1554262369262 the itbll failed to open the region as hbase-22163 and retry 170813 to reopen. after i fixed the problem and restart master, i found it need take a long time to init the old procedure logs because there are too many old logs... code in walprocedurestore,java.   private long initoldlogs(filestatus[] logfiles) throws ioexception {   if (logfiles == null || logfiles.length == 0) {     return 0l;   }   long maxlogid = 0;   for (int i = 0; i < logfiles.length; ++i) {     final path logpath = logfiles[i].getpath();     leaserecovery.recoverfilelease(fs, logpath);     if (!isrunning()) {       throw new ioexception(\"wal aborting\");     }     maxlogid = math.max(maxlogid, getlogidfromname(logpath.getname()));     procedurewalfile log = initoldlog(logfiles[i], this.walarchivedir);     if (log != null) {       this.logs.add(log);     }   }   inittrackerfromoldlogs();   return maxlogid; }       ",
        "label": 187
    },
    {
        "text": "ensure that there is only master with zookeeper  part of ha master   ",
        "label": 342
    },
    {
        "text": "remove delayed rpc  i don't really know anyone using it and it makes things a bit more complex. ",
        "label": 154
    },
    {
        "text": "tif needs to be able to set scanner caching size for smaller row tables   performance  tif goes with the default scanner caching size (1). when each row is processed very fast and is small, this limits the overall performance. by setting a higher scanner caching level you can achieve 100x+ the performance with the exact same map-reduce and table. ",
        "label": 38
    },
    {
        "text": "convert rowprocessorprotocol to protocol buffer service  with coprocessor endpoints now exposed as protobuf defined services, we should convert over all of our built-in endpoints to pb services. ",
        "label": 139
    },
    {
        "text": "testtablemapreduce doesn't work properly   no map function is called because there are no test data put before test starts. the following three tests are in the same situation: org.apache.hadoop.hbase.mapred.testtablemapreduce org.apache.hadoop.hbase.mapreduce.testtablemapreduce org.apache.hadoop.hbase.mapreduce.testmulitthreadedtablemapper ",
        "label": 436
    },
    {
        "text": "replaced deprecated interface closeable  hadoop interface closeable has been deprecated for quite some time to favor java.io.closeable. since we support only hadoop 1.0 and above now, we can replace the deprecated closeable with the java one. we can also clean up some hack we did as well, for example, hbase-10029. https://github.com/apache/hadoop-common/blob/branch-1.0/src/core/org/apache/hadoop/io/closeable.java ",
        "label": 242
    },
    {
        "text": "backport to branch hbase miss metrics when coprocessor use region scanner to read data  making subissue to backport the parent issue to branch-1. i'll attach first attempt at a backport. it is failing in testregionservermetrics in an assert. making a new issue because time has elapsed since parent went into master and branch-1 and i want to resolve the parent. thanks. fyi [~zghaobac] if you've input, just say sir and i can take another look. ",
        "label": 187
    },
    {
        "text": "testregionservercoprocessorexceptionwithabort testexceptionfromcoprocessorduringput fails  see http://jenkins-public.iridiant.net/job/hbase-0.95-hadoop-2/org.apache.hbase$hbase-server/903/testreport/junit/org.apache.hadoop.hbase.coprocessor/testregionservercoprocessorexceptionwithabort/testexceptionfromcoprocessorduringput/ ",
        "label": 314
    },
    {
        "text": "previous active master can still serves rpc request when it is trying recovering expired zk session  when the active master's zk session expires, it'll try to recover zk session, but without turn off its rpcserver. what if a previous backup master has already become the now active master, and some client tries to send request to this expired master by using the cached master info? any problem here? ",
        "label": 203
    },
    {
        "text": "too many zk connections  currently we open tons of new connections to zookeeper, like every time we instantiate a new htable. there is a maximum number of client connections as described here:   <property>     <name>hbase.zookeeper.property.maxclientcnxns</name>     <value>30</value>     <description>property from zookeeper's config zoo.cfg.     limit on number of concurrent connections (at the socket level) that a     single client, identified by ip address, may make to a single member of     the zookeeper ensemble. set high to avoid zk connection issues running     standalone and pseudo-distributed.     </description>   </property> if you hit that max number, zk will just refuse your connections. suppose you have 4 maps running on a server hosting a rs, you may actually lose your connection in the rs and eventually hit a session timeout. maybe we should singleton zkw? ",
        "label": 229
    },
    {
        "text": "npe out of hrs run at startup when clock is out of sync  this is what i get when i start a region server that's not properly sync'ed: exception in thread \"regionserver60020\" java.lang.nullpointerexception at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:603) at java.lang.thread.run(thread.java:637) i this case the line was: hlogroller.interruptifnecessary(); i guess we could add a bunch of other null checks. the end result is the same, the rs dies, but i think it's misleading. ",
        "label": 229
    },
    {
        "text": "track the remaining unimplemented methods for async admin  ",
        "label": 187
    },
    {
        "text": "hbase io index interval need be configuratable in column family  setting parameter hbase.io.index.interval to smaller can improve hbase reading performance significantly, esp. in large value size column families. however, small hbase.io.index.interval cause more memory usage, because all index will read into memory when loading a mapfile. in my test env, i set hbase.io.index.interval to 1, after inserting about 3m samll size records to a table(about 1.5g in hadoop file), the regionserver throws oome. then i found total size of map file index is 350m. however, i can't adjust hbase.io.index.interval to a larger one, like 32, because other big cell size tables need it be 1. so, i think make hbase.io.index.interval a column family property should be very important for performance tuning. ",
        "label": 38
    },
    {
        "text": "convert  deprecated hbasetestcase tests junit4 style tests  this will class has 47 references so separating out into a separate subtask. ",
        "label": 53
    },
    {
        "text": "make bulk assignment on cluster startup run faster  currently, as of hbase-3018, we come up with a bulk assignment plan that is sorted by server. we then spawn a thread to assign out the regions per server so we are assigning in parallel. this works but is still slow enough (it looks to be slower than the old assignment where we'd do lumps of n regions at a time). we should be able to pass a regionserver all the regions to open in one rpc. we need to figure how to keep up zk state while regionserver is processing a big lot of regions. this looks a little awkward to do since currently open handler just opens region \u2013 there is no notion of doing a ping while waiting to run. being able to start the cluster fast is important for those times we take it down to do major upgrade; the longer it takes to spin up, the longer our 'downtime'. ",
        "label": 314
    },
    {
        "text": "implement a general framework to execute remote procedure on rs  when building the basic framework for hbase-19064, i found that the enable/disable peer is built upon the watcher of zk. the problem of using watcher is that, you do not know the exact time when all rses in the cluster have done the change, it is a 'eventually done'. and for synchronous replication, when changing the state of a replication peer, we need to know the exact time as we can only enable read/write after that time. so i think we'd better use procedure to do this. change the flag on zk, and then execute a procedure on all rses to reload the flag from zk. another benefit is that, after the change, zk will be mainly used as a storage, so it will be easy to implement another replication peer storage to replace zk so that we can reduce the dependency on zk. ",
        "label": 149
    },
    {
        "text": "spacequota disabletableviolationpolicy will cause cycles of enable disable table  space quota: policy state is getting changed from disable to observance after sometime automatically. steps: 1: create a table with space quota policy as disable  2: put some data so that table state is in space quota violation  3: so observe that table state is in violation  4: now wait for some time  5: observe that after some time table state is changing to to observance however table is still disabled edit (elserj): the table is automatically moved back from the violation state because of the code added that tried to ride over rits. when a region is not online (whether normally or abnormally), the regionsizereports are not sent from rs to master. eventually, enough regions are not reported which dips below the acceptable threshold and we automatically move the table back to the \"acceptable\" space quota state (not in violation). we could skip this failsafe when we're checking for a quota that has the disabletable violation policy. ",
        "label": 412
    },
    {
        "text": "backport  fix coverage for org apache hadoop hbase mapreduce  to  do you want this test update backported? see hbase-8534 for a 0.94 patch. ",
        "label": 217
    },
    {
        "text": "showing bytes in log when should be string    see hbase-701 - spotted some more byte output: regionserver.compactsplitthread: \"compaction failed for region ...\" [twice in run()] regionserver.compactsplitthread: \"updating ... with region split info\" [l.157] util.softsortedmap: \"reference for key ... has been cleared\" [l.181] master.basescanner: \"no longer has references to ... \" [l.339] info org.apache.hadoop.ipc.server: ipc server handler 5 on 60020, call batchupdate([b@11b8a00, org.apache.hadoop.hbase.io.batchupdate@10134ba) from 127.0.0.2:59620: error: ",
        "label": 38
    },
    {
        "text": " hbase thirdparty  rc0 doesn't include google protobuf so fails pb generation if you try to use it   tried to use the rc0. build failed with this sort of stuff: [info] --- protobuf-maven-plugin:0.5.0:compile (compile-protoc) @ hbase-protocol-shaded --- [info] compiling 31 proto file(s) to /users/stack/checkouts/hbase.git/hbase-protocol-shaded/target/generated-sources/protobuf/java [error] protoc failed: google/protobuf/any.proto: file not found. procedure.proto: import \"google/protobuf/any.proto\" was not found or had errors. procedure.proto:61:12: \"google.protobuf.any\" is not defined. lockservice.proto: import \"procedure.proto\" was not found or had errors. lockservice.proto:86:12: \"procedure\" is not defined. lockservice.proto:88:12: \"procedure\" is not defined. [error] /users/stack/checkouts/hbase.git/hbase-protocol-shaded/src/main/protobuf/client.proto [0:0]: google/protobuf/any.proto: file not found. procedure.proto: import \"google/protobuf/any.proto\" was not found or had errors. procedure.proto:61:12: \"google.protobuf.any\" is not defined. lockservice.proto: import \"procedure.proto\" was not found or had errors. lockservice.proto:86:12: \"procedure\" is not defined. lockservice.proto:88:12: \"procedure\" is not defined. [error] /users/stack/checkouts/hbase.git/hbase-protocol-shaded/src/main/protobuf/test_rpc_service.proto [0:0]: google/protobuf/any.proto: file not found. procedure.proto: import \"google/protobuf/any.proto\" was not found or had errors. procedure.proto:61:12: \"google.protobuf.any\" is not defined. lockservice.proto: import \"procedure.proto\" was not found or had errors. lockservice.proto:86:12: \"procedure\" is not defined. lockservice.proto:88:12: \"procedure\" is not defined. .... its because of the hbase-18709 clean out bundled proto files changes. let me revert them. am afraid relocating will introduce interesting incompat issues. ",
        "label": 314
    },
    {
        "text": "filter to support scanning multiple row key ranges  hbase is quite efficient when scanning only one small row key range. if user needs to specify multiple row key ranges in one scan, the typical solutions are: 1. through filterlist which is a list of row key filters, 2. using the sql layer over hbase to join with two table, such as hive, phoenix etc. however, both solutions are inefficient. both of them can\u2019t utilize the range info to perform fast forwarding during scan which is quite time consuming. if the number of ranges are quite big (e.g. millions), join is a proper solution though it is slow. however, there are cases that user wants to specify a small number of ranges to scan (e.g. <1000 ranges). both solutions can\u2019t provide satisfactory performance in such case.   we provide this filter (multirowrangefilter) to support such use case (scan multiple row key ranges), which can construct the row key ranges from user specified list and perform fast-forwarding during scan. thus, the scan will be quite efficient. ",
        "label": 237
    },
    {
        "text": "up default index interval in trunk and branch  see tail of hbase-900. index interval has biggest effect on ram. the pe test with its 1k cell size is fine w/ the current default of 32 but cells smaller than this are common and these fellas run into memory issues (see recent discussion on list). ",
        "label": 314
    },
    {
        "text": "copy loadincrementalhfiles to another package and mark the old one as deprecated  loadincrementalhfiles does not depend on map reduce. ",
        "label": 149
    },
    {
        "text": "test for hbase  avoid reading index blocks   in hbase-9915 we found that for encoded blocks the quick check whether the seek key falls on the current block did not work. this adds a unittest so we won't regress. the test counts the number of blocks accessed (real data/index block in 0.94 and cache hits in trunk). without hbase-9915 we see about 50% more blocks hit in this test. ",
        "label": 286
    },
    {
        "text": "testzookeeper testmultiplezk fails due to missing method getkeepalivezookeeperwatcher  from https://builds.apache.org/job/hbase-flaky-tests/23335/testreport/junit/org.apache.hadoop.hbase/testzookeeper/testmultiplezk/ : java.lang.nosuchmethodexception: org.apache.hadoop.hbase.client.connectionimplementation.getkeepalivezookeeperwatcher() at org.apache.hadoop.hbase.testzookeeper.getzookeeperwatcher(testzookeeper.java:136) at org.apache.hadoop.hbase.testzookeeper.testmultiplezk(testzookeeper.java:291) this is the top flaky test. ",
        "label": 402
    },
    {
        "text": "provide throttling for replication  when we disable a peer for a time of period, and then enable it, the replicationsource in master cluster will push the accumulated hlog entries during the disabled interval to the re-enabled peer cluster at full speed. if the bandwidth of the two clusters is shared by different applications, the push at full speed for replication can use all the bandwidth and severely influence other applications. though there are two config replication.source.size.capacity and replication.source.nb.capacity to tweak the batch size each time a push delivers, but if decrease these two configs, the number of pushes increase, and all these pushes proceed continuously without pause. and no obvious help for the bandwidth throttling. from bandwidth-sharing and push-speed perspective, it's more reasonable to provide a bandwidth up limit for each peer push channel, and within that limit, peer can choose a big batch size for each push for bandwidth efficiency. any opinion? ",
        "label": 203
    },
    {
        "text": "use default mode for hbase thrift gateway if not specified  the thrift gateway should start with a default mode if one is not selected. currently, instead we see: exception in thread \"main\" java.lang.assertionerror: exactly one option out of [-hsha, -nonblocking, -threadpool, -threadedselector] has to be specified at org.apache.hadoop.hbase.thrift.thriftserverrunner$impltype.setserverimpl(thriftserverrunner.java:201) at org.apache.hadoop.hbase.thrift.thriftserver.processoptions(thriftserver.java:169) at org.apache.hadoop.hbase.thrift.thriftserver.domain(thriftserver.java:85) at org.apache.hadoop.hbase.thrift.thriftserver.main(thriftserver.java:192) see also bigtop-648. ",
        "label": 38
    },
    {
        "text": "script to find hanging test cases in build  a script that parses the console output to get the hanging test names. this will be very useful to know the hanging test case names when we see in some builds all the test cases run but still the build shows failed. ",
        "label": 544
    },
    {
        "text": "allow coprocessor to interact with batches per region sent from a client  currently the coprocessor write hooks - {pre|post} {put|delete} - are strictly one row|cell operations.  it might be a good idea to allow a coprocessor to deal with batches of puts and deletes as they arrive from the client. ",
        "label": 46
    },
    {
        "text": "cellvalue class for transporting cell timestamp with cell value simultaneously  all of the get* methods take a timestamp parameter that means \"at least as old as x\". this is handy for getting data that fits your expectations about when it should exist. however, the result you get back doesn't actually contain the real timestamp the cell was stored at. for example, let's say you write the stock price for your favorite company into row \"yhoo\" at cell \"stock:price\". it takes the default timestamp of right now. then, a day passes. you want to get the most recent stock price for yhoo, and also when the price was gathered. in the current system, you couldn't do this at all without also doing a scan at the same time. if we added a new class called cellvalue that contained the byte[] cell value as well as the long timestamp of when it was stored, we could return an instance of this class wherever we used to return just the byte[]. this could be used in all the get() methods, getrow, getclosestatorbefore, etc. this has the advantage of making timestamp into a first-class citizen in hbase, which it hasn't been so far. thoughts? ",
        "label": 86
    },
    {
        "text": " dev  in the script called 'hbase'  we don't check for errors when generating the classpath with mvn  when it happens, it's difficult to guess. let's fix this. ",
        "label": 340
    },
    {
        "text": "hmaster cleanup and refactor  before doing the more significant changes to hmaster, it would benefit greatly from some cleanup, commenting, and a bit of refactoring. one motivation is to nail down the initialization flow and comment each step. another is to add a couple new classes to break up functionality into helpers to reduce hmaster size (for example, pushing all filesystem operations into their own class). and lastly to stop the practice of passing around references to hmaster everywhere and instead pass along only what is necessary. ",
        "label": 247
    },
    {
        "text": "master crashes on data that was moved from different host  while testing an upgrade to 0.90.0 rc3 i noticed that if i seeded our test data on one machine and transferred to another machine the hmaster on the new machine dies on startup. based on the following stack trace it looks as though it is attempting to find the .meta region with the ip address of the original machine. instead of waiting around for regionserver's to register with new location data, hmaster throws it's hands up with a fatal exception. note that deleting the zookeeper dir makes no difference. also note that so far i have only reproduced this in my own environment using the hbase-trx extension of hbase and an applicationstarter that starts the master and regionserver together in the same jvm. while the issue seems likely isolated from those factors it is far from a vanilla hbase environment. i will spend some time trying to reproduce the issue in a proper hbase test. but perhaps someone can beat me to it? how do i simulate the ip switch? may require a data.tar upload. [14/01/11 10:45:20] 6396 [ thread-298] error server.quorum.quorumpeerconfig - invalid configuration, only one server specified (ignoring)  [14/01/11 10:45:21] 7178 [ main] info ion.service.hbaseregionservice - troove> region port: 60010  [14/01/11 10:45:21] 7180 [ main] info ion.service.hbaseregionservice - troove> region interface: org.apache.hadoop.hbase.ipc.indexedregioninterface  [14/01/11 10:45:21] 7180 [ main] info ion.service.hbaseregionservice - troove> root dir: hdfs://localhost:8701/hbase  [14/01/11 10:45:21] 7180 [ main] info ion.service.hbaseregionservice - troove> initializing region server.  [14/01/11 10:45:21] 7631 [ main] info ion.service.hbaseregionservice - troove> starting region server thread.  [14/01/11 10:46:54] 100764 [ hmaster] fatal he.hadoop.hbase.master.hmaster - unhandled exception. starting shutdown.  java.net.sockettimeoutexception: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.socketchannel[connection-pending remote=192.168.1.102/192.168.1.102:60020]  at org.apache.hadoop.net.socketiowithtimeout.connect(socketiowithtimeout.java:213)  at org.apache.hadoop.net.netutils.connect(netutils.java:404)  at org.apache.hadoop.hbase.ipc.hbaseclient$connection.setupiostreams(hbaseclient.java:311)  at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:865)  at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:732)  at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:258)  at $proxy14.getprotocolversion(unknown source)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:419)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:393)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:444)  at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:349)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:954)  at org.apache.hadoop.hbase.catalog.catalogtracker.getcachedconnection(catalogtracker.java:384)  at org.apache.hadoop.hbase.catalog.catalogtracker.getmetaserverconnection(catalogtracker.java:283)  at org.apache.hadoop.hbase.catalog.catalogtracker.verifymetaregionlocation(catalogtracker.java:478)  at org.apache.hadoop.hbase.master.hmaster.assignrootandmeta(hmaster.java:435)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:382)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:277) ",
        "label": 314
    },
    {
        "text": "convert source tree into maven modules  when we originally converted the build to maven we had a single \"core\" module defined, but later reverted this to a module-less build for the sake of simplicity. it now looks like it's time to re-address this, as we have an actual need for modules to: provide a trimmed down \"client\" library that applications can make use of more cleanly support building against different versions of hadoop, in place of some of the reflection machinations currently required incorporate the secure rpc engine that depends on some secure hadoop classes i propose we start simply by refactoring into two initial modules: core - common classes and utilities, and client-side code and interfaces server - master and region server implementations and supporting code this would also lay the groundwork for incorporating the hbase security features that have been developed. once the module structure is in place, security-related features could then be incorporated into a third module \u2013 \"security\" \u2013 after normal review and approval. the security module could then depend on secure hadoop, without modifying the dependencies of the rest of the hbase code. ",
        "label": 236
    },
    {
        "text": "avoid arraylist iterator  explicitcolumntracker  i noticed that in a profiler (sampler) run scanquerymatcher.setrow(...) showed up at all.  in turns out that the expensive part is iterating over the columns in excplicitcolumntracker.reset(). i did some microbenchmarks and found that private arraylist<x> l; ... for (int i=0; i<l.size(); i++) {    x = l.get(i);    ... } is twice as fast as: private arraylist<x> l; ... for (x : l) {    ... } the indexed version asymptotically approaches the iterator version, but even at 1m entries it is still faster.  in my tight loop scans this provides for a 5% performance improvement overall when the excplicitcolumntracker is used. edit: private x[] l; ... for (int i=0; i<l.length; i++) {    x = l[i];    ... } is even better. apparently the jvm can even save the boundary check in each iteration. ",
        "label": 286
    },
    {
        "text": "interfaceaudience breaks on older versions of hadoop  ",
        "label": 154
    },
    {
        "text": "enforcer npe on hbase shaded invariants  mvn validate -prelease fails on hbase-shaded-check-invariants with: [error] failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-m1:enforce (min-maven-min-java-banned-xerces) on project hbase-shaded-check-invariants: execution min-maven-min-java-banned-xerces of goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-m1:enforce failed.: nullpointerexception -> [help 1] org.apache.maven.lifecycle.lifecycleexecutionexception: failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-m1:enforce (min-maven-min-java-banned-xerces) on project hbase-shaded-check-invariants: execution min-maven-min-java-banned-xerces of goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-m1:enforce failed.         at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:213)         at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:154)         at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:146)         at org.apache.maven.lifecycle.internal.lifecyclemodulebuilder.buildproject(lifecyclemodulebuilder.java:117)         at org.apache.maven.lifecycle.internal.lifecyclemodulebuilder.buildproject(lifecyclemodulebuilder.java:81)         at org.apache.maven.lifecycle.internal.builder.singlethreaded.singlethreadedbuilder.build(singlethreadedbuilder.java:51)         at org.apache.maven.lifecycle.internal.lifecyclestarter.execute(lifecyclestarter.java:128)         at org.apache.maven.defaultmaven.doexecute(defaultmaven.java:309)         at org.apache.maven.defaultmaven.doexecute(defaultmaven.java:194)         at org.apache.maven.defaultmaven.execute(defaultmaven.java:107)         at org.apache.maven.cli.mavencli.execute(mavencli.java:993)         at org.apache.maven.cli.mavencli.domain(mavencli.java:345)         at org.apache.maven.cli.mavencli.main(mavencli.java:191)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:498)         at org.codehaus.plexus.classworlds.launcher.launcher.launchenhanced(launcher.java:289)         at org.codehaus.plexus.classworlds.launcher.launcher.launch(launcher.java:229)         at org.codehaus.plexus.classworlds.launcher.launcher.mainwithexitcode(launcher.java:415)         at org.codehaus.plexus.classworlds.launcher.launcher.main(launcher.java:356) caused by: org.apache.maven.plugin.pluginexecutionexception: execution min-maven-min-java-banned-xerces of goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-m1:enforce failed.         at org.apache.maven.plugin.defaultbuildpluginmanager.executemojo(defaultbuildpluginmanager.java:145)         at org.apache.maven.lifecycle.internal.mojoexecutor.execute(mojoexecutor.java:208)         ... 20 more caused by: java.lang.nullpointerexception         at org.apache.maven.plugins.enforcer.enforcebytecodeversion.isbadartifact(enforcebytecodeversion.java:221)         at org.apache.maven.plugins.enforcer.enforcebytecodeversion.checkdependencies(enforcebytecodeversion.java:206)         at org.apache.maven.plugins.enforcer.enforcebytecodeversion.handleartifacts(enforcebytecodeversion.java:132)         at org.apache.maven.plugins.enforcer.abstractresolvedependencies.execute(abstractresolvedependencies.java:77)         at org.apache.maven.plugins.enforcer.enforcemojo.execute(enforcemojo.java:202)         at org.apache.maven.plugin.defaultbuildpluginmanager.executemojo(defaultbuildpluginmanager.java:134)         ... 21 more cc: sean busbey ",
        "label": 320
    },
    {
        "text": "testfixedlengthwrapper testreadwrite occasionally fails with the ibm jdk  here's the trace. tests run: 4, failures: 1, errors: 0, skipped: 0, time elapsed: 29.288 sec <<< failure! testreadwrite(org.apache.hadoop.hbase.types.testfixedlengthwrapper)  time elapsed: 0.025 sec  <<< failure! arrays first differed at element [8]; expected:<-40> but was:<0>         at org.junit.internal.comparisoncriteria.arrayequals(comparisoncriteria.java:50)         at org.junit.assert.internalarrayequals(assert.java:473)         at org.junit.assert.assertarrayequals(assert.java:294)         at org.junit.assert.assertarrayequals(assert.java:305)         at org.apache.hadoop.hbase.types.testfixedlengthwrapper.testreadwrite(testfixedlengthwrapper.java:60) this is with 0.98.0. ",
        "label": 38
    },
    {
        "text": "remove deprecated usage of hadoop httpserver in infoserver  recent changes in hadoop httpserver give npe when running on hadoop 3.0.0-snapshot. this way we use httpserver is deprecated and will probably be not fixed (see hdfs-5760). we'd better move to the new proposed builder pattern, which means we can no more use inheritance to build our nice infoserver. ",
        "label": 158
    },
    {
        "text": "rpc call may not be notified in secureclient  in secureclient.java, rpc responses will be processed by receiveresponse() which looks like: try {         int id = in.readint();                    // try to read an id         if (log.isdebugenabled())           log.debug(getname() + \" got value #\" + id);         call call = calls.remove(id);         int state = in.readint();     // read call status         if (log.isdebugenabled()) {           log.debug(\"call #\"+id+\" state is \" + state);         }         if (state == status.success.state) {           writable value = reflectionutils.newinstance(valueclass, conf);           value.readfields(in);                 // read value           if (log.isdebugenabled()) {             log.debug(\"call #\"+id+\", response is:\\n\"+value.tostring());           }           // it's possible that this call may have been cleaned up due to a rpc           // timeout, so check if it still exists before setting the value.           if (call != null) {             call.setvalue(value);           }         } else if (state == status.error.state) {           if (call != null) {             call.setexception(new remoteexception(writableutils.readstring(in), writableutils                 .readstring(in)));           }         } else if (state == status.fatal.state) {           // close the connection           markclosed(new remoteexception(writableutils.readstring(in),                                          writableutils.readstring(in)));         }       } catch (ioexception e) {         if (e instanceof sockettimeoutexception && remoteid.rpctimeout > 0) {           // clean up open calls but don't treat this as a fatal condition,           // since we expect certain responses to not make it by the specified           // {@link connectionid#rpctimeout}.           closeexception = e;         } else {           // since the server did not respond within the default ping interval           // time, treat this as a fatal condition and close this connection           markclosed(e);         }       } finally {         if (remoteid.rpctimeout > 0) {           cleanupcalls(remoteid.rpctimeout);         }       }     } in above code, in the try block, the call will be firstly removed from call map by:         call call = calls.remove(id); there may be two cases leading the call couldn't be notified and the invoking thread will wait forever.   firstly, if the returned status is status.fatal.state by:         int state = in.readint();     // read call status the code will come into: } else if (state == status.fatal.state) {           // close the connection           markclosed(new remoteexception(writableutils.readstring(in),                                          writableutils.readstring(in)));         } here, the secureconnection is marked as closed and all rpc calls in call map of this connection will be notified to receive an exception. however, the current rpc call has been removed from the call map, it won't be notified.  secondly, after the call has been removed by:         call call = calls.remove(id); if we encounter any exception before the 'try' block finished, the code will come into 'catch' and 'finally' block, neither 'catch' block nor 'finally' block will notify the rpc call because it has been removed from call map.  compared with receiveresponse() in hbaseclient.java, it may be better to get the rpc call from call map and remove it at the end of the 'try' block. ",
        "label": 238
    },
    {
        "text": "create hbase it module  create hbase-it, as per parent issue, and re-introduce hbase-4454 ",
        "label": 155
    },
    {
        "text": "count versions with same r c ts as one instance with most recent addition the winner  a few of us were chatting yesterday. if two edits go into hbase with same r/c/ts and they both go into the same memcache instance \u2013 i.e. there has not been a flush between additions \u2013 then the last insert will overwrite any added earlier (edits go into a map). that seems fine to me (what others think?). but if a flush happens in between edits, hbase gives back different answers. here's how. each instance of a cell is treated as a 'version', even if same r/c/ts. if you ask for n > 1 versions, and if two cells of same r/c/ts, only there has been a flush between their additions, you'll get back two versions with same r/c/ts. if no flush between their upload, you'll get back one version only because the later overrode the former. this issue is about closing this hole. suppress same r/c/ts when returning 'versions'. ",
        "label": 357
    },
    {
        "text": "rewrite getclosestatorjustbefore  doesn't scale as currently written  as currently written, as a table gets bigger, the number of rows .meta. needs to keep count of grows. as written, our getclosestatorjustbefore, goes through every storefile and in each picks up any row that could be a possible candidate for closest before. it doesn't just get the closest from the storefile, but all keys that are closest before. its not selective because how can it tell at the store file level which of the candidates will survive deletes that are sitting in later store files or up in memcache. so, if a store file has keys 0-10 and we ask to get the row that is closest or just before 7, it returns rows 0-7.. and so on per store file. can bet big and slow weeding key wanted. ",
        "label": 314
    },
    {
        "text": "add multi get to remotehtable  rest server supports multi-get, so the remotehtable class should as well. ",
        "label": 154
    },
    {
        "text": "hbck  repairholes  usage inconsistent with  fixhdfsorphans   according to the hbck's help info, shortcut - \"-repairholes\" will enable \"-fixhdfsorphans\" as below.  -repairholes      shortcut for -fixassignments -fixmeta -fixhdfsholes -fixhdfsorphans however, in the implementation, the function \"fsck.setfixhdfsorphans(false);\" is called in \"-repairholes\". this is not consistent with the usage information. ",
        "label": 239
    },
    {
        "text": "tableservers's cachedregionlocation doesn't have size limit   cachedregionlocation stores region locations of tables whenever new region is looked up. however, the enties are deleted only when tableservers object is closed or locateregion is called with false usecache argument. therefore, it seems to grow without limit and cause out of memory exception. ",
        "label": 38
    },
    {
        "text": "package name for compression should not contain hfile  compression codecs does not have any hfile specific functionality, and can be used elsewhere (rpc, hlog, see: hbase-6966) we should move o.a.h.h.io.hfile.compession and related files from io.hfile pacakge. we can use io.compress to be in line with hadoop. ",
        "label": 155
    },
    {
        "text": "master should reassign regions away from regionservers under heap stress  once the changes for hbase-1018 go in, the master should reassign regions away from regionservers that indicate heap stress to those that do not. reassignment activity must be smart enough not to overload remaining regionservers. ",
        "label": 38
    },
    {
        "text": "error handling for filenotfoundexception should consider remoteexception in openreader   in region server log, i observed the following: org.apache.hadoop.ipc.remoteexception(java.io.filenotfoundexception): file does not exist: /apps/hbase/data/wals/lx.p.com,16020,1497300923131/497300923131. default.1497302873178   at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:71)   at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:61)   at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsint(fsnamesystem.java:1860)   at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1831)   at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1744) ...   at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81)   at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:326)   at org.apache.hadoop.fs.filterfilesystem.open(filterfilesystem.java:162)   at org.apache.hadoop.fs.filesystem.open(filesystem.java:782)   at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:293)   at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:267)   at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:255)   at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:414)   at org.apache.hadoop.hbase.replication.regionserver.replicationwalreadermanager.openreader(replicationwalreadermanager.java:69)   at org.apache.hadoop.hbase.replication.regionserver.replicationsource.openreader(replicationsource.java:605)   at org.apache.hadoop.hbase.replication.regionserver.replicationsource.run(replicationsource.java:364) we have code in replicationsource#openreader() which is supposed to handle filenotfoundexception but remoteexception wrapping filenotfoundexception was missed. ",
        "label": 441
    },
    {
        "text": "testscannersfromclientside testscanbatch failed with 'there should have room before prefetching is submitted'  see http://54.241.6.143/job/hbase-0.95-hadoop-2/org.apache.hbase$hbase-server/543/testreport/org.apache.hadoop.hbase.client/testscannersfromclientside/testscanbatch_2_/ ",
        "label": 314
    },
    {
        "text": "move memcache to concurrentskiplistmap from concurrentskiplistset  the cslm will replace old entry with a new when you put. the csls will not replace if existent key making for a test, and if present, remove semantic which to be safe needs synchronizing (replacement is a ryan rawson suggestion). ",
        "label": 314
    },
    {
        "text": "fix findbugs warnings in snapshot classes  fix findbugs warnings introduced by snapshot code ",
        "label": 441
    },
    {
        "text": "add delete setwritetowal functionality  for puts, write to wal can be disabled, but for deletes this functionality is missing. the regionserver internally already passes around a writetowal flag, but it is not possible to set this from the client. the attached patch introduces this. this changes the serialization format of delete, so bumped up the version. i verified manually that the wal is indeed not growing when writetowal is set to false. ",
        "label": 83
    },
    {
        "text": "hbase shouldperformmajorcompaction logic is not correct  we can know that the major compaction is skipping  from the below regionserver's log, but it is compacting that region. and read the code and find it is not correct  and i add mark  \"/*** ***/\" below   public boolean shouldperformmajorcompaction(final collection<storefile> filestocompact)  throws ioexception {  if (lowtimestamp > 0l && lowtimestamp < (now - mctime)) {  if (filestocompact.size() == 1) {  if (sf.ismajorcompaction() && (cfttl == long.max_value || oldest < cfttl)) {  float blocklocalityindex =  sf.gethdfsblockdistribution().getblocklocalityindex(  rsrpcservices.gethostname(comconf.conf, false));  if (blocklocalityindex < comconf.getminlocalitytoforcecompact()) { result = true; } else { log.debug(\"skipping major compaction of \" + regioninfo + \" because one (major) compacted file only, oldesttime \" + oldest + \"ms is < ttl=\" + cfttl + \" and blocklocalityindex is \" + blocklocalityindex + \" (min \" + comconf.getminlocalitytoforcecompact() + \")\"); } } else if (cfttl != hconstants.forever && oldest > cfttl) { log.debug(\"major compaction triggered on store \" + regioninfo + \", because keyvalues outdated; time since last major compaction \" + (now - lowtimestamp) + \"ms\"); result = true; } } else { log.debug(\"major compaction triggered on store \" + regioninfo + \"; time since last major compaction \" + (now - lowtimestamp) + \"ms\"); } result = true;  /****here it is not right, it should be move to the above *****/  }  return result;  }   2019-09-27 09:09:35,960 debug [st129,16020,1568236573216_choreservice_1] compactions.ratiobasedcompactionpolicy: skipping major compaction of 100e_point_point_2ddata_z3_geom_gpstime_v6,\\x17,1568215725799.413a563092544e8df480fd601b2de71b. because one (major) compacted file only, oldesttime 3758085589ms is < ttl=9223372036854775807 and blocklocalityindex is 1.0 (min 0.0)  2019-09-27 09:09:35,961 debug [st129,16020,1568236573216_choreservice_1] compactions.sortedcompactionpolicy: selecting compaction from 1 store files, 0 compacting, 1 eligible, 100 blocking  2019-09-27 09:09:35,961 debug [st129,16020,1568236573216_choreservice_1] regionserver.hstore: 413a563092544e8df480fd601b2de71b - d: initiating major compaction (all files)  2019-09-27 09:09:35,961 debug [st129,16020,1568236573216_choreservice_1] regionserver.compactsplitthread: large compaction requested: org.apache.hadoop.hbase.regionserver.defaultstoreengine$defaultcompactioncontext@4b5582f1; because: compactionchecker requests major compaction; use default priority; compaction_queue=(1:0), split_queue=0, merge_queue=0  2019-09-27 09:09:35,961 info [regionserver/st129/10.3.72.129:16020-longcompactions-1568236575579] regionserver.hregion: starting compaction on d in region 100e_point_point_2ddata_z3_geom_gpstime_v6,\\x17,1568215725799.413a563092544e8df480fd601b2de71b.  2019-09-27 09:09:35,961 info [regionserver/st129/10.3.72.129:16020-longcompactions-1568236575579] regionserver.hstore: starting compaction of 1 file(s) in d of 100e_point_point_2ddata_z3_geom_gpstime_v6,\\x17,1568215725799.413a563092544e8df480fd601b2de71b. into tmpdir=hdfs://st129:8020/hbase/data/default/100e_point_point_2ddata_z3_geom_gpstime_v6/413a563092544e8df480fd601b2de71b/.tmp, totalsize=5.1 g  2019-09-27 09:09:35,961 debug [regionserver/st129/10.3.72.129:16020-longcompactions-1568236575579] compactions.compactor: compacting hdfs://st129:8020/hbase/data/default/100e_point_point_2ddata_z3_geom_gpstime_v6/413a563092544e8df480fd601b2de71b/d/3b4080f9b6f149e1b0a476058c8564e6, keycount=83914030, bloomtype=none, size=5.1 g, encoding=fast_diff, compression=snappy, seqnum=2621061, earliestputts=1565788490371 ",
        "label": 532
    },
    {
        "text": "integrationtest for backup and restore  see chatter at https://docs.google.com/document/d/1xbpllkjocpq2ldqjbskf6und  ag0mzgoxek6p3polemc/edit?usp=sharing we need to get an integrationtest in place for backup and restore. ",
        "label": 478
    },
    {
        "text": "hbase won't start when hbase rootdir uses viewfilesystem  hbase currently doesn't work with hdfs federation (hbase.rootdir with a client that uses viewfs) because hlog#init uses filesystem#getdefaultblocksize and getdefaultreplication. these throw an exception because there is no default filesystem in a viewfs client so there's no way to determine a default block size or replication factor. they could use the versions of these methods that take a path, however these were introduced in hadoop-8014 and are not yet available in hadoop 1.x. ",
        "label": 441
    },
    {
        "text": "testlogrollperiod testwithedits may fail due to insufficient waiting  the test waits for minrolls rolls by sleeping:     thread.sleep((minrolls + 1) * log_roll_period); however, the above wait period may not be sufficient.  see https://builds.apache.org/job/hbase-trunk/4895/testreport/junit/org.apache.hadoop.hbase.regionserver.wal/testlogrollperiod/testwithedits/ : 2014-02-06 23:02:25,710 debug [rs:0;quirinus:56476.logroller] regionserver.logroller(87): hlog roll period 4000ms elapsed ... 2014-02-06 23:02:30,275 debug [rs:0;quirinus:56476.logroller] regionserver.logroller(87): hlog roll period 4000ms elapsed the interval between two successive periodic rolls was ~1.5s longer than log_roll_period (4s)  1.5s * 4 (minrolls-1) > 4s (log_roll_period) this led to the test failure: java.lang.assertionerror at org.junit.assert.fail(assert.java:86) at org.junit.assert.asserttrue(assert.java:41) at org.junit.assert.assertfalse(assert.java:64) at org.junit.assert.assertfalse(assert.java:74) at org.apache.hadoop.hbase.regionserver.wal.testlogrollperiod.checkminlogrolls(testlogrollperiod.java:168) at org.apache.hadoop.hbase.regionserver.wal.testlogrollperiod.testwithedits(testlogrollperiod.java:130) ",
        "label": 309
    },
    {
        "text": "revisit compaction policies after hbase commit  hbase-2248 turned gets into scans server-side. it also removed the invariant that deletes in a file only apply to other files and not itself (no longer processes memstore deletes when the delete happens). this has implications for our minor compaction policy. we are currently processing deletes during minor compactions in a way that makes it so we do the actual deleting as we compact, but we retain the delete records themselves. this makes it so we retain the invariant of deletes only applying to other files. since this is now gone post hbase-2248, we should revisit our compaction policies. ",
        "label": 247
    },
    {
        "text": " shell  record table property splits file in descriptor  when i check the properties of hbase table on web ui, some tables have splits_file property but some don't. in fact, those tables pre-split correctly but this property is not stored in .tableinfo in hdfs. but some table do, that's a little weird. knowing splits_file helps us to compare the backup table in different cluster with the original one. ",
        "label": 520
    },
    {
        "text": "illegalaccesserror running rowcounter  below is from billy pearson up on the list: billy pearson wrote: > i get this when i run rowcounter in the hbase jar > > java.lang.illegalaccesserror: tried to access method org.apache.hadoop.ipc.client.inccount()v from class org.apache.hadoop.ipc.hbaseclient >        at org.apache.hadoop.ipc.hbaseclient.inccount(hbaseclient.java:39) >        at org.apache.hadoop.hbase.ipc.hbaserpc$clientcache.getclient(hbaserpc.java:179) >        at org.apache.hadoop.hbase.ipc.hbaserpc$clientcache.access$200(hbaserpc.java:156) >        at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.<init>(hbaserpc.java:224) >        at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:336) >        at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:327) >        at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:364) >        at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:302) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.gethregionconnection(hconnectionmanager.java:764) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locaterootregion(hconnectionmanager.java:815) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:457) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:431) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:510) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:467) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:431) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregioninmeta(hconnectionmanager.java:510) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:471) >        at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.locateregion(hconnectionmanager.java:431) >        at org.apache.hadoop.hbase.client.htable.<init>(htable.java:125) >        at org.apache.hadoop.hbase.client.htable.<init>(htable.java:110) >        at org.apache.hadoop.hbase.mapred.tableinputformat.configure(tableinputformat.java:60) >        at org.apache.hadoop.util.reflectionutils.setconf(reflectionutils.java:58) >        at org.apache.hadoop.util.reflectionutils.newinstance(reflectionutils.java:82) >        at org.apache.hadoop.mapred.jobconf.getinputformat(jobconf.java:400) >        at org.apache.hadoop.mapred.jobclient.submitjob(jobclient.java:705) >        at org.apache.hadoop.mapred.jobclient.runjob(jobclient.java:973) >        at com.compspy.mapred.rowcounter.run(rowcounter.java:111) >        at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65) >        at com.compspy.mapred.rowcounter.main(rowcounter.java:126) >        at sun.reflect.nativemethodaccessorimpl.invoke0(native method) >        at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) >        at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) >        at java.lang.reflect.method.invoke(method.java:597) >        at org.apache.hadoop.util.programdriver$programdescription.invoke(programdriver.java:68) >        at org.apache.hadoop.util.programdriver.driver(programdriver.java:139) >        at com.compspy.mapred.driver.main(driver.java:24) >        at sun.reflect.nativemethodaccessorimpl.invoke0(native method) >        at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) >        at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) >        at java.lang.reflect.method.invoke(method.java:597) >        at org.apache.hadoop.util.runjar.main(runjar.java:155) >        at org.apache.hadoop.mapred.jobshell.run(jobshell.java:194) >        at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65) >        at org.apache.hadoop.util.toolrunner.run(toolrunner.java:79) >        at org.apache.hadoop.mapred.jobshell.main(jobshell.java:220) sebastien rainville just had a related issue. j-d investigating found a workaround. adding hbase.jar to $hadoop_home/conf/hadoop-env.sh#hadoop_classpath ",
        "label": 314
    },
    {
        "text": "implement compactor for stripe compactions  compactor needs to be implemented. see details in parent and blocking jira. ",
        "label": 406
    },
    {
        "text": "document hbck improvements  hbase  hbase  we had a couple hbck improvements recently: hbase-6173 and hbase-5360.  we should document them. especially, for hbase-5360, it's something  one normally doesn't do. ",
        "label": 242
    },
    {
        "text": "cleanup old snapshots on start  if the master is hard stopped (i.e. kill -9), the snapshot handler or snapshotmanager may not have a chance to cleanup after the snapshot, leaving extraneous files in the working snapshot directory (/hbase/.snapshot/.tmp directory). ",
        "label": 236
    },
    {
        "text": "there should be a search option in the book which helps you to search the content in the book   i am not able to search a particular content in the book.html. there is no provision to parse all the html pages (topics in the book) with a keyword and get back the list of topics related. ",
        "label": 330
    },
    {
        "text": "move compression decompression to an encoder specific encoding context  as part of working on hbase-5313, we want to add a new columnar encoder/decoder. it makes sense to move compression to be part of encoder/decoder:  1) a scanner for a columnar encoded block can do lazy decompression to a specific part of a key value object  2) avoid an extra bytes copy from encoder to hblock-writer. if there is no encoder specified for a writer, the hblock.writer will use a default compression-context to do something very similar to today's code. ",
        "label": 197
    },
    {
        "text": "add mergeregionsasync with a list of region names method in asyncadmin  although we only support merging two regions until now, but the rpc protocol does support passing more than two regions to master. so i think we should provide the methods, but need to add comments to say that for now we only support merging two regions so do not try to pass more than two regions. ",
        "label": 149
    },
    {
        "text": "few logging improvements around enabling tables  few log statements between enable/disable/create table handler event classes have a typo with word \"attempting\" (its misspelled \"attemping\"). even upon an enable operation's failure, the tailing message is a mere info with a state of 'false'. this isn't as visible as i'd like it to be when diagnosing logs for issues. i've put it in a proper if-else for this case. ",
        "label": 194
    },
    {
        "text": "eliminate log spam when tailing files  tailing a file involves reopening it for each seek, so for example when running replication it looks like this every time: 2013-08-28 21:45:00,205 debug [regionserver60020-eventthread.replicationsource,1] org.apache.hadoop.hbase.fs.hfilesystem: /hbase-master/wals/jdec2hbase0403-5,60020,1377724981186/jdec2hbase0403-5%2c60020%2c1377724981186.1377725847406 is an hlog file, so reordering blocks, last hostname will be:jdec2hbase0403-5 2013-08-28 21:45:00,211 debug [regionserver60020-eventthread.replicationsource,1] org.apache.hadoop.hbase.regionserver.wal.protobuflogreader: after reading the trailer: waleditsstopoffset: 0, filelength: 0, trailerpresent: false the frequency makes it more relevant for traceing. ",
        "label": 229
    },
    {
        "text": "add client side metrics for received pushback signals  hbase-12911 added client side metrics. hbase-5162 added a mechanism for sending advisory backpressure signals to clients when the server is heavily loaded, and hbase-12702 and subtasks backported this to all active branches. add client-side metrics for received pushback signal. ",
        "label": 198
    },
    {
        "text": "backport hbase  disable error prone for hbase protocol shaded  to branch  otherwise the error prone compile will cost a very long time when processing the hbase-shaded-protocol module... ",
        "label": 149
    },
    {
        "text": "improve stack trace info dumped by foreignexceptionsnare rethrowexception  currently the way rethrowexception works it throws an exception with the stack where the foreignexception was locally created (normally where it gets deserialized, and with a getcause with the stack of the original thread on the remote thread. unfortunately, this doesn't provide any in formation about which call to rethrowexception locally tripped over the exception. this simple patch wraps again to get the stack info which provides this missing context. ",
        "label": 248
    },
    {
        "text": "run with   wal in hregionserver  ",
        "label": 402
    },
    {
        "text": "htabledescriptor properties not preserved when cloning  when cloning a snapshot to a new table the values[1] set on the original htabledescriptor are not preserved. this is because the values are not pulled off and set in the restoresnapshothelper.clonetableschema(..)[2] method. this affects a number of things such as custom properties but also for configured coprocessors for the table. [1] - http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/htabledescriptor.html#setvalue(byte[], byte[])  [2] - https://github.com/apache/hbase/blob/0.94.6.1/src/main/java/org/apache/hadoop/hbase/snapshot/restoresnapshothelper.java#l582 ",
        "label": 309
    },
    {
        "text": "move tablestatemanager and zktablestatemanager and server to hbase server  ",
        "label": 21
    },
    {
        "text": "vet categorization of tests so they for sure go into the right small medium large buckets  i tried doing runsmalltests, runmediumtests, etc., and i noticed that some tests are larger than our categorization. at least for small tests it means they area all running in the one jvm. i also noticed that the categorization only takes effect in hbase-server. this patch makes it so runsmalltests runs all the small tests only in each category. also moves tests that were larger than small out to medium so they don't run in the one jvm anymore. ",
        "label": 314
    },
    {
        "text": " not flushing memstore for region  keep on printing for half an hour  \"not flushing memstore for region\" keep on printing for half an hour in the regionserver. then i restart hbase. i think there may be deadlock or cycling.  i know that when splitting region, it will doclose of region, and set writestate.writesenabled = false and may run close preflush. this will make flush fail and print \"not flushing memstore for region\". but it should be finished after a while. logs:  2011-04-18 16:28:27,960 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. because regionserver60020.cacheflusher; priority=-1, compaction queue size=1  2011-04-18 16:28:30,171 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.  2011-04-18 16:28:30,171 warn org.apache.hadoop.hbase.regionserver.memstoreflusher: region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. has too many store files; delaying flush up to 90000ms  2011-04-18 16:28:32,119 info org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter: using syncfs \u2013 hdfs-200  2011-04-18 16:28:32,285 info org.apache.hadoop.hbase.regionserver.wal.hlog: roll /hbase/.logs/linux253,60020,1303123943360/linux253%3a60020.1303124206693, entries=5226, filesize=255913736. new hlog /hbase/.logs/linux253,60020,1303123943360/linux253%3a60020.1303124311822  2011-04-18 16:28:32,287 debug org.apache.hadoop.hbase.regionserver.wal.hlog: found 1 hlogs to remove out of total 2; oldest outstanding sequenceid is 11037 from region 031f37c9c23fcab17797b06b90205610  2011-04-18 16:28:32,288 info org.apache.hadoop.hbase.regionserver.wal.hlog: moving old hlog file /hbase/.logs/linux253,60020,1303123943360/linux253%3a60020.1303123945481 whose highest sequenceid is 6052 to /hbase/.oldlogs/linux253%3a60020.1303123945481  2011-04-18 16:28:42,701 info org.apache.hadoop.hbase.regionserver.store: completed major compaction of 4 file(s), new file=hdfs://10.18.52.108:9000/hbase/ufdr/031f37c9c23fcab17797b06b90205610/value/4398465741579485290, size=281.4m; total size for store is 468.8m  2011-04-18 16:28:42,712 info org.apache.hadoop.hbase.regionserver.hregion: completed compaction on region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. after 1mins, 40sec  2011-04-18 16:28:42,741 info org.apache.hadoop.hbase.regionserver.splittransaction: starting split of region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.  2011-04-18 16:28:42,770 debug org.apache.hadoop.hbase.regionserver.hregion: closing ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.: disabling compactions & flushes  2011-04-18 16:28:42,770 info org.apache.hadoop.hbase.regionserver.hregion: running close preflush of ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.  2011-04-18 16:28:42,771 debug org.apache.hadoop.hbase.regionserver.hregion: started memstore flush for ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., current region memstore size 105.6m  2011-04-18 16:28:42,818 debug org.apache.hadoop.hbase.regionserver.hregion: finished snapshotting, commencing flushing stores  2011-04-18 16:28:42,846 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesenabled=false  2011-04-18 16:28:42,849 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.  2011-04-18 16:28:42,849 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesenabled=false ......  2011-04-18 17:04:08,803 debug org.apache.hadoop.hbase.regionserver.hregion: not flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesenabled=false  2011-04-18 17:04:08,803 debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.  mon apr 18 17:04:24 ist 2011 starting regionserver on linux253 ulimit -n 1024 ",
        "label": 562
    },
    {
        "text": "completebulkload does not use hbase configuration  the completebulkupload tool should be using the hbaseconfiguration.create() method to get the hbase configuration in 0.90.*. in it's present state, you receive a connection error when running this tool. ",
        "label": 338
    },
    {
        "text": "deprecate chore  its a thread per task when we should have one thread to do all tasks  should use something like scheduledthreadpoolexecutor instead (elliott said this first i think; j-d said something similar just now). ",
        "label": 249
    },
    {
        "text": "duplicate servlet api jars in hbase  on mailing list, http://search-hadoop.com/m/wtckhs5ujq, jerry he reports we have doubled jars: [biadmin@hdtest009 lib]$ ls -l jsp-api* -rw-rw-r-- 1 biadmin biadmin 134910 sep 17 01:13 jsp-api-2.1-6.1.14.jar -rw-rw-r-- 1 biadmin biadmin 100636 sep 17 01:27 jsp-api-2.1.jar [biadmin@hdtest009 lib]$ ls -l servlet-api* -rw-rw-r-- 1 biadmin biadmin 132368 sep 17 01:13 servlet-api-2.5-6.1.14.jar -rw-rw-r-- 1 biadmin biadmin 105112 sep 17 01:12 servlet-api-2.5.jar fix in 0.96.2. ",
        "label": 314
    },
    {
        "text": "typo in namespace docs  in the docs at http://hbase.apache.org/book.html#_namespace - \"region server groups (hbase-6721) - a namespace/table can be pinned onto a subset of regionservers thus guaranteeing a course level of isolation.\" should be \"coarse\" ",
        "label": 106
    },
    {
        "text": "consolidate multiple overloaded methods in hregioninterface  hregionserver  there are too many overloaded methods in hregionserverinterface and consequently hregionserver. these should be consolidated into one method per operation and the client should pass the appropriate parameters to the server. on the server side, the single method should be able to handle parameters that are not supplied, e.g., long values that are not supplied should be -1 boolean values should be supplied appropriately objects that are not supplied should be passed as null all these overloaded methods eventually call the same method on the server side eventually. removing the overloads would make following control flow easier. ",
        "label": 229
    },
    {
        "text": "hot region   write diagnosis  we should provide a basic way for end users to operationally diagnose hot row problems. thinking about a 2-phase approach: 1. diagnose hot regions  2. inspect those regions/servers to find the hot rows. to diagnose hot regions, we could query the master or regionservers for these regions + sort. to inspect the regions for hot rows, we could write another script to analyze the hlogs on a server and basically do: sort log|uniq -n|sort -n|top ",
        "label": 376
    },
    {
        "text": "possible inconsistency in a memstore read after a reseek  possible performance improvement  this follows the dicussion around hbase-3855, and the random errors (20% failure on trunk) on the unit test org.apache.hadoop.hbase.regionserver.testhregion.testwriteswhilegetting i saw some points related to numiterreseek, used in the memstorescanner#getnext (line 690): 679     protected keyvalue getnext(iterator it) { 680       keyvalue ret = null; 681       long readpoint = readwriteconsistencycontrol.getthreadreadpoint(); 682       //debugprint.println( \" ms@\" + hashcode() + \": threadpoint = \" + readpoint); 683   684       while (ret == null && it.hasnext()) { 685         keyvalue v = it.next(); 686         if (v.getmemstorets() <= readpoint) { 687           // keep it. 688           ret = v; 689         } 690         numiterreseek--; 691         if (numiterreseek == 0) { 692           break; 693          } 694       } 695       return ret; 696     } this function is called by seek, reseek, and next. the numiterreseek is only usefull for reseek. there are some issues, i am not totally sure it's the root cause of the test case error, but it could explain partly the randomness of the error, and one point is for sure a bug. 1) in getnext, numiterreseek is decreased, then compared to zero. the seek function sets numiterreseek to zero before calling getnext. it means that the value will be actually negative, hence the test will always fail, and the loop will continue. it is the expected behaviour, but it's quite smart. 2) in \"reseek\", numiterreseek is not set between the loops on the two iterators. if the numiterreseek is equals to zero after the loop on the first one, the loop on the second one will never call seek, as numiterreseek will be negative. 3) still in \"reseek\", the test to call \"seek\" is (kvsetnextrow == null && numiterreseek == 0). in other words, if kvsetnextrow is not null when numiterreseek equals zero, numiterreseek will start to be negative at the next iteration and seek will never be called. 4) you can have side effects if reseek ends with a numiterreseek > 0: the following calls to the \"next\" function will decrease numiterreseek to zero, and getnext will break instead of continuing the loop. as a result, later calls to next() may return null or not depending on how is configured the default value for numiterreseek. to check if the issue comes from point 4, you can set the numiterreseek to zero before returning in reseek:       numiterreseek = 0;       return (kvsetnextrow != null || snapshotnextrow != null);     } on my env, on trunk, it seems to work, but as it's random i am not really sure. i also had to modify the test (i added a loop) to make it fails more often, the original test was working quite well here. it has to be confirmed that this totally fix (it could be partial or unrelated) org.apache.hadoop.hbase.regionserver.testhregion.testwriteswhilegetting before implementing a complete solution. ",
        "label": 340
    },
    {
        "text": "fileinputstream in jenkinshash main  is never closed      fileinputstream in = new fileinputstream(args[0]); the above fileinputstream is not closed. ",
        "label": 441
    },
    {
        "text": "use zk based read write lock to make flush type snapshot robust against table enable disable alter  current region split following flush would fail snapshot. we can utilize zk-based read/write lock to make flush-type snapshot robust ",
        "label": 441
    },
    {
        "text": "speedup loadincrementalhfiles by parallelizing hfile splitting  from adam w.r.t. hfile splitting:  there's actually a good number of messages of that type (hfile no longer fits inside a single region), unfortunately i didn't take a timestamp on just when i was running with the patched jars vs the regular ones, however from the logs i can say that this is occurring fairly regularly on this system. the cluster i tested this on is our backup cluster, the mapreduce jobs on our production cluster output hfiles which are copied to the backup and then loaded into hbase on both. since the regions may be somewhat different on the backup cluster i would expect it to have to split somewhat regularly. this jira complements hbase-3721 by parallelizing hfile splitting which is done in the main thread. ",
        "label": 441
    },
    {
        "text": " rest  result generators do not need to query table schema  now, the rowresultgenerator and the scanerresultgenerator will fit the column family if the request doesn't contain any column info.       if (rowspec.hascolumns()) {         //addcolumn for get or scan       } else {         for (hcolumndescriptor family:              table.gettabledescriptor().getfamilies()) {           scan/get.addfamily(family.getname());         }       } the table.gettabledescriptor() will cost 10+ milliseconds in our hbase cluster each request. we can remove these code because the server will auto add the columns. ",
        "label": 38
    },
    {
        "text": "deprecated rsgroupadminendpoint and make rsgroup feature always enabled  as said in the design doc, rs group feature will always be enabled, but if you do not add any new rs groups, all the region servers and tables will be in the default group. ",
        "label": 149
    },
    {
        "text": "hbase hadoop1 compat conflicts with  dhadoop profile  i'm unclear on the root cause / fix. here is the scenario: mvn clean package install -dhadoop.profile=2.0 -dskiptests bin/start-hbase.sh fails with caused by: java.lang.classnotfoundexception: org.apache.hadoop.metrics2.lib.metricmutable         at java.net.urlclassloader$1.run(urlclassloader.java:202)         at java.security.accesscontroller.doprivileged(native method)         at java.net.urlclassloader.findclass(urlclassloader.java:190)         at java.lang.classloader.loadclass(classloader.java:306)         at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) doing rm -rf hbase-hadoop1-compat/target/ makes it work. in the pom.xml, we never reference hadoop2-compat. but doing so does not help: hadoop1-compat is compiled and takes precedence over hadoop2... ",
        "label": 154
    },
    {
        "text": "hbase book client html has a broken link  in section 9.3.2 - writebuffer and batch methods, the link \"acid semantics\" is broken. ",
        "label": 476
    },
    {
        "text": " jdk7  testhbasesaslrpcclient testhbasesaslrpcclientcreation failing on jenkins  testhbasesaslrpcclient.testhbasesaslrpcclientcreation will fail up on jenkins in builds using \"jdk 7 (latest)\" but not those using \"jdk 6 (latest)\". the stacktrace: java.lang.assertionerror         at org.junit.assert.fail(assert.java:86)         at org.junit.assert.asserttrue(assert.java:41)         at org.junit.assert.asserttrue(assert.java:52)         at org.apache.hadoop.hbase.security.testhbasesaslrpcclient.testhbasesaslrpcclientcreation(testhbasesaslrpcclient.java:119) ",
        "label": 38
    },
    {
        "text": "on startup  rinse startcode and server from  meta   look into what it would take purging startcode and server from .meta. on startup. it might make startup run faster. in particular, clint morgan was asking for faster startup. below is from a reply. the +1 is from jk. > > looking at code, we have the concept of an 'initial' scan. i  > > wonder if things would run faster for you if on the initial  > > scan we just cleared all server and startcode entries in  > > .meta. rather than wait on regionserver reports? +1 ",
        "label": 229
    },
    {
        "text": "testsnapshotcloneindependence fails in trunk builds intermittently  i was looking at https://builds.apache.org/job/hbase-trunk/3959/testreport/org.apache.hadoop.hbase.client/testsnapshotcloneindependence/testofflinesnapshotregionoperationsindependent/ and found the following: 2013-03-14 11:11:07,323 info  [pool-1-thread-1] hbase.resourcechecker(171): after: client.testsnapshotcloneindependence#testofflinesnapshotregionoperationsindependent thread=275 (was 273) potentially hanging thread: janus.apache.org,53542,1363259346619-daughteropener=34172719c055b187015add70302ab50b java.lang.object.wait(native method) java.lang.object.wait(object.java:503) org.apache.hadoop.ipc.client.call(client.java:1093) org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:229) $proxy10.rename(unknown source) sun.reflect.generatedmethodaccessor28.invoke(unknown source) sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) java.lang.reflect.method.invoke(method.java:601) org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:85) org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:62) $proxy10.rename(unknown source) sun.reflect.generatedmethodaccessor28.invoke(unknown source) sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) java.lang.reflect.method.invoke(method.java:601) org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:267) $proxy19.rename(unknown source) org.apache.hadoop.hdfs.dfsclient.rename(dfsclient.java:955) org.apache.hadoop.hdfs.distributedfilesystem.rename(distributedfilesystem.java:227) org.apache.hadoop.fs.filterfilesystem.rename(filterfilesystem.java:144) org.apache.hadoop.hbase.regionserver.hregionfilesystem.writeregioninfoonfilesystem(hregionfilesystem.java:476) org.apache.hadoop.hbase.regionserver.hregionfilesystem.checkregioninfoonfilesystem(hregionfilesystem.java:435) org.apache.hadoop.hbase.regionserver.hregion.initializeregioninternals(hregion.java:567) org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:546) org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:4041) org.apache.hadoop.hbase.regionserver.splittransaction.opendaughterregion(splittransaction.java:527) org.apache.hadoop.hbase.regionserver.splittransaction$daughteropener.run(splittransaction.java:508) java.lang.thread.run(thread.java:722) ",
        "label": 155
    },
    {
        "text": "hbcktestingutil needs to keep a static executor to lower the number of threads used  i can't run testhbasefsck on my machine without running out of threads, that's because each new hbasefsck creates 50 threads. it has a constructor that takes an executor and hbcktestingutil could keep a static one to pass in. i was able to cut thousands of threads with this. ",
        "label": 229
    },
    {
        "text": "htabledescriptor addcoprocessor will always make regioncoprocessorhost create new configuration  htabledescriptor#addcoprocessor will set the coprocessor value as following format:  public htabledescriptor addcoprocessor(string classname, path jarfilepath,                              int priority, final map<string, string> kvs)   throws ioexception {   ...   string value = ((jarfilepath == null)? \"\" : jarfilepath.tostring()) +         \"|\" + classname + \"|\" + integer.tostring(priority) + \"|\" +         kvstring.tostring();   ... } if the 'jarfilepath' is null, the 'value' will always has the format '|classname|priority|' even if 'kvs' is null, which means no extra arguments for the coprocessor. then, in the server side, regioncoprocessorhost#gettablecoprocessorattrsfromschema will load the table coprocessors as:   static list<tablecoprocessorattribute> gettablecoprocessorattrsfromschema(configuration conf,       htabledescriptor htd) {     ...             try {               cfgspec = matcher.group(4); // => cfgspec will be '|' for the format '|classname|priority|'             } catch (indexoutofboundsexception ex) {               // ignore             }             configuration ourconf;             if (cfgspec != null) {  // => cfgspec will be '|' for the format '|classname|priority|'               ourconf = new configuration(false);               hbaseconfiguration.merge(ourconf, conf);             }     ... } the 'cfgspec' will be '|' for the coprocessor formatted as '|classname|priority|', so that always create a new configuration.  in our production, there are a lot of tables having table-level coprocessors, so that the region server will create new configurations for each region of the table, this will consume a certain number of memory when we have many such regions.  to fix the problem, we can make the htabledescriptor not append the '|' if no extra arguments for the coprocessor, or check the 'cfgspec' more strictly in server side which could avoid creating new configurations for existed such regions after the regions reopened. discussions and suggestions are welcomed. ",
        "label": 314
    },
    {
        "text": "testmasterfailover fails occasionally  it seems a bug. the root in rit can't be moved..  in the failover process, it enforces root on-line. but not clean zk node.   test will wait forever.  void processfailover() throws keeperexception, ioexception, interruptedexception {  // we enforce on-line root.  hserverinfo hsi =  this.servermanager.gethserverinfo(this.catalogtracker.getmetalocation());  regiononline(hregioninfo.first_meta_regioninfo, hsi);  hsi = this.servermanager.gethserverinfo(this.catalogtracker.getrootlocation());  regiononline(hregioninfo.root_regioninfo, hsi); it seems that we should wait finished as meta region   int assignrootandmeta()  throws interruptedexception, ioexception, keeperexception {  int assigned = 0;  long timeout = this.conf.getlong(\"hbase.catalog.verification.timeout\", 1000);  // work on root region. is it in zk in transition?  boolean rit = this.assignmentmanager.  processregionintransitionandblockuntilassigned(hregioninfo.root_regioninfo);  if (!catalogtracker.verifyrootregionlocation(timeout)) { this.assignmentmanager.assignroot(); this.catalogtracker.waitforroot(); //we need add this code and guarantee that the transition has completed this.assignmentmanager.waitforassignment(hregioninfo.root_regioninfo); assigned++; } logs:  2011-08-16 07:45:40,715 debug [regionserver:0;c4s2.site,47710,1313495126115-eventthread] zookeeper.zookeeperwatcher(252): regionserver:47710-0x131d2690f780004 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/hbase/unassigned/70236052  2011-08-16 07:45:40,715 debug [rs_open_root-c4s2.site,47710,1313495126115-0] zookeeper.zkassign(712): regionserver:47710-0x131d2690f780004 successfully transitioned node 70236052 from rs_zk_region_opening to rs_zk_region_opening  2011-08-16 07:45:40,715 debug [thread-760-eventthread] zookeeper.zookeeperwatcher(252): master:60701-0x131d2690f780009 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/hbase/unassigned/70236052  2011-08-16 07:45:40,716 info [postopendeploytasks:70236052] catalog.rootlocationeditor(62): setting root region location in zookeeper as c4s2.site:47710  2011-08-16 07:45:40,716 debug [thread-760-eventthread] zookeeper.zkutil(1109): master:60701-0x131d2690f780009 retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=root,,0, server=c4s2.site,47710,1313495126115, state=rs_zk_region_opening  2011-08-16 07:45:40,717 debug [thread-760-eventthread] master.assignmentmanager(477): handling transition=rs_zk_region_opening, server=c4s2.site,47710,1313495126115, region=70236052/root  2011-08-16 07:45:40,725 debug [rs_open_root-c4s2.site,47710,1313495126115-0] zookeeper.zkassign(661): regionserver:47710-0x131d2690f780004 attempting to transition node 70236052/root from rs_zk_region_opening to rs_zk_region_opened  2011-08-16 07:45:40,727 debug [rs_open_root-c4s2.site,47710,1313495126115-0] zookeeper.zkutil(1109): regionserver:47710-0x131d2690f780004 retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052; data=region=root,,0, server=c4s2.site,47710,1313495126115, state=rs_zk_region_opening  2011-08-16 07:45:40,740 debug [regionserver:0;c4s2.site,47710,1313495126115-eventthread] zookeeper.zookeeperwatcher(252): regionserver:47710-0x131d2690f780004 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/hbase/unassigned/70236052  2011-08-16 07:45:40,740 debug [thread-760-eventthread] zookeeper.zookeeperwatcher(252): master:60701-0x131d2690f780009 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/hbase/unassigned/70236052  2011-08-16 07:45:40,740 debug [rs_open_root-c4s2.site,47710,1313495126115-0] zookeeper.zkassign(712): regionserver:47710-0x131d2690f780004 successfully transitioned node 70236052 from rs_zk_region_opening to rs_zk_region_opened  2011-08-16 07:45:40,741 debug [rs_open_root-c4s2.site,47710,1313495126115-0] handler.openregionhandler(121): opened root,,0.70236052  2011-08-16 07:45:40,741 debug [thread-760-eventthread] zookeeper.zkutil(1109): master:60701-0x131d2690f780009 retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=root,,0, server=c4s2.site,47710,1313495126115, state=rs_zk_region_opened  2011-08-16 07:45:40,741 debug [thread-760-eventthread] master.assignmentmanager(477): handling transition=rs_zk_region_opened, server=c4s2.site,47710,1313495126115, region=70236052/root //.............................................it said that zk node can't be cleaned because of we have enforced on-line the root.......................................  // the test will wait forever. 2011-08-16 07:45:40,741 warn [thread-760-eventthread] master.assignmentmanager(540): received opened for region 70236052/root from server c4s2.site,47710,1313495126115 but region was in the state null and not in expected pending_open or opening states 2011-08-16 07:45:41,018 debug [master:0;c4s2.site:60701] zookeeper.zkutil(1109): master:60701-0x131d2690f780009 retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=root,,0, server=c4s2.site,47710,1313495126115, state=rs_zk_region_opened  2011-08-16 07:45:41,233 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:41,337 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:41,439 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:41,543 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:41,645 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:41,748 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:41,900 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:42,002 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:42,105 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:42,206 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:42,308 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052  2011-08-16 07:45:42,410 debug [thread-760] zookeeper.zkassign(807): zk rit -> 70236052 ",
        "label": 529
    },
    {
        "text": "hbase backup should set the details to mr job name  for incremental and full backups job names should be set correctly. it would make the debug and monitoring better. current :   incremental -> walplayer_1506332227619  exportsnapshot-snapshot_1506323760362_default_t1 proposed :  incremental -> backup_incremental_<backupid>_<table_name>  full -> backups_full_<backupid>_<table_name> ",
        "label": 475
    },
    {
        "text": "region close needs to be fast  e g  if compacting  abandon it  over last week or so i've seen slow closes cause regions be off line for a good amount of time. just now, i saw a big compaction go into effect because \"too many store files\". this compaction took nearly two minutes on loaded server. but during this time flushing was held up. when the order to close came in (overloaded), we started the close \u2013 so incoming writes were rejected \u2013 but then we had to wait on the compaction to finish before the close went ahead... though incoming clients by now are being turned away. eventually the compaction completed and then the held-up flush was allowed run..... 91m in about 5 seconds. only now was the close allowed complete and the region deployed elsewhere. another time i saw the flush take a good long time because hdfs was running slow. probably not much we can do about this one but we should at least look into the above. interrupt an ongoing compaction and abandon it... or else keep region open while the compaction is going on and only when compete, then start up the close (would require new state of closing keeping up a progressable with the master). ",
        "label": 341
    },
    {
        "text": "convert list  extends cell  anti pattern to list cell  pattern   as described in hbase-9142, using list<? extends cell> is an anti pattern that adds unnecessary typing and casting clutter to the code base. it would be best to remove this before we release 0.95.2 or 0.96. ",
        "label": 248
    },
    {
        "text": "simple flush snapshot  this snapshot style just issues a region flush and then \"snapshots\" the region. this is a simple implementation that gives the equivalent of copytable consistency. while by most definitions of consistency if a client writes a and then write b to different region servers, only neither, only a, or both a+b writes should be present, this one allows the only b case. ",
        "label": 248
    },
    {
        "text": "remove iterablesutil class  remove mostly unused and obsolete class iterablesutil ",
        "label": 130
    },
    {
        "text": "hbase rpc should allow protocol extension with common interfaces   hbase rpc fails if myprotocol extends an interface, which is not a versionedprotocol even if myprotocol also directly extends versionedprotocol. the reason is that rpc invocation uses method.getdeclaringclass(), which returns the interface class rather than the class of myprotocol. ",
        "label": 280
    },
    {
        "text": "fix coverage org apache hadoop hbase metrics histogram  ",
        "label": 40
    },
    {
        "text": "decommission hbase wiki  we have an awesome community resource in our online book. it's maintained and looked after with diligence. we also have an hbase section on the hadoop wiki that hasn't been updated since 2012. let's sift through the pages of the wiki, bring over any content that's still relevant and not already present in the book, and kill the wiki. ",
        "label": 330
    },
    {
        "text": "make the dfs replication factor configurable per table  today, it's an application level configuration. so all the hfiles are replicated 3 times per default. there are some reasons to make it per table: some tables are critical while some others are not. for example, meta would benefit from an higher level of replication, to ensure we continue working even when we lose 20% of the cluster. some tables are backuped somewhere else, used by non essential process, so the user may accept a lower level of replication for these ones. it should be a dynamic parameter. for example, during a bulk load we set a replication of 1 or 2, then we increase it. it's in the same space as disabling the wal for some writes. the case that seems important to me is meta. we can also handle this one by a specific parameter in the usual hbase-site.xml if we don't want a generic solution. ",
        "label": 139
    },
    {
        "text": "rat finds about files missing licenses  giri is trying to hook up the patch build. we need to pass the rat tool. i ran it, http://incubator.apache.org/rat/index.html, and found about 40 files missing licenses (and about 5 purely empty files). ",
        "label": 314
    },
    {
        "text": " doc  add a section about rs failover  the following questions were asked on irc and i couldn't find a good source of info in the online book (so i had to answer them, sad i know): when a region server goes down, 1) do any queries get affected which are using that region server, if they do what happens to the transactions 2) how long does it take to switch over to another region server? brief answers: to q1: the queries get retried (i'm not sure if we have a chapter that describes how that works, might be worth adding however bad it currently is). to q2: zk session timeout + split time + assignment/replay time. ",
        "label": 330
    },
    {
        "text": "allow job names to be overridden  as a general feature, we should allow mr job names to be overridden by the user. see also hbase-8077. ",
        "label": 53
    },
    {
        "text": "the wait on compaction because  too many store files  holds up all flushing  the method memstoreflusher#checkstorefilecount is called from flushregion. flushregion is called by memstoreflusher#run thread. if the checkstorefilecount finds too many store files, it'll stick around waiting on a compaction to happen. while its hanging, the memstoreflusher#run is held up. no other region can flush. meantime wals will be rolling and memory will be accumulating writes. ",
        "label": 229
    },
    {
        "text": "show all the hbase configuration in the web ui  the motivation is to show all the hbase configuration, which takes effect in the run time, in a global place.  so we can easily know which configuration takes effect and what the value is. the configuration shows all the hbase and dfs configuration entry in the configuration file and also includes all the hbase default setting in the code, which is not the config file. ",
        "label": 294
    },
    {
        "text": "add conf which disables metamigrationconvertingtopb check  for experts only   up in parent issue, virag wants to avoid expensive scan on startup. ",
        "label": 314
    },
    {
        "text": "rs requestspersecond counter seems to be off  in testing trunk, i had ycsb reporting some 40,000 requests/second, but the summary info on the master webpage was consistently indicating somewhere around 3x that. i'm guessing that we may have a bug where we forgot to divide by time. ",
        "label": 550
    },
    {
        "text": "memstorelab chunkcreator may memory leak  we use hbase 2.1.2 with memstorelab enable  regionserver crashed case of oom i dump the heap ,found the chunkcreator may be memory leak the heap is 32gb,   hbase.regionserver.global.memstore.size=0.4,  hbase.hregion.memstore.mslab.enabled=true  hbase.hregion.memstore.chunkpool.initialsize=0.5,  hbase.hregion.memstore.chunkpool.maxsize=1.0  bucketcache with offheap ",
        "label": 544
    },
    {
        "text": " replication  create an interface for replication queues  ",
        "label": 103
    },
    {
        "text": "memstore should retain multiple kvs with the same timestamp when memstorets differs  there appears to be a bug in hbase-2248 as committed to trunk. see following failing test:  http://hudson.zones.apache.org/hudson/job/hbase-trunk/1296/testreport/junit/org.apache.hadoop.hbase/testacidguarantees/testatomicity/  think this is the same bug we saw early on in 2248 in the 0.20 branch, looks like the fix didn't make it over. ",
        "label": 453
    },
    {
        "text": "typo in proceduremanagerhost master proceudre conf key  the constant should read proce du re. ",
        "label": 280
    },
    {
        "text": "defaultbalancer selects plans to move regions onto draining nodes  we have quite a large cluster > 100k regions, and we needed to isolate a region was very hot until we could push a patch. we put this region on its own regionserver and set it in the draining state. the default balancer was selecting regions to move to this cluster for its region plans. it just so happened for other tables, the default load balancer was creating plans for the draining servers, even though they were not available to move regions to. thus we were closing regions, then attempting to move them to the draining server then finding out its draining. we had to disable the balancer to resolve this issue. there are some approaches we can take here. 1. exclude draining servers altogether, don't even pass those into the load balancer from hmaster. 2. we could exclude draining servers from ceiling and floor calculations where we could potentially skip load balancing because those draining servers wont be represented when deciding whether to balance. 3. along with #2 when assigning regions, we would skip plans to assign regions to those draining servers. i am in favor of #1 which is simply removes servers as candidates for balancing if they are in the draining state. but i would love to hear what everyone else thinks. ",
        "label": 522
    },
    {
        "text": "testwalrollonlowreplication has some risk to assert failed after hbase  after hbase-14600, we catch runtime exception if dn recover slowly, but it has some risk to assert failed. for example, https://builds.apache.org/job/hbase-trunk/6907/testreport/ the reason is we catch the exception, but in walprocedurestore, it will still stop the procedure. so when we assert stop.isrunning, it will failed. ",
        "label": 198
    },
    {
        "text": "test changing hbase hregion memcache block multiplier to  currently its set to 1. under load, seeing fill-cache/pause-while-cache-is-flushed cycle. changing multiplier to 2 could make hbase take on load faster (was set to 1 because compactions used to overwhelm the system but now hbase-745 added a not-so-dumb compaction algorithim, we might be able to run w/ a value of 2). ",
        "label": 314
    },
    {
        "text": "if gettasklist  returns null  splitlogworker would go down and it won't serve any requests  during the hlog split operation if all the zookeepers are down ,then the paths will be returned as null and the splitworker thread wil be exited  now this regionserver wil not be able to acquire any other tasks since the splitworker thread is exited  please find the attached code for more details private list<string> gettasklist() {     for (int i = 0; i < zkretries; i++) {       try {         return (zkutil.listchildrenandwatchfornewchildren(this.watcher,             this.watcher.splitlogznode));       } catch (keeperexception e) {         log.warn(\"could not get children of znode \" +             this.watcher.splitlogznode, e);         try {           thread.sleep(1000);         } catch (interruptedexception e1) {           log.warn(\"interrupted while trying to get task list ...\", e1);           thread.currentthread().interrupt();           return null;         }       }     } in the org.apache.hadoop.hbase.regionserver.splitlogworker ",
        "label": 100
    },
    {
        "text": "testsnapshotscannerhdfsaclcontroller is broken on branch  testsnapshotscannerhdfsaclcontroller.testcleanarchivetabledir always fails on branch-2. java.lang.assertionerror at org.apache.hadoop.hbase.security.access.testsnapshotscannerhdfsaclcontroller.testcleanarchivetabledir(testsnapshotscannerhdfsaclcontroller.java:745)   test run: https://builds.apache.org/job/hbase-flaky-tests/job/branch-2/4148/testreport/junit/org.apache.hadoop.hbase.security.access/testsnapshotscannerhdfsaclcontroller/testcleanarchivetabledir/ ",
        "label": 500
    },
    {
        "text": "bin rolling restart sh restarts all active rs's with each iteration instead of one at a time  i'm exercising the patch over on hbase-8803 and i've noticed something in the logs: it looks like rolling-restart.sh is restarting all the region servers multiple times instead of just the current entry in the loop iteration. the logic looks like this: for each rs in active region server list:   unload $rs // move all regions to other rs's   restart all region servers // !?! bug?   reload $rs // pile 'em back on shouldn't that step 2 be only restart $rs? this is what i see in the logs. my cluster has 9 active regionservers. notice the bit in the middle where all 9 are stopped and started again after unloading the target rs. $ time /usr/lib/hbase/bin/rolling-restart.sh --rs-only --graceful --maxthreads 30                                                                                                        gracefully restarting: hor18n39.gq1.ygridcore.net disabling balancer! ... unloading hor18n39.gq1.ygridcore.net region(s) ... valid region move targets:  hor18n37.gq1.ygridcore.net,60020,1374094975268 hor17n37.gq1.ygridcore.net,60020,1374094975264 hor18n35.gq1.ygridcore.net,60020,1374094975327 hor17n39.gq1.ygridcore.net,60020,1374094975281 hor18n36.gq1.ygridcore.net,60020,1374094975254 hor17n36.gq1.ygridcore.net,60020,1374094975277 hor17n34.gq1.ygridcore.net,60020,1374094975291 hor18n38.gq1.ygridcore.net,60020,1374094975259 13/07/17 21:44:38 info region_mover: moving 330 region(s) from hor18n39.gq1.ygridcore.net,60020,1374094975326 during this cycle 13/07/17 21:44:38 info region_mover: moving region b59050cf97aabcef838e3c50e93e6d13 (1 of 330) to server=hor18n37.gq1.ygridcore.net,60020,1374094975268 ... 13/07/17 21:54:20 info region_mover: moving region d00026d7cc396bb3e6ea91106cc6ab55 (329 of 330) to server=hor18n37.gq1.ygridcore.net,60020,1374094975268 13/07/17 21:54:20 info region_mover: moving region a722179b33e6ece8c9cee3fba3056acd (330 of 330) to server=hor17n37.gq1.ygridcore.net,60020,1374094975264 13/07/17 21:54:21 info region_mover: wrote list of moved regions to /tmp/hor18n39.gq1.ygridcore.net unloaded hor18n39.gq1.ygridcore.net region(s) hor18n35.gq1.ygridcore.net: stopping regionserver. hor17n39.gq1.ygridcore.net: stopping regionserver. hor18n36.gq1.ygridcore.net: stopping regionserver. hor17n37.gq1.ygridcore.net: stopping regionserver. hor17n34.gq1.ygridcore.net: stopping regionserver. hor18n38.gq1.ygridcore.net: stopping regionserver. hor18n37.gq1.ygridcore.net: stopping regionserver. hor17n36.gq1.ygridcore.net: stopping regionserver. hor18n39.gq1.ygridcore.net: stopping regionserver. hor18n36.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n36.gq1.ygridcore.net.out hor17n36.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n36.gq1.ygridcore.net.out hor17n37.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n37.gq1.ygridcore.net.out hor18n37.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n37.gq1.ygridcore.net.out hor18n38.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n38.gq1.ygridcore.net.out hor17n34.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n34.gq1.ygridcore.net.out hor18n35.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n35.gq1.ygridcore.net.out hor18n39.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n39.gq1.ygridcore.net.out hor17n39.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n39.gq1.ygridcore.net.out reloading hor18n39.gq1.ygridcore.net region(s) ... 13/07/17 21:54:27 info region_mover: moving 330 regions to hor18n39.gq1.ygridcore.net,60020,1374098064602 13/07/17 21:56:47 info region_mover: moving region 7d0a02f452c334a12026b45346a87d36 (1 of 330) to server=hor18n39.gq1.ygridcore.net,60020,1374098064602 in thread 0 13/07/17 21:56:54 info region_mover: moving region af5448c90e78a8f0d935efb0b380502e (2 of 330) to server=hor18n39.gq1.ygridcore.net,60020,1374098064602 in thread 1 ... ",
        "label": 339
    },
    {
        "text": "rewrite replicationpeer with the new replication storage interface  ",
        "label": 187
    },
    {
        "text": "client metacache is cleared if a throttlingexception is thrown  during performance test with the request throttling, i saw that hbase:meta table had been read a lot. currently the metacache of the client is cleared, if a throttlingexception is thrown. it seems to be not needed. ",
        "label": 165
    },
    {
        "text": "remove asyncadmin's methods which were already deprecated in admin interface  since we are not release hbase 2.0 now, i thought it is ok to remove the methods which were already de deprecated in admin interface. the methods which were marked as deprecated in hbase-18241.  htabledescriptor[] deletetables(pattern)  htabledescriptor[] enabletables(pattern)  htabledescriptor[] disabletables(pattern)  getalterstatus()  closeregion() ",
        "label": 187
    },
    {
        "text": "support deleted rows using import export  parent allows keeping deleted rows around. would be nice if those could be exported and imported as well.  all the building blocks are there. ",
        "label": 286
    },
    {
        "text": "where new text string  might be used in client side method calls  add an overload that takes strings  there are a lot of places in our client side api where people probably don't care that text is used in the underlying wire protocol. for instance, new htable(hbaseconfig, text) adds a little bit of extra hassle and code. where it makes sense, we should make an overload for constructors and client methods that can take a string and just convert it into a text behind the scenes. this would beautify some of our code. ",
        "label": 314
    },
    {
        "text": "for hfileoutputformat  on timeout failure kill clean up half written hfile  below is from mailing list. read from bottom to top:  i was going to write that perhaps you needed to turn mapred.reduce.tasks.speculative.execution off, but if enabling it and things work, that would seem to indicate that a our reducer first takes longer than the task timeout maximum and secondly, on failure, we should clean up the hfile. on the first issue, you are using keyvaluesortreducer?  are your values large?  we set reducer status every 100 values.  maybe this is not enough?  we should set status more frequently?  if you call context setstatus more frequently, do things work w/o speculative execution?   on the second, hfileoutputformat close will set the metadata on the hfile and then close it.  on kill, this code is not being called.   let me see if can do something about that (e.g. register a shutdown hook to clean away incomplete files -- ). thanks, st.ack on sun, dec 20, 2009 at 11:26 pm, chingshen <chingshenchen@gmail.com> wrote: i think i found a way. i set the \"mapred.reduce.tasks.speculative.execution\" to true and output hfiles again, then successfully load hfiles into hbase. is it best solution? or hfileoutputformat bug? shen on mon, dec 21, 2009 at 8:25 am, chingshen <chingshenchen@gmail.com> wrote: > thanks, stack. > > i checked this file that isn't empty. but i found that as long as the > \"killed task attempts\" > 0 in reduce phase, and run the loadtable.rb script > to load hfiles then failed. > how to avoid this problem? > > thanks. > > shen > > > on sat, dec 19, 2009 at 3:49 am, stack <stack@duboce.net> wrote: > >> check the >> file >> hdfs://domu-12-31-39-09-c5-54.compute-1.internal/osm2_hfile/level4/197894389945760574. >>  is it empty?  was there an error during running of your mr job?  perhaps >> a >> task failed? >> >> st.ack >> >> >> >> on thu, dec 17, 2009 at 9:46 pm, chingshen <chingshenchen@gmail.com> >> wrote: >> >> > hi, >> >  i use the script loadtable.rb to load my hfiles into hbase, but i got >> an >> > exception as below. >> >  does anyone have any suggestions? >> > >> > 09/12/17 23:59:33 info loadtable: 18 read firstkey of -3.9290_52.5534 >> from >> > >> > >> hdfs://domu-12-31-39-09-c5-54.compute-1.internal/osm2_hfile/level4/1978943899457605747 >> > org/apache/hadoop/hbase/io/hfile/hfile.java:1335:in `deserialize': >> > java.io.ioexception: trailer 'header' is wrong; does the trailer size >> match >> > content? (nativeexception) >> >    from org/apache/hadoop/hbase/io/hfile/hfile.java:813:in `readtrailer' >> >    from org/apache/hadoop/hbase/io/hfile/hfile.java:758:in >> `loadfileinfo' >> >    from sun.reflect.generatedmethodaccessor7:-1:in `invoke' >> >    from sun/reflect/delegatingmethodaccessorimpl.java:25:in `invoke' >> >    from java/lang/reflect/method.java:597:in `invoke' >> >    from org/jruby/javasupport/javamethod.java:298:in >> > `invokewithexceptionhandling' >> >    from org/jruby/javasupport/javamethod.java:259:in `invoke' >> >    from org/jruby/java/invokers/instancemethodinvoker.java:36:in `call' >> >     ... 18 levels... >> >    from org/jruby/main.java:94:in `main' >> >    from loadtable.rb:83:in `each' >> >    from loadtable.rb:83 >> > complete java stacktrace >> > java.io.ioexception: trailer 'header' is wrong; does the trailer size >> match >> > content? >> >    at >> > >> > >> org.apache.hadoop.hbase.io.hfile.hfile$fixedfiletrailer.deserialize(hfile.java:1335) >> >    at >> > >> org.apache.hadoop.hbase.io.hfile.hfile$reader.readtrailer(hfile.java:813) >> >    at >> > >> org.apache.hadoop.hbase.io.hfile.hfile$reader.loadfileinfo(hfile.java:758) >> >    at sun.reflect.generatedmethodaccessor7.invoke(unknown source) >> >    at >> > >> > >> sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) >> >    at java.lang.reflect.method.invoke(method.java:597) >> >    at >> > >> > >> org.jruby.javasupport.javamethod.invokewithexceptionhandling(javamethod.java:298) >> >    at org.jruby.javasupport.javamethod.invoke(javamethod.java:259) >> >    at >> > >> > >> org.jruby.java.invokers.instancemethodinvoker.call(instancemethodinvoker.java:36) >> >    at >> > org.jruby.runtime.callsite.cachingcallsite.call(cachingcallsite.java:70) >> >    at loadtable.ensure_1$ruby$__ensure___2(loadtable.rb:86) >> >    at loadtable.block_0$ruby$__for__(loadtable.rb:85) >> >    at loadtableblockcallback$block_0$ruby$__for__xx1.call(unknown >> source) >> >    at org.jruby.runtime.compiledblock.yield(compiledblock.java:102) >> >    at org.jruby.runtime.block.yield(block.java:100) >> >    at >> org.jruby.java.proxies.arrayjavaproxy.each(arrayjavaproxy.java:112) >> >    at >> > >> > >> org.jruby.java.proxies.arrayjavaproxy$i_method_0_0$rubyinvoker$each.call(org/jruby/java/proxies/arrayjavaproxy$i_method_0_0$rubyinvoker$each.gen) >> >    at >> > >> > >> org.jruby.runtime.callsite.cachingcallsite.cacheandcall(cachingcallsite.java:263) >> >    at >> > >> > >> org.jruby.runtime.callsite.cachingcallsite.callblock(cachingcallsite.java:81) >> >    at >> > >> > >> org.jruby.runtime.callsite.cachingcallsite.calliter(cachingcallsite.java:96) >> >    at loadtable.__file__(loadtable.rb:83) >> >    at loadtable.load(loadtable.rb) >> >    at org.jruby.ruby.runscript(ruby.java:577) >> >    at org.jruby.ruby.runnormally(ruby.java:480) >> >    at org.jruby.ruby.runfrommain(ruby.java:354) >> >    at org.jruby.main.run(main.java:229) >> >    at org.jruby.main.run(main.java:110) >> >    at org.jruby.main.main(main.java:94) >> > >> > > > > -- ***************************************************** ching-shen chen advanced technology center, information & communications research lab. e-mail: chenchingshen@itri.org.tw tel:+886-3-5915542 ***************************************************** ",
        "label": 314
    },
    {
        "text": "catalog janitor logic bug causes region leackage  when region split takes a significant amount of time, catalogjanitor can cleanup one of split records, but left another in meta. when another split finish, janitor cleans left split record, but parent regions haven't removed from fs and meta not cleared. the race condition is follows:  1. region split started  2. one of regions splitted, i.e. a (have no reference storefiles) but other (b) doesn't  3. janitor started and in routine checkdaughter removes splita from meta, but see that splitb has references and does nothing.  4. region b completes split  5. janitor wakes up, removes splitb, but see that there is no records for a and does nothing again. result - parent region hangs forever. ",
        "label": 310
    },
    {
        "text": "investigate outliers in fb  umbrella task to investigate outliers in the get and put path. ",
        "label": 154
    },
    {
        "text": "hfile v2 does not honor setcacheblocks when scanning   while testing the lru cache during the scanning i noticed quite some churn in the cache even when scan.cacheblocks is set to false. after debugging this, i found that hfile v2 always caches blocks in the lru cache regardless of the cacheblocks setting. here's a trace (from eclipse) showing the problem: hfilereaderv2.readblock(long, int, boolean, boolean, boolean) line: 279   hfilereaderv2.readblockdata(long, long, int, boolean) line: 219   hfileblockindex$blockindexreader.seektodatablock(byte[], int, int, hfileblock) line: 191   hfilereaderv2$scannerv2.seekto(byte[], int, int, boolean) line: 502   hfilereaderv2$scannerv2.reseekto(byte[], int, int) line: 539   storefilescanner.reseekatorafter(hfilescanner, keyvalue) line: 151   storefilescanner.reseek(keyvalue) line: 110   keyvalueheap.reseek(keyvalue) line: 255   storescanner.reseek(keyvalue) line: 409   storescanner.next(list<keyvalue>, int) line: 304   keyvalueheap.next(list<keyvalue>, int) line: 114   keyvalueheap.next(list<keyvalue>) line: 143   hregion$regionscannerimpl.nextrow(byte[]) line: 2774   hregion$regionscannerimpl.nextinternal(int) line: 2722   hregion$regionscannerimpl.next(list<keyvalue>, int) line: 2682   hregion$regionscannerimpl.next(list<keyvalue>) line: 2699   hregionserver.next(long, int) line: 2092 every scanner.next causes a reseek, which eventually causes a call to hfileblockindex$blockindexreader.seektodatablock(...) at which point the cacheblocks information is lost. hfilereaderv2.readblockdata calls hfilereaderv2.readblock with cacheblocks set unconditionally to true. the fix is not immediately clear, unless we want to pass cacheblocks to hfileblockindex$blockindexreader.seektodatablock and then on to hfileblock.basicreader.readblockdata and all its implementers, which is ugly as readblockdata should not care about caching. avoiding caching during scans is somewhat important for us. ",
        "label": 324
    },
    {
        "text": "incorrect or confusing test value is used in block caches  default_blocksize_small is described as:   // make default block size for storefiles 8k while testing.  todo: fix!   // need to make it 8k for testing.   public static final int default_blocksize_small = 8 * 1024; this value is used on production path in cacheconfig thru hstore/hregion, and passed to various cache object. we should change it to actual block size, or if it is somehow by design at least we should clarify it and remove the comment. ",
        "label": 406
    },
    {
        "text": "testreplicationsmalltests fails after hbase  testemptywalrecovery and testverifyrepjob ",
        "label": 149
    },
    {
        "text": "block count is when bucketcache using persistent ioengine and retrieve from file  we use fileioengine to store and read block data in bucketcache. before stop rs, the bucketcache metrics in webui is like before_rs_restart.png, the block count is 3. when stop rs, the backing map is persistent to file. and then start rs, the metrics is like after_rs_restart.png, the block count is 0, but the size of blocks is correct.  ",
        "label": 59
    },
    {
        "text": "log recovery  deleted items may be resurrected  while working on hbase-2283, noticed that if you do a put followed by a delete, and then crash the rs, and trigger log recovery to happen, then deleted entries may be resurrected. suprisingly, the issue only affected delete of a specific column. full row delete didn't run into this issue. \u2014 code inspection revealed that we might have an issue with timestamps & wal stuff for delete that come in with \"latest\" timestamp. [note: the \"latest\" timestamp is syntax sugar/hint to the rs to convert it to \"now\". ] basically, in: delete(byte [] family, list<keyvalue> kvs, boolean writetowal) the \"kv.updatelateststamp(bytenow);\" time stamp massaging (from latest to now) happens after the wal log.append() call. so the keyvalue entries written to the hlog do not have the massaged timestamp. on recovery, when these entries are replayed, we add them back to reconstructioncache but don't do anything with timestamps. the above could be the potential source of the problem. but there could be more to the problem than my simple analysis. for instance, we still don't know why full row delete worked fine, but delete of a specific column didn't work ok. forking this off as a separate issue from hbase-2283. [note: aravind is starting to take a look at this issue.] ",
        "label": 49
    },
    {
        "text": "build fails on windows if user is logged into a domain  build fails due to invalid output by saveversion.sh: user=`whoami` results in the string \"domain\\user\" which is an invalid escape sequence down the line (or something else bad). i tried to figure out how to use sed to escape the backslash, but failed miserably and just removed the whoami altogether to workaround. ",
        "label": 241
    },
    {
        "text": "expose a command to manually trigger an hlog roll  hbase-4222 added a version of hlog.rollwriter() that allows \"forcing\" a log roll when requested. it would be useful to expose this as an hregioninterface rpc method and provide a corresponding shell command to allow explicit log rolling when desired. ",
        "label": 544
    },
    {
        "text": "line length check in test patch sh is broken  here is related code in the script:   max_line_length_patch=`expr $max_line_length + 1` ...   ll=`echo \"$lines\" | wc -l`   if [[ \"$ll\" -gt \"$max_line_length_patch\" ]]; then here was the result from dry run: tyus-macbook-pro:m7 tyu$ lines=`cat ~/trunk/7226-trunk.patch | grep \"^+\" | grep -v \"^@@\" | grep -v \"^+++\" | grep -v \"import\" | grep -v \"hbase.protobuf.generated\" | awk -v len=\"101\" 'length ($0) > len' | head -n 10` tyus-macbook-pro:m7 tyu$ ll=`echo \"$lines\" | wc -l` tyus-macbook-pro:m7 tyu$ echo $ll 3 tyus-macbook-pro:m7 tyu$ echo $lines + // test compareop.less_or_equal: original = val2, compare with val2, succeed (value still = val2) + // test compareop.less_or_equal: original = val2, compare with val1, succeed (now value = val3) + // test compareop.greater_or_equal: original = val2, compare with val2, succeed (value still = val2) the value of $ll should be compared with 0, not the value of $max_line_length_patch ",
        "label": 441
    },
    {
        "text": "deprecate htablepool in favor of hconnection gettable   update:  i now propose deprecating htablepool and instead introduce a gettable method on hconnection and allow hconnection to manage the threadpool. initial proposal:  here i propose a very simple tablepool.  it could be called lighthtablepool (or something - if you have a better name).  internally it would maintain an hconnection and an executor service and each invocation of gettable(...) would create a new htable and close() would just close it.  in testing i find this more light weight than htablepool and easier to monitor in terms of resources used. it would hardly be more than a few dozen lines of code. ",
        "label": 286
    },
    {
        "text": "split report before we finish parent region open  workaround till  race between split and opened processing  this issue is about adding a workaround to 0.90 until we get proper fix in 0.92 (hbase-3559). here is the sequence of events: 1. we start to process opened region event.  2. we receive a split of this region report.  3. split processing offline the region and onlines daughters.  4. metascanner runs and clears out the region from .meta. deleting it  5. the opened handler runs. marks the region online in master memory.  6. balancer runs. trys to balance a region that has been deleted. loops for ever. here is excerpt from logs. it happened during startup, lots going on. could happen on regionserver crash i suppose, maybe, but we're susceptible during cluster start: # we assign the region 2011-03-16 15:18:29,053 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x22e286f0b9c98f1 async create of unassigned node for 3516b74d0c9d4458c2f2f715249e3f78 with offline state ... 2011-03-16 15:18:32,298 debug org.apache.hadoop.hbase.master.assignmentmanager$createunassignedasynccallback: rs=tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. state=offline, ts=1300313909053, server=sv4borg39,60020,1300313564807 ... 2011-03-16 15:18:32,732 debug org.apache.hadoop.hbase.master.assignmentmanager$existsunassignedasynccallback: rs=tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. state=offline, ts=1300313909053 ... 2011-03-16 15:23:02,114 debug org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: master:60000-0x22e286f0b9c98f1 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78 ... 2011-03-16 15:23:02,183 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:60000-0x22e286f0b9c98f1 retrieved 127 byte(s) of data from znode /prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78 and set watcher; region=tsdb,^@^d2mcz@^@^@^a^@^@g^@^@^l^@^@f^@^@^u^@^@\ufffd^@^@(^@^c^g,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., server=sv4borg39,60020,1300313564807, state=rs_zk_region_opened 2011-03-16 15:23:02,183 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_opened, server=sv4borg39,60020,1300313564807, region=3516b74d0c9d4458c2f2f715249e3f78 # at this point we've queued an excecutor to run to process the opened event.  now in comes the split. 2011-03-16 15:23:18,199 info org.apache.hadoop.hbase.master.servermanager: received region_split: tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.: daughters; tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1300314189812.74c51400bb8dfa127fadfd11a04d72f2., tsdb,\\x00\\x042mmd\\x88\\x00\\x00\\x01\\x00\\x00s\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x029\\x00\\x00(\\x00\\x03\\x03,1300314189812.87b061739a11d0f9d02acfb92ef961a2. from sv4borg39,60020,1300313564807 2011-03-16 15:23:18,870 warn org.apache.hadoop.hbase.master.assignmentmanager: split report has rit node (shouldnt have one): region => {name => 'tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.', startkey => '\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07', endkey => '\\x00\\x043l\\xe7\\xf50\\x00\\x00\\x01\\x00\\x00i\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x0e\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x02u', encoded => 3516b74d0c9d4458c2f2f715249e3f78, table => {{name => 'tsdb', families => [{name => 't', bloomfilter => 'none', replication_scope => '0', versions => '3', compression => 'lzo', ttl => '2147483647', blocksize => '65536', in_memory => 'false', blockcache => 'true'}]}} node: region=tsdb,^@^d2mcz@^@^@^a^@^@g^@^@^l^@^@f^@^@^u^@^@\ufffd^@^@(^@^c^g,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., server=sv4borg39,60020,1300313564807, state=rs_zk_region_opened # now metascanner runs and actually removes the parent region, deleting it all 2011-03-16 15:28:34,352 info org.apache.hadoop.hbase.catalog.metaeditor: deleted daughter reference tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1300314189812.74c51400bb8dfa127fadfd11a04d72f2., qualifier=splita, from parent tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. 2011-03-16 15:28:34,356 info org.apache.hadoop.hbase.catalog.metaeditor: deleted daughter reference tsdb,\\x00\\x042mmd\\x88\\x00\\x00\\x01\\x00\\x00s\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x029\\x00\\x00(\\x00\\x03\\x03,1300314189812.87b061739a11d0f9d02acfb92ef961a2., qualifier=splitb, from parent tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. 2011-03-16 15:28:34,356 debug org.apache.hadoop.hbase.master.catalogjanitor: deleting region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. because daughter splits no longer hold references 2011-03-16 15:28:34,356 debug org.apache.hadoop.hbase.master.catalogjanitor: deleting region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. because daughter splits no longer hold references 2011-03-16 15:28:34,444 debug org.apache.hadoop.hbase.regionserver.hregion: deleting region hdfs://sv4borg29:9000/hbase/tsdb/3516b74d0c9d4458c2f2f715249e3f78 2011-03-16 15:28:34,542 info org.apache.hadoop.hbase.catalog.metaeditor: deleted region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. from meta # now the opened executor runs, a good while after the above # 2011-03-16 15:30:26,679 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: handling opened event for 3516b74d0c9d4458c2f2f715249e3f78; deleting unassigned node 2011-03-16 15:30:26,679 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x22e286f0b9c98f1 deleting existing unassigned node for 3516b74d0c9d4458c2f2f715249e3f78 that is in expected state rs_zk_region_opened 2011-03-16 15:30:26,725 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:60000-0x22e286f0b9c98f1 retrieved 127 byte(s) of data from znode /prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78; data=region=tsdb,^@^d2mcz@^@^@^a^@^@g^@^@^l^@^@f^@^@^u^@^@\ufffd^@^@(^@^c^g,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., server=sv4borg39,60020,1300313564807, state=rs_zk_region_opened 2011-03-16 15:30:26,875 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x22e286f0b9c98f1 successfully deleted unassigned node for region 3516b74d0c9d4458c2f2f715249e3f78 in expected state rs_zk_region_opened 2011-03-16 15:30:27,051 debug org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: master:60000-0x22e286f0b9c98f1 received zookeeper event, type=nodedeleted, state=syncconnected, path=/prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78 2011-03-16 15:30:27,051 debug org.apache.hadoop.hbase.master.handler.openedregionhandler: opened region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. on sv4borg39,60020,1300313564807 # now we have a region online in master's memory but its not out in .meta. nor in the fs. # the balancer runs 2011-03-16 23:18:41,716 info org.apache.hadoop.hbase.master.hmaster: balance hri=tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., src=sv4borg39,60020,1300313564807, dest=sv4borg33,60020,1300342574666 2011-03-16 23:18:41,716 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. (offlining) 2011-03-16 23:18:41,718 debug org.apache.hadoop.hbase.master.assignmentmanager: server servername=sv4borg39,60020,1300313564807, load=(requests=2, regions=504, usedheap=929, maxheap=6973) returned org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: received close for tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. but we are not serving it for 3516b74d0c9d4458c2f2f715249e3f78 2011-03-16 23:20:34,436 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out:  tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. state=pending_close, ts=1300342802734 2011-03-16 23:20:34,437 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_close for too long, running forced unassign again on region=tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. 2011-03-16 23:20:34,437 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:60000-0x22e286f0b9c98f1 set watcher on existing znode /prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78 2011-03-16 23:20:34,437 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. (offlining) 2011-03-16 23:20:34,438 debug org.apache.hadoop.hbase.master.assignmentmanager: attempting to unassign region tsdb,\\x00\\x042mcz@\\x00\\x00\\x01\\x00\\x00g\\x00\\x00\\x0c\\x00\\x00f\\x00\\x00\\x15\\x00\\x00\\xa9\\x00\\x00(\\x00\\x03\\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. which is already pending close but forcing an additional close ad infinitum ",
        "label": 314
    },
    {
        "text": "zk event thread waiting for root region assignment may block server shutdown handler for the region sever the root region was on  a rs died. the servershutdownhandler kicked in and started the logspliting. splilogmanager  installed the tasks asynchronously, then started to wait for them to complete. the task znodes were not created actually. the requests were just queued.  at this time, the zookeeper connection expired. hmaster tried to recover the expired zk session.  during the recovery, a new zookeeper connection was created. however, this master became the  new master again. it tried to assign root and meta. because the dead rs got the old root region, the master needs to wait for the log splitting to complete.  this waiting holds the zookeeper event thread. so the async create split task is never retried since  there is only one event thread, which is waiting for the root region assigned. ",
        "label": 242
    },
    {
        "text": "should not show split parent regions in hbck report ui  now the split parent region is showed in inconsistent regions and region holes. attached a screen for this. ",
        "label": 187
    },
    {
        "text": "enhance remote meta updates  currently, if the meta region is on a regionserver instead of the master, meta update is synchronized on one htable instance. we should be able to do better. ",
        "label": 472
    },
    {
        "text": "hbase shell deleteall should not require column to be defined  it appears that the shell does not allow users to delete a row without specifying a column (deleteall). it looks like the deleteall.rb used to pre-define column as nil, making it optional. i've created a patch and confirmed it to be working in standalone mode, i will upload it shortly. ",
        "label": 375
    },
    {
        "text": "fix typo in javadoc description  for example : 'a http' change into 'an http' 'an unique' change into 'a unique' 'an url' change into 'a url' ",
        "label": 262
    },
    {
        "text": "enabling a non existent table from shell prints no error  hbase(main):001:0> enable 'testtable'  0 row(s) in 0.3120 seconds only thing is that i don't have a table called 'testtable' ",
        "label": 314
    },
    {
        "text": "introduce a zk hosted table wide read write lock so only one table operation at a time  i saw this facility over in the accumulo code base. currently we just try to sort out the mess when splits come in during an online schema edit; somehow we figure we can figure all possible region transition combinations and make the right call. we could try and narrow the number of combinations by taking out a zk table lock when doing table operations. for example, on split or merge, we could take a read-only lock meaning the table can't be disabled while these are running. we could then take a write only lock if we want to ensure the table doesn't change while disabling or enabling process is happening. shouldn't be too hard to add. ",
        "label": 19
    },
    {
        "text": " test  a hbase1 cluster should be able to replicate to a hbase2 cluster  verify  from the mailing list thread \"[discuss] hbase-2.0.0 compatibility expectations\", esteban gutierrez asks: should we add additional details around replication as well? for instance, shall we consider a hbase-1.x cluster as a client for a hbase-2.x cluster? the latter should be a blocker. verify it works. ",
        "label": 163
    },
    {
        "text": "space quota issue  deleting snapshot doesn't update the usage of table  space quota issue: deleting snapshot doesn't update the usage of table steps: 1: set_quota type => space, table => 'bugatti', limit => '7m', policy => no_writes_compactions 2: ./hbase pe --table=\"bugatti\" --nomapred --rows=200 sequentialwrite 10 3: ./hbase pe --table=\"bugatti\" --nomapred --rows=200 sequentialwrite 10 4: snapshot 'bugatti','bugatti_snapshot' 5: ./hbase pe --table=\"bugatti\" --nomapred --rows=200 sequentialwrite 10 6: major_compact 'bugatti' 7: delete_snapshot 'bugatti_snapshot' now check the usage and observe that it is not getting updated. ",
        "label": 391
    },
    {
        "text": "online schema change causes test load and verify to fail   ",
        "label": 242
    },
    {
        "text": "balancer and servershutdownhandler concurrently reassign the same region  the first assign thread exits with success after updating the regionstate to pending_open, while the second assign follows immediately into \"assign\" and fails the regionstate check in setofflineinzookeeper(). this causes the master to abort. in the below case, the two concurrent assigns occurred when am tried to assign a region to a dying/dead rs, and meanwhile the shutdownserverhandler tried to assign this region (from the region plan) spontaneously. 2012-04-17 05:44:57,648 info org.apache.hadoop.hbase.master.hmaster: balance hri=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=hadoop05.sh.intel.com,60020,1334544902186, dest=xmlqa-clv16.sh.intel.com,60020,1334612497253 2012-04-17 05:44:57,648 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. (offlining) 2012-04-17 05:44:57,648 debug org.apache.hadoop.hbase.master.assignmentmanager: sent close to servername=hadoop05.sh.intel.com,60020,1334544902186, load=(requests=0, regions=0, usedheap=0, maxheap=0) for region table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. 2012-04-17 05:44:57,666 debug org.apache.hadoop.hbase.master.assignmentmanager: handling new unassigned node: /hbase/unassigned/fe38fe31caf40b6e607a3e6bbed6404b (region=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., server=hadoop05.sh.intel.com,60020,1334544902186, state=rs_zk_region_closing) 2012-04-17 05:52:58,984 debug org.apache.hadoop.hbase.master.assignmentmanager: forcing offline; was=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. state=closed, ts=1334612697672, server=hadoop05.sh.intel.com,60020,1334544902186 2012-04-17 05:52:58,984 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x236b912e9b3000e creating (or updating) unassigned node for fe38fe31caf40b6e607a3e6bbed6404b with offline state 2012-04-17 05:52:59,096 debug org.apache.hadoop.hbase.master.assignmentmanager: using pre-existing plan for region table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b.; plan=hri=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=hadoop05.sh.intel.com,60020,1334544902186, dest=xmlqa-clv16.sh.intel.com,60020,1334612497253 2012-04-17 05:52:59,096 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. to xmlqa-clv16.sh.intel.com,60020,1334612497253 2012-04-17 05:54:19,159 debug org.apache.hadoop.hbase.master.assignmentmanager: forcing offline; was=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. state=pending_open, ts=1334613179096, server=xmlqa-clv16.sh.intel.com,60020,1334612497253 2012-04-17 05:54:59,033 warn org.apache.hadoop.hbase.master.assignmentmanager: failed assignment of table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. to servername=xmlqa-clv16.sh.intel.com,60020,1334612497253, load=(requests=0, regions=0, usedheap=0, maxheap=0), trying to assign elsewhere instead; retry=0 java.net.sockettimeoutexception: call to /10.239.47.87:60020 failed on socket timeout exception: java.net.sockettimeoutexception: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.socketchannel[connected local=/10.239.47.89:41302 remote=/10.239.47.87:60020]         at org.apache.hadoop.hbase.ipc.hbaseclient.wrapexception(hbaseclient.java:805)         at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:778)         at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:283)         at $proxy7.openregion(unknown source)         at org.apache.hadoop.hbase.master.servermanager.sendregionopen(servermanager.java:573)         at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1127)         at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:912)         at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:892)         at org.apache.hadoop.hbase.master.handler.closedregionhandler.process(closedregionhandler.java:92)         at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:162)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) caused by: java.net.sockettimeoutexception: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.socketchannel[connected local=/10.239.47.89:41302 remote=/10.239.47.87:60020]         at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:164)         at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:155)         at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:128)         at java.io.filterinputstream.read(filterinputstream.java:116)         at org.apache.hadoop.hbase.ipc.hbaseclient$connection$pinginputstream.read(hbaseclient.java:301)         at java.io.bufferedinputstream.fill(bufferedinputstream.java:218)         at java.io.bufferedinputstream.read(bufferedinputstream.java:237)         at java.io.datainputstream.readint(datainputstream.java:370)         at org.apache.hadoop.hbase.ipc.hbaseclient$connection.receiveresponse(hbaseclient.java:541)         at org.apache.hadoop.hbase.ipc.hbaseclient$connection.run(hbaseclient.java:479) 2012-04-17 05:54:59,035 debug org.apache.hadoop.hbase.master.assignmentmanager: no previous transition plan was found (or we are ignoring an existing plan) for table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. so generated a random one; hri=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=, dest=hadoop06.sh.intel.com,60020,1334544901894; 7 (online=7, exclude=servername=xmlqa-clv16.sh.intel.com,60020,1334612497253, load=(requests=0, regions=0, usedheap=0, maxheap=0)) available servers 2012-04-17 05:54:59,035 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x236b912e9b3000e creating (or updating) unassigned node for fe38fe31caf40b6e607a3e6bbed6404b with offline state 2012-04-17 05:54:59,045 debug org.apache.hadoop.hbase.master.assignmentmanager: using pre-existing plan for region table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b.; plan=hri=table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=, dest=hadoop06.sh.intel.com,60020,1334544901894 2012-04-17 05:54:59,045 debug org.apache.hadoop.hbase.master.assignmentmanager: assigning region table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. to hadoop06.sh.intel.com,60020,1334544901894 2012-04-17 05:54:59,046 fatal org.apache.hadoop.hbase.master.hmaster: unexpected state trying to offline; table_order_customer,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. state=pending_open, ts=1334613299045, server=hadoop06.sh.intel.com,60020,1334544901894 java.lang.illegalstateexception         at org.apache.hadoop.hbase.master.assignmentmanager.setofflineinzookeeper(assignmentmanager.java:1167)         at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1107)         at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:912)         at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:892)         at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:259)         at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:162)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) 2012-04-17 05:54:59,047 info org.apache.hadoop.hbase.master.hmaster: aborting ",
        "label": 544
    },
    {
        "text": "buck complains about build target path containing double slash  when i use buck built from latest git repo, i get the following build error: build failed: build target path cannot contain // other than at the start (or after a cell name) (found //connection//...) this error happened while trying to get dependency '//exceptions:exceptions' of target '//connection:connection' it was due to the following line from hbase-native-client/exceptions/buck     visibility=['//core/...', '//connection//...'],) ",
        "label": 441
    },
    {
        "text": "ensure dbe interfaces can work with cell  some changes to the interfaces may be needed for dbes or may be the way it works currently may be need to be modified inorder to make dbes work with cells. suggestions and ideas welcome. ",
        "label": 544
    },
    {
        "text": " site  add book all in one page  makes searching easier if can get a single-page view of hbase 'book'. ",
        "label": 314
    },
    {
        "text": "merge tool expects regions all have different sequence ids  currently merging two regions, the merge tool will compare their sequence ids. if same, it will decrement one. it needs to do this because on region open, files are keyed by their sequenceid; if two the same, one will erase the other. well, with the move to the aggregating hfile format, the sequenceid is written when the file is created and its no longer written into an aside file but as metadata on to the end of the file. changing the sequenceid is no longer an option. this issue is about figuring a solution for the rare case where two store files have same sequence id and we want to merge the two regions. ",
        "label": 230
    },
    {
        "text": "split parents ending up deployed along with daughters  testing rc3 got several regions in this state as reported by hbck:  error: region unknown_region on haus02.sf.cloudera.com:57020, key=9f2822a04028c86813fe71264da5c167, not on hdfs or in meta but deployed on haus02.sf.cloudera.com:57020  (this without any injected failures or anything) ",
        "label": 314
    },
    {
        "text": "hadoop jars used in hbase lib are not compatible with hadoop trunk  the hadoop jars included in the hbase tree under /lib are not compatible with hadoop-core trunk. apparently there have been a couple of revisions to the hadoop rpc protocol so an hbase built with the included jars will not run against a hadoop trunk cluster. ",
        "label": 241
    },
    {
        "text": "change hbase  color  from purple to  international orange  engineering   see http://en.wikipedia.org/wiki/international_orange see the bit about the color of the golden gate bridge. ",
        "label": 314
    },
    {
        "text": "support multiple tables and scanners as input to the mapper in map reduce jobs  it seems that in many cases feeding data from multiple tables or multiple scanners on a single table can save a lot of time when running map/reduce jobs.  i propose a new multitableinputformat class that would allow doing this. ",
        "label": 85
    },
    {
        "text": "  save object creation for scanning with block encodings  i noticed that (at least in 0.98 - master is entirely different) we create bytebuffer just to create a byte[], which is then used to create a keyvalue. we can save the creation of the bytebuffer and hence save allocating an extra object for each kv we find by creating the byte[] directly. in a phoenix count(*) query that saved from 10% of runtime. ",
        "label": 286
    },
    {
        "text": "testsplitlogmanager testmultipleresubmits fails  see https://builds.apache.org/job/hbase-0.95/381/testreport/org.apache.hadoop.hbase.master/testsplitlogmanager/testmultipleresubmits/ ava.lang.assertionerror: expected:<2> but was:<3> at org.junit.assert.fail(assert.java:88) at org.junit.assert.failnotequals(assert.java:743) at org.junit.assert.assertequals(assert.java:118) at org.junit.assert.assertequals(assert.java:555) at org.junit.assert.assertequals(assert.java:542) at org.apache.hadoop.hbase.master.testsplitlogmanager.waitforcounter(testsplitlogmanager.java:167) at org.apache.hadoop.hbase.master.testsplitlogmanager.waitforcounter(testsplitlogmanager.java:153) at org.apache.hadoop.hbase.master.testsplitlogmanager.testmultipleresubmits(testsplitlogmanager.java:287) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:601) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:26) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:27) at org.junit.runners.parentrunner.runleaf(parentrunner.java:271) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63) at org.junit.runners.parentrunner.runchildren(parentrunner.java:236) at org.junit.runners.parentrunner.access$000(parentrunner.java:53) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229) at org.junit.runners.parentrunner.run(parentrunner.java:309) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:471) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334) at java.util.concurrent.futuretask.run(futuretask.java:166) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) jimmy xiang or jeffrey zhong is this code path even used any more? should i just remove this test? thanks. ",
        "label": 242
    },
    {
        "text": "wal tools doc cleanup  talk of wal reader verifier  link walplayer  we had a walplayer that loads edits up into hbase cluster but what i wanted yesterday was a wal verifier so i could find the bad wal messing me up. ",
        "label": 314
    },
    {
        "text": "hbaseadmin creates new configurations in getcatalogtracker  hbaseadmin.getcatalogtracker creates new configuration every time it's called, instead hba should reuse the same one and do the copy inside the constructor. ",
        "label": 229
    },
    {
        "text": "use writableutils readvint  in regionload readfields   currently in.readint() is used in regionload.readfields()  more metrics would be added to regionload in the future, we should utilize writableutils.readvint() to reduce the amount of data exchanged between master and region servers. ",
        "label": 333
    },
    {
        "text": "support deletes in hfileoutputformat based bulk import mechanism  during bulk imports, it'll be useful to be able to do delete mutations (either to delete data that already exists in hbase or was inserted earlier during this run of the import). for example, we have a use case, where we are processing a log of data which may have both inserts and deletes in the mix and we want to upload that into hbase using the bulk import mechanism. ",
        "label": 339
    },
    {
        "text": "support cf level storage policy  after reading hbase-12848 and hbase-12934, i wrote a patch to implement cf-level storage policy.   my main purpose is to improve random-read performance for some really hot data, which usually locates in certain column family of a big table. usage:  $ hbase shell  > alter 'table_name', metadata => {'hbase.hstore.block.storage.policy' => 'policy_name'} > alter 'table_name', {name=>'cf_name', metadata => {'hbase.hstore.block.storage.policy' => 'policy_name'}} hdfs's setstoragepolicy can only take effect when new hfile is created in a configured directory, so i had to make sub directories(for each cf) in region's .tmp directory and set storage policy for them. besides, i had to upgrade hadoop version to 2.6.0 because dfs.getstoragepolicy cannot be easily written in reflection, and i needed this api to finish my unit test. ",
        "label": 504
    },
    {
        "text": "rsgroup cleanup unassign code  while walking through rsgroup code, i found that variable misplacedregions has never been added any element into. this makes the unassign region code is not functional. and according to my test, it is actually unnecessary to do that. rsgroupbasedloadbalancer.java private map<servername, list<hregioninfo>> correctassignments(        map<servername, list<hregioninfo>> existingassignments)   throws hbaseioexception{     map<servername, list<hregioninfo>> correctassignments = new treemap<>();     list<hregioninfo> misplacedregions = new linkedlist<>();     correctassignments.put(loadbalancer.bogus_server_name, new linkedlist<>());     for (map.entry<servername, list<hregioninfo>> assignments : existingassignments.entryset()){       servername sname = assignments.getkey();       correctassignments.put(sname, new linkedlist<>());       list<hregioninfo> regions = assignments.getvalue();       for (hregioninfo region : regions) {         rsgroupinfo info = null;         try {           info = rsgroupinfomanager.getrsgroup(               rsgroupinfomanager.getrsgroupoftable(region.gettable()));         } catch (ioexception exp) {           log.debug(\"rsgroup information null for region of table \" + region.gettable(),               exp);         }         if ((info == null) || (!info.containsserver(sname.getaddress()))) {           correctassignments.get(loadbalancer.bogus_server_name).add(region);         } else {           correctassignments.get(sname).add(region);         }       }     }     //todo bulk unassign?     //unassign misplaced regions, so that they are assigned to correct groups.     for(hregioninfo info: misplacedregions) {       try {         this.masterservices.getassignmentmanager().unassign(info);       } catch (ioexception e) {         throw new hbaseioexception(e);       }     }     return correctassignments;   } ",
        "label": 480
    },
    {
        "text": "use optional t  return types when t can be null  i've already done lots of nullable to optional change when purging the interfaces for cp. this is a big one so open a separated issue for it. ",
        "label": 149
    },
    {
        "text": "testhfileoutputformat testexcludeallfromminorcompaction fails  this one fails pretty frequently in hadoopqa but also on build servers. i'm going to disable it for now and make a new issue to reenable for whoever wants to take a looksee. here are some of the fails: from precommit: https://builds.apache.org/job/precommit-hbase-build/6484//testreport/org.apache.hadoop.hbase.mapreduce/testhfileoutputformat/testexcludeallfromminorcompaction/ fails like this: java.lang.assertionerror at org.junit.assert.fail(assert.java:86) at org.junit.assert.fail(assert.java:95) at org.apache.hadoop.hbase.mapreduce.testhfileoutputformat.quickpoll(testhfileoutputformat.java:808) at org.apache.hadoop.hbase.mapreduce.testhfileoutputformat.testexcludeallfromminorcompaction(testhfileoutputformat.java:710) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.runners.parentrunner.runleaf(parentrunner.java:271) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63) at org.junit.runners.parentrunner.runchildren(parentrunner.java:236) at org.junit.runners.parentrunner.access$000(parentrunner.java:53) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229) at org.junit.runners.parentrunner.run(parentrunner.java:309) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) here is from ec2 build box: http://54.241.6.143/job/hbase-trunk-hadoop-2/org.apache.hbase$hbase-server/423/testreport/org.apache.hadoop.hbase.mapreduce/testhfileoutputformat/testexcludeallfromminorcompaction/ fails like this (the same): java.lang.assertionerror at org.junit.assert.fail(assert.java:86) at org.junit.assert.fail(assert.java:95) at org.apache.hadoop.hbase.mapreduce.testhfileoutputformat.quickpoll(testhfileoutputformat.java:808) at org.apache.hadoop.hbase.mapreduce.testhfileoutputformat.testexcludeallfromminorcompaction(testhfileoutputformat.java:710) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.runners.parentrunner.runleaf(parentrunner.java:271) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:70) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:50) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:63) at org.junit.runners.parentrunner.runchildren(parentrunner.java:236) at org.junit.runners.parentrunner.access$000(parentrunner.java:53) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:229) at org.junit.runners.parentrunner.run(parentrunner.java:309) at org.junit.runners.suite.runchild(suite.java:127) at org.junit.runners.suite.runchild(suite.java:26) at org.junit.runners.parentrunner$3.run(parentrunner.java:238) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662) ",
        "label": 314
    },
    {
        "text": "improve reporting of incorrect peer address in replication  i was running some replication code that incorrectly advertised the peer address for replication. hbase complained that the format of the record was not what it was expecting but it didn't include what it saw in the exception message. including that string would help cutting down the time it takes to debug issues like that. ",
        "label": 371
    },
    {
        "text": "bad characters in logs for server names  splitlogmanager  task following pbuf  see how the server name is printed: 2012-08-29 14:28:53,567 info org.apache.hadoop.hbase.master.splitlogmanager: task /hbase/splitlog/hdfs%3a%2f%2flocalhost%3a9000%2fhbase%2f.logs%2flocalhost%2c60202%2c1346241077569-splitting%2flocalhost%252c60202%252c1346241077569.1346241967431 entered state pbuf\b\u0003\u0012\u0016  localhost\u0010\ufffd\ufffd\u0003\u0018\ufffd\ufffd\u0791\ufffd' ",
        "label": 15
    },
    {
        "text": "should run a 'mvn install' at the end of hadoop check in pre commit job  now for branch-2.x, we will build with hadoop 3.x in the hadoop check stage, so in later unit check, if we run from a sub module, then the other hbase modules we depend on will depend on hadoop 3.x while the module we build will depend on 2.x, this will cause the following problem 2019-05-23 04:47:41,156 warn  [rs_close_meta-regionserver/b001f91a596c:0-0] handler.assignregionhandler(157): fatal error occurred while opening region hbase:meta,,1.1588230740, aborting... java.lang.incompatibleclasschangeerror: found class org.apache.hadoop.hdfs.protocol.hdfsfilestatus, but interface was expected at org.apache.hadoop.hbase.io.asyncfs.fanoutoneblockasyncdfsoutputhelper.createoutput(fanoutoneblockasyncdfsoutputhelper.java:496) at org.apache.hadoop.hbase.io.asyncfs.fanoutoneblockasyncdfsoutputhelper.access$400(fanoutoneblockasyncdfsoutputhelper.java:116) at org.apache.hadoop.hbase.io.asyncfs.fanoutoneblockasyncdfsoutputhelper$8.docall(fanoutoneblockasyncdfsoutputhelper.java:576) at org.apache.hadoop.hbase.io.asyncfs.fanoutoneblockasyncdfsoutputhelper$8.docall(fanoutoneblockasyncdfsoutputhelper.java:571) at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81) at org.apache.hadoop.hbase.io.asyncfs.fanoutoneblockasyncdfsoutputhelper.createoutput(fanoutoneblockasyncdfsoutputhelper.java:584) at org.apache.hadoop.hbase.io.asyncfs.asyncfsoutputhelper.createoutput(asyncfsoutputhelper.java:51) at org.apache.hadoop.hbase.regionserver.wal.asyncprotobuflogwriter.initoutput(asyncprotobuflogwriter.java:169) at org.apache.hadoop.hbase.regionserver.wal.abstractprotobuflogwriter.init(abstractprotobuflogwriter.java:166) at org.apache.hadoop.hbase.wal.asyncfswalprovider.createasyncwriter(asyncfswalprovider.java:105) at org.apache.hadoop.hbase.regionserver.wal.asyncfswal.createasyncwriter(asyncfswal.java:664) at org.apache.hadoop.hbase.regionserver.wal.asyncfswal.createwriterinstance(asyncfswal.java:670) at org.apache.hadoop.hbase.regionserver.wal.asyncfswal.createwriterinstance(asyncfswal.java:128) at org.apache.hadoop.hbase.regionserver.wal.abstractfswal.rollwriter(abstractfswal.java:832) at org.apache.hadoop.hbase.regionserver.wal.abstractfswal.rollwriter(abstractfswal.java:538) at org.apache.hadoop.hbase.regionserver.wal.abstractfswal.init(abstractfswal.java:479) at org.apache.hadoop.hbase.wal.abstractfswalprovider.getwal(abstractfswalprovider.java:156) at org.apache.hadoop.hbase.wal.abstractfswalprovider.getwal(abstractfswalprovider.java:61) at org.apache.hadoop.hbase.wal.walfactory.getwal(walfactory.java:293) at org.apache.hadoop.hbase.regionserver.hregionserver.getwal(hregionserver.java:2170) at org.apache.hadoop.hbase.regionserver.handler.assignregionhandler.process(assignregionhandler.java:133) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:104) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at java.lang.thread.run(thread.java:748) ",
        "label": 187
    },
    {
        "text": "remove htabledescriptor from hregioninfo  there is an hregioninfo for every region in hbase. currently hregioninfo also contains the htabledescriptor (the schema). that means we store the schema n times where n is the number of regions in the table. additionally, for every region of the same table that the region server has open, there is a copy of the schema. thus it is stored in memory once for each open region. if hregioninfo merely contained the table name the htabledescriptor could be stored in a separate file and easily found. ",
        "label": 428
    },
    {
        "text": "master aborted when hbck asked the master to assign a region that was already online  came across this situation (with a version of 0.96 very close to rc5 version created on 10/11): the sequence of events that happened: 1. the hbck tool couldn't communicate with the regionserver hosting namespace region due to some security exceptions. hbck incorrectly assumed the region was not deployed.  in output.log (client side): 2013-10-12 10:42:57,067|beaver.machine|info|error: region { meta => hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a., hdfs => hdfs://gs-hdp2-secure-1381559462-hbase-12.cs1cloud.internal:8020/apps/hbase/data/data/hbase/namespace/a0ac0825ba2d0830614e7f808f31787a, deployed =>  } not deployed on any region server. 2013-10-12 10:42:57,067|beaver.machine|info|trying to fix unassigned region... 2. this led to the hbck tool trying to tell the master to \"assign\" the region.  in master log (hbase-hbase-master-gs-hdp2-secure-1381559462-hbase-12.log): 2013-10-12 10:52:35,960 info  [rpcserver.handler=4,port=60000] master.hmaster: client=hbase//172.18.145.105 assign hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a. 3. the master went through the steps - sent a close to the regionserver hosting namespace region.  from master log: 2013-10-12 10:52:35,981 debug [rpcserver.handler=4,port=60000] master.assignmentmanager: sent close to gs-hdp2-secure-1381559462-hbase-1.cs1cloud.internal,60020,1381564439794 for region hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a. 4. the master then tried to assign the namespace region to a region server, and in the process aborted:  from master log: 2013-10-12 10:52:36,025 debug [rpcserver.handler=4,port=60000] master.assignmentmanager: no previous transition plan found (or ignoring an existing plan) for hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a.; generated random plan=hri=hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a., src=, dest=gs-hdp2-secure-1381559462-hbase-9.cs1cloud.internal,60020,1381564439807; 4 (online=4, available=4) available servers, forcenewplan=true 2013-10-12 10:52:36,026 fatal [rpcserver.handler=4,port=60000] master.hmaster: master server abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.accesscontroller] 2013-10-12 10:52:36,027 fatal [rpcserver.handler=4,port=60000] master.hmaster: unexpected state : {a0ac0825ba2d0830614e7f808f31787a state=open, ts=1381564451344, server=gs-hdp2-secure-1381559462-hbase-1.cs1cloud.internal,60020,1381564439794} .. cannot transit it to offline. java.lang.illegalstateexception: unexpected state : {a0ac0825ba2d0830614e7f808f31787a state=open, ts=1381564451344, server=gs-hdp2-secure-1381559462-hbase-1.cs1cloud.internal,60020,1381564439794} .. cannot transit it to offline. assignmentmanager.assign(hregioninfo region, boolean setofflineinzk, boolean forcenewplan) is the method that does all the above. this was called from the hmaster with true for both the boolean arguments. ",
        "label": 242
    },
    {
        "text": "use zookeeper in hbase  zookeeper =~ chubby. this means that we could take advantage of a distributed lock manager to coordinate things like failover masters, regionservers staying online when master is dead, atomic region->regionserver assignments, etc. there are a lot of opportunities for improvements here. please add discussions of particular features in comments or sub-tasks. ",
        "label": 229
    },
    {
        "text": "inclusivestopfilter does not respect reverse filter property  inclusivestopfilter only works with non-reversed scans, it will not filter for reversed scans, because it doesn't flip the cmp-operand in the reversed case. in fact, it doesn't even use the filter.reverse flag. it should be something like this: if (reversed) {  if (cmp > 0) { done = true; }  }  else {  if (cmp < 0) { done = true; } } ",
        "label": 29
    },
    {
        "text": " fb  allow compaction related configurations to be reloaded on the fly  we already have the infrastructure to reload the configurations on-the-fly for regionservers (through hbase-8544 and hbase-8576). this change allows us to change compaction related configurations by using that infrastructure. this is already done, and should appear in the 89-fb branch soon. ",
        "label": 181
    },
    {
        "text": "broken master failover  master crashed, sigsegv (0xb) at pc=0x00000031a40fea07, pid=14689, tid=1133910336. four other masters running ready to take the failover. i see where we move to new master but there is an error: 2009-09-13 22:07:02,061 debug org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: wrote master address xx.xx.xx.251:20000 to zookeeper 2009-09-13 22:07:02,064 warn org.apache.hadoop.hbase.zookeeper.zookeeperwrapper: failed to set state node in zookeeper org.apache.zookeeper.keeperexception$nodeexistsexception: keepererrorcode = nodeexists for /hbase/shutdown         at org.apache.zookeeper.keeperexception.create(keeperexception.java:110)         at org.apache.zookeeper.keeperexception.create(keeperexception.java:42)         at org.apache.zookeeper.zookeeper.create(zookeeper.java:522)         at org.apache.hadoop.hbase.zookeeper.zookeeperwrapper.setclusterstate(zookeeperwrapper.java:279)         at org.apache.hadoop.hbase.master.hmaster.writeaddresstozookeeper(hmaster.java:270)         at org.apache.hadoop.hbase.master.hmaster.<init>(hmaster.java:255)         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(unknown source)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(unknown source)         at java.lang.reflect.constructor.newinstance(unknown source)         at org.apache.hadoop.hbase.master.hmaster.domain(hmaster.java:1200)         at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:1241) is /hbase/shutdown now ephemeral? otherwise, the transition went off well it seems. except, if i look in zk \u2013 this is a good while after teh event \u2013 i do not see a master.. its empty. do we not record in zk on failover? but then a split comes in: 2009-09-17 05:50:05,070 info org.apache.hadoop.hbase.master.basescanner: all 1 .meta. region(s) scanned  2009-09-17 05:50:30,745 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_split: enwikibase_dumpurls,,1253145470066: daughters; enwikibase_dum  purls,,1253166628107, enwikibase_dumpurls,ezadzwpbtg_o9blsequ4bv\\x3d\\x3d,1253166628107 from aa0-018-6.u.powerset.com,20020,1251458355425; 1 of 3  2009-09-17 05:50:30,745 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.44.91:20020, startcode: 1251458355425, load: (requests=0, r  egions=3, usedheap=490, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  2009-09-17 05:50:30,838 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.44.49:20020, startcode: 1250638276455, load: (requests=3, r  egions=4, usedheap=134, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  2009-09-17 05:50:31,117 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.45.128:20020, startcode: 1250638269214, load: (requests=5,   regions=4, usedheap=130, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  2009-09-17 05:50:31,119 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.45.221:20020, startcode: 1250638268709, load: (requests=5,   regions=4, usedheap=82, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  2009-09-17 05:50:31,150 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.44.75:20020, startcode: 1250638276632, load: (requests=9, r  egions=4, usedheap=284, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  2009-09-17 05:50:31,215 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.45.180:20020, startcode: 1250638269143, load: (requests=11,  regions=4, usedheap=132, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  2009-09-17 05:50:31,265 debug org.apache.hadoop.hbase.master.regionmanager: assigning for address: xx.xx.45.121:20020, startcode: 1250638269297, load: (requests=5,   regions=4, usedheap=54, maxheap=2031): total nregions to assign=2, nregions to reach balance=4, ismetaassign=false  ... and we never recover from the above (12 hours and still at it). ",
        "label": 247
    },
    {
        "text": "add branch post commit builds  point hbase-1.3 build to branch-1.3, add hbase-1.4 and point it to branch-1. ",
        "label": 441
    },
    {
        "text": "change unittests that use  table  or  testtable  to use method names   while working on hbase-9686, many tests left files that indicated the method they had come from but several drop data in \"table\" or \"testtable\" tables. naming them this way makes it hard to track which tests these came from. we should make all test use @rule testname name = new testname();  ...  tablename t = tablename.valueof(name.getmethodname()); ",
        "label": 226
    },
    {
        "text": "consolidate user guide style documentation  it would be great to clean up our documentation prior to the next major release. we have various bits of docs strewn throughout the javadoc, but it's a lot of \"hidden gems\" (eg the mapreduce package docs) whereas a separate \"programmers guide\" would be a lot better. ",
        "label": 314
    },
    {
        "text": "port hadoop  protect authentication cookies with the httponly and secure flags  this came via a security scanner, since we have a fork of httpserver2 in hbase we should include it too. ",
        "label": 163
    },
    {
        "text": "hbck can not fix meta not assigned issue  when meta table region is not assigned to any rs, hbck run will get exception. i can see code added in checkmetaregion() to solve this issue but it wont work. it still refers to root region! ",
        "label": 46
    },
    {
        "text": "backport loadtesttool to  loadtesttool is very useful.  this jira backports loadtesttool to 0.92 so that users don't have to build trunk in order to use it against 0.92 cluster. ",
        "label": 38
    },
    {
        "text": "heap size computation for hstore is fishy  there's something off with heap size computation for hstore. if one adds a long to hstore, and size of long to the computation, testheapsize passes both locally (for me) and in jenkins. if one adds an int and size of int, it passes locally but fails in jenkins. perhaps the numbers are already off and some sort of packing is taking/not taking place differently. on a tangentially related note, if we can obtain size programmatically (the way we do it in test), and only need to do it once, i wonder if we should just do it and remove all the manually modifiable constants stuff. ",
        "label": 406
    },
    {
        "text": "scripts passed to hbase shell do not have shell context set up for them  the run of a passed script is happening before all of the setup in hirb.rb. fix. ",
        "label": 314
    },
    {
        "text": "add force compaction and force split operations to ui and admin  would help debugging or at start of big upload or if we have a hot region we want to distribute over the cluster if we could just manually force the splitting of regions. i thought it would be just a little messing in jsp but its a bit more than that. actual decision to split is made down in guts of the store. won't split if any references still around and a file must be > maxsize. chatting w/ jon gray, could set a flag on the region that we want to split. could do this from ui (can get to hregion instance from hrs instance in hrs jsp). stores have references to their hosting regions (i think). when figuring if we're to split, check the force split flag up in the hosting region. to get the flag checked, safest way is to just force a flush (otherwise, have to mess w/ locks in the compact/splitter thread). registering a flush request on a region is easy enough to do. its a public method on hrs (or hr). ",
        "label": 38
    },
    {
        "text": "testmasterfailover testmasterfailoverwithmockedritondeadrs occasionally fails  look this logs:  https://builds.apache.org/view/g-l/view/hbase/job/hbase-0.92/105/testreport/org.apache.hadoop.hbase.master/testmasterfailover/testmasterfailoverwithmockedritondeadrs/ ",
        "label": 314
    },
    {
        "text": "hstore close does not honor config hbase rs evictblocksonclose  i noticed moving regions was slow and due to the wait for the bucket cache to clear. i tried setting hbase.rs.evictblocksonclose and it did not help. i see the hstore::close method has evictonclose hard coded to true instead of letting the config dictate: // close each store file in parallel  completionservice<void> completionservice =  new executorcompletionservice<void>(storefilecloserthreadpool);  for (final storefile f : result) {  completionservice.submit(new callable<void>() {  @override  public void call() throws ioexception { f.closereader(true); return null; } });  } ",
        "label": 478
    },
    {
        "text": "can't enable a table on a cluster from a client  in 0.92 we know a table's enabled by doing this in hcm.isenabledtable: return gettablestate(zkw, tablename) == null; in 0.94 we do: return gettablestate(zkw, tablename) == tablestate.enabled; so what happens is that the the 0.92 client will hang forever since the znode contains enabled instead of being absent. ",
        "label": 441
    },
    {
        "text": "fix the link in the docs to  understanding hbase and bigtable  by jim r  wilson  blog post understanding hbase and bigtable by jim r. wilson link is dead. please update or remove. https://hbase.apache.org/book.html#conceptual.view ",
        "label": 334
    },
    {
        "text": " fb  add more context per operation at htable level  provide a context object for all the hbase api call, which contains the row, cf, region, regionserver, retrynumbers and lastpotentialexceptions ",
        "label": 378
    },
    {
        "text": "the hbase hbase daemon sh sigkills master when stopping it  there's a bit of code in hbase-daemon.sh that makes hbase master being sigkilled when stopping it rather than trying sigterm (like it does for other daemons). when hbase is executed in a standalone mode (and the only daemon you need to run is master) that causes newly created tables to go missing as unflushed data is thrown out. if there was not a good reason to kill master with sigkill perhaps we can take that special case out and rely on sigterm. ",
        "label": 382
    },
    {
        "text": "backport hbase  don't delete hfiles in backup mode   see hbase-5547 ",
        "label": 236
    },
    {
        "text": "list of committers on credits page is out of date  the list of committers on the hbase credits page http://hadoop.apache.org/hbase/credits.html is out of date and needs to be updated before the next time we build the site. ",
        "label": 314
    },
    {
        "text": "restore api compat for performanceevaluation generatevalue   observed: a couple of my client tests fail to compile against trunk because the method performanceevaluation.generatevalue was removed as part of hbase-8496. this is an issue because it was used in a number of places, including unit tests. since we did not explicitly label this api as private, it's ambiguous as to whether this could/should have been used by people writing apps against 0.96. if they used it, then they would be broken upon upgrade to 0.98 and trunk. potential solution:  the method was renamed to generatedata, but the logic is still the same. we can reintroduce it as deprecated in 0.98, as compat shim over generatedata. the patch should be a few lines. we may also consider doing so in trunk, but i'd be just as fine with leaving it out. more generally, this raises the question about what other code is in this \"grey-area\", where it is public, is used outside of the package, but is not explicitly labeled with an audienceinterface. ",
        "label": 141
    },
    {
        "text": "lower ok findbugs warnings in test patch properties  hbase-9903 removed generated classes from findbugs checking. ok_findbugs_warnings in test-patch.properties should be lowered. according to https://builds.apache.org/job/precommit-hbase-build/7776/artifact/trunk/patchprocess/newpatchfindbugswarningshbase-server.html , there were: 3 warnings for org.apache.hadoop.hbase.generated classes  19 warnings for org.apache.hadoop.hbase.tmpl classes ",
        "label": 441
    },
    {
        "text": "setting bucket cache combined key to false disables stats on rs ui  enabling offheap cache and setting bucket_cache_combined_key to false in site xml to make offheap cache a strict l2 cache to lru cache, disables the l2 stats normally rendered on region server ui. ",
        "label": 72
    },
    {
        "text": "backport to   hbase prefetching  meta  rows in case only when usecache is set to true   ",
        "label": 46
    },
    {
        "text": "error message is wrong when a wrong namspace is specified in grant in hbase shell  in hbase shell, specify a non-existing namespace in \"grant\" command, such as hbase(main):001:0> grant 'a1', 'r', '@aaa'    <--- there is no namespace called \"aaa\" the error message issued is not correct error: unknown namespace a1! a1 is the user name, not the namespace. the following error message would be better error: unknown namespace aaa! or can't find a namespace: aaa ",
        "label": 459
    },
    {
        "text": "when 'fs default name' not configured  the hbck tool and merge tool throw illegalargumentexception   the hbase do not configure the 'fs.default.name' attribute, the hbck tool and merge tool throw illegalargumentexception.  the hbck tool and merge tool, we should add 'fs.default.name' attriubte to the code. hbck exception:  exception in thread \"main\" java.lang.illegalargumentexception: wrong fs: hdfs://160.176.0.101:9000/hbase/.meta./1028785192/.regioninfo, expected: file:///  at org.apache.hadoop.fs.filesystem.checkpath(filesystem.java:412)  at org.apache.hadoop.fs.rawlocalfilesystem.pathtofile(rawlocalfilesystem.java:59)  at org.apache.hadoop.fs.rawlocalfilesystem.getfilestatus(rawlocalfilesystem.java:382)  at org.apache.hadoop.fs.filterfilesystem.getfilestatus(filterfilesystem.java:285)  at org.apache.hadoop.fs.checksumfilesystem$checksumfsinputchecker.<init>(checksumfilesystem.java:128)  at org.apache.hadoop.fs.checksumfilesystem.open(checksumfilesystem.java:301)  at org.apache.hadoop.fs.filesystem.open(filesystem.java:489)  at org.apache.hadoop.hbase.util.hbasefsck.loadhdfsregioninfo(hbasefsck.java:565)  at org.apache.hadoop.hbase.util.hbasefsck.loadhdfsregioninfos(hbasefsck.java:596)  at org.apache.hadoop.hbase.util.hbasefsck.onlineconsistencyrepair(hbasefsck.java:332)  at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:360)  at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:2907) merge exception:   [2012-05-05 10:48:24,830] [error] [main] [org.apache.hadoop.hbase.util.merge 381] exiting due to error  java.lang.illegalargumentexception: wrong fs: hdfs://160.176.0.101:9000/hbase/.meta./1028785192/.regioninfo, expected: file:///  at org.apache.hadoop.fs.filesystem.checkpath(filesystem.java:412)  at org.apache.hadoop.fs.rawlocalfilesystem.pathtofile(rawlocalfilesystem.java:59)  at org.apache.hadoop.fs.rawlocalfilesystem.getfilestatus(rawlocalfilesystem.java:382)  at org.apache.hadoop.fs.filterfilesystem.getfilestatus(filterfilesystem.java:285)  at org.apache.hadoop.fs.filesystem.exists(filesystem.java:823)  at org.apache.hadoop.hbase.regionserver.hregion.checkregioninfoonfilesystem(hregion.java:415)  at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:340)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2679)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2665)  at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:2634)  at org.apache.hadoop.hbase.util.metautils.openmetaregion(metautils.java:276)  at org.apache.hadoop.hbase.util.metautils.scanmetaregion(metautils.java:261)  at org.apache.hadoop.hbase.util.merge.run(merge.java:115)  at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65)  at org.apache.hadoop.hbase.util.merge.main(merge.java:379) ",
        "label": 528
    },
    {
        "text": "adding gc details prevents hbase from starting in non distributed mode  the conf/hbase-env.sh that ships with hbase contains a few commented out examples of variables that could be useful, such as adding -xx:+printgcdetails -xx:+printgcdatestamps to hbase_opts. this has the annoying side effect that the jvm prints a summary of memory usage when it exits, and it does so on stdout: $ ./bin/hbase org.apache.hadoop.hbase.util.hbaseconftool hbase.cluster.distributed false heap  par new generation   total 19136k, used 4908k [0x000000073a200000, 0x000000073b6c0000, 0x0000000751860000)   eden space 17024k,  28% used [0x000000073a200000, 0x000000073a6cb0a8, 0x000000073b2a0000)   from space 2112k,   0% used [0x000000073b2a0000, 0x000000073b2a0000, 0x000000073b4b0000)   to   space 2112k,   0% used [0x000000073b4b0000, 0x000000073b4b0000, 0x000000073b6c0000)  concurrent mark-sweep generation total 63872k, used 0k [0x0000000751860000, 0x00000007556c0000, 0x00000007f5a00000)  concurrent-mark-sweep perm gen total 21248k, used 6994k [0x00000007f5a00000, 0x00000007f6ec0000, 0x0000000800000000) $ ./bin/hbase org.apache.hadoop.hbase.util.hbaseconftool hbase.cluster.distributed >/dev/null (nothing printed) and this confuses bin/start-hbase.sh when it does  distmode=`$bin/hbase --config \"$hbase_conf_dir\" org.apache.hadoop.hbase.util.hbaseconftool hbase.cluster.distributed`, because then the distmode variable is not just set to false, it also contains all this jvm spam. if you don't pay enough attention and realize that 3 processes are getting started (zk, hm, rs) instead of just one (hm), then you end up with this confusing error message:  could not start zk at requested port of 2181. zk was started at port: 2182. aborting as clients (e.g. shell) will not be able to find this zk quorum., which is even more puzzling because when you run netstat to see who owns that port, then you won't find any rogue process other than the one you just started. i'm wondering if the fix is not to just change the if [ \"$distmode\" == 'false' ] to a switch $distmode case (false*) type of test, to work around this annoying jvm misfeature that pollutes stdout. ",
        "label": 312
    },
    {
        "text": "client requires write access to hbase local dir unnecessarily  per this thread from the dev list. it appears that as of hbase-1936, we now require that client applications have write access to hbase.local.dir. this is because protobufutil instantiates a dyanamicclassloader as part of static initialization. this classloader is used for instantiating comparators, filters, and exceptions. client applications do not need to use dynamicclassloader and so should not require this write access. ",
        "label": 242
    },
    {
        "text": "hconnection create final connection conf  does not clone  it creates a new configuration reading  xmls and then does a merge   its more expensive that it should be; its causing testadmin to fail after hbase-4417 went in. ",
        "label": 340
    },
    {
        "text": "add a tool to dump the procedure info in hfile  ",
        "label": 149
    },
    {
        "text": " snapshot merge  fix testhfilearchving testarchiveontabledelete  after the 2/1/13 merge this test case started to fail. looks like after deleting a table files are moved to <hbase>/.tmp/.archive instead of <hbase>/.archive. 2013-02-01 16:23:25,037 debug [main] util.fsutils(1395): current file system: 2013-02-01 16:23:25,040 debug [main] util.fsutils(1411): |-user/ 2013-02-01 16:23:25,042 debug [main] util.fsutils(1411): |----jon/ 2013-02-01 16:23:25,045 debug [main] util.fsutils(1411): |-------hbase/ 2013-02-01 16:23:25,048 debug [main] util.fsutils(1411): |-----------root-/ 2013-02-01 16:23:25,051 debug [main] util.fsutils(1414): |-------------.tableinfo.0000000001 2013-02-01 16:23:25,051 debug [main] util.fsutils(1411): |-------------.tmp/ 2013-02-01 16:23:25,053 debug [main] util.fsutils(1411): |-------------70236052/ 2013-02-01 16:23:25,056 debug [main] util.fsutils(1411): |----------------.oldlogs/ 2013-02-01 16:23:25,059 debug [main] util.fsutils(1414): |-------------------hlog.1359764552904 2013-02-01 16:23:25,059 debug [main] util.fsutils(1414): |----------------.regioninfo 2013-02-01 16:23:25,060 debug [main] util.fsutils(1411): |----------------info/ 2013-02-01 16:23:25,062 debug [main] util.fsutils(1414): |-------------------73d7663c7ffa47089e5138f79af6da5c 2013-02-01 16:23:25,062 debug [main] util.fsutils(1411): |----------.meta./ 2013-02-01 16:23:25,064 debug [main] util.fsutils(1414): |-------------.tableinfo.0000000001 2013-02-01 16:23:25,065 debug [main] util.fsutils(1411): |-------------.tmp/ 2013-02-01 16:23:25,066 debug [main] util.fsutils(1411): |-------------1028785192/ 2013-02-01 16:23:25,069 debug [main] util.fsutils(1411): |----------------.oldlogs/ 2013-02-01 16:23:25,072 debug [main] util.fsutils(1414): |-------------------hlog.1359764553440 2013-02-01 16:23:25,073 debug [main] util.fsutils(1414): |----------------.regioninfo 2013-02-01 16:23:25,073 debug [main] util.fsutils(1411): |----------------info/ 2013-02-01 16:23:25,075 debug [main] util.fsutils(1411): |----------.logs/ 2013-02-01 16:23:25,078 debug [main] util.fsutils(1411): |-------------localhost,39842,1359764552342/ 2013-02-01 16:23:25,080 debug [main] util.fsutils(1414): |----------------localhost%2c39842%2c1359764552342.1359764554756 2013-02-01 16:23:25,081 debug [main] util.fsutils(1414): |----------------localhost%2c39842%2c1359764552342.1359764555929.meta 2013-02-01 16:23:25,081 debug [main] util.fsutils(1411): |----------.oldlogs/ 2013-02-01 16:23:25,083 debug [main] util.fsutils(1411): |----------.tmp/ 2013-02-01 16:23:25,085 debug [main] util.fsutils(1411): |-------------.archive/ 2013-02-01 16:23:25,087 debug [main] util.fsutils(1411): |----------------test_table/ 2013-02-01 16:23:25,089 debug [main] util.fsutils(1411): |-------------------8293b640249b4c47e469119a9ba4514a/ 2013-02-01 16:23:25,091 debug [main] util.fsutils(1411): |----------------------fam/ 2013-02-01 16:23:25,093 debug [main] util.fsutils(1414): |-------------------------2b6faae136d648b38f335ff9c885b294 2013-02-01 16:23:25,093 debug [main] util.fsutils(1414): |-------------------------4ba2080dbd6f4c3eb9b7a70ccefe9f9b 2013-02-01 16:23:25,094 debug [main] util.fsutils(1414): |-------------------------50dd7ee0cb2049158de3139e814f4784 2013-02-01 16:23:25,094 debug [main] util.fsutils(1414): |-------------------------86544bf4d23a4413b01c3b2e7e2f5562 2013-02-01 16:23:25,094 debug [main] util.fsutils(1414): |-------------------------aecf536371374393a113af2875b38997 2013-02-01 16:23:25,094 debug [main] util.fsutils(1414): |----------hbase.id 2013-02-01 16:23:25,094 debug [main] util.fsutils(1414): |----------hbase.version ",
        "label": 248
    },
    {
        "text": "commons collections object deserialization remote command execution vulnerability  read: http://foxglovesecurity.com/2015/11/06/what-do-weblogic-websphere-jboss-jenkins-opennms-and-your-application-have-in-common-this-vulnerability/ tl;dr: if you have commons-collections on your classpath and accept and process java object serialization data, then you probably have an exploitable remote command execution vulnerability. 0.94 and earlier hbase releases are vulnerable because we might read in and rehydrate serialized java objects out of rpc packet data in hbaseobjectwritable using objectinputstream#readobject (see https://hbase.apache.org/0.94/xref/org/apache/hadoop/hbase/io/hbaseobjectwritable.html#714) and we have commons-collections on the classpath on the server. 0.98 also carries some limited exposure to this problem through inclusion of backwards compatible deserialization code in hbaseobjectwritablefor96migration. this is used by the 0.94-to-0.98 migration utility, and by the accesscontroller when reading permissions from the acl table serialized in legacy format by 0.94. unprivileged users cannot run the tool nor access the acl table. unprivileged users can however attack a 0.94 installation. an attacker might be able to use the method discussed on that blog post to capture valid hbase rpc payloads for 0.94 and prior versions, rewrite them to embed an exploit, and replay them to trigger a remote command execution with the privileges of the account under which the hbase regionserver daemon is running. we need to make a patch release of 0.94 that changes hbaseobjectwritable to disallow processing of random java object serializations. this will be a compatibility break that might affect old style coprocessors, which quite possibly may rely on this catch-all in hbaseobjectwritable for custom object (de)serialization. we can introduce a new configuration setting, \"hbase.allow.legacy.object.serialization\", defaulting to false. to be thorough, we can also use the new configuration setting \"hbase.allow.legacy.object.serialization\" (defaulting to false) in 0.98 to prevent the accesscontroller from falling back to the vulnerable legacy code. this turns out to not affect the ability to migrate permissions because tablepermission implements writable, which is safe, not serializable. ",
        "label": 38
    },
    {
        "text": "ability to load filterlist class is dependent on context classloader  in the 0.94 branch, the filterlist class contains a static call to hbaseconfiguration.create(). this create call in turn adds the needed hbase resources to the configuration object, and sets the classloader of the configuration object to be the context classloader of the current thread (if it isn't null). this approach causes issues if the filterlist class is loaded from a thread that has a custom context classloader that doesn't run back up to the main application classloader. in this case, hbaseconfiguration.checkdefaultsversion fails because the hbase.defaults.for.version configuration value can't be found (because hbase-default.xml can't be found by the custom context classloader). this is a concrete issue that was discovered via apache phoenix within a commercial tool, when a (jdbc) connection is opened via a pool, and then passed off to a ui thread that has a custom context classloader. the ui thread is then the first thing to load filterlist, leading to this issue. ",
        "label": 178
    },
    {
        "text": "set branch eol  ",
        "label": 402
    },
    {
        "text": "allow creating table in group when rs group contains no live servers  this is for api consistency. in general, a rs group could have no live servers if all the region servers are dead, and then the regions in side this group can not online. so it is a bit strange that we do not allow new regions here since they are just the same with the old regions... ",
        "label": 149
    },
    {
        "text": "fake cells created in read path not implementing settablesequenceid  this issue found by apekshit sharma. in hbase-14099 he says,  i was doing some testing when i hit a weird issue, seems related to this, so re-opening it (apologies in advance if it's not). here's the stack trace java.io.ioexception: java.lang.unsupportedoperationexception: cell is not of type org.apache.hadoop.hbase.settablesequenceid at org.apache.hadoop.hbase.cellutil.setsequenceid(cellutil.java:923) at org.apache.hadoop.hbase.regionserver.storefilescanner.setcurrentcell(storefilescanner.java:231) at org.apache.hadoop.hbase.regionserver.storefilescanner.requestseek(storefilescanner.java:389) at org.apache.hadoop.hbase.regionserver.storescanner.seekscanners(storescanner.java:348) at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:212) at org.apache.hadoop.hbase.regionserver.hstore.createscanner(hstore.java:1873) at org.apache.hadoop.hbase.regionserver.hstore.getscanner(hstore.java:1863) at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.<init>(hregion.java:5487) at org.apache.hadoop.hbase.regionserver.hregion.instantiateregionscanner(hregion.java:2577) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:2563) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:2544) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:2534) at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:6659) at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:6624) at org.apache.hadoop.hbase.regionserver.testwithsinglehregion.test(testwithsinglehregion.java:48) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:50) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:47) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.runners.parentrunner.runleaf(parentrunner.java:325) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:78) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:57) at org.junit.runners.parentrunner$3.run(parentrunner.java:290) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:71) at org.junit.runners.parentrunner.runchildren(parentrunner.java:288) at org.junit.runners.parentrunner.access$000(parentrunner.java:58) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:268) at org.junit.runners.parentrunner.run(parentrunner.java:363) at org.junit.runner.junitcore.run(junitcore.java:137) at com.intellij.junit4.junit4ideatestrunner.startrunnerwithargs(junit4ideatestrunner.java:117) at com.intellij.rt.execution.junit.junitstarter.preparestreamsandstart(junitstarter.java:234) at com.intellij.rt.execution.junit.junitstarter.main(junitstarter.java:74) i think it's because of using changing from keyvalue to a different sub-class of cell}}l which doesn't implement {{settablesequenceid -    this.startkey = keyvalueutil.createfirstdeletefamilyonrow(scan.getstartrow(), +    this.startkey = cellutil.createfirstdeletefamilycellonrow(scan.getstartrow(), to replicate it, download the attached hfiles somewhere, copy the testwithsinglehregion class to regionserver tests, change the root_dir appropriately and run it. ",
        "label": 29
    },
    {
        "text": "illegalargumentexception in halfhfilereader next  from posix4e up on irc # 2009-06-07 20:22:33,367 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region t3,*******************,1244420117045 # java.lang.illegalargumentexception #         at java.nio.buffer.position(buffer.java:218) #         at org.apache.hadoop.hbase.io.hfile.hfile$reader$scanner.next(hfile.java:1072) #         at org.apache.hadoop.hbase.io.halfhfilereader$1.next(halfhfilereader.java:108) #         at org.apache.hadoop.hbase.regionserver.storefilescanner.next(storefilescanner.java:52) #         at org.apache.hadoop.hbase.regionserver.keyvalueheap.next(keyvalueheap.java:79) #         at org.apache.hadoop.hbase.regionserver.minorcompactingstorescanner.next(minorcompactingsto # rescanner.java:101) #         at org.apache.hadoop.hbase.regionserver.store.compact(store.java:849) #         at org.apache.hadoop.hbase.regionserver.store.compact(store.java:714) #         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:766) #         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:723) #         at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:105) ",
        "label": 314
    },
    {
        "text": "get with timestamp will return a value if there is a version with an earlier timestamp  when an explicit timestamp is specified, no results should be returned if there is no value stored at that timestamp. a value should be returned (as it currently is) if the timestamp is defaulted or is latest_timestamp (which is the same thing). this works correctly. table name \"web\", columns: \"contents:\", \"anchor:\" store com.cnn.www/contents:/5 value = \"t5\"  store com.cnn.www/anchor:my.look.ca:/8 value = \"cnn.com\"  com.cnn.www/anchor:cnnsi.com/9 value = \"cnn\" get(com.cnn.www/contents:/8) should return nothing but returns value=\"t5\", timestamp=5  get(com.cnn.www/anchor:my.look.ca:/9) should return nothing, but returns value=\"cnn.com\", timestamp=8 ",
        "label": 247
    },
    {
        "text": "asyncrpcclient hangs if connection closes before rpc call response  the test for hbase-15212 discovered an issue with async rpc client. in that test, we are closing the connection if an rpc call writes a call larger than max allowed size, the server closes the connection. however the async client does not seem to handle connection closes with outstanding rpc calls. the client just hangs. marking this blocker against 2.0 since it is default there. ",
        "label": 198
    },
    {
        "text": "close region shell command breaks region  it used to be that you could use the close_region command from the shell to close a region on one server and have the master reassign it elsewhere. now if you close a region, you get the following errors in the master log: 2010-11-23 00:46:34,090 warn org.apache.hadoop.hbase.master.assignmentmanager: received closing for region ffaa7999e909dbd6544688cc8ab303bd from server haus01.sf.cloudera.com,12020,1290501789693 but region was in the state null and not in expected pendi  2010-11-23 00:46:34,530 debug org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: master:60000-0x12c537d84e10062 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/hbase/unassigned/ffaa7999e909dbd6544688cc8ab303bd  2010-11-23 00:46:34,531 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:60000-0x12c537d84e10062 retrieved 128 byte(s) of data from znode /hbase/unassigned/ffaa7999e909dbd6544688cc8ab303bd and set watcher; region=usertable,user1951957302,1290501969  2010-11-23 00:46:34,531 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_closed, server=haus01.sf.cloudera.com,12020,1290501789693, region=ffaa7999e909dbd6544688cc8ab303bd  2010-11-23 00:46:34,531 warn org.apache.hadoop.hbase.master.assignmentmanager: received closed for region ffaa7999e909dbd6544688cc8ab303bd from server haus01.sf.cloudera.com,12020,1290501789693 but region was in the state null and not in expected pendin and the region just gets stuck closed ",
        "label": 314
    },
    {
        "text": " documentation  exportsnapshot tool package incorrectly documented  documentation page: http://hbase.apache.org/book/ops.snapshots.html expected documentation:  the class should be specified as org.apache.hadoop.hbase.snapshot.exportsnapshot current documentation:  specified as: org.apache.hadoop.hbase.snapshot.tool.exportsnapshot this makes sense because the class is located in the org.apache.hadoop.hbase.snapshot package: https://github.com/apache/hbase/blob/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/exportsnapshot.java#19 ",
        "label": 15
    },
    {
        "text": "directmemoryutils getdirectmemoryusage spams when none is configured  my logs are full of \"failed to retrieve nio.bufferpool direct memoryused\". even if it's debug, it adds no value. it'd just remove. ",
        "label": 339
    },
    {
        "text": " hbase thirdparty  shade jetty  ",
        "label": 149
    },
    {
        "text": "  port hbase race between splitlogmanager task creation   timeoutmonitor  see hbase-11217. this can lead to massive data loss as explained there. ",
        "label": 286
    },
    {
        "text": "refresh our hadoop jar because of hdfs  hdfs-1520 adds a new lightweight lease recovery mechanism, but also bumped the protocol's version to 42 which means that currently 0.90 doesn't work on a clean checkout of 0.20-append. ",
        "label": 314
    },
    {
        "text": "alter table broke with new shell returns invalidcolumnnameexception  create table disable table alter table output below: hbase(main):041:0> create 't1', {name => 'f1', versions => 5} 08/06/30 01:26:43 debug client.hconnectionmanager$tableservers: reloading table servers because: no server address listed in .meta. for region t1,,1214807203247 08/06/30 01:26:43 debug client.hconnectionmanager$tableservers: removed .meta.,,1 from cache because of t1,,99999999999999 08/06/30 01:26:43 debug client.hconnectionmanager$tableservers: found root region => {name => '-root-,,0', startkey => '', endkey => '', encoded => 70236052, table => {name => '-root-', families => [{name => 'info', versions => 1, compression => 'none', in_memory => false, blockcache => false, length => 2147483647, ttl => forever, bloomfilter => none}]} 0 row(s) in 10.4300 seconds hbase(main):042:0> disable 't1' 08/06/30 01:27:08 debug client.hbaseadmin: sleep. waiting for first region to be disabled from t1 08/06/30 01:27:18 debug client.hbaseadmin: wake. waiting for first region to be disabled from [b@1bc93a7 08/06/30 01:27:18 info client.hbaseadmin: disabled t1 0 row(s) in 10.0810 seconds hbase(main):043:0> alter 't1', {name => 'f1', versions => 1} nativeexception: org.apache.hadoop.hbase.invalidcolumnnameexception: org.apache.hadoop.hbase.invalidcolumnnameexception: column family 'f1' doesn't exist, so cannot be modified.         at org.apache.hadoop.hbase.master.modifycolumn.postprocessmeta(modifycolumn.java:51)         at org.apache.hadoop.hbase.master.tableoperation$processtableoperation.call(tableoperation.java:130)         at org.apache.hadoop.hbase.master.tableoperation$processtableoperation.call(tableoperation.java:67)         at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:62)         at org.apache.hadoop.hbase.master.tableoperation.process(tableoperation.java:141)         at org.apache.hadoop.hbase.master.hmaster.modifycolumn(hmaster.java:655)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:424)         at org.apache.hadoop.ipc.server$handler.run(server.java:896)         from sun/reflect/nativeconstructoraccessorimpl.java:-2:in `newinstance0'         from sun/reflect/nativeconstructoraccessorimpl.java:39:in `newinstance'         from sun/reflect/delegatingconstructoraccessorimpl.java:27:in `newinstance'         from java/lang/reflect/constructor.java:513:in `newinstance'         from org/apache/hadoop/hbase/remoteexceptionhandler.java:82:in `decoderemoteexception'         from org/apache/hadoop/hbase/client/hbaseadmin.java:658:in `modifycolumn'         from org/apache/hadoop/hbase/client/hbaseadmin.java:636:in `modifycolumn'         from sun/reflect/nativemethodaccessorimpl.java:-2:in `invoke0'         from sun/reflect/nativemethodaccessorimpl.java:39:in `invoke'         from sun/reflect/delegatingmethodaccessorimpl.java:25:in `invoke'         from java/lang/reflect/method.java:597:in `invoke'         from org/jruby/javasupport/javamethod.java:250:in `invokewithexceptionhandling'         from org/jruby/javasupport/javamethod.java:219:in `invoke'         from org/jruby/javasupport/javaclass.java:416:in `execute'         from org/jruby/internal/runtime/methods/simplecallbackmethod.java:67:in `call'         from org/jruby/internal/runtime/methods/dynamicmethod.java:94:in `call' ... 118 levels...         from ruby.hbase_minus_671438.bin.hirbinvokermethod__23$ruby$startopt:-1:in `call'         from org/jruby/internal/runtime/methods/dynamicmethod.java:74:in `call'         from org/jruby/internal/runtime/methods/compiledmethod.java:48:in `call'         from org/jruby/runtime/callsite.java:123:in `cacheandcall'         from org/jruby/runtime/callsite.java:298:in `call'         from ruby/hbase_minus_671438/bin//hbase/bin/hirb.rb:348:in `__file__'         from ruby/hbase_minus_671438/bin//hbase/bin/hirb.rb:-1:in `__file__'         from ruby/hbase_minus_671438/bin//hbase/bin/hirb.rb:-1:in `load'         from org/jruby/ruby.java:512:in `runscript'         from org/jruby/ruby.java:432:in `runnormally'         from org/jruby/ruby.java:312:in `runfrommain'         from org/jruby/main.java:144:in `run'         from org/jruby/main.java:89:in `run'         from org/jruby/main.java:80:in `main'         from /hbase/bin/hirb.rb:229:in `alter'         from (hbase):44:in `binding'hbase(main):044:0> ",
        "label": 314
    },
    {
        "text": "update to bootstrap  there was a major revision on bootstrap css we should take the upgrade as it make responsive layouts much easier in the future. ",
        "label": 154
    },
    {
        "text": "fuzzyrowkeyfilter should not modify the filter pairs  current implementation of fuzzyrowkeyfilter modifies the provided filter pairs: https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/fuzzyrowfilter.java#l89-l90 filter should not change the filter pair. ",
        "label": 553
    },
    {
        "text": "speed up simultaneous reads of a block when block caching is turned off  with block caching, when one client starts reading a block and another one comes around asking for the same block, the second client waits for the first one to finish reading and returns the block from cache. this is achieved by locking on the block offset using idlock, a \"sparse lock\" primitive allowing to lock on arbitrary long numbers. however, in case there is no block caching, there is no reason to wait for other clients that are reading the same block. one challenge optimizing this that we don't necessary have accurate information about whether other hfile api clients interested in the block would cache it. setting priority as minor, as it is very unusual to turn off block caching. ",
        "label": 324
    },
    {
        "text": "deprecate o a h h rest in favor of stargate  ",
        "label": 314
    },
    {
        "text": "number of compacting kvs is not reset at the end of compaction  looking at master:60010/master-status#compactstas , i noticed that 'num. compacting kvs' column stays unchanged at non-zero value(s). in defaultcompactor#compact(), we have this at the beginning:     this.progress = new compactionprogress(fd.maxkeycount); but progress.totalcompactingkvs is not reset at the end of compact(). ",
        "label": 191
    },
    {
        "text": "mapreduce based tests broken on hadoop alpha  some fairly recent change in hadoop 2.0.0-alpha has broken our mapreduce test rigging. below is a representative error, can be easily reproduced with: mvn -plocaltests -psecurity \\   -dhadoop.profile=23 -dhadoop.version=2.0.0-snapshot \\   clean test \\   -dtest=org.apache.hadoop.hbase.mapreduce.testtablemapreduce and the result: -------------------------------------------------------  t e s t s ------------------------------------------------------- running org.apache.hadoop.hbase.mapreduce.testtablemapreduce tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 54.292 sec <<< failure! ------------------------------------------------------------------------------- test set: org.apache.hadoop.hbase.mapreduce.testtablemapreduce ------------------------------------------------------------------------------- tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 54.292 sec <<< failure! testmultiregiontable(org.apache.hadoop.hbase.mapreduce.testtablemapreduce)  time elapsed: 21.935 sec  <<< error! java.lang.reflect.undeclaredthrowableexception at org.apache.hadoop.yarn.exceptions.impl.pb.yarnremoteexceptionpbimpl.unwrapandthrowexception(yarnremoteexceptionpbimpl.java:135) at org.apache.hadoop.yarn.api.impl.pb.client.clientrmprotocolpbclientimpl.getnewapplication(clientrmprotocolpbclientimpl.java:134) at org.apache.hadoop.mapred.resourcemgrdelegate.getnewjobid(resourcemgrdelegate.java:183) at org.apache.hadoop.mapred.yarnrunner.getnewjobid(yarnrunner.java:216) at org.apache.hadoop.mapreduce.jobsubmitter.submitjobinternal(jobsubmitter.java:339) at org.apache.hadoop.mapreduce.job$11.run(job.java:1226) at org.apache.hadoop.mapreduce.job$11.run(job.java:1223) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:416) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1232) at org.apache.hadoop.mapreduce.job.submit(job.java:1223) at org.apache.hadoop.mapreduce.job.waitforcompletion(job.java:1244) at org.apache.hadoop.hbase.mapreduce.testtablemapreduce.runtestontable(testtablemapreduce.java:151) at org.apache.hadoop.hbase.mapreduce.testtablemapreduce.testmultiregiontable(testtablemapreduce.java:129) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:616) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) at org.junit.rules.testwatcher$1.evaluate(testwatcher.java:47) at org.junit.rules.runrules.evaluate(runrules.java:18) at org.junit.runners.parentrunner.runleaf(parentrunner.java:263) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:68) at org.junit.runners.blockjunit4classrunner.runchild(blockjunit4classrunner.java:47) at org.junit.runners.parentrunner$3.run(parentrunner.java:231) at org.junit.runners.parentrunner$1.schedule(parentrunner.java:60) at org.junit.runners.parentrunner.runchildren(parentrunner.java:229) at org.junit.runners.parentrunner.access$000(parentrunner.java:50) at org.junit.runners.parentrunner$2.evaluate(parentrunner.java:222) at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:28) at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:30) at org.junit.runners.parentrunner.run(parentrunner.java:300) at org.apache.maven.surefire.junit4.junit4testset.execute(junit4testset.java:53) at org.apache.maven.surefire.junit4.junit4provider.executetestset(junit4provider.java:123) at org.apache.maven.surefire.junit4.junit4provider.invoke(junit4provider.java:104) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:616) at org.apache.maven.surefire.util.reflectionutils.invokemethodwitharray(reflectionutils.java:164) at org.apache.maven.surefire.booter.providerfactory$providerproxy.invoke(providerfactory.java:110) at org.apache.maven.surefire.booter.surefirestarter.invokeprovider(surefirestarter.java:175) at org.apache.maven.surefire.booter.surefirestarter.runsuitesinprocesswhenforked(surefirestarter.java:81) at org.apache.maven.surefire.booter.forkedbooter.main(forkedbooter.java:68) caused by: com.google.protobuf.serviceexception: java.net.connectexception: call from acer.localdomain/192.168.122.1 to 0.0.0.0:8032 failed on connection exception: java.net.connectexception: connection refused; for more details see:  http://wiki.apache.org/hadoop/connectionrefused at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:188) at $proxy89.getnewapplication(unknown source) at org.apache.hadoop.yarn.api.impl.pb.client.clientrmprotocolpbclientimpl.getnewapplication(clientrmprotocolpbclientimpl.java:132) ... 45 more caused by: java.net.connectexception: call from acer.localdomain/192.168.122.1 to 0.0.0.0:8032 failed on connection exception: java.net.connectexception: connection refused; for more details see:  http://wiki.apache.org/hadoop/connectionrefused at org.apache.hadoop.net.netutils.wrapexception(netutils.java:725) at org.apache.hadoop.ipc.client.call(client.java:1160) at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:185) ... 47 more caused by: java.net.connectexception: connection refused at sun.nio.ch.socketchannelimpl.checkconnect(native method) at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:592) at org.apache.hadoop.net.socketiowithtimeout.connect(socketiowithtimeout.java:206) at org.apache.hadoop.net.netutils.connect(netutils.java:522) at org.apache.hadoop.net.netutils.connect(netutils.java:487) at org.apache.hadoop.ipc.client$connection.setupconnection(client.java:469) at org.apache.hadoop.ipc.client$connection.setupiostreams(client.java:563) at org.apache.hadoop.ipc.client$connection.access$2000(client.java:212) at org.apache.hadoop.ipc.client.getconnection(client.java:1266) at org.apache.hadoop.ipc.client.call(client.java:1136) ... 48 more ",
        "label": 242
    },
    {
        "text": "unclaimed replication queues can go undetected  we have come across this situation multiple times where a zookeeper issues can cause nodefailoverworker to fail picking up replication queue for a dead region server silently. one example is when the znode size for a particular queue exceed jute.maxbuffer value. there can be other situations that may lead to this and just go undetected. we need to have a metric for number of unclaimed replication queues. this will help in mitigating the problem through alerting on the metric and identifying underlying issues. ",
        "label": 494
    },
    {
        "text": " hbase  refactor classes into client  master  and regionserver packages  i've been crawling around the hbase codebase for a little while now, and i think i have a proposal that would make it easer to find your way around in the codebase in general. i think that we should make three new packages below org.apache.hadoop.hbase, client, master, and regionserver. the client package would contain htable, client-side scanning stuff, hbaseadmin, the mapreduce-related stuff, the shell, rest and thrift. the master package would contain hmaster, maybe leases, any other classes that belong to master. the regionserver package would contain hregionserver, hregion, hstore and all its subclasses (hstorefile, etc). whatever is left over should be stuff that's pretty common to all the sub-packages, so we can either leave that in the hbase package, or push it down into a common subpackage. this would make it much easier for new contributors to decide where to look for stuff, as well as make it more obvious what the architectural divisions of the system are. to boot, it would allow us to reorganize our tests into similar subpackages, which has the advantage of allowing us to think about, for instance, client tests passing/failing as a group, rather than scattered alphabetically throughout the entire suite. this idea would probably erase hadoop-2518, or at least change the goal to factor hstore down into o.a.h.h.regionserver.store. ",
        "label": 86
    },
    {
        "text": "integration tests shouldn't set the number or reties   setting the number of client reties should be a function of the environment, not of the test. ",
        "label": 154
    },
    {
        "text": "singlecolumnvaluefilter with private fields and methods  why are most fields and methods declared private in singlecolumnvaluefilter? i'm trying to extend the functions of the singlecolumnvaluefilter to support complex column types such as json, array, csv, etc. but inheriting the singlecolumnvaluefilter doesn't give any benefits for i have to rewrite the codes. i think all private fields and methods could turn into protected mode. ",
        "label": 69
    },
    {
        "text": "fix  meta  migration after hbase  hbase-3171 doesn't manage the migration correctly, see metamigrationconvertingtopb and its unit test. ",
        "label": 543
    },
    {
        "text": "cleanup stoppable abortable closeable in the online snapshot cases   the regionserver managers implement abort, close, and stop \u2013 although the interfaces are similar and all their meanings are muddled. the conventions in hbase are gernally:   abort == passed into managers so they can trigger a suicide kill (for rs or hmaster)  stop == *managers for on the way to cleanup  cancel == operations that don't kill long running processes but bail out of the current attempt.  close == files or network resources. this patch brings the naming into line. ",
        "label": 248
    },
    {
        "text": "loadincrementalhfiles loops forever if the target table misses a cf  i have some hfiles for two column families 'y','z', but i specified a target table that only has cf 'y'.  i see the following repeated forever.  ...  12/02/23 22:57:37 warn mapreduce.loadincrementalhfiles: attempt to bulk load region containing into table z with files [family:y path:hdfs://bunnypig:9000/bulk/z2/y/bd6f1c3cc8b443fc9e9e5fddcdaa3b09, family:z path:hdfs://bunnypig:9000/bulk/z2/z/38f12fdbb7de40e8bf0e6489ef34365d] failed. this is recoverable and they will be retried.  12/02/23 22:57:37 debug client.metascanner: scanning .meta. starting at row=z,,00000000000000 for max=2147483647 rows using org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7b7a4989  12/02/23 22:57:37 info mapreduce.loadincrementalhfiles: split occured while grouping hfiles, retry attempt 1596 with 2 files remaining to group or split  12/02/23 22:57:37 info mapreduce.loadincrementalhfiles: trying to load hfile=hdfs://bunnypig:9000/bulk/z2/y/bd6f1c3cc8b443fc9e9e5fddcdaa3b09 first=r last=r  12/02/23 22:57:37 info mapreduce.loadincrementalhfiles: trying to load hfile=hdfs://bunnypig:9000/bulk/z2/z/38f12fdbb7de40e8bf0e6489ef34365d first=r last=r  12/02/23 22:57:37 debug mapreduce.loadincrementalhfiles: going to connect to server region=z,,1330066309814.d5fa76a38c9565f614755e34eacf8316., hostname=localhost, port=60020 for row   ... ",
        "label": 504
    },
    {
        "text": "eof trying to read reconstruction log stops region deployment  regions are just being reallocated over and over again because log file is hosed: 2008-03-29 10:37:53,762 error org.apache.hadoop.hbase.hregionserver: error opening region pdc-docs,ep92114798nwa1,1205741702057 java.io.eofexception         at java.io.datainputstream.readfully(datainputstream.java:178)         at java.io.datainputstream.readfully(datainputstream.java:152)         at org.apache.hadoop.io.sequencefile$reader.init(sequencefile.java:1421)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1398)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1387)         at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1382)         at org.apache.hadoop.hbase.hstore.doreconstructionlog(hstore.java:839)         at org.apache.hadoop.hbase.hstore.<init>(hstore.java:773)         at org.apache.hadoop.hbase.hregion.<init>(hregion.java:389)         at org.apache.hadoop.hbase.hregionserver.openregion(hregionserver.java:1159)        at org.apache.hadoop.hbase.hregionserver$worker.run(hregionserver.java:1105)         at java.lang.thread.run(thread.java:595) ",
        "label": 314
    },
    {
        "text": "wrongly delete cells in some case which can not be deleted  first, i put a cell using put interface, but i don't specify timestamp. then i delete the the same row, specify a timestamp of 1l. unfortunately, the former cell is deleted. we should know this cell can not be deleted in this case. (using original client api) code like this;  public static void main(string[] args) throws exception { cluster cluster = new cluster(); cluster.add(\"10.28.171.38\", 8080); client client = new client(cluster); remotehtable table = new remotehtable(client, \"demotime\"); put put = new put(bytes.tobytes(\"row21\")); put.add(\"info\".getbytes(), \"name\".getbytes(), \"huanggang\".getbytes()); table.put(put); delete delete = new delete(bytes.tobytes(\"row21\")); delete.settimestamp(1l); table.delete(delete); } ",
        "label": 230
    },
    {
        "text": "display master server and regionserver start time on respective info servers   with operations like rolling restart or master failovers, it is difficult to tell if a server is the \"old\" instance or the \"new\" restarted instance. adding a start date stamp on the info web pages would be helpful for determining this. ",
        "label": 248
    },
    {
        "text": "provide utility method equivalent to 92's result getbytes getsize   currently user has to write code similar to the following for replacement of result.getbytes().getsize() : +            cell[] cellvalues = resultrow.rawcells(); + +            long size = 0l; +            if (null != cellvalues) { +              for (cell cellvalue : cellvalues) { +                size += keyvalueutil.ensurekeyvalue(cellvalue).heapsize(); +              }  +            } in clientscanner, we have:               for (cell kv : rs.rawcells()) {                 // todo make method in cell or cellutil                 remainingresultsize -= keyvalueutil.ensurekeyvalue(kv).heapsize();               } a utility method should be provided which computes summation of cell sizes in a result. ",
        "label": 191
    },
    {
        "text": "avoid passing null in admin methods  for some methods we must pass null if we want specific behaviors, for example, move region to a random server, split region without providing an explicit split point, etc. we should provide methods without some parameters so user do not need to pass null. so in the future users do not need to guess whether a method accept null parameters, the answer is always no. ",
        "label": 149
    },
    {
        "text": "fix the new testtransform breakage up on hudson  this new test has been failing up on hudson since it was introduce at #1606. i took a look. it looks reasonable but its failing in an odd way \u2013 can't find blocks in hdfs. i'm moving it aside for now till test gets some loving. breakage lasted till at least #1613. ",
        "label": 38
    },
    {
        "text": "row level atomicity  the flow during a hregionserver.put() seems to be the following. [for now, let's just consider single row put containing edits to multiple column families/columns. hregionserver.put() does a:  hregion.put();  syncwal() (the hdfs sync call). /* this is assuming we have hdfs-200 */ hregion.put() does a:  for each column family { hlog.append(all edits to the colum family); write all edits to memstore; } hlog.append() does a :  foreach edit in a single column family { dowrite() } dowrite() does a:  this.writer.append(). there seems to be two related issues here that could result in inconsistencies. issue #1: a put() does a bunch of hlog.append() calls. these in turn do a bunch of \"write\" calls on the underlying dfs stream. if we crash after having written out some append's to dfs, recovery will run and apply a partial transaction to memstore. issue #2: the updates to memstore should happen after the sync rather than before. otherwise, there is the danger that the write to dfs (sync) fails for some reason & we return an error to the client, but we have already taken edits to the memstore. so subsequent reads will serve uncommitted data. ",
        "label": 263
    },
    {
        "text": "multirowrangefilter should provide a method for creating a filter which is functionally equivalent to multiple prefix filters  hi, i think current formal way to make multiple prefix filters is to create a filterlist and add prefixfilter instances to the list: filterlist allfilters = new filterlist(filterlist.operator.must_pass_one); allfilters.addfilter(new prefixfilter(bytes.tobytes(\"123\"))); allfilters.addfilter(new prefixfilter(bytes.tobytes(\"456\"))); allfilters.addfilter(new prefixfilter(bytes.tobytes(\"678\"))); scan.setfilter(allfilters); (c.f., https://stackoverflow.com/questions/41074213/hbase-how-to-specify-multiple-prefix-filters-in-a-single-scan-operation ) however, in the case of creating a single prefix filter, hbase provides scan.setrowprefixfilter method.  this method creates a range filter by setting a start row and a stop row.  the value of a stop row is decided by calling calculatetheclosestnextrowkeyforprefix ( c.f., https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/client/scan.java#l574-l597 ) multirowrangefilter could leverage a list of start row and stop row pairs and calculatetheclosestnextrowkeyforprefix could compute the stop row value corresponding to given start row (i.e., a prefix). i think this kind of filter (a filter which is functionally equivalent to multiple prefix filters) should be creatable by multirowrangefilter and it's better than the current formal way. cheers, ",
        "label": 216
    },
    {
        "text": "record the stack trace for current thread in futureutils get  this is for debugging. as in async client, the retry will be done in the retry timer thread, so the exception we get from the completablefuture will have a stack trace starting from the root of the retry timer. if we just throw this exception out when calling future.get(by unwrapping the executionexception), the upper layer even can not know where is the exception thrown... this happens for me many times, so i propose that we always create a new exception in futureutils.get, so at least we can record the stack trace for the method calling futureutils.get... ",
        "label": 149
    },
    {
        "text": "tablemapreduceutil should not rely on org apache hadoop util jarfinder getjar  this is the problem: tablemapreduceutil#adddependencyjars relies on org.apache.hadoop.util.jarfinder if available to call getjar(). however getjar() uses file.createtempfile() to create a temporary file under hadoop.tmp.dir/target/test-dir. due hadoop-9737 the created jar and its content is not purged after the jvm is destroyed. since most configurations point hadoop.tmp.dir under /tmp the generated jar files get purged by tmpwatch or a similar tool, but boxes that have hadoop.tmp.dir pointing to a different location not monitored by tmpwatch will pile up a collection of jars causing all kind of issues. since jarfinder#getjar is not a public api from hadoop (see alejandro abdelnur comment on hadoop-9737) we shouldn't use that as part of tablemapreduceutil in order to avoid this kind of issues. ",
        "label": 163
    },
    {
        "text": "improve testshell coverage of grant and revoke comamnds  the testshell coverage of grant and revoke commands doesn't seem sufficient to catch a botch that prevented global grants. also cover the alternative grant syntax introduced in hbase-11001. ",
        "label": 423
    },
    {
        "text": "move compareop and comparators out of filter to client package  table.checkandput() and its cousins depend on compareop from the filter package. originally, comparaop and bytearraycomparable, and various \"comparators\" have been used in filters, so these are in the filter package. however, for checkandput(), etc we depend on the filter subpackage although these are not filter related operations. we can use some clean up.   boolean checkandput(byte[] row, byte[] family, byte[] qualifier,     comparefilter.compareop compareop, byte[] value, put put) throws ioexception; some ideas cleanup bytearraycomparable interface (see the todo at the class) maybe introduce a condition or a similar concept and do checkandput(condition condition, put put) and change filters to use that as well. introducing condition like thing will allow us to have an interface like: checkandmutate(list<condition> conditions, list<mutation> mutations). binarycomparator, etc are not \"comparators\", they are comparables. ",
        "label": 314
    },
    {
        "text": "void return types not handled correctly for coprocessorprotocol methods  if a coprocessorprotocol derived interface defines a method with a void return type, the method cannot be called using htable.coprocessorexec(). instead execresult will throw an ioexception on the client trying to do a class.forname() on \"void\". looking at execresult, it appears that the valuetype field (which causes the error) is no longer even used, so i'd suggest we just get rid of it. ",
        "label": 180
    },
    {
        "text": "run namespaceupgrade from hbase migration script  namespaceupgrade tool should be run from the migration script or hmaster should automatically detect and migrate the layout upon startup. ",
        "label": 314
    },
    {
        "text": "preput coprocessor hook causing substantial cpu usage  i was running an insert workload against trunk under oprofile and saw that a significant portion of cpu usage was going to calling the \"preput\" coprocessor hook inside dominibatchput, even though i don't have any coprocessors installed. i ran a million-row insert and collected cpu time spent in the rs after commenting out the preput hook, and found cpu usage reduced by 33%. ",
        "label": 453
    },
    {
        "text": "triage lack of branch nightlies  last run of either branch-1.2 job was june 22nd. probably an infra change in labels. while we're at it, either turn it back into a matrix job (using appy's \"short workspace name\" bit) or better yet convert to yetus test-patch's nightly mode in anticipation of the coming loss of jdk7. https://builds.apache.org/view/h-l/view/hbase/job/hbase-1.2-jdk7/ https://builds.apache.org/view/h-l/view/hbase/job/hbase-1.2-jdk8/ ",
        "label": 402
    },
    {
        "text": "investigate time taken to snapshot memstore  snapshotting memstores is normally quick. but, sometimes it seems to take long. this jira is to track the investigation and fix to improve the outliers. ",
        "label": 154
    },
    {
        "text": "add troubleshooting section for centos page allocation failure issue  tim robertson reports: hbase centos version 6.2 reports kernel: java: page allocation failure. order:4, mode:0x20. any ideas anyone? then: echo 360448 > /proc/sys/vm/min_free_kbytes appears to stop page allocation failure using hbase on centos 6.2 if this is the proper fix for this condition, we should document it. @tim, how did you arrive at 360448? ",
        "label": 330
    },
    {
        "text": "regionserver addresses are still not right in the new tables page  they are mostly right. i'm guessing its stale cache of regions in the client hosted by the ui. if the webserver ran a scan, it'd probably fix it all up but thats a bit messy. i tried using the address that is in the .meta. table directly but that doesn't work.... we don't seem to deploy table properly and ui complains \"no server address for row testtable,,1213074650399\". i'll attach my patch. ",
        "label": 241
    },
    {
        "text": "support rpc interface changes at runtime  now we are able to append methods to interfaces without breaking rpc compatibility with earlier releases. however there is no way that i am aware of to dynamically add entire new rpc interfaces. methods/parameters are fixed to the class used to instantiate the server at that time. coprocessors need this. they will extend functionality on regions in arbitrary ways. how to support that on the client side? a couple of options: 1. new rpc from scratch. 2. modify hbaseserver such that multiple interface objects can be used for reflection and objects can be added or removed at runtime. 3. have the coprocessor host instantiate new hbaseserver instances on ephemeral ports and publish the endpoints to clients via zookeeper. couple this with a small modification to hbaseserver to support elastic thread pools to minimize the number of threads that might be kept around in the jvm. 4. add a generic method to hregioninterface, an ioctl-like construction, which accepts a immutablebyteswritable key and an array of writable as parameters. my opinion is we should opt for #4 as it is the simplest and most expedient approach. i could also do #3 if consensus prefers. really we should do #1 but it's not clear who has the time for that at the moment. ",
        "label": 180
    },
    {
        "text": "timeout monitor thread should be started after atleast one region server registers   currently timeout monitor thread is started even before the region server has registered with the master.  in timeout monitor we depend on the region server to be online boolean allrssoffline = this.servermanager.getonlineserverslist().         isempty(); now when the master starts up it sees there are no online servers and hence sets   allrssoffline to true. setallregionserversoffline(allrssoffline); so this.allregionserversoffline is also true.  by this time an rs has come up,  now timeout comes up again (after 10secs) in the next cycle he sees allrssoffline as false.  hence else if (this.allregionserversoffline && !allrssoffline) {             // if some rss just came back online, we can start the             // the assignment right away             actontimeout(regionstate); this condition makes him to take action based on timeout.  because of this even if one region assignment of root is going on, this piece of code triggers another assignment and thus we get regionalreadyintransition exception. later we need to wait for 30 mins for assigning root itself. ",
        "label": 543
    },
    {
        "text": "fix flaky testsnapshotfrommaster  testasyncsnapshotwillnotblocksnapshothfilecleaner is flaky.  the assert may fail. asserttrue(master.getsnapshotmanager().istakinganysnapshot()); future.get(); // in branch-2.2, here is thread.sleep assertfalse(master.getsnapshotmanager().istakinganysnapshot()); see https://builds.apache.org/job/hbase-flaky-tests/job/master/5227/testreport/junit/org.apache.hadoop.hbase.master.cleaner/testsnapshotfrommaster/testasyncsnapshotwillnotblocksnapshothfilecleaner/   https://builds.apache.org/view/h-l/view/hbase/job/hbase-find-flaky-tests/job/branch-2.2/lastsuccessfulbuild/artifact/dashboard.html ",
        "label": 187
    },
    {
        "text": "modify the hadoop support matrix in the ref guide  ",
        "label": 149
    },
    {
        "text": "purge distributed log replay from codebase  configurations  text  mark the feature as unsupported  broken   kill it. it keeps coming up and over again. needs proper burial. ",
        "label": 48
    },
    {
        "text": "hbase shell's create table command ignores 'normalization enabled' attribute  i am trying to create a new table and set the normalization_enabled as true, but seems like the argument normalization_enabled is being ignored. and the attribute normalization_enabled is not displayed on doing a desc command on that table hbase(main):020:0> create 'test-table-4', 'cf', {normalization_enabled => 'true'} an argument ignored (unknown or overridden): normalization_enabled 0 row(s) in 4.2670 seconds => hbase::table - test-table-4 hbase(main):021:0> desc 'test-table-4' table test-table-4 is enabled                                                                                                                                                                                test-table-4                                                                                                                                                                                                 column families description                                                                                                                                                                                  {name => 'cf', bloomfilter => 'row', versions => '1', in_memory => 'false', keep_deleted_cells => 'false', data_block_encoding => 'none', ttl => 'forever', compression => 'none', min_versions => '0', bloc kcache => 'true', blocksize => '65536', replication_scope => '0'}                                                                                                                                            1 row(s) in 0.0430 seconds however, on doing an alter command on that table we can set the normalization_enabled attribute for that table hbase(main):022:0> alter 'test-table-4', {normalization_enabled => 'true'} unknown argument ignored: normalization_enabled updating all regions with the new schema... 1/1 regions updated. done. 0 row(s) in 2.3640 seconds hbase(main):023:0> desc 'test-table-4' table test-table-4 is enabled                                                                                                                                                                                test-table-4, {table_attributes => {normalization_enabled => 'true'}                                                                                                                                         column families description                                                                                                                                                                                  {name => 'cf', bloomfilter => 'row', versions => '1', in_memory => 'false', keep_deleted_cells => 'false', data_block_encoding => 'none', ttl => 'forever', compression => 'none', min_versions => '0', bloc kcache => 'true', blocksize => '65536', replication_scope => '0'}                                                                                                                                            1 row(s) in 0.0190 seconds i think it would be better to have a single step process to enable normalization while creating the table itself, rather than a two step process to alter the table later on to enable normalization ",
        "label": 230
    },
    {
        "text": "verifyreplication should use peer configuration in peer connection  verifyreplication uses the replication peer's configuration to construct the zookeeper quorum address for the peer connection. however, other configuration properties in the peer's configuration are dropped. it should merge all configuration properties from the replicationpeerconfig when creating the peer connection and obtaining a credentials for the peer cluster. ",
        "label": 180
    },
    {
        "text": "coprocessor exec result map is not thread safe  i develop a coprocessor program ,but found some different results in repeated tests.for example,normally,the result's size is 10.but sometimes it appears 9.  i read the htable.java code,found a treemap(thread-unsafe) be used in multithreading environment.it cause the bug happened ",
        "label": 261
    },
    {
        "text": "hregionserver underreports readrequestcounts by under certain conditions  in hregionserver.scan(), if  (a) the number of results returned, n, is greater than zero  (b) but less than the size of the batch (nbrows)  (c) and the size in bytes is smaller than the max size (maxscannerresultsize) then the readrequestcount will be reported as n - 1 rather than n. (this is because the for-loop counter i is used to update the readrequestcount, and if the scan runs out of rows before reaching max rows or size, the code `break`s out of the loop and i is not incremented for the final time.) to reproduce, create a test table and open its details page in the web ui. insert a single row, then note the current request count, c. scan the table, returning 1 row; the request count will still be c, whereas it should be c + 1. i have a patch against trunk i can submit. at splice machine we're running 0.94, & i'd be happy to submit a patch against that as well. ",
        "label": 351
    },
    {
        "text": "handle inconsistencies in hadoop libraries naming in hbase script  when using an hadoop tarball that has a library naming of \"hadoop-x.y.z-core\" as opposed to \"hadoop-core-x.y.z\" then the hbase script throws errors. $ bin/start-hbase.sh  ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: no such file or directory exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/hadoop/util/platformname caused by: java.lang.classnotfoundexception: org.apache.hadoop.util.platformname at java.net.urlclassloader$1.run(urlclassloader.java:202) at java.security.accesscontroller.doprivileged(native method) at java.net.urlclassloader.findclass(urlclassloader.java:190) at java.lang.classloader.loadclass(classloader.java:306) at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) at java.lang.classloader.loadclass(classloader.java:247) ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: no such file or directory exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/hadoop/util/platformname caused by: java.lang.classnotfoundexception: org.apache.hadoop.util.platformname at java.net.urlclassloader$1.run(urlclassloader.java:202) at java.security.accesscontroller.doprivileged(native method) at java.net.urlclassloader.findclass(urlclassloader.java:190) at java.lang.classloader.loadclass(classloader.java:306) at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) at java.lang.classloader.loadclass(classloader.java:247) localhost: starting zookeeper, logging to /projects/opensource/hbase-trunk-rw//logs/hbase-larsgeorge-zookeeper-de1-app-mbp-2.out localhost: /projects/opensource/hadoop-0.20.2-append localhost: ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: no such file or directory localhost: exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/hadoop/util/platformname localhost: caused by: java.lang.classnotfoundexception: org.apache.hadoop.util.platformname localhost:  at java.net.urlclassloader$1.run(urlclassloader.java:202) localhost:  at java.security.accesscontroller.doprivileged(native method) localhost:  at java.net.urlclassloader.findclass(urlclassloader.java:190) localhost:  at java.lang.classloader.loadclass(classloader.java:306) localhost:  at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) localhost:  at java.lang.classloader.loadclass(classloader.java:247) starting master, logging to /projects/opensource/hbase-trunk-rw/bin/../logs/hbase-larsgeorge-master-de1-app-mbp-2.out /projects/opensource/hadoop-0.20.2-append ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: no such file or directory exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/hadoop/util/platformname caused by: java.lang.classnotfoundexception: org.apache.hadoop.util.platformname at java.net.urlclassloader$1.run(urlclassloader.java:202) at java.security.accesscontroller.doprivileged(native method) at java.net.urlclassloader.findclass(urlclassloader.java:190) at java.lang.classloader.loadclass(classloader.java:306) at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) at java.lang.classloader.loadclass(classloader.java:247) localhost: starting regionserver, logging to /projects/opensource/hbase-trunk-rw//logs/hbase-larsgeorge-regionserver-de1-app-mbp-2.out localhost: /projects/opensource/hadoop-0.20.2-append localhost: ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: no such file or directory localhost: exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/hadoop/util/platformname localhost: caused by: java.lang.classnotfoundexception: org.apache.hadoop.util.platformname localhost:  at java.net.urlclassloader$1.run(urlclassloader.java:202) localhost:  at java.security.accesscontroller.doprivileged(native method) localhost:  at java.net.urlclassloader.findclass(urlclassloader.java:190) localhost:  at java.lang.classloader.loadclass(classloader.java:306) localhost:  at sun.misc.launcher$appclassloader.loadclass(launcher.java:301) localhost:  at java.lang.classloader.loadclass(classloader.java:247) the naming in this case: $ ll /projects/opensource/hadoop-0.20.2-append/ total 14960 drwxr-xr-x@  26 larsgeorge  staff      884 apr 13 09:09 . drwxr-xr-x  114 larsgeorge  staff     3876 sep 22 19:42 .. -rw-r--r--@   1 larsgeorge  staff   348624 apr 13 08:58 changes.txt -rw-r--r--@   1 larsgeorge  staff    13366 apr 13 08:58 license.txt -rw-r--r--@   1 larsgeorge  staff      101 apr 13 08:58 notice.txt -rw-r--r--@   1 larsgeorge  staff     1366 apr 13 08:58 readme.txt drwxr-xr-x@  17 larsgeorge  staff      578 apr 13 08:58 bin -rw-r--r--@   1 larsgeorge  staff    74035 apr 13 08:58 build.xml drwxr-xr-x@   4 larsgeorge  staff      136 apr 13 08:58 c++ drwxr-xr-x   18 larsgeorge  staff      612 aug  9 15:11 conf drwxr-xr-x@  15 larsgeorge  staff      510 apr 13 08:58 conf.original drwxr-xr-x@  13 larsgeorge  staff      442 apr 13 08:58 contrib drwxr-xr-x@  63 larsgeorge  staff     2142 apr 13 08:58 docs -rw-r--r--@   1 larsgeorge  staff     6839 apr 13 08:58 hadoop-0.20.2-ant.jar -rw-r--r--    1 larsgeorge  staff  2707920 apr 13 09:06 hadoop-0.20.2-core.jar -rw-r--r--@   1 larsgeorge  staff  2689741 apr 13 08:58 hadoop-0.20.2-core.jar.original -rw-r--r--@   1 larsgeorge  staff   142466 apr 13 08:58 hadoop-0.20.2-examples.jar -rw-r--r--@   1 larsgeorge  staff  1563859 apr 13 08:58 hadoop-0.20.2-test.jar -rw-r--r--@   1 larsgeorge  staff    69940 apr 13 08:58 hadoop-0.20.2-tools.jar drwxr-xr-x@   6 larsgeorge  staff      204 apr 13 08:58 ivy -rw-r--r--@   1 larsgeorge  staff     8852 apr 13 08:58 ivy.xml drwxr-xr-x@  30 larsgeorge  staff     1020 jul 13 10:20 lib drwxr-xr-x@   3 larsgeorge  staff      102 apr 13 08:58 librecordio drwxr-xr-x    3 larsgeorge  staff      102 may 16 09:56 logs drwxr-xr-x@  17 larsgeorge  staff      578 apr 13 08:58 src drwxr-xr-x@   8 larsgeorge  staff      272 apr 13 08:58 webapps ",
        "label": 285
    },
    {
        "text": " no hserverinfo found for  should be a warning message  the message from regionservertracker \"no hserverinfo found for...\" is easy to miss. it should not be info. from irc chat jdcryans johnp789: can you grep for \"no hserverinfo found for\" in that log? jdcryans wait i see it jdcryans ok there's your problem shrijeet_ yes it is there shrijeet_ jdcryans: it should be info, why? jdcryans it shouldn't be info, it's so easy to miss jdcryans it's not the first time we have to look super closely to figure this one out shrijeet_ yes , i will file a jira jdcryans in any case it's a mismatch in that machine's dns config shrijeet_ anyways johnp789 is waiting :) go on johnp789 haha! johnp789 yes...  ???  :-) jdcryans the master is expecting a rs called \"localhost.localdomain,53875,1328924863478\" 17:26 jdcryans but the rs calls itself \"localhost,53875,1328924863478\" ",
        "label": 275
    },
    {
        "text": "take trunk back to hadoop  revert the dependency on hadoop 0.21, back to hadoop 0.20 (we hardly knew ye) ",
        "label": 547
    },
    {
        "text": "make memory locking configuration of regioservers more flexible  the current implementation of the memory locking feature of regisoservers has a downside of not being flexible to configure for permanent use. sure there is a --mlock flag but that needs to be explicitly passed on every invocation and thus require extra steps to be configured for permanent use (iow, there's not a single env variable i can set to have a desired effect). the only other alternative \u2013 the explicit setting of hbase_regionserver_opts \u2013 has a downside of being pretty cryptic to the novice user and has a killer problem of not explicitly telling higher level scripts (like init.d or upstart ones) which user the initial hbase process should be executed as. i propose a very simple solution (which is essentially making --mlock setting into an env. variable): add a variable called hbase_regionserver_mlock that can be set in hbase-env.sh and has the following semantics: [default] not set: mlocking feature is disabled set but empty: mlocking feature is enabled and the target user is hbase set and not empty: mlocking feature is enabled and the target user is the value of the variable thoughts? ",
        "label": 309
    },
    {
        "text": "pluggable rpcscheduler  today, the rpc scheduling mechanism is pretty simple: it execute requests in isolated thread-pools based on their priority. in the current implementation, all normal get/put requests are using the same pool. we'd like to add some per-user or per-region level isolation, so that a misbehaved user/region will not saturate the thread-pool and cause dos to others easily. the idea is similar to fairscheduler in mr. the current scheduling code is not standalone and is mixed with others (connection#processrequest). the issue is the first step to extract it to an interface, so that people are free to write and test their own implementations. this patch doesn't make it completely pluggable yet, as some parameters are pass from constructor. this is because hmaster and hregionserver both use rpcserver and they have different thread-pool size config. let me know if you have a solution to this. ",
        "label": 92
    },
    {
        "text": "write out multiple files when compaction  add datetieredcompactor and datetieredmultifilewriter to support writing out multiple files based on timestamp windows when doing date tiered compactions.  abstract abstractmultioutputcompactor and abstractmultifilewriter which contain the general logic for multi output and can be used to implement new compaction policies that requires multi output in the future. ",
        "label": 149
    },
    {
        "text": "perf  parallelize puts  right now with large region count tables, the write buffer is not efficient. this is because we issue potentially n rpcs, where n is the # of regions in the table. when n gets large (lets say 1200+) things become sloowwwww. instead if we batch things up using a different rpc and use thread pools, we could see higher performance! this requires a rpc change... ",
        "label": 547
    },
    {
        "text": "support for wire compatible security functionality  ",
        "label": 309
    },
    {
        "text": "include simple call toshortstring  in sendcall exceptions  failure diagnosis isn't very straightforward with call stack traces like org.apache.hadoop.hbase.ipc.calltimeoutexception: call to c501d28b0dfa/172.17.0.2:45657 failed on local exception: org.apache.hadoop.hbase.ipc.calltimeoutexception: call id=508, waittime=60006, rpctimeout=60000 at org.apache.hadoop.hbase.ipc.ipcutil.wrapexception(ipcutil.java:204) at org.apache.hadoop.hbase.ipc.abstractrpcclient.oncallfinished(abstractrpcclient.java:392) at org.apache.hadoop.hbase.ipc.abstractrpcclient.access$100(abstractrpcclient.java:97) at org.apache.hadoop.hbase.ipc.abstractrpcclient$3.run(abstractrpcclient.java:423) at org.apache.hadoop.hbase.ipc.abstractrpcclient$3.run(abstractrpcclient.java:419) at org.apache.hadoop.hbase.ipc.call.settimeout(call.java:96) at org.apache.hadoop.hbase.ipc.rpcconnection$1.run(rpcconnection.java:199) at org.apache.hbase.thirdparty.io.netty.util.hashedwheeltimer$hashedwheeltimeout.expire(hashedwheeltimer.java:680) at org.apache.hbase.thirdparty.io.netty.util.hashedwheeltimer$hashedwheelbucket.expiretimeouts(hashedwheeltimer.java:755) at org.apache.hbase.thirdparty.io.netty.util.hashedwheeltimer$worker.run(hashedwheeltimer.java:483) at java.lang.thread.run(thread.java:748) caused by: org.apache.hadoop.hbase.ipc.calltimeoutexception: call id=508, waittime=60006, rpctimeout=60000 at org.apache.hadoop.hbase.ipc.rpcconnection$1.run(rpcconnection.java:200) ... 4 more probably the \"affectsversions\" goes back farther than this. see if we can provide more calling context, even stack trace from the call origin, in these exceptions. ",
        "label": 339
    },
    {
        "text": "improve tablesnapshotinputformat to allow more multiple mappers per region  tablesnapshotinputformat runs one map task per region in the table snapshot. this places unnecessary restriction that the region layout of the original table needs to take the processing resources available to mr job into consideration. allowing to run multiple mappers per region (assuming reasonably even key distribution) would be useful. ",
        "label": 556
    },
    {
        "text": "any htabledescriptor we give out should be immutable  from enis soztutar in https://issues.apache.org/jira/browse/hbase-15505:  ps should unmodifyablehtabledescriptor be renamed to unmodifiablehtabledescriptor? it should be named immutablehtabledescriptor to be consistent with collections naming. let's do this as a subtask of the parent jira, not here. thinking about it though, why would we return an immutable htd in htable.gettabledescriptor() versus a mutable htd in admin.gettabledescriptor(). it does not make sense. should we just get rid of the immutable ones?  we also have unmodifyablehregioninfo which is not used at the moment it seems. ",
        "label": 98
    },
    {
        "text": "interrupt of a region open comes across as a successful open  meta was offline when below happened: 2010-12-21 19:45:23,023 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:60020-0x12d0a53c540000e attempting to transition node 337038b50e467fbd6b031f278bbd9c22 from rs_zk_region_opening to rs_zk_region_opening 2010-12-21 19:45:23,046 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:60020-0x12d0a53c540000e successfully transitioned node 337038b50e467fbd6b031f278bbd9c22 from rs_zk_region_opening to rs_zk_region_opening 2010-12-21 19:45:26,379 debug org.apache.hadoop.hbase.regionserver.handler.openregionhandler: interrupting thread thread[postopendeploytasks:337038b50e467fbd6b031f278bbd9c22,5,main] 2010-12-21 19:45:26,379 debug org.apache.hadoop.hbase.zookeeper.zkassign: regionserver:60020-0x12d0a53c540000e attempting to transition node 337038b50e467fbd6b031f278bbd9c22 from rs_zk_region_opening to rs_zk_region_opened 2010-12-21 19:45:26,381 warn org.apache.hadoop.hbase.regionserver.handler.openregionhandler: exception running postopendeploytasks; region=337038b50e467fbd6b031f278bbd9c22 org.apache.hadoop.hbase.notallmetaregionsonlineexception: interrupted     at org.apache.hadoop.hbase.catalog.catalogtracker.waitformetaserverconnectiondefault(catalogtracker.java:364)     at org.apache.hadoop.hbase.catalog.metaeditor.updateregionlocation(metaeditor.java:146)     at org.apache.hadoop.hbase.regionserver.hregionserver.postopendeploytasks(hregionserver.java:1331)     at org.apache.hadoop.hbase.regionserver.handler.openregionhandler$postopendeploytasksthread.run(openregionhandler.java:195) ... so, we timed out trying to open the region but rather than close the region because edit failed, we missed seeing the interruptedexception. here is suggested fix: diff --git a/src/main/java/org/apache/hadoop/hbase/catalog/metareader.java b/src/main/java/org/apache/hadoop/hbase/catalog/metareader.java index 7bf680d..2b0078c 100644 --- a/src/main/java/org/apache/hadoop/hbase/catalog/metareader.java +++ b/src/main/java/org/apache/hadoop/hbase/catalog/metareader.java @@ -339,7 +339,7 @@ public class metareader {      get.addfamily(hconstants.catalog_family);      byte [] meta = getcatalogregionnameforregion(regionname);      result r = catalogtracker.waitformetaserverconnectiondefault().get(meta, get); -    if(r == null || r.isempty()) { +    if (r == null || r.isempty()) {        return null;      }      return metarowtoregionpair(r); let me try it. w/o this, what we see is hbck showing that region is on server x but in .meta. it shows as being on y (its pre-balance server) ",
        "label": 314
    },
    {
        "text": "compactions at  re start on a large table can overwhelm dfs  given a large table, > 1000 regions for example, if a cluster restart is necessary, the compactions undertaken by the regionservers when the master makes initial region assignments can overwhelm dfs, leading to file errors and data loss. this condition is exacerbated if write load was heavy before restart and so many regions want to split as soon as they are opened. ",
        "label": 38
    },
    {
        "text": "allow override of scan cache value for rowcounter  doing a row count for a large table via mapreduce may take long time.  trying to set the default cache size but there is no knob to tune it. see here for more details, http://search-hadoop.com/m/eces6237aix&subj=re+speeding+up+rowcount ",
        "label": 441
    },
    {
        "text": "add support for protocol buffer based rpc  this will help hbase to achieve wire compatibility across versions. the idea (to start with) is to leverage the recent work that has gone in in the hadoop core in this area. ",
        "label": 139
    },
    {
        "text": "remove add modify deletecolumnfamilyprocedure in favor of using modifytableprocedure  the shell changed from using separate add/modify/delete column calls to funneling everything through modify table for performance reasons. we know that using modify table works for everything. let's drop the old code for add/modify/delete column so that we have a lower maintenance burden and fewer code paths to reason about. -------- was: shell 'alter' command no longer distinguishes column add/modify/delete after hbase-15641 all 'alter' commands go through a single modifytable call at the end, so we no longer can easily distinguish add, modify, and delete column events. this potentially affects coprocessors that needed the update notifications for new or removed columns. let's let the shell still make separate behaviour calls like it did before without undoing the batching that seems pretty useful. ",
        "label": 320
    },
    {
        "text": "the timeout handler in assignmentmanager does an rpc while holding lock on rit  a big no no  j-d found this debugging a failure on dmitriy's cluster; we're rpc'ing under a synchronized(regionsintransition). fix. ",
        "label": 441
    },
    {
        "text": "improve zombie detector  be more discerning  currently, any surefire process with the hbase flag is a potential zombie. our zombie check currently takes a reading and if it finds candidate zombies, it waits 30 seconds and then does another reading. if a concurrent build going on, in both cases the zombie detector will come up positive though the adjacent test run may be making progress; i.e. the cast of surefire processes may have changed between readings but our detector just sees presence of hbase surefire processes. here is example: suspicious java process found - waiting 30s to see if there are just slow to stop there appear to be 5 zombie tests, they should have been killed by surefire but survived 12823 surefirebooter852180186418035480.jar -enableassertions -dhbase.test -xmx2800m -xx:maxpermsize=256m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true 7653 surefirebooter8579074445899448699.jar -enableassertions -dhbase.test -xmx2800m -xx:maxpermsize=256m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true 12614 surefirebooter136529596936417090.jar -enableassertions -dhbase.test -xmx2800m -xx:maxpermsize=256m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true 7836 surefirebooter3217047564606450448.jar -enableassertions -dhbase.test -xmx2800m -xx:maxpermsize=256m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true 13566 surefirebooter2084039411151963494.jar -enableassertions -dhbase.test -xmx2800m -xx:maxpermsize=256m -djava.security.egd=file:/dev/./urandom -djava.net.preferipv4stack=true -djava.awt.headless=true ************ begin zombies jstack extract ************ end  zombies jstack extract 5 is the number of forked processes we allow when doing medium and large tests.... so an adjacent build will always show as '5 zombies'. need to add discerning if list of processes changes between readings. can i also add a tag per build run that all forked processes pick up so i can look at the current builds progeny only? ",
        "label": 314
    },
    {
        "text": "handlers being blocked during reads  i'm having a lot of handlers (90 - 300 aprox) being blocked when reading rows. they are blocked during changedreaderobserver registration. lars hofhansl suggests to change the implementation of changedreaderobserver from copyonwritelist to concurrenthashmap. here is a stack trace: \"ipc server handler 99 on 60020\" daemon prio=10 tid=0x0000000041c84000 nid=0x2244 waiting on condition [0x00007ff51fefd000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00000000c5c13ae8> (a java.util.concurrent.locks.reentrantlock$nonfairsync)  at java.util.concurrent.locks.locksupport.park(locksupport.java:156)  at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(abstractqueuedsynchronizer.java:811)  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(abstractqueuedsynchronizer.java:842)  at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1178)  at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(reentrantlock.java:186)  at java.util.concurrent.locks.reentrantlock.lock(reentrantlock.java:262)  at java.util.concurrent.copyonwritearraylist.addifabsent(copyonwritearraylist.java:553)  at java.util.concurrent.copyonwritearrayset.add(copyonwritearrayset.java:221)  at org.apache.hadoop.hbase.regionserver.store.addchangedreaderobserver(store.java:1085)  at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:138)  at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:2077)  at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.<init>(hregion.java:3755)  at org.apache.hadoop.hbase.regionserver.hregion.instantiateregionscanner(hregion.java:1804)  at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1796)  at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1771)  at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:4776)  at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:4750)  at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:2152)  at org.apache.hadoop.hbase.regionserver.hregionserver.multi(hregionserver.java:3700)  at sun.reflect.generatedmethodaccessor26.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:320)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426) ",
        "label": 154
    },
    {
        "text": "support reverse scan  reversed scan means scan the rows backward.   and startrow bigger than stoprow in a reversed scan. for example, for the following rows: aaa/c1:q1/value1  aaa/c1:q2/value2  bbb/c1:q1/value1  bbb/c1:q2/value2  ccc/c1:q1/value1  ccc/c1:q2/value2  ddd/c1:q1/value1  ddd/c1:q2/value2  eee/c1:q1/value1  eee/c1:q2/value2 you could do a reversed scan from 'ddd' to 'bbb'(exclude) like this: scan scan = new scan();  scan.setstartrow('ddd');  scan.setstoprow('bbb');  scan.setreversed(true);  for(result result:htable.getscanner(scan)){  system.out.println(result);  } aslo you could do the reversed scan with shell like this:  hbase> scan 'table', {reversed => true,startrow=>'ddd', stoprow=>'bbb'} and the output is:  ddd/c1:q1/value1  ddd/c1:q2/value2  ccc/c1:q1/value1  ccc/c1:q2/value2 all the documentation i find about hbase says that if you want forward and reverse scans you should just build 2 tables and one be ascending and one descending. is there a fundamental reason that hbase only supports forward scan? it seems like a lot of extra space overhead and coding overhead (to keep them in sync) to support 2 tables. i am assuming this has been discussed before, but i can't find the discussions anywhere about it or why it would be infeasible. ",
        "label": 107
    },
    {
        "text": "cme iterating return from servermanager getmarkedtoclose  below is a note from list from david alves i'm seeing some cmes in the logs they occurred while i still had bad dfs.replication settings between hadoop and hbase but still thought you should know. trace: 2008-04-21 13:20:46,443 warn org.apache.hadoop.hbase.regionserver.hregionserver: processing message (retry: 0) java.io.ioexception: java.io.ioexception: java.util.concurrentmodificationexception at java.util.hashmap$hashiterator.nextentry(hashmap.java:793) at java.util.hashmap$valueiterator.next(hashmap.java:822) at org.apache.hadoop.hbase.master.servermanager.processmsgs(servermanager.java:350) at org.apache.hadoop.hbase.master.servermanager.processregionserverallswell(servermanager.java:299) at org.apache.hadoop.hbase.master.servermanager.regionserverreport(servermanager.java:217) at org.apache.hadoop.hbase.master.hmaster.regionserverreport(hmaster.java:560) at sun.reflect.generatedmethodaccessor2.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413) at org.apache.hadoop.ipc.server$handler.run(server.java:896) at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27) at java.lang.reflect.constructor.newinstance(constructor.java:513) at org.apache.hadoop.hbase.remoteexceptionhandler.decoderemoteexception(remoteexceptionhandler.java:82) at org.apache.hadoop.hbase.remoteexceptionhandler.checkioexception(remoteexceptionhandler.java:48) at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:388) at java.lang.thread.run(thread.java:619) looking at it, the fix would seem to be cloning the map we give out but the clone needs to be done inside a synchronize of the map being cloned so some more extensive synchronizing needs to be added to the servermanager class. looking too, it seems like regionstoclose is not protected everywhere (maybe i'm not reading it right). bryan, you'd probably be best handlng this issue. ",
        "label": 241
    },
    {
        "text": "limit the ipc queue size based on calls' payload size  currently we limit the number of calls in the ipc queue only on their count. it used to be really high and was dropped down recently to num_handlers * 10 (so 100 by default) because it was easy to oome yourself when huge calls were being queued. it's still possible to hit this problem if you use really big values and/or a lot of handlers, so the idea is that we should take into account the payload size. i can see 3 solutions: do the accounting outside of the queue itself for all calls coming in and out and when a call doesn't fit, throw a retryable exception. same accounting but instead block the call when it comes in until space is made available. add a new parameter for the maximum size (in bytes) of a call and then set the size the ipc queue (in terms of the number of items) so that it could only contain as many items as some predefined maximum size (in bytes) for the whole queue. ",
        "label": 229
    },
    {
        "text": "reversed scan could hang  when i play with itbll (from trunk tip), sometimes, meta scan hangs when the cluster is rolling restarted. when this happens, the master takes about 1000% of cpu. it looks like there is an infinite loop somewhere. the logs show nothing interesting except some meta scanner rpc calls timed out. jstask shows the 10 high qos rpc handlers are busy with meta scanning. however, if i run it again without hbase-10018, things are fine. i suspect there is something to do with the small/reverse scan. by the way, i see this problem even with log replay off and hfile version = 2. ",
        "label": 242
    },
    {
        "text": "backport hbase 'provide mutability to compoundconfiguration' to  in the email thread: http://search-hadoop.com/m/dcqod1uy32h  yonghu encountered the following exception when he tried to retrieve htableinterface: error: org.apache.hadoop.hbase.client.retriesexhaustedwithdetailsexception: failed 1 action: org.apache.hadoop.hbase.donotretryioexception: coprocessor: 'org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$regionenvironment@9a99eb' threw: 'java.lang.unsupportedoperationexception: immutable configuration' and has been removedfrom the active coprocessor set.     at org.apache.hadoop.hbase.coprocessor.coprocessorhost.handlecoprocessorthrowable(coprocessorhost.java:740)     at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.preput(regioncoprocessorhost.java:810)     at org.apache.hadoop.hbase.regionserver.hregion.dopremutationhook(hregion.java:2196)     at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2172)     at org.apache.hadoop.hbase.regionserver.hregionserver.multi(hregionserver.java:3811)     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:320)     at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426) caused by: java.lang.unsupportedoperationexception: immutable configuration     at org.apache.hadoop.hbase.regionserver.compoundconfiguration.set(compoundconfiguration.java:484)     at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.ensurezookeepertrackers(hconnectionmanager.java:721)     at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:986)     at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:961)     at org.apache.hadoop.hbase.client.htable.finishsetup(htable.java:251)     at org.apache.hadoop.hbase.client.htable.<init>(htable.java:243)     at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gettable(hconnectionmanager.java:671)     at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gettable(hconnectionmanager.java:658)     at cdctrigger.triggerformodification.preput(triggerformodification.java:61)     at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.preput(regioncoprocessorhost.java:808)     ... 9 more : 1 time, servers with issues: hans-laptop:60020 compoundconfiguration is mutable in 0.96 and beyond.  this should be backported to 0.94 ",
        "label": 441
    },
    {
        "text": "stale connection could stay for a while  in rpcclient, we cache the connection to each region server. when the connection goes bad, it stays in the cache till it's removed. before it's removed, new calls will try to use it and just fail. the connection is a thread. it could be stuck in trying to receive some response. before this receiving thread times out, it won't remove itself from the cache. it will be better to interrupt the receiving thread and let it clean up sooner. ",
        "label": 242
    },
    {
        "text": "more options on the row query in rest interface  the prior implementation only supports the exact column name(col_family:label), but we need more such as get a col family, query the version. ",
        "label": 549
    },
    {
        "text": "fix test failures for avro server on hudson  hudson is failing the tests from hbase-2400: http://hudson.hbase.org/job/hbase-trunk/119/#showfailureslink. these tests pass locally, so it's not clear why they are failing with hudson. ",
        "label": 231
    },
    {
        "text": "data loss because lastseqwritten can miss memstore edits  (i don't have a test case to prove this yet but i have run it by dhruba and kannan internally and wanted to put this up for some feedback.) in this discussion let us assume that the region has only one column family. that way i can use region/memstore interchangeably. after a memstore flush it is possible for lastseqwritten to have a log-sequence-id for a region that is not the earliest log-sequence-id for that region's memstore. hlog.append() does a putifabsent into lastsequencewritten. this is to ensure that we only keep track of the earliest log-sequence-number that is present in the memstore. every time the memstore is flushed we remove the region's entry in lastsequencewritten and wait for the next append to populate this entry again. this is where the problem happens. step 1:  flusher.prepare() snapshots the memstore under hregion.updateslock.writelock(). step 2 :  as soon as the updateslock.writelock() is released new entries will be added into the memstore. step 3 :  wal.completecacheflush() is called. this method removes the region's entry from lastseqwritten. step 4:  the next append will create a new entry for the region in lastseqwritten(). but this will be the log seq id of the current append. all the edits that were added in step 2 are missing. == as a temporary measure, instead of removing the region's entry in step 3 i will replace it with the log-seq-id of the region-flush-event. ",
        "label": 544
    },
    {
        "text": " hbase operator tools  readme edits in prep for release  did an edit of the readmes to get us ready for 1.0.0 release. ",
        "label": 314
    },
    {
        "text": "move active master and backup master znodes to use pbs  ",
        "label": 314
    },
    {
        "text": "taking down root meta regionserver can result in cluster becoming in operational  take down a regionserver via controlled or uncontrolled shutdown, the master doesn't properly reassign the root/meta regions. ",
        "label": 547
    },
    {
        "text": "scan returns rows beyond the endrow when the column is specified  we ran into an issue where the scan resulted in rows beyond the endrow. are we doing something incorrectly here? the test case is given below. when the scan.addcolumn(...) is commented, the rows has { \"row333\" } but having the scan.addcolumn(...) in the scan gives rows { \"row555\" } . ",
        "label": 547
    },
    {
        "text": "hbaseadmin createtable cannot handle creating three regions  createtable(htabledescriptor desc, byte [] startkey, byte [] endkey, int numregions) (line #370) dictates that you must specify a minimum of three regions, however is not able to handle being fed a value three. this is a result of line #379 where it attempts to create the key splits, and calls bytes.split with a value of 0 for the third parameter. createtable should instead just create a byte[][] with the startkey and endkey in this scenario. ",
        "label": 230
    },
    {
        "text": " hbase  stuck regionserver   looking in logs, a regionserver went down because it could not contact the master after 60 seconds. watching logging, the hrs is repeatedly checking all 150 loaded regions over and over again w/ a pause of about 5 seconds between runs... then there is a suspicious 60+ second gap with no logging as though the regionserver had hung up on something: 2007-12-03 13:14:54,178 debug hbase.hregionserver - flushing region postlog,img151/60/plakatlepperduzy1hh7.jpg,1196614355635 2007-12-03 13:14:54,178 debug hbase.hregion - not flushing cache for region postlog,img151/60/plakatlepperduzy1hh7.jpg,1196614355635: snapshotmemcaches() determined that there was nothing to do 2007-12-03 13:14:54,205 debug hbase.hregionserver - flushing region postlog,img247/230/seanpaul4li.jpg,1196615889965 2007-12-03 13:14:54,205 debug hbase.hregion - not flushing cache for region postlog,img247/230/seanpaul4li.jpg,1196615889965: snapshotmemcaches() determined that there was nothing to do 2007-12-03 13:16:04,305 fatal hbase.hregionserver - unable to report to master for 67467 milliseconds - aborting server 2007-12-03 13:16:04,455 info  hbase.leases - regionserver/0:0:0:0:0:0:0:0:60020 closing leases 2007-12-03 13:16:04,455 info  hbase.leases$leasemonitor - regionserver/0:0:0:0:0:0:0:0:60020.leasechecker exiting master seems to be running fine scanning its ~700 regions. then you see this in log, before the hrs shuts itself down. 2007-12-03 13:14:31,416 info  hbase.leases - hmaster.leasechecker lease expired 153260899/1532608992007-12-03 13:14:31,417 info  hbase.hmaster - xx.xx.xx.102:60020 lease expired ... and we go on to process shutdown. ",
        "label": 314
    },
    {
        "text": "print procedure wal content  let's have a printer to print the content of procedure wal. ",
        "label": 234
    },
    {
        "text": "improve error message when master fail over happens and zk unassigned node contains stale znode s   when master fail-over happens, if we have number of rits under /hbase/unassigned and if we have stale znode(s) (encoded region names) under /hbase/unassigned, we are getting 2011-12-30 10:27:35,623 info org.apache.hadoop.hbase.master.hmaster: master startup proceeding: master failover  2011-12-30 10:27:36,002 info org.apache.hadoop.hbase.master.assignmentmanager: failed-over master needs to process 1717 regions in transition  2011-12-30 10:27:36,004 fatal org.apache.hadoop.hbase.master.hmaster: unhandled exception. starting shutdown.  java.lang.arrayindexoutofboundsexception: -256  at org.apache.hadoop.hbase.executor.regiontransitiondata.readfields(regiontransitiondata.java:148)  at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:105)  at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:75)  at org.apache.hadoop.hbase.executor.regiontransitiondata.frombytes(regiontransitiondata.java:198)  at org.apache.hadoop.hbase.zookeeper.zkassign.getdata(zkassign.java:743)  at org.apache.hadoop.hbase.master.assignmentmanager.processregionintransition(assignmentmanager.java:262)  at org.apache.hadoop.hbase.master.assignmentmanager.processfailover(assignmentmanager.java:223)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:401)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:283) and there is no clue on how to clean-up the stale znode(s) from unassigned using zkcli.sh (del /hbase/unassigned/<bad region name>). it would be good if we include the bad region name in ioexception from regiontransitiondata.readfields(). @override   public void readfields(datainput in) throws ioexception {     // the event type byte     eventtype = eventtype.values()[in.readshort()];     // the timestamp     stamp = in.readlong();     // the encoded name of the region being transitioned     regionname = bytes.readbytearray(in);     // remaining fields are optional so prefixed with boolean     // the name of the regionserver sending the data     if (in.readboolean()) {       byte [] versionedbytes = bytes.readbytearray(in);       this.origin = servername.parseversionedservername(versionedbytes);     }     if (in.readboolean()) {       this.payload = bytes.readbytearray(in);     }   } if the code execution has survived until regionname then we can include the regionname in ioexception with error message to clean-up the stale znode(s) under /hbase/unassigned. ",
        "label": 333
    },
    {
        "text": "rewrite testscannerheartbeatmessages  ",
        "label": 149
    },
    {
        "text": "initial web ui for region memstore storefiles details  click on a region in ui and get a listing of hfiles in hdfs and summary of memstore content; click on an hfile and see its content ",
        "label": 323
    },
    {
        "text": "integrationtestimporttsv testrunfromoutputcommitter misses credential initialization  integrationtestimporttsv#testrunfromoutputcommitter a parent job that ships the hbase dependencies.  however, call to tablemapreduceutil.initcredentials(job) is missing - making this test fail on a secure cluster. ",
        "label": 462
    },
    {
        "text": "coprocessors are unable to load if regionserver is launched using a different classloader than system default  if a region server is launched in a context such that its classloader is different from the system class loader, then the class object used to represent the coprocessor interface of the coprocessor will be different than the coprocessor class object that is used by regioncoprocessorhost.loadsystemcoprocessors() . there's a few options that come to mind to fix this problem: 1. remove the logic where loadsystemcoprocessors changes the context's classloader back to the system default classloader. 2. remove the cast to coprocessor in coprocessorhost.load() and invoke methods via reflection. 3. set the class loader back to the system default before launching any daemon threads. ",
        "label": 38
    },
    {
        "text": "purge pb from bulkloadobserver  noticed by apekshit sharma we expose pbs in this cp. pass pojos. this is like anoop sam john and josh elser pb purging that is going on contemporaneously. ",
        "label": 314
    },
    {
        "text": "findbugs issues   performance warnings as suggested by findbugs  integer.valueof favored instead of new integer() map.entryset() favored instead of map.keyset() ",
        "label": 266
    },
    {
        "text": "scanner doesnt reset when a snapshot is created  could miss new updates into the 'kvset'  active part   when a scanner is created, it creates 2 memstorescanners on the kvset and the snapshot (internal names of memstore)... if the snapshot is originally empty, it only creates the 1, for kvset. when the snapshot is created, the outstanding scanners now have a pointer to the tree that is now the snapshot, but no pointer to the kvset. when the flush completes, the scanner will reset the memstore scanners and 'see' the new values again. if there is a large delay between snapshot and finalization of the flush, there can be a large period of time a scanner doesnt see 'new' values that are being inserted. the canonical 'bad' case where this can do things is the meta scanner, and we end up with double assignment. the snapshot is really lightweight, it only takes out a small lock in memstore, so im not sure there is an easy mechanism to hook to without building out a bit more code or restructuring the memstore scanner. ",
        "label": 314
    },
    {
        "text": "warn org apache hadoop hbase regionserver store  not in set  double remove  org apache hadoop hbase regionserver storescanner 76607d3d  we see this ugly message in running hbase. its a little disorientating. i added stack traces around the call to close. here are the two i see (the warn message is the bad one... the info is the good case). looks like compaction is the one that triggers the warn. 2010-12-01 18:20:28,307 warn org.apache.hadoop.hbase.regionserver.store: not in set (double-remove?) org.apache.hadoop.hbase.regionserver.storescanner@68faedc7 java.io.ioexception: why     at org.apache.hadoop.hbase.regionserver.store.deletechangedreaderobserver(store.java:583)     at org.apache.hadoop.hbase.regionserver.storescanner.close(storescanner.java:204)     at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:242)     at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:326)     at org.apache.hadoop.hbase.regionserver.store.compact(store.java:939)     at org.apache.hadoop.hbase.regionserver.store.compact(store.java:748)     at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:753)     at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:698)     at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:81) 2010-12-01 18:21:18,103 info org.apache.hadoop.hbase.regionserver.store: close org.apache.hadoop.hbase.regionserver.storescanner@3dca256e java.io.ioexception: why     at org.apache.hadoop.hbase.regionserver.store.deletechangedreaderobserver(store.java:592)     at org.apache.hadoop.hbase.regionserver.storescanner.close(storescanner.java:204)     at org.apache.hadoop.hbase.regionserver.keyvalueheap.close(keyvalueheap.java:192)     at org.apache.hadoop.hbase.regionserver.hregion$regionscanner.close(hregion.java:2361)     at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2945)     at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2844)     at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1566)     at sun.reflect.generatedmethodaccessor5.invoke(unknown source)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570)     at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1036) ",
        "label": 314
    },
    {
        "text": "document that http webui's should redirect to https when enabled  when configured to listen on https, we should redirect non-secure requests to the appropriate port/protocol. currently we respond with a 200 and no data, which is perplexing. ",
        "label": 330
    },
    {
        "text": "unknown metrics type  org apache hadoop hbase metrics histogram metricshistogram  when trying to run a unit test that just starts up and shutdown the server the following errors occur in system.out 01:10:59,874 error metricsutil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.metricshistogram  01:10:59,874 error metricsutil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.metricshistogram  01:10:59,875 error metricsutil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.metricshistogram  01:10:59,875 error metricsutil:116 - unknown metrics type: org.apache.hadoop.hbase.metrics.histogram.metricshistogram ",
        "label": 154
    },
    {
        "text": "region assigned to two regionservers after split  after reassignment of region split, scanners (mapred tasks) fail: org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server 10.30.94.49:60020 for region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257, row 'c4e518dc579ca98bf97ea13d1f7c80fc', but failed after 10 attempts.  exceptions:  java.io.ioexception: java.io.ioexception: hstorescanner failed construction  at org.apache.hadoop.hbase.regionserver.storefilescanner.(storefilescanner.java:70)  at org.apache.hadoop.hbase.regionserver.hstorescanner.(hstorescanner.java:84)  at org.apache.hadoop.hbase.regionserver.hstore.getscanner(hstore.java:2113)  at org.apache.hadoop.hbase.regionserver.hregion$hscanner.(hregion.java:1847)  at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1158)  at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:1521)  at sun.reflect.generatedmethodaccessor18.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:634)  at org.apache.hadoop.ipc.server$handler.run(server.java:888)  caused by: java.io.filenotfoundexception: file does not exist: hdfs://sjdc-atr-dc-1.atr.trendmicro.com:50000/data/hbase/content/1137747785/info/mapfiles/3405780294391796198/data  at org.apache.hadoop.dfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:394)  at org.apache.hadoop.fs.filesystem.getlength(filesystem.java:695)  at org.apache.hadoop.io.sequencefile$reader.(sequencefile.java:1420)  at org.apache.hadoop.io.sequencefile$reader.(sequencefile.java:1415)  at org.apache.hadoop.io.mapfile$reader.createdatafilereader(mapfile.java:301)  at org.apache.hadoop.hbase.io.hbasemapfile$hbasereader.createdatafilereader(hbasemapfile.java:98)  at org.apache.hadoop.io.mapfile$reader.open(mapfile.java:283)  at org.apache.hadoop.hbase.io.hbasemapfile$hbasereader.(hbasemapfile.java:81)  at org.apache.hadoop.hbase.io.bloomfiltermapfile$reader.(bloomfiltermapfile.java:66)  at org.apache.hadoop.hbase.io.halfmapfilereader.(halfmapfilereader.java:86)  at org.apache.hadoop.hbase.regionserver.hstorefile.getreader(hstorefile.java:438)  at org.apache.hadoop.hbase.regionserver.storefilescanner.openreaders(storefilescanner.java:96)  at org.apache.hadoop.hbase.regionserver.storefilescanner.(storefilescanner.java:67)  ... 10 more according to the master the regions around this row are: content,c4580f7292ceb2f6c8d732d935302633,1228328680554 10.30.94.50:60020 693011073 c4580f7292ceb2f6c8d732d935302633 c4e518dc579ca98bf97ea13d1f7c80fc  content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257 10.30.94.49:60020 2031068422 c4e518dc579ca98bf97ea13d1f7c80fc c528c8864e67a3addcb50587586dfe38 on 10.30.94.49, just before the hstorescanner exceptions begin: 2008-12-04 16:18:30,538 info org.apache.hadoop.hbase.regionserver.hregionserver: msg_region_open: content,c528c8864e67a3addcb50587586dfe38,1228407504257  2008-12-04 16:18:30,539 info org.apache.hadoop.hbase.regionserver.hregionserver: msg_region_open: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  2008-12-04 16:18:30,539 info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: content,c528c8864e67a3addcb50587586dfe38,1228407504257  2008-12-04 16:18:30,633 info org.apache.hadoop.hbase.regionserver.hregion: region content,c528c8864e67a3addcb50587586dfe38,1228407504257/1813242175 available  2008-12-04 16:18:30,633 info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  2008-12-04 16:18:30,725 info org.apache.hadoop.hbase.regionserver.hregion: region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257/2031068422 available  2008-12-04 16:18:36,659 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region content,c528c8864e67a3addcb50587586dfe38,1228407504257  2008-12-04 16:18:36,673 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region content,c528c8864e67a3addcb50587586dfe38,1228407504257  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:863)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:708)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:666)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:87)  2008-12-04 16:18:36,689 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  2008-12-04 16:18:36,691 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:863)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:708)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:666)  at org.apache.hadoop.hbase.regionserver.compactsplitthread.run(compactsplitthread.java:87) this is the region history: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  thu, 4 dec 2008 11:18:33 open region opened on server : 10.30.94.49  thu, 4 dec 2008 11:18:30 assignment region assigned to server 10.30.94.49:60020  thu, 4 dec 2008 11:18:28 open region opened on server : 10.30.94.54:60020  thu, 4 dec 2008 11:18:25 assignment region assigned to server 10.30.94.54:60020  thu, 4 dec 2008 11:18:25 split region split from: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554 on 10.30.94.49: 2008-12-04 16:17:39,550 info org.apache.hadoop.hbase.regionserver.hregionserver:  msg_region_open: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554  2008-12-04 16:17:39,550 info org.apache.hadoop.hbase.regionserver.hregionserver:  msg_region_open: content,61938ad11c92493c1aac82932651b873,1228317826934  2008-12-04 16:18:17,469 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554  2008-12-04 16:18:24,255 info org.apache.hadoop.hbase.regionserver.hregion: compac  tion completed on region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554 in 6sec  2008-12-04 16:18:24,255 info org.apache.hadoop.hbase.regionserver.hregion: starting split of region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554  2008-12-04 16:18:24,258 info org.apache.hadoop.hbase.regionserver.hregion: closed content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554  2008-12-04 16:18:24,745 info org.apache.hadoop.hbase.regionserver.hregion: region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257/2031068422 available  2008-12-04 16:18:24,746 info org.apache.hadoop.hbase.regionserver.hregion: closed content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  2008-12-04 16:18:25,051 info org.apache.hadoop.hbase.regionserver.hregion: region content,c528c8864e67a3addcb50587586dfe38,1228407504257/1813242175 available  2008-12-04 16:18:25,052 info org.apache.hadoop.hbase.regionserver.hregion: closed content,c528c8864e67a3addcb50587586dfe38,1228407504257  2008-12-04 16:18:25,066 info org.apache.hadoop.hbase.regionserver.compactsplitthread: region split, meta updated, and report to master all successful. old region=  region => {name => content,c4e518dc579ca98bf97ea13d1f7c80fc,1228328680554', startkey => 'c4e518dc579ca98bf97ea13d1f7c80fc', endkey => c56eb508be0009d04429b6977c  91a425', encoded => 1137747785, offline => true, split => true, table => {{name => 'content', is_root => 'false', is_meta => 'false', families => [ {name => 'url', bloomfilter => 'false', versions => '1', compression => 'none', length => '2147483647', ttl => '2592000', in_memory => 'false', blockcache => 'false'} , {name => 'info', bloomfilter => 'false', versions => '1', compression => 'none', length => '2147483647', ttl => '2592000', in_memory => 'false', blockcache => 'false'} , {name => 'content', bloomfilter => 'false', versions => '1', compression => 'none', length => '2147483647', ttl => '2592000', in_memory => 'false', blockcache => 'false'} ]}}, new regions: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257, content,c528c8864e67a3addcb50587586dfe38,1228407504257. split took 0sec on 10.30.94.54: 2008-12-04 16:18:25,681 info org.apache.hadoop.hbase.regionserver.hregionserver:  worker: msg_region_open: content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  2008-12-04 16:18:25,681 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region content,c528c8864e67a3addcb50587586dfe38,1228407504257  2008-12-04 16:18:25,867 info org.apache.hadoop.hbase.regionserver.hregion: region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257/2031068422 available  2008-12-04 16:18:32,951 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region content,c528c8864e67a3addcb50587586dfe38,1228407504257 in 7sec  2008-12-04 16:18:32,951 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257  2008-12-04 16:18:35,882 info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region content,c4e518dc579ca98bf97ea13d1f7c80fc,1228407504257 in 2sec ",
        "label": 241
    },
    {
        "text": "npe in processregionstatuschange getmetaregion  testing killing of regionserver hosting .meta., came across following: 2009-05-20 03:50:44,130 [hmaster] info org.apache.hadoop.hbase.regionserver.hlog: triggering lease recovery org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hdfs.protocol.alreadybeingcreatedexception: failed to create file /hbasetrunk2/.logs/aa0-000-15.u.powerset.com_1242789840075_60021/hlog.dat.1242790948114 for dfsclient_1104727689 on client 208.76.44.139, because this file is already being created by dfsclient_1020405213 on 208.76.44.142     at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.startfileinternal(fsnamesystem.java:1058)     at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.appendfile(fsnamesystem.java:1146)     at org.apache.hadoop.hdfs.server.namenode.namenode.append(namenode.java:392)     at sun.reflect.generatedmethodaccessor18.invoke(unknown source)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.hadoop.ipc.rpc$server.call(rpc.java:508)     at org.apache.hadoop.ipc.server$handler$1.run(server.java:959)     at org.apache.hadoop.ipc.server$handler$1.run(server.java:955)     at java.security.accesscontroller.doprivileged(native method)     at javax.security.auth.subject.doas(subject.java:396)     at org.apache.hadoop.ipc.server$handler.run(server.java:953)     at org.apache.hadoop.ipc.client.call(client.java:739)     at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:220)     at $proxy0.append(unknown source)     at sun.reflect.generatedmethodaccessor11.invoke(unknown source)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:82)     at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:59)     at $proxy0.append(unknown source)     at org.apache.hadoop.hdfs.dfsclient.append(dfsclient.java:487)     at org.apache.hadoop.hdfs.distributedfilesystem.append(distributedfilesystem.java:186)     at org.apache.hadoop.fs.filesystem.append(filesystem.java:525)     at org.apache.hadoop.hbase.regionserver.hlog.splitlog(hlog.java:799)     at org.apache.hadoop.hbase.regionserver.hlog.splitlog(hlog.java:752)     at org.apache.hadoop.hbase.master.processservershutdown.process(processservershutdown.java:248)     at org.apache.hadoop.hbase.master.hmaster.processtodoqueue(hmaster.java:456)     at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:392) 2009-05-20 03:50:45,496 [hmaster] info org.apache.hadoop.hbase.regionserver.hlog: past out lease recovery 2009-05-20 03:50:45,524 [hmaster] debug org.apache.hadoop.hbase.regionserver.hlog: adding queue for testtable,0026910374,1242790893613 2009-05-20 03:50:45,765 [hmaster] debug org.apache.hadoop.hbase.regionserver.hlog: adding queue for .meta.,,1 2009-05-20 03:50:46,737 [hmaster] debug org.apache.hadoop.hbase.regionserver.hlog: ioe pushed 61380 entries from hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/.logs/aa0-000-15.u.powerset.com_1242789840075_60021/hlog.dat.1242790948114 2009-05-20 03:50:46,750 [pool-4-thread-1] debug org.apache.hadoop.hbase.regionserver.hlog: thread got 10 to process 2009-05-20 03:50:46,750 [pool-4-thread-1] debug org.apache.hadoop.hbase.regionserver.hlog: applied 10 total edits to .meta.,,1 in 0ms 2009-05-20 03:50:46,762 [pool-4-thread-2] debug org.apache.hadoop.hbase.regionserver.hlog: thread got 61370 to process 2009-05-20 03:50:47,990 [pool-4-thread-2] debug org.apache.hadoop.hbase.regionserver.hlog: applied 61370 total edits to testtable,0026910374,1242790893613 in 1227ms 2009-05-20 03:50:48,289 [hmaster] info org.apache.hadoop.hbase.regionserver.hlog: hlog file splitting completed in 106378 millis for hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/.logs/aa0-000-15.u.powerset.com_1242789840075_60021 2009-05-20 03:50:48,299 [hmaster] debug org.apache.hadoop.hbase.master.processservershutdown$scanrootregion: process server shutdown scanning root region on 208.76.44.141 2009-05-20 03:50:48,304 [hmaster] debug org.apache.hadoop.hbase.master.regionserveroperation: shutdown scanner for aa0-000-15.u.powerset.com_1242789840075_60021 processing .meta.,,1 2009-05-20 03:50:48,305 [hmaster] debug org.apache.hadoop.hbase.master.regionserveroperation: removing meta region .meta.,,1 from online meta regions 2009-05-20 03:50:48,307 [hmaster] debug org.apache.hadoop.hbase.master.regionserveroperation: process server shutdown scanning root region on 208.76.44.141 finished hmaster 2009-05-20 03:50:48,307 [hmaster] debug org.apache.hadoop.hbase.master.regionserveroperation: numberofmetaregions: 1, onlinemetaregions.size(): 0 2009-05-20 03:50:48,307 [hmaster] debug org.apache.hadoop.hbase.master.regionserveroperation: requeuing because not all meta regions are online 2009-05-20 03:50:48,308 [hmaster] debug org.apache.hadoop.hbase.master.hmaster: processing todo: pendingopenoperation from aa0-000-12.u.powerset.com_1242789840315_60021 2009-05-20 03:50:48,309 [hmaster] warn org.apache.hadoop.hbase.master.hmaster: processing pending operations: pendingopenoperation from aa0-000-12.u.powerset.com_1242789840315_60021 java.lang.nullpointerexception     at org.apache.hadoop.hbase.master.processregionstatuschange.getmetaregion(processregionstatuschange.java:74)     at org.apache.hadoop.hbase.master.processregionopen.process(processregionopen.java:61)     at org.apache.hadoop.hbase.master.hmaster.processtodoqueue(hmaster.java:456)     at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:392) ",
        "label": 314
    },
    {
        "text": "hbck should check fs owner permissions   in some cases, hbck needs to be run as a user that has write perms to the file system. if it writes data to hbase's directories, it may write new files/dirs that the hbase processes's user does not have permissions to. we should alert the user of this situation. ",
        "label": 248
    },
    {
        "text": " regression  htable get getrow with a ts is broken  if using the old api with 0.20, the behavior of get and getrow is changed when setting a timestamp. previously, setting a ts was working like a time range and now it works like an exact time. ",
        "label": 229
    },
    {
        "text": "allow to compile with jdk8  fix the following two problems: 1. poolmap 2. inputsampler ",
        "label": 286
    },
    {
        "text": "provide an option to skip calculating block locations for snapshotinputformat  when a mr job is reading from snapshotinputformat, it needs to calculate the splits based on the block locations in order to get best locality. however, this process may take a long time for large snapshots. in some setup, the computing layer, spark, hive or presto could run out side of hbase cluster. in these scenarios, the block locality doesn't matter. therefore, it will be great to have an option to skip calculating the block locations for every job. that will super useful for the hive/presto/spark connectors. ",
        "label": 490
    },
    {
        "text": "serious bug in cyclic replication  while studying the code for hbase-7709, i found a serious bug in the current cyclic replication code. the problem is here in hregion.dominibatchmutation:       mutation first = batchop.operations[firstindex].getfirst();       txid = this.log.appendnosync(regioninfo, this.htabledescriptor.getname(),                waledit, first.getclusterid(), now, this.htabledescriptor); now note that edits replicated from remote cluster and local edits might interleave in the wal, we might also receive edit from multiple remote clusters. hence that <waledit> might have edits from many clusters in it, but all are just labeled with the clusterid of the first mutation. fixing this in dominibatchmutation seems tricky to do efficiently (imagine we get a batch with cluster1, cluster2, cluster1, cluster2, ..., in that case each edit would have to be its own batch). the coprocessor handling would also be difficult. the other option is create batches of puts grouped by the cluster id in replicationsink.replicateentries(...), this is not as general, but equally correct. this is the approach i would favor. lastly this is very hard to verify in a unittest. ",
        "label": 286
    },
    {
        "text": "backup should check permission for snapshot copy in advance  when the user running the backup doesn't have permission to copy the snapshot , he / she would see: 2017-11-02 18:21:33,654 error [main] util.abstracthbasetool: error running command-line tool org.apache.hadoop.hbase.snapshot.exportsnapshotexception: failed to copy the snapshot directory: from=hdfs://ctr-e134-1499953498516-263664-01-000003.hwx.site:8020/apps/hbase/data/.hbase-snapshot/snapshot_1509646891251_default_integrationtestbackuprestore.table2 to=hdfs://ctr-e134-1499953498516-263664-01-000003.hwx.site:8020/user/root/test-data/fb919a6f-3cb4-4d57-bbcf-561d6e5b3ae8/backupit/backup_1509646884252/default/integrationtestbackuprestore.table2/.hbase-snapshot/.tmp/snapshot_1509646891251_default_integrationtestbackuprestore.table2 at org.apache.hadoop.hbase.snapshot.exportsnapshot.dowork(exportsnapshot.java:1009) at org.apache.hadoop.hbase.util.abstracthbasetool.run(abstracthbasetool.java:154) at org.apache.hadoop.hbase.backup.mapreduce.mapreducebackupcopyjob.copy(mapreducebackupcopyjob.java:386) at org.apache.hadoop.hbase.backup.impl.fulltablebackupclient.snapshotcopy(fulltablebackupclient.java:103) at org.apache.hadoop.hbase.backup.impl.fulltablebackupclient.execute(fulltablebackupclient.java:175) at org.apache.hadoop.hbase.backup.impl.backupadminimpl.backuptables(backupadminimpl.java:601) at org.apache.hadoop.hbase.integrationtestbackuprestore.runtest(integrationtestbackuprestore.java:180) at org.apache.hadoop.hbase.integrationtestbackuprestore.testbackuprestore(integrationtestbackuprestore.java:134) at org.apache.hadoop.hbase.integrationtestbackuprestore.runtestfromcommandline(integrationtestbackuprestore.java:263) it would be more user friendly if the permission is checked before taking the snapshot. ",
        "label": 228
    },
    {
        "text": "ignore deprecations during build  i'd like to propose tweaking the maven compiler plugin definition to ignore deprecations. firstly, during a maven build, the deprecation output is totally ignored by everyone i'm sure, and ide's do a much better job of tracking these anyway, so the value to the output is just not there. secondly when one has this many deprecation warnings it's actually hiding any compiler errors. i was in a conversation with 'adragomir' on irc last night (my time) and he was bitten by this, because the error level stuff is done first, but quickly scrolls off the screen and becomes hidden. ",
        "label": 350
    },
    {
        "text": "store ignores checksum errors when opening files  if you corrupt one of the storefiles in a region (eg using vim to muck up some bytes), the region will still open, but that storefile will just be ignored with a log message. we should probably not do this in general - better to keep that region unassigned and force an admin to make a decision to remove the bad storefile. ",
        "label": 290
    },
    {
        "text": "npe in htable checkandsave when row doesn't exist  to reproduce, just invoke htable.checkandsave(batchupdate, expectedvalues, lock) using a batchupdate of a row that doesn't exist. ",
        "label": 189
    },
    {
        "text": "move master branch hbase to jdk only  set build and pom target jvm version as jdk8. we chatted about it here: http://osdir.com/ml/general/2016-04/msg09691.html set it as blocker on 2.0.0. we need to work on yetus-369 before we can finish up this issue. ",
        "label": 149
    },
    {
        "text": "better error message when httpserver fails to start due to java net bindexception  starting hbase using hoya, i saw the following in log: 2013-12-17 21:49:06,758 info  [master:hor12n19:42587] http.httpserver: httpserver.start() threw a non bind ioexception java.net.bindexception: port in use: hor12n14.gq1.ygridcore.net:12432         at org.apache.hadoop.http.httpserver.openlistener(httpserver.java:742)         at org.apache.hadoop.http.httpserver.start(httpserver.java:686)         at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:586)         at java.lang.thread.run(thread.java:722) caused by: java.net.bindexception: cannot assign requested address         at sun.nio.ch.net.bind0(native method)         at sun.nio.ch.net.bind(net.java:344)         at sun.nio.ch.net.bind(net.java:336)         at sun.nio.ch.serversocketchannelimpl.bind(serversocketchannelimpl.java:199)         at sun.nio.ch.serversocketadaptor.bind(serversocketadaptor.java:74)         at org.mortbay.jetty.nio.selectchannelconnector.open(selectchannelconnector.java:216)         at org.apache.hadoop.http.httpserver.openlistener(httpserver.java:738) this was due to hbase.master.info.bindaddress giving static address but hoya allocates master dynamically. better error message should be provided: when bindaddress points another host than local host, message should remind user to remove / adjust hbase.master.info.bindaddress config param from hbase-site.xml ",
        "label": 278
    },
    {
        "text": "support parallel request cancellation for multi get  ",
        "label": 139
    },
    {
        "text": "thriftserver instantiates a new htable per request  every request creates a new htable in thriftserver, this is highly inefficient. it's even worse now that the htable constructor does a rpc to the master. assigning to cosmin since he said they have some code they can share. ",
        "label": 76
    },
    {
        "text": "scanner misses columns   rows when the scanner is obtained durring a memcache flush  i first noticed that some columns for a row were missing if they are coming from a scanner that was obtained while a memecache flush on the region was in progress. i tried to write a simple unit test to reproduce, however the problem i get in the unit test is that some rows are being missed. ",
        "label": 314
    },
    {
        "text": "change balancer sloppyness from to  this is a quick workaround until we do a better balancer. taking a region off line when cluster is under load is bad news. latency goes up as we wait on regions to come up in new locations. the load balancer should only cut in if the cluster is way out of alignment. i'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. we'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in. ",
        "label": 314
    },
    {
        "text": " fb  allow wal to be disabled pertable   there are use cases which want to get better performance by turning off waledits. at the server side, the config setting currently allows us to turn on or turn off the wal edits. but, the setting applies to all the tables. we should be able to control this setting at a per table level. ",
        "label": 34
    },
    {
        "text": "fix hfile v1 detector to handle accesscontrolexception for non existant files  on some hadoop versions, fs.exists() throws an accesscontrolexception if there is a non-searchable inode in the file path. versions such as 2.1.0-beta just returns false.  this jira is to fix hfile v1 detector tool to avoid making such calls. see the below exception when running the tool on one hadoop version error util.hfilev1detector: org.apache.hadoop.security.accesscontrolexception: permission denied: user=hbase, access=execute, inode=\"/hbase/.meta./.tableinfo.0000000001\":hbase:supergroup:-rw-r--r-- at org.apache.hadoop.hdfs.server.namenode.fspermissionchecker.check(fspermissionchecker.java:234) at org.apache.hadoop.hdfs.server.namenode.fspermissionchecker.checktraverse(fspermissionchecker.java:187) at org.apache.hadoop.hdfs.server.namenode.fspermissionchecker.checkpermission(fspermissionchecker.java:150) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.checkpermission(fsnamesystem.java:5141) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.checkpermission(fsnamesystem.java:5123) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.checktraverse(fsnamesystem.java:5102) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getfileinfo(fsnamesystem.java:3265) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.getfileinfo(namenoderpcserver.java:719) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.getfileinfo(clientnamenodeprotocolserversidetranslatorpb.java:692) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java:59628) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:585) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:1026) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2040) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2036) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:396) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1477) at org.apache.hadoop.ipc.server$handler.run(server.java:2034) ",
        "label": 199
    },
    {
        "text": "performance update in storefilewriter java for string replacement  change the replaceall() function using regex with a static pattern for performance upgrade ",
        "label": 63
    },
    {
        "text": "rename config names in user scan snapshot feature  as discussed in hbase-22578, hbase-22580, the config names are not so proper. and make the snapshotscannerhdfsaclcleaner load automatically if this feature is enabled. ",
        "label": 500
    },
    {
        "text": "move to slf4j  allows logging with less friction. see http://logging.apache.org/log4j/2.x/ this rather radical transition can be done w/ minor change given they have an adapter for apache's logging, the one we use. they also have and adapter for slf4j so we likely can remove at least some of the 4 versions of this module our dependencies make use of. i made a start in attached patch but am currently stuck in maven dependency resolve hell courtesy of our slf4j. fixing will take some concentration and a good net connection, an item i currently lack. other todos are that will need to fix our little log level setting jsp page \u2013 will likely have to undo our use of hadoop's tool here \u2013 and the config system changes a little. i will return to this project soon. will bring numbers. ",
        "label": 60
    },
    {
        "text": "htable commit no longer works with existing rowlocks though it's still in api  introduced by hbase-748, the rowlock passed into htable.commit is now ignored. this causes the update the hang until that rowlock expires, and then it proceeds with getting a new row lock. ",
        "label": 229
    },
    {
        "text": "backport hbase and hbase to  both hbase-5359 and hbase-7596 are useful and should be added to 0.94. ",
        "label": 233
    },
    {
        "text": "review all old api that takes a table name as a byte array and ensure none can pass ns   tablename  go over all old apis that take a table name and ensure that it is not possible to pass in a byte array that is a namespace + tablename; instead throw an exception. ",
        "label": 437
    },
    {
        "text": "book seems to be broken  seems that the content after:   https://hbase.apache.org/book.html#jython seems to be broken. no more titles and links.   misty linville fyi. ",
        "label": 330
    },
    {
        "text": "add visibility into flush and compaction queues  on busy spurts we can see regionservers start to see large queues for compaction. it's really hard to tell if the server is queueing a lot of compactions for the same region, lots of compactions for lots of regions, or just falling behind. for flushes much the same. there can be flushes in queue that aren't being run because of delayed flushes. there's no way to know from the metrics how many flushes are for each region, how many are delayed. etc. we should add either more metrics around this ( num per region, max per region, min per region ) or add on a ui page that has the list of compactions and flushes. or both. ",
        "label": 3
    },
    {
        "text": "hfileprettyprinter finds incorrect largest row  when one specifies 'printstats' option to hfileprettyprinter one of the findings in the report is key row key of the largest row in hfile. due to a bug it prints in correct row.       if (currowbytes > maxrowbytes && prevkv != null) {         biggestrow = prevkv.getrow();       } above piece should update maxrowbytes to currowbytes. ",
        "label": 417
    },
    {
        "text": "fix unescaped   characters in javadoc  from https://builds.apache.org/job/hbase%20website%20link%20ckecker/28/artifact/link_report/index.html: host: hbase.apache.org date: mon, 29-feb-2016 12:06:21 (local) linklint version: 2.3.5_ingo_020 #------------------------------------------------------------ # warn    2 warnings (cross referenced) #------------------------------------------------------------ unquoted \"<\" in <0.90.5, <0.90.5, < occurred in     /devapidocs/org/apache/hadoop/hbase/util/hbasefsck.html unquoted \"<\" in <0.92.0) a master  res occurred in     /devapidocs/org/apache/hadoop/hbase/util/hbasefsck.html ",
        "label": 177
    },
    {
        "text": "add bloom block index support  add a way to save hbase bloom filters into an array of meta blocks instead of one big meta block, and load only the blocks required to answer a query. this will allow us faster bloom load times for large storefiles & pave the path for adding bloom filter support to hfileoutputformat bulk load. ",
        "label": 324
    },
    {
        "text": "in hbase daemons sh  description of the default backup master file path is wrong  in hbase-daemons.sh, description of the default backup-master file path is wrong #   hbase_backup_masters file naming remote hosts. #     default is ${hadoop_conf_dir}/backup-masters it says the default backup-masters file path is at a hadoop-conf-dir, but shouldn't this be hbase_conf_dir? also adding following lines to conf/hbase-env.sh would be helpful # file naming hosts on which backup hmaster will run.  $hbase_home/conf/backup-masters by default. export hbase_backup_masters=${hbase_home}/conf/backup-masters ",
        "label": 69
    },
    {
        "text": "facilitate timeouts in hbase client  currently, there is no way to force an operation on the hbase client (viz. htable) to time out if a certain amount of time has elapsed. in other words, all invocations on the htable class are veritable blocking calls, which will not return until a response (successful or otherwise) is received. in general, there are two ways to handle timeouts: (a) call the operation in a separate thread, until it returns a response or the wait on the thread times out and (b) have the underlying socket unblock the operation if the read times out. the downside of the former approach is that it consumes more resources in terms of threads and callables. here, we describe a way to specify and handle timeouts on the htable client, which relies on the latter approach (i.e., socket timeouts). right now, the hbaseclient sets the socket timeout to the value of the \"ipc.ping.interval\" parameter, which is also how long it waits before pinging the server in case of a failure. the goal is to allow clients to set that timeout on the fly through htable. rather than adding an optional timeout argument to every htable operation, we chose to make it a property of htable which effectively applies to every method that involves a remote operation. in order to propagate the timeout from htable to hbaseclient, we replaced all occurrences of servercallable in htable with an extension called clientcallable, which sets the timeout on the region server interface, once it has been instantiated, through the hconnection object. the latter, in turn, asks hbaserpc to pass that timeout to the corresponding invoker, so that it may inject the timeout at the time the invocation is made on the region server proxy. right before the request is sent to the server, we set the timeout specified by the client on the underlying socket. in conclusion, this patch will afford clients the option of performing an hbase operation until it completes or a specified timeout elapses. note that a timeout of zero is interpreted as an infinite timeout. ",
        "label": 265
    },
    {
        "text": "requests count is completely off  i tried 0.95.1 rc1 in standalone, and the requests count in both the master and rs web uis are wrong. i haven't dug too much in but it seems too low when i'm sending load, and it takes >10 seconds to clear up when the cluster becomes completely idle. ",
        "label": 224
    },
    {
        "text": "change the rpc stack to thrift  this umbrella task is to talk about the effort to change the rpc stack in 0.89-fb to thrift instead of using hadooprpc. we are using swift(https://github.com/facebook/swift) which is a fast and easy-to-use, annotation-based java library for creating thrift serializable types and services. the idea is to annotate the data structures that need to be transported across the wire and services interfaces that produce and consume these data structures. this will enable us to move away from the proxy based thrift server implementation and adopt the native thrift server interface. this also allows us to write c++ and php clients which directly talk to the actual regionserver demons rather than proxy demons which introduce additional hops, memory and cpu overhead. ",
        "label": 154
    },
    {
        "text": "deleted rows reappear after a hstore flushcache  and or hstore compactstores   when deleting a row, that row seems to be deleted and everything to be normal but some time afterwards that row somehow reappears (htable.get() and scanners find it again). looking at the log files it seems like deleted rows reappear after a flush of the memcache or after the compaction... ",
        "label": 218
    },
    {
        "text": "when merging expired stripes  we need to create an empty file to preserve metadata   stripe compaction is a good feature in 0.96 and 0.98. but when i used it in a heavy-write non-uniform row keys scenario(e.g. time dimension in a key), i came across some problems.   i made my stripes split at the size of 2g(hbase.store.stripe.sizetosplit=2g), and soon there were tens of them. it was true that only the last stripe receiving the new keys kept compacting - old data didn't compact as much, or at all. however, the old stripes were still there when they all expired. i checked the source code and found that when compacting expired stripes, the storescanner may return no kvs so that sizemultiwriter.append() is never called. that's to say, no new file will be created.   my solution is to create an empty file to preserve metadata at the end of the sizemultiwriter.commitwritersinternal(). ",
        "label": 466
    },
    {
        "text": "bad version  failed opening to opened but master thinks it is open anyways  i have this in rs log: 2012-10-22 02:21:50,698 error org.apache.hadoop.hbase.regionserver.handler.openregionhandler: failed transitioning node b9,\\xee\\xae\\x9biqo\\x89]+a\\xe0\\x7f\\xb7'x?,1349052737638.9af7cfc9b15910a0b3d714bf40a3248f. from opening to opened -- closing region org.apache.zookeeper.keeperexception$badversionexception: keepererrorcode = badversion for /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f master says this (it is bulk assigning): .... 2012-10-22 02:21:40,673 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:10302-0xb3a862e57a503ba set watcher on existing znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f ... then this .... 2012-10-22 02:23:47,089 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:10302-0xb3a862e57a503ba set watcher on existing znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f .... 2012-10-22 02:24:34,176 debug org.apache.hadoop.hbase.zookeeper.zkutil: master:10302-0xb3a862e57a503ba retrieved 112 byte(s) of data from znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f and set watcher; region=b9,\\xee\\xae\\x9biqo\\x89]+a\\xe0\\x7f\\xb7'x?,1349052737638.9af7cfc9b15910a0b3d714bf40a3248f., origin=sv4r17s44,10304,1350872216778, state=rs_zk_region_opened etc. disagreement as to what is going on here. ",
        "label": 46
    },
    {
        "text": "webuis https support  with https enabled on hadoop 1.2  https://issues.apache.org/jira/browse/mapreduce-4661  hbase automatically inherits the feature. however, there are some hardcoded places need to be fixed. ",
        "label": 316
    },
    {
        "text": "getting exceptions in shell when creating disabling tables  on the list from dru jensen: i am testing the release candidate with hadoop 0.17.2.1 release. i am curious if others are seeing this or if i have something mis-configured. i reformatted the dfs and recreated everything from scratch. hbase(main):009:0> version  version: 0.2.1, r691710, wed sep 3 11:50:24 pdt 2008 i occasionally get the following error performing a create table. however when i do a list, the table was successfully created. hbase(main):007:0> create 'test', 'avg', 'std', 'max'  nativeexception: org.apache.hadoop.hbase.client.noserverforregionexception: no server address listed in .meta. for region test,,1220628716239  from org/apache/hadoop/hbase/client/hconnectionmanager.java:536:in `locateregioninmeta'  from org/apache/hadoop/hbase/client/hconnectionmanager.java:459:in `locateregion'  from org/apache/hadoop/hbase/client/hconnectionmanager.java:419:in `locateregion'  from org/apache/hadoop/hbase/client/hbaseadmin.java:157:in `createtable'  from sun/reflect/nativemethodaccessorimpl.java:-2:in `invoke0'  from sun/reflect/nativemethodaccessorimpl.java:39:in `invoke'  from sun/reflect/delegatingmethodaccessorimpl.java:25:in `invoke'  from java/lang/reflect/method.java:585:in `invoke'  from org/jruby/javasupport/javamethod.java:250:in `invokewithexceptionhandling'  from org/jruby/javasupport/javamethod.java:219:in `invoke'  from org/jruby/javasupport/javaclass.java:416:in `execute'  from org/jruby/internal/runtime/methods/simplecallbackmethod.java:67:in `call'  from org/jruby/internal/runtime/methods/dynamicmethod.java:78:in `call'  from org/jruby/runtime/callsite.java:329:in `call'  from org/jruby/evaluator/astinterpreter.java:649:in `callnode'  from org/jruby/evaluator/astinterpreter.java:324:in `evalinternal'  ... 121 levels...  from ruby.opt.hbase_minus_0_dot_2_dot_1.bin.hirbinvokermethod__23$ruby$startopt:-1:in `call'  from org/jruby/internal/runtime/methods/dynamicmethod.java:74:in `call'  from org/jruby/internal/runtime/methods/compiledmethod.java:48:in `call'  from org/jruby/runtime/callsite.java:123:in `cacheandcall'  from org/jruby/runtime/callsite.java:298:in `call'  from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:351:in `_file_'  from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `_file_'  from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `load'  from org/jruby/ruby.java:512:in `runscript'  from org/jruby/ruby.java:432:in `runnormally'  from org/jruby/ruby.java:312:in `runfrommain'  from org/jruby/main.java:144:in `run'  from org/jruby/main.java:89:in `run'  from org/jruby/main.java:80:in `main'  from /opt/hbase/bin/../bin/hirb.rb:228:in `create'  from (hbase):8:in `binding'hbase(main):008:0> and occasionally, i get this exception when trying to disable a table. however it was successfully disabled. hbase(main):002:0> disable 'test'  nativeexception: java.io.ioexception: unable to disable table test  from org/apache/hadoop/hbase/client/hbaseadmin.java:418:in `disabletable'  from org/apache/hadoop/hbase/client/hbaseadmin.java:379:in `disabletable'  from sun/reflect/nativemethodaccessorimpl.java:-2:in `invoke0'  from sun/reflect/nativemethodaccessorimpl.java:39:in `invoke'  from sun/reflect/delegatingmethodaccessorimpl.java:25:in `invoke'  from java/lang/reflect/method.java:585:in `invoke'  from org/jruby/javasupport/javamethod.java:250:in `invokewithexceptionhandling'  from org/jruby/javasupport/javamethod.java:219:in `invoke'  from org/jruby/javasupport/javaclass.java:416:in `execute'  from org/jruby/internal/runtime/methods/simplecallbackmethod.java:67:in `call'  from org/jruby/internal/runtime/methods/dynamicmethod.java:78:in `call'  from org/jruby/runtime/callsite.java:155:in `cacheandcall'  from org/jruby/runtime/callsite.java:332:in `call'  from org/jruby/evaluator/astinterpreter.java:649:in `callnode'  from org/jruby/evaluator/astinterpreter.java:324:in `evalinternal'  from org/jruby/evaluator/astinterpreter.java:620:in `blocknode'  ... 121 levels...  from ruby.opt.hbase_minus_0_dot_2_dot_1.bin.hirbinvokermethod__23$ruby$startopt:-1:in `call'  from org/jruby/internal/runtime/methods/dynamicmethod.java:74:in `call'  from org/jruby/internal/runtime/methods/compiledmethod.java:48:in `call'  from org/jruby/runtime/callsite.java:123:in `cacheandcall'  from org/jruby/runtime/callsite.java:298:in `call'  from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:351:in `_file_'  from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `_file_'  from ruby/opt/hbase_minus_0_dot_2_dot_1/bin//opt/hbase/bin/../bin/hirb.rb:-1:in `load'  from org/jruby/ruby.java:512:in `runscript'  from org/jruby/ruby.java:432:in `runnormally'  from org/jruby/ruby.java:312:in `runfrommain'  from org/jruby/main.java:144:in `run'  from org/jruby/main.java:89:in `run'  from org/jruby/main.java:80:in `main'  from /opt/hbase/bin/../bin/hirb.rb:254:in `disable' ",
        "label": 229
    },
    {
        "text": "hfile meta block handling bugs  hfile doesn't handle 'get meta block' when there are no meta blocks. it throws an unhelpful exception \"meta index not loaded\", which is not the case. no meta blocks = no meta index. it should return null instead. additionally, hfile doesn't even get all meta names properly, due to the incorrect use of the file's comparator, instead of using just a bytes comparator in the index. this is manifested by npes in some tests. ",
        "label": 547
    },
    {
        "text": "testhregion testwriteswhilescanning flaky on trunk  iirc we added this test in response to a bug in hbase-2248 in the old branch. it's failing about half the time on my internal hudson. i=36 expected:<1000> but was:<0> ",
        "label": 453
    },
    {
        "text": "replace fsreadlatency metrics with preads latency  with hbase now using preads for all the read ops, the read latency metrics need to be updated to capture the stats about the pread latencies. this issue to track that. ",
        "label": 378
    },
    {
        "text": "use multi to batch offline regions in zookeeper  bulk assigner needs to set regions offline in zookeeper one by one. i was wondering if we can have some performance improvement if we batch these operations using zookeeper#multi. ",
        "label": 242
    },
    {
        "text": "add metric for number of callqueuetoobigexceptions  this exception is being thrown more. we should add a metric for this one. ",
        "label": 243
    },
    {
        "text": "replicationsourcemanager should be able to track multiple wal paths  currently replicationsourcemanager uses logrolled() to receive notification about new hlog and remembers it in latestpath.  when region server has multiple wal support, we need to keep track of multiple path's in replicationsourcemanager ",
        "label": 504
    },
    {
        "text": "add metric to report client shortcircuit reads  with the availability of shortcircuit reads, when the feature is enabled there is no metric which exposes how many times the regionserver was able to shortcircuit the read and not make a ipc to the datanode. it will be great to add the metric and expose it via ganglia. ",
        "label": 483
    },
    {
        "text": "scans respect row locks  in the javadoc package org.apache.hadoop.hbase.client description it states that \"scans (currently) operate without respect for row locks.\" i think that since 0.20.4 and hbase-2248 this is no longer the case. ",
        "label": 547
    },
    {
        "text": "implement a cancel for in progress compactions  depending on current server load, it can be extremely expensive to run periodic minor / major compactions. it would be helpful to have a feature where a user could use the shell or a client tool to explicitly cancel an in-progress compactions. this would allow a system to recover when too many regions became eligible for compactions at once ",
        "label": 332
    },
    {
        "text": "add m r over snapshots to  i think we want drive towards all (or most) m/r over hbase to be against snapshots and hdfs directly.  adopting a simple input format (even if just as a sample) as part of hbase will allow us to direct users this way. ",
        "label": 286
    },
    {
        "text": "add doublecolumninterpreter  doublecolumninterpreter was requested by royston sellman. ",
        "label": 254
    },
    {
        "text": "update branch pom version  branch 1.3 currently has version=1.3.1 set in the poms. i think this should be 1.3.2-snapshot, in case anybody tries to deploy nightlies for any reason. wdyt mikhail antonov? ",
        "label": 320
    },
    {
        "text": "ensure that kv's newer than the oldest living scanner is not accounted for the maxversions during flush compaction   ",
        "label": 34
    },
    {
        "text": "make gettableregions return an empty list if the table does not exist  making the gettableregions thrift api method handle tablenotfoundexception and return an empty list in that case. without this the behavior is dependent on whether an htable object is present in the thread-local cache in case a table was deleted. ",
        "label": 324
    },
    {
        "text": "tidy up naming consistency and documentation in coprocessor framework  we have a few naming inconsistencies in the coprocessor api and some stale javadocs that have been spotted by lars george as he digs through it. we should clean these up before we have an official release and are forced to go through a round of deprecation to make any changes. current items on the list: rename baseregionobservercoprocessor -> baseregionobserver in basemasterobserver, rename observercontext parameter variable from \"env\" to \"c\" or \"ctx\" unnecessary public modifier for methods in regionobserver interface as part of this, we should take a pass through the javadocs and verify they are up to date with what is currently implemented. please tack on other cosmetic changes or inconsistencies as you find them. ",
        "label": 327
    },
    {
        "text": "ensure javax annotation doesn't get include in shaded artifacts when built with java  master & branch-2 build fails on java 11. complaints about the hbase-shaded-check-invariants. will paste the stacktrace if needed in the comments.  ",
        "label": 391
    },
    {
        "text": "add client ability to perform mutations without the wal  there are a number of cases where a client might not want/need its edits to be written into the hlog. the most obvious use of this would be during a bulk import where we want the best performance possible and data loss is acceptable (can rerun the import). does this become a flag on put or a special mode of htable? ",
        "label": 247
    },
    {
        "text": "recovered edits files not deleted if it only contain edits that have already been flushed  hurts perf for all future opens of the region  on rs crash, master processes the rs's logs, splits them into per region log files, and puts them in recovered.edits sub-directory of the region. it may be the case the some of these files contain only old edits that have already been flushed, and don't need to be reapplied again. however, in this case the file is not deleted, and stays in recovered.edits for ever. this will slow down every future \"open\" of this region, as the region will unnecessarily spend time processing this file. in hregion.java:replacerecoverededitsifany(), the code below checks if the file we just processed contain any edits that were applied, and in that case flushes the memstore into which things were being recovered.   if (seqid > minseqid) {       // then we added some edits to memory. flush and cleanup split edit files.       internalflushcache(null, seqid);       for (path file: files) {         if (!this.fs.delete(file, false)) {           log.error(\"failed delete of \" + file);         } else {           log.debug(\"deleted recovered.edits file=\" + file);         }       }     } but it is not clear why the 'for' loop to clean up the recovered.edits file is inside the if check. ",
        "label": 314
    },
    {
        "text": "move targeted trunk to hadoop  bunch of work doing this one i'd say \u2013 different layout, different build system (ivy). ",
        "label": 547
    },
    {
        "text": "compaction events should be written to hlog  the sequence for a compaction should look like this: 1. compact region to \"new\" files 2. write a \"compacted region\" entry to the hlog 3. delete \"old\" files this deals with a case where the rs has paused between step 1 and 2 and the regions have since been reassigned. ",
        "label": 314
    },
    {
        "text": "new api  htable getrow with numversion specified  i'd like to be able to call htable.getrow with numversions, and get multiple versions for each column. ",
        "label": 144
    },
    {
        "text": "make hlog more resilient to write pipeline failures  the current implementation of hlog rolling to recover from transient errors in the write pipeline seems to have two problems: 1. when hlog.logsyncer triggers an ioexception during time-based sync operations, it triggers a log rolling request in the corresponding catch block, but only after escaping from the internal while loop. as a result, the logsyncer thread will exit and never be restarted from what i can tell, even if the log rolling was successful. 2. log rolling requests triggered by an ioexception in sync() or append() never happen if no entries have yet been written to the log. this means that write errors are not immediately recovered, which extends the exposure to more errors occurring in the pipeline. in addition, it seems like we should be able to better handle transient problems, like a rolling restart of datanodes while the hbase regionservers are running. currently this will reliably cause regionserver aborts during log rolling: either an append or time-based sync triggers an initial ioexception, initiating a log rolling request. however the log rolling then fails in closing the current writer (\"all datanodes are bad\"), causing a regionserver abort. in this case, it seems like we should at least allow you an option to continue with the new writer and only abort on subsequent errors. ",
        "label": 180
    },
    {
        "text": "update loadincrementalhfiles to use atomic bulk load rs mechanism  mr jobs and command line bulk load execution runs use the loadincrementalhfile.dobulkload. this needs to be updated to group hfiles by row/region so that rows can be atomically loaded multiple column families. ",
        "label": 248
    },
    {
        "text": "regenerate thrift2 python examples  looking at the files under hbase-examples/src/main/python/thrift2/gen-py/hbase, they were last updated in 2013. now that we use thrift 0.9.3, we should regenerate these examples based on the latest thrift definition. ",
        "label": 352
    },
    {
        "text": "master rpc server needs to be started before an rs can check in  starting up an rpc server is done in two steps. in the constructor, we instantiate the rpc server. then in startservicethreads() we start() it. if someone rpcs in between the instantiation and the start(), it seems that bad things can happen. we need to make sure this can't happen and there aren't any races here. ",
        "label": 547
    },
    {
        "text": "introduce a thread at rs side to call reportproceduredone  so that we can do some batching and also prevent blocking too many threads when hmaster is temporary done. ",
        "label": 149
    },
    {
        "text": "htablemultiplexer  there is a known issue in hbase client that single slow/dead region server could slow down the multiput operations across all the region servers. so the hbase client will be as slow as the slowest region server in the cluster. to solve this problem, htablemultiplexer will separate the multiput submitting threads with the flush threads, which means the multiput operation will be a nonblocking operation.   the submitting thread will shard all the puts into different queues based on its destination region server and return immediately. the flush threads will flush these puts from each queue to its destination region server. currently the htablemultiplexer only supports the put operation. ",
        "label": 292
    },
    {
        "text": "add the ability for performance evaluation to set the table compression  when testing it's nice to get a more realistic set of numbers. as such allowing the tool to create pre-split regions was needed. compression should be the same. ",
        "label": 309
    },
    {
        "text": "hbaseadmin istableenabled  should throw exception for non existent table  hbaseadmin#istableenabled() returns true for a table which doesn't exist. we should check table existence. ",
        "label": 441
    },
    {
        "text": "jenkins hash implementation uses longs unnecessarily  i don't believe you need to use long for a,b,c and as a result no longer need to & against int_mask. at a minimum the private static longs should be made final, and the \"main\" method should not print the absolute value of the hash but instead use something like integer.tohexstring ",
        "label": 384
    },
    {
        "text": "hbase rpc aspires to grow an infinite tree of trace scopes  some other places are also unsafe  ",
        "label": 406
    },
    {
        "text": "align the methods in table and asynctable  ",
        "label": 352
    },
    {
        "text": "hostname returned via reverse dns lookup contains trailing period if configured interface is not  default   if you are using an interface anything other than 'default' (literally that keyword) dns.java 's getdefaulthost will return a string which will   have a trailing period at the end. it seems javadoc of reversedns in dns.java (see below) is conflicting with what that function is actually doing.   it is returning a ptr record while claims it returns a hostname. the ptr record always has period at the end , rfc: http://irbs.net/bog-4.9.5/bog47.html we make call to dns.getdefaulthost at more than one places and treat that as actual hostname. quoting hregionserver for example string machinename = dns.getdefaulthost(conf.get(         \"hbase.regionserver.dns.interface\", \"default\"), conf.get(         \"hbase.regionserver.dns.nameserver\", \"default\")); this causes inconsistencies. an example of such inconsistency was observed while debugging the issue \"regions not getting reassigned if rs is brought down\". more here   http://search-hadoop.com/m/canua1qrckq1 we may want to sanitize the string returned from dns class. or better we can take a path of overhauling the way we do dns name matching all over. ",
        "label": 417
    },
    {
        "text": "convert aggregateprotocol to protobuf defined coprocessor service  with coprocessor endpoints now exposed as protobuf defined services, we should convert over all of our built-in endpoints to pb services. ",
        "label": 139
    },
    {
        "text": " findbugs  fix lock release on all paths  see https://builds.apache.org/job/precommit-hbase-build/1313//artifact/trunk/patchprocess/newpatchfindbugswarnings.html#warnings_mt_correctness category ul ",
        "label": 186
    },
    {
        "text": " findbugs  address wait notify synchronization inconsistency in sync  see https://builds.apache.org/job/precommit-hbase-build/1313//artifact/trunk/patchprocess/newpatchfindbugswarnings.html#warnings_mt_correctness fix classes is,li,mwm, nn, swl, ug, uw ",
        "label": 544
    },
    {
        "text": "fix hadoop build  some of the pom definitions are broken for the hadoop 2.0 build. its breaking the build for that version ",
        "label": 236
    },
    {
        "text": "if i call split fast enough  while inserting  rows disappear   i'll attach a unit test for this. basically if you call split, while inserting data you can get to the point to where the cluster becomes unstable, or rows will disappear. the unit test gives you some flexibility of: how many rows how wide the rows are the frequency of the split. the default settings crash unit tests or cause the unit tests to fail on my laptop. on my macbook air, i could actually turn down the number of total rows, and the frequency of the splits which is surprising. i think this is because the macbook air has much better io than my backup acer. ",
        "label": 544
    },
    {
        "text": "need hfile version check in security coprocessors  cell level visibility labels are stored as cell tags. so hfile v3 is the minimum version which can support this feature. better to have a version check in visibilitycontroller. some one using this cp but with any hfile version as v2, we can better throw error. ",
        "label": 38
    },
    {
        "text": "using scan startrow stoprow  will cause you to iterate the entire table  right now the only way for the client scanner to know that we are at the 'end' of a scan is to client-side-wise use the filter to figure this out. this is not easy to fix because the server is unable to indicate the difference between 'done with this region', and 'you're at the end of your scan'. in both cases we return 0 results, and the client can't figure out what it means. right now the best solution is to use filters, which is tricky since there is no stoprowfilter because that functionality is built in we might have to hack the 'stop row' functionality as a filter until we can improve the client-server api/rpc. ",
        "label": 547
    },
    {
        "text": "hfilecleaner should not delete empty ns table directories used for user san snapshot feature  hbase-21995 add a coprocessor to set hdfs acls for hbase users who own hbase read permission to mask users have the ability to scan snapshot directly. it creates empty directories for namespace and table under archive directory and set hdfs acls to these directories after namespace or table is created, in this way, users can read files under archive directory. but the hfilecleaner will delete empty directories and this will break this feature. so if the user scan snapshot feature is enabled, hfilecleaner should not delete empty ns/table directories. ",
        "label": 500
    },
    {
        "text": "hadoop compilation broken because jobtrackerrunner getjobtracker  method is not found  after hbase-5861 on 0.94 we are left with this issue on trunk. $ mvn clean test -plocaltests -dskiptests -dhadoop.profile=23 ... [error] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testcompile (default-testcompile) on project hbase: compilation failure [error] /home/jon/proj/hbase-svn/hbase/src/test/java/org/apache/hadoop/hbase/hbasetestingutility.java:[1333,35] cannot find symbol [error] symbol  : method getjobtracker() [error] location: class org.apache.hadoop.mapred.minimrcluster.jobtrackerrunner [error] -> [help 1] ",
        "label": 441
    },
    {
        "text": " mttr  improve region server recovery time   distributed log replay  just saw interesting issue where a cluster went down hard and 30 nodes had 1700 wals to replay. replay took almost an hour. it looks like it could run faster that much of the time is spent zk'ing and nn'ing. putting in 0.96 so it gets a look at least. can always punt. ",
        "label": 233
    },
    {
        "text": "fall back to filesystem block size default if hbase regionserver hlog blocksize is not specified  fall back to filesystem block size default if hbase.regionserver.hlog.blocksize is not specified. ",
        "label": 314
    },
    {
        "text": "testsplittransactiononcluster fails on occasion when it tries to move a region  testsplittransactiononcluster tries to have meta on one server and table region on a different server. if the luck of the draw has the table and meta on same server, it'll move the table region elsewhere. this has been failing since hbase-4300 because we've been doing hbaseadmin#move passing versioned servername bytes when hbaseadmin#move expects the raw, unversioned bytes. ",
        "label": 314
    },
    {
        "text": "server should not produce raite for already opening region in  because master retry logic handles this case poorly   the code in 0.94 am sets the region plan to point to the same server when retrying the assignment due to raite. log.warn(\"failed assignment of \"             + state.getregion().getregionnameasstring()             + \" to \"             + plan.getdestination()             + \", trying to assign \"             + (regionalreadyintransitionexception ? \"to the same region server\"                 + \" because of regionalreadyintransitionexception;\" : \"elsewhere instead; \")             + \"retry=\" + i, t); however, there's no wait time in the loop that retries the assignment, and if region is being marked failed to open, which may take some time, master can easily exhaust retries in less than half a second, going to the same server every time and getting the same exception (unfortunately i no longer have logs); then the region will be stuck. do you think this is worth fixing (for example, by not using the same server here after a few retries, or by adding timed backoff in such cases)? ",
        "label": 406
    },
    {
        "text": "secure bulk load for 'completebulkload' fails for version  secure bulk load with kerberos enabled fails for complete bulk loadloadincrementalhfile with following exception error org.apache.hadoop.hbase.regionserver.hregionserver:   org.apache.hadoop.hbase.ipc.hbaserpc$unknownprotocolexception: no matching handler for protocol org.apache.hadoop.hbase.security.access.securebulkloadprotocol in region t1,,1389699438035.28bb0284d971d0676cf562efea80199b.  at org.apache.hadoop.hbase.regionserver.hregion.exec(hregion.java)  at org.apache.hadoop.hbase.regionserver.hregionserver.execcoprocessor(hregionserver.java)  at sun.reflect.generatedmethodaccessor18.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java)  at java.lang.reflect.method.invoke(method.java)  at org.apache.hadoop.hbase.ipc.securerpcengine$server.call(securerpcengine.java)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java) ",
        "label": 236
    },
    {
        "text": "convert all tests that use hbasetestingutility createmultiregions to hba createtable  like i discussed in hbase-7534, hbasetestingutility.createmultiregions should disappear and not come back. there's about 25 different places in the code that rely on it that need to be changed the same way i changed testreplication. perfect for someone that wants to get started with hbase dev ",
        "label": 249
    },
    {
        "text": "invalid regions listed by regionserver jsp  the region list displayed by regionserver.jsp contains regions that have ceased existence due to splits. example:  region name encoded name start key end key  ...  maxentriestest,acacdk,1214292085212 732557990 acacdk   maxentriestest,acacdk,1214297936860 1583424516 acacdk acqtzk  maxentriestest,acacdk,1214293855954 1509492302 acacdk adhlxw  maxentriestest,acqtzk,1214297936862 1120286366 acqtzk adhlxw  maxentriestest,adhlxw,1214293855955 400707061 adhlxw   maxentriestest,adhlxw,1214299372674 2060549477 adhlxw aelrxo  maxentriestest,adhlxw,1214297324386 336026175 adhlxw afpxzs  maxentriestest,aelrxo,1214299372674 1352588233 aelrxo afpxzs  maxentriestest,afpxzs,1214297324387 1235754353 afpxzs ",
        "label": 218
    },
    {
        "text": " hbck  does not complain about regions with startkey endkey   hbck doesn't seem to complain or have an error condition if there is a region where startkey==endkey. ",
        "label": 248
    },
    {
        "text": "multithreaded table mapper analogous to multithreaded mapper in hadoop  there is no multithreadedtablemapper in hbase currently just like we have a multithreadedmapper in hadoop for io bound jobs.   usecase, webcrawler: take input (urls) from a hbase table and put the content (urls, content) back into hbase.   running these kind of hbase mapreduce job with normal table mapper is quite slow as we are not utilizing cpu fully (n/w io bound). moreover, i want to know whether it would be a good/bad idea to use hbase for these kind of usecases ?. ",
        "label": 222
    },
    {
        "text": " c  don't link jni jvm stuff into libhbaseclient  it doesn't seem like we should be linking the objects from the test-util directory into the library, as those [i believe] are just for testing. ",
        "label": 155
    },
    {
        "text": "all option for an commands of enable disable drop  problem 1. hql > help enable; syntax error : type 'help;' for usage. hql > help disable; syntax error : type 'help;' for usage. ..... ??? added feature 1. hql > disable all; hql > enable all; hql > drop all; ",
        "label": 152
    },
    {
        "text": "hbase default xml refers to hbase regionserver global memstore upperlimit which is deprecated  i noticed the following in log: 2014-05-27 16:50:46,978 warn  [main] hbase.hbaseconfiguration(78): hbase.regionserver.global.memstore.upperlimit is deprecated by hbase.regionserver.global.memstore.size hbase.regionserver.global.memstore.size should be used instead. ",
        "label": 441
    },
    {
        "text": "add compaction chaos monkey action  we can have a compaction chaos monkey action (major/minor). we can add this action to data ingestion integration tests. ",
        "label": 242
    },
    {
        "text": "require exec permission to call coprocessor endpoints  the exec action currently exists as only a placeholder in access control. it should really be used to enforce access to coprocessor endpoint rpc calls, which are currently unrestricted. how the acls to support this would be modeled deserves some discussion: should access be scoped to a specific table and coprocessorprotocol extension? should it be possible to grant access to a coprocessorprotocol implementation globally (regardless of table)? are per-method restrictions necessary? should we expose hooks available to endpoint implementors so that they could additionally apply their own permission checks? some cp endpoints may want to require read permissions, others may want to enforce write, or read + write. to apply these kinds of checks we would also have to extend the regionobserver interface to provide hooks wrapping hregion.exec(). ",
        "label": 38
    },
    {
        "text": "hregion internalobtainrowlock shouldn't wait forever  we just had a weird episode where one user was trying to insert a lot of data with overlapping keys into a single region (all of that is a separate problem), and the region server rapidly filled up all it's handlers + queues with those calls. basically it wasn't deadlocked but almost. worse, now that we have a 60 seconds socket timeout the clients were eventually getting the timeout and then retrying another call to that same region server. we should have a timeout on lockedrows.wait() in hregion.internalobtainrowlock in order to survive this better. ",
        "label": 441
    },
    {
        "text": "create metrics for per block type hit miss ratios  missing a root index block is worse than missing a data block. we should know the difference ",
        "label": 154
    },
    {
        "text": "create replication endpoint asynchronously when adding a replication source  as the discussion in hbase-19617, after the replication procedure replace the zookeeper notification , the addpeer operation may be blocked because the regionserver will create a connection to peer cluster synchronously. ",
        "label": 149
    },
    {
        "text": "typo in loaded coprocessors on master status page  the master status web ui page says:  \"coprocessors currently loaded loaded by the master\"  should be one \"loaded\" there. ",
        "label": 280
    },
    {
        "text": "fix javadoc warnings  we have hundreds of javadoc warnings emitted on every build. ",
        "label": 314
    },
    {
        "text": "storefile writer appendgeneralbloomfilter generates extra kv  accounts for 10% memory allocation in compaction thread when bloomfiltertype is rowcol. ",
        "label": 544
    },
    {
        "text": "deprecate check meta rb  we should depreate check_meta.rb and suggest users to use hbck instead.  hbck should give more accurate region hole information. should we remove check_meta.rb from both 92 and 94? ",
        "label": 248
    },
    {
        "text": "testhfileperformance and hfileperformanceevaluation should be merged in a single hfile performance test class   today testhfileperformance and hfileperformanceevaluation are doing slightly different kind of performance tests both for the hfile. we should consider merging those 2 tests in a single class. ",
        "label": 469
    },
    {
        "text": "hbase shell count command doesn't escape binary output  when running the the count command in the hbase shell, the row key is printed each time a count interval is reached. however, the key is printed verbatim, meaning that non-printable characters are directly printed to the terminal. this can cause confusing results, or even leave the terminal in an unusable state. ",
        "label": 178
    },
    {
        "text": "the fsync wal flag does not work on branch x  you can check the reference to syncfuture.isforcesync, no one calls it. and in fshlog, we always use the field usehsync instead of getting one from the syncfuture. i think the problem here is that we have done some optimization for batching the sync futures on branch-2.x so it is not easy to get the forcesync flag from a single syncfuture when doing sync. but anyway, we need to fix this. ",
        "label": 149
    },
    {
        "text": "use jacoco to generate unit test coverage reports  enabling the code coverage tool jacoco in maven ",
        "label": 302
    },
    {
        "text": "shell displaying uninformative exceptions  the shell seems to hang longer than the normal client and then display uninformative messages when doing wrong things. for example, inserting into a non-existing family or reading from a disabled table. i believe in both these cases, htable will throw informative exceptions like invalidfamilyexception and tabledisabledexception (need to confirm exactly what it does). but in the shell, i get things like: inserting to a family that does not exist: error: org.apache.hadoop.hbase.client.retriesexhaustedexception: still had 1 puts left after retrying 7 times. reading from a disabled table (this takes a long time before anything is displayed): error: org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server 10.0.0.4:62505 for region sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a., row '', but failed after 7 attempts. exceptions: org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:2221) at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:1812) at sun.reflect.generatedmethodaccessor4.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:557) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1007) org.apache.hadoop.hbase.client.regionofflineexception: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. org.apache.hadoop.hbase.client.regionofflineexception: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. org.apache.hadoop.hbase.client.regionofflineexception: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. org.apache.hadoop.hbase.client.regionofflineexception: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. org.apache.hadoop.hbase.client.regionofflineexception: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. org.apache.hadoop.hbase.client.regionofflineexception: region offline: sample,,1286217181143.1459f8fdde55752fd91022fb1153d39a. ",
        "label": 547
    },
    {
        "text": "master won't go down  stuck joined on rootscanner  here are the problem threads: \"regionmanager.rootscanner\" daemon prio=10 tid=0x00000000404a4800 nid=0x4d57 runnable [0x0000000042014000..0x0000000042014a80]    java.lang.thread.state: runnable     at org.apache.hadoop.hbase.chore.run(chore.java:58) \"hmaster\" prio=10 tid=0x0000000040644800 nid=0x4d56 in object.wait() [0x00000000415a4000..0x00000000415a4d00]    java.lang.thread.state: waiting (on object monitor)     at java.lang.object.wait(native method)     - waiting on <0x00007fd8975d00c0> (a org.apache.hadoop.hbase.master.rootscanner)     at java.lang.thread.join(thread.java:1143)     - locked <0x00007fd8975d00c0> (a org.apache.hadoop.hbase.master.rootscanner)     at java.lang.thread.join(thread.java:1196)     at org.apache.hadoop.hbase.master.regionmanager.stop(regionmanager.java:481)     at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:388) here is code at #58 in chore:   public void run() {     try {       while (!initialchore()) {         this.sleeper.sleep();       } ... looks like we sleep for ever. have to kill master. ",
        "label": 241
    },
    {
        "text": "utilize rowcol bloom filter if multiple columns within same family are requested in a get  noticed the following snippet in storefile.java:scanner:shouldseek():         switch(bloomfiltertype) {           case row:             key = row;             break;           case rowcol:             if (columns.size() == 1) {               byte[] col = columns.first();               key = bytes.add(row, col);               break;             }             //$fall-through$           default:             return true;         } if columns.size > 1, then we currently don't take advantage of the bloom filter. we should optimize this to check bloom for each of columns and if none of the columns are present in the bloom avoid opening the file. ",
        "label": 324
    },
    {
        "text": "test failures on trunk   arrayindexoutofboundsexception in o a h h regionserver hstore compact  i think hudson is still hung up on the failure in testfilter there.  [junit] test org.apache.hadoop.hbase.regionserver.testcompaction failed  [junit] test org.apache.hadoop.hbase.regionserver.testhregion failed  [junit] test org.apache.hadoop.hbase.regionserver.testsplit failed  [junit] test org.apache.hadoop.hbase.util.testmergetool failed same error on all but testmergetool: java.lang.arrayindexoutofboundsexception: 2  at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:872)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:725)  at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:682)  at org.apache.hadoop.hbase.regionserver.testhregion.splitandmerge(testhregion.java:518)  at org.apache.hadoop.hbase.regionserver.testhregion.testhregion(testhregion.java:63) for testmergetool: 'unit.framework.assertionfailederror: 'merging regions 0 and 1' failed  at org.apache.hadoop.hbase.util.testmergetool.mergeandverify(testmergetool.java:162)  at org.apache.hadoop.hbase.util.testmergetool.testmergetool(testmergetool.java:211) ",
        "label": 38
    },
    {
        "text": "hconnectionmanager should remove aborted connections  when an hconnection is abort()'ed (i.e. if numerous services are lost) the connection becomes unusable. hconnectionmanager cache of hconnections currently does not have any logic around removing aborted connections automatically. currently it is up to the consumer to do so using hconnectionmanager.deletestaleconnection(hconnection). ",
        "label": 85
    },
    {
        "text": "improve the current client api by creating new container classes  the current api does not scale very well. for each new feature, we have to add many methods to take care of all the overloads. also, the need to batch row operations (gets, inserts, deletes) implies that we have to manage some \"entities\" like we are able to do with batchupdate but not with the other operations. the rowlock should be an attribute of such an entity. the scope of this jira is only to replace current api with another feature-compatible one, other methods will be added in other issues. ",
        "label": 247
    },
    {
        "text": " book  dfs client read shortcircuit is referenced as hbase site xml config and not described in section  after trying to figure out whether shortcircuit reads would work on my system, i studied the book and found conflicting information. it's suggested in section 92.2, that dfs.client.read.shortcircuit is an option in hbase-site.xml, but the supposedly complete default configuration in section 7 does not include this setting. this leads to confusion on whether it's sufficient to enable this setting in hdfs-site.xml, or whether it needs to be added to both configurations. ",
        "label": 330
    },
    {
        "text": "we shouldn't be injecting 'killing  daemon ' into logs  when we aren't doing that   hbase-4209 changed the behavior of the scripts such that we do not kill the daemons away anymore. we should have also changed the message shown in the logs. ",
        "label": 194
    },
    {
        "text": "revisit hfilelink file name format   valid table names are concatted with a '.' to a valid regions names is also a valid table name, and lead to the incorrect interpretation. true hfile name constraints: [0-9]+(?:_seqid_[0-9]+)? region name constraints    : [a-f0-9]{16}  (but we currently just use [a-f0-9]+.) table name constraints     : [a-za-z0-9_][a-za-z0-9_.-]* notice that the table name constraints completely covers all region name constraints and true hfile name constraints. (a valid hfile name is a valid part of a table name, and a valid enc region name is a valid part of a table name. currently the hfilelink filename convention is <hfile><region><table>. unfortunately, making a ref to this uses the name <hfile><region><table>.<parentregion> \u2013 the contactnation of <table>.<parentregion> is a valid table name used to get interpreted as such. the fix in hbase-7339 requires a filenotfoundexception before going down the hfile link resolution path. regardless of what we do, we need to add some char invalid for table names to the hfilelink or reference filename convention. suggestion: if we changed the order of the hfile-link name we could avoid some of the confusion \u2013 <table>@<region>-<hfile>.<parentregion> (or some other separator char than '@') could be used to avoid handling on the initial filenotfoundexception but i think we'd still need a good chunk of the logic to handle opening half-storefile reader throw a hfilelink. ",
        "label": 309
    },
    {
        "text": "htable returned by metareader getmetahtable  is not closed in metaeditor addregiontometa   here is related code:   public static void addregiontometa(catalogtracker catalogtracker, hregioninfo regioninfo,       hregioninfo splita, hregioninfo splitb) throws ioexception {     addregiontometa(metareader.getmetahtable(catalogtracker), regioninfo, splita, splitb);   } htable returned by metareader#getmetahtable() is not closed in metaeditor#addregiontometa() ",
        "label": 309
    },
    {
        "text": "online configuration updates  currently, any changes to the configuration options: compaction tuning/etc. requires that we restart the regionserver. perhaps, some of the configs can be updated on the fly, without having to restart the region server. ",
        "label": 154
    },
    {
        "text": "only one worker in hrs  on startup  if assigned tens of regions  havoc of reassignments because open processing is done in series  on the lars clusters, he's up into the thousands of regions. starting this cluster, there is a load of churn in the master log as we assign regions, they report their opening and then after the hbase.hbasemaster.maxregionopen of one minute elapses, we assign the region elsewhere. problem seems to be the fact that we only run a single worker thread in our regionserver; means that region opens are processed in series. for example, the below shows when a master assigned a region and then the regionserver side log when it got around to opening it: 2008-03-29 04:48:51,638 info org.apache.hadoop.hbase.hmaster: assigning region pdc-docs,us20060158177_20060720,1205765009844 to server 192.168.105.19:60020 .. 2008-03-29 04:50:58,124 info org.apache.hadoop.hbase.hregionserver: msg_region_open : pdc-docs,us20060158177_20060720,1205765009844 there is > 2 minutes between the two loggings (i checked clocks on this cluster and they are synced). looking in the regionserver log, its just filled with logging on the opening of regions. the region opens are running pretty fast at about a second each but there are hundreds of regions to open in this case so its easy to go over our default of 60 seconds. ",
        "label": 314
    },
    {
        "text": "testrestoreflushsnapshotfromclient and testrestoresnapshotfromclient fail to finish occasionally  looking at the test code i see htable table = new htable(...) try {    ... test code ... } finally {   table.close(); } the try-finally, while well meaning seems to hide the actual failure.  i'll just remove that. ",
        "label": 286
    },
    {
        "text": "coprocessors  colocate user code with regions  support user code that runs run next to each region in table. as regions split and move, coprocessor code should automatically move also. use classloader which looks on hdfs. associate a list of classes to load with each table. put this in hri so it inherits from table but can be changed on a per region basis (so then those region specific changes can inherited by daughters). not completely arbitrary code, should require implementation of an interface with callbacks for: open close split compact (multi)get and scanner next() (multi)put (multi)delete add method to htableinterface for invoking coprocessor methods and retrieving results. add methods in o.a.h.h.regionserver or subpackage which implement convenience functions for coprocessor methods and consistent/controlled access to internals: store access, threading, persistent and ephemeral state, scratch storage, etc. github: https://github.com/trendmicro/hbase/tree/coprocessor please see the latest attached package-info.html for updated description. ",
        "label": 327
    },
    {
        "text": "add to download page  ",
        "label": 149
    },
    {
        "text": "purge deprecated apis  and code   making a place-holder to remove deprecated apis. could get rid of some code too. ",
        "label": 248
    },
    {
        "text": "remove deprecated methods in result  the result class defines the deprecated ispartial() method, which should be removed. for now it should be for 3.0.0 only. ",
        "label": 398
    },
    {
        "text": "add on protobuf to c  chain  we have protobufs.  we need c++ libraries. ",
        "label": 154
    },
    {
        "text": " findbugs  exclude thrift and protobuf warnings  exclude thrift and protobuf warnings since these are machine generated. ",
        "label": 458
    },
    {
        "text": "'hbasefsck numthreads' property isn't passed to hbck via cmdline  d option  we use generic option way to pass 'hbasefsck.numthreads' property to 'hbase hbck', but it does not accept our new setting value hbase hbck -d hbasefsck.numthreads=5 we can still find there are threads over than 5 we already set via generic opttion [2013-10-24 09:25:02,561][pool-2-thread-6][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,562][pool-2-thread-10][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,565][pool-2-thread-13][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,566][pool-2-thread-11][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,567][pool-2-thread-9][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,568][pool-2-thread-12][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,570][pool-2-thread-7][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) [2013-10-24 09:25:02,571][pool-2-thread-14][debug][org.apache.hadoop.security.usergroupinformation]: privilegedaction as:hbase/spn-d-hdn1.sjdc@ispn.trendmicro.com (auth:kerberos) from:sun.reflect.nativemethodaccessorimpl.invoke0(native method) (usergroupinformation.java:1430) ",
        "label": 551
    },
    {
        "text": "making region assignment more robust  from eran kutner:  my concern is that the region allocation process seems to rely too much on  timing considerations and doesn't seem to take enough measures to guarantee  conflicts do not occur. i understand that in a distributed environment, when  you don't get a timely response from a remote machine you can't know for  sure if it did or did not receive the request, however there are things that  can be done to mitigate this and reduce the conflict time significantly. for  example, when i run dbck it knows that some regions are multiply assigned,  the master could do the same and try to resolve the conflict. another  approach would be to handle late responses, even if the response from the  remote machine arrives after it was assumed to be dead the master should  have enough information to know it had created a conflict by assigning the  region to another server. an even better solution, i think, is for the rs to  periodically test that it is indeed the rightful owner of every region it  holds and relinquish control over the region if it's not.  obviously a state where two rss hold the same region is pathological and can  lead to data loss, as demonstrated in my case. the system should be able to  actively protect itself against such a scenario. it probably doesn't need  saying but there is really nothing worse for a data storage system than data  loss. in my case the problem didn't happen in the initial phase but after  disabling and enabling a table with about 12k regions. for more background information, see 'errors after major compaction' discussion on user@hbase.apache.org ",
        "label": 544
    },
    {
        "text": "major compact should be done when there is only one storefile and some keyvalue is outdated   in the function store.ismajorcompaction:  if (filestocompact.size() == 1) {  // single file  storefile sf = filestocompact.get(0);  long oldest =  (sf.getreader().timerangetracker == null) ?  long.min_value :  now - sf.getreader().timerangetracker.minimumtimestamp;  if (sf.ismajorcompaction() &&  (this.ttl == hconstants.forever || oldest < this.ttl)) {  if (log.isdebugenabled()) { log.debug(\"skipping major compaction of \" + this.storenamestr + \" because one (major) compacted file only and oldesttime \" + oldest + \"ms is < ttl=\" + this.ttl); } }  } else {  when there is only one storefile in the store, and some keyvalues' ttl are overtime, the majorcompactchecker should send this region to the compactquene and run a majorcompact to clean these outdated data. but according to the code in 0.90.1, it will do nothing. ",
        "label": 562
    },
    {
        "text": "have snappy support properly documented would be helpful to hadoop and hbase users  the currentl document for configuring snappy support(http://hbase.apache.org/book/snappy.compression.html) is not complete and it's a bit obscure. imo, there are several improvments can be made:  1. describe the relationship among hadoop,hbase,snappy. is the snappy actually needed by hadoop hdfs or hbase itself? that's to make clear if you need to configure snappy support in hbase or hadoop.  2. it didn't mention the default hadoop binary package is compiled without snappy support and you need to compile it with snappy option manually. actually it didn't work with any native libs on 64 bits os as the libhadoop.so in the binary package is only for 32 bits os(this of course is a hadoop issue not hbase. but it's good to mention it.).  3. in my experience, i actually need to install both snappy and hadoop-snappy. so the doc lack of the steps to install hadoop-snappy.   4. during my set up, i found difference where hadoop and hbase to pick up the native lib files. hadoop picks those files in ./lib while hbase picks in ./lib/[platform]. if it's correct, it can also be mentioned. ",
        "label": 330
    },
    {
        "text": "make compaction to use pread instead of sequential read  as we discovered lately, hfile compactions use sequential reads to fetch blocks. it cause unwanted streaming of data from hdfs to region servers when there is a cache hit. lets change to use preads to reduce iops on disks. ",
        "label": 378
    },
    {
        "text": "deprecate keyvalue getbuffer  make the deprecation a subtask of the parent. let the parent stand as an umbrella issue. ",
        "label": 314
    },
    {
        "text": "add bigdecimalcolumninterpreter for doing aggregations using aggregationclient  i recently created a class for doing aggregations(sum,min,max,std) on values stored as bigdecimal in hbase. i would like to commit the bigdecimalcolumninterpreter into hbase. in my opinion this class can be used by a wide variety of users. please let me know if its not appropriate to add this class in hbase. thanks,  anil gupta  software engineer ii, intuit, inc ",
        "label": 43
    },
    {
        "text": "regionserver should abort when wal close encounters an error with unflushed edits  the ability to ride over wal close errors on log rolling added in hbase-4222 could lead to missing hlog entries if: a table has deferred_log_flush=true there are unflushed waledit entries for that table in the current sequencefile writer buffer since the writes were already acknowledged to the client, just ignoring the close error to allow for another log roll doesn't seem like the right thing to do here. we could easily flag this state and only ride over the close error if there aren't unflushed entries. this would bring the above condition back to the previous behavior of aborting the region server. however, aborting the region server in this state is still guaranteeing data loss. is there anything we can do better in this case? ",
        "label": 180
    },
    {
        "text": "add backup cli option to hmaster  the hmaster main() should allow a toggle like --backup, which forces it to be a secondary master on startup versus a primary candidate. that way, we can start up multiple masters at once and deterministically know which one will be the original primary. ",
        "label": 341
    },
    {
        "text": "upgrade jackson to version due to cve and cve  a polymorphic typing issue was discovered in fasterxml jackson-databind before 2.9.10. it is related to com.zaxxer.hikari.hikaridatasource. this is a different vulnerability than cve-2019-14540.  https://nvd.nist.gov/vuln/detail/cve-2019-16335  a polymorphic typing issue was discovered in fasterxml jackson-databind before 2.9.10. it is related to com.zaxxer.hikari.hikariconfig.  https://nvd.nist.gov/vuln/detail/cve-2019-14540 ",
        "label": 337
    },
    {
        "text": "update hadoop in lib on hbase branch to  ",
        "label": 241
    },
    {
        "text": "compression tool section is referring to wrong link in hbase book   http://hbase.apache.org/book/ops_mgt.html#compression.tool  above section is refering to itself (recursive) in hbase book.  this needs to be corrected. ",
        "label": 146
    },
    {
        "text": "loadincrementalhfiles does not return an error code nor throw exception when failures occur due to timeouts  the loadincrementalhfiles (completebulkload) command will exit with a success code (or lack of exception) when one or more of the hfiles fail to be imported through a few ways (mainly when timeouts occur). instead, it simply logs error messages to the log which makes it difficult to automate the import of hfiles programmatically. the heart of the loadincrementalhfiles class (dobulkload) returns void and has essentially the following structure. loadincrementalhfiles.java try {       ...       } finally {       pool.shutdown();       if (queue != null && !queue.isempty()) {         stringbuilder err = new stringbuilder();         err.append(\"-------------------------------------------------\\n\");         err.append(\"bulk load aborted with some files not yet loaded:\\n\");         err.append(\"-------------------------------------------------\\n\");         for (loadqueueitem q : queue) {           err.append(\"  \").append(q.hfilepath).append('\\n');         }         log.error(err);       }     } as you can see, instead of returning an error code, a success indicator, or simply throwing an exception, an error message is sent to the log. this results in something like the following in the logs. bulk load aborted with some files not yet loaded:  -------------------------------------------------  hdfs://prmdprod/user/userxxx/hfile/table-1365721885510/record/_tmp/table,2.bottom  hdfs://prmdprod/user/userxxx/hfile/table-1365721885510/record/_tmp/table,2.top  hdfs://prmdprod/user/userxxx/hfile/table-1365721885510/record/_tmp/table,1.bottom  hdfs://prmdprod/user/userxxx/hfile/table-1365721885510/record/_tmp/table,1.top without some sort of indication, it's not currently possible to chain this command to another or to programmatically consume this class and be certain of a successful import. this class should be enhanced to return non-success in whatever way makes sense to the community. i don't really have a strong preference, but one of the following should work fine (at least for my needs). boolean return value on dobulkload (non-zero on run method) response object on dobulkload detailing the files that failed (non-zero on run method) throw exception in the finally block when files failed after the error is written to the log (should automatically cause non-zero on run method) it would also be nice to get this to the 0.94.x stream so it get included in the next cloudera release. thanks! ",
        "label": 80
    },
    {
        "text": "htable getrow text  does not work  updated from svn to find that hbase.getrow(text) always return empty map. ",
        "label": 86
    },
    {
        "text": "excessive readpoint checks in memstorescanner  brought up by vladimir rodionov on the mailing list. see also hbase-9751. ",
        "label": 441
    },
    {
        "text": "refactor checkandput and checkanddelete to use dominibatchmutation  currently the checkandput and checkanddelete api internally calls the internalput and internaldelete. may be we can just call dominibatchmutation  only. this will help in future like if we have some hooks and the cp  handles certain cases in the dominibatchmutation the same can be done while  doing a put thro checkandput or while doing a delete thro checkanddelete. ",
        "label": 360
    },
    {
        "text": "hbase failsafe broke mvn site  back it out or fix  mvn site is broke in head of trunk. if i back out the last pom change it works again: ------------------------------------------------------------------------ r1177168 | stack | 2011-09-29 05:42:13 +0000 (thu, 29 sep 2011) | 1 line hbase-4454 add failsafe plugin to build and rename integration tests ",
        "label": 11
    },
    {
        "text": "refactor storefile code  currently, the storefile code is a thin wrapper around an hfile.reader. with the addition of bloomfilters and other features that operate at the hfile layer, we need to clarify the difference between a storefile & hfile. to that end, we need to refactor the storefile.reader code and the code that inter-operates with it. ",
        "label": 547
    },
    {
        "text": "modify pom and jenkins jobs for hadoop versions  ",
        "label": 149
    },
    {
        "text": "lucenedocumentwrapper needs no argument constructor  for reading an object out a sequencefile, a no argument constructor is required...   if not you end up with a java.lang.nosuchmethodexception from owen o'malley assumption for writables that should be documented somewhere: each type must have a 0 argument constructor. each call to write must not assume any shared state. each call to readfields must consume exactly the number of bytes  produced by write. sequencefile also assumes: all keys are exactly the same type (not polymorphic). all values are exactly the same type. both types are specified by the writer in the create call. ",
        "label": 266
    },
    {
        "text": "hanging test   org apache hadoop hbase mapreduce testimportexport  this test hangs a bunch: here is latest: https://builds.apache.org/job/hbase-1.2/418/jdk=latest1.7,label=hadoop/consoletext ",
        "label": 198
    },
    {
        "text": "allow include and next col in filters and use it in columnpaginationfilter  there are various usecases and filter types where evaluating the filter before version are handled either do not make sense, or make filter handling more complicated. also see this comment in scanquerymatcher:     /**      * filters should be checked before checking column trackers. if we do      * otherwise, as was previously being done, columntracker may increment its      * counter for even that kv which may be discarded later on by filter. this      * would lead to incorrect results in certain cases.      */ so we had filters after the column trackers (which do the version checking), and then moved it.  should be at the discretion of the filter.  could either add a new method to filterbase (maybe excludeversions() or something). or have a new filter wrapper (like whilematchfilter), that should only be used as outmost filter and indicates the same (maybe excludeversionsfilter). see latest comments on hbase-5229 for motivation. ",
        "label": 463
    },
    {
        "text": "the compaction writer may access the lastcell whose memory has been released when appending fileinfo in the final  copy the comment from chenxu under hbase-21879: https://issues.apache.org/jira/browse/hbase-21879?focusedcommentid=16862244&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16862244 in compactor#compact, we have the following: protected list<path> compact(final compactionrequest request...   ...   try {     ...   } finally {     closeables.close(scanner, true);     if (!finished && writer != null) {       abortwriter(writer);     }   }   assert finished : \"we should have exited the method on all error paths\";   assert writer != null : \"writer should be non-null if no error\";   return commitwriter(writer, fd, request); } should we call writer#beforeshipped() before closeables.close(scanner, true);  in order to copy some cell's data out of the bytebuff before it released, or commitwriter may be wrong in the following call stack compactor#commitwriter     \uff0d> hfilewriterimpl#close          \uff0d> hfilewriterimpl#writefileinfo                \uff0d> hfilewriterimpl#finishfileinfo protected void finishfileinfo() throws ioexception {   if (lastcell != null) {     // make a copy. the copy is stuffed into our fileinfo map. needs a clean     // byte buffer. won't take a tuple.     byte [] lastkey = privatecellutil.getcellkeyserializedaskeyvaluekey(this.lastcell);     fileinfo.append(fileinfo.lastkey, lastkey, false);   }   ... } because the lastcell may refer to a reused bytebuff. checked the code, it's a bug and will need to fix in all 2.x & master branch. ",
        "label": 514
    },
    {
        "text": "hbase load balancer needs locality awareness  previously, hbase-4114 implements the metrics for hfile hdfs block locality, which provides the hfile level locality information.  but in order to work with load balancer and region assignment, we need the region level locality information. let's define the region locality information first, which is almost the same as hfile locality index. hregion locality index (hregion a, regionserver b) =   (total number of hdfs blocks that can be retrieved locally by the regionserver b for the hregion a) / ( total number of the hdfs blocks for the region a)  so the hregion locality index tells us that how much locality we can get if the hmaster assign the hregion a to the regionserver b. so there will be 2 steps involved to assign regions based on the locality.  1) during the cluster start up time, the master will scan the hdfs to calculate the \"hregion locality index\" for each pair of hregion and region server. it is pretty expensive to scan the dfs. so we only needs to do this once during the start up time. 2) during the cluster run time, each region server will update the \"hregion locality index\" as metrics periodically as hbase-4114 did. the region server can expose them to the master through zk, meta table, or just rpc messages. based on the \"hregion locality index\", the assignment manager in the master would have a global knowledge about the region locality distribution and can run the min cost maximum flow solver to reach the global optimization. let's construct the graph first:  [graph]  imaging there is a bipartite graph and the left side is the set of regions and the right side is the set of region servers.  there is a source node which links itself to each node in the region set.   there is a sink node which is linked from each node in the region server set. [capacity]  the capacity between the source node and region nodes is 1.  and the capacity between the region nodes and region server nodes is also 1.  (the purpose is each region can only be assigned to one region server at one time) the capacity between the region server nodes and sink node are the avg number of regions which should be assigned each region server.  (the purpose is balance the load for each region server) [cost]  the cost between each region and region server is the opposite of locality index, which means the higher locality is, if region a is assigned to region server b, the lower cost it is.  the cost function could be more sophisticated when we put more metrics into account. so after running the min-cost max flow solver, the master could assign the regions based on the global locality optimization. also the master should share this global view to secondary master in case the master fail over happens.  in addition, the hbase-4491 (locality checker) is the tool, which is based on the same metrics, to proactively to scan dfs to calculate the global locality information in the cluster. it will help us to verify data locality information during the run time. ",
        "label": 294
    },
    {
        "text": "add hadoop profile  hadoop 1.2.0 is beta release. we should add profile in pom.xml for this release. ",
        "label": 441
    },
    {
        "text": "splits can create temporary holes in  meta  that confuse clients and regionservers  when a splittransaction is performed, three updates are done to .meta.:  1. the parent region is marked as splitting (and hence offline)  2. the first daughter region is added (same start key as parent)  3. the second daughter region is added (split key is start key)  (later, the original parent region is deleted, but that's not important to this discussion) steps 2 and 3 are actually done concurrently by splittransaction.daughteropener threads. while the master is notified when a split is complete, the only visibility that clients have is whether the daughter regions have appeared in .meta. if the second daughter is added to .meta. first, then .meta. will contain the (offline) parent region followed by the second daughter region. if the client looks up a key that is greater than (or equal to) the split, the client will find the second daughter region and use it. if the key is less than the split key, the client will find the parent region and see that it is offline, triggering a retry. if the first daughter is added to .meta. before the second daughter, there is a window during which .meta. has a hole: the first daughter effectively hides the parent region (same start key), but there is no entry for the second daughter. a region lookup will find the first daughter for all keys in the parent's range, but the first daughter does not include keys at or beyond the split key. see hbase-4333 and hbase-4334 for details on how this causes problems and suggestions for mitigating this in the client and regionserver. ",
        "label": 286
    },
    {
        "text": "add changelog  readme and releasenotes to binary tarball  the binary tarball of hbase-connectors does not contain changelog.md, readme.md and releasenotes.md files. this issue came up on the [vote] hbase connectors 1.0.0rc0 thread. ",
        "label": 51
    },
    {
        "text": "correct comments for default values of major compaction in sortedcompactionpolicy getnextmajorcompacttime   hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/sortedcompactionpolicy.java public long getnextmajorcompacttime(collection<hstorefile> filestocompact) { {     // default = 24hrs    long period = comconf.getmajorcompactionperiod();     ...     // default = 20% = +/- 4.8 hrs     double jitterpct = comconf.getmajorcompactionjitter();     ... } if i get it correctly, currently, default major compaction period is 7 days\uff0cand jitter is 0.5, so 3.5 days.  we could either remove those comments, or add {@link} to link to the default value definitions. ",
        "label": 490
    },
    {
        "text": "add bloomfilters  add bloomfiltering to hfile. can be enabled on a family-level basis. ability to configure a row vs row+col level bloom. we size the bloomfilter with the number of entries we are about to flush which seems like usually we'd be making a filter too big, so our implementation needs to take that into account. ",
        "label": 341
    },
    {
        "text": "hbase store delete expired storefile should be true by default  hbase-5199 introduces this logic into store: +      // delete the expired store files before the compaction selection. +      if (conf.getboolean(\"hbase.store.delete.expired.storefile\", false) +          && (ttl != long.max_value) && (this.scaninfo.minversions == 0)) { +        compactselection expiredselection = compactselection +            .selectexpiredstorefilestocompact( +                environmentedgemanager.currenttimemillis() - this.ttl); + +        // if there is any expired store files, delete them  by compaction. +        if (expiredselection != null) { +          return expiredselection; +        } +      } is there any reason why that should not be default true? ",
        "label": 38
    },
    {
        "text": "add a method to fsdelegationtoken to accept token kind  the acquiredelegationtoken method [1] defaults to checking for delegation token of kind \"hdfs_delegation_token\" before fetching it from the filesystem. it would be helpful to have a method that accepts the token kind and fetches delegation token from userprovider for that token kind. [1] - https://github.com/apache/hbase/blob/rel/2.1.4/hbase-server/src/main/java/org/apache/hadoop/hbase/security/token/fsdelegationtoken.java#l67  ",
        "label": 465
    },
    {
        "text": "testregionobserverinterface inithregion not invoked in tests  up on the mailing lists jason reports: on mon, mar 14, 2011 at 6:57 am, jason rutherglen <jason.rutherglen@gmail.com> wrote: > in building a coprocessor test case, i noticed that > testregionobserverinterface.inithregion isn't called within the > containing class. > i'm not sure how its supposed to be called. ",
        "label": 327
    },
    {
        "text": "hbase can get stuck if updates to meta are blocked  (we noticed this on a import-style test in a small test cluster.) if compactions are running slow, and we are doing a lot of region splits, then, since meta has a much smaller hard-coded memstore flush size (16kb), it quickly accumulates lots of store files. once this exceeds \"hbase.hstore.blockingstorefiles\", flushes to meta become no-ops. this causes metas memstore footprint to grow. once this exceeds \"hbase.hregion.memstore.block.multiplier * 16kb\", we block further updates to meta. in my test setup:  hbase.hregion.memstore.block.multiplier = 4.  and,  hbase.hstore.blockingstorefiles = 15. and we saw messages of the form: 2010-04-09 18:37:39,539 info org.apache.hadoop.hbase.regionserver.hregion: blocking updates for 'ipc server handler 23 on 60020' on region .meta.,,1: memstore size 64.2k is >= than blocking 64.0k size now, if around the same time the compactsplitthread does a compaction and determines it is going split the region. as part of finishing the split, it wants to update meta about the daughter regions. it'll end up waiting for the meta to become unblocked. the single compactsplitthread is now held up, and no further compactions can proceed. meta's compaction request is itself blocked because the compaction queue will never get cleared. this essentially creates a deadlock and the region server is able to not progress any further. eventually, each region server's compactsplitthread ends up in the same state. ",
        "label": 263
    },
    {
        "text": "java lang abstractmethoderror in hbase rest server  just run it this one while testing some scripts. basically any call to service will end up with 500 error. after some checking it looks like we have some issues with dependencies incompatibility.   here is more details: stack trace: 2017-08-02 20:46:25,407 warn  [qtp422330142-30] servlet.servlethandler: error for /status/cluster java.lang.abstractmethoderror: javax.ws.rs.core.uribuilder.uri(ljava/lang/string;)ljavax/ws/rs/core/uribuilder;         at javax.ws.rs.core.uribuilder.fromuri(uribuilder.java:119)         at org.glassfish.jersey.servlet.servletcontainer.service(servletcontainer.java:298)         at org.glassfish.jersey.servlet.servletcontainer.service(servletcontainer.java:228)         at org.eclipse.jetty.servlet.servletholder.handle(servletholder.java:845)         at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1689)         at org.apache.hadoop.hbase.rest.filter.gzipfilter.dofilter(gzipfilter.java:77)         at org.eclipse.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1676)         at org.eclipse.jetty.servlet.servlethandler.dohandle(servlethandler.java:581)         at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:143)         at org.eclipse.jetty.security.securityhandler.handle(securityhandler.java:548)         at org.eclipse.jetty.server.session.sessionhandler.dohandle(sessionhandler.java:226)         at org.eclipse.jetty.server.handler.contexthandler.dohandle(contexthandler.java:1160)         at org.eclipse.jetty.servlet.servlethandler.doscope(servlethandler.java:511)         at org.eclipse.jetty.server.session.sessionhandler.doscope(sessionhandler.java:185)         at org.eclipse.jetty.server.handler.contexthandler.doscope(contexthandler.java:1092)         at org.eclipse.jetty.server.handler.scopedhandler.handle(scopedhandler.java:141)         at org.eclipse.jetty.server.handler.handlerwrapper.handle(handlerwrapper.java:134)         at org.eclipse.jetty.server.server.handle(server.java:518)         at org.eclipse.jetty.server.httpchannel.handle(httpchannel.java:308)         at org.eclipse.jetty.server.httpconnection.onfillable(httpconnection.java:244)         at org.eclipse.jetty.io.abstractconnection$readcallback.succeeded(abstractconnection.java:273)         at org.eclipse.jetty.io.fillinterest.fillable(fillinterest.java:95)         at org.eclipse.jetty.io.selectchannelendpoint$2.run(selectchannelendpoint.java:93)         at org.eclipse.jetty.util.thread.strategy.executeproduceconsume.produceandrun(executeproduceconsume.java:246)         at org.eclipse.jetty.util.thread.strategy.executeproduceconsume.run(executeproduceconsume.java:156)         at org.eclipse.jetty.util.thread.queuedthreadpool.runjob(queuedthreadpool.java:654)         at org.eclipse.jetty.util.thread.queuedthreadpool$3.run(queuedthreadpool.java:572)         at java.lang.thread.run(thread.java:745) and here are suspects from lib dir $ grep -r \"uribuilder\" . binary file ./javax.ws.rs-api-2.0.1.jar matches binary file ./jersey-common-2.25.1.jar matches binary file ./jersey-core-1.9.jar matches i have also checked hbase-1.2.6 we have only jersey-core-1.9.jar there ",
        "label": 393
    },
    {
        "text": "provide strong authentication with a secure rpc engine  the hbase rpc code (org.apache.hadoop.hbase.ipc.*) was originally forked off of hadoop rpc classes, with some performance tweaks added. those optimizations have come at a cost in keeping up with hadoop rpc changes however, both bug fixes and improvements/new features. in particular, this impacts how we implement security features in hbase (see hbase-1697 and hbase-2016). the secure hadoop implementation (hadoop-4487) relies heavily on rpc changes to support client authentication via kerberos and securing and mutual authentication of client/server connections via sasl. making use of the built-in hadoop rpc classes will gain us these pieces for free in a secure hbase. so, i'm proposing that we drop the hbase forked version of rpc and convert to direct use of hadoop rpc, while working to contribute important fixes back upstream to hadoop core. based on a review of the hbase rpc changes, the key divergences seem to be: hbaseclient: added use of tcp keepalive (hbase-1754) made connection retries and sleep configurable (hbase-1815) prevent npe if socket == null due to creation failure (hbase-2443) hbaserpc: mapping of method names <-> codes (removed in hbase-2219) hbaseserver: use of tcp keep alives (hbase-1754) oome in server does not trigger abort (hbase-1198) hbaseobjectwritable: allows list<> serialization includes it's own class <-> code mapping (hbase-328) proposed process is: 1. open issues with patches on hadoop core for important fixes/adjustments from hbase rpc (hbase-1198, hbase-1815, hbase-1754, hbase-2443, plus a pluggable objectwritable implementation in rpc.invocation to allow use of hbaseobjectwritable). 2. ship a hadoop version with rpc patches applied \u2013 ideally we should avoid another copy-n-paste code fork, subject to ability to isolate changes from impacting hadoop internal rpc wire formats 3. if all hadoop core patches are applied we can drop back to a plain vanilla hadoop version i realize there are many different opinions on how to proceed with hbase rpc, so i'm hoping this issue will kick off a discussion on what the best approach might be. my own motivation is maximizing re-use of the authentication and connection security work that's already gone into hadoop core. i'll put together a set of patches around #1 and #2, but obviously we need some consensus around this to move forward. if i'm missing other differences between hbase and hadoop rpc, please list as well. discuss! ",
        "label": 180
    },
    {
        "text": "testhfilebackedbybucketcache is flakey since it went in  looks like cache content changes during running of test... let me fix. critical. ",
        "label": 314
    },
    {
        "text": "deprecate and remove the avro gateway  deprecate the avro gateway in 0.94. remove in 0.96. made a blocker against that release. ",
        "label": 154
    },
    {
        "text": "fsutils isinsafemode  checks should operate on hbase root dir  where we have permissions  the hdfs safe mode check workaround introduced by hbase-4510 performs a filesystem.setpermission() operation on the root directory (\"/\") when attempting to trigger a safemodeexception. as a result, it requires superuser privileges when running with dfs permission checking enabled. changing the operations to act on the hbase root directory should be safe, since the master process must have write access to it. ",
        "label": 180
    },
    {
        "text": "lru statistics thread should be daemon  here was from 'hbase 0.92/hadoop 0.22 test results' discussion on dev@hbase \"lru statistics #0\" prio=10 tid=0x00007f4edc7dd800 nid=0x211a waiting on condition [0x00007f4e631e2000]   java.lang.thread.state: timed_waiting (parking)        at sun.misc.unsafe.park(native method)        - parking to wait for  <0x00007f4e88acc968> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)        at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:198)        at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.awaitnanos(abstractqueuedsynchronizer.java:2025)        at java.util.concurrent.delayqueue.take(delayqueue.java:164)        at java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:583)        at java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:576)        at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)        at java.lang.thread.run(thread.java:619) we should make this thread daemon thread. ",
        "label": 38
    },
    {
        "text": "per cell ttl tags may get duplicated with increments append causing tags length overflow  2017-04-29 14:24:14,135 error [b.fifo.qrpcserver.handler=49,queue=1,port=16020] ipc.rpcserver: unexpected throwable object java.lang.illegalstateexception: invalid currtagslen -32712. block offset: 3707853, block length: 72841, position: 0 (without header). at org.apache.hadoop.hbase.io.hfile.hfilereaderv3$scannerv3.checktagslen(hfilereaderv3.java:226) i am not not using any hbase tags feature.  the increment operation from the application side is triggering this error. the same is happening when scanner is run on this table. it feels that one or more particular hfile block is corrupt (with negative taglength). hbase(main):007:0> scan 'table-name', {limit=>1,startrow=>'ad:event_count:a'} returning the result hbase(main):008:0> scan 'table-name', {limit=>1,startrow=>'ad:event_count:b'} row column+cell   error: java.io.ioexception: java.lang.illegalstateexception: invalid currtagslen -32701. block offset: 272031, block length: 72441, position: 32487 (without header).  at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.handleexception(hregion.java:5607)  at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.<init>(hregion.java:5579)  at org.apache.hadoop.hbase.regionserver.hregion.instantiateregionscanner(hregion.java:2627)  at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:2613)  at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:2595)  at org.apache.hadoop.hbase.regionserver.rsrpcservices.scan(rsrpcservices.java:2282)  at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32295) ",
        "label": 46
    },
    {
        "text": "extra commas in lrublockcache logstats  the stats log line for the lrublockcache contains extra commas introduced in hbase-5616: 2013-04-23 18:40:12,774 debug [lru statistics #0] org.apache.hadoop.hbase.io.hfile.lrublockcache: stats: total=9.23 mb, free=500.69 mb, max=509.92 mb, blocks=95, accesses=322822, hits=107003, hitratio=33.14%, , cachingaccesses=232794, cachinghits=106994, cachinghitsratio=45.96%, , evictions=0, evicted=12, evictedperrun=infinity marking as \"noob\" ",
        "label": 314
    },
    {
        "text": "action should not store or serialize regionname  action stores the regionname, but an action comes from a multiaction, which contains:  public map<byte[], list<action<r>>> actions which means we are storing the regionname multiple times. in fact, no one even calls the accessor getregionname! it changes the serialization of action and multiaction, but reduces the byte overhead. ",
        "label": 441
    },
    {
        "text": "add document for  hbase  hbtop  top n heavy hitter user and client drill downs   add the new feature of hbtop in hbase-23065 to the ref guide. ",
        "label": 455
    },
    {
        "text": "puts of region location to meta may be out of order which causes inconsistent of region location  in product hbase cluster, we found inconsistency of region location in the meta table. region cdfa2ed711bbdf054d9733a92fd43eb5 is onlined in regionserver 10.237.12.13:11600 but the region location in meta table is 10.237.12.15:11600.  this is because of the out-of-order puts for meta table. 1. hmaster try to assign the region to 10.237.12.15:11600. 2. regionserver: 10.237.12.15:11600. during the opening the region, the put of region location(10.237.12.15:11600) to meta table is timeout(60s) and the htable retry for second time. (regionserver serving meta has got the request of the put. the timeout is beause ther is a bad disk in this regionserver and sync of hlog is very slow.  )  during the retry in htable, the openregionhandler is timeout(100s) and the postopendeploytasksthread is interrupted. through the htable is closed in the metaeditor finally, the share connection the htable used is not closed and the call of put for meta table is on-flying in the connection. assumed that this on-flying call of put to meta is named call a. 3. regionserver: 10.237.12.15:11600. for the timeout of openregionhandler, the openregionhandler marks the assign state of this region to failed_open. 4. hmaster watchs this event of failed_open and assigns the region to another regionserver: 10.237.12.13:11600 5. regionserver: 10.237.12.13:11600. this regionserver opens the region successfully . assumed that the put of region location(10.237.12.13:11600) to meta table in this regionserver is named b. there is no order guarantee for call a and b. if call a is processed after call b in regionserver serving meta region, the region location in meta table will be wrong. from the raw scan of meta table we found: scan '.meta.', {raw => true, limit => 1, versions => 10, startrow => 'xxx.adfa2ed711bbdf054d9733a92fd43eb5.'}  xxx.adfa2ed711bbdf054d9733a92fd43eb5. column=info:server, timestamp=1404885460553(=> wed jul 09 13:57:40 +0800 2014), value=10.237.12.15:11600 --> retry put from 10.237.12.15 xxx.adfa2ed711bbdf054d9733a92fd43eb5. column=info:server, timestamp=1404885456731(=> wed jul 09 13:57:36 +0800 2014), value=10.237.12.13:11600 --> put from 10.237.12.13 xxx.adfa2ed711bbdf054d9733a92fd43eb5. column=info:server, timestamp=1404885353122( wed jul 09 13:55:53 +0800 2014), value=10.237.12.15:11600 --> first put from 10.237.12.15 related hbase log is attached in this issue and disscusions are welcomed. for there is no order guarantee for puts from different htables, one solution for this issue is to give an increased id for each assignment of a region and use this id as the timestamp of put of region location to meta table. the region location with large assign id will be got by hbase clients. ",
        "label": 411
    },
    {
        "text": "operations using unsafe path broken for platforms not having sun misc unsafe  hbase crashes in standalone mode with the following log:  __________________________________________________________  2016-02-24 22:38:37,578 error [main] master.hmastercommandline: master exiting  java.lang.runtimeexception: failed construction of master: class org.apache.hadoop.hbase.master.hmaster  at org.apache.hadoop.hbase.master.hmaster.constructmaster(hmaster.java:2341)  at org.apache.hadoop.hbase.master.hmastercommandline.startmaster(hmastercommandline.java:233)  at org.apache.hadoop.hbase.master.hmastercommandline.run(hmastercommandline.java:139)  at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)  at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:126)  at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:2355)  caused by: java.lang.noclassdeffounderror: could not initialize class org.apache.hadoop.hbase.util.bytes$lexicographicalcomparerholder$unsafecomparer  at org.apache.hadoop.hbase.util.bytes.putint(bytes.java:899)  at org.apache.hadoop.hbase.keyvalue.createbytearray(keyvalue.java:1082)  at org.apache.hadoop.hbase.keyvalue.<init>(keyvalue.java:652)  at org.apache.hadoop.hbase.keyvalue.<init>(keyvalue.java:580)  at org.apache.hadoop.hbase.keyvalue.<init>(keyvalue.java:483)  at org.apache.hadoop.hbase.keyvalue.<init>(keyvalue.java:370)  at org.apache.hadoop.hbase.keyvalue.<clinit>(keyvalue.java:267)  at org.apache.hadoop.hbase.hconstants.<clinit>(hconstants.java:978)  at org.apache.hadoop.hbase.htabledescriptor.<clinit>(htabledescriptor.java:1488)  at org.apache.hadoop.hbase.util.fstabledescriptors.<init>(fstabledescriptors.java:124)  at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:570)  at org.apache.hadoop.hbase.master.hmaster.<init>(hmaster.java:365)  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)  at java.lang.reflect.constructor.newinstance(constructor.java:423)  at org.apache.hadoop.hbase.master.hmaster.constructmaster(hmaster.java:2336)  __________________________________________________________ the class is in the hbase-common.jar and its there in the classpath as can be seen from the log: _________________________________________________________  2016-02-24 22:38:32,538 info [main] util.servercommandline: env:classpath=/home/hduser/hbase/hbase-1.1.3:/home/hduser/hbase/hbase-1.1.3/lib/activation-1.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/aopalliance-1.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/apacheds-i18n-2.0.0-m15.jar:/home/hduser/hbase/hbase-1.1.3/lib/apacheds-kerberos-codec-2.0.0-m15.jar:/home/hduser/hbase/hbase-1.1.3/lib/api-asn1-api-1.0.0-m20.jar:/home/hduser/hbase/hbase-1.1.3/lib/api-util-1.0.0-m20.jar:/home/hduser/hbase/hbase-1.1.3/lib/asm-3.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/avro-1.7.4.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-beanutils-1.7.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-beanutils-core-1.8.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-cli-1.2.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-codec-1.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-collections-3.2.2.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-compress-1.4.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-configuration-1.6.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-daemon-1.0.13.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-digester-1.8.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-el-1.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-httpclient-3.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-io-2.4.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-lang-2.6.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-logging-1.2.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-math-2.2.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-math3-3.1.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/commons-net-3.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/disruptor-3.3.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/findbugs-annotations-1.3.9-1.jar:/home/hduser/hbase/hbase-1.1.3/lib/guava-12.0.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/guice-3.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/guice-servlet-3.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-annotations-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-auth-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-client-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-common-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-hdfs-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-mapreduce-client-app-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-mapreduce-client-common-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-mapreduce-client-core-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-yarn-api-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-yarn-client-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-yarn-common-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hadoop-yarn-server-common-2.5.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-annotations-1.1.3-tests.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-annotations-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-client-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-common-1.1.3-tests.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-common-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-examples-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-hadoop-compat-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-hadoop2-compat-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-it-1.1.3-tests.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-it-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-prefix-tree-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-procedure-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-protocol-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-resource-bundle-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-rest-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-server-1.1.3-tests.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-server-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-shell-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/hbase-thrift-1.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/htrace-core-3.1.0-incubating.jar:/home/hduser/hbase/hbase-1.1.3/lib/httpclient-4.2.5.jar:/home/hduser/hbase/hbase-1.1.3/lib/httpcore-4.1.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/jackson-core-asl-1.9.13.jar:/home/hduser/hbase/hbase-1.1.3/lib/jackson-jaxrs-1.9.13.jar:/home/hduser/hbase/hbase-1.1.3/lib/jackson-mapper-asl-1.9.13.jar:/home/hduser/hbase/hbase-1.1.3/lib/jackson-xc-1.9.13.jar:/home/hduser/hbase/hbase-1.1.3/lib/jamon-runtime-2.3.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/jasper-compiler-5.5.23.jar:/home/hduser/hbase/hbase-1.1.3/lib/jasper-runtime-5.5.23.jar:/home/hduser/hbase/hbase-1.1.3/lib/java-xmlbuilder-0.4.jar:/home/hduser/hbase/hbase-1.1.3/lib/javax.inject-1.jar:/home/hduser/hbase/hbase-1.1.3/lib/jaxb-api-2.2.2.jar:/home/hduser/hbase/hbase-1.1.3/lib/jaxb-impl-2.2.3-1.jar:/home/hduser/hbase/hbase-1.1.3/lib/jcodings-1.0.8.jar:/home/hduser/hbase/hbase-1.1.3/lib/jersey-client-1.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/jersey-core-1.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/jersey-guice-1.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/jersey-json-1.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/jersey-server-1.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/jets3t-0.9.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/jettison-1.3.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/jetty-6.1.26.jar:/home/hduser/hbase/hbase-1.1.3/lib/jetty-sslengine-6.1.26.jar:/home/hduser/hbase/hbase-1.1.3/lib/jetty-util-6.1.26.jar:/home/hduser/hbase/hbase-1.1.3/lib/joni-2.1.2.jar:/home/hduser/hbase/hbase-1.1.3/lib/jruby-complete-1.6.8.jar:/home/hduser/hbase/hbase-1.1.3/lib/jsch-0.1.42.jar:/home/hduser/hbase/hbase-1.1.3/lib/jsp-2.1-6.1.14.jar:/home/hduser/hbase/hbase-1.1.3/lib/jsp-api-2.1-6.1.14.jar:/home/hduser/hbase/hbase-1.1.3/lib/jsr305-1.3.9.jar:/home/hduser/hbase/hbase-1.1.3/lib/junit-4.12.jar:/home/hduser/hbase/hbase-1.1.3/lib/leveldbjni-all-1.8.jar:/home/hduser/hbase/hbase-1.1.3/lib/libthrift-0.9.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/log4j-1.2.17.jar:/home/hduser/hbase/hbase-1.1.3/lib/metrics-core-2.2.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/netty-3.2.4.final.jar:/home/hduser/hbase/hbase-1.1.3/lib/netty-all-4.0.23.final.jar:/home/hduser/hbase/hbase-1.1.3/lib/paranamer-2.3.jar:/home/hduser/hbase/hbase-1.1.3/lib/protobuf-java-2.5.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/servlet-api-2.5-6.1.14.jar:/home/hduser/hbase/hbase-1.1.3/lib/servlet-api-2.5.jar:/home/hduser/hbase/hbase-1.1.3/lib/slf4j-api-1.7.7.jar:/home/hduser/hbase/hbase-1.1.3/lib/slf4j-log4j12-1.7.5.jar:/home/hduser/hbase/hbase-1.1.3/lib/snappy-java-1.0.4.1.jar:/home/hduser/hbase/hbase-1.1.3/lib/spymemcached-2.11.6.jar:/home/hduser/hbase/hbase-1.1.3/lib/xmlenc-0.52.jar:/home/hduser/hbase/hbase-1.1.3/lib/xz-1.0.jar:/home/hduser/hbase/hbase-1.1.3/lib/zookeeper-3.4.6.jar::/home/hduser/hbase/hbase-1.1.3/conf  __________________________________________________________ ",
        "label": 46
    },
    {
        "text": "its attempt to modify immutable table column descriptors  its: integrationtestingestwithmob (hbase-18419) integrationtestddlmasterfailover (hbase-18428) integrationtestingestwithencryption::setup (hbase-18440) integrationtestbulkload::installslowingcoproc (hbase-18440) other related: changebloomfilteraction (hbase-18419) changecompressionaction (hbase-18419) changeencodingaction (hbase-18419) changeversionsaction (hbase-18419) removecolumnaction (hbase-18419) addcolumnaction::perform (hbase-18440) changesplitpolicyaction::perform (hbase-18440) decreasemaxhfilesizeaction::perform (hbase-18440) ",
        "label": 320
    },
    {
        "text": "document hfile v3  0.98 added hfile v3. there are a couple of mentions of it in the book on the sections on cell tags, but there isn't an actual overview or design explanation like there is for hfile v2. ",
        "label": 402
    },
    {
        "text": "no longer include multiple httpcore and httpclient versions in binary package  we are including multiple versions of httpcore and httpclient in the binary package. httpclient-4.1.2.jar httpclient-4.2.5.jar httpclient-4.4.1.jar httpcore-4.1.2.jar httpcore-4.2.4.jar httpcore-4.4.1.jar ",
        "label": 363
    },
    {
        "text": "bulkload fails to remove files if fs default name   fs defaultfs is configured without default port  when fs.default.name or fs.defaultfs in hadoop core-site.xml is configured as hdfs://ip, and hbase.rootdir is configured as hdfs://ip:port/hbaserootdir where port is the hdfs namenode's default port. the bulkload operation will not remove the file in bulk output dir. store::bulkloadhfile will think hdfs:://ip and hdfs:://ip:port as different filesystem and go with copy approaching instead of rename. the root cause is that hbase master will rewrite fs.default.name/fs.defaultfs according to hbase.rootdir when regionserver started, thus, dest fs uri from the hregion will not matching src fs uri passed from client. any suggestion what is the best approaching to fix this issue? i kind of think that we could check for default port if src uri come without port info. ",
        "label": 531
    },
    {
        "text": "reduce shell exception dump on console  as discussed on irc and seen over and over, the shell is too verbose when it prints java related exceptions. the huge stack trace on the console is often causing more harm then actually helping. ... [11:31pm] larsgeorge: the only concern is to keep it in sync with new changes and also reduce its stacktrace [11:31pm] larsgeorge: that can be quite nasty [11:31pm] _dodger_: i've seen a prime example of that on the mailing list today [11:32pm] larsgeorge: yeah, those do repeat themselves [11:32pm] larsgeorge: also that debug is on by default [11:33pm] larsgeorge: mind you, that is a good idea for the daemons [11:33pm] larsgeorge: but prolly not the shell [11:33pm] jdcryans: i was thinking [11:33pm] larsgeorge: maybe we can set error logging level just for the shell when it is started? [11:34pm] jdcryans: we should stop printing the stack trace for nsre [11:34pm] larsgeorge: there are a few others of that sort [11:34pm] larsgeorge: be it zk reconnects etc. [11:35pm] jdcryans: yeah there's a lot of hbase-generated zk-related noise ",
        "label": 229
    },
    {
        "text": "allow aggregationclient to skip specifying column family for row count aggregate  for rowcounter job, column family is not required as input parameter. aggregationclient requires the specification of one column family:     } else if (scan.getfamilymap().size() != 1) {       throw new ioexception(\"there must be only one family.\");     } we should relax the above requirement for row count aggregate where firstkeyonlyfilter would be automatically applied. ",
        "label": 441
    },
    {
        "text": "add unit tests for thriftserver hbasehandler  tim sell over in hbase-697 suggests we add unit tests for our thrift guts (shouldn't need to fire up thrift to do this iirc). ",
        "label": 218
    },
    {
        "text": "backport hbase to  backport hbase-6197 'hregion's append operation may lose data' and the accompanying hbase-6195 to 0.94 and 0.92 ",
        "label": 544
    },
    {
        "text": "evaluate the remaining api compatibility concerns between branch and branch   branch  for discussion, here are the remaining compatibility concerns you will encounter upon moving up from 1.3 to 1.4: the below all relate to source level compatibility. recompilation of a client program may be terminated with the message \"class c is not abstract and does not override abstract method m in type\". according to our compatibility guidelines we are allowed to add methods to interfaces, so these are allowed. please let me know if you disagree. package org.apache.hadoop.hbase.client  interface admin abstract method cleardeadservers ( list<servername> ) has been added to this interface. abstract method clonesnapshot ( string, tablename, boolean ) has been added to this interface. abstract method iscleanerchoreenabled ( ) has been added to this interface. abstract method ismasterinmaintenancemode ( ) has been added to this interface. abstract method listdeadservers ( ) has been added to this interface. abstract method restoresnapshot ( string, boolean, boolean ) has been added to this interface. abstract method runcleanerchore ( ) has been added to this interface. abstract method setcleanerchorerunning ( boolean ) has been added to this interface. package org.apache.hadoop.hbase.client  interface resultscanner abstract method renewlease ( ) has been added to this interface. abstract method getscanmetrics ( ) has been added to this interface. package org.apache.hadoop.hbase.client  interface table abstract method getreadrpctimeout ( ) has been added to this interface. abstract method getwriterpctimeout ( ) has been added to this interface. abstract method setreadrpctimeout ( int ) has been added to this interface. abstract method setwriterpctimeout ( int ) has been added to this interface. package org.apache.hadoop.hbase.replication  interface replicationpeer abstract method getpeerbandwidth ( ) has been added to this interface. abstract method trackpeerconfigchanges ( replicationpeerconfiglistener ) has been added to this interface. package org.apache.hadoop.hbase.coprocessor  interface mastercoprocessorenvironment abstract method getmetricregistryformaster ( ) has been added to this interface. package org.apache.hadoop.hbase.coprocessor  interface masterobserver abstract method postaddrsgroup ( observercontext<mastercoprocessorenvironment>, string ) has been added to this interface. abstract method postbalancersgroup ( observercontext<mastercoprocessorenvironment>, string, boolean ) has been added to this interface. abstract method postcleardeadservers ( observercontext<mastercoprocessorenvironment> ) has been added to this interface. abstract method postlistdeadservers ( observercontext<mastercoprocessorenvironment> ) has been added to this interface. abstract method postmoveservers ( observercontext<mastercoprocessorenvironment>, set<address>, string ) has been added to this interface. abstract method postmoveserversandtables ( observercontext<mastercoprocessorenvironment>, set<address>, set<tablename>, string ) has been added to this interface. abstract method postmovetables ( observercontext<mastercoprocessorenvironment>, set<tablename>, string ) has been added to this interface. abstract method postremoversgroup ( observercontext<mastercoprocessorenvironment>, string ) has been added to this interface. abstract method preaddrsgroup ( observercontext<mastercoprocessorenvironment>, string ) has been added to this interface. abstract method prebalancersgroup ( observercontext<mastercoprocessorenvironment>, string ) has been added to this interface. abstract method precleardeadservers ( observercontext<mastercoprocessorenvironment> ) has been added to this interface. abstract method prelistdeadservers ( observercontext<mastercoprocessorenvironment> ) has been added to this interface. abstract method premoveservers ( observercontext<mastercoprocessorenvironment>, set<address>, string ) has been added to this interface. abstract method premoveserversandtables ( observercontext<mastercoprocessorenvironment>, set<address>, set<tablename>, string ) has been added to this interface. abstract method premovetables ( observercontext<mastercoprocessorenvironment>, set<tablename>, string ) has been added to this interface. abstract method preremoversgroup ( observercontext<mastercoprocessorenvironment>, string ) has been added to this interface. package org.apache.hadoop.hbase.coprocessor  interface regioncoprocessorenvironment abstract method getmetricregistryforregionserver ( ) has been added to this interface. package org.apache.hadoop.hbase.coprocessor  interface regionobserver abstract method postcommitstorefile ( observercontext<regioncoprocessorenvironment>, byte[ ], path, path ) has been added to this interface. abstract method precommitstorefile ( observercontext<regioncoprocessorenvironment>, byte[ ], list<pair<path,path>> ) has been added to this interface. abstract method precompactscanneropen ( observercontext<regioncoprocessorenvironment>, store, list<?>, scantype, long, internalscanner, compactionrequest, long ) has been added to this interface. abstract method preflushscanneropen ( observercontext<regioncoprocessorenvironment>, store, keyvaluescanner, internalscanner, long ) has been added to this interface. package org.apache.hadoop.hbase.coprocessor  interface walcoprocessorenvironment abstract method getmetricregistryforregionserver ( ) has been added to this interface. package org.apache.hadoop.hbase.ipc  class rpcscheduler abstract method getactivereadrpchandlercount ( ) has been added to this class. abstract method getactivescanrpchandlercount ( ) has been added to this class. abstract method getactivewriterpchandlercount ( ) has been added to this class. abstract method getreadqueuelength ( ) has been added to this class. abstract method getscanqueuelength ( ) has been added to this class. abstract method getwritequeuelength ( ) has been added to this class. package org.apache.hadoop.hbase.regionserver  interface region abstract method closeregionoperation ( region.operation ) has been added to this interface. abstract method waitforflushes ( ) has been added to this interface. package org.apache.hadoop.hbase.replication  interface replicationendpoint 1 added super-interface replicationpeerconfiglistener. (added methods.) .  . asyncrpcclient has been removed. it was not public. this change is allowed. package org.apache.hadoop.hbase.ipc  class asyncrpcclient .  . there are also changes to the limitedprivate interface store. we previously discussed this, and solicited and received feedback from the phoenix project saying they do not represent a problem, so the changes will be allowed. package org.apache.hadoop.hbase.regionserver  interface store abstract method add ( iterable<cell> ) has been added to this interface. abstract method bulkloadhfile ( byte[ ], string, path ) has been added to this interface. abstract method getscanners ( list<storefile>, boolean, boolean, boolean, boolean, scanquerymatcher, byte[ ], byte[ ], long, boolean ) has been added to this interface. abstract method getscanners ( boolean, boolean, boolean, boolean, scanquerymatcher, byte[ ], byte[ ], long ) has been added to this interface. abstract method upsert ( iterable<cell>, long, list<cell> ) has been added to this interface. abstract method bulkloadhfile ( string, long ) has been removed from this interface. abstract method getscanners ( list<storefile>, boolean, boolean, boolean, boolean, scanquerymatcher, byte[ ], byte[ ], long, boolean ) has been removed from this interface. abstract method getscanners ( boolean, boolean, boolean, boolean, scanquerymatcher, byte[ ], byte[ ], long ) has been removed from this interface. abstract method upsert ( iterable<cell>, long ) has been removed from this interface. .  . a number of configuration constants were (re)moved from simplerpcscheduler, tagged limitedprivate. i think these can be allowed. package org.apache.hadoop.hbase.ipc  class simplerpcscheduler removed constants call_queue_codel_default_interval call_queue_codel_default_lifo_threshold call_queue_codel_default_target_delay call_queue_codel_interval call_queue_codel_lifo_threshold call_queue_codel_target_delay call_queue_handler_factor_conf_key call_queue_read_share_conf_key call_queue_scan_share_conf_key call_queue_type_codel_conf_value call_queue_type_conf_default call_queue_type_conf_key call_queue_type_deadline_conf_value call_queue_type_fifo_conf_value queue_max_call_delay_conf_key .  . two constants were removed from public class tableinputformatbase. i think these are internal implementation details and so removal can be allowed, but we could put them back if need be. package org.apache.hadoop.hbase.mapreduce  class tableinputformatbase removed constants input_autobalance_maxskewratio table_row_textkey ",
        "label": 38
    },
    {
        "text": "column families allow to have slashes in name  the check in hcolumndescriptor.islegalfamilyname() does not check for slashes and may cause issue. create 'test2', 'cf/am/2' $ bin/hadoop dfs -lsr /hbase/test2/d1da5042d2b233f056f7604398f29537 drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/.oldlogs -rw-r--r--   3 larsgeorge supergroup        124 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/.oldlogs/hlog.1297800748239 -rw-r--r--   3 larsgeorge supergroup        714 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/.regioninfo drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/cf drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/cf/am drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/cf/am/2 ",
        "label": 428
    },
    {
        "text": "comparefilters are broken from client side  some filters pass region-level tests but seem to freeze client-side. ",
        "label": 247
    },
    {
        "text": "retry all 'retryable' zk operations  e g  connection loss  the 'new' master refactored our zk code tidying up all zk accesses and coralling them behind nice zk utility classes. one improvement was letting out all keeperexceptions letting the client deal. thats good generally because in old days, we'd suppress important state zk changes in state. but there is at least one case the new zk utility could handle for the application and thats the class of retryable keeperexceptions. the one that comes to mind is conection loss. on connection loss we should retry the just-failed operation. usually the retry will just work. at worse, on reconnect, we'll pick up the expired session event. adding in this change shouldn't be too bad given the refactor of zk corralled all zk access into one or two classes only. one thing to consider though is how much we should retry. we could retry on a timer or we could retry for ever as long as the stoppable interface is passed so if another thread has stopped or aborted the hosting service, we'll notice and give up trying. doing the latter is probably better than some kinda timeout. hbase-3062 adds a timed retry on the first zk operation. this issue is about generalizing what is over there across all zk access. ",
        "label": 294
    },
    {
        "text": "broken formatting in lru stats output  i've seen this a few times - the output for lru stats ends up with some invalid characters in it: 11/03/03 14:36:06 debug hfile.lrublockcache: lru stats: total=1.49 mb, free=180.56 mb, max=182.05 mb, blocks=0, accesses=0, hits=0, hitratio=\u00ef\u00bf\u00bd%, cachingaccesses=0, cachinghits=0, cachinghitsratio=\u00ef\u00bf\u00bd%, evictions=0, evicted=0, evictedperrun=nan note the messed up \"hitratio\" and \"cachinghitsratio\". i can't figure out how to reproduce, though. ",
        "label": 162
    },
    {
        "text": "remove prefix tree from compression adoc  compression.adoc still refers to prefix_tree though the encoding has been removed:  -data_block_encoding <arg>   encoding algorithm (e.g. prefix compression) to                               use for data blocks in the test column family, one                               of [none, prefix, diff, fast_diff, prefix_tree]. row_index_v1 should be put in its place. ",
        "label": 352
    },
    {
        "text": "start hbase cmd doesn't need the execute flag   when you do start- and tabulation, since there start-hbase.cmd has the execution flag, completion go only up to start-hbase. we should remove the execution flag for this script. ",
        "label": 314
    },
    {
        "text": " hbck2  expose replication fixes from hbck1  ",
        "label": 314
    },
    {
        "text": "testhcm failing sporadically on jenkins and always for me on an ubuntu machine  testhcm takes 13 minutes for me on ubuntu and fails in testclosing. it runs fine on a mac. the problem test is not testclosing as i thought originally, its the test just previous, testconnectionuniqueness. testconnectionuniqueness creates the maximum cached hconnections + 10 to verify each is unique if the passed in configuration has a unique hash. problem comes when zk enforces its default max from single host of 30 connections which is < (max cached + 10). the max does not seem to be enforced on mac for me. the max connections runs up to max of 31 \u2013 zk max + 1 \u2013 and works fine until we do the +10. on ubuntu, when we hit the zk max of 30, we'll then go into a fail mode where we cannot set up a zk session... each attempt takes a while. test passes, it just takes a while. only, the uniqueness test does not clean up after itself and so all sessions to zk are outstanding so then when the subsequent testclosing runs, it can't set up connections successfully so fails. ",
        "label": 314
    },
    {
        "text": "bin hbase migrate upgrade fails when redo logs exists  i migrated several hbase-0.1.3 instances to hbase trunk and even if i stop hbase-0.1.3 cleanup it leaves redo logs on hdfs. the problems is that when migrating the data with hbase-trunk it fails because it finds these redo-logs and quit with a error message saying that we should reinstall the old hbase and shut it down cleanly and that in theory it erases the redo logs. the work around has been to delete the redo logs manually... which is bad. ",
        "label": 218
    },
    {
        "text": "improve mttr  split wal to hfile  after hbase-20724, the compaction event marker is not used anymore when failover. so our new proposal is split wal to hfile to imporve mttr. it has 3 steps: 1. read wal and write hfile to region\u2019s column family\u2019s recovered.hfiles directory. 2. open region. 3. bulkload the recovered.hfiles for every column family. the design doc was attathed by a google doc. any suggestions are welcomed. ",
        "label": 187
    },
    {
        "text": "hbase wont run on ipv6 on oses that use zone indexes  in ipv6, an address may have a zone-index, which is specified with a percent, eg: ...%0. this looks like a format string, and thus in a part of the code which uses the hostname as a prefix to another string which is interpreted with string.format, you end up with an exception: 2012-07-31 18:21:39,848 fatal org.apache.hadoop.hbase.master.hmaster:  unhandled exception. starting shutdown.  java.util.unknownformatconversionexception: conversion = '0'  at java.util.formatter.checktext(formatter.java:2503)  at java.util.formatter.parse(formatter.java:2467)  at java.util.formatter.format(formatter.java:2414)  at java.util.formatter.format(formatter.java:2367)  at java.lang.string.format(string.java:2769)  at com.google.common.util.concurrent.threadfactorybuilder.setnameformat(threadfactorybuilder.java:68)  at org.apache.hadoop.hbase.executor.executorservice$executor.<init>(executorservice.java:299)  at org.apache.hadoop.hbase.executor.executorservice.startexecutorservice(executorservice.java:185)  at org.apache.hadoop.hbase.executor.executorservice.startexecutorservice(executorservice.java:227)  at org.apache.hadoop.hbase.master.hmaster.startservicethreads(hmaster.java:821)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:507)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:344)  at org.apache.hadoop.hbase.master.hmastercommandline$localhmaster.run(hmastercommandline.java:220)  at java.lang.thread.run(thread.java:680)  2012-07-31 18:21:39,908 info org.apache.hadoop.hbase.master.hmaster: aborting ",
        "label": 547
    },
    {
        "text": "don't allow multi's to over run the max result size   if a user puts a list of tons of different gets into a table we will then send them along in a multi. the server un-wraps each get in the multi. while no single get may be over the size limit the total might be. we should protect the server from this.   we should batch up on the server side so each rpc is smaller. ",
        "label": 154
    },
    {
        "text": "generate changes md and releasenotes md for  ",
        "label": 149
    },
    {
        "text": " rest  sending html for errors is unhelpful  the large html 404 page returned by stargate is not helpful. the rest interface is not intended for humans to read, esp. when the client is known to be a program because it's asking for binary, but really any time. nice big readable error pages use bandwidth and clutter network traces to no purpose. please allow the 404 and other error pages to be configured away, or just stop sending them (my preference). if some body must be sent, a simple text/plain \"not found\" would be fine, i think. ",
        "label": 38
    },
    {
        "text": "per region metrics are not showing up for system tables   per region metrics are not showing up for system tables. ",
        "label": 154
    },
    {
        "text": "option to set ttl for columns in hbase  i would like to see the option to have a ttl on the columns in hbase this feature could be helpfully in removing stale data from large datasets with out havening to do a full scan of the dataset and then issuing deletes. example   say i am crawling pages and only refreshing pages based on a set score and some pages doe not get updated over x days the old version of the page gets removed from the data set. say i am striping out links form html and storing them say a link is removed from a page then i would need to issue a delete statement to remove that links form the data set with a ttl the link data would remove its self if not updated in x secs. these are just examples based on crawling like nutch but i can foresee many apps using this option. this is a feature in bigtables thats is handled when bigtable does garbage-collection. ",
        "label": 38
    },
    {
        "text": "if multiple of same key in an hfile and they span blocks  may miss the earlier keys on a lookup  see hbase-1818 for description by schubert zhang \u2013 discovered by him doing a code review of hfile. ",
        "label": 399
    },
    {
        "text": "hbckservercrashprocedure can double assign  the new scp that does scp plus cleanup 'unknown servers' with mentions in hbase:meta added by the below can make for double assignments. commit c238891a26734e1e4276b6b1677a58cf83de5dc4 author: stack <stack@apache.org> date:   wed nov 13 22:36:26 2019 -0800     hbase-23282 hbckservercrashprocedure for 'unknown servers' ",
        "label": 314
    },
    {
        "text": "minor compaction needs to check if still over compactionthreshold after compacting  i have a busy region, and there are 43 storefiles (>compactionthreshold=8) in this region.  now, i stopped the client and stopped putting new data into it. i expect these storefiles to be compacted later. but, almost one day later, these 43 storefiles are still there.  (note: in my hbase instance, i disabled the major compaction.) it seems the minor compaction does not be started continuiously to compact remaining storefiles.  and i checked the code, it is true. after more test, a obvious issue/problem is, the complete of a minor compaction does not check if current storefiles need more minor compaction. i think this may be a bug or leak. try this test: 1. put many data to a region, then there are 30 storefiles accumulated, because the backend compaction cannot catch up with the fast puts. (hbase.hstore.compactionthreshold=8, base.hstore.compaction.max=12) 2. then stop put. 3. then, these 30 storefiles are still there for a long time, (no automatic minor compaction) 4. submit a compaction on this region, then, only 12 files are compaction, now, we have 19 storefiles. the minor compaction stopped. i think, when a minor compaction complete, it should check if the number of storefiles still many, if so, another minor compaction should start continuiously. ",
        "label": 341
    },
    {
        "text": "add provision for adding mutations to memstore or able to write to same region in batchmutate coprocessor hooks  as part of phoenix-1734 we need to write the index updates to same region from coprocessors but writing from batchmutate api is not allowed because of mvcc. raised phoenix-2742 to discuss any alternative way to write to the same region directly or not but not having any proper solution there. currently we have provision to write wal edits from coprocessors. we can set wal edits in minibatchoperationinprogress.   /**    * sets the waledit for the operation(mutation) at the specified position.    * @param index    * @param waledit    */   public void setwaledit(int index, waledit waledit) {     this.waleditsfromcoprocessors[getabsoluteindex(index)] = waledit;   } similarly we can allow to write mutations from coprocessors to memstore as well. or else we should provide the batch mutation api allow write in batch mutate coprocessors. ",
        "label": 366
    },
    {
        "text": "waledit doesn't implement heapsize  waledit from hbase-2283 defines a method heapsize() but doesn't implement heapsize. make it implement the interface. add a test to testheapsize. ",
        "label": 263
    },
    {
        "text": "document workaround for minidfscluster exceptions when running tests  the hbasetestingutility appears to have an unhandled nullpointerexception in certain environments. it works on apache hbase 0.94.6.1 on mac osx but not in certain linux environments such as centos. in java: hbasetestingutility testingutility = new hbasetestingutility(); testingutility.startminicluster(1);  // blows up on centos linux but not mac osx creates this exception: error in (create-table) (minidfscluster.java:426) uncaught exception, not in assertion. expected: nil   actual: java.lang.nullpointerexception: null  at org.apache.hadoop.hdfs.minidfscluster.startdatanodes (minidfscluster.java:426)     org.apache.hadoop.hdfs.minidfscluster.<init> (minidfscluster.java:284)     org.apache.hadoop.hbase.hbasetestingutility.startminidfscluster (hbasetestingutility.java:444)     org.apache.hadoop.hbase.hbasetestingutility.startminicluster (hbasetestingutility.java:612)     org.apache.hadoop.hbase.hbasetestingutility.startminicluster (hbasetestingutility.java:568)     org.apache.hadoop.hbase.hbasetestingutility.startminicluster (hbasetestingutility.java:555) i also created a stack overflow ticket here, with more info: http://stackoverflow.com/questions/17625938/hbase-minidfscluster-java-fails-in-certain-environments steps to reproduce: 1. download a copy of centos  2. install java  3. fire up a minicluster as part of the hbasetestingutility ",
        "label": 38
    },
    {
        "text": "extend testheapsize and classsize to do  deep  sizing of objects  as discussed in hbase-1554 there is a bit of a disconnect between how classsize calculates the heap size and how we need to calculate heap size in our implementations. for example, the lru block cache can be sized via classsize, but it is a shallow sizing. there is a backing concurrenthashmap that is the largest memory consumer. however, classsize only counts that as a single reference. but in our heapsize() reporting, we want to include everything within that object. this issue is to resolve that dissonance. we may need to create an additional classsize.estimatedeep(), we may need to rethink our heapsize interface, or maybe just leave it as is. the two primary goals of all this testing is to 1) ensure that if something is changed and the sizing is not updated, our tests fail, and 2) ensure our sizing is as accurate as possible. ",
        "label": 314
    },
    {
        "text": "testclassfinder fails when run on jdk11  when built on java 8 and unit tests are run on java 11, testclassfinder fails. a sample stacktrace: java.lang.classcastexception: class jdk.internal.loader.classloaders$appclassloader cannot be cast to class java.net.urlclassloader (jdk.internal.loader.classloaders$appclassloader and java.net.urlclassloader are in module java.base of loader 'bootstrap') at org.apache.hadoop.hbase.testclassfinder.packageandloadjar(testclassfinder.java:418) at org.apache.hadoop.hbase.testclassfinder.createandloadjar(testclassfinder.java:193) at org.apache.hadoop.hbase.testclassfinder.testclassfindercanfindclassesindirs(testclassfinder.java:233) ",
        "label": 391
    },
    {
        "text": "adding the tests' hbase site xml to the jar breaks some clients  since our move to maven we started including the src/test/resources/hbase-site.xml to our test jar. this breaks the standalone clients that include it like hive as depicted in hive-1597. ",
        "label": 229
    },
    {
        "text": "remove copy table rb script  remove copy_table.rb script as per mailing list discussion. it hasn't been maintained in a while and does not run against any recent hbase release. there is also an mr job to do the same thing that does work. ",
        "label": 131
    },
    {
        "text": "multiregion transactions with optimistic concurrency control  we have a need for acid transactions across tables. this issue is about adding transactions which span multiple regions. we do not envision many competing writes, and will be read-dominated in general. this makes optimistic concurrency control (occ) seem like the way to go. ",
        "label": 110
    },
    {
        "text": "failed to run rowcounter on top of hadoop branch  :/hadoop$ hadoop_classpath=`/hbase/bin/hbase classpath` bin/hadoop jar ~/hbase/hbase-0.91.0-snapshot.jar rowcounter usertable   exception in thread \"main\" java.lang.nosuchmethoderror: org.apache.hadoop.util.programdriver.driver([ljava/lang/string;)v   at org.apache.hadoop.hbase.mapreduce.driver.main(driver.java:51)   at sun.reflect.nativemethodaccessorimpl.invoke0(native method)   at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)   at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)   at java.lang.reflect.method.invoke(method.java:597)   at org.apache.hadoop.util.runjar.main(runjar.java:192) ",
        "label": 316
    },
    {
        "text": "proto files should be in the same palce  currently, proto files are under hbase-server/src/main/protobuf and hbase-server/src/protobuf. it's better to put them together. ",
        "label": 242
    },
    {
        "text": "adopt releasedocmaker for better generated release notes  we should consider adopting hadoop's releasedocmaker for better release notes. this would pull out text from the jira 'release notes' field with clean presentation and is vastly superior to our current notes, which are simply jira's list of issues by fix version. could hook it into the site build. a convenient part of yetus to get up and running with. ",
        "label": 314
    },
    {
        "text": "fix asciidoc warnings  when building docs, i noticed: failed to parse formatted text: to supply filters to the scanner object or configure the scanner in any other way, you can create a text file and add your filter to the file. for example, to return only rows for which keys start with &lt;codeph&gt;u123&lt;/codeph&gt; and use a batch size of 100, the filter file would look like this:  <pre> &lt;scanner batch=\"100\"&gt; &lt;filter&gt; { \"type\": \"prefixfilter\", \"value\": \"u123\" } &lt;/filter&gt; &lt;/scanner&gt; </pre> working hypthesis is that we should either be using proper codeblocks rather than pre tags. otherwise we may need to do something to escape curly braces. asciidoctor is probably trying to interpret them as liquid tags. ",
        "label": 226
    },
    {
        "text": "support both hadoop   and  since hadoop 0.21 isn't going to be well supported and that a lot of users may wish to stick on 0.20, the next hbase major release should support both 0.20 and 0.21. hdfs-265 support will be swapped for hdfs-200 if running on hdfs 0.20. a cluster without that patchset shouldn't be supported. ",
        "label": 314
    },
    {
        "text": "update of hadoop jar in hbase broke testmulticlusters  testmulticlusters failing following hbase-2771 ",
        "label": 229
    },
    {
        "text": "fix testkillingserversfrommaster in trunk  it just hangs since new master went in  this test depends on mechanisms much changed after new master went in so it just hangs; its not getting the confirmations it used expect. this issue is about recasting the test \u2013 because what it tests is useful (just reading the test i found bug in new master) \u2013 so need to get it going again. marking critiical for 0.90. ",
        "label": 314
    },
    {
        "text": "add to download page  ",
        "label": 149
    },
    {
        "text": "need a rolling restart script  need a script that will do a rolling restart. it should be configurable in 2 ways: how long to keep the daemon down per host how long to wait between hosts for regionservers in my own hacky command line i used 10/60. ",
        "label": 341
    },
    {
        "text": "add markers to changes md and releasenotes md  i want to have scripts interpolate release notes and changes. need markers so can find where to do the interpolation. let me add a few. ",
        "label": 314
    },
    {
        "text": "hfiledatablockencoderimpl disktocacheformat uses wrong format  in this method, we have     if (block.getblocktype() == blocktype.encoded_data) {       if (block.getdatablockencodingid() == ondisk.getid()) {         // the block is already in the desired in-cache encoding.         return block;       } this assumes ondisk encoding is the same as that of incache. this is not true when we change the encoding of a cf. this could be one of the reasons i got data loss with online encoding change? if i make sure ondisk == incache all the time, my itbll with online encoding change worked once for me. ",
        "label": 242
    },
    {
        "text": "hbase book updates for replication after hbase  ",
        "label": 330
    },
    {
        "text": "islegalfamilyname  can throw arrayoutofboundexception  org.apache.hadoop.hbase.hcolumndescriptor.islegalfamilyname(byte[]) accesses byte[0] w/o first checking the array length. ",
        "label": 458
    },
    {
        "text": "thrift filter language documentation is inconsistent  syntax: singlecolumnvaluefilter(<compare operator>, '<comparator>', '<family>', '<qualifier>), as described here: http://hbase.apache.org/book/thrift.html is not correct.  the correct syntax is: singlecolumnvaluefilter('<family>', '<qualifier>', <compare operator>, '<comparator>')  also, <comparator> parameter must always contain the comparator, e.g. binary: or binaryprefix: etc. without it (except prefixfilter and maybe some other filters) tsocket class throws ttransportexception: tsocket read 0 bytes.   all examples in section 9.3.1.9. individual filter syntax are written without comparator. there also a typo:   in section 9.3.1.9.12 - family filter, syntax and example described for qualifierfilter ",
        "label": 235
    },
    {
        "text": "remove migration code  remove the objects and code only needed for supporting migration to 0.96 from 0.94. ",
        "label": 352
    },
    {
        "text": "usability regression  we don't parse compression algos anymore  it seems that string with 0.92.0 we can't create tables in the shell by specifying \"lzo\" anymore. i remember we used to do better parsing than that, but right now if you follow the wiki doing this: create 'mytable', {name=>'colfam:', compression=>'lzo'} you'll get: error: java.lang.illegalargumentexception: no enum const class org.apache.hadoop.hbase.io.hfile.compression$algorithm.lzo bad for usability. ",
        "label": 286
    },
    {
        "text": "performance  scanners and getrow return maps with duplicate data  right now, whenever we get back multiple cells worth of data at a time, we do so in a map of hstorekey->byte[]. this means that there is a duplicated text row and long timestamp at the very least between every cell. this is quite a bit wasted. it also means we have to do a lot of translation every time. we could create a new writable that contains just one row, one timestamp, and a map of text->byte[]. ",
        "label": 86
    },
    {
        "text": "netty bytebuf leak in rpc client implementation  2019-08-31 21:48:27,067 error [rs-eventloopgroup-6-3] util.resourceleakdetector(317): leak: bytebuf.release() was not called before it's garbage-collected. see http://netty.io/wiki/reference-counted-objects.html for more information. recent access records:  created at: org.apache.hbase.thirdparty.io.netty.buffer.unpooledbytebufallocator.newdirectbuffer(unpooledbytebufallocator.java:96) org.apache.hbase.thirdparty.io.netty.buffer.abstractbytebufallocator.directbuffer(abstractbytebufallocator.java:187) org.apache.hbase.thirdparty.io.netty.buffer.abstractbytebufallocator.directbuffer(abstractbytebufallocator.java:178) org.apache.hbase.thirdparty.io.netty.buffer.unpooled.directbuffer(unpooled.java:125) org.apache.hadoop.hbase.ipc.nettyrpcconnection.<init>(nettyrpcconnection.java:96) org.apache.hadoop.hbase.ipc.nettyrpcclient.createconnection(nettyrpcclient.java:74) org.apache.hadoop.hbase.ipc.nettyrpcclient.createconnection(nettyrpcclient.java:40) org.apache.hadoop.hbase.ipc.abstractrpcclient.getconnection(abstractrpcclient.java:364) org.apache.hadoop.hbase.ipc.abstractrpcclient.callmethod(abstractrpcclient.java:433) org.apache.hadoop.hbase.ipc.abstractrpcclient.access$300(abstractrpcclient.java:97) org.apache.hadoop.hbase.ipc.abstractrpcclient$rpcchannelimplementation.callmethod(abstractrpcclient.java:605) org.apache.hadoop.hbase.shaded.protobuf.generated.clientprotos$clientservice$stub.scan(clientprotos.java:42345) org.apache.hadoop.hbase.client.asyncclientscanner.callopenscanner(asyncclientscanner.java:152) org.apache.hadoop.hbase.client.asyncsinglerequestrpcretryingcaller.call(asyncsinglerequestrpcretryingcaller.java:82) org.apache.hadoop.hbase.client.asyncsinglerequestrpcretryingcaller.lambda$docall$7(asyncsinglerequestrpcretryingcaller.java:115) org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) java.util.concurrent.completablefuture.complete(completablefuture.java:1962) org.apache.hadoop.hbase.client.asyncregionlocator.lambda$getregionlocation$3(asyncregionlocator.java:124) org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) java.util.concurrent.completablefuture.complete(completablefuture.java:1962) org.apache.hadoop.hbase.client.connectionutils.lambda$getorfetch$6(connectionutils.java:573) org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) java.util.concurrent.completablefuture.complete(completablefuture.java:1962) org.apache.hadoop.hbase.client.zkasyncregistry.trycomplete(zkasyncregistry.java:123) org.apache.hadoop.hbase.client.zkasyncregistry.lambda$getmetaregionlocation$1(zkasyncregistry.java:165) org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) java.util.concurrent.completablefuture.complete(completablefuture.java:1962) org.apache.hadoop.hbase.client.zkasyncregistry.lambda$getandconvert$0(zkasyncregistry.java:81) org.apache.hadoop.hbase.util.futureutils.lambda$addlistener$0(futureutils.java:68) java.util.concurrent.completablefuture.uniwhencomplete(completablefuture.java:760) java.util.concurrent.completablefuture$uniwhencomplete.tryfire(completablefuture.java:736) java.util.concurrent.completablefuture.postcomplete(completablefuture.java:474) java.util.concurrent.completablefuture.complete(completablefuture.java:1962) org.apache.hadoop.hbase.zookeeper.readonlyzkclient$zktask$1.exec(readonlyzkclient.java:174) org.apache.hadoop.hbase.zookeeper.readonlyzkclient.run(readonlyzkclient.java:342) java.lang.thread.run(thread.java:748) https://builds.apache.org/job/hbase-flaky-tests/job/master/4148/testreport/junit/org.apache.hadoop.hbase.client/testconnection/testconnectioncloseallowsinterrupt/ ",
        "label": 149
    },
    {
        "text": "postopen hook called twice  postopen hook is called twice when a region is initializing: once at the end of the body of the initializeregioninternals() method of the hregion class.  once at the end initializeregionstores() method of the hregion class; initializeregionstores() is called inside initializeregioninternals() and as such causes the postopen hook to be called twice. ",
        "label": 340
    },
    {
        "text": "regionserver status webpage bucketcache list can become huge  the regionserver status page, such as http://127.0.0.1:60030/rs-status always downloads information about every bucket in the cache. in some cases this can be hundreds of thousands of buckets, causing megabytes of info to need to be downloaded and significant browser instability and memory usage: wc -l hbase-region-server-hb22.html  2010116 hbase-region-server-hb22.html ls -lah hbase-region-server-hb22.html  32m oct 6 19:23 hbase-region-server-hb22.html firefox \"about:memory\":  1,330.18 mb (48.22%) \u2013 top(http://hb22:60030/rs-status#bc_l2, id=2010)  1,329.61 mb (48.20%) \u2013 active/window(http://hb22:60030/rs-status#bc_l2) ",
        "label": 314
    },
    {
        "text": "splitting of region with replica  doesn't update region list in serverholding  a server crash leads to overlap   the situation can appear in following steps in release hbase1.2.6 1. create 'testtable', 'info', {region_replication=>2} 2. write somerecords into 'testtable'  3. split the table 'testtable'  4. after the spliting, the serverholdings in regionstates still holds the regioninfo for the replica of parent region  5. restart the regionserver where the parent replica-region located  6. the offlined replica of parent region will be assigned in servercrashprocedure. hbase hbck 'testtable\u2018 error: region { meta => null, hdfs => null, deployed => qabb-qa-hdp-hbase1,16020,1503022958093;testtable,,1503022907686_0001.42d11cfe195b3cc4d08b2c078a687f6d ., replicaid => 1 } not in meta, but deployed on qabb-qa-hdp-hbase1,16020,1503022958093  18 error: no regioninfo in meta or hdfs. { meta => null, hdfs => null, deployed => qabb-qa-hdp-hbase1,16020,1503022958093;testtable,,1503022907686_0001.42d11cfe 195b3cc4d08b2c078a687f6d., replicaid => 1 } ",
        "label": 205
    },
    {
        "text": "fix fsutils createtabledescriptor   currently createtabledescriptor() doesn't return anything.  the caller wouldn't know whether the descriptor is created or not. see exception handling:    } catch(ioexception ioe) {      log.info(\"ioexception while trying to create tableinfo in hdfs\", ioe);    } we should return a boolean. if the table descriptor exists already, maybe we should deserialize from hdfs and compare with htabledescriptor argument. if they differ, i am not sure what the proper action would be. maybe we can add a boolean argument, force, to createtabledescriptor(). when force is true, existing table descriptor would be overwritten. ",
        "label": 544
    },
    {
        "text": "preemptivefastfailinterceptor clean repeatedfailuresmap issue  preemptivefastfailinterceptor do not set fastfailclearingtimemillisec, so in  occasionallycleanupfailureinformation function this branch else if (now > entry.getvalue().timeoffirstfailuremillisec     + this.fastfailclearingtimemillisec) { will be always be true,then the repeatedfailuresmap will be clean. and in the constructor function  public preemptivefastfailinterceptor(configuration conf) {   this.fastfailthresholdmillisec = conf.getlong(       hconstants.hbase_client_fast_fail_threashold_ms,       hconstants.hbase_client_fast_fail_threashold_ms_default);   this.failuremapcleanupintervalmillisec = conf.getlong( // this constant seem to set fastfailclearingtimemillisec, it may be a mistake.       hconstants.hbase_client_fast_fail_cleanup_ms_duration_ms,       hconstants.hbase_client_fast_fail_cleanup_duration_ms_default);   lastfailuremapcleanuptimemillisec = environmentedgemanager.currenttime(); }     ",
        "label": 563
    },
    {
        "text": "quick  smoke tests  testsuite  it would be nice if there was a known subset of the tests that run fast (e.g. not more than a few seconds) and quickly help us check whether the code isn't horribly broken. this way one could run those tests at a frequent interval when iterating and only run the entire testsuite at the end, when they think they're done, since doing so is very time consuming. someone would need to identify which tests really focus on the core functionality and add a target in the build system to just run those tests. as a bonus, it would be awesome++ if the core tests ran, say, 10x faster than they currently do. there's a lot of \"sleep\"-based \"synchronization\" in the tests and it would be nice to remove some of that where possible to make the tests run as fast as the machine can handle them. ",
        "label": 340
    },
    {
        "text": "feature to enable client side scanning client side merging  in hbase   the motivation of this was to enable the client to be able to open the region scanner(and in turn open storescanners) and perform the merge on the client side. this will lower the cpu ops that are consumed by the regionserver since the data is pulled directly from the datanode. in cases where the user is interested to perform a large scan on hbase data check-pointed at a point of time, we think that clientsidescan(clientsidemerge) would give a very high throughput as compared to using the clientscanner in htable. ",
        "label": 302
    },
    {
        "text": "disable region splits and merges switch in master  in large clusters where region splits are frequent, and hbck runs take longer, the concurrent splits cause further problems in hbck since hbck assumes a static state for the region partition map. we have just seen a case where hbck undo's a concurrently splitting region causing number of inconsistencies to go up. we can have a mode in master where splits and merges are disabled like the balancer and catalog janitor switches. master will reject the split requests if regionservers decide to split. this switch can be turned on / off by the admins and also automatically by hbck while it is running (similar to balancer switch being disabled by hbck). hbck should also disable the catalog janitor just in case. ",
        "label": 198
    },
    {
        "text": "enhance test patch sh to run against both jdk and jdk  currently test-patch.sh only runs against jdk 1.6  however trunk build is using jdk 1.7 test-patch.sh should be enhanced to run against both jdk versions. ",
        "label": 441
    },
    {
        "text": "upgrade zookeeper dependency to  zookeeper 3.4.5 works with oracle jdk 1.7  we should upgrade to zookeeper 3.4.5 in trunk ",
        "label": 314
    },
    {
        "text": "we need to pass something like compactionrequest in cp to give user some information about the compaction  for example, it is a major compaction or minor compaction. ",
        "label": 352
    },
    {
        "text": "checkandmutate used an incorrect row to check the condition  in branch-1.4, checkandmutate used the row of rowmutation to check the condition which is incorrect. it will fail in the case which is checking a row and mutate a different row. the issue doesn't happen in the master branch.   ",
        "label": 8
    },
    {
        "text": "table web ui is corrupted sometime  the web ui page source is like below: <h2>table attributes</h2> <table class=\"table table-striped\">   <tr>       <th>attribute name</th>       <th>value</th>       <th>description</th>   </tr>   <tr>       <td>enabled</td>       <td>true</td>       <td>is the table enabled</td>   </tr>   <tr>       <td>compaction</td>       <td> <p><hr/></p> no sure if it is a hbase issue, or a network/browser issue. ",
        "label": 230
    },
    {
        "text": "loadincrementalhfiles should be able to handle cfs with blooms  when loadincrementalhfiles loads a store file that crosses region boundaries, it will split the file at the boundary to create two store files. if the store file is for a column family that has a bloom filter, then a \"java.lang.arithmeticexception: / by zero\" will be raised because bytebloomfilter() is called with maxkeys of 0. the included patch assumes that the number of keys in each split child will be equal to the number of keys in the parent's bloom filter (instead of 0). this is an overestimate, but it's safe and easy. ",
        "label": 126
    },
    {
        "text": "add write permissions check before any hbck run that modifies hdfs   we encoutered a situation where hbck was run by an under-privileged user that was unable to write/modify/merge regions due to hdfs perms. unfortunately, this user was alerted of this after several minutes of read-only operations. hbck should fail early by having a write perm check and providing actionable advice to the hbase admin. maybe something like: \"current user yy does not have write perms to <hbase home>. please run hbck as hdfs user xxx\" ",
        "label": 290
    },
    {
        "text": "update the link of  bending time in hbase   datamodel.adoc link:http://outerthought.org/blog/417-ot.html[bending time in hbase]  the link is changed to \"https://www.ngdata.com/bending-time-in-hbase/\" ",
        "label": 226
    },
    {
        "text": "if re transition to opening during log replay fails  server aborts  instead  should just cancel region open   the progressable used on region open to tickle the zk opening node to prevent the master from timing out a region open operation will currently abort the regionserver if this fails for some reason. however it could be \"normal\" for an rs to have a region open operation aborted by the master, so should just handle as it does other places by reverting the open. we had a cluster trip over some other issue (for some reason, the tickle was not happening in < 30 seconds, so master was timing out every time). because of the abort on badversion, this eventually led to every single rs aborting itself eventually taking down the cluster. ",
        "label": 247
    },
    {
        "text": " optimization  cache cell maximum length  hcd getmaxvaluelength  its used checking batch size   in profiler, i can see doing an upload that 4% of cpu is spent doing bytes.tostring on hcd string representation of the cell maximum value. cache it. ",
        "label": 314
    },
    {
        "text": "review names of all znodes in zk to make sure they are sensible before  hbase-4451 was a rename we needed. my guess is we could do a few more now we have the opportunity (e.g. unassigned as name of dir for regions in transition..). ",
        "label": 139
    },
    {
        "text": "noroutetohostexception during balancing will cause master abort  via tatsuya up on the list: 2011-03-10 07:48:39,192 fatal org.apache.hadoop.hbase.master.hmaster: remote unexpected exception java.net.noroutetohostexception: no route to host       at sun.nio.ch.socketchannelimpl.checkconnect(native method)       at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:567)       at org.apache.hadoop.net.socketiowithtimeout.connect(socketiowithtimeout.java: 206)       at org.apache.hadoop.net.netutils.connect(netutils.java:408)       at org.apache.hadoop.hbase.ipc.hbaseclient $connection.setupiostreams(hbaseclient.java:328)       at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java: 883)       at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:750)       at org.apache.hadoop.hbase.ipc.hbaserpc $invoker.invoke(hbaserpc.java:257)       at $proxy6.closeregion(unknown source)       at org.apache.hadoop.hbase.master.servermanager.sendregionclose(servermanager.java: 589)       at org.apache.hadoop.hbase.master.assignmentmanager.unassign(assignmentmanager.java: 1093)       at org.apache.hadoop.hbase.master.assignmentmanager.unassign(assignmentmanager.java: 1040)       at org.apache.hadoop.hbase.master.assignmentmanager.balance(assignmentmanager.java: 1831)       at org.apache.hadoop.hbase.master.hmaster.balance(hmaster.java: 692)       at org.apache.hadoop.hbase.master.hmaster$1.chore(hmaster.java: 583)       at org.apache.hadoop.hbase.chore.run(chore.java:66) 2011-03-10 07:48:39,192 info org.apache.hadoop.hbase.master.hmaster: aborting 2011-03-10 07:48:39,192 info org.apache.hadoop.hbase.master.hmaster: balance hri=specialobject_speed_test,, 1299710751983.f0e5544339870a510c338b3029979d3e., src=ap13.secur2,60020,1299710609447, dest=ap12.secur2,60020,1299710609148 2011-03-10 07:48:39,192 debug org.apache.hadoop.hbase.master.assignmentmanager: starting unassignment of region specialobject_speed_test,, 1299710751983.f0e5544339870a510c338b3029979d3e. (offlining) 2011-03-10 07:48:39,852 debug org.apache.hadoop.hbase.master.hmaster: stopping service threads 2011-03-10 07:48:39,852 info org.apache.hadoop.ipc.hbaseserver: stopping server on 60000 2011-03-10 07:48:39,852 fatal org.apache.hadoop.hbase.master.hmaster: remote unexpected exception java.io.interruptedioexception: interruped while waiting for io on channel java.nio.channels.socketchannel[connection-pending remote=/ 10.x.x.18:60020]. 19340 millis timeout left.       at org.apache.hadoop.net.socketiowithtimeout $selectorpool.select(socketiowithtimeout.java:349)       at org.apache.hadoop.net.socketiowithtimeout.connect(socketiowithtimeout.java: 203)       at org.apache.hadoop.net.netutils.connect(netutils.java:408)       at org.apache.hadoop.hbase.ipc.hbaseclient $connection.setupiostreams(hbaseclient.java:328)       at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java: 883)       at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:750)       at org.apache.hadoop.hbase.ipc.hbaserpc $invoker.invoke(hbaserpc.java:257)       at $proxy6.closeregion(unknown source)       at org.apache.hadoop.hbase.master.servermanager.sendregionclose(servermanager.java: 589)       at org.apache.hadoop.hbase.master.assignmentmanager.unassign(assignmentmanager.java: 1093)       at org.apache.hadoop.hbase.master.assignmentmanager.unassign(assignmentmanager.java: 1040)       at org.apache.hadoop.hbase.master.assignmentmanager.balance(assignmentmanager.java: 1831)       at org.apache.hadoop.hbase.master.hmaster.balance(hmaster.java: 692)       at org.apache.hadoop.hbase.master.hmaster$1.chore(hmaster.java: 583)       at org.apache.hadoop.hbase.chore.run(chore.java:66) 2011-03-10 07:48:39,852 info org.apache.hadoop.hbase.master.hmaster: aborting ",
        "label": 441
    },
    {
        "text": "lightweight data transfer for class result  currently,the data transferring between 2 result objects in the same process, will cause additional/unnecessary data parsing & copying; as we have to do that via \"writables.copywritable(result1, result2)\", which internally is serialization, data copying, and de-serialization. the use case are quite common when integrated with hadoop job running;  the protocol org.apache.hadoop.mapred.recordreader defined in hadoop, provides 3 interfaces:  1) k createkey();  2) v createvalue();  3) boolean next(k key, v value) throws ioexception; in the 3rd method implementation, most likely requires the value (should be result object) to be filled, with the result object from hbase. ",
        "label": 95
    },
    {
        "text": "classes using log4j directly  below should be changed so use apache commons logging rather than log4j directly. src/test/org/apache/hadoop/hbase/abstractmergetestbase.java:import org.apache.log4j.logger; src/test/org/apache/hadoop/hbase/mapfileperformanceevaluation.java:import org.apache.log4j.logger; src/test/org/apache/hadoop/hbase/mapred/testtableindex.java:import org.apache.log4j.level; src/test/org/apache/hadoop/hbase/mapred/testtableindex.java:import org.apache.log4j.logger; src/test/org/apache/hadoop/hbase/minihbasecluster.java:import org.apache.log4j.logger; src/test/org/apache/hadoop/hbase/performanceevaluation.java~:import org.apache.log4j.logger; src/test/org/apache/hadoop/hbase/regionserver/testhregion.java:import org.apache.log4j.logger; src/test/org/apache/hadoop/hbase/regionserver/testsplit.java:import org.apache.log4j.level; src/test/org/apache/hadoop/hbase/regionserver/testsplit.java:import org.apache.log4j.logger; ",
        "label": 314
    },
    {
        "text": " hbase  create an hbase specific mapfile implementation  today, hbase uses the hadoop mapfile class to store data persistently to disk. this is convenient, as it's already done (and maintained by other people . however, it's beginning to look like there might be possible performance benefits to be had from doing an hbase-specific implementation of mapfile that incorporated some precise features. this issue should serve as a place to track discussion about what features might be included in such an implementation. ",
        "label": 547
    },
    {
        "text": "typo in thrift2 docs  there seems to be a typo in the thrift2 documentation with the example on how to start it. the example shows this: ./bin/hbase-daemon.sh stop thrift when it should be this: ./bin/hbase-daemon.sh stop thrift2 ",
        "label": 91
    },
    {
        "text": "document hboss test cases known to fail under null lock implementation  some existing test fail when the null locking implementation is used, because correctness relies on locks: [info] running org.apache.hadoop.hbase.oss.testatomicrename [error] tests run: 1, failures: 1, errors: 0, skipped: 0, time elapsed: 1.661 s <<< failure! - in org.apache.hadoop.hbase.oss.testatomicrename [error] testatomicrename(org.apache.hadoop.hbase.oss.testatomicrename)  time elapsed: 1.498 s  <<< failure! java.lang.assertionerror: rename source is still visible after rename finished or target showed up. at org.apache.hadoop.hbase.oss.testatomicrename.testatomicrename(testatomicrename.java:73) [info] running org.apache.hadoop.hbase.oss.testcreatenonrecursive [error] tests run: 2, failures: 1, errors: 0, skipped: 0, time elapsed: 1.088 s <<< failure! - in org.apache.hadoop.hbase.oss.testcreatenonrecursive [error] testcreatenonrecursiveparallel(org.apache.hadoop.hbase.oss.testcreatenonrecursive)  time elapsed: 0.872 s  <<< failure! java.lang.assertionerror: all but exactly 1 call should have thrown exceptions. experiment 1 of 10. expected:<9> but was:<0> at org.apache.hadoop.hbase.oss.testcreatenonrecursive.testcreatenonrecursiveparallel(testcreatenonrecursive.java:103) ",
        "label": 486
    },
    {
        "text": "build failing on site goal  'failed to get report for org apache maven plugins maven project info reports plugin  could not find goal 'dependency info''  i cannot reproduce locally using same mvn. let me try upgrading our report plugin. apparently 'dependency-info' is a new target since 2.5 and our version is 2.4 going by http://maven.apache.org/plugins/maven-project-info-reports-plugin/ (i can't find an explicity invocation of 'dependency-info') ",
        "label": 314
    },
    {
        "text": "orderedbytes  an ordered encoding strategy  once the spec is agreed upon, it must be implemented. ",
        "label": 339
    },
    {
        "text": "move errorhandling protos from hbase server to hbase protocol  when hbase-7206 was committed, the errorhandling protobufs were in hbase-server instead of hbase-protocol (there were moved in hbase-7185) ",
        "label": 248
    },
    {
        "text": "support variable sized chunks from chunkcreator  when cellchunkmap is created it allocates a special index chunk (or chunks) where array of cell-representations is stored. when the number of cell-representations is small, it is preferable to allocate a chunk smaller than a default value which is 2mb. on the other hand, those \"non-standard size\" chunks can not be used in pool. on-demand allocations in off-heap are costly. so this jira is about to investigate the trade of between memory usage and the final performance. ",
        "label": 35
    },
    {
        "text": "write response directly instead of creating a fake call when setup connection  we do not execute the 'call' with callrunner, so it is not necessary to create it. we can remove several fields in rpcserver.connection, and we can also remove setsasltokenresponse and setconnectionheaderresponse in rpcserver.call. ",
        "label": 149
    },
    {
        "text": "rowcounter may return incorrect result if column name is specified in command line  the rowcounter use firstkeyonlyfilter regardless of whether or not the  command line argument specified a column family (or family:qualifier).  in case when no qualifier was specified as argument, the scan will  give correct result. however in the other case the scan instance may  have been set with columns other than the very first column in the  row, causing scan to get nothing as the firstkeyonlyfilter removes  everything else. https://issues.apache.org/jira/browse/hbase-6042 is related. ",
        "label": 417
    },
    {
        "text": "opendataexception because hbaseprotos serverload cannot be converted to an open data type  i saw this error in the master log: caused by: java.lang.illegalargumentexception: method org.apache.hadoop.hbase.master.mxbean.getregionservers has parameter or return type that cannot be translated into an open type  at com.sun.jmx.mbeanserver.convertingmethod.from(convertingmethod.java:32)  at com.sun.jmx.mbeanserver.mxbeanintrospector.mfrom(mxbeanintrospector.java:63)  at com.sun.jmx.mbeanserver.mxbeanintrospector.mfrom(mxbeanintrospector.java:33)  at com.sun.jmx.mbeanserver.mbeananalyzer.initmaps(mbeananalyzer.java:118)  at com.sun.jmx.mbeanserver.mbeananalyzer.<init>(mbeananalyzer.java:99)  ... 14 more  caused by: javax.management.openmbean.opendataexception: cannot convert type: java.util.map<java.lang.string, org.apache.hadoop.hbase.serverload>  at com.sun.jmx.mbeanserver.openconverter.opendataexception(openconverter.jav ",
        "label": 186
    },
    {
        "text": "try a pool of direct byte buffers handling incoming ipc requests  ipc takes in a query by allocating a bytebuffer of the size of the request and then reading off the socket into this on-heap bb. experiment with keeping a pool of bbs so we have some buffer reuse to cut on garbage generated. could checkout from pool in rpcserver#reader. could check back into the pool when handler is done just before it queues the response on the responder's queue. we should be good since, at least for now, kvs get copied up into mslab (not references) when data gets stuffed into memstore; this should make it so no references left over when we check the bb back into the pool for use next time around. if on-heap bbs work, we could then try direct bbs (allocation of dbbs takes time so if already allocated, should be good. gc of dbbs is a pain but if in a pool, we shouldn't be wanting this to happen). the copy from socket to the dbb will be off-heap (should be fast). could start w/ the hdfs directbufferpool. it is unbounded and keeps items by size (we might want to bypass the pool if an object is > size n). dbbs for this task would contend w/ offheap bbs used in blockreadlocal when short-circuit reading. it'd be a bummer if we had to allocate big objects on-heap. would still be an improvement. ",
        "label": 340
    },
    {
        "text": "move internal classes out of hstore  memcache memcachescanner compactionreader mapfilecompactionreader hstoresize storefilescanner hstorescanner ",
        "label": 86
    },
    {
        "text": "not suitable params of storescanner in the test case teststorescanner testscansametimestamp  public void testscansametimestamp() throws ioexception {   // returns only 1 of these 2 even though same timestamp   keyvalue [] kvs = new keyvalue[] {       create(\"r1\", \"cf\", \"a\", 1, keyvalue.type.put, \"dont-care\"),       create(\"r1\", \"cf\", \"a\", 1, keyvalue.type.put, \"dont-care\"),   };   list<keyvaluescanner> scanners = arrays.aslist(       new keyvaluescanner[] {           new keyvaluescanfixture(cellcomparator.getinstance(), kvs)       });   scan scanspec = new scan().withstartrow(bytes.tobytes(\"r1\"));   // this only uses maxversions (default=1) and timerange (default=all)   try (storescanner scan = new storescanner(scanspec, scaninfo, getcols(\"a\"), scanners)) {     list<cell> results = new arraylist<>();     assertequals(true, scan.next(results));     assertequals(1, results.size());     assertequals(kvs[0], results.get(0));   } }    it will not to compare the timestamp of the two cells,because of the 'getcols(\"a\")' limit only one column and one version(default). for example,modify the timestamp of the first cell to 2 can also pass. ",
        "label": 515
    },
    {
        "text": "blockcache prefetch option  attached patch implements a prefetching function for hfile (v3) blocks, if indicated by a column family or regionserver property. the purpose of this change is to as rapidly after region open as reasonable warm the blockcache with all the data and index blocks of (presumably also in-memory) table data, without counting those block loads as cache misses. great for fast reads and keeping the cache hit ratio high. can tune the io impact versus time until all data blocks are in cache. works a bit like compactsplitthread. makes some effort not to stampede. i have been using this for setting up various experiments and thought i'd polish it up a bit and throw it out there. if the data to be preloaded will not fit in blockcache, or if as a percentage of blockcache it is large, this is not a good idea, will just blow out the cache and trigger a lot of useless gc activity. might be useful as an expert tuning option though. or not. ",
        "label": 38
    },
    {
        "text": "region of a disabling or disabled table could be stuck in transition state when rs dies during master initialization  the issue happens when a rs dies during a master starts up. after the rs reports open to the new master instance and dies immediately thereafter, the rits of disabling tables(or disabled table) on the died rs will be in rit state forever. i attached a patch to simulate the situation and you can run the following command to reproduce the issue: mvn test -plocaltests -dtest=testmasterfailover#testmasterfailoverwithmockedritondeadrs basically, we skip regions of a dead server inside am.processdeadserversandrecoverlostregions as the following code and relies on ssh to process those skipped regions:           for (pair<hregioninfo, result> deadregion : deadserver.getvalue()) {             nodes.remove(deadregion.getfirst().getencodedname());           } while in ssh, we skip regions of disabling(or disabled table) again by function processdeadregion. finally comes to the issue that rits of disabling(or disabled table) stuck there forever. ",
        "label": 543
    },
    {
        "text": "retain assignment information between cluster shutdown startup  over in hbase-57 we want to consider block locations for region assignment. this is most important during cluster startup where you currently lose all locality because regions are assignment randomly. this jira is about a shot-term solution to the cluster startup problem by retaining assignment information after a cluster shutdown and using it on the next cluster startup. ",
        "label": 247
    },
    {
        "text": "document the new changes on mapreduce stuffs  ",
        "label": 314
    },
    {
        "text": "potential unclosed tracescope in fshlog replacewriter   in the finally block starting at line 924:     } finally {       // let the writer thread go regardless, whether error or not.       if (zigzaglatch != null) {         zigzaglatch.releasesafepoint();         // it will be null if we failed our wait on safe point above.         if (syncfuture != null) blockonsync(syncfuture);       }       scope.close(); if blockonsync() throws ioexception, the tracescope would be left unclosed. ",
        "label": 339
    },
    {
        "text": "create make rc sh for hbase thirdparty  make it easier to automate the process... some notes, may not correspond to exact commands i ran... mvn clean deploy -papache-release git tag -s head git archive head -o release.tar.gz gpg --sign release.tar.gz gpg --print-mds gpg --print-md md5 gpg --print-md sha512 ",
        "label": 252
    },
    {
        "text": "hdfs space is not reclaimed when a column family is deleted  when a column family of a table is deleted, the hdfs space of the column family does not seem to be reclaimed even after a major compaction. ",
        "label": 219
    },
    {
        "text": " hbck2  add recoverededitsplayer  we need a recovered edits player. messing w/ the 'adoption service' \u2013 tooling to adopt orphan regions and hfiles \u2013 i've been manufacturing damaged clusters by moving stuff around under the running cluster. no reason to think that an hbase couldn't lose accounting of a whole region if a cataclysm. if so, region will have stuff like the '.regioninfo', dirs per column family w/ store files but it could too have a 'recovered_edits' directory with content in it. we have a walplayer for errant wals. we have the fshlog tool which can read recovered_edits content for debugging data loss. missing is a recoverededitsplayer. i took a look at extending the walplayer since it has a bunch of nice options and it can run at bulk. ideally, it would just digest recovered edits content if passed an option or recovered edits directories. on first glance, it didn't seem like an easy integration.... would be worth taking a look again. would be good if we could avoid making a new, distinct tool, just for recovered edits. the bulkload tool expects hfiles in column family directories. recovered edits files are not hfiles and the files are x-columnfamily so this is not the way to go though a bulkload-like tool that moved the recovered edits files under the appropriate region dir and asked the region reopen would be a possibility (would need the bulk load complete trick of splitting input if the region boundaries in the live cluster do not align w/ those of the errant recovered edits files). ",
        "label": 314
    },
    {
        "text": "revisit compaction configuration parameters  currently we will make the decision to split a region when a single storefile in a single family exceeds the maximum region size. this issue is about changing the decision to split to be based on the aggregate size of all storefiles in a single family (but still not aggregating across families). this would move a check to split after flushes rather than after compactions. this issue should also deal with revisiting our default values for some related configuration parameters. the motivating factor for this change comes from watching the behavior of regionservers during heavy write scenarios. today the default behavior goes like this: we fill up regions, and as long as you are not under global rs heap pressure, you will write out 64mb (hbase.hregion.memstore.flush.size) storefiles. after we get 3 storefiles (hbase.hstore.compactionthreshold) we trigger a compaction on this region. compaction queues notwithstanding, this will create a 192mb file, not triggering a split based on max region size (hbase.hregion.max.filesize). you'll then flush two more 64mb memstores and hit the compactionthreshold and trigger a compaction. you end up with 192 + 64 + 64 in a single compaction. this will create a single 320mb and will trigger a split. while you are performing the compaction (which now writes out 64mb more than the split size, so is about 5x slower than the time it takes to do a single flush), you are still taking on additional writes into memstore. compaction finishes, decision to split is made, region is closed. the region now has to flush whichever edits made it to memstore while the compaction ran. this flushing, in our tests, is by far the dominating factor in how long data is unavailable during a split. we measured about 1 second to do the region closing, master assignment, reopening. flushing could take 5-6 seconds, during which time the region is unavailable. the daughter regions re-open on the same rs. immediately when the storefiles are opened, a compaction is triggered across all of their storefiles because they contain references. since we cannot currently split a split, we need to not hang on to these references for long. this described behavior is really bad because of how often we have to rewrite data onto hdfs. imports are usually just io bound as the rs waits to flush and compact. in the above example, the first cell to be inserted into this region ends up being written to hdfs 4 times (initial flush, first compaction w/ no split decision, second compaction w/ split decision, third compaction on daughter region). in addition, we leave a large window where we take on edits (during the second compaction of 320mb) and then must make the region unavailable as we flush it. if we increased the compactionthreshold to be 5 and determined splits based on aggregate size, the behavior becomes: we fill up regions, and as long as you are not under global rs heap pressure, you will write out 64mb (hbase.hregion.memstore.flush.size) storefiles. after each memstore flush, we calculate the aggregate size of all storefiles. we can also check the compactionthreshold. for the first three flushes, both would not hit the limit. on the fourth flush, we would see total aggregate size = 256mb and determine to make a split. decision to split is made, region is closed. this time, the region just has to flush out whichever edits made it to the memstore during the snapshot/flush of the previous memstore. so this time window has shrunk by more than 75% as it was the time to write 64mb from memory not 320mb from aggregating 5 hdfs files. this will greatly reduce the time data is unavailable during splits. the daughter regions re-open on the same rs. immediately when the storefiles are opened, a compaction is triggered across all of their storefiles because they contain references. this would stay the same. in this example, we only write a given cell twice (instead of 4 times) while drastically reducing data unavailability during splits. on the original flush, and post-split to remove references. the other benefit of post-split compaction (which doesn't change) is that we then get good data locality as the resulting storefile will be written to the local datanode. in another jira, we should deal with opening up one of the daughter regions on a different rs to distribute load better, but that's outside the scope of this one. ",
        "label": 247
    },
    {
        "text": "manage a hbase branch branch and merge it to branch  ",
        "label": 149
    },
    {
        "text": "dfs failures did not shutdown regionserver  i lost three datanodes, reasons of which are still being investigated, but it has left a number of regions unable to be written to. relevant logs: 2008-12-23 02:35:59,591 warn org.apache.hadoop.hdfs.dfsclient: datastreamer exception: java.io.ioexception: connection reset by peer  at sun.nio.ch.filedispatcher.write0(native method)  at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47)  at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:122)  at sun.nio.ch.ioutil.write(ioutil.java:93)  at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:352)  at org.apache.hadoop.net.socketoutputstream$writer.performio(socketoutputstream.java:55)  at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:140)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:146)  at org.apache.hadoop.net.socketoutputstream.write(socketoutputstream.java:107)  at java.io.bufferedoutputstream.write(bufferedoutputstream.java:122)  at java.io.dataoutputstream.write(dataoutputstream.java:107)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2209) 2008-12-23 02:35:59,591 warn org.apache.hadoop.hdfs.dfsclient: dfsoutputstream responseprocessor exception for block blk_3615512604618056881_86411java.io.eofexception  at java.io.datainputstream.readfully(datainputstream.java:197)  at java.io.datainputstream.readlong(datainputstream.java:416)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$responseprocessor.run(dfsclient.java:2318) 2008-12-23 02:35:59,591 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_3615512604618056881_86411 bad datanode[0] 72.34.249.214:50010  2008-12-23 02:35:59,595 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_3615512604618056881_86411 in pipeline 72.34.249.214:50010, 72.34.249.213:50010, 72.34.249.219:50010: bad datanode 72.34.249.214:50010 2008-12-23 02:38:27,698 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.net.connectexception: connection refused  2008-12-23 02:38:27,698 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_-3678518999439029831_86910  2008-12-23 02:38:27,711 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 5392007859847346106 has been explicitly released by client  2008-12-23 02:38:30,048 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock -5905479324505886709 explicitly acquired by client  2008-12-23 02:38:33,700 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.net.connectexception: connection refused  2008-12-23 02:38:33,700 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_-226119866881174578_86911  2008-12-23 02:38:34,908 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 346704317670569896 explicitly acquired by client  2008-12-23 02:38:39,702 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.net.connectexception: connection refused  2008-12-23 02:38:39,702 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_1719395740576248920_86913  2008-12-23 02:38:40,945 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 3819942931078736534 explicitly acquired by client  2008-12-23 02:38:45,572 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 254119037927296402 explicitly acquired by client  2008-12-23 02:38:45,703 info org.apache.hadoop.hdfs.dfsclient: exception in createblockoutputstream java.net.connectexception: connection refused  2008-12-23 02:38:45,703 info org.apache.hadoop.hdfs.dfsclient: abandoning block blk_2443399503093377808_86915  2008-12-23 02:38:49,092 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 8573046623144113301 explicitly acquired by client  2008-12-23 02:38:49,385 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 7686739650257547105 explicitly acquired by client  2008-12-23 02:38:49,512 debug org.apache.hadoop.hbase.regionserver.hregionserver: row lock 5582966798894532276 explicitly acquired by client  2008-12-23 02:38:51,704 warn org.apache.hadoop.hdfs.dfsclient: datastreamer exception: java.io.ioexception: unable to create new block.  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2723)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183) 2008-12-23 02:38:51,704 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_2443399503093377808_86915 bad datanode[0] nodes == null  2008-12-23 02:38:51,704 warn org.apache.hadoop.hdfs.dfsclient: could not get block locations. aborting...  2008-12-23 02:38:51,704 fatal org.apache.hadoop.hbase.regionserver.hlog: could not append. requesting close of log  java.net.connectexception: connection refused  at sun.nio.ch.socketchannelimpl.checkconnect(native method)  at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:592)  at sun.nio.ch.socketadaptor.connect(socketadaptor.java:118)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2748)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2704)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183)  2008-12-23 02:38:51,706 error org.apache.hadoop.hbase.regionserver.logroller: log rolling failed with ioe:  java.net.connectexception: connection refused  at sun.nio.ch.socketchannelimpl.checkconnect(native method)  at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:592)  at sun.nio.ch.socketadaptor.connect(socketadaptor.java:118)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.createblockoutputstream(dfsclient.java:2748)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.nextblockoutputstream(dfsclient.java:2704)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$2000(dfsclient.java:1997)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2183)  2008-12-23 02:38:51,706 error org.apache.hadoop.hbase.regionserver.hregionserver: java.net.connectexception: connection refused ",
        "label": 314
    },
    {
        "text": "limit compaction speed  there is no speed or resource limit for compaction\uff0ci think we should add this feature especially when request burst. ",
        "label": 149
    },
    {
        "text": "upgrade mockito version for java compatibility  pasting the discussion from hbase-22534 here: \"currently mockito-core version is at 2.1.0. according to https://github.com/mockito/mockito/blob/release/2.x/doc/release-notes/official.md, looks like java 11 compatibility was introduced in 2.19+. and 2.23.2 claims to have full java 11 support after byte-buddy fix etc.\" ",
        "label": 363
    },
    {
        "text": "use less contended classes for metrics  running the benchmarks now, but it looks like the results are pretty extreme. the locking in our histograms is pretty extreme. ",
        "label": 154
    },
    {
        "text": "enable all disable all drop all can call  list  command with regex directly   created few tables. then performing disable_all operation in shell prompt.  but it is not performing operation successfully. hbase(main):043:0> disable_all '*' table12 zk0113 zk0114 disable the above 3 tables (y/n)? y/ 3 tables successfully disabled just it is showing the message but operation is not success. but the following way only performing successfully hbase(main):043:0> disable_all '*.*' table12 zk0113 zk0114 disable the above 3 tables (y/n)? y 3 tables successfully disabled ",
        "label": 309
    },
    {
        "text": "fix non daemon threads in hbase server implementation  \"pool-8-thread-3\" #7252 prio=5 os_prio=0 tid=0x00007f91040044c0 nid=0xd71e waiting on condition [0x00007f8f4d209000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00000005c0e49ed0> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:175)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:2039)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:442)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1074)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1134)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)  at java.lang.thread.run(thread.java:748) locked ownable synchronizers: none \"pool-8-thread-2\" #7251 prio=5 os_prio=0 tid=0x00007f910c010be0 nid=0xd71d waiting on condition [0x00007f8f4daab000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00000005c0e49ed0> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:175)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:2039)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:442)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1074)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1134)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)  at java.lang.thread.run(thread.java:748) locked ownable synchronizers: none \"pool-8-thread-1\" #7250 prio=5 os_prio=0 tid=0x00007f91000019d0 nid=0xd71c waiting on condition [0x00007f8f4da6a000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00000005c0e49ed0> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:175)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:2039)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:442)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1074)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1134)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)  at java.lang.thread.run(thread.java:748) locked ownable synchronizers: none \"pool-5-thread-3\" #7248 prio=5 os_prio=0 tid=0x00007f9238005ad0 nid=0xd71a waiting on condition [0x00007f8f4cb65000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00000005c0ec51e0> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:175)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:2039)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:442)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:1074)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1134)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)  at java.lang.thread.run(thread.java:748) ",
        "label": 492
    },
    {
        "text": "testloglevel broken  i think i might have broken testloglevel on hadoop 3 in hbase-22467. ca00cbeed21c639622b86967d82a70338689b347 seems to fail consistently but 1c1638f698036d6b63cb52fb94a42464d8cc1211 is passing. figure out what happened to cause this (and only on h3). ",
        "label": 328
    },
    {
        "text": " hbase thirdparty  add error prone annotations dependency explicitly and do not relocate it  ",
        "label": 352
    },
    {
        "text": " branch  testendtoendsplittransaction testmasteropswhilesplitting fails with npe  [info] running org.apache.hadoop.hbase.regionserver.testendtoendsplittransaction  [error] tests run: 3, failures: 0, errors: 1, skipped: 0, time elapsed: 50.388 s <<< failure! - in org.apache.hadoop.hbase.regionserver.testendtoendsplittransaction  [error] testmasteropswhilesplitting(org.apache.hadoop.hbase.regionserver.testendtoendsplittransaction) time elapsed: 8.903 s <<< error!  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.testendtoendsplittransaction.test(testendtoendsplittransaction.java:239)  at org.apache.hadoop.hbase.regionserver.testendtoendsplittransaction.testmasteropswhilesplitting(testendtoendsplittransaction.java:148) ",
        "label": 38
    },
    {
        "text": "chaosmonkey action for root is irrelevant  1) meta action doesn't work, it always fails to find the server holding meta, which has been the case for some time now. 2) root action is no longer relevant:  13/04/01 20:36:25 info util.chaosmonkey: performing action: restart region server holding root  org.apache.hadoop.hbase.exceptions.tablenotfoundexception: cannot find row in .meta. for table: root, row=root,,99999999999999  at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:164) ",
        "label": 406
    },
    {
        "text": "new thread introduced by hbase part is not daemon so can cause jvm to stick around on abort  \"pool-1-thread-1\" prio=10 tid=0x00007f582805c800 nid=0x6fd waiting on condition [0x00000000436a7000..0x00000000436a7c00]    java.lang.thread.state: waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x00007f583b3578e0> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)         at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)         at java.util.concurrent.delayqueue.take(delayqueue.java:160)         at java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:583)         at java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:576)         at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)         at java.lang.thread.run(thread.java:619) ",
        "label": 314
    },
    {
        "text": "split testhbasefsck in order to help with hanging tests  this one hangs regularly. let me at least add timeouts. looking in log, a bunch of tests are potentially hanging tests since they don't see to clean up after themselves. will start watching and just disable likely candidates unless someone wants to have a go at fixing this. ",
        "label": 154
    },
    {
        "text": "the hbase zkcli will connection failure the first and the second ip from zookeepermainserverarg return connection string  the zookeepermainserverarg return string like this:<ip1>,<ip2>,<ip3>:<port>, but zookeeper client connection string shoud like this :<ip1>:<port>,<ip1>:<port>,<ip1>:<port>, if changed the zookeeper client port, the zkcli can't connection the first and the second ip. ",
        "label": 314
    },
    {
        "text": " shell  purge close region command that allows by pass of master  in amv2, if a rs is not aligned with master notions of how the world is, then the master will kill the deviant rs (todo: is forcing compliance via less radical means \u2013 but that is how it is currently). the shell currently allows by-passing the master to make cluster modifications such as our being able to send a close directly to a regionserver for it to execute locally. this facility was used in the past to do fix-up when master lost account of region locations. in the new regime, such mis-accounting should no longer happen and, should a user mistakenly do an explicit close against a rs, the consequences will be more than the user bargained for; the master will shut down the rs as soon as it reports close of a region the master thinks should be open (no independence allowed!). this issue is to review shell region and table manipulation commands to purge those that by-pass master or at least to add big warning. ",
        "label": 48
    },
    {
        "text": "hlog pretty printer  we currently have a rudimentary way to print hlog data, but it is limited and currently prints key-only information. we need extend this functionality, similar to how we developed hfile's pretty printer. ideas for functionality: filter by sequence_id filter by row / region option to print values in addition to key info option to print output in json format (so scripts can easily parse for analysis) ",
        "label": 376
    },
    {
        "text": "refactor so that site materials are in the standard maven place  for some reason we currently have our site materials in src/main/site rather than the maven prescribed src/site. ",
        "label": 226
    },
    {
        "text": "pair does not deep check arrays for equality   pair does not deep check arrays for equality. it merely does x.equals for the sent object. however, with any type of array this is merely going to compare the array pointers, rather than the underlying data structure. it requires a rewriting of the private equals method in pair to check for elements being an array, then checking the underlying elements. ",
        "label": 236
    },
    {
        "text": "site  center logo  ",
        "label": 314
    },
    {
        "text": "  testinputsampler and testinputsamplertool fail under hadoop profiles   from tip of 0.94 and from 0.94.15. jon@swoop:~/proj/hbase-0.94$ mvn clean test -dhadoop.profile=2.0 -dtest=testinputsampler,testinputsamplertool -plocaltests ... running org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsamplertool tests run: 4, failures: 0, errors: 3, skipped: 0, time elapsed: 3.718 sec <<< failure! running org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsampler tests run: 2, failures: 0, errors: 2, skipped: 0, time elapsed: 0.666 sec <<< failure! results : tests in error:    testsplitinterval(org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsamplertool): failed getting constructor   testsplitramdom(org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsamplertool): failed getting constructor   testsplitsample(org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsamplertool): failed getting constructor   testsplitsampler(org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsampler): failed getting constructor   testintervalsampler(org.apache.hadoop.hbase.mapreduce.hadoopbackport.testinputsampler): failed getting constructor tests run: 6, failures: 0, errors: 5, skipped: 0 ",
        "label": 286
    },
    {
        "text": "update docs   build for maven   our new hbase-spark module raises our minimum maven version from 3.0.0 (though i've only tried 3.0.3) to 3.0.4: [error] failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:add-source (scala-compile-first) on project hbase-spark: the plugin net.alchim31.maven:scala-maven-plugin:3.2.0 requires maven version 3.0.4 -> [help 1] update the docs to call out 3.0.4 and add an enforcer rule so that this failure can happen at the start of a build rather than 15 minutes in. ",
        "label": 284
    },
    {
        "text": "testvisibilitylabels fails occasionally  i got the following test failures running test suite on hadoop-2 where distributed log replay was turned on : testaddvisibilitylabelsonrsrestart(org.apache.hadoop.hbase.security.visibility.testvisibilitylabels)  time elapsed: 0.019 sec  <<< failure! java.lang.assertionerror: the count should be 8 expected:<8> but was:<6>   at org.junit.assert.fail(assert.java:88)   at org.junit.assert.failnotequals(assert.java:743)   at org.junit.assert.assertequals(assert.java:118)   at org.junit.assert.assertequals(assert.java:555)   at org.apache.hadoop.hbase.security.visibility.testvisibilitylabels.testaddvisibilitylabelsonrsrestart(testvisibilitylabels.java:408) ... testclearuserauths(org.apache.hadoop.hbase.security.visibility.testvisibilitylabels)  time elapsed: 0.002 sec  <<< failure! java.lang.assertionerror   at org.junit.assert.fail(assert.java:86)   at org.junit.assert.asserttrue(assert.java:41)   at org.junit.assert.asserttrue(assert.java:52)   at org.apache.hadoop.hbase.security.visibility.testvisibilitylabels.testclearuserauths(testvisibilitylabels.java:505) logs to be attached ",
        "label": 46
    },
    {
        "text": "improve the 'too many blocks' message on ui blockcache status page  if metrics calculations over blockcache contents stopped after examining hbase.ui.blockcache.by.file.max items, the ui will put up a message. however, this notion of \"too many blocks\" / fullness refers to structures used for calculating blockcache metrics. see blockcacheutil. we should improve this message so it does not leave a user the impression the blockcache may be in a bad state. ",
        "label": 323
    },
    {
        "text": "proxy created by hfilesystem createreorderingproxy  should properly close when connecting to ha namenode  proxy to ha namenode with qjm created from org.apache.hadoop.hdfs.server.namenode.ha.configuredfailoverproxyprovider should close properly. mail archive 13/11/26 09:55:55 error ipc.rpc: rpc.stopproxy called on non proxy. java.lang.illegalargumentexception: object is not an instance of declaring class         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:266)         at $proxy16.close(unknown source)         at org.apache.hadoop.ipc.rpc.stopproxy(rpc.java:621)         at org.apache.hadoop.hdfs.dfsclient.closeconnectiontonamenode(dfsclient.java:738)         at org.apache.hadoop.hdfs.dfsclient.close(dfsclient.java:794)         at org.apache.hadoop.hdfs.distributedfilesystem.close(distributedfilesystem.java:847)         at org.apache.hadoop.fs.filesystem$cache.closeall(filesystem.java:2524)         at org.apache.hadoop.fs.filesystem$cache$clientfinalizer.run(filesystem.java:2541)         at org.apache.hadoop.util.shutdownhookmanager$1.run(shutdownhookmanager.java:54) 13/11/26 09:55:55 warn util.shutdownhookmanager: shutdownhook 'clientfinalizer' failed, org.apache.hadoop.hadoopillegalargumentexception: cannot close proxy - is not closeable or does not provide closeable invocation handler class $proxy16 org.apache.hadoop.hadoopillegalargumentexception: cannot close proxy - is not closeable or does not provide closeable invocation handler class $proxy16         at org.apache.hadoop.ipc.rpc.stopproxy(rpc.java:639)         at org.apache.hadoop.hdfs.dfsclient.closeconnectiontonamenode(dfsclient.java:738)         at org.apache.hadoop.hdfs.dfsclient.close(dfsclient.java:794)         at org.apache.hadoop.hdfs.distributedfilesystem.close(distributedfilesystem.java:847)         at org.apache.hadoop.fs.filesystem$cache.closeall(filesystem.java:2524)         at org.apache.hadoop.fs.filesystem$cache$clientfinalizer.run(filesystem.java:2541)         at org.apache.hadoop.util.shutdownhookmanager$1.run(shutdownhookmanager.java:54) ",
        "label": 441
    },
    {
        "text": "ui visibility into zookeeper  add zookeeper information/administration to ui. discussion showed particular interest in a tree-viewer application, something like zookeeper-418. there was talk between lars/jimk about how often the viewer should update its data. see hbase-1329 for more information. ",
        "label": 314
    },
    {
        "text": "mvn assembly is over filling the hbase lib dir  here is what our lib dir looks this in 0.90.1: -rwxr-xr-x  1 stack  staff    62983 mar 16  2009 activation-1.1.jar -rwxr-xr-x  1 stack  staff  1034049 may 21  2009 ant-1.6.5.jar -rwxr-xr-x  1 stack  staff  1323005 jul 20  2009 ant-1.7.1.jar -rwxr-xr-x  1 stack  staff    12143 jul 20  2009 ant-launcher-1.7.1.jar -rwxr-xr-x  1 stack  staff    43033 may  5  2009 asm-3.1.jar -rwxr-xr-x  1 stack  staff   339831 oct 18 10:05 avro-1.3.3.jar -rwxr-xr-x  1 stack  staff    41123 dec  8  2009 commons-cli-1.2.jar -rwxr-xr-x  1 stack  staff    58160 oct 18 10:05 commons-codec-1.4.jar -rwxr-xr-x  1 stack  staff   112341 mar 16  2009 commons-el-1.0.jar -rwxr-xr-x  1 stack  staff   305001 mar 16  2009 commons-httpclient-3.1.jar -rwxr-xr-x  1 stack  staff   279193 may 17  2010 commons-lang-2.5.jar -rwxr-xr-x  1 stack  staff    60686 mar 13  2009 commons-logging-1.1.1.jar -rwxr-xr-x  1 stack  staff   180792 mar  4  2010 commons-net-1.4.1.jar -rwxr-xr-x  1 stack  staff  3566844 jun  5  2009 core-3.1.1.jar -rwxr-xr-x  1 stack  staff   936397 oct 18 10:05 guava-r06.jar -rwxr-xr-x  1 stack  staff  2707856 jan 11 13:26 hadoop-core-0.20-append-r1056497.jar -rwxr-xr-x  1 stack  staff  2241521 feb  9 15:57 hbase-0.90.1.jar -rwxr-xr-x  1 stack  staff   706710 mar  4  2010 hsqldb-1.8.0.10.jar -rwxr-xr-x  1 stack  staff   171958 oct 18 10:05 jackson-core-asl-1.5.5.jar -rwxr-xr-x  1 stack  staff    17065 oct 18 10:05 jackson-jaxrs-1.5.5.jar -rwxr-xr-x  1 stack  staff   386509 oct 18 10:05 jackson-mapper-asl-1.4.2.jar -rwxr-xr-x  1 stack  staff    24745 oct 18 10:05 jackson-xc-1.5.5.jar -rwxr-xr-x  1 stack  staff   408133 may 21  2010 jasper-compiler-5.5.23.jar -rwxr-xr-x  1 stack  staff    76844 may 17  2010 jasper-runtime-5.5.23.jar -rwxr-xr-x  1 stack  staff   103515 may  6  2009 jaxb-api-2.1.jar -rwxr-xr-x  1 stack  staff   867801 mar  4  2010 jaxb-impl-2.1.12.jar -rwxr-xr-x  1 stack  staff   455517 oct 18 10:05 jersey-core-1.4.jar -rwxr-xr-x  1 stack  staff   142827 oct 18 10:05 jersey-json-1.4.jar -rwxr-xr-x  1 stack  staff   677600 oct 18 10:05 jersey-server-1.4.jar -rwxr-xr-x  1 stack  staff   377780 mar  4  2010 jets3t-0.7.1.jar -rwxr-xr-x  1 stack  staff    67758 may  6  2009 jettison-1.1.jar -rwxr-xr-x  1 stack  staff   539912 jan  3 16:51 jetty-6.1.26.jar -rwxr-xr-x  1 stack  staff   177131 jan  3 16:51 jetty-util-6.1.26.jar -rwxr-xr-x  1 stack  staff    87325 jul 20  2009 jline-0.9.94.jar -rwxr-xr-x  1 stack  staff  4477138 jan  3 16:51 jruby-complete-1.0.3.jar -rwxr-xr-x  1 stack  staff  1024680 may 17  2010 jsp-2.1-6.1.14.jar -rwxr-xr-x  1 stack  staff   134910 may 17  2010 jsp-api-2.1-6.1.14.jar -rwxr-xr-x  1 stack  staff    46367 mar  4  2010 jsr311-api-1.1.1.jar -rwxr-xr-x  1 stack  staff   121070 mar 13  2009 junit-3.8.1.jar -rwxr-xr-x  1 stack  staff    11981 mar  4  2010 kfs-0.3.jar -rwxr-xr-x  1 stack  staff   481535 oct 18 10:05 log4j-1.2.16.jar -rwxr-xr-x  1 stack  staff    65261 apr 14  2009 oro-2.0.8.jar -rwxr-xr-x  1 stack  staff    29392 jun 14  2010 paranamer-2.2.jar -rwxr-xr-x  1 stack  staff     5420 jun 14  2010 paranamer-ant-2.2.jar -rwxr-xr-x  1 stack  staff     6931 jun 14  2010 paranamer-generator-2.2.jar -rwxr-xr-x  1 stack  staff   328635 mar  4  2010 protobuf-java-2.3.0.jar -rwxr-xr-x  1 stack  staff   173236 jun 14  2010 qdox-1.10.1.jar drwxr-xr-x  7 stack  staff      238 feb  8 16:23 ruby -rwxr-xr-x  1 stack  staff   132368 may 17  2010 servlet-api-2.5-6.1.14.jar -rwxr-xr-x  1 stack  staff    23445 mar  4  2010 slf4j-api-1.5.8.jar -rwxr-xr-x  1 stack  staff     9679 mar  4  2010 slf4j-log4j12-1.5.8.jar -rwxr-xr-x  1 stack  staff    26514 may  6  2009 stax-api-1.0.1.jar -rwxr-xr-x  1 stack  staff   187530 mar  4  2010 thrift-0.2.0.jar -rwxr-xr-x  1 stack  staff    15010 mar  4  2010 xmlenc-0.52.jar -rwxr-xr-x  1 stack  staff   598364 dec 10 15:13 zookeeper-3.3.2.jar we are picking up bunch of hadoop dependencies. i'd think it harmless other than the bulk. ",
        "label": 314
    },
    {
        "text": "usability improvements to htablepool  a discussion on the hbase user mailing list (http://markmail.org/thread/7leeha56ny5mwecg) led to some suggested improvements for the org.apache.hadoop.hbase.client.htablepool class. i will be submitting a patch that contains the following changes to htablepool: remove constructors that were not used. change access to remaining contstructor from public to private to enforce use of the static factory method getpool. change internal map from treemap to hashmap because i couldn't see any reason it needed to be sorted. remove hbaseconfiguration and tablename member variables since they aren't really properties of the pool itself. they are associated with the htable that should get instantiated when one is requested from the pool, but not already there. ",
        "label": 274
    },
    {
        "text": "allow table name expressed in regex in drop  disable  enable operations  ability to disable, drop and enable tables using regex expression is desirable. ",
        "label": 246
    },
    {
        "text": "administrative functions for table region maintenance  it would be useful to have some administrative functions available through htable or hbaseadmin. the two functions i'm thinking of right now are to force memcache flushes (on all regions, or individually specified) and to run major compactions (on all regions, or individually specified). one reason to have this is currently major compactions run once a day by default. the time that they run is related to when you brought up your cluster or created the tables. in my case, this has been during peak load rather than offpeak times. we have times that we run other administrative tasks like backups, cleanup, etc... during off times, and this would be a good time to also trigger major compactions and memcache flushes. memcache force flushing can also be useful in the case that your cluster starts to have issues. they might be isolated to a single region or regionserver, but with lots of edits sitting in memcache and potentially unappended hlogs, you want to just flush things out to remove the possibility of losing anything. ",
        "label": 314
    },
    {
        "text": "use openjdk7 instead of oracle jdk7 in pre commit docker file  context: https://builds.apache.org/job/precommit-hbase-build/7064/console  ref: http://www.webupd8.org/2017/06/why-oracle-java-7-and-6-installers-no.html i make this as critical because it is hard to get a +1 from hadoopqa for branch-1. ",
        "label": 149
    },
    {
        "text": "data block encoding of keyvalues  aka delta encoding   prefix compression  a compression for keys. keys are sorted in hfile and they are usually very similar. because of that, it is possible to design better compression than general purpose algorithms, it is an additional step designed to be used in memory. it aims to save memory in cache as well as speeding seeks within hfileblocks. it should improve performance a lot, if key lengths are larger than value lengths. for example, it makes a lot of sense to use it when value is a counter. initial tests on real data (key length = ~ 90 bytes , value length = 8 bytes) shows that i could achieve decent level of compression:  key compression ratio: 92%  total compression ratio: 85%  lzo on the same data: 85%  lzo after delta encoding: 91%  while having much better performance (20-80% faster decompression ratio than lzo). moreover, it should allow far more efficient seeking which should improve performance a bit. it seems that a simple compression algorithms are good enough. most of the savings are due to prefix compression, int128 encoding, timestamp diffs and bitfields to avoid duplication. that way, comparisons of compressed data can be much faster than a byte comparator (thanks to prefix compression and bitfields). in order to implement it in hbase two important changes in design will be needed:  -solidify interface to hfileblock / hfilereader scanner to provide seeking and iterating; access to uncompressed buffer in hfileblock will have bad performance  -extend comparators to support comparison assuming that n first bytes are equal (or some fields are equal) link to a discussion about something similar:  http://search-hadoop.com/m/5aqgxjenad1/hbase+windows&subj=re+prefix+compression ",
        "label": 324
    },
    {
        "text": "nullpointerexception when accessing master web ui while master is initializing  probably i tried to check the master web ui too soon. i got some internal error page. in the master log, there is such exception: 2012-08-17 16:06:25,146 error org.mortbay.log: /master-status java.lang.nullpointerexception         at org.apache.hadoop.hbase.master.hmaster.iscatalogjanitorenabled(hmaster.java:1213)         at org.apache.hadoop.hbase.master.masterstatusservlet.doget(masterstatusservlet.java:72)         at javax.servlet.http.httpservlet.service(httpservlet.java:707)         at javax.servlet.http.httpservlet.service(httpservlet.java:820)         at org.mortbay.jetty.servlet.servletholder.handle(servletholder.java:511)         at org.mortbay.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1221)         at org.apache.hadoop.http.httpserver$quotinginputfilter.dofilter(httpserver.java:835)         at org.mortbay.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1212)         at org.mortbay.jetty.servlet.servlethandler.handle(servlethandler.java:399)         at org.mortbay.jetty.security.securityhandler.handle(securityhandler.java:216)         at org.mortbay.jetty.servlet.sessionhandler.handle(sessionhandler.java:182)         at org.mortbay.jetty.handler.contexthandler.handle(contexthandler.java:766)         at org.mortbay.jetty.webapp.webappcontext.handle(webappcontext.java:450)         at org.mortbay.jetty.handler.contexthandlercollection.handle(contexthandlercollection.java:230)         at org.mortbay.jetty.handler.handlerwrapper.handle(handlerwrapper.java:152)         at org.mortbay.jetty.server.handle(server.java:326)         at org.mortbay.jetty.httpconnection.handlerequest(httpconnection.java:542)         at org.mortbay.jetty.httpconnection$requesthandler.headercomplete(httpconnection.java:928)         at org.mortbay.jetty.httpparser.parsenext(httpparser.java:549)         at org.mortbay.jetty.httpparser.parseavailable(httpparser.java:212)         at org.mortbay.jetty.httpconnection.handle(httpconnection.java:404)         at org.mortbay.io.nio.selectchannelendpoint.run(selectchannelendpoint.java:410)         at org.mortbay.thread.queuedthreadpool$poolthread.run(queuedthreadpool.java:582) ",
        "label": 242
    },
    {
        "text": "port hadoop   threading scalability for rpc reads   to hbase  hadoop-6713 has patch to fix the read scalability of hadoop rpc. right now a single thread accepts() then receives the rpc payload for every single rpc in hbase. including object creation, writable deserialization, etc. apply the patch from that issue to our own forked hbaserpc code. ",
        "label": 547
    },
    {
        "text": "failing because of missing commons io after upgrade to hadoop   see this note: http://search-hadoop.com/m/0uror19bg8v1/test+failure+after+upgrading+to+hadoop+1.0.3+was%253a+classnotfoundexception%253a+org.apache.commons.io.fileutils&subj=test+failure+after+upgrading+to+hadoop+1+0+3+was+classnotfoundexception+org+apache+commons+io+fileutils ",
        "label": 314
    },
    {
        "text": "edits can be appended out of seqid order since hbase  create a table with 1000 splits, after the region assignemnt, kill the regionserver wich contains meta table. here few regions are missing after the log splitting and region assigment. hbck report shows multiple region holes are got created. same scenario was verified mulitple times in 0.92.1, no issues. ",
        "label": 286
    },
    {
        "text": "balancer will try to rebalance thousands of regions in one go  needs an upper bound added   see hbase-3420. therein, a wonky cluster had 5k regions on one server and < 1k on others. balancer ran and wanted to redistribute 3k+ all in one go. madness. if a load of rebalancing to be done, should be done somewhat piecemeal. we need maximum regions to rebalance at a time upper bound at a minimum. ",
        "label": 441
    },
    {
        "text": "rsgroupadminendpoint precreatetable triggers tablenotfoundexception  in a cluster where rs group endpoint is installed, i noticed the following in master log: 2017-12-13 21:42:14,230 error [rpcserver.default.fpbq.fifo.handler=29,queue=2,port=20000] master.tablestatemanager: unable to get table ltt-diff state org.apache.hadoop.hbase.tablenotfoundexception: ltt-diff         at org.apache.hadoop.hbase.master.tablestatemanager.gettablestate(tablestatemanager.java:175)         at org.apache.hadoop.hbase.master.tablestatemanager.istablestate(tablestatemanager.java:132)         at org.apache.hadoop.hbase.master.assignment.assignmentmanager.istabledisabled(assignmentmanager.java:345)         at org.apache.hadoop.hbase.rsgroup.rsgroupadminserver.movetables(rsgroupadminserver.java:409)         at org.apache.hadoop.hbase.rsgroup.rsgroupadminendpoint.assigntabletogroup(rsgroupadminendpoint.java:334)         at org.apache.hadoop.hbase.rsgroup.rsgroupadminendpoint.precreatetable(rsgroupadminendpoint.java:346)         at org.apache.hadoop.hbase.master.mastercoprocessorhost$11.call(mastercoprocessorhost.java:319)         at org.apache.hadoop.hbase.master.mastercoprocessorhost$11.call(mastercoprocessorhost.java:316)         at org.apache.hadoop.hbase.coprocessor.coprocessorhost$observeroperationwithoutresult.callobserver(coprocessorhost.java:599)         at org.apache.hadoop.hbase.coprocessor.coprocessorhost.execoperation(coprocessorhost.java:672)         at org.apache.hadoop.hbase.master.mastercoprocessorhost.precreatetable(mastercoprocessorhost.java:316)         at org.apache.hadoop.hbase.master.hmaster$3.run(hmaster.java:1738)         at org.apache.hadoop.hbase.master.procedure.masterprocedureutil.submitprocedure(masterprocedureutil.java:134)         at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1734)         at org.apache.hadoop.hbase.master.masterrpcservices.createtable(masterrpcservices.java:559) it seems rsgroupadminendpoint#precreatetable should take into account that the table doesn't exist and avoid the extraneous tablenotfoundexception . ",
        "label": 38
    },
    {
        "text": "hlogkey walkey  constructor needs to be either removed  deprecated or fixed  it is currently impossible to use hlogkey(walkey) because of the npe that occurs due to constructor itself not initializing clusterids but instead immediately calling readfieldsfrompb which then references it:  https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/hlogkey.java#l475 ",
        "label": 382
    },
    {
        "text": "implement new open close logic in handlers and stop using heartbeats for open close messages  this issue is doing the meat of what hbase-2485 is about and continues what was started in hbase-2694 after some code cleanup to make life easier. this deals with no longer piggybacking messages from master to regionservers on heartbeat responses and instead sending direct unsolicited messages. this also deals with moving the open/close logic fully into handlers and removing the existing open/close code on both the rs and m sides. there may also be some changes to the master in-memory state of regions in transition. the new load balancer will probably be introduced with this issue but not fully integrated yet. ",
        "label": 247
    },
    {
        "text": "change rpc callqueue size from  handlercount   max queue size per handler   yesterday debugging w/ jack we noticed that with few handlers on a big box, he was seeing stats like this: 2011-04-21 11:54:49,451 debug org.apache.hadoop.ipc.hbaseserver: server connection from x.x.x.x:60931; # active connections: 11; # queued calls: 2500 we had 2500 items in the rpc queue waiting to be processed. turns out he had too few handlers for number of clients (but also, it seems like he figured hw issues in that his ram bus was running at 1/4 the rate that it should have been running at). chatting w/ j-d this morning, he asked if the queues hold 'data'. the queues hold 'calls'. calls are the client request. they contain data. jack had 2500 items queued. if each item to insert was 1mb, thats 2.5k * 1mb of memory that is outside of our generally accounting. currently the queue size is handlers * max_queue_size_per_handler where max_queue_size_per_handler is hardcoded to be 100. if the queue is full we block (linkedblockingqueue). going to change the queue size from 100 to 10 by default \u2013 but also will make it configurable and will doc. this as possible cause of oome. will try it on production here before committing patch. ",
        "label": 314
    },
    {
        "text": "throttle region opens  when a dead region server comes up, the regions that belonged to the region server are assigned to it very aggressively. this causes a increase in the get latencies over all these regions which are under movement and hurt the performance. instead spreading out this process over a period of time will be beneficial. ",
        "label": 154
    },
    {
        "text": "frequent failed to parse at eof warnings from walentrystream  lots of messages like this 2017-08-17 15:10:36,363 info [main-eventthread.replicationsource,2.replicationsource.replicationwalreaderthread.onyx%2c8120%2c1503007587035,2] regionserver.walentrystream: reached the end of wal file 'hdfs://localhost:8020/hbase-1/wals/onyx,8120,1503007587035/onyx%2c8120%2c1503007587035.1503007826083'. it was not closed cleanly, so we did not parse 8 bytes of data. fix this, either the message (if harmless) or the underlying cause. ",
        "label": 38
    },
    {
        "text": "move splitlogmanager splitlog taskstate and assignmentmanager regiontransitiondata znode datas to pb  ",
        "label": 314
    },
    {
        "text": "regularize tostring  make all of our tostrings work the same. while at it, make them ruby hash style so they play well in the (jruby) shell ",
        "label": 314
    },
    {
        "text": "make chaosmonkey   loadtest tools extensible  to allow addition of more actions and policies   let me split this requirement into 2 parts: i) chaosmonkey  i was trying to add more tests around new actions and policies by leveraging the existing classes nested inside chaosmonkey.  but it turned out that some of the classes cannot be used outside, unless we make those visible to the world.  here is an example:  i cannot extend chaosmonkey.action, as the init(actioncontext context) method has package-wide visibility.  there are other places as well which makes it impossible for anyone to extend on top of this hierarchy. ii) loadtesttool  i wanted to extend this tool to define failure/pass criteria based on % of read/write failed, rather than comparing against absolute 0.   for that this beautiful class should mark some of its properties usable by its child, by marking those protected. i wanted to get unblocked here first.   once this gets fixed, i think i can take up a jira item to refactor these tools, if required. ",
        "label": 530
    },
    {
        "text": "htable unnecessarily coupled with hmaster  htable constructor calls \"getcurrentnrhrs()\" to get the region server count for thread pool creation. this code calls hbaseadmin.getclusterstatus() [aka: the hmaster] to get the server count. this information can be scraped from counting the zookeeper /hbase/rs/--- znodes. need to remove unnecessary master queries when zookeeper can do the same job. ",
        "label": 341
    },
    {
        "text": "start rsgroupinfomanager as default  start rsgroupinfomanager as default. if no rsgroup information, all regionservers belong to default group. ",
        "label": 149
    },
    {
        "text": "serializing  list  containing null elements will cause nullpointerexception in hbaseobjectwritable writeobject   an error case could be in coprocessor aggregationclient, the median() function handles an empty region and returns a list object with the first element as a null value. npe occurs in the rpc response stage and the response never gets sent. ",
        "label": 482
    },
    {
        "text": "should add the synchronous parameter for the xxxswitch method in asyncadmin  for now we always pass true to hmaster, maybe the decision is that user just do not need to calling get on the returned future if it wants asynchronous. but the problem here is that, the return value is not void, it is a boolean, which is the previous state of the flag, sometimes users do not need to wait until the previous transitions or split/merge to complete, but they still want to get the previous value of the flag. so we still need to provide the synchronous parameter. ",
        "label": 149
    },
    {
        "text": "make it easier to add per cf metrics  add some key per cf metrics to start with  add plumbing needed to add various types of per columnfamily metrics. and to start with add a bunch per-cf metrics such as: 1) blocks read, cache hit, avg time of read for a column family.  2) similar stats for compaction related reads.  3) stats for meta block reads per cf  4) bloom filter stats per cf  etc. ",
        "label": 154
    },
    {
        "text": "thrift2 does not close scanner instance  closescanner at thrifthbaseservicehandler simply remove scannerid from internal map, but fails to close the scanner instance.   public void closescanner(int scannerid) throws tioerror, tillegalargument, texception {     if (removescanner(scannerid) == null) {       tillegalargument ex = new tillegalargument();       ex.setmessage(\"invalid scanner id\");       throw ex;     }   } ",
        "label": 193
    },
    {
        "text": "regions stuck in transition after rolling restart  perpetual timeout handling but nothing happens  the rolling restart script is great for bringing on the weird stuff. on my little loaded cluster if i run it, it horks the cluster and it doesn't recover. i notice two issues that need fixing: 1. we'll miss noticing that a server was carrying .meta. and it never gets assigned \u2013 the shutdown handlers get stuck in perpetual wait on a .meta. assign that will never happen.  2. perpetual cycling of the this sequence per region not succesfully assigned:  2010-10-23 21:37:57,404 info org.apache.hadoop.hbase.master.assignmentmanager: regions in transition timed out:  usertable,user510588360,1287547556587.7f2d92497d2d03917afd574ea2aca55b. state=pending_open,                       ts=1287869814294  45154 2010-10-23 21:37:57,404 info org.apache.hadoop.hbase.master.assignmentmanager: region has been pending_open or opening for too long, reassigning region=usertable,user510588360,1287547556587.                                     7f2d92497d2d03917afd574ea2aca55b.  45155 2010-10-23 21:37:57,404 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x2bd57d1475046a attempting to transition node 7f2d92497d2d03917afd574ea2aca55b from rs_zk_region_opening to m_zk_region_offline  45156 2010-10-23 21:37:57,404 warn org.apache.hadoop.hbase.zookeeper.zkassign: master:60000-0x2bd57d1475046a attempt to transition the unassigned node for 7f2d92497d2d03917afd574ea2aca55b from rs_zk_region_opening to                 m_zk_region_offline failed, the node existed but was in the state m_zk_region_offline  45157 2010-10-23 21:37:57,404 info org.apache.hadoop.hbase.master.assignmentmanager: region transitioned opening to offline so skipping timeout, region=usertable,user510588360,1287547556587.7f2d92497d2d03917afd574ea2aca55b.   ,,, timeout period again elapses an then same sequence. this is what i've been working on. ",
        "label": 314
    },
    {
        "text": "review improve hlog compression's memory consumption  from ram in http://mail-archives.apache.org/mod_mbox/hbase-dev/201205.mbox/%3c00bc01cd31e6$7caf1320$760d3960$%25vasudevan@huawei.com%3e: one small observation after giving +1 on the rc.  the wal compression feature causes oome and causes full gc. the problem is, if we have 1500 regions and i need to create recovered.edits  for each of the region (i don\u2019t have much data in the regions (~300mb)).  now when i try to build the dictionary there is a node object getting  created.  each node object occupies 32 bytes.  we have 5 such dictionaries. initially we create indextonodes array and its size is 32767. so now we have 32*5*32767 = ~5mb. now i have 1500 regions. so 5mb*1500 = ~7gb.(excluding actual data). this seems to a very high  initial memory foot print and this never allows me to split the logs and i  am not able to make the cluster up at all. our configured heap size was 8gb, tested in 3 node cluster with 5000  regions, very less data( 1gb in hdfs cluster including replication), some  small data is spread evenly across all regions. the formula is 32(node object size)*5(no of dictionary)*32767(no of node  objects)*noofregions. ",
        "label": 544
    },
    {
        "text": "re expose cellcomparator  on hbase-18995 we moved a bunch of public methods to private places. this inadvertently breaks donwstream consumers. let's see if we can ease up on some of the lockdown and make life easier for them. copying ramkrishna.s.vasudevan's previous analysis: i read the crunch projec't hbase-support related code.   -> it uses both cellutil (public exposed) and keyvalueutil (@private) classes for helper methods.  -> all methods in cellutil that are getting used are even now exposed in branch-2's cellutil and they are very common helper methods. so we are safe here.  -> wrt keyvalueutil the api is createfirstonrow(). it is used in test cases and in some core code. in most of the places they are trying to create the splitkeys from the region's start keys and that is also getting persisted. i think here they can safely create a cell out of the given byte[] of the row.  but there is one place where they are trying to do some scanning on a hfilescanner directly (@private) scanner. so this should be changed because it is an internal interface for us. and on this scanner they have copied our seekto() code into their source files for some scanning purpose. in this code they are actually using the kvutil.createfirstonrow() to seek to that first cell of that row.  more over i think in branch-2 we are restricting even cps from accessing some of our internal scanners and they can only use internalscanner interface. so this code in crunch needs heavy refactoring to work with branch-2 in case they want to fit into the public/private exposed semantics that hbase presents to the downstreamers.  -> if still they want some apis like this we can expose cellutil#createfirstonrow, createlastonrow, createfirstoncol and createlastoncol at the maximum. i think others are not useful and are more internal stuffs. ",
        "label": 320
    },
    {
        "text": " hbck2  setregionstate should update master in memory state too  setregionstate changes the hbase:meta table info:state column. it does not alter the master's in-memory state. this means you have to kill master and have another assume active master role of a state-change to be noticed. better if the setregionstate just went via master and updated master and hbase:meta. ",
        "label": 486
    },
    {
        "text": "update to hadoop and zk  ",
        "label": 314
    },
    {
        "text": " fb  allow individual classes to get notified when configuration is reloaded  the purpose of this feature is to let individual classes who are interested in observing online configuration changes, to be notified when the conf is reloaded from disk. this is the second part of the online configuration upgrade feature (the first being hbase-8544). ",
        "label": 181
    },
    {
        "text": "hfileprettyprinter processfile should return immediately if file does not exist  hfileprettyprinter#processfile should return immediately if file does not exists same like hlogprettyprinter#run     if (!fs.exists(file)) {       system.err.println(\"error, file doesnt exist: \" + file);     } ",
        "label": 53
    },
    {
        "text": "during master startup  rs can be you are dead ed by master in error  not sure of the root cause yet, i am at \"how did this ever work\" stage.  we see this problem in 0.96.1, but didn't in 0.96.0 + some patches. it looks like rs information arriving from 2 sources - zk and server itself, can conflict. master doesn't handle such cases (timestamp match), and anyway technically timestamps can collide for two separate servers. so, master youaredead-s the already-recorded reporting rs, and adds it too. then it discovers that the new server has died with fatal error! note the threads.  addition is called from master initialization and from rpc. 2013-12-19 11:16:45,290 info  [master:h2-ubuntu12-sec-1387431063-hbase-10:60000] master.servermanager: finished waiting for region servers count to settle; checked in 2, slept for 18262 ms, expecting minimum of 1, maximum of 2147483647, master is running. 2013-12-19 11:16:45,290 info  [master:h2-ubuntu12-sec-1387431063-hbase-10:60000] master.servermanager: registering server=h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 2013-12-19 11:16:45,290 info  [master:h2-ubuntu12-sec-1387431063-hbase-10:60000] master.hmaster: registered server found up in zk but who has not yet reported in: h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 2013-12-19 11:16:45,380 info  [rpcserver.handler=4,port=60000] master.servermanager: triggering server recovery; existingserver h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 looks stale, new server:h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 2013-12-19 11:16:45,380 info  [rpcserver.handler=4,port=60000] master.servermanager: master doesn't enable servershutdownhandler during initialization, delay expiring server h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 ... 2013-12-19 11:16:46,925 error [rpcserver.handler=7,port=60000] master.hmaster: region server h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 reported a fatal error: aborting region server h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800: org.apache.hadoop.hbase.youaredeadexception: server report rejected; currently processing h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 as dead server presumably some of the recent zk listener related changes b ",
        "label": 406
    },
    {
        "text": "add timerange support into checkandmutate  ",
        "label": 98
    },
    {
        "text": "rename rowmutation to rowmutations  ",
        "label": 34
    },
    {
        "text": "hmasterinterface ismasterrunning  requires clean up  this jira is in reference to jd's comments regarding the clean up needed in ismasterrunning(). refer to   https://issues.apache.org/jira/browse/hbase-6240?focusedcommentid=13400772&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13400772 ",
        "label": 544
    },
    {
        "text": "divide hlog interface into user and implementor specific interfaces  hbase-5937 introduces the hlog interface as a first step to support multiple wal implementations. this interface is a good start, but has some limitations/drawbacks in its current state, such as:  1) there is no clear distinction b/w user and implementor apis, and it provides apis both for wal users (append, sync, etc) and also wal implementors (reader/writer interfaces, etc). there are apis which are very much implementation specific (getfilenum, etc) and a user such as a regionserver shouldn't know about it.  2) there are about 14 methods in fshlog which are not present in hlog interface but are used at several places in the unit test code. these tests typecast hlog to fshlog, which makes it very difficult to test multiple wal implementations without doing some ugly checks. i'd like to propose some changes in hlog interface that would ease the multi wal story: 1) have two interfaces wal and walservice. wal provides apis for implementors. walservice provides apis for users (such as regionserver).  2) a skeleton implementation of the above two interface as the base class for other wal implementations (abstractwal). it provides required fields for all subclasses (fs, conf, log dir, etc). make a minimal set of test only methods and add this set in abstractwal.  3) hlogfactory returns a walservice reference when creating a wal instance; if a user need to access impl specific apis (there are unit tests which get wal from a hregionserver and then call impl specific apis), use abstractwal type casting,  4) make testhlog abstract and let all implementors provide their respective test class which extends testhlog (testfshlog, for example). ",
        "label": 402
    },
    {
        "text": " c  add pause for rpc test  ",
        "label": 491
    },
    {
        "text": "list regions may throw an error if a region is rit  the 'list_regions' shell command gets a list of regions for a given table and then prints them and some attributes such as the server where they are located, current request count, data locality, and such. however if a region is in transition the command might fail with error: undefined method `getdatalocality' for nil:nilclass and there may be other ways this can happen. protect against use of nil references and just display what we can. ",
        "label": 473
    },
    {
        "text": "assignmentmanager should handle rejectedexecutionexception  in test output, i noticed the following: 2013-05-21 03:39:37,015 debug [thread-1446-eventthread] zookeeper.zookeeperwatcher(307): master:35255-0x13ec52b4ce30000 received zookeeper event, type=nodedatachanged, state=syncconnected, path=/hbase/region-in-transition/8b6ce00aafd483a6ed7a76425009ebf8 2013-05-21 03:39:37,015 error [thread-1446-eventthread] zookeeper.clientcnxn$eventthread(521): error while calling watcher java.util.concurrent.rejectedexecutionexception         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:1768)         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:767)         at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:658)         at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:78)         at org.apache.hadoop.hbase.master.assignmentmanager.zkeventworkerssubmit(assignmentmanager.java:1136)         at org.apache.hadoop.hbase.master.assignmentmanager.handleassignmentevent(assignmentmanager.java:1306)         at org.apache.hadoop.hbase.master.assignmentmanager.nodedatachanged(assignmentmanager.java:1095)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:338)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:519)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:495) by default, threadpoolexecutor.abortpolicy is used for zkeventworkers, leading to the above exception. we should examine the calls to zkeventworkerssubmit() and devise proper handling for rejectedexecutionexception. ",
        "label": 441
    },
    {
        "text": "  assignmentmanager throws illegalstateexception from pending open to offline  am throws this exception which subsequently causes the master to abort: java.lang.illegalstateexception: unexpected state : testretrying,jjj,1372891751115.9b828792311001062a5ff4b1038fe33b. state=pending_open, ts=1372891751912, server=hemera.apache.org,39064,1372891746132 .. cannot transit it to offline. at org.apache.hadoop.hbase.master.assignmentmanager.setofflineinzookeeper(assignmentmanager.java:1879) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1688) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1424) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1399) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1394) at org.apache.hadoop.hbase.master.handler.closedregionhandler.process(closedregionhandler.java:105) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:175) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662) this exception trace is from the failing test testmetareadereditor which is failing pretty frequently, but looking at the test code, i think this is not a test-only issue, but affects the main code path. https://builds.apache.org/job/hbase-0.94/1036/testreport/junit/org.apache.hadoop.hbase.catalog/testmetareadereditor/testretrying/ ",
        "label": 286
    },
    {
        "text": "please add my public key to committer keys  ",
        "label": 149
    },
    {
        "text": "race between splitregionhandlers for the same region kills the master  i just saw that multiple splitregionhandlers can be created for the same region because of the rs tickling, but it becomes deadly when more than 1 are trying to delete the znode at the same time: 2011-11-16 02:25:28,778 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_split, server=sv4r7s38,62023,1321410237387, region=f80b6a904048a99ce88d61420b8906d1  2011-11-16 02:25:28,780 debug org.apache.hadoop.hbase.master.assignmentmanager: handling transition=rs_zk_region_split, server=sv4r7s38,62023,1321410237387, region=f80b6a904048a99ce88d61420b8906d1  2011-11-16 02:25:28,796 debug org.apache.hadoop.hbase.master.handler.splitregionhandler: handling split event for f80b6a904048a99ce88d61420b8906d1; deleting node  2011-11-16 02:25:28,798 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x132f043bbde094b deleting existing unassigned node for f80b6a904048a99ce88d61420b8906d1 that is in expected state rs_zk_region_split  2011-11-16 02:25:28,804 debug org.apache.hadoop.hbase.master.handler.splitregionhandler: handling split event for f80b6a904048a99ce88d61420b8906d1; deleting node  2011-11-16 02:25:28,806 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x132f043bbde094b deleting existing unassigned node for f80b6a904048a99ce88d61420b8906d1 that is in expected state rs_zk_region_split  2011-11-16 02:25:28,821 debug org.apache.hadoop.hbase.zookeeper.zkassign: master:62003-0x132f043bbde094b successfully deleted unassigned node for region f80b6a904048a99ce88d61420b8906d1 in expected state rs_zk_region_split  2011-11-16 02:25:28,821 info org.apache.hadoop.hbase.master.handler.splitregionhandler: handled split report); parent=testtable,0000006304,1321409743253.f80b6a904048a99ce88d61420b8906d1. daughter a=testtable,0000006304,1321410325564.e0f5d201683bcabe14426817224334b8.daughter b=testtable,0000007054,1321410325564.1b82eeb5d230c47ccc51c08256134839.  2011-11-16 02:25:28,829 warn org.apache.hadoop.hbase.zookeeper.recoverablezookeeper: node /hbase/unassigned/f80b6a904048a99ce88d61420b8906d1 already deleted, and this is not a retry  2011-11-16 02:25:28,830 fatal org.apache.hadoop.hbase.master.hmaster: error deleting split node in zk for transition zk node (f80b6a904048a99ce88d61420b8906d1)  org.apache.zookeeper.keeperexception$nonodeexception: keepererrorcode = nonode for /hbase/unassigned/f80b6a904048a99ce88d61420b8906d1  at org.apache.zookeeper.keeperexception.create(keeperexception.java:102)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:42)  at org.apache.zookeeper.zookeeper.delete(zookeeper.java:728)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.delete(recoverablezookeeper.java:107)  at org.apache.hadoop.hbase.zookeeper.zkutil.deletenode(zkutil.java:884)  at org.apache.hadoop.hbase.zookeeper.zkassign.deletenode(zkassign.java:506)  at org.apache.hadoop.hbase.zookeeper.zkassign.deletenode(zkassign.java:453)  at org.apache.hadoop.hbase.master.handler.splitregionhandler.process(splitregionhandler.java:95)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:168)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) stack and i came up with the solution that we need just manage that exception because handlesplitreport is an in-memory thing. ",
        "label": 544
    },
    {
        "text": "cleanup  findbugs and javadoc warning fixes as well as making it illegal passing null row to put delete  etc   part of hbase-7900 broken out so that patch gets smaller. this is a patch with cleanup mostly findbugs fixes (general ones) as well as adding check for null row being passed to put, get, etc. this patch helps rpc along. ",
        "label": 314
    },
    {
        "text": "rolling restart sh script hangs when attempting to detect expiration of  hbase master znode   due to bugfix zookeeper-1059 (zk 3.4.0+), the rolling-restart.sh script will hang when attempting to make sure the /hbase/master znode is deleted. here's the code # make sure the master znode has been deleted before continuing     zparent=`$bin/hbase org.apache.hadoop.hbase.util.hbaseconftool zookeeper.znode.parent`     if [ \"$zparent\" == \"null\" ]; then zparent=\"/hbase\"; fi     zmaster=`$bin/hbase org.apache.hadoop.hbase.util.hbaseconftool zookeeper.znode.master`     if [ \"$zmaster\" == \"null\" ]; then zmaster=\"master\"; fi     zmaster=$zparent/$zmaster     echo -n \"waiting for master znode ${zmaster} to expire\"     while bin/hbase zkcli stat $zmaster >/dev/null 2>&1; do       echo -n \".\"       sleep 1     done     echo #force a newline prior to zookeeper-1059, stat on a null znode would npe and cause zkcli to exit with retcode 1. afterwards, the null is caught, zkcli will exit with 0 in the case where the znode is present and in the case where it does not exist. ",
        "label": 248
    },
    {
        "text": "support for batch version of checkandmutate   the use case is that the user has multiple threads loading hundreds of keys into a hbase table. occasionally there are collisions in the keys being uploaded by different threads. so for correctness, it is required to do checkandmutate() instead of a put(). however, doing a checkandmutate() rpc for every key update is non optimal. it would be good to have a batch version of checkandmutate() similar to batch put(). the client can partition the keys on region boundaries. the jira is not looking for any type of cross-row locking or multi-row atomicity with checkandmutate(). ",
        "label": 455
    },
    {
        "text": " coprocessors  table coprocessor loaded twice when region is initialized  i'm debugging a preput hook which i've implemented as part of the coprocessor work being developed. this hook is loaded via a table coprocessor attribute and i've noticed that the preput method is being called twice for a single put. after setting up the region server to run in a debugger, i'm noticing the call to loadtablecoprocessors() being invoked twice during region initialization, specifically: 1. hregion.init => regioncoprocessorhost.init => regioncoprocessorhost.loadtablecoprocessors 2. ... => regioncoprocessorhost.preopen => regioncoprocessorhost.loadtablecoprocessors this results in two regionenvironment instances, each containing a new instance of my coprocessor, being added to the regioncoprocessorhost. when i issue a put, the list of regionenvironments is iterated over and each calls the preput method in my coprocessor. reason why this is posing a problem for me is that i modify the family map passed in to my preput method. since this family map is the same instance used in both preput calls, the second preput call operates on the modified family map, which leads to an unexpected result. is the double loading of the same coprocessor class intentional, is this a bug? ",
        "label": 38
    },
    {
        "text": "clearjmxcache does not take effect actually  when trying to backport hbase-14166 to 0.98.6, i find jmxcachebuster::clearjmxcache() does no take effect actually. the related code are listed below: org.apache.hadoop.metrics2.impl.jmxcachebuster.java // fut is initialized to null private static atomicreference<scheduledfuture> fut = new atomicreference<>(null); public static void clearjmxcache() {     // clearjmxcache return directly when fut is null, which is always true.     // the actual intent is 'if (future != null && !future.isdone ...)' ?     scheduledfuture future = fut.get();     if ((future == null || (!future.isdone() && future.getdelay(timeunit.milliseconds) > 100))) {       return;     }     ...... } ",
        "label": 154
    },
    {
        "text": "all rs should already start work with the new peer change when replication peer procedure is finished  when replication peer operations use zk, the master will modify zk directly. then the rs will asynchronous track the zk event to start work with the new peer change. when replication peer operations use procedure, need to make sure this process is synchronous. all rs should already start work with the new peer change when procedure is finished. ",
        "label": 187
    },
    {
        "text": "prefixrowfilter  i quickly implemented a prefix row filter and an accompanying test case. this is important in terms of hbase emulating big table's capabilities. the regex filter is going to be too slow i think. this just does a straight byte comparison so should be very fast. ",
        "label": 313
    },
    {
        "text": "punt on the timeout doesn't work in bulkenabler waituntildone  master's enabletablehandler   please take a look at the code below in enabletablehandler(hbase master): enabletablehandler.java     protected boolean waituntildone(long timeout)     throws interruptedexception {            .....       int lastnumberofregions = this.countofregionsintable;       while (!server.isstopped() && remaining > 0) {         thread.sleep(waitingtimeforevents);         regions = assignmentmanager.getregionsoftable(tablename);         if (isdone(regions)) break;         // punt on the timeout as long we make progress         if (regions.size() > lastnumberofregions) {           lastnumberofregions = regions.size();           timeout += waitingtimeforevents;         }         remaining = timeout - (system.currenttimemillis() - starttime);     ....     }     private boolean isdone(final list<hregioninfo> regions) {       return regions != null && regions.size() >= this.countofregionsintable;     } we can easily find out if we let lastnumberofregions = this.countofregionsintable , the function of punt on timeout code will never be executed. i think initlize lastnumberofregions = 0 can make it work. ",
        "label": 552
    },
    {
        "text": "set version as in branch in prep for first rc of  ",
        "label": 149
    },
    {
        "text": "import uses system out println instead of logging  per the title. ",
        "label": 191
    },
    {
        "text": "don't reopen file if already open when updating readers underneath scanners  doing this is costly in scenario where where many scanners and many concurrent updates. ",
        "label": 314
    },
    {
        "text": "regionservers should report detailed health to master  master should flag troubled regionservers in ui  regionservers should report detailed health to master. the master should flag troubled regionservers in the ui. the concern at the moment is primarily heap. regionservers should report used, committed, and max heap metrics in the periodic report. the master should flag in the regionserver list on /master.jsp those regionservers where available heap is below a configurable threshold. ",
        "label": 38
    },
    {
        "text": "branch am issues  parent jira for issues found through it tests with chaos monkey. ",
        "label": 154
    },
    {
        "text": "testhregionbusywait testwriteswhilescanning fails frequently in  have seen a few of these: error message failed clearing memory after 6 attempts on region: testwriteswhilescanning,,1397727647509.2c968a587c4cb7e84a52c7aa8d2afcac. stacktrace org.apache.hadoop.hbase.droppedsnapshotexception: failed clearing memory after 6 attempts on region: testwriteswhilescanning,,1397727647509.2c968a587c4cb7e84a52c7aa8d2afcac. at org.apache.hadoop.hbase.regionserver.hregion.doclose(hregion.java:1087) at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:1024) at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:989) at org.apache.hadoop.hbase.regionserver.hregion.closehregion(hregion.java:4346) at org.apache.hadoop.hbase.regionserver.testhregion.testwriteswhilescanning(testhregion.java:3406) ",
        "label": 314
    },
    {
        "text": "ct verifyregionlocation isn't doing a very good check  can delay cluster recovery  after some extensive debugging in the thread a sudden msg of \"java.io.ioexception: server not running, aborting\", we figured that the region servers weren't able to talk to the new .meta. location because the old one was still alive but on it's way down after a oome. it translates into exceptions like \"server not running\" coming from trying to edit .meta. and digging in the code i see that ct.waitformetaserverconnectiondefault -> waitformeta -> getmetaserverconnection(true) calls verifyregionlocation since we force the refresh. in this method we check if the rs is good by calling getregioninfo which does not check if the region server is trying to close. what this means is that a cluster can't recover a .meta.-serving rs failure until it has fully shutdown since every time a rs tries to open a region (like right after the log splitting) or split it fails editing .meta. ",
        "label": 229
    },
    {
        "text": "our build yetus personality will run tests on individual modules and then on all  i e  'root'  should do one or other  in runs on end of hbase-17056, a patch that touches all modules, sean busbey noticed that we were doing unit suite twice... once for each individual module and then again for all/root because patch had root changes in it. we shouldn't do all if we are doing 'root' as per sean busbey here is tail of console output: .... 10:50:30 cd /testptch/hbase/hbase-spark 10:50:30 mvn -dmaven.repo.local=/home/jenkins/yetus-m2/hbase-master-patch-1 -dhbasepatchprocess -prunalltests -dtest.exclude.pattern=**/master.procedure.testprocedureadmin.java,**/master.assignment.testmergetableregionsprocedure.java,**/quotas.testsnapshotquotaobserverchore.java,**/quotas.testquotathrottle.java,**/client.testreplicasclient.java,**/client.locking.testentitylocks.java,**/security.visibility.testvisibilitylabelsreplication.java,**/client.testshell.java,**/master.assignment.testassignmentmanager.java,**/replication.testmultislavereplication.java,**/coprocessor.testregionobserverinterface.java,**/master.balancer.testdefaultloadbalancer.java,**/client.testreplicawithcluster.java,**/io.hfile.testlrublockcache.java,**/master.balancer.testfavoredstochasticloadbalancer.java,**/regionserver.wal.testasynclogrolling.java,**/master.balancer.teststochasticloadbalancer.java,**/client.testmultiparallel.java,**/replication.testreplicationwithtags.java,**/security.access.testcoprocessorwhitelistmasterobserver.java,**/replication.regionserver.testreplicator.java,**/master.assignment.testassignmentonrscrash.java,**/master.procedure.testmasterfailoverwithprocedures.java,**/quotas.testquotastatusrpcs.java,**/regionserver.testhregionwithinmemoryflush.java,**/master.cleaner.testhfilecleaner.java clean test -fae > /testptch/patchprocess/patch-unit-hbase-spark.txt 2>&1 10:55:35 elapsed:   5m 14s 10:55:45 cd /testptch/hbase 10:55:45 mvn -dmaven.repo.local=/home/jenkins/yetus-m2/hbase-master-patch-1 -dhbasepatchprocess -prunalltests -dtest.exclude.pattern=**/master.procedure.testprocedureadmin.java,**/master.assignment.testmergetableregionsprocedure.java,**/quotas.testsnapshotquotaobserverchore.java,**/quotas.testquotathrottle.java,**/client.testreplicasclient.java,**/client.locking.testentitylocks.java,**/security.visibility.testvisibilitylabelsreplication.java,**/client.testshell.java,**/master.assignment.testassignmentmanager.java,**/replication.testmultislavereplication.java,**/coprocessor.testregionobserverinterface.java,**/master.balancer.testdefaultloadbalancer.java,**/client.testreplicawithcluster.java,**/io.hfile.testlrublockcache.java,**/master.balancer.testfavoredstochasticloadbalancer.java,**/regionserver.wal.testasynclogrolling.java,**/master.balancer.teststochasticloadbalancer.java,**/client.testmultiparallel.java,**/replication.testreplicationwithtags.java,**/security.access.testcoprocessorwhitelistmasterobserver.java,**/replication.regionserver.testreplicator.java,**/master.assignment.testassignmentonrscrash.java,**/master.procedure.testmasterfailoverwithprocedures.java,**/quotas.testquotastatusrpcs.java,**/regionserver.testhregionwithinmemoryflush.java,**/master.cleaner.testhfilecleaner.java clean test -fae > /testptch/patchprocess/patch-unit-root.txt 2>&1 13:00:13 build was aborted ... i'd aborted the run because it seemed to be taking too long but on subsequent examination, it was actually making progress. ",
        "label": 320
    },
    {
        "text": "update website for branch eol  ",
        "label": 402
    },
    {
        "text": "assignmentmanager should use the same logic for clean startup and failover  currently assignmentmanager handles clean startup and failover very differently.  different logic is mingled together so it is hard to find out which is for which. we should clean it up and share the same logic so that assignmentmanager handles  both cases the same way. this way, the code will much easier to understand and  maintain. ",
        "label": 242
    },
    {
        "text": "script to return servers where meta and root tables are deployed  simple script (bash or ruby) that would print servers addresses(hostnames) where root or meta tables are deployed. it could be located in bin dir. i have attached simple example that i use for this. ",
        "label": 393
    },
    {
        "text": "should be able to enable disable  meta  table  ",
        "label": 241
    },
    {
        "text": "snapshot manifest file instead of multiple empty files  currently taking a snapshot means creating one empty file for each file in the source table directory, plus copying the .regioninfo file for each region, the table descriptor file and a snapshotinfo file. during the restore or snapshot verification we traverse the filesystem (fs.liststatus()) to find the snapshot files, and we open the .regioninfo files to get the information. to avoid hammering the namenode and having lots of empty files, we can use a manifest file that contains the list of files and information that we need.  to keep the rs parallelism that we have, each rs can write its own manifest. message snapshotdescriptor {   required string name;   optional string table;   optional int64 creationtime;   optional type type;   optional int32 version; } message snapshotregionmanifest {   optional int32 version;   required regioninfo regioninfo;   repeated familyfiles familyfiles;   message storefile {     required string name;     optional reference reference;   }   message familyfiles {     required bytes familyname;     repeated storefile storefiles;   } } /hbase/.snapshot/<snapshotname> /hbase/.snapshot/<snapshotname>/snapshotinfo /hbase/.snapshot/<snapshotname>/<tablename> /hbase/.snapshot/<snapshotname>/<tablename>/tableinfo /hbase/.snapshot/<snapshotname>/<tablename>/regionmanifest(.n) ",
        "label": 309
    },
    {
        "text": "add new svn  ml  and website locations into branches src doc and into trunk pom xml  etc   ",
        "label": 314
    },
    {
        "text": "processservershutdown throws nullpointerexception for offline regions  when a regionsserver dies the master can run into the following bug. 2010-04-27 17:20:37,303 debug org.apache.hadoop.hbase.master.hmaster: processing todo: processservershutdown of dell106.cluster,60020,1272377612991  2010-04-27 17:20:37,303 info org.apache.hadoop.hbase.master.regionserveroperation: process shutdown of server dell106.cluster,60020,1272377612991: logsplit: true, rootrescanned: true, numberofmetaregions: 1, onlinemetaregions.size(): 1  2010-04-27 17:20:01,637 info org.apache.hadoop.hbase.master.regionserveroperation: log split complete, meta reassignment and scanning:  2010-04-27 17:20:01,653 debug org.apache.hadoop.hbase.master.processservershutdown$scanrootregion: process server shutdown scanning root region on 10.1.3.124  2010-04-27 17:20:01,664 debug org.apache.hadoop.hbase.master.regionserveroperation: process server shutdown scanning root region on 10.1.3.124 finished master  2010-04-27 17:20:01,683 debug org.apache.hadoop.hbase.master.processservershutdown$scanmetaregions: process server shutdown scanning .meta.,,1 on 10.1.3.104:60020  2010-04-27 17:20:18,087 debug org.apache.hadoop.hbase.master.processservershutdown$scanmetaregions: exception in retryablemetaoperation:  2010-04-27 17:20:18,118 warn org.apache.hadoop.hbase.master.hmaster: adding to delayed queue: processservershutdown of dell106.cluster,60020,1272377612991  java.lang.runtimeexception: java.lang.nullpointerexception  at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:100)  at org.apache.hadoop.hbase.master.processservershutdown.process(processservershutdown.java:345)  at org.apache.hadoop.hbase.master.hmaster.processtodoqueue(hmaster.java:509)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:448)  caused by: java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.bytes.tolong(bytes.java:487)  at org.apache.hadoop.hbase.util.bytes.tolong(bytes.java:461)  at org.apache.hadoop.hbase.master.processservershutdown.scanmetaregion(processservershutdown.java:147)  at org.apache.hadoop.hbase.master.processservershutdown$scanmetaregions.call(processservershutdown.java:264)  at org.apache.hadoop.hbase.master.processservershutdown$scanmetaregions.call(processservershutdown.java:250)  at org.apache.hadoop.hbase.master.retryablemetaoperation.dowithretries(retryablemetaoperation.java:69)  ... 3 more the problem is in processservershutdown.java at line 148-149: 146 string serveraddress =   147 bytes.tostring(values.getvalue(catalog_family, server_qualifier));  148 long startcode =  149 bytes.tolong(values.getvalue(catalog_family, startcode_qualifier));  150 string servername = null;  151 if (serveraddress != null && serveraddress.length() > 0) { 152 servername = hserverinfo.getservername(serveraddress, startcode); 153 } it should be modified to: 146 string serveraddress =   147 bytes.tostring(values.getvalue(catalog_family, server_qualifier));  150 string servername = null;  151 if (serveraddress != null && serveraddress.length() > 0) { 148 long startcode = 149 bytes.tolong(values.getvalue(catalog_family, startcode_qualifier)); 152 servername = hserverinfo.getservername(serveraddress, startcode); 153 } as bytes.tolong cannot handle the null pointer returned by getvalue for missing startcode_qualifier of offline regions in meta. ",
        "label": 325
    },
    {
        "text": "killing server in testmastertransitions causes npes and test deadlock  ",
        "label": 314
    },
    {
        "text": "upgrade to yetus  branch-1/jdk7 checkstyle dtd xml parse complaint; \"script engine for language js can not be found\" see parent for some context. checkstyle references dtds that were hosted on puppycrawl, then on sourceforge up until ten days ago. nightlies are failing for among other reasons, complaint that there is bad xml in the build... notably, the unresolvable dtds. i'd just update the dtds but there is a need for a js engine some where and openjdk7 doesn't ship with one (openjdk8 does). that needs addressing and then we can backport the parent issue... see https://builds.apache.org/view/h-l/view/hbase/job/hbase%20nightly/job/branch-1/710/artifact/output-general/xml.txt ... which in case its rolled away, is filled with this message: \"script engine for language js can not be found\" ",
        "label": 402
    },
    {
        "text": "wrong result in one case of scan that use raw and versions and filter together  create 'testscanraw',{name => 'f', versions => 1}     put 'testscanraw','r1','f:q','1'  put 'testscanraw','r1','f:q','2'  put 'testscanraw','r1','f:q','3'     hbase(main):005:0> scan 'testscanraw',{raw => true, startrow => 'r1', stoprow=>'r1',versions=>2}  row column+cell  r1 column=f:q, timestamp=1563430154757, value=3  r1 column=f:q, timestamp=1563430153120, value=2     hbase(main):006:0> scan 'testscanraw',{raw => true, startrow => 'r1', stoprow=>'r1',versions=>2,filter => \"(qualifierfilter (=, 'binary:q'))\"}  row column+cell  r1 column=f:q, timestamp=1563430154757, value=3   btw,the result is right in hbase1.2. ",
        "label": 515
    },
    {
        "text": "remove redundant byte conversion methods from hconstants  hconstants#tobytes and hconstants#tostring are identical implementations to implementations in bytes. consolidate. ",
        "label": 339
    },
    {
        "text": "hconnectionimplementation is closed but not deleted  in abort() of hconnectionmanager$hconnectionimplementation, instance of hconnectionimplementation is marked as this.closed=true. there is no way for client application to check the hbase client connection whether it is still opened/good (this.closed=false) or not. we need a method to validate the state of a connection like isclosed(). public boolean isclosed(){    return this.closed; }  once the connection is closed and it should get deleted. client application still gets a connection from hconnectionmanager.getconnection(configuration) and tries to make a rpc call to rs, since connection is already closed, hconnectionimplementation.getregionserverwithretries throws retriesexhaustedexception with error message caused by: org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server null for region , row 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxx', but failed after 10 attempts. exceptions: java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed java.io.ioexception: org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation@7eab48a7 closed at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getregionserverwithretries(hconnectionmanager.java:1008) at org.apache.hadoop.hbase.client.htable.get(htable.java:546) ",
        "label": 333
    },
    {
        "text": "refactor  corrupt data  tests in testhlogsplit  while fixing hbase-2643, i noticed that a couple of the hlogsplit tests from hbase-2437 were now failing. 3 tests are trying to detect proper handling of garbage data: testcorruptedfilegetsarchivedifskiperrors, testtrailinggarbagecorruptionlogfileskiperrorsfalsethrows, testcorruptedlogfilesskiperrorsfalsedoesnottouchlogs. however, these tests are corrupting data at the hbase level. data corruption should be tested at the hdfs level, because the filesystem is responsible for data validation. these tests need to inject corrupt data at the hdfs level & then verify that checksumexceptions are thrown. ",
        "label": 21
    },
    {
        "text": "add new compression and hfile blocksize to hcolumndescriptor  ",
        "label": 314
    },
    {
        "text": "need documentation for rest atomic operations  hbase   hbase-4720 added checkandput/checkanddelete capability to the rest interface, but the rest documentation (in the package summary) needs to be updated so people know that this feature exists and how to use it. http://wiki.apache.org/hadoop/hbase/stargate  http://hbase.apache.org/book/rest.html ",
        "label": 137
    },
    {
        "text": "hbase trunk does not build against hadoop trunk  when i build hadoop's library from trunk and then build hbase from trunk,  when i replace hbase hadoop's libraries with the one built from trunk, i get the following error. #/opt/ant/bin/ant clean tar  buildfile: build.xml clean:  [delete] deleting directory /opt/hbase-src/build init:  [mkdir] created dir: /opt/hbase-src/build  [mkdir] created dir: /opt/hbase-src/build/classes  [mkdir] created dir: /opt/hbase-src/build/test  [mkdir] created dir: /opt/hbase-src/build/examples  [mkdir] created dir: /opt/hbase-src/build/webapps  [copy] copying 8 files to /opt/hbase-src/build/webapps  [mkdir] created dir: /opt/hbase-src/build/lib  [copy] copying 22 files to /opt/hbase-src/build/lib  [mkdir] created dir: /opt/hbase-src/build/conf  [copy] copying 5 files to /opt/hbase-src/build/conf  [mkdir] created dir: /opt/hbase-src/build/bin  [copy] copying 7 files to /opt/hbase-src/build/bin javacc: compile:  [javac] compiling 182 source files to /opt/hbase-src/build/classes  [javac] note: some input files use or override a deprecated api.  [javac] note: recompile with -xlint:deprecation for details.  [javac] note: some input files use unchecked or unsafe operations.  [javac] note: recompile with -xlint:unchecked for details. jar:  [jar] building jar: /opt/hbase-src/build/hbase-0.2.0-dev.jar javadoc:  [mkdir] created dir: /opt/hbase-src/build/docs/api  [javadoc] generating javadoc  [javadoc] javadoc execution  [javadoc] loading source files for package org.apache.hadoop.hbase...  [javadoc] loading source files for package org.apache.hadoop.hbase.client...  [javadoc] loading source files for package org.apache.hadoop.hbase.filter...  [javadoc] loading source files for package org.apache.hadoop.hbase.generated.master...  [javadoc] loading source files for package org.apache.hadoop.hbase.generated.regionserver...  [javadoc] loading source files for package org.apache.hadoop.hbase.hql...  [javadoc] loading source files for package org.apache.hadoop.hbase.hql.formatter...  [javadoc] loading source files for package org.apache.hadoop.hbase.hql.generated...  [javadoc] loading source files for package org.apache.hadoop.hbase.io...  [javadoc] loading source files for package org.apache.hadoop.hbase.ipc...  [javadoc] loading source files for package org.apache.hadoop.hbase.mapred...  [javadoc] loading source files for package org.apache.hadoop.hbase.master...  [javadoc] loading source files for package org.apache.hadoop.hbase.regionserver...  [javadoc] loading source files for package org.apache.hadoop.hbase.rest...  [javadoc] loading source files for package org.apache.hadoop.hbase.thrift...  [javadoc] loading source files for package org.apache.hadoop.hbase.thrift.generated...  [javadoc] loading source files for package org.apache.hadoop.hbase.util...  [javadoc] loading source files for package org.onelab.filter...  [javadoc] constructing javadoc information...  [javadoc] standard doclet version 1.6.0_03  [javadoc] building tree for all the packages and classes...  [javadoc] building index for all the packages and classes...  [javadoc] building index for all classes... compile-test:  [javac] compiling 58 source files to /opt/hbase-src/build/test  [javac] /opt/hbase-src/src/test/org/apache/hadoop/hbase/performanceevaluation.java:148: org.apache.hadoop.hbase.performanceevaluation.evaluationmaptask is not abstract and does not override abstract method map(java.lang.object,java.lang.object,org.apache.hadoop.mapred.outputcollector,org.apache.hadoop.mapred.reporter) in org.apache.hadoop.mapred.mapper  [javac] public static class evaluationmaptask extends mapreducebase  [javac] ^  [javac] note: some input files use or override a deprecated api.  [javac] note: recompile with -xlint:deprecation for details.  [javac] 1 error build failed  /opt/hbase-src/build.xml:308: compile failed; see the compiler error output for details. ",
        "label": 241
    },
    {
        "text": "parallelize offline snapshot in disabledtablesnapshothandler  in disabledtablesnapshothandler, there is todo:   // todo consider parallelizing these operations since they are independent. right now its just   // easier to keep them serial though   @override   public void snapshotregions(list<pair<hregioninfo, servername>> regionsandlocations) throws ioexception, ",
        "label": 441
    },
    {
        "text": "buffferedmutatorparams opertationtimeout  is misspelt  the method `opertationtimeout()` in `buffferedmutatorparams` is misspelt.  ",
        "label": 443
    },
    {
        "text": "recursive call in regionmergetransactionimpl getjournal   hbase-12975 in its branch-1 patch (https://issues.apache.org/jira/secure/attachment/12708578/hbase-12975-branch-1.patch) introduced a recursive call for getjournal(). needs to return just the journal variable like master patch does. ",
        "label": 284
    },
    {
        "text": "testnodehealthcheckchore testhealthchecker failed build   failed with this: java.lang.assertionerror: expected:<success> but was:<timed_out> at org.junit.assert.fail(assert.java:88) the first execution timed_out. timeout invoking shell is 200ms. ",
        "label": 314
    },
    {
        "text": "testiofencing testfencingaroundcompactionafterwalsync is flaky  i'm looking into some more of the flaky tests on trunk and this one seems to be particularly gross, failing about half the time in recent days. some probably-relevant output from a recent run: 2015-08-27 18:50:14,318 info  [main] hbase.testiofencing(326): allowing compaction to proceed 2015-08-27 18:50:14,318 debug [main] hbase.testiofencing$compactionblockerregion(110): allowing compactions 2015-08-27 18:50:14,318 debug [rs:0;hemera:35619-shortcompactions-1440701403303] regionserver.hstore(1732): removing store files after compaction... 2015-08-27 18:50:14,323 debug [rs:0;hemera:35619-longcompactions-1440701391112] regionserver.hstore(1732): removing store files after compaction... 2015-08-27 18:50:14,330 debug [rs:0;hemera:35619-longcompactions-1440701391112] backup.hfilearchiver(224): archiving compacted store files. 2015-08-27 18:50:14,331 debug [rs:0;hemera:35619-shortcompactions-1440701403303] backup.hfilearchiver(224): archiving compacted store files. 2015-08-27 18:50:14,337 debug [rs:0;hemera:35619-longcompactions-1440701391112] backup.hfilearchiver(438): finished archiving from class org.apache.hadoop.hbase.backup.hfilearchiver$fileablestorefile, file:hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/99e903ad7e0f4029862d0e35c5548464, to hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/archive/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/99e903ad7e0f4029862d0e35c5548464 2015-08-27 18:50:14,337 debug [rs:0;hemera:35619-shortcompactions-1440701403303] backup.hfilearchiver(438): finished archiving from class org.apache.hadoop.hbase.backup.hfilearchiver$fileablestorefile, file:hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/74a80cc06d134361941085bc2bb905fe, to hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/archive/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/74a80cc06d134361941085bc2bb905fe 2015-08-27 18:50:14,341 debug [rs:0;hemera:35619-longcompactions-1440701391112] backup.hfilearchiver(438): finished archiving from class org.apache.hadoop.hbase.backup.hfilearchiver$fileablestorefile, file:hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/7067addd325446089ba15ec2c77becbc, to hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/archive/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/7067addd325446089ba15ec2c77becbc 2015-08-27 18:50:14,342 info  [rs:0;hemera:35619-longcompactions-1440701391112] regionserver.hstore(1353): completed compaction of 2 (all) file(s) in family of tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311. into e138bb0ec6c64ad19efab3b44dbbcb1a(size=68.7 k), total size for store is 146.9 k. this selection was in queue for 0sec, and took 10sec to execute. 2015-08-27 18:50:14,343 info  [rs:0;hemera:35619-longcompactions-1440701391112] regionserver.compactsplitthread$compactionrunner(527): completed compaction: request = regionname=tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311., storename=family, filecount=2, filesize=73.1 k, priority=998, time=525052314434020; duration=10sec 2015-08-27 18:50:14,343 debug [rs:0;hemera:35619-shortcompactions-1440701403303] backup.hfilearchiver(438): finished archiving from class org.apache.hadoop.hbase.backup.hfilearchiver$fileablestorefile, file:hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/2926c09f1941416eb557ee5d283d7e2b, to hdfs://localhost:34675/user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/archive/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/2926c09f1941416eb557ee5d283d7e2b 2015-08-27 18:50:14,347 debug [rs_close_region-hemera:35619-0] regionserver.hregion(1405): updates disabled for region tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311. 2015-08-27 18:50:14,343 debug [rs:0;hemera:35619-longcompactions-1440701391112] regionserver.compactsplitthread$compactionrunner(550): compactsplitthread status: compaction_queue=(0:3), split_queue=0, merge_queue=0 2015-08-27 18:50:14,348 error [rs:0;hemera:35619-shortcompactions-1440701403303] regionserver.compactsplitthread$compactionrunner(547): compaction failed request = regionname=tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311., storename=family, filecount=2, filesize=51.6 k, priority=998, time=525051341420855 java.lang.nullpointerexception at org.apache.hadoop.hbase.regionserver.hstore.logcompactionendmessage(hstore.java:1343) at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:1259) at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1812) at org.apache.hadoop.hbase.testiofencing$compactionblockerregion.compact(testiofencing.java:127) at org.apache.hadoop.hbase.regionserver.compactsplitthread$compactionrunner.run(compactsplitthread.java:524) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:744) 2015-08-27 18:50:14,349 debug [rs:0;hemera:35619-shortcompactions-1440701403303] regionserver.compactsplitthread$compactionrunner(550): compactsplitthread status: compaction_queue=(0:3), split_queue=0, merge_queue=0 2015-08-27 18:50:14,351 info  [storecloserthread-tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311.-1] regionserver.hstore(889): closed family 2015-08-27 18:50:14,352 error [rs_close_region-hemera:35619-0] regionserver.hregion(1493): memstore size is 62664 2015-08-27 18:50:14,353 info  [rs_close_region-hemera:35619-0] regionserver.hregion(1506): closed tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311. 2015-08-27 18:50:14,353 debug [rs_close_region-hemera:35619-0] handler.closeregionhandler(122): closed tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311. 2015-08-27 18:50:14,361 error [rs:1;hemera:39431-shortcompactions-1440701414271] regionserver.compactsplitthread$compactionrunner(541): compaction failed request = regionname=tabletest,,1440701396419.94d6f21f7cf387d73d8622f535c67311., storename=family, filecount=9, filesize=289.6 k (25.6 k, 47.0 k, 26.0 k, 68.7 k, 26.1 k, 26.1 k, 26.1 k, 26.1 k, 18.0 k), priority=991, time=525062316035397 java.io.filenotfoundexception: file does not exist: /user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/7067addd325446089ba15ec2c77becbc at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:64) at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:54) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsupdatetimes(fsnamesystem.java:1795) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsint(fsnamesystem.java:1738) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1718) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1690) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.getblocklocations(namenoderpcserver.java:519) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.getblocklocations(clientnamenodeprotocolserversidetranslatorpb.java:337) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:585) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:928) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2013) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2009) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:415) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1614) at org.apache.hadoop.ipc.server$handler.run(server.java:2007) at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:526) at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:106) at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:73) at org.apache.hadoop.hdfs.dfsclient.callgetblocklocations(dfsclient.java:1167) at org.apache.hadoop.hdfs.dfsclient.getlocatedblocks(dfsclient.java:1155) at org.apache.hadoop.hdfs.dfsclient.getlocatedblocks(dfsclient.java:1145) at org.apache.hadoop.hdfs.dfsinputstream.fetchlocatedblocksandgetlastblocklength(dfsinputstream.java:268) at org.apache.hadoop.hdfs.dfsinputstream.openinfo(dfsinputstream.java:235) at org.apache.hadoop.hdfs.dfsinputstream.<init>(dfsinputstream.java:228) at org.apache.hadoop.hdfs.dfsclient.open(dfsclient.java:1318) at org.apache.hadoop.hdfs.distributedfilesystem$3.docall(distributedfilesystem.java:293) at org.apache.hadoop.hdfs.distributedfilesystem$3.docall(distributedfilesystem.java:289) at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81) at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:289) at org.apache.hadoop.fs.filterfilesystem.open(filterfilesystem.java:160) at org.apache.hadoop.fs.filesystem.open(filesystem.java:764) at org.apache.hadoop.hbase.io.fsdatainputstreamwrapper.<init>(fsdatainputstreamwrapper.java:106) at org.apache.hadoop.hbase.io.fsdatainputstreamwrapper.<init>(fsdatainputstreamwrapper.java:82) at org.apache.hadoop.hbase.regionserver.storefileinfo.open(storefileinfo.java:248) at org.apache.hadoop.hbase.regionserver.storefile.open(storefile.java:385) at org.apache.hadoop.hbase.regionserver.storefile.createreader(storefile.java:492) at org.apache.hadoop.hbase.regionserver.storefilescanner.getscannersforstorefiles(storefilescanner.java:115) at org.apache.hadoop.hbase.regionserver.storefilescanner.getscannersforstorefiles(storefilescanner.java:99) at org.apache.hadoop.hbase.regionserver.compactions.compactor.createfilescanners(compactor.java:203) at org.apache.hadoop.hbase.regionserver.compactions.defaultcompactor.compact(defaultcompactor.java:70) at org.apache.hadoop.hbase.regionserver.defaultstoreengine$defaultcompactioncontext.compact(defaultstoreengine.java:124) at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:1231) at org.apache.hadoop.hbase.regionserver.hregion.compact(hregion.java:1812) at org.apache.hadoop.hbase.testiofencing$compactionblockerregion.compact(testiofencing.java:127) at org.apache.hadoop.hbase.regionserver.compactsplitthread$compactionrunner.run(compactsplitthread.java:524) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:744) caused by: org.apache.hadoop.ipc.remoteexception(java.io.filenotfoundexception): file does not exist: /user/jenkins/test-data/19edea13-027b-4c6a-9f3f-edaf1fc590ab/data/default/tabletest/94d6f21f7cf387d73d8622f535c67311/family/7067addd325446089ba15ec2c77becbc at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:64) at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:54) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsupdatetimes(fsnamesystem.java:1795) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsint(fsnamesystem.java:1738) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1718) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1690) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.getblocklocations(namenoderpcserver.java:519) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.getblocklocations(clientnamenodeprotocolserversidetranslatorpb.java:337) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:585) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:928) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2013) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2009) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:415) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1614) at org.apache.hadoop.ipc.server$handler.run(server.java:2007) at org.apache.hadoop.ipc.client.call(client.java:1411) at org.apache.hadoop.ipc.client.call(client.java:1364) at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:206) at com.sun.proxy.$proxy21.getblocklocations(unknown source) at sun.reflect.generatedmethodaccessor31.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:187) at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:102) at com.sun.proxy.$proxy21.getblocklocations(unknown source) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocoltranslatorpb.getblocklocations(clientnamenodeprotocoltranslatorpb.java:225) at sun.reflect.generatedmethodaccessor32.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:279) at com.sun.proxy.$proxy22.getblocklocations(unknown source) at org.apache.hadoop.hdfs.dfsclient.callgetblocklocations(dfsclient.java:1165) ... 29 more ",
        "label": 198
    },
    {
        "text": "testrestoreflushsnapshotfromclient fails intermittently in trunk builds  from https://builds.apache.org/job/hbase-trunk/3945/testreport/org.apache.hadoop.hbase.snapshot/testrestoreflushsnapshotfromclient/testrestoresnapshot/: 2013-03-11 19:35:54,162 debug [ipc server handler 1 on 51923] master.hmaster(2410): submitting snapshot request for:{ ss=snaptb1-1363030509407 table=testtb-1363030509407 type=flush } 2013-03-11 19:35:54,162 debug [ipc server handler 1 on 51923] snapshot.snapshotdescriptionutils(235): creation time not specified, setting to:1363030554162 (current time:1363030554162). 2013-03-11 19:35:54,163 debug [ipc server handler 1 on 51923] snapshot.snapshotmanager(465): no existing snapshot, attempting snapshot... ... 2013-03-11 19:36:52,139 debug [pool-1-thread-1] client.hbaseadmin(2234): getting current status of snapshot from master... 2013-03-11 19:36:52,140 debug [ipc server handler 1 on 51923] master.hmaster(2481): checking to see if snapshot from request:{ ss=snaptb1-1363030509407 table=testtb-1363030509407 type=flush } is done 2013-03-11 19:36:52,140 debug [ipc server handler 1 on 51923] snapshot.snapshotmanager(344): snapshoting '{ ss=snaptb1-1363030509407 table=testtb-1363030509407 type=flush }' is still in progress! 2013-03-11 19:36:52,140 debug [pool-1-thread-1] client.hbaseadmin(2226): (#16) sleeping: 8000ms while waiting for snapshot completion. 2013-03-11 19:36:55,740 debug [timer-28] errorhandling.foreignexceptiondispatcher(68):  accepting received exception org.apache.hadoop.hbase.errorhandling.timeoutexception via timer-java.util.timer@1ed8384b:org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1363030555739, end:1363030615740, diff:60001, max:60000 ms at org.apache.hadoop.hbase.errorhandling.timeoutexceptioninjector$1.run(timeoutexceptioninjector.java:71) at java.util.timerthread.mainloop(timer.java:555) at java.util.timerthread.run(timer.java:505) caused by: org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1363030555739, end:1363030615740, diff:60001, max:60000 ms at org.apache.hadoop.hbase.errorhandling.timeoutexceptioninjector$1.run(timeoutexceptioninjector.java:68) ... 2 more 2013-03-11 19:36:55,759 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.hconnectionmanager$hconnectionimplementation(944): looking up meta region location in zk, connection=hconnection 0x25c6f10 2013-03-11 19:36:55,760 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] zookeeper.zkutil(1682): hconnection 0x25c6f10-0x13d5aef12b70004 retrieved 35 byte(s) of data from znode /hbase/meta-region-server; data=janus.apache.org,42570,1363030252791 2013-03-11 19:36:55,760 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.hconnectionmanager$hconnectionimplementation(949): looked up meta region location, connection=hconnection 0x25c6f10; servername=janus.apache.org,42570,1363030252791 2013-03-11 19:36:55,760 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.clientscanner(96): creating scanner over .meta. starting at key 'testtb-1363030509407,,' 2013-03-11 19:36:55,760 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.clientscanner(209): advancing internal scanner to startkey at 'testtb-1363030509407,,' 2013-03-11 19:36:55,760 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.hconnectionmanager$hconnectionimplementation(944): looking up meta region location in zk, connection=hconnection 0x25c6f10 2013-03-11 19:36:55,761 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] zookeeper.zkutil(1682): hconnection 0x25c6f10-0x13d5aef12b70004 retrieved 35 byte(s) of data from znode /hbase/meta-region-server; data=janus.apache.org,42570,1363030252791 2013-03-11 19:36:55,761 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.hconnectionmanager$hconnectionimplementation(949): looked up meta region location, connection=hconnection 0x25c6f10; servername=janus.apache.org,42570,1363030252791 2013-03-11 19:36:55,764 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] client.clientscanner(196): finished with scanning at {name => '.meta.,,1', startkey => '', endkey => '', encoded => 1028785192,} 2013-03-11 19:36:55,790 error [master_table_operations-janus.apache.org,51923,1363030251194-0] snapshot.takesnapshothandler(152): failed taking snapshot { ss=snaptb1-1363030509407 table=testtb-1363030509407 type=flush } due to exception:no region directory found for region:{name => 'testtb-1363030509407,4,1363030509409.8a41cf1a6517ac9f9d4e6aaf2c906588.', startkey => '4', endkey => '5', encoded => 8a41cf1a6517ac9f9d4e6aaf2c906588,} org.apache.hadoop.hbase.exceptions.corruptedsnapshotexception: no region directory found for region:{name => 'testtb-1363030509407,4,1363030509409.8a41cf1a6517ac9f9d4e6aaf2c906588.', startkey => '4', endkey => '5', encoded => 8a41cf1a6517ac9f9d4e6aaf2c906588,} at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifyregion(mastersnapshotverifier.java:166) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifyregions(mastersnapshotverifier.java:151) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifysnapshot(mastersnapshotverifier.java:114) at org.apache.hadoop.hbase.master.snapshot.takesnapshothandler.process(takesnapshothandler.java:145) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:130) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) 2013-03-11 19:36:55,791 info  [master_table_operations-janus.apache.org,51923,1363030251194-0] snapshot.takesnapshothandler(203): stop taking snapshot={ ss=snaptb1-1363030509407 table=testtb-1363030509407 type=flush } because: failed to take snapshot '{ ss=snaptb1-1363030509407 table=testtb-1363030509407 type=flush }' due to exception 2013-03-11 19:36:55,791 debug [master_table_operations-janus.apache.org,51923,1363030251194-0] snapshot.takesnapshothandler(159): launching cleanup of working dir:hdfs://localhost:50807/user/jenkins/hbase/.snapshot/.tmp/snaptb1-1363030509407 2013-03-11 19:36:55,838 debug [timer-31] errorhandling.foreignexceptiondispatcher(68):  accepting received exception org.apache.hadoop.hbase.errorhandling.timeoutexception via timer-java.util.timer@4baf3db1:org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1363030555838, end:1363030615838, diff:60000, max:60000 ms at org.apache.hadoop.hbase.errorhandling.timeoutexceptioninjector$1.run(timeoutexceptioninjector.java:71) at java.util.timerthread.mainloop(timer.java:555) at java.util.timerthread.run(timer.java:505) caused by: org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1363030555838, end:1363030615838, diff:60000, max:60000 ms at org.apache.hadoop.hbase.errorhandling.timeoutexceptioninjector$1.run(timeoutexceptioninjector.java:68) ... 2 more 2013-03-11 19:36:55,839 debug [timer-31] procedure.zkprocedurememberrpcs(285): aborting procedure (snaptb1-1363030509407) in zk 2013-03-11 19:36:56,061 error [(janus.apache.org,51923,1363030251194)-proc-coordinator-pool1-thread-1] procedure.procedure(225): procedure 'snaptb1-1363030509407' execution failed! org.apache.hadoop.hbase.errorhandling.timeoutexception via timer-java.util.timer@1ed8384b:org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1363030555739, end:1363030615740, diff:60001, max:60000 ms at org.apache.hadoop.hbase.errorhandling.foreignexceptiondispatcher.rethrowexception(foreignexceptiondispatcher.java:85) at org.apache.hadoop.hbase.procedure.procedure.waitforlatch(procedure.java:371) at org.apache.hadoop.hbase.procedure.procedure.call(procedure.java:215) at org.apache.hadoop.hbase.procedure.procedure.call(procedure.java:68) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334) at java.util.concurrent.futuretask.run(futuretask.java:166) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) caused by: org.apache.hadoop.hbase.errorhandling.timeoutexception: timeout elapsed! source:timeout caused foreign exception start:1363030555739, end:1363030615740, diff:60001, max:60000 ms at org.apache.hadoop.hbase.errorhandling.timeoutexceptioninjector$1.run(timeoutexceptioninjector.java:68) at java.util.timerthread.mainloop(timer.java:555) at java.util.timerthread.run(timer.java:505) 2013-03-11 19:36:56,061 debug [(janus.apache.org,51923,1363030251194)-proc-coordinator-pool1-thread-1] procedure.procedure(228): running finish phase. 2013-03-11 19:36:56,061 debug [(janus.apache.org,51923,1363030251194)-proc-coordinator-pool1-thread-1] procedure.procedure(279): finished coordinator procedure - removing self from list of running procedures 2013-03-11 19:36:56,061 debug [(janus.apache.org,51923,1363030251194)-proc-coordinator-pool1-thread-1] procedure.zkprocedurecoordinatorrpcs(142): attempting to clean out zk node for op:snaptb1-1363030509407 2013-03-11 19:36:56,061 info  [(janus.apache.org,51923,1363030251194)-proc-coordinator-pool1-thread-1] procedure.zkprocedureutil(279): clearing all znodes for procedure snaptb1-1363030509407including nodes /hbase/online-snapshot/acquired /hbase/online-snapshot/reached /hbase/online-snapshot/abort 2013-03-11 19:36:56,080 debug [regionserver:1;janus.apache.org,59357,1363030252847-eventthread] zookeeper.zookeeperwatcher(274): regionserver:59357-0x13d5aef12b70002 received zookeeper event, type=nodecreated, state=syncconnected, path=/hbase/online-snapshot/abort/snaptb1-1363030509407 2013-03-11 19:36:56,080 debug [regionserver:2;janus.apache.org,37320,1363030252872-eventthread] zookeeper.zookeeperwatcher(274): regionserver:37320-0x13d5aef12b70003 received zookeeper event, type=nodecreated, state=syncconnected, path=/hbase/online-snapshot/abort/snaptb1-1363030509407 ",
        "label": 441
    },
    {
        "text": "nightly build is failing with hadoop x  [error] found artifact with unexpected contents: '/testptch/hbase/hbase-shaded/hbase-shaded-client/target/hbase-shaded-client-2.2.1-snapshot.jar'     please check the following and either correct the build or update     the allowed list with reasoning.     javax/     javax/servlet/     javax/servlet/annotation/     javax/servlet/annotation/handlestypes.class     javax/servlet/annotation/httpconstraint.class     javax/servlet/annotation/httpmethodconstraint.class     javax/servlet/annotation/multipartconfig.class     javax/servlet/annotation/package.html     javax/servlet/annotation/servletsecurity$emptyrolesemantic.class     javax/servlet/annotation/servletsecurity$transportguarantee.class     javax/servlet/annotation/servletsecurity.class     javax/servlet/annotation/webfilter.class     javax/servlet/annotation/webinitparam.class     javax/servlet/annotation/weblistener.class     javax/servlet/annotation/webservlet.class     javax/servlet/asynccontext.class     javax/servlet/asyncevent.class     javax/servlet/asynclistener.class     javax/servlet/descriptor/     javax/servlet/descriptor/jspconfigdescriptor.class     javax/servlet/descriptor/jsppropertygroupdescriptor.class     javax/servlet/descriptor/package.html     javax/servlet/descriptor/taglibdescriptor.class     javax/servlet/dispatchertype.class     javax/servlet/filter.class     javax/servlet/filterchain.class     javax/servlet/filterconfig.class     javax/servlet/filterregistration$dynamic.class     javax/servlet/filterregistration.class     javax/servlet/genericservlet.class     javax/servlet/http/     javax/servlet/http/cookie.class     javax/servlet/http/httpservlet.class     javax/servlet/http/httpservletrequest.class     javax/servlet/http/httpservletrequestwrapper.class     javax/servlet/http/httpservletresponse.class     javax/servlet/http/httpservletresponsewrapper.class     javax/servlet/http/httpsession.class     javax/servlet/http/httpsessionactivationlistener.class     javax/servlet/http/httpsessionattributelistener.class     javax/servlet/http/httpsessionbindingevent.class     javax/servlet/http/httpsessionbindinglistener.class     javax/servlet/http/httpsessioncontext.class     javax/servlet/http/httpsessionevent.class     javax/servlet/http/httpsessionidlistener.class     javax/servlet/http/httpsessionlistener.class     javax/servlet/http/httpupgradehandler.class     javax/servlet/http/httputils.class     javax/servlet/http/localstrings.properties     javax/servlet/http/localstrings_es.properties     javax/servlet/http/localstrings_fr.properties     javax/servlet/http/localstrings_ja.properties     javax/servlet/http/nobodyoutputstream.class     javax/servlet/http/nobodyresponse.class     javax/servlet/http/package.html     javax/servlet/http/part.class     javax/servlet/http/webconnection.class     javax/servlet/httpconstraintelement.class     javax/servlet/httpmethodconstraintelement.class     javax/servlet/localstrings.properties     javax/servlet/localstrings_fr.properties     javax/servlet/localstrings_ja.properties     javax/servlet/multipartconfigelement.class     javax/servlet/package.html     javax/servlet/readlistener.class     javax/servlet/registration$dynamic.class     javax/servlet/registration.class     javax/servlet/requestdispatcher.class     javax/servlet/servlet.class     javax/servlet/servletconfig.class     javax/servlet/servletcontainerinitializer.class     javax/servlet/servletcontext.class     javax/servlet/servletcontextattributeevent.class     javax/servlet/servletcontextattributelistener.class     javax/servlet/servletcontextevent.class     javax/servlet/servletcontextlistener.class     javax/servlet/servletexception.class     javax/servlet/servletinputstream.class     javax/servlet/servletoutputstream.class     javax/servlet/servletregistration$dynamic.class     javax/servlet/servletregistration.class     javax/servlet/servletrequest.class     javax/servlet/servletrequestattributeevent.class     javax/servlet/servletrequestattributelistener.class     javax/servlet/servletrequestevent.class     javax/servlet/servletrequestlistener.class     javax/servlet/servletrequestwrapper.class     javax/servlet/servletresponse.class     javax/servlet/servletresponsewrapper.class     javax/servlet/servletsecurityelement.class     javax/servlet/sessioncookieconfig.class     javax/servlet/sessiontrackingmode.class     javax/servlet/singlethreadmodel.class     javax/servlet/unavailableexception.class     javax/servlet/writelistener.class     com/     com/sun/     com/sun/jersey/     com/sun/jersey/api/     com/sun/jersey/api/core/     com/sun/jersey/api/core/servlet/     com/sun/jersey/api/core/servlet/webappresourceconfig.class     com/sun/jersey/server/     com/sun/jersey/server/impl/     com/sun/jersey/server/impl/cdi/     com/sun/jersey/server/impl/cdi/abstractbean.class     com/sun/jersey/server/impl/cdi/annotatedcallableimpl.class     com/sun/jersey/server/impl/cdi/annotatedconstructorimpl.class     com/sun/jersey/server/impl/cdi/annotatedfieldimpl.class     com/sun/jersey/server/impl/cdi/annotatedimpl.class     com/sun/jersey/server/impl/cdi/annotatedmemberimpl.class     com/sun/jersey/server/impl/cdi/annotatedmethodimpl.class     com/sun/jersey/server/impl/cdi/annotatedparameterimpl.class     com/sun/jersey/server/impl/cdi/annotatedtypeimpl.class     com/sun/jersey/server/impl/cdi/beangenerator$1.class     com/sun/jersey/server/impl/cdi/beangenerator.class     com/sun/jersey/server/impl/cdi/cdicomponentproviderfactory$1.class     com/sun/jersey/server/impl/cdi/cdicomponentproviderfactory$2.class     com/sun/jersey/server/impl/cdi/cdicomponentproviderfactory$componentproviderdestroyable.class     com/sun/jersey/server/impl/cdi/cdicomponentproviderfactory.class     com/sun/jersey/server/impl/cdi/cdicomponentproviderfactoryinitializer.class     com/sun/jersey/server/impl/cdi/cdiextension$1.class     com/sun/jersey/server/impl/cdi/cdiextension$2.class     com/sun/jersey/server/impl/cdi/cdiextension$3.class     com/sun/jersey/server/impl/cdi/cdiextension$contextannotationliteral.class     com/sun/jersey/server/impl/cdi/cdiextension$injectannotationliteral.class     com/sun/jersey/server/impl/cdi/cdiextension$jndicontextdiver.class     com/sun/jersey/server/impl/cdi/cdiextension$parameterbean.class     com/sun/jersey/server/impl/cdi/cdiextension$patchinformation.class     com/sun/jersey/server/impl/cdi/cdiextension$predefinedbean.class     com/sun/jersey/server/impl/cdi/cdiextension$syntheticqualifierannotationimpl.class     com/sun/jersey/server/impl/cdi/cdiextension.class     com/sun/jersey/server/impl/cdi/discoveredparameter.class     com/sun/jersey/server/impl/cdi/initializedlater.class     com/sun/jersey/server/impl/cdi/providerbasedbean.class     com/sun/jersey/server/impl/cdi/syntheticqualifier.class     com/sun/jersey/server/impl/cdi/utils.class     com/sun/jersey/server/impl/container/     com/sun/jersey/server/impl/container/servlet/     com/sun/jersey/server/impl/container/servlet/include.class     com/sun/jersey/server/impl/container/servlet/jsptemplateprocessor.class     com/sun/jersey/server/impl/container/servlet/jerseyservletcontainerinitializer.class     com/sun/jersey/server/impl/container/servlet/persessionfactory$1.class     com/sun/jersey/server/impl/container/servlet/persessionfactory$abstractpersession.class     com/sun/jersey/server/impl/container/servlet/persessionfactory$persesson.class     com/sun/jersey/server/impl/container/servlet/persessionfactory$persessoninstantiated.class     com/sun/jersey/server/impl/container/servlet/persessionfactory$persessonproxied.class     com/sun/jersey/server/impl/container/servlet/persessionfactory$sessionmap.class     com/sun/jersey/server/impl/container/servlet/persessionfactory.class     com/sun/jersey/server/impl/container/servlet/requestdispatcherwrapper.class     com/sun/jersey/server/impl/container/servlet/servletadaptor$1$1.class     com/sun/jersey/server/impl/container/servlet/servletadaptor$1.class     com/sun/jersey/server/impl/container/servlet/servletadaptor.class     com/sun/jersey/server/impl/container/servlet/wrapper.class     com/sun/jersey/server/impl/ejb/     com/sun/jersey/server/impl/ejb/ejbcomponentproviderfactory$ejbmanagedcomponentprovider.class     com/sun/jersey/server/impl/ejb/ejbcomponentproviderfactory.class     com/sun/jersey/server/impl/ejb/ejbcomponentproviderfactoryinitilizer.class     com/sun/jersey/server/impl/ejb/ejbexceptionmapper.class     com/sun/jersey/server/impl/ejb/ejbinjectioninterceptor$1.class     com/sun/jersey/server/impl/ejb/ejbinjectioninterceptor.class     com/sun/jersey/server/impl/ejb/ejbrequestdispatcherprovider$1.class     com/sun/jersey/server/impl/ejb/ejbrequestdispatcherprovider.class     com/sun/jersey/server/impl/managedbeans/     com/sun/jersey/server/impl/managedbeans/managedbeancomponentproviderfactory$managedbeancomponentprovider.class     com/sun/jersey/server/impl/managedbeans/managedbeancomponentproviderfactory.class     com/sun/jersey/server/impl/managedbeans/managedbeancomponentproviderfactoryinitilizer.class     com/sun/jersey/spi/     com/sun/jersey/spi/container/     com/sun/jersey/spi/container/servlet/     com/sun/jersey/spi/container/servlet/persession.class     com/sun/jersey/spi/container/servlet/servletcontainer$contextinjectableprovider.class     com/sun/jersey/spi/container/servlet/servletcontainer$internalwebcomponent.class     com/sun/jersey/spi/container/servlet/servletcontainer.class     com/sun/jersey/spi/container/servlet/webcomponent$1.class     com/sun/jersey/spi/container/servlet/webcomponent$2.class     com/sun/jersey/spi/container/servlet/webcomponent$3.class     com/sun/jersey/spi/container/servlet/webcomponent$4.class     com/sun/jersey/spi/container/servlet/webcomponent$contextinjectableprovider.class     com/sun/jersey/spi/container/servlet/webcomponent$writer.class     com/sun/jersey/spi/container/servlet/webcomponent.class     com/sun/jersey/spi/container/servlet/webconfig$configtype.class     com/sun/jersey/spi/container/servlet/webconfig.class     com/sun/jersey/spi/container/servlet/webfilterconfig.class     com/sun/jersey/spi/container/servlet/webservletconfig.class     com/sun/jersey/spi/scanning/     com/sun/jersey/spi/scanning/servlet/     com/sun/jersey/spi/scanning/servlet/webappresourcesscanner$1.class     com/sun/jersey/spi/scanning/servlet/webappresourcesscanner$2.class     com/sun/jersey/spi/scanning/servlet/webappresourcesscanner.class [error] command execution failed. org.apache.commons.exec.executeexception: process exited with an error: 1 (exit value: 1)     at org.apache.commons.exec.defaultexecutor.executeinternal (defaultexecutor.java:404)     at org.apache.commons.exec.defaultexecutor.execute (defaultexecutor.java:166)     at org.codehaus.mojo.exec.execmojo.executecommandline (execmojo.java:804)     at org.codehaus.mojo.exec.execmojo.executecommandline (execmojo.java:751)     at org.codehaus.mojo.exec.execmojo.execute (execmojo.java:313)     at org.apache.maven.plugin.defaultbuildpluginmanager.executemojo (defaultbuildpluginmanager.java:137)     at org.apache.maven.lifecycle.internal.mojoexecutor.execute (mojoexecutor.java:208)     at org.apache.maven.lifecycle.internal.mojoexecutor.execute (mojoexecutor.java:154)     at org.apache.maven.lifecycle.internal.mojoexecutor.execute (mojoexecutor.java:146)     at org.apache.maven.lifecycle.internal.lifecyclemodulebuilder.buildproject (lifecyclemodulebuilder.java:117)     at org.apache.maven.lifecycle.internal.lifecyclemodulebuilder.buildproject (lifecyclemodulebuilder.java:81)     at org.apache.maven.lifecycle.internal.builder.singlethreaded.singlethreadedbuilder.build (singlethreadedbuilder.java:56)     at org.apache.maven.lifecycle.internal.lifecyclestarter.execute (lifecyclestarter.java:128)     at org.apache.maven.defaultmaven.doexecute (defaultmaven.java:305)     at org.apache.maven.defaultmaven.doexecute (defaultmaven.java:192)     at org.apache.maven.defaultmaven.execute (defaultmaven.java:105)     at org.apache.maven.cli.mavencli.execute (mavencli.java:954)     at org.apache.maven.cli.mavencli.domain (mavencli.java:288)     at org.apache.maven.cli.mavencli.main (mavencli.java:192)     at sun.reflect.nativemethodaccessorimpl.invoke0 (native method)     at sun.reflect.nativemethodaccessorimpl.invoke (nativemethodaccessorimpl.java:62)     at sun.reflect.delegatingmethodaccessorimpl.invoke (delegatingmethodaccessorimpl.java:43)     at java.lang.reflect.method.invoke (method.java:498)     at org.codehaus.plexus.classworlds.launcher.launcher.launchenhanced (launcher.java:289)     at org.codehaus.plexus.classworlds.launcher.launcher.launch (launcher.java:229)     at org.codehaus.plexus.classworlds.launcher.launcher.mainwithexitcode (launcher.java:415)     at org.codehaus.plexus.classworlds.launcher.launcher.main (launcher.java:356) ",
        "label": 149
    },
    {
        "text": "fix source files missing licenses in and trunk  running 'mvn rat:check' shows that a few files have snuck in that do not have proper apache licenses. ideally we should fix these before we cut another release/release candidate. this is a blocker for 0.94, and probably should be for the other branches as well. ",
        "label": 154
    },
    {
        "text": "npe in hstore compact  i think i've seen this one before (if line numbers agree): 2008-12-28 00:06:21,111 error org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction failed for region content,10a1c144cf729885001e71a5ff5108dc,1230416158498 java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.hstore.compact(hstore.java:870)         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:709)         at org.apache.hadoop.hbase.regionserver.hregion.compactstores(hregion.java:666) ",
        "label": 314
    },
    {
        "text": "ivy nit regarding checking with latest snapshots  currently - if a new jar gets published in one of the dependent m2 snapshots - ivy does not retrieve it unless the cache is cleared. add changing=\"true\" to the dependency. ( there has to be an alternate way to do it at the resolver level, but for now this works without a hitch). ",
        "label": 266
    },
    {
        "text": "adopt interface stability audience classifications from hadoop  as hbase gets more widely used, we need to be more explicit about which apis are stable and not expected to break between versions, which apis are still evolving, etc. we also have many public classes that are really internal to the rs or master and not meant to be used by users. hadoop has adopted a classification scheme for audience (public, private, or limited-private) as well as stability (stable, evolving, unstable). i think we should copy these annotations to hbase and start to classify our public classes. ",
        "label": 242
    },
    {
        "text": "canary  regionserver fails with cast exception  ./bin/hbase org.apache.hadoop.hbase.tool.canary -regionserver exception in thread \"main\" java.lang.classcastexception: org.apache.hadoop.hbase.tool.canary$stdoutsink cannot be cast to org.apache.hadoop.hbase.tool.canary$extendedsink  at org.apache.hadoop.hbase.tool.canary.newmonitor(canary.java:622)  at org.apache.hadoop.hbase.tool.canary.run(canary.java:536)  at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)  at org.apache.hadoop.hbase.tool.canary.main(canary.java:1154) ",
        "label": 441
    },
    {
        "text": "make sure hbase apis are compatible between and  follow-up for hbase-9477.  some other methods are now different between 94 and 96 (result::getcolumnlatest, put::get, anything that takes a collection of cell, e.g. result ctor, mutation::setfamilymap etc.).  i am assuming things that accept cell (increment::add, delete::adddeletemarker) don't need to change. ",
        "label": 406
    },
    {
        "text": "add zk attributes to list of attributes in master and regionserver uis  list the quorum the server is pointed at. ",
        "label": 229
    },
    {
        "text": "pe defaults to 1k rows   uncommon use case  and easy to hit benchmarks  the performanceevaluation uses 1k rows, which i would argue is uncommon, and also provides an easy to hit performance goal. most of the harder performance issues happens at the low and high side of cell size. in our own application, our key sizes range from 4 bytes to maybe 100 bytes. very rarely 1000 bytes. if we have large values, they are very large, like multiple k sizes. recently a change went into hbase that ran well with pe because the overhead of 1k rows is very low in memory, but under small rows, the expected performance would be hit much more. this is because the per-value overhead (eg: node objects of the skip list/memstore) is amortized more with 1k values. we should make this a tunable setting, and have a low default. i would argue for a 10-30 byte default. ",
        "label": 314
    },
    {
        "text": "testdistributedlogsplitting testdelayeddeleteonfailure times out occasionally  seen twice recently: error message test timed out after 25000 milliseconds stacktrace java.lang.exception: test timed out after 25000 milliseconds at java.lang.object.wait(native method) at java.lang.object.wait(object.java:485) at org.apache.hadoop.ipc.client.call(client.java:1056) at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:225) at $proxy8.complete(unknown source) at sun.reflect.generatedmethodaccessor17.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:82) at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:59) at $proxy8.complete(unknown source) at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.closeinternal(dfsclient.java:3897) at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.close(dfsclient.java:3812) at org.apache.hadoop.fs.fsdataoutputstream$positioncache.close(fsdataoutputstream.java:61) at org.apache.hadoop.fs.fsdataoutputstream.close(fsdataoutputstream.java:86) at org.apache.hadoop.hbase.master.testdistributedlogsplitting.testdelayeddeleteonfailure(testdistributedlogsplitting.java:287) ",
        "label": 286
    },
    {
        "text": "online snapshots scaffolding  basic scaffolding for taking a snapshot of an offline table. this includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots. ",
        "label": 248
    },
    {
        "text": "compaction user priority is slightly broken  the code to get compaction priority is as such:    public int getcompactpriority(int priority) {      // if this is a user-requested compaction, leave this at the highest priority      if(priority == store.priority_user) {        return store.priority_user;      } else {        return this.blockingstorefilecount - this.storefiles.size();      }    } .  priority_user is 1.  the priorities are compared as numbers in hregion, so compactions of blocking stores will override user priority (probably intended); also, if you have blockingfiles minus one, your priority is suddenly priority_user, which may cause at least this:  log.debug(\"warning, compacting more than \" + comconf.getmaxfilestocompact() +  \" files because of a user-requested major compaction\");  as well as some misleading logging. ",
        "label": 406
    },
    {
        "text": "illegalargumentexception is thrown when an empty region is splitted  this is w.r.t a mail sent in the dev mail list. empty region split should be handled gracefully. either we should not allow the split to happen if we know that the region is empty or we should allow the split to happen by setting the no of threads to the thread pool executor as 1. int nbfiles = hstorefilestosplit.size(); threadfactorybuilder builder = new threadfactorybuilder();     builder.setnameformat(\"storefilesplitter-%1$d\");     threadfactory factory = builder.build();     threadpoolexecutor threadpool =       (threadpoolexecutor) executors.newfixedthreadpool(nbfiles, factory);     list<future<void>> futures = new arraylist<future<void>>(nbfiles); here the nbfiles needs to be a non zero positive value. ",
        "label": 360
    },
    {
        "text": "unbundle our rpc versioning  rather than a global for all interfaces   region  master  region to master  and coprocesssors   instead version each individually  we'd undo the global rpc version so a change in cp interface or a change in the 'private' regionserver to master interface would not break clients who do not use cps or who don't care about the private regionserver to master protocol. beno\u00eet suggested this. i want it because i want to get rid of heartbeating so will want to change the regionserver to master interface. ",
        "label": 314
    },
    {
        "text": "rename hmaster listsnapshots as getcompletedsnapshots   during code review, i proposed renaming hmaster#listsnapshots as getcompletedsnapshots() jon agreed. this task would perform the renaming across java and ruby code ",
        "label": 441
    },
    {
        "text": " replication  add a check to make sure we don't replicate to ourselves  it's currently possible to add a peer for replication and point it to the local cluster, which i believe could very well happen for those like us that use only one zk ensemble per dc so that only the root znode changes when you want to set up replication intra-dc. i don't think comparing just the cluster id would be enough because you would normally use a different one for another cluster and nothing will block you from pointing elsewhere.  comparing the zk ensemble address doesn't work either when you have multiple dns entries that point at the same place. i think this could be resolved by looking up the master address in the relevant znode as it should be exactly the same thing in the case where you have the same cluster. ",
        "label": 134
    },
    {
        "text": " brainstorm   restore  snapshots for hbase  discussion ticket around the definitions/expectations of different parts of snapshot restoration. this is complementary, but separate from the how of taking a snapshot of a table. ",
        "label": 309
    },
    {
        "text": "extend the walactionslistener api to accomodate log archival  the walobserver interface exposes the log roll events. it would be nice to extend it to accomodate log archival events as well. ",
        "label": 140
    },
    {
        "text": "issue with only using the old config param hbase hstore compactionthreshold but not the corresponding new one  one observation while going through the code:- in memstoreflusher constructor this.blockingstorefilesnumber =       conf.getint(\"hbase.hstore.blockingstorefiles\", 7);     if (this.blockingstorefilesnumber == -1) {       this.blockingstorefilesnumber = 1 +         conf.getint(\"hbase.hstore.compactionthreshold\", 3);     } here as per the code if hbase.hstore.blockingstorefiles is configured as -1, we are making this value to be 1+ min files to compact but here we read the old config item only! here also we need to read the new config 1st and if not there then the old one.. is this a miss? like  conf.getint(\"hbase.hstore.compaction.min\",  conf.getint(\"hbase.hstore.compactionthreshold\", 3)) ",
        "label": 46
    },
    {
        "text": "zerocopyliteralbytestring zerocopygetbytes has an unusable prototype and conflicts with asynchbase  in hbase-9868 the trick that asynchbase uses to extract byte arrays from protobufs without copying them was ported, however the signature of zerocopygetbytes was changed for some reason. there are two problems with the changed signature: 1. it makes the helper function unusable since it refers to a package-private class. 2. it clashes with the signature asynchbase expects, thereby making user's life miserable for those who pull in both asynchbase and hbase on their classpath. ",
        "label": 70
    },
    {
        "text": "stargate needs both jar and war artifacts  since the move to maven we have lost the ability to have stargate bundled as both a jar and a war file (currently just the latter). i'm proposing to split the current stargate module further into 2 sub-modules 'stargate-war' and 'stargate-core' (totally up for discussion on naming). basically moving existing sources down to 'stargate-core' and relocating the conf section into the war sub-module. i'll be doing this via a github fork to make the review and merge process easier. however i suspect i'll do the maven side ok but totally hose the git part, so strap yourselves in.. ",
        "label": 38
    },
    {
        "text": "if fail to open reference because fnfe  make it plain it is a reference  if root file for a reference is missing, takes a while to figure it. master-side says failed open of region. regionserver side it talks about fnfe for some random file. better, dump out reference data. helps figuring what has gone wrong. otherwise its confusing hard to tie the fnfe to root cause. ",
        "label": 314
    },
    {
        "text": "a concurrency issue on softvaluesortedmap  softvaluesortedmap is backed by a treemap. all the methods in this class are synchronized. if we use this method to add/delete elements, it's ok.  but in hconnectionmanager#getcachedlocation, it use headmap to get a view from softvaluesortedmap#internalmap. once we operate   on this view map(like add/delete) in other threads, a concurrency issue may occur. ",
        "label": 286
    },
    {
        "text": "fix  bin rb scripts or remove them  i started to review the bin content to see what had rotted and what had not. the first script i tried doesn't say how to run it and when i did it did this: durruti:hbase-0.97-snapshot stack$ ./bin/get-active-master.rb env: hbase-jruby: no such file or directory durruti:hbase-0.97-snapshot stack$ ./bin/hbase org.jruby.main bin/get-active-master.rb nativeexception: java.lang.illegalargumentexception: not a host:port pair: pbuf 172.21.3.100??????   __ensure__ at bin/get-active-master.rb:37       (root) at bin/get-active-master.rb:34 durruti:hbase-0.97-snapshot stack$ vi bin/ check and fix all *.rb scripts or remove. blocker on 0.95. ",
        "label": 543
    },
    {
        "text": "some more unwanted reference to unshaded pb classes  protobuflogreader - seems using unshaded cis which seems a miss  hbasefsck - some public methods throw pb serviceexception. its strange. no code within that throws this.  public exposed pbtype class. i dont know what this type allows the users to do. make their own type? if so the unshaded might be ok. ",
        "label": 314
    },
    {
        "text": "multiple slf4j log4j provider versions included in binary package  branch   examining binary assembly results there are multiple versions of slf4j-log4j in lib/ slf4j-api-1.7.7.jar slf4j-log4j12-1.6.1.jar slf4j-log4j12-1.7.10.jar slf4j-log4j12-1.7.7.jar we aren't managing slf4j-log4j12 dependency versions correctly, somehow. ",
        "label": 473
    },
    {
        "text": "npe when master joins running cluster if a rit references a rs no longer present  ",
        "label": 314
    },
    {
        "text": " test compat  test delegation tokens continue to work when hbase1 going against hbase2 cluster  suggested by francis christopher liu need to test. ",
        "label": 252
    },
    {
        "text": "tool to regenerate an hbase table from the data files  the purpose of this jira is provide a place to coordinate the development of a utility that will regenerate an hbase table from the data files. here are some comments from stack on this subject from the hbase-user mailing list: well, in the bin directory, there are scripts that do various things with  the .meta. (copy a table, move a table, load a table whose source is hfiles  written by a mapreduce job; i.e. hbase-48). so, to 'regenerate an hbase table from the data files', you'd need to do  something like the following: + delete all exisiting table references from .meta.  + move the backuped up table into position under hbase.rootdir  + per region under hbase.rootdir, add an entry to .meta. do this by opening  the .regioninfo file. its content is needed to generate the rowid for  .meta. and its value becomes the info:regioninfo cell value. hbase does not need to be down. on next .meta. scan, the newly added  regions will be noticed. they won't have associated info:server and  info:startcode entries so master will go ahead and assign them and you  should be up and running. code-wise, a study of copy_table.rb (this uses old api ... needs updating  but the concepts are the same) and loadtable.rb would probably be fruitful. ",
        "label": 314
    },
    {
        "text": "provide clear and consistent logging about the period of enabled chores  similar to hbase-23038, we should always log info about our enabled chores. right now wether or not we get some information is up to particular chore constructors and by and large we don't get any log messages when things can get started, even if the period is something impossibly long (e.g. 3000 days). when we go to schedule the chore here:       if (chore.getperiod() <= 0) {         log.info(\"the period is {} seconds, {} is disabled\", chore.getperiod(), chore.getname());         return false;       } we should add an else clause that says it's enabled. it looks like we could then just call chore.tostring to get the proper details about the chore and its period. ",
        "label": 328
    },
    {
        "text": "start hbase sh failed with classnotfoundexception when build with hadoop3  exception in thread \"main\" java.lang.noclassdeffounderror: com/ctc/wstx/io/inputbootstrapperexception in thread \"main\" java.lang.noclassdeffounderror: com/ctc/wstx/io/inputbootstrapper at org.apache.hadoop.hbase.util.hbaseconftool.main(hbaseconftool.java:39)caused by: java.lang.classnotfoundexception: com.ctc.wstx.io.inputbootstrapper at java.net.urlclassloader.findclass(urlclassloader.java:382) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) ... 1 moreexception in thread \"main\" java.lang.noclassdeffounderror: com/ctc/wstx/io/inputbootstrapper at org.apache.hadoop.hbase.zookeeper.zkservertool.main(zkservertool.java:63)caused by: java.lang.classnotfoundexception: com.ctc.wstx.io.inputbootstrapper at java.net.urlclassloader.findclass(urlclassloader.java:382) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) ... 1 more ",
        "label": 187
    },
    {
        "text": "remove flush related records from wal and make locking more granular  comments from many people in hbase-6466 and hbase-6980 indicate that flush records in wal are not useful. if so, they should be removed. ",
        "label": 406
    },
    {
        "text": "fix testmetawithreplicas  ",
        "label": 314
    },
    {
        "text": " book  zookeeper quorum mistake  in chapter 2, section 2.5 \"zookeeper\" under \"how many zookeepers should i run?\" there is the sentence  there can be no quorum if the number of members is an even number. this is not true. in zookeeper, an even number of peers is supported, but it is normally not used because an even sized ensemble requires, proportionally, more peers to form a quorum than an odd sized ensemble requires. for example, an ensemble with 4 peers requires 3 to form a quorum, while an ensemble with 5 also requires 3 to form a quorum. thus, an ensemble of 5 allows 2 peers to fail, and thus is more fault tolerant than the ensemble of 4, which allows only 1 down peer. ",
        "label": 146
    },
    {
        "text": "repeated split causes hregionserver failures and breaks table  repeated splits on large tables (2 consecutive would suffice) will essentially \"break\" the table (and the cluster), unrecoverable.  the regionserver doing the split dies and the master will get into an infinite loop trying to assign regions that seem to have the files missing from hdfs. the table can be disabled once. upon trying to re-enable it, it will remain in an intermediary state forever. i was able to reproduce this on a smaller table consistently. hbase(main):030:0> (0..10000).each{|x| put 't1', \"#{x}\", 'f1:t', 'dd'} hbase(main):030:0> (0..1000).each{|x| split 't1', \"#{x*10}\"} running overlapping splits in parallel (e.g. \"# {x*10+1} \", \"# {x*10+2} \"... ) will reproduce the issue almost instantly and consistently. 2012-03-28 10:57:16,320 info org.apache.hadoop.hbase.catalog.metaeditor: offlined parent region t1,,1332957435767.2fb0473f4e71339e88dab0ee0d4dffa1. in meta 2012-03-28 10:57:16,321 debug org.apache.hadoop.hbase.regionserver.compactsplitthread: split requested for t1,5,1332957435767.648d30de55a5cec6fc2f56dcb3c7eee1..  compaction_queue=(0:1), split_queue=10 2012-03-28 10:57:16,343 info org.apache.hadoop.hbase.regionserver.splitrequest: running rollback/cleanup of failed split of t1,,1332957435767.2fb0473f4e71339e88dab0ee0d4dffa1.; failed ld2,60020,1332957343833-daughteropener=2469c5650ea2aeed631eb85d3cdc3124 java.io.ioexception: failed ld2,60020,1332957343833-daughteropener=2469c5650ea2aeed631eb85d3cdc3124         at org.apache.hadoop.hbase.regionserver.splittransaction.opendaughters(splittransaction.java:363)         at org.apache.hadoop.hbase.regionserver.splittransaction.execute(splittransaction.java:451)         at org.apache.hadoop.hbase.regionserver.splitrequest.run(splitrequest.java:67)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) caused by: java.io.filenotfoundexception: file does not exist: /hbase/t1/589c44cabba419c6ad8c9b427e5894e3.2fb0473f4e71339e88dab0ee0d4dffa1/f1/d62a852c25ad44e09518e102ca557237         at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.openinfo(dfsclient.java:1822)         at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.<init>(dfsclient.java:1813)         at org.apache.hadoop.hdfs.dfsclient.open(dfsclient.java:544)         at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:187)         at org.apache.hadoop.fs.filesystem.open(filesystem.java:456)         at org.apache.hadoop.hbase.io.hfile.hfile.createreader(hfile.java:341)         at org.apache.hadoop.hbase.regionserver.storefile$reader.<init>(storefile.java:1008)         at org.apache.hadoop.hbase.io.halfstorefilereader.<init>(halfstorefilereader.java:65)         at org.apache.hadoop.hbase.regionserver.storefile.open(storefile.java:467)         at org.apache.hadoop.hbase.regionserver.storefile.createreader(storefile.java:548)         at org.apache.hadoop.hbase.regionserver.store.loadstorefiles(store.java:284)         at org.apache.hadoop.hbase.regionserver.store.<init>(store.java:221)         at org.apache.hadoop.hbase.regionserver.hregion.instantiatehstore(hregion.java:2511)         at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:450)         at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:3229)         at org.apache.hadoop.hbase.regionserver.splittransaction.opendaughterregion(splittransaction.java:504)         at org.apache.hadoop.hbase.regionserver.splittransaction$daughteropener.run(splittransaction.java:484)         ... 1 more 2012-03-28 10:57:16,345 fatal org.apache.hadoop.hbase.regionserver.hregionserver: aborting region server ld2,60020,1332957343833: abort; we got an error after point-of-no-return http://hastebin.com/diqinibajo.avrasm later edit: (i'm using the last 4 characters from each string)  region 94e3 has storefile 7237  region 94e3 gets splited in daughters a: ffa1 and b: eee1  daughter region ffa1 get's splitted in daughters a: 3124 and b: dc77  ffa1 has a reference: 7237.94e3 for it's store file  when ffa1 gets splited it will create another reference: 7237.94e3.ffa1  when splittransaction will execute() it will try to open that (opendaughters above) and it will match it from left to right [storefile].[region] \"^([0-9a-f]+)(?:\\\\.(.+))?$\" and will attempt to go to /hbase/t1/[region] which resolves to   /hbase/t1/94e3.ffa1/f1/7237 - which obviously doesn't exist and will fail. this seems like a design problem: we should either stop from splitting if the path is reference or be able to recursively resolve reference paths (e.g. parse right to left 7237.94e3.ffa1 -> [7237.94e3].ffa1 -> open /hbase/t1/ffa1/f1/7237.94e3 -> [7237].94e3 -> open /hbase/t1/94e3/7237) ",
        "label": 113
    },
    {
        "text": "hbaseadmin should perform validation of connection it holds  through hbase-3777, hconnectionmanager reuses the connection to hbase servers.  one challenge, discovered in troubleshooting hbase-4052, is how we invalidate connection(s) to server which gets restarted.  there're at least two ways.  1. hconnectionmanager utilizes background thread(s) to periodically perform validation of connections in hbase_instances and remove stale connection(s).  2. allow hbaseclient (including hbaseadmin) to provide feedback to hconnectionmanager. the solution can be a combination of both of the above. ",
        "label": 441
    },
    {
        "text": "should deal with error in the callback of rawasynchbaseadmin splitregion methods  should be typos... ",
        "label": 149
    },
    {
        "text": "list hdfs enhancements to speed up backups for hbase  there are a host of improvements that help: hdfs fast copy various enhancements to fast copy to speed up things file level hard links - which does ext3 hardlinks instead of copying blocks thereby saving a lot of iops need to list out the hdfs jira's and have patches on them. ",
        "label": 359
    },
    {
        "text": "hbase client should not require jackson for pure hbase queries be executed  including the hbase-client module dependency and excluding jackson dependencies causes the pure hbase query (run with htableinterface) fail with jackson objectmapper classnotfoundexception. this is due to org.apache.hadoop.hbase.client.operation having objectmapper statically initialized. moving objectmapper to a dedicated utility will help. the patch will be attached. ",
        "label": 405
    },
    {
        "text": "use cellcomparator instead of kvcomparator  ",
        "label": 544
    },
    {
        "text": "add regular expression filters back into  add ability to match on kv  it seems these no longer work. should be reimplemented with new interface. ",
        "label": 247
    },
    {
        "text": "table jsp fails if ugly regions in table  i'm playing w/ messing up hbase:meta as part of hbck2 messings. noticed that a bad server or an unknown region makes it so the table.jsp fails. i noticed that admin trying to get the compaction stats for the table will get stuck trying to contact an unknown region and that when it does complete eventually (tens of minutes...), it will spew an exception in the compaction state location. the listing of servers has an issue where if unknown server, then we'll try to use a null address which does another spew of jsp complaint instead of just logging an unknown.... i tried messing around with admin trying to set its operation timeout... but operation timeout is not really settable unless you get a new configuration for a new connection \u2013 which is madness \u2013 so instead just used the new asyncadminbuilder where i can set operation timeout.... it times out too quick but thats another issue.... the ui at least draws with a messy hbase;meta. ",
        "label": 314
    },
    {
        "text": "supporting for hlog appends  i thank we should open a ticket to track what needs changed to support appends when the coding is done on hadoop-1700. ",
        "label": 241
    },
    {
        "text": "fuzzyrowfilter skips valid rows  the issue may affect not only master branch, but previous releases as well.  this is from one of our customers: we are experiencing a problem with the fuzzyrowfilter for hbase scan. we think that it is a bug.   fuzzy filter should pick a row if it matches filter criteria irrespective of other rows present in table but filter is dropping a row depending on some other row present in table. details/step to reproduce/sample outputs below: missing row key: \\x9c\\x00\\x044\\x00\\x00\\x00\\x00   causing row key: \\x9c\\x00\\x03\\xe9e\\xbb{x\\x1fwts\\x1f\\x15vrx prerequisites   1. create a test table. hbase shell command \u2013 create 'fuzzytest','d'   2. insert some test data. hbase shell commands:   \u2022 put 'fuzzytest',\"\\x9c\\x00\\x044\\x00\\x00\\x00\\x00\",'d:a','junk'   \u2022 put 'fuzzytest',\"\\x9c\\x00\\x044\\x01\\x00\\x00\\x00\",'d:a','junk'   \u2022 put 'fuzzytest',\"\\x9c\\x00\\x044\\x00\\x01\\x00\\x00\",'d:a','junk'   \u2022 put 'fuzzytest',\"\\x9c\\x00\\x044\\x00\\x00\\x01\\x00\",'d:a','junk'   \u2022 put 'fuzzytest',\"\\x9c\\x00\\x044\\x00\\x01\\x00\\x01\",'d:a','junk'   \u2022 put 'fuzzytest',\"\\x9b\\x00\\x044e\\xbb\\xb2\\xbb\",'d:a','junk'   \u2022 put 'fuzzytest',\"\\x9d\\x00\\x044e\\xbb\\xb2\\xbb\",'d:a','junk'   now when you run the code, you will find \\x9c\\x00\\x044\\x00\\x00\\x00\\x00 in output because it matches filter criteria. (refer how to run code below)   insert the row key causing bug:   hbase shell command: put 'fuzzytest',\"\\x9c\\x00\\x03\\xe9e\\xbb{x\\x1fwts\\x1f\\x15vrx\",'d:a','junk'   now when you run the code, you will not find \\x9c\\x00\\x044\\x00\\x00\\x00\\x00 in output even though it still matches filter criteria. verified the issue on master. ",
        "label": 478
    },
    {
        "text": "table need to replication unless all of cfs are excluded  there is duplicate code in replicationutils.contains and replicationpeerconfig.needtoreplicate about deciding whether a table need replicate to the peer cluster.  and the implementation of replicationpeerconfig.needtoreplicate has a bug, it return false when replicateallusertables flag is true and excludetablecfsmap contains not all of cfs.     we should copy the code from replicationutils.contains to replicationpeerconfig.needtoreplicate, and delete replicationutils.contains.because replicationutils is from module replication, replicationpeerconfig is from module client, and module replication depends on module client.     in the following code, replicationpeerconfig.needtoreplicate return false, when replicateallusertables flag is true and excludetablecfsmap contains not all of cfs. public boolean needtoreplicate(tablename table) {   if (replicateallusertables) {   ......     if (excludetablecfsmap != null && excludetablecfsmap.containskey(table)) {       return false;     }   ...... } ",
        "label": 430
    },
    {
        "text": "consolidate printusage in integrationtestloadandverify  investigating the use of itlav is a little screwy. subclasses are not overriding the printusage() methods correctly, so you have to pass --help to get some info and no arguments to get the rest. [hbase@ndimiduk-112rc2-7 ~]$ hbase org.apache.hadoop.hbase.test.integrationtestloadandverify --help usage: bin/hbase org.apache.hadoop.hbase.test.integrationtestloadandverify <options> options:  -h,--help                 show usage  -m,--monkey <arg>         which chaos monkey to run  -monkeyprops <arg>        the properties file for specifying chaos monkey properties.  -ncc,--noclustercleanup   don't clean up the cluster at the end [hbase@ndimiduk-112rc2-7 ~]$ hbase org.apache.hadoop.hbase.test.integrationtestloadandverify integrationtestloadandverify [-doptions] <load|verify|loadandverify>   loads a table with row dependencies and verifies the dependency chains options   -dloadmapper.table=<name>        table to write/verify (default autogen)   -dloadmapper.backrefs=<n>        number of backreferences per row (default 50)   -dloadmapper.num_to_write=<n>    number of rows per mapper (default 100,000 per mapper)   -dloadmapper.deleteafter=<bool>  delete after a successful verify (default true)   -dloadmapper.numpresplits=<n>    number of presplit regions to start with (default 40)   -dloadmapper.map.tasks=<n>       number of map tasks for load (default 200)   -dverify.reduce.tasks=<n>        number of reduce tasks for verify (default 35)   -dverify.scannercaching=<n>      number hbase scanner caching rows to read (default 50) ",
        "label": 370
    },
    {
        "text": "coprocessor failure during batchmutation leaves the memstore datastructs in an inconsistent state  observed this in the testing with phoenix. the test in phoenix - mutableindexfailureit deliberately fails the batchmutation call via the installed coprocessor. but the update is not rolled back. that leaves the memstore inconsistent. in particular, i observed that getflushablesize is updated before the coprocessor was called but the update is not rolled back. when the region is being closed at some later point, the assert introduced in hbase-10514 in the hregion.doclose() causes the regionserver to shutdown abnormally. ",
        "label": 339
    },
    {
        "text": "design procedures for replicationmanager to notify peer change event from master  after we store peer states / peer queues information into hbase table, rs can not track peer config change by adding watcher znode. so we need design procedures for replicationmanager to notify peer change event. the replication rpc interfaces which may be implemented by procedures are following: 1. addreplicationpeer 2. removereplicationpeer 3. enablereplicationpeer 4. disablereplicationpeer 5. updatereplicationpeerconfig btw, our rs states will still be store in zookeeper, so when rs crash, the tracker which will trigger to transfer queues of crashed rs will still be a zookeeper tracker. we need not implement that by procedures. as we will release 2.0 in next weeks, and the hbase-15867 can not be resolved before the release, so i'd prefer to create a new feature branch for hbase-15867. ",
        "label": 149
    },
    {
        "text": "upgrade branch to hadoop  if it works   making new issue that we upgrade to 0.16.3 for the hbase 0.1.2 release. be sure to upgrade the content of hbase/lib/native at same time. ",
        "label": 241
    },
    {
        "text": "upgrade yetus version in rm scripts  the rm scripts use yetus 0.9.0 to generate release notes. we can upgrade it to the latest version. ",
        "label": 175
    },
    {
        "text": "dist apache org must not be used for public downloads  the dist.apache.org server is only intended for use by developers in staging releases. it must not be used on public download pages.  please use www.apache.org/dist (for keys, hashes and sigs) and the mirror system instead. the current download page has lots of references to dist.a.o; please replace thes. ",
        "label": 141
    },
    {
        "text": "testmasterfailover often times out  tests run: 7, failures: 0, errors: 1, skipped: 0, time elapsed: 301.644 sec <<< failure! - in org.apache.hadoop.hbase.master.testmasterfailover  testmasterfailoverwithmockedrit(org.apache.hadoop.hbase.master.testmasterfailover) time elapsed: 240.112 sec <<< error!  org.junit.runners.model.testtimedoutexception: test timed out after 240000 milliseconds  at java.lang.thread.sleep(native method)  at org.apache.hadoop.hbase.util.threads.sleep(threads.java:146)  at org.apache.hadoop.hbase.minihbasecluster.waitforactiveandreadymaster(minihbasecluster.java:535)  at org.apache.hadoop.hbase.hbasecluster.waitforactiveandreadymaster(hbasecluster.java:280)  at org.apache.hadoop.hbase.master.testmasterfailover.testmasterfailoverwithmockedrit(testmasterfailover.java:400) results : tests in error:  testmasterfailover.testmasterfailoverwithmockedrit:400 \u00bb testtimedout test tim... tests run: 7, failures: 0, errors: 1, skipped: 0 ",
        "label": 314
    },
    {
        "text": "thrift   replace  bool writetowal  with  tdurability durability   introduce new enum tdurability to expose more options for write to wal. ",
        "label": 193
    },
    {
        "text": "branches precommit and nightly yetus jobs are using jdk8 for jdk7 jobs  branch-1 nightly job is failing while trying to do our yetus shaded jar test because the enforcer plugin complains about a jdk8 jdk-tools jar being present.   looking at the yetus footer, it's because jdk8 is being used instead of jdk7: [2019-08-30t13:30:34.353z] | maven | version: apache maven 3.0.5 | [2019-08-30t13:30:34.353z] | default java | 1.8.0_222 |   same thing in the footer of the jdk7 build (which passes, even though it shouldn't) [2019-08-30t15:43:29.103z] | maven | version: apache maven 3.0.5 | [2019-08-30t15:43:29.103z] | default java | 1.8.0_222 | both builds appear to be properly running in docker mode. ",
        "label": 402
    },
    {
        "text": "dont use reflection for security  security.user class uses reflection so that hbase can work with older hadoop's not having security. now that we require 1.x, or 0.23 or 2.x, all hadoop versions have security code. we can get rid of most of the user class. ",
        "label": 320
    },
    {
        "text": "servershutdownhandler fails on npe if a plan has a random region assignment  by chance, we were able to revert the ulimit on one of our clusters to 1024 and it started dying non-stop on \"too many open files\". now the bad thing is that some region servers weren't completely servershutdownhandler'd because they failed on: 2011-05-07 00:04:46,203 error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event m_server_shutdown  java.lang.nullpointerexception  at org.apache.hadoop.hbase.master.assignmentmanager.processservershutdown(assignmentmanager.java:1804)  at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:101)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:156)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) reading the code, it seems the npe is in the if statement: map.entry<string, regionplan> e = i.next(); if (e.getvalue().getdestination().equals(hsi)) {   // use iterator's remove else we'll get cme   i.remove(); } which means that the destination (hsi) is null. looking through the code, it seems we instantiate a regionplan with a null hsi when it's a random assignment. it means that if there's a random assignment going on while a node dies then this issue might happen. initially i thought that this could mean data loss, but the logs are already split so it's just the reassignment that doesn't happen (still bad). also it left the master with dead server being processed, so for two days the balancer didn't run failing on: org.apache.hadoop.hbase.master.hmaster: not running balancer because processing dead regionserver(s): [] and the reason why the array is empty is because we are running 0.90.3 which removes the rs from the dead list if it comes back. ",
        "label": 229
    },
    {
        "text": "hlogs in zk are not cleaned up when replication lag is minimal  on a cluster with very low replication lag (as measured by ageoflastshippedop on source), we found hlogs accumulating and not being cleaned up as new wal(s) are rolled. each time, we call logpositionandcleanoldlogs() to clean older logs whenever the current wal is not being written to any more - as suggested by currentwalbeingwrittento being false. however, when lags are small, we may hit the following block first and continue onto the next wal without clearing the old wal(s)... replicationsource::run() {  if (readallentriestoreplicateornextfile(currentwalisbeingwrittento = false)) { // if we are here, then we advance to the next wal without any cleaning // and close existing wal continue; } // ship some edits and call logpositionandcleanoldlogs  } if we hit readallentriestoreplicateornextfile(false) only once - then older logs are not cleaned out and persist in the zookeeper node since we simply call \"continue\" and skip the subsequent logpositionandcleanoldlogs call - if its called more than once, we do end up clearing the old logs. ",
        "label": 463
    },
    {
        "text": "do not wrap the original compactionlifecycletracker when calling cp hooks  my fault... the ut added in hbase-18989 does not follow the new rule so the cp is not loaded actually... when fixed the ut fails... the reason is that i use a wrapped compactionlifecycletracker for implementing compactionlifecycletracker. obviously this is not the correct approach as we need to pass the original one to cp hooks... let me fix. ",
        "label": 149
    },
    {
        "text": "testreplicationadmin testconcurrentpeeroperations hangs  ",
        "label": 187
    },
    {
        "text": "add  master  link to regionserver pages  use the zk info where the master is to add a ui link on the top of each regionserver page. currently you cannot navigate directly to the master ui once you are on a rs page. not sure if the info port is exposed ottomh, but we could either use the rs local config setting for that or add it to zk to enable lookup. ",
        "label": 186
    },
    {
        "text": "hbck report showed the offline regions which belong to disabled table  for disabled table, the regions were offline and the info:server may be an unknownserver. the hbck report should not show these regions in ui. because these regions no need to fix. ",
        "label": 187
    },
    {
        "text": " findbugs  he  class defines equals  and uses object hashcode   findbugs warns that several classes define equals but not hashcode: he: class defines equals() and uses object.hashcode() (he_equals_use_hashcode) this class overrides equals(object), but does not override hashcode(), and inherits the implementation of hashcode() from java.lang.object (which returns the identity hash code, an arbitrary value assigned to the object by the vm).  therefore, the class is very likely to violate the invariant that equal objects must have equal hashcodes. ",
        "label": 320
    },
    {
        "text": "replication can have data loss if peer id contains hyphen    this is an extension to hbase-8207. it seems that there is no check for the peer id string (which is the short name for the replication peer) format. so in case a peer id containing \"-\", it will cause data loss silently on server failure. i did not verify the claim via testing though, this is just purely from reading the code. ",
        "label": 437
    },
    {
        "text": "hbaseclient gets stuck waiting for a response  we've seen in a couple of replication use cases that the client thread keeps waiting for a response but waits for ever, as it does not get a response. the client waits indefintely even if a rpctimeout is specified. 1) need to find out what is causing this. 2) convert the unconditional wait() in hbaseclient into a timed wait, so that the client can bail out if it waits longer than the rpctimeout ",
        "label": 154
    },
    {
        "text": "update links to hadoop wiki in code and book  seems hadoop has moved their wiki, so now links throughout our book are broken. we've found and fixed a couple one-offs, but we should do a sweep and clean up the rest. ",
        "label": 328
    },
    {
        "text": "hbase ec2 scripts  attached tarball is a clone of the hadoop ec2 scripts, modified significantly to start up a hbase storage only cluster on top of hdfs backed by instance storage. tested with the hbase 0.20 branch but should work with trunk also. only the ami create and launch scripts are tested. will bring up a functioning hbase cluster. do \"create-hbase-image c1.xlarge\" to create an x86_64 ami, or \"create-hbase-image c1.medium\" to create an i386 ami. public hadoop/hbase 0.20.1 amis are available:  i386: ami-c644a7af  x86_64: ami-f244a79b launch-hbase-cluster brings up the cluster: first, a small dedicated zk quorum, specifiable in size, default of 3. then, the dfs namenode (formatting on first boot) and one datanode and the hbase master. then, a specifiable number of slaves, instances running dfs datanodes and hbase region servers. for example:     launch-hbase-cluster testcluster 100 5 would bring up a cluster with 100 slaves supported by a 5 node zk ensemble. we must colocate a datanode with the namenode because currently the master won't tolerate a brand new dfs with only namenode and no datanodes up yet. see hbase-1960. by default the launch scripts provision zookeeper as c1.medium and the hbase master and region servers as c1.xlarge. the result is a hbase cluster supported by a zookeeper ensemble. zk ensembles are not dynamic, but hbase clusters can be grown by simply starting up more slaves, just like hadoop. hbase-ec2-init-remote.sh can be trivially edited to bring up a jobtracker on the master node and task trackers on the slaves. ",
        "label": 38
    },
    {
        "text": "possible loss of data in snapshot taken after region split  right after a region split but before the daughter regions are compacted, we have two daughter regions containing reference files to the parent hfiles. if we take snapshot right at the moment, the snapshot will succeed, but it will only contain the daughter reference files. since there is no hold on the parent hfiles, they will be deleted by the hfile cleaner after they are no longer needed by the daughter regions soon after. a minimum we need to do is the keep these parent hfiles from being deleted. ",
        "label": 309
    },
    {
        "text": "create empty hbase client module  ",
        "label": 154
    },
    {
        "text": "add support for merging implicit regions in merge tool  currently org.apache.hadoop.hbase.util.merge needs 2 region names to be explicitly specified to perform a merge. this can be cumbersome.  one idea for improvement is to have merge to figure out all the adjacent regions and perform the merges. for example:  regions before merge: row-10, row-20, row-30, row-40, row-50  regions after merge: row-10, row-30, row-50 in the above example, region names of \"row-10\" and \"row-20\" are merged to become a new bigger region of \"row-10\". ",
        "label": 230
    },
    {
        "text": "fix javadoc warnings introduced in hbase  fix below two javadoc warnings introduced as part of hbase-10169 2 warnings [warning] javadoc warnings [warning] d:\\d\\hbasecommunity\\hbasetrunktest\\hbase-client\\src\\main\\java\\org\\apache\\hadoop\\hbase\\client\\regioncoprocessorserviceexec.java:43: warning - tag @link: can't find batchcoprocessorservice(methoddescriptor, message, byte[], byte[], [warning] message, batch.callback) in org.apache.hadoop.hbase.client.htable [warning] d:\\d\\hbasecommunity\\hbasetrunktest\\hbase-client\\src\\main\\java\\org\\apache\\hadoop\\hbase\\client\\regioncoprocessorserviceexec.java:43: warning - tag @link: can't find batchcoprocessorservice(methoddescriptor, message, byte[], byte[], message) in org.apache.hadoop.hbase.client.htable ",
        "label": 38
    },
    {
        "text": "npe on failed open of region  from bryan duxbury supplied log:    1044 2007-12-15 04:37:56,052 info org.apache.hadoop.hbase.hregionserver: msg_region_open : spider_pages,7_202623541,1197662034823    1045 2007-12-15 04:37:56,060 error org.apache.hadoop.hbase.hregionserver: error opening region spider_pages,7_202623541,1197662034823    1046 java.io.eofexception    1047     at java.io.datainputstream.readbyte(datainputstream.java:250)    1048     at org.apache.hadoop.hbase.hstorefile.loadinfo(hstorefile.java:594)    1049     at org.apache.hadoop.hbase.hstore.<init>(hstore.java:613)    1050     at org.apache.hadoop.hbase.hregion.<init>(hregion.java:287)    1051     at org.apache.hadoop.hbase.hregionserver.openregion(hregionserver.java:1182)    1052     at org.apache.hadoop.hbase.hregionserver$worker.run(hregionserver.java:1133)    1053     at java.lang.thread.run(thread.java:619)    1054 2007-12-15 04:37:56,061 fatal org.apache.hadoop.hbase.hregionserver: unhandled exception                                                                                                                                                                                                                               1055 java.lang.nullpointerexception    1056     at org.apache.hadoop.hbase.hregionserver.reportclose(hregionserver.java:1066)    1057     at org.apache.hadoop.hbase.hregionserver.openregion(hregionserver.java:1188)    1058     at org.apache.hadoop.hbase.hregionserver$worker.run(hregionserver.java:1133)    1059     at java.lang.thread.run(thread.java:619)    1060 2007-12-15 04:37:56,061 info org.apache.hadoop.hbase.hregionserver: worker thread exiting i see same exception when we try to deploy same region on another server; the info file must be horked (seems like something we could recover from reading through looking for highest sequence number; would be expensive but alternative is lost region). ",
        "label": 241
    },
    {
        "text": "use the official versions of surefire   junit  we currently use private versions for surefire & junit since hbase-4763. this jira traks what we need to move to official versions. surefire 2.11 is just out, but, after some tests, it does not contain all what we need. junit. could be for junit 4.11. issue to monitor:  https://github.com/kentbeck/junit/issues/359: fixed in our version, no feedback for an integration on trunk surefire: could be for surefire 2.12. issues to monitor are:  329 (category support): fixed, we use the official implementation from the trunk  786 (@category with forkmode=always): fixed, we use the official implementation from the trunk  791 (incorrect elapsed time on test failure): fixed, we use the official implementation from the trunk  793 (incorrect time in the xml report): not fixed (reopen) on trunk, fixed on our version.  760 (does not take into account the test method): fixed in trunk, not fixed in our version  798 (print immediately the test class name): not fixed in trunk, not fixed in our version  799 (allow test parallelization when forkmode=always): not fixed in trunk, not fixed in our version  800 (redirecttestoutputtofile not taken into account): not yet fix on trunk, fixed on our version 800 & 793 are the more important to monitor, it's the only ones that are fixed in our version but not on trunk. ",
        "label": 21
    },
    {
        "text": "fix flakey testassignmentmanager testassignsockettimeout  the problem is that we may kill the rs which holds meta so the assertion of the number of procedures maybe incorrect, as we may schedule another trsp for assigning meta... ",
        "label": 149
    },
    {
        "text": "nullpointerexception thrown when stopping regionserver  the hbase cluster is a fresh start with one regionserver.  when we stop hbase, an unhandled nullpointerexception is throwed in the regionserver.  the regionserver's log is as follows: 2013-06-21 10:21:11,284 info [regionserver61020] regionserver.hregionserver: closing user regions  2013-06-21 10:21:14,288 debug [regionserver61020] regionserver.hregionserver: waiting on 1028785192  2013-06-21 10:21:14,290 fatal [regionserver61020] regionserver.hregionserver: aborting region server hostname_test,61020,1371781086817  : unhandled: null  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.hregionserver.tryregionserverreport(hregionserver.java:988)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:832)  at java.lang.thread.run(thread.java:662)  2013-06-21 10:21:14,292 fatal [regionserver61020] regionserver.hregionserver: regionserver abort: loaded coprocessors are: [org.apache  .hadoop.hbase.coprocessor.multirowmutationendpoint]  2013-06-21 10:21:14,293 info [regionserver61020] regionserver.hregionserver: stopped: unhandled: null  2013-06-21 10:21:14,293 info [regionserver61020] ipc.rpcserver: stopping server on 61020 it seems that after closing user regions, the rssstub is null. update:  we found that if setting hbase.client.ipc.pool.type to roundrobinpool(or other pool type) and hbase.client.ipc.pool.size to 10(possibly other values) in hbase-site.xml, the regionserver is continuously attempting connect to master. and if we stop hbase, the above nullpointerexception occurred. with hbase.client.ipc.pool.size set to 1, the cluster can be completely stopped. ",
        "label": 290
    },
    {
        "text": "blockcache always on even when disabled  comes from billy pearson up on list. ",
        "label": 285
    },
    {
        "text": "lots of dns queries from client  in running a ycsb workload, i managed to ddos a dns server since it seems to be flooding lots of dns requests. installing nscd on the client machines improved throughput by a factor of 6 and stopped killing the server. these are long-running clients, so it's not clear why we do so many lookups. ",
        "label": 314
    },
    {
        "text": "rolling restart sh have old value of  zookeeper znode unassiged  config causing infinite loop  while running rolling-restart.sh without arguments or with --master-only argument script throws error when trying to check did all regions are cleared from transition state. here is line causing error: if [ \"$zunassigned\" == \"null\" ]; then zunassigned=\"unassigned\"; fi  afaik default value of \"zookeeper.znode.unassiged\" is changed to \"region-in-transition\". so we should change this line to: if [ \"$zunassigned\" == \"null\" ]; then zunassigned=\"region-in-transition\"; fi  i will attach patch today. ",
        "label": 393
    },
    {
        "text": "added the possibility to load custom cost functions  hi, we wouls like to open the discussion about bringing the possibility to have regions deployed on heterogeneous deployment, i.e hbase cluster running different kind of hardware. why? cloud deployments means that we may not be able to have the same hardware throughout the years some tables may need special requirements such as ssd whereas others should be using hard-drives  in our usecase(single table, dedicated hbase and hadoop tuned for our usecase, good key distribution), the number of regions per rs was the real limit for us. our usecase we found out that in our usecase(single table, dedicated hbase and hadoop tuned for our usecase, good key distribution), the number of regions per rs was the real limit for us. over the years, due to historical reasons and also the need to benchmark new machines, we ended-up with differents groups of hardware: some servers can handle only 180 regions, whereas the biggest can handle more than 900. because of such a difference, we had to disable the loadbalancing to avoid the roundrobinassigmnent. we developed some internal tooling which are responsible for load balancing regions across regionservers. that was 1.5 year ago. our proof-of-concept we did work on a proof-of-concept here, and some early tests here, here, and here. we wrote the balancer for our use-case, which means that: there is one table there is no region-replica good key dispersion there is no regions on master a rule file is loaded before balancing. it contains lines of rules. a rule is composed of a regexp for hostname, and a limit. for example, we could have:   rs[0-9] 200 rs1[0-9] 50   regionservers with hostname matching the first rules will have a limit of 200, and the others 50. if there's no match, a default is set. thanks to the rule, we have two informations: the max number of regions for this cluster, and the rules for each servers. heterogeneousbalancer will try to balance regions according to their capacity. let's take an example. let's say that we have 20 rs: 10 rs, named through rs0 to rs9 loaded with 60 regions each, and each can handle 200 regions. 10 rs, named through rs10 to rs19 loaded with 60 regions each, and each can support 50 regions. based on the following rules:   rs[0-9] 200 rs1[0-9] 50   the second group is overloaded, whereas the first group has plenty of space. we know that we can handle at maximum 2500 regions (200*10 + 50*10) and we have currently 1200 regions (60*20). heterogeneousbalancer will understand that the cluster is full at 48.0% (1200/2500). based on this information, we will then try to put all the regionservers to ~48% of load according to the rules. in this case, it will move regions from the second group to the first. the balancer will: compute how many regions needs to be moved. in our example, by moving 36 regions on rs10, we could go from 120.0% to 46.0% select regions with lowest data-locality try to find an appropriate rs for the region. we will take the lowest available rs. other implementations and ideas clay baenziger proposed this idea on the dev ml: could it work to have the stochastic load balancer use pluggable cost functions instead of this static list of cost functions? then, could this type of a load balancer be implemented simply as a new cost function which folks could choose to load and mix with the others? i think this could be an interesting way to include user-functions in the mix. as you know your hardawre and the pattern access, you can easily know which metrics is important for balancing, for us, it will only be the number of regions, but we could mix-it with the incoming writes!   bhupendra.jain proposed also the ideas of \"labels\"   internally, we are also having discussion to develop similar solution. in our approach, we were also thinking of adding \"rs label\" feature similar to hadoop node label feature.  each rs can have a label to denote its capabilities / resources . when user create table, there can be extra attributes with its descriptor. the balancer can decide to host region of table based on rs label and these attributes further.    with rs label feature, balancer can be more intelligent.  example tables with high read load needs more cache backed by ssds , so such table regions should be hosted on rs having ssds ...  i love the idea, but i think clay's idea is better for a better and faster first set of commits on the subject! what do you think? ",
        "label": 354
    },
    {
        "text": "enable compression in hbase export  org.apache.hadoop.hbase.mapreduce.export should set compression codec in createsubmittablejob(), the following should be added:  fileoutputformat.setcompressoutput(job, true);  fileoutputformat.setoutputcompressorclass(job, org.apache.hadoop.io.compress.gzipcodec.class); from my experiment, 10% to 50% reduction in export output has been observed. sequencefileinputformat used by the import tool is able to detect gzipcodec - there is no change for import class. ",
        "label": 263
    },
    {
        "text": "npe in rsrpcservices get on trunk  you seen this one jimmy xiang? here is the exception: 2014-03-27 11:38:16,649 error [rpcserver.handler=5,port=16020] ipc.rpcserver: unexpected throwable object java.lang.nullpointerexception         at org.apache.hadoop.hbase.regionserver.rsrpcservices.get(rsrpcservices.java:1577)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:29493)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2002)         at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:98)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:162)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:38)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler$1.run(simplerpcscheduler.java:110)         at java.lang.thread.run(thread.java:744) code looks like this on trunk: 1576     } finally { 1577       regionserver.metricsregionserver.updateget( 1578         environmentedgemanager.currenttimemillis() - before); 1579     } 1580   } ",
        "label": 314
    },
    {
        "text": "testmastershutdown failing in trunk trunk    unable to get data of znode  hbase meta region server because node does not exist  not an error   i'm looking at this too... jeffrey zhong you are too? ",
        "label": 233
    },
    {
        "text": "using hadoop profile property leads to confusing failures  when building master branch against hadoop 3 beta1, mvn clean install -dhadoop-profile=3.0 -dhadoop-three.version=3.0.0-beta1 -dhadoop-two.version=3.0.0-beta1 -dskiptests i got: [warning] rule 0: org.apache.maven.plugins.enforcer.banneddependencies failed with message: we don't allow the jsr305 jar from the findbugs project, see hbase-16321. found banned dependency: com.google.code.findbugs:jsr305:jar:1.3.9 here is part of the dependency tree showing the dependency: [info] org.apache.hbase:hbase-client:jar:3.0.0-snapshot ... [info] +- org.apache.hadoop:hadoop-auth:jar:3.0.0-beta1:compile ... [info] |  \\- com.google.guava:guava:jar:11.0.2:compile [info] |     \\- com.google.code.findbugs:jsr305:jar:1.3.9:compile we need to exclude jsr305 so that build succeed. ",
        "label": 320
    },
    {
        "text": "provide a means for coprocessors to create tables for internal use  when developing access controls as a coprocessor, we needed to create an internal \"acl\" table for persisting the permission grants. we could have done this in the coprocessor through hbaseadmin, but it seems silly to go through an rpc loop when we're already implementing a masterobserver running on hmaster. so the simplest approach was to expose createtable() in masterservices:   /**    * create a table using the given table definition.    * @param desc the table definition    * @param splitkeys starting row keys for the initial table regions.  if null    *     a single region is created.    * @param sync if true, waits for all initial regions to be assigned before    *     returning    */   public void createtable(htabledescriptor desc, byte [][] splitkeys)       throws ioexception; other coprocessor implementations will likely have similar needs, so i propose we add this to masterservices. the alternative would be to expose some sort of hbaseadmin like interface via mastercoprocessorenvironment, similar to what we do for htable, but this would be a fair bit more work, and i still think we'll need a way to provide this capability in the short term. ",
        "label": 180
    },
    {
        "text": "resultscanner allowing partial result will miss the rest of the row if the region is moved between two rpc requests  hbase-11544 allow scan rpc return partial of a row to reduce memory usage for one rpc request. and client can setallowpartial or setbatch to get several cells in a row instead of the whole row. however, the status of the scanner is saved on server and we need this to get the next part if there is a partial result before. if we move the region to another rs, client will get a notservingregionexception and open a new scanner to the new rs which will be regarded as a new scan from the end of this row. so the rest cells of the row of last result will be missing. ",
        "label": 353
    },
    {
        "text": "cells cannot be overwritten with bulk loaded hfiles  let's say you have a pre-built hfile that contains a cell: ('rowkey1', 'family1', 'qual1', 1234l, 'value1') we bulk load this first hfile. now, let's create a second hfile that contains a cell that overwrites the first: ('rowkey1', 'family1', 'qual1', 1234l, 'value2') that gets bulk loaded into the table, but the value that hbase bubbles up is still 'value1'. it seems that there's no way to overwrite a cell for a particular timestamp without an explicit put operation. this seems to be the case even after minor and major compactions happen. my guess is that this is pretty closely related to the sequence number work being done on the compaction algorithm via hbase-7842, but i'm not sure if one of would fix the other. ",
        "label": 230
    },
    {
        "text": "handle null regions to flush in hlog cleanoldlogs  note from kannan findmemstoreswitheditsequalorolderthan() can return null it seems like. and we don't check null, before \"region.length\".  regions = findmemstoreswitheditsequalorolderthan(this.outputfiles.firstkey(),  this.lastseqwritten);  stringbuilder sb = new stringbuilder();  for (int i = 0; i < regions.length; i++) { === stack trace 2010-11-15 19:19:54,258 debug org.apache.hadoop.hbase.io.hfile.lrublockcache: lru stats: total=6.1 gb, free=1.71 gb, max=7.81 gb, blocks=385740, accesses=7020255, hits=6329399, hitratio=90.15%%, cachingaccesses=6765050, cachinghits=6329399, cachinghitsratio=93.56%%, evictions=1, evicted=49911, evictedperrun=49911.0  2010-11-15 19:21:05,204 info org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter: using syncfs \u2013 hdfs-200  2010-11-15 19:21:05,211 info org.apache.hadoop.hbase.regionserver.wal.hlog: roll /pumahbase001-snc5-hbase/.logs/pumahbase042.snc5.facebook.com,60020,1289856892583/10.38.28.57%3a60020.1289877154987, entries=649004, filesize=255069060. new hlog /pumahbase001-snc5-hbase/.logs/pumahbase042.snc5.facebook.com,60020,1289856892583/10.38.28.57%3a60020.1289877665062  2010-11-15 19:21:05,222 error org.apache.hadoop.hbase.regionserver.logroller: log rolling failed  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.wal.hlog.cleanoldlogs(hlog.java:648)  at org.apache.hadoop.hbase.regionserver.wal.hlog.rollwriter(hlog.java:528)  at org.apache.hadoop.hbase.regionserver.logroller.run(logroller.java:94)  2010-11-15 19:21:05,226 fatal org.apache.hadoop.hbase.regionserver.hregionserver: aborting region server servername=pumahbase042.snc5.facebook.com,60020,1289856892583, load=(requests=3476, regions=40, usedheap=8388, maxheap=15987): log rolling failed  java.lang.nullpointerexception  at org.apache.hadoop.hbase.regionserver.wal.hlog.cleanoldlogs(hlog.java:648)  at org.apache.hadoop.hbase.regionserver.wal.hlog.rollwriter(hlog.java:528)  at org.apache.hadoop.hbase.regionserver.logroller.run(logroller.java:94)  2010-11-15 19:21:05,227 info org.apache.hadoop.hbase.regionserver.hregionserver: dump of metrics: request=1264.5834, regions=40, stores=70, storefiles=98, storefileindexsize=35, memstoresize=83, compactionqueuesize=0, usedheap=8370, maxheap=15987, blockcachesize=6593768536, blockcachefree=1788154792, blockcachecount=388283, blockcachehitratio=90, blockcachehitcachingratio=93  2010-11-15 19:21:05,227 info org.apache.hadoop.hbase.regionserver.hregionserver: stopped: log rolling failed  2010-11-15 19:21:05,227 info org.apache.hadoop.hbase.regionserver.logroller: logroller exiting.  2010-11-15 19:21:07,255 info org.apache.hadoop.ipc.hbaseserver: stopping server on 60020 === ",
        "label": 263
    },
    {
        "text": "regionserver group based assignment  in multi-tenant deployments of hbase, it is likely that a regionserver will be serving out regions from a number of different tables owned by various client applications. being able to group a subset of running regionservers and assign specific tables to it, provides a client application a level of isolation and resource allocation. the proposal essentially is to have an assignmentmanager which is aware of regionserver groups and assigns tables to region servers based on groupings. load balancing will occur on a per group basis as well. this is essentially a simplification of the approach taken in hbase-4120. see attached document. ",
        "label": 174
    },
    {
        "text": "rolling restart sh script unable to check expiration of master znode  when rolling-restart.sh script stop master it enters loop trying to detect that master znode is deleted. since it is unable to execute check command script hangs in infinite loop. problematic line of script is: while ! bin/hbase zkcli stat $zmaster 2>&1 | grep \"node does not exist\"; do \"bin/hbase zkcli stat\" can not be executed since script is run from bin directory. my suggestion is that this line should be like this in order to work: while ! \"$bin\"/hbase zkcli stat $zmaster 2>&1 | grep \"node does not exist\"; do after i made this change i was able to execute rolling restart. ",
        "label": 393
    },
    {
        "text": "hregion incrementcolumnvalue  doesn't have a consistent behavior when the field that we are incrementing is less than bytes long  we wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. later, we increment the counter using htable.incrementcolumnvalue(). this call results in one of two outcomes. 1. the call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read).  2. throws ioexception/illegalargumentexception.  java.io.ioexception: java.io.ioexception: java.lang.illegalargumentexception: offset (65547) + length (8) exceed the capacity of the array: 65551  at org.apache.hadoop.hbase.util.bytes.explainwronglengthoroffset(bytes.java:502)  at org.apache.hadoop.hbase.util.bytes.tolong(bytes.java:480)  at org.apache.hadoop.hbase.regionserver.hregion.incrementcolumnvalue(hregion.java:3139)  at org.apache.hadoop.hbase.regionserver.hregionserver.incrementcolumnvalue(hregionserver.java:2468)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1039) based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), i would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. however, the exception is thrown only rarely and i am not sure what determines the case to be executed. i am wondering if this has something to do with flush. here is a hregion unit test that can reproduce this problem. http://paste.lisp.org/display/122822 we modified our code to initialize the counter with a 64 bit value. but, i was also wondering if something has to change in hregion.incrementcolumnvalue() to handle inconsistent counter sizes gracefully without corrupting existing data. please let me know if you need additional information. ",
        "label": 289
    },
    {
        "text": "hdfs dfsclient  dfs read  java io ioexception  could not obtain block  after restarting datanode in hdfs  i am stoping datanode.  and get a lot of exception. then i am starting datanode back.  master is working again but region server returns exceptions: info ipc.hbaseserver: ipc server handler 9 on 60020, call openscanner([b@e3404f, [[b@1930611, [b@1735f84, 9223372036854775807, null) from 127.0.0.1:55743: error: java.io.ioexception: hstorescanner failed construction java.io.ioexception: hstorescanner failed construction at org.apache.hadoop.hbase.regionserver.storefilescanner.<init>(storefilescanner.java:73) at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:87) at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:1769) at org.apache.hadoop.hbase.regionserver.hregion$hscanner.<init>(hregion.java:1971) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1161) at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:1720) at sun.reflect.generatedmethodaccessor3.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:912) caused by: java.io.ioexception: could not obtain block: blk_-5468922917120193795_1013 file=/hbase/-root-/70236052/info/2088312954622080873 at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.choosedatanode(dfsclient.java:1708) at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.blockseekto(dfsclient.java:1536) at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.read(dfsclient.java:1663) at java.io.datainputstream.read(datainputstream.java:132) at org.apache.hadoop.hbase.io.hfile.boundedrangefileinputstream.read(boundedrangefileinputstream.java:99) at org.apache.hadoop.io.ioutils.readfully(ioutils.java:100) at org.apache.hadoop.hbase.io.hfile.hfile$reader.decompress(hfile.java:910) at org.apache.hadoop.hbase.io.hfile.hfile$reader.readblock(hfile.java:863) at org.apache.hadoop.hbase.io.hfile.hfile$reader$scanner.seekto(hfile.java:1151) at org.apache.hadoop.hbase.regionserver.storefilescanner.getnext(storefilescanner.java:301) at org.apache.hadoop.hbase.regionserver.storefilescanner.openscanner(storefilescanner.java:102) at org.apache.hadoop.hbase.regionserver.storefilescanner.<init>(storefilescanner.java:70) ... 10 more this one is returned in a loop and there is second one: warn hdfs.dfsclient: dfs read: java.io.ioexception: could not obtain block: blk_-5468922917120193795_1013 file=/hbase/-root-/70236052/info/2088312954622080873 at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.choosedatanode(dfsclient.java:1708) at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.blockseekto(dfsclient.java:1536) at org.apache.hadoop.hdfs.dfsclient$dfsinputstream.read(dfsclient.java:1663) at java.io.datainputstream.read(datainputstream.java:132) at org.apache.hadoop.hbase.io.hfile.boundedrangefileinputstream.read(boundedrangefileinputstream.java:99) at org.apache.hadoop.io.ioutils.readfully(ioutils.java:100) at org.apache.hadoop.hbase.io.hfile.hfile$reader.decompress(hfile.java:910) at org.apache.hadoop.hbase.io.hfile.hfile$reader.readblock(hfile.java:863) at org.apache.hadoop.hbase.io.hfile.hfile$reader$scanner.seekto(hfile.java:1151) at org.apache.hadoop.hbase.regionserver.storefilescanner.getnext(storefilescanner.java:301) at org.apache.hadoop.hbase.regionserver.storefilescanner.openscanner(storefilescanner.java:102) at org.apache.hadoop.hbase.regionserver.storefilescanner.<init>(storefilescanner.java:70) at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:87) at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:1769) at org.apache.hadoop.hbase.regionserver.hregion$hscanner.<init>(hregion.java:1971) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1161) at org.apache.hadoop.hbase.regionserver.hregionserver.openscanner(hregionserver.java:1720) at sun.reflect.generatedmethodaccessor3.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:912) ",
        "label": 167
    },
    {
        "text": "hbase broke shell's status 'simple' and 'detailed'  this is due to the jruby code using the now removed hserverinfo. also getservers() is now getserverssize() etc. ",
        "label": 285
    },
    {
        "text": "fix coverage org apache hadoop hbase rest client  fix coverage org.apache.hadoop.hbase.rest.client ",
        "label": 40
    },
    {
        "text": "securebulkload dispatches file load requests to all regions  when running a bulk load on a secure environment and loading data into the first region of a table, the request to load the hfile set is dispatched to all regions for the table. this is reproduced consistently by running integrationtestbulkload on a secure cluster. the load fails with an exception that looks like: 2013-08-30 07:37:22,993 info  [main] mapreduce.loadincrementalhfiles: split occured while grouping hfiles, retry attempt 1 with 3 files remaining to group or split 2013-08-30 07:37:22,999 error [main] mapreduce.loadincrementalhfiles: ioexception during splitting java.util.concurrent.executionexception: java.io.filenotfoundexception: file does not exist: /user/hbase/test-data/c45ddfe9-ee30-4d32-8042-928db12b1cee/integrationtestbulkload-0/l/bf41ea13997b4e228d05e67ba7b1b686 at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:61) at org.apache.hadoop.hdfs.server.namenode.inodefile.valueof(inodefile.java:51) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsupdatetimes(fsnamesystem.java:1489) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsint(fsnamesystem.java:1438) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1418) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1392) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.getblocklocations(namenoderpcserver.java:438) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.getblocklocations(clientnamenodeprotocolserversidetranslatorpb.java:269) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java:59566) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:585) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:928) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2048) at org.apache.hadoop.ipc.server$handler$1.run(server.java:2044) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:396) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1477) at org.apache.hadoop.ipc.server$handler.run(server.java:2042) at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222) at java.util.concurrent.futuretask.get(futuretask.java:83) at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles.grouporsplitphase(loadincrementalhfiles.java:403) at org.apache.hadoop.hbase.mapreduce.loadincrementalhfiles.dobulkload(loadincrementalhfiles.java:284) at org.apache.hadoop.hbase.mapreduce.integrationtestbulkload.runlinkedlistmrjob(integrationtestbulkload.java:200) at org.apache.hadoop.hbase.mapreduce.integrationtestbulkload.testbulkload(integrationtestbulkload.java:133) ",
        "label": 339
    },
    {
        "text": "redo procedureinfo and lockinfo  procedureinfo was introduced as a lowest-common-denominator pojo that could be used as a facade on pb procedures. it was good for showing state of procedure framework in shell and ui. its a bit weird though. its up in hbase-common rather than in procedure and it can only ever show a subset of the procedure info. i was thinking we could use the pb3.1 pb->json utility instead and emit a json string wherever we need to export a view on procedure internals. this issue is about exploring this possibility. would depend on our having an upgraded guava (so probably depends on the 'pre-build' project). from procedureinfo and lockinfo need fixing in https://docs.google.com/document/d/1evka7fhdeoj1-9o8yzcotaqbv0u0bblblcczvsin69g/edit#heading=h.kid1jzo114xw ",
        "label": 60
    },
    {
        "text": "report region in transition only ever operates on one region  report region in transition takes a list of regions but it only ever operates on one region however more than one region can be reported. seems like this could cause some serious weirdness on failure cases. ",
        "label": 151
    },
    {
        "text": "hlog compression  the current bottleneck to hbase write speed is replicating the wal appends across different datanodes. we can speed up this process by compressing the hlog. current plan involves using a dictionary to compress table name, region id, cf name, and possibly other bits of repeated data. also, hlog format may be changed in other ways to produce a smaller hlog. ",
        "label": 289
    },
    {
        "text": "avoid empty columns while doing bulk load  in bulk load feature of hbase (importtsv and completebulkload), it just stores everything in the table but some times we want to avoid empty column values to save some disk space. so, we can have a feature to skip those empty columns values while bulk load (if user wants). ",
        "label": 52
    },
    {
        "text": "zk restarted while a region is being assigned  new active hm re assigns it but the rs warns 'already online on this server'   zk restarted while assigning a region, new active hm re-assign it but the rs warned 'already online on this server'. issue:  the rs failed besause of 'already online on this server' and return; the hm can not receive the message and report 'regions in transition timed out'. ",
        "label": 529
    },
    {
        "text": "major compaction may not be triggered  even though region server log says it is triggered  the trunk version of regionserver/store.java, method list<storefile> compactselection(list<storefile> candidates) has this code to determine whether major compaction should be done or not:  // major compact on user action or age (caveat: we have too many files)  boolean majorcompaction = (forcemajor || ismajorcompaction(filestocompact))  && filestocompact.size() < this.maxfilestocompact; the ismajorcompaction(filestocompact) method internally determines whether or not major compaction is required (and logs this as \"major compaction triggered ... \" log message. however, after the call, the compactselection method subsequently applies the filestocompact.size() < this.maxfilestocompact check which can turn off major compaction. this would result in a \"major compaction triggered\" log message without actually triggering a major compaction. the filestocompact.size() check should probably be moved inside the ismajorcompaction(filestocompact) method. ",
        "label": 441
    },
    {
        "text": "remove cancel command from backup code  after recent refactoring of backup code, cancel command is no longer applicable since there is no server procedure performing backup / restore. this issue is to remove cancel command from backupcommands.java ",
        "label": 352
    },
    {
        "text": "callqueuesize should be decremented in a fail fast scenario  discussed on the user@hbase mailing list (http://markmail.org/thread/w3cqjxwo2smkn2jw). if a client disconnects the call queue size is not decremented causing new calls to get rejected with a callqueuetoobigexception. ",
        "label": 163
    },
    {
        "text": "fix units in rs ui metrics  currently the metrics are a mix of mb and bytes. its confusing. ",
        "label": 550
    },
    {
        "text": "do not collect deleted kvs when they are still in use by a scanner   i noticed this because testatomicoperation.testmultirowmutationmultithreads fails rarely.  the solution is similar to hbase-2856, where expired kvs are not collected when in use by a scanner. \u2014  what i pieced together so far is that it is the scanning side that has problems sometimes. every time i see a assertion failure in the log i see this before: 2012-03-12 21:48:49,523 debug [thread-211] regionserver.storescanner(499): storescanner.peek() is changed where before = rowb/colfamily11:qual1/75366/put/vlen=6,and after = rowb/colfamily11:qual1/75203/deletecolumn/vlen=0 the order of if the put and delete is sometimes reversed. the test threads should always see exactly one kv, if the \"before\" was the put the thread see 0 kvs, if the \"before\" was the delete the threads see 2 kvs. this debug message comes from storescanner to checkreseek. it seems we still some consistency issue with scanning sometimes ",
        "label": 286
    },
    {
        "text": "htable is not closed in aggregationclient  in aggregationclient, htable instance is not closed. ",
        "label": 292
    },
    {
        "text": "missing break in next row case of filterlist mergereturncodefororoperator   here is related code, around line 569:       if (isinreturncodes(rc, returncode.next_row)) {         return returncode.next_row;       }     case seek_next_using_hint: break was missing for the next_row case. ",
        "label": 72
    },
    {
        "text": "upon page refresh new ui should return to the previously selected tab  ",
        "label": 239
    },
    {
        "text": "htable getregionsinrange  should provide a non cached api  getregionsinrange() calls getregionlocation() without reloading it. it will return wrong result if the cache is outdated due to region split. if the cost of always reloading isn't significant, we should consider doing that by default. otherwise, let's have an api for getregionsinrange() that forces a reload. ",
        "label": 309
    },
    {
        "text": "regexstringcomparator supports java util regex pattern flags  add constructor that takes in a pattern add pattern's flags to writable fields, and actually use them when recomposing the filter ",
        "label": 127
    },
    {
        "text": "add client side hedged read metrics  need some metrics to represent indicate read high-availability.  +hedgedreadops \u2013 the number of hedged read that have occurred.  +hedgedreadwin \u2013 the number of hedged read returned faster than the original read. ",
        "label": 507
    },
    {
        "text": "upgrade hadoop dependencies  0.92 branch currently depends on hadoop 1.0.0, but this has been moved to the archive. the earliest release on www.apache.org/dist/ is 1.0.1. consider moving up? ",
        "label": 314
    },
    {
        "text": "update integrationtestingestwithmob and actions to use columnfamily builders for modification  when attempting to run itiwm against a cluster i get the following error: java.lang.unsupportedoperationexception: hcolumndescriptor is read-only at org.apache.hadoop.hbase.client.immutablehcolumndescriptor.getdelegateeformodification(immutablehcolumndescriptor.java:44) at org.apache.hadoop.hbase.hcolumndescriptor.setmobenabled(hcolumndescriptor.java:735) at org.apache.hadoop.hbase.integrationtestingestwithmob.inittable(integrationtestingestwithmob.java:122) at org.apache.hadoop.hbase.integrationtestingest.setupcluster(integrationtestingest.java:92) at org.apache.hadoop.hbase.integrationtestbase.setup(integrationtestbase.java:148) at org.apache.hadoop.hbase.integrationtestbase.dowork(integrationtestbase.java:131) at org.apache.hadoop.hbase.util.abstracthbasetool.run(abstracthbasetool.java:154) at org.apache.hadoop.util.toolrunner.run(toolrunner.java:76) at org.apache.hadoop.hbase.integrationtestingestwithmob.main(integrationtestingestwithmob.java:153) ",
        "label": 320
    },
    {
        "text": "region could get lost during assignment  i observed test timeout running against hadoop 2.1.0 with distributed log replay turned on.  looks like region state for 1588230740 became inconsistent between master and the surviving region server: 2013-08-29 22:15:34,180 info  [am.zk.worker-pool2-t4] master.regionstates(299): onlined 1588230740 on kiyo.gq1.ygridcore.net,57016,1377814510039 ... 2013-08-29 22:15:34,587 debug [thread-221] client.hconnectionmanager$hconnectionimplementation(1269): locateregioninmeta parenttable=hbase:meta, metalocation={region=hbase:meta,,1.1588230740, hostname=kiyo.gq1.ygridcore.net,57016,1377814510039, seqnum=0}, attempt=2 of 35 failed; retrying after sleep of 302 because: org.apache.hadoop.hbase.exceptions.regionopeningexception: region is being opened: 1588230740         at org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname(hregionserver.java:2574)         at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:3949)         at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:2733)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26965)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2063)         at org.apache.hadoop.hbase.ipc.rpcserver$callrunner.run(rpcserver.java:1800)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.consumerloop(simplerpcscheduler.java:165)         at org.apache.hadoop.hbase.ipc.simplerpcscheduler.access$000(simplerpcscheduler.java:41) ",
        "label": 441
    },
    {
        "text": "problem with row keys beginnig with characters   than ' ' and the region location cache  we currently have a problem the way we design .meta. row keys. when user table row keys begin with characters lesser than ',' like a '$', any operation will fail when: a client has a certain set of regions in cache one region with the faulty row key splits the client receives a request for a row in the split region the reason is that it will first get a nsre then it will try to locate a region using the passed row key. for example:   row in meta: entities,,1216750777411  row passed: entities,$-94f9386f-e235-4cbd-aacc-37210a870991,99999999999999 the passed row is lesser then the row in .meta. ",
        "label": 229
    },
    {
        "text": "fstabledescriptors should handle random folders in hbase root dir better  i faked an upgrade on a test cluster using our dev data so i had to distcp the data between the two clusters, but after starting up and doing the migration and whatnot the web ui didn't show any table. the reason was in the master's log: org.apache.hadoop.hbase.tableexistsexception: no descriptor for _distcp_logs_e0ehek  at org.apache.hadoop.hbase.util.fstabledescriptors.get(fstabledescriptors.java:164)  at org.apache.hadoop.hbase.util.fstabledescriptors.getall(fstabledescriptors.java:182)  at org.apache.hadoop.hbase.master.hmaster.gethtabledescriptors(hmaster.java:1554)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1326) i don't think we need to show a full stack (just a warn maybe), this shouldn't kill the request (still see tables in the web ui), and why is that a tableexistsexception? ",
        "label": 229
    },
    {
        "text": "deprecate hconnection getzookeeperwatcher in prep for hbase  need a +1 on this from someone else who agrees hbase-1762 should be done. makes sense to me. will take a little bit of work doing the actual removal over hbase-1762 but first step is this deprecating step. ",
        "label": 314
    },
    {
        "text": " hbase  when hbase needs to be migrated  it should display a message on stdout  not just in the logs  when you upgrade your hbase code version, there is occasionally the need to migrate the underlying data store to a new version. however, if you are unaware of this need, then you'll be very confused by what happens when you restart hbase. using start-hbase.sh, you get messages indicating that the master and regionservers started as expected. however, in reality, it will have tried to start and failed due to a version mismatch. this information is displayed in the logs, but you won't know that until you go log diving. instead, let's have the start-hbase.sh script do a check to see if the version number is wrong itself. that way, if it fails, it can write messages about startup failure to the console instead of to the logs. this will make new admins much happier. ",
        "label": 241
    },
    {
        "text": "remove testfromclientside testgetstartendkeyswithregionreplicas  it tests nothing after hbase-21753... ",
        "label": 149
    },
    {
        "text": "performanceevaluation is in hbase server  and create a dependency to minidfscluster  it's the only dependency that is not in the tests package. i'm not clear on how to fix it. any idea? ",
        "label": 340
    },
    {
        "text": "improper usage of map putifabsent  when using map#putifabsent, the argument should not be a new object. otherwise, if the item is present, the object that was instantiated is immediately thrown away. instead, use map#computeifabsent so that the object is only instantiated if it is needed. there exists a good example in the map javadoc: https://docs.oracle.com/javase/8/docs/api/java/util/map.html#computeifabsent-k-java.util.function.function- locations https://github.com/apache/hbase/blob/9370347efea5b09e2fa8f4e5d82fa32491e1181b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/quotacache.java#l227-l236 https://github.com/apache/hbase/blob/025ddce868eb06b4072b5152c5ffae5a01e7ae30/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/storehotnessprotector.java#l124-l129 https://github.com/apache/hbase/blob/1170f28122d9d36e511ba504a5263ec62e11ef6a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/regionstates.java#l555 https://github.com/apache/hbase/blob/4ca760fe9dd373b8d8a4c48db15e42424920653c/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/rsgroupadminserver.java#l584-l586 https://github.com/apache/hbase/blob/4ca760fe9dd373b8d8a4c48db15e42424920653c/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/rsgroupadminserver.java#l585 https://github.com/apache/hbase/blob/5b01e613fbbb92e243e99a1d199b4ffbb21ed2d9/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/procedureexecutor.java#l834 ",
        "label": 130
    },
    {
        "text": "tablesplit getlength returns  inputsplits should be sorted by length but tablesplit does not contain real getlength implementation:  @override  public long getlength() { // not clear how to obtain this... seems to be used only for sorting splits return 0; } this is causing us problem with scheduling - we have got jobs that are supposed to finish in limited time but they get often stuck in last mapper working on large region. can we implement this method ?   what is the best way ? we were thinking about estimating size by size of files on hdfs.  we would like to get scanner from tablesplit, use startrow, stoprow and column families to get corresponding region than computing size of hdfs for given region and column family. update:  this ticket was about production issue - i talked with guy who worked on this and he said our production issue was probably not directly caused by getlength() returning 0. ",
        "label": 297
    },
    {
        "text": " hbase spark  enhance dataframe filters to handle naively encoded short  integer  long  float and double  currently, the range filter is based on the order of bytes. but for java primitive type, such as short, int, long, double, float, etc, their order is not consistent with their byte order, extra manipulation has to be in place to take care of them correctly. for example, for the integer range (-100, 100), the filter <= 1, the current filter will return 0 and 1, and the right return value should be (-100, 1] ",
        "label": 512
    },
    {
        "text": "testsplitwaldataloss fails on all branches  with some regularity i am seeing: org.apache.hadoop.hbase.client.retriesexhaustedwithdetailsexception: failed 1 action: testsplitwaldataloss:dataloss: 1 time,  at org.apache.hadoop.hbase.client.asyncprocess$batcherrors.makeexception(asyncprocess.java:228) at org.apache.hadoop.hbase.client.asyncprocess$batcherrors.access$1800(asyncprocess.java:208) at org.apache.hadoop.hbase.client.asyncprocess.waitforallpreviousopsandreset(asyncprocess.java:1712) at org.apache.hadoop.hbase.client.bufferedmutatorimpl.backgroundflushcommits(bufferedmutatorimpl.java:240) at org.apache.hadoop.hbase.client.bufferedmutatorimpl.flush(bufferedmutatorimpl.java:190) at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1430) at org.apache.hadoop.hbase.client.htable.put(htable.java:1021) at org.apache.hadoop.hbase.regionserver.testsplitwaldataloss.test(testsplitwaldataloss.java:121) ",
        "label": 314
    },
    {
        "text": "call to equals  comparing different types in hregionserver handlereportfordutyresponse   here is related code:           if (!this.servernamefrommasterpov.equals(this.isa.gethostname())) { servernamefrommasterpov is of type servername but this.isa.gethostname() returns string ",
        "label": 441
    },
    {
        "text": "deal with hbck tests disabled by hbase amv2 when hbck works again   disabled by hbase-14614, enabling amv2. see hbase-18110. here is the list: testhbasefscktwors testofflinemetarebuildbase testhbasefsckreplicas testofflinemetarebuildoverlap testhbasefsckoners testofflinemetarebuildhole ",
        "label": 314
    },
    {
        "text": "implement async admin operations for normalizer cleanerchore catalogjanitor  ",
        "label": 187
    },
    {
        "text": "testregionrebalance is flakey  besides failing many times on the prebuild testregionrebalance fails on my local machine eventually simply with  export runnin=true; mvn clean install -dskiptests ; while ($runnin) ; do mvn test -dtest=testregionrebalancing || runnin=false;done ",
        "label": 407
    },
    {
        "text": "add lz4 to test compression util usage string  add lz4 to the list of compression codecs on test compression util. ",
        "label": 154
    },
    {
        "text": " hbck2  add fix of overlaps to fixmeta hbck service  fixmeta currently does holes in meta only courtesy of hbase-22771 which added fixmeta to hbck service; missing was fix of overlaps too. this jira is about adding fix of overlaps to general fixmeta call. ",
        "label": 391
    },
    {
        "text": "don't create an unnecessary linkedlist when evicting from the blockcache  when evicting from the blockcache, the code creates a linkedlist containing every single block sorted by access time. this list is created from a priorityqueue. i don't believe it is necessary, as the priorityqueue can be used directly. ",
        "label": 441
    },
    {
        "text": " stargate   failed tests  warning junit framework testsuite  and debug output is dumped to console since move to mavenized build  two problems with running unit tests since stargate was moved to maven, possibly related: 1) debug output from tests is dumped to console. 2) \"failed tests: warning(junit.framework.testsuite$1)\" \u2013 what? results : failed tests:    warning(junit.framework.testsuite$1) tests run: 76, failures: 1, errors: 0, skipped: 0 someone who knows maven, can you take a look at this? ",
        "label": 38
    },
    {
        "text": "move table and column family attributes to zk to make them live  this is a first cut at moving table and column family attributes up to zk where they can be modified any time without requiring a disable/enable table cycle. adds methods to zookeeperwrapper for support server side. adds methods to htableinterface for client side access. client side talks directly to zk. the new hierarchy for attributes in zk is like: /hbase   /table     /<table-name>       /attr         /<table-attr-name>         /<family-name>           /<family-attr-name> as before attribute names and values are byte[], but now that they are hosted in zk, the path delimiter ('/') cannot be used. attached patch compiles but has not been tested. i put it up for a concept review. next step before this is functional is to deprecate the attribute related methods on htd and hcd and chase through the warnings to find all users who should be updated to use the zk wrapper or new htable methods instead. and, of course, then add test cases that successfully complete. ",
        "label": 38
    },
    {
        "text": "expose filtered read requests metric to metrics framework and web ui  filtered read requests metric for scan is added in hbase-5980. but it can be retrieved from scan object only. i think that it would be more informative when it is exposed to metrics framework. so i suggest to add filtered read requests metric and to expose it metrics framework and web ui. ps. i think i found a bug; read requests count increased when get operation returns no record by filtering out. should it be fixed? ",
        "label": 165
    },
    {
        "text": "port hbase and hbase to   create cluster id and version file in a tmp location and move it into place  see hbase-3270 and hbase-11650 ",
        "label": 286
    },
    {
        "text": "new backup export import mr for  erik has built a new backup tool that's compatible with newer versions of hbase. ",
        "label": 161
    },
    {
        "text": "api htable getmetadata addfamily shouldn't be exposed to user  it seems like htable.getmetadata().addfamily could be used to add a column family to an existing table, but actually it doesn't do anything (i'm guessing it just modifies the client data structure). htable.getmetadata() should probably return a read-only version of the htabledescriptor to prevent programmers from making this error. ",
        "label": 241
    },
    {
        "text": "avoid a wide line on the hmaster webui if we have many zookeeper servers  add a line break for every four zookeeper quorums on the hmaster webui. i don't think this need a test case. just manual testing is enough. i've tested on my testing cluster. everything works well. ",
        "label": 170
    },
    {
        "text": "align offline merge with online merge  after hbase-7403 we now have two different tools for online and offline merge, and the result produced by the two are different. (the online one works with snapshots, the offline not) we should remove the offline one, or align it to the online code.  most of the offline code in hregion.merge() can be replaced with the one in regionmergetransaction, used by the online version. ",
        "label": 107
    },
    {
        "text": "secure hbase cluster   client not able to call some admin apis  in case of secure cluster, we allow the hbase clients to read the zk nodes by providing the global read permissions to all for certain nodes. these nodes are the master address znode, root server znode and the clusterid znode. in zkutil.createacl() , we can see these node names are specially handled.  but there are some other client side admin apis which makes a read call into the zookeeper from the client. this include the istableenabled() call (may be some other. i have seen this). here the client directly reads a node in the zookeeper ( node created for this table ) and the data is matched to know whether this is enabled or not.  now in secure cluster case any client can read zookeeper nodes which it needs for its normal operation like the master address and root server address. but what if the client calls this api? [istableenaled () ]. ",
        "label": 309
    },
    {
        "text": "update security unit tests to use a kdc if available  we currently have large holes in the test coverage of hbase with security enabled. two recent examples of bugs which really should have been caught with testing are hbase-7771 and hbase-7772. the long standing problem with testing with security enabled has been the requirement for supporting kerberos infrastructure. we need to close this gap and provide some automated testing with security enabled, if necessary standing up and provisioning a temporary kdc as an option for running integration tests, see hadoop-8078 and hadoop-9004 where a similar approach was taken. ",
        "label": 149
    },
    {
        "text": "add a function a mark a server as dead and start the recovery the process  zookeeper is used a a monitoring tool: we use znode and we start the recovery process when a znode is deleted by zk because it got a timeout. this timeout is defaulted to 90 seconds, and often set to 30s however, some hw issues could be detected by specialized hw monitoring tools before the zk timeout. for this reason, it makes sense to offer a very simple function to mark a rs as dead. this should not take in it could be a hbase shell function such as  considerasdead ipaddress|servername this would delete all the znodes of the server running on this box, starting the recovery process. such a function would be easily callable (at callers risk) by any fault detection tool... we could have issues to identify the right master & region servers around ipv4 vs ipv6 vs and multi networked boxes however. ",
        "label": 437
    },
    {
        "text": "balancer   ssh interfering with each other leading to unavailability  came across this in our cluster:  1. the meta was assigned to a server 10.0.0.149,16020,1443507203340 2015-09-29 06:16:22,472 debug [am.zk.worker-pool2-t56]  master.regionstates: onlined 1588230740 on  10.0.0.149,16020,1443507203340 {encoded => 1588230740, name =>  'hbase:meta,,1', startkey => '', endkey => ''} 2. the server dies at some point: 2015-09-29 06:18:25,952 info  [main-eventthread]  zookeeper.regionservertracker: regionserver ephemeral node deleted,  processing expiration [10.0.0.149,16020,1443507203340] 2015-09-29 06:18:25,955 debug [main-eventthread] master.assignmentmanager: based on am, current  region=hbase:meta,,1.1588230740 is on server=10.0.0.149,16020,1443507203340 server being checked:  10.0.0.149,16020,1443507203340 3. the balancer had computed a plan that contained a move for the meta: 2015-09-29 06:18:26,833 info  [b.defaultrpcserver.handler=12,queue=0,port=16000] master.hmaster:  balance hri=hbase:meta,,1.1588230740,  src=10.0.0.149,16020,1443507203340, dest=10.0.0.205,16020,1443507257905 4. the following ensues after this, leading to the meta remaining unassigned: 2015-09-29 06:18:26,859 debug [b.defaultrpcserver.handler=12,queue=0,port=16000]  master.assignmentmanager: offline hbase:meta,,1.1588230740, no need to  unassign since it's on a dead server: 10.0.0.149,16020,1443507203340 ...................... 2015-09-29 06:18:26,899 info  [b.defaultrpcserver.handler=12,queue=0,port=16000] master.regionstates:  offlined 1588230740 from 10.0.0.149,16020,1443507203340 ..................... 2015-09-29 06:18:26,914 info  [b.defaultrpcserver.handler=12,queue=0,port=16000]  master.assignmentmanager: skip assigning hbase:meta,,1.1588230740, it is  on a dead but not processed yet server: 10.0.0.149,16020,1443507203340 .................... 2015-09-29 06:18:26,915 debug [am.zk.worker-pool2-t58] master.assignmentmanager: znode hbase:meta,,1.1588230740 deleted,  state: {1588230740 state=offline, ts=1443507506914,  server=10.0.0.149,16020,1443507203340} .................... 2015-09-29 06:18:29,447 debug [master_meta_server_operations-10.0.0.148:16000-2] master.assignmentmanager: based on am, current  region=hbase:meta,,1.1588230740 is on server=null server being checked:  10.0.0.149,16020,1443507203340 2015-09-29 06:18:29,451 info  [master_meta_server_operations- 10.0.0.148:16000-2] handler.metaservershutdownhandler: meta has been  assigned to otherwhere, skip assigning. 2015-09-29 06:18:29,452 debug [master_meta_server_operations-10.0.0.148:16000-2]  master.deadserver: finished processing 10.0.0.149,16020,1443507203340 ",
        "label": 426
    },
    {
        "text": "add integration test for importtsv loadincrementalhfiles workflow  we have existing unit tests for smoke-testing the packaged mr jobs, however they do not create a runtime environment that is true to running on a real mr cluster. this is particularly true in regard to classpaths (hbase-7934) but also other static state (hbase-4802). an integration test that can be pointed to run on a pseudo-distributed hadoop deployed on localhost would find these kinds of problems. ",
        "label": 339
    },
    {
        "text": "too many rit page numbers show confusion  too much rits can cause page rendering clutter, which can be resolved by adding pagers. ",
        "label": 525
    },
    {
        "text": "add metrics for regions in transition  the following metrics would be useful for monitoring the master: the number of regions in transition the number of regions in transition that have been in transition for more than a minute how many seconds has the oldest region-in-transition been in transition ",
        "label": 199
    },
    {
        "text": "open region failed cause memory leak  in some cases (for example, coprocessor path is wrong) region open failed, metricsregionwrapperimpl is already init and not close, cause memory leak; 2019-02-21 15:41:32,929 error [rs_open_region-hb-2zedsc3fxjn12dl6u-005:16020-7] regionserver.regioncoprocessorhost(362): failed to load coprocessor org.apache.kylin.storage.hbase.cube.v2.coprocessor.endpoint.cubevisitservice java.lang.illegalargumentexception: java.net.unknownhostexception: emr-cluster at org.apache.hadoop.security.securityutil.buildtokenservice(securityutil.java:378) at org.apache.hadoop.hdfs.namenodeproxies.createnonhaproxy(namenodeproxies.java:310) at org.apache.hadoop.hdfs.namenodeproxies.createproxy(namenodeproxies.java:176) at org.apache.hadoop.hdfs.dfsclient.<init>(dfsclient.java:678) at org.apache.hadoop.hdfs.dfsclient.<init>(dfsclient.java:619) at org.apache.hadoop.hdfs.distributedfilesystem.initialize(distributedfilesystem.java:149) at org.apache.hadoop.fs.filesystem.createfilesystem(filesystem.java:2653) at org.apache.hadoop.fs.filesystem.access$200(filesystem.java:92) at org.apache.hadoop.fs.filesystem$cache.getinternal(filesystem.java:2687) at org.apache.hadoop.fs.filesystem$cache.get(filesystem.java:2669) at org.apache.hadoop.fs.filesystem.get(filesystem.java:371) at org.apache.hadoop.fs.path.getfilesystem(path.java:295) at org.apache.hadoop.hbase.util.coprocessorclassloader.init(coprocessorclassloader.java:165) at org.apache.hadoop.hbase.util.coprocessorclassloader.getclassloader(coprocessorclassloader.java:250) at org.apache.hadoop.hbase.coprocessor.coprocessorhost.load(coprocessorhost.java:194) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.loadtablecoprocessors(regioncoprocessorhost.java:352) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.<init>(regioncoprocessorhost.java:240) at org.apache.hadoop.hbase.regionserver.hregion.<init>(hregion.java:749) at org.apache.hadoop.hbase.regionserver.hregion.<init>(hregion.java:657) at sun.reflect.generatedconstructoraccessor13.newinstance(unknown source) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:423) at org.apache.hadoop.hbase.regionserver.hregion.newhregion(hregion.java:6727) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7037) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7009) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:6965) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:6916) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.openregion(openregionhandler.java:362) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:129)     ",
        "label": 74
    },
    {
        "text": "classcastexception  filesystem cache clientfinalizer cannot be cast to thread  12/05/08 19:49:26 info regionserver.hregionserver: stopped: failed initialization exception in thread \"main\" java.lang.classcastexception: org.apache.hadoop.fs.filesystem$cache$clientfinalizer cannot be cast to java.lang.thread at org.apache.hadoop.hbase.regionserver.shutdownhook.suppresshdfsshutdownhook(shutdownhook.java:181) at org.apache.hadoop.hbase.regionserver.shutdownhook.install(shutdownhook.java:82) at org.apache.hadoop.hbase.regionserver.hregionserver.startregionserver(hregionserver.java:3601) at org.apache.hadoop.hbase.regionserver.hregionserver.startregionserver(hregionserver.java:3585) at org.apache.hadoop.hbase.regionserver.hregionservercommandline.start(hregionservercommandline.java:61) at org.apache.hadoop.hbase.regionserver.hregionservercommandline.run(hregionservercommandline.java:75) at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70) at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:76) at org.apache.hadoop.hbase.regionserver.hregionserver.main(hregionserver.java:3645) ",
        "label": 441
    },
    {
        "text": "null pointer exception in hbaseclient receiveresponse  relevant stack trace: 2011-11-30 13:10:26,557 [ipc client (47) connection to xx.xx.xx/172.22.4.68:60020 from an unknown user] warn org.apache.hadoop.ipc.hbaseclient - unexpected exception receiving call responses  java.lang.nullpointerexception  >-at org.apache.hadoop.hbase.ipc.hbaseclient$connection.receiveresponse(hbaseclient.java:583)  >-at org.apache.hadoop.hbase.ipc.hbaseclient$connection.run(hbaseclient.java:511)   if (log.isdebugenabled())           log.debug(getname() + \" got value #\" + id);         call call = calls.remove(id);         // read the flag byte         byte flag = in.readbyte();         boolean iserror = responseflag.iserror(flag);         if (responseflag.islength(flag)) {           // currently length if present is unused.           in.readint();         }         int state = in.readint(); // read the state.  currently unused.         if (iserror) {           //noinspection throwableinstanceneverthrown           call.setexception(new remoteexception( writableutils.readstring(in),               writableutils.readstring(in)));         } else { this line call call = calls.remove(id); may return a null 'call'. it is so because if you have rpc timeout enable, we proactively clean up other calls which have expired their lifetime along with the call for which socket timeout exception happend. ",
        "label": 417
    },
    {
        "text": "incorrect level for headings in asciidoc  mvn site prints the following errors for appendix_hbase_incompatibilities.adoc file. asciidoctor: warning: _chapters/appendix_hbase_incompatibilities.adoc: line 128: section title out of sequence: expected level 4, got level 5 asciidoctor: warning: _chapters/appendix_hbase_incompatibilities.adoc: line 200: list item index: expected 1, got 2 the heading level is incorrect and the listing is broken into multiple sections which is not supported. ",
        "label": 334
    },
    {
        "text": "add an hbase shell command to clear deadserver list in servermanager  currently if a regionserver is aborted due to fatal error or stopped by operator on purpose, it will be added into servermanager#deadservers list and shown as \"dead servers\" in the master ui. this is a valid warn for operators to notice the self-aborted servers and give a sanity check to avoid further issues. however, after necessary checks, even if operator is sure that the node is decommissioned (such as for repair), there's no way to clear the dead server list except restarting master. see more details in this discussion in mail list here we propose to add a hbase shell command to allow clearing dead server list in servermanager for advanced users, and the command should be executed with caution. ",
        "label": 188
    },
    {
        "text": "testasyncclusteradminapi2 failing sometimes  investigating a test failure seen on hbase-12349 git bisect shows me the following: # first bad commit: [473446719b7b81b56216862bf2a94a576ff90f60] hbase-18511 default no regions on master it wouldn't fail every time, but i used this command with the bisect export opts='test -dtest=org.apache.hadoop.hbase.client.testasyncclusteradminapi2 -pl hbase-server -am' mvn clean $opts && mvn $opts && mvn $opts && mvn $opts ",
        "label": 314
    },
    {
        "text": "update thrift examples to work with changed idl  hbase   examples are now out of date since hbase-697 went in. ",
        "label": 229
    },
    {
        "text": "tableinputformatbase with row filters scan too far  when tableinputformatbase has a non-null rowfilterinterface to apply, it creates combines the row filter with a stoprowfilter to get a scanner for each input split. however, the stoprowfilter never indicates that fitlerallremaining is true, so each input split will end up scanning to the end of the table. (contrast with htable.getscanner(byte[][] columns, byte[] starrow, byte[] stoprow, long timestamp) which uses a stoprowfilter wrapped in a whilematchrowfilter to ensure that scanning ends at the stop row. ",
        "label": 38
    },
    {
        "text": "master will lose hlog entries while splitting if region has empty oldlogfile log  i don't know yet how an empty oldlogfile.log can exist, however it happened.  master will fail to put the splits in the region oldlogfile.log if an empty oldlogfile.log already exists there.  this is the master log after i artificially reproduced it by placing an empty oldlogfile.log in /hbase/.meta./1028785192/oldlogfile.log and then killed the regionserver that was holding the .meta. table 2009-11-19 09:08:36,012 info org.apache.hadoop.hbase.regionserver.wal.hlog: splitting 1 hlog(s) in hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773  2009-11-19 09:08:36,012 debug org.apache.hadoop.hbase.regionserver.wal.hlog: splitting hlog 1 of 1: hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773/hlog.dat.1258637493128, length=0  2009-11-19 09:08:36,019 debug org.apache.hadoop.hbase.regionserver.wal.hlog: adding queue for .meta.,,1  2009-11-19 09:08:36,037 debug org.apache.hadoop.hbase.regionserver.wal.hlog: pushed=795 entries from hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773/hlog.dat.1258637493128  2009-11-19 09:08:36,038 debug org.apache.hadoop.hbase.regionserver.wal.hlog: thread got 795 to process  2009-11-19 09:08:36,043 warn org.apache.hadoop.hbase.regionserver.wal.hlog: old hlog file hdfs://b0:9000/hbase/.meta./1028785192/oldlogfile.log already exists. copying existing file to new file  2009-11-19 09:08:36,079 warn org.apache.hadoop.hbase.regionserver.wal.hlog: got while writing region .meta.,,1 log java.io.eofexception  2009-11-19 09:08:36,081 info org.apache.hadoop.hbase.regionserver.wal.hlog: hlog file splitting completed in 70 millis for hdfs://b0:9000/hbase/.logs/b4,60020,1258637492773 ",
        "label": 285
    },
    {
        "text": "initializing custom metrics implementation failed in mapper or reducer  when i want to initialize a custom metricssource implementation which extends class basesourceimpl in mapper or reducer\u2019s setup stage, an exception will be thrown:  error: org.apache.hadoop.metrics2.metricsexception: metrics source jvmmetrics already exists!  at org.apache.hadoop.metrics2.lib.defaultmetricssystem.newsourcename(defaultmetricssystem.java:126)  at org.apache.hadoop.metrics2.lib.defaultmetricssystem.sourcename(defaultmetricssystem.java:107)  at org.apache.hadoop.metrics2.impl.metricssystemimpl.register(metricssystemimpl.java:217)  at org.apache.hadoop.metrics2.source.jvmmetrics.create(jvmmetrics.java:78)  at org.apache.hadoop.hbase.metrics.basesourceimpl$defaultmetricssysteminitializer.init(basesourceimpl.java:51)  at org.apache.hadoop.hbase.metrics.basesourceimpl.<init>(basesourceimpl.java:74)  at org.apache.hadoop.hbase.mapreduce.identitytablereducer.setup(identitytablereducer.java:89)  at org.apache.hadoop.mapreduce.reducer.run(reducer.java:168)  at org.apache.hadoop.mapred.reducetask.runnewreducer(reducetask.java:627)  at org.apache.hadoop.mapred.reducetask.run(reducetask.java:389)  at org.apache.hadoop.mapred.yarnchild$2.run(yarnchild.java:167)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:415)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1548)  at org.apache.hadoop.mapred.yarnchild.main(yarnchild.java:162) the bug is similar to https://issues.apache.org/jira/browse/mapreduce-3976 ",
        "label": 206
    },
    {
        "text": "document namespaces in hbase book  we need to add documentation about the namespaces feature. it should go into the hbase book. ",
        "label": 174
    },
    {
        "text": "update community docs to recommend use of  co authored by  in git commits  discussion on [discuss] switch from \"ammending-author\" to \"co-authored-by\" in commit messages seems to have out in favor.   updated section should include a brief explanation (that includes \"multiple authors\" expressly instead of just the \"fixed up this thing\" that's there for amending-author). it should also have pointers to the github feature explanation. so long as those docs exist they're pretty good. ",
        "label": 343
    },
    {
        "text": "hbase filterlist cause keyonlyfilter not work  when use filterlist and keyonlyfilter together, if we put keyonlyfilter before filterlist, the keyonlyfilter may not work, means it will also grab the cell values: list<filter> filters = new arraylist<filter>();         filter filter1 = new singlecolumnvaluefilter(bytes.tobytes(\"cf\"), bytes.tobytes(\"column1\"),                 compareop.equal, bytes.tobytes(\"value1\"));         filter filter2 = new singlecolumnvaluefilter(bytes.tobytes(\"cf\"), bytes.tobytes(\"column1\"),                 compareop.equal, bytes.tobytes(\"value2\"));         filters.add(filter1);         filters.add(filter2);         filterlist filterlistall = new filterlist(operator.must_pass_all,                  new keyonlyfilter(),                 new filterlist(operator.must_pass_one, filters)); use the above code as filter to scan a table, it will return the cells with value instead of only return the key, if we put keyonlyfilter after filterlist as following, it works well. filterlist filterlistall = new filterlist(operator.must_pass_all,                 new filterlist(operator.must_pass_one, filters),                 new keyonlyfilter()); the cause should due to the following code at hbase-client filterlist.java @override   @edu.umd.cs.findbugs.annotations.suppresswarnings(value=\"sf_switch_fallthrough\",     justification=\"intentional\")   public returncode filterkeyvalue(cell v) throws ioexception {     this.referencekv = v;     // accumulates successive transformation of every filter that includes the cell:     cell transformed = v;     returncode rc = operator == operator.must_pass_one?         returncode.skip: returncode.include;     int listize = filters.size();     for (int i = 0; i < listize; i++) {       filter filter = filters.get(i);       if (operator == operator.must_pass_all) {         if (filter.filterallremaining()) {           return returncode.next_row;         } line1      returncode code = filter.filterkeyvalue(v);{color}         switch (code) {         // override include and continue to evaluate.         case include_and_next_col:           rc = returncode.include_and_next_col; // findbugs sf_switch_fallthrough         case include: line2          transformed = filter.transformcell(transformed);{color}           continue;         case seek_next_using_hint:           seekhintfilter = filter;           return code;         default:           return code;         }       } notice the \u201cline1\u201d,\"line2\" , first line is a recursive invocation, it will assign a cell results to the filterlist.transformedkv(we call it a), the results is from the filterlist with 2 singlecolumnvaluefilter, so a with contains the cell value, while the second line with return a to the var transformed.  back to the following loop, we can see the filterlist return results is var \"transformed \" which will override in each loop, so the value is determined by the last filter, so the order of keyonlyfilter will impact the results.  cell transformed = v;     returncode rc = operator == operator.must_pass_one?         returncode.skip: returncode.include;     int listize = filters.size();     for (int i = 0; i < listize; i++) {       filter filter = filters.get(i);       if (operator == operator.must_pass_all) {         if (filter.filterallremaining()) {           return returncode.next_row;         }         returncode code = filter.filterkeyvalue(v);         switch (code) {         // override include and continue to evaluate.         case include_and_next_col:           rc = returncode.include_and_next_col; // findbugs sf_switch_fallthrough         case include:           transformed = filter.transformcell(transformed);           continue;         case seek_next_using_hint:           seekhintfilter = filter;           return code;         default:           return code;         }        ",
        "label": 514
    },
    {
        "text": "unnecessary kv order check in storescanner  in storescanner.next(list<keyvalue>, int, string) i find this code:       // check that the heap gives us kvs in an increasing order.       if (prevkv != null && comparator != null           && comparator.compare(prevkv, kv) > 0) {         throw new ioexception(\"key \" + prevkv + \" followed by a \" +             \"smaller key \" + kv + \" in cf \" + store);       }       prevkv = kv; so this checks for bugs in the hfiles or the scanner code. it needs to compare each kvs with its predecessor. this seems unnecessary now, i propose that we remove this. ",
        "label": 286
    },
    {
        "text": "reseek directly to next column  when done with the current column, reseek directly to the next column rather than spending time reading more keys of current row-column which are not required. ",
        "label": 357
    },
    {
        "text": "filter out the expired store file scanner during the compaction  during the compaction time, hbase will generate a store scanner which will scan a list of store files. and it would be more efficient to filer out the expired store file since there is no need to read any key values from these store files. this optimization has been already implemented on 89-fb and this is the building block for hbase-5199 as well. it is supposed to be no-ops to compact the expired store files. ",
        "label": 324
    },
    {
        "text": "the configuration returned by cpenv should be read only   the configuration a cp gets when it does a getconfiguration on the environment is that of the regionserver. the cp should not be able to modify this config. we should throw exception if they try to write us. ditto w/ the connection they can get from the env. they should not be able to close it at min. ",
        "label": 314
    },
    {
        "text": "faster enable disable delete  the enable/disable/delete is slow. looking at code, its heavyweight. it doesn't do bulk scanning nor bulk writing. try doing some code client side that does bulk scan and bulk puts. it might run faster. ",
        "label": 314
    },
    {
        "text": "hbck doesn't reset the number of errors when retrying  using hbck to fix a problem, i see that when it retries it doesn't reset the number of inconsistencies so the number doubles. ",
        "label": 229
    },
    {
        "text": "25s sleep when expiring sessions in tests  there's a hardcoded 25 seconds sleep in hbasetestingutility.expiresession: int sessiontimeout = 5 * 1000; // 5 seconds ... final long sleep = sessiontimeout * 5l; log.info(\"zk closed session 0x\" + long.tohexstring(sessionid) +   \"; sleeping=\" + sleep); thread.sleep(sleep); i'm pretty sure this can be lowered at lot, and it would speed up a couple of tests. the only thing i'm afraid of is if this was made to accomodate flaky tests. ",
        "label": 340
    },
    {
        "text": "name of parameter quote need update in hbase default xml  in description of parameter \"hbase.zookeeper.quorum\", old name \"hbase.zookeeper.clientport \" should be replaced to \"hbase.zookeeper.property.clientport\". ",
        "label": 145
    },
    {
        "text": "testmultithreadedtablemapper fails in branch  from https://builds.apache.org/job/hbase-1.4/1023/jdk=jdk_1_7,label=hadoop&&!h13/testreport/org.apache.hadoop.hbase.mapreduce/testmultithreadedtablemapper/testmultithreadedtablemapper/ : java.lang.assertionerror at org.apache.hadoop.hbase.mapreduce.testmultithreadedtablemapper.verify(testmultithreadedtablemapper.java:195) at org.apache.hadoop.hbase.mapreduce.testmultithreadedtablemapper.runtestontable(testmultithreadedtablemapper.java:163) at org.apache.hadoop.hbase.mapreduce.testmultithreadedtablemapper.testmultithreadedtablemapper(testmultithreadedtablemapper.java:136) i ran the test locally which failed.  noticed the following in test output: 2017-11-18 19:28:13,929 error [hconnection-0x11db8653-shared--pool24-t9] protobuf.responseconverter(425): results sent from server=703. but only got 0 results completely at    client. resetting the scanner to scan again. 2017-11-18 19:28:13,929 error [hconnection-0x11db8653-shared--pool24-t3] protobuf.responseconverter(425): results sent from server=703. but only got 0 results completely at    client. resetting the scanner to scan again. 2017-11-18 19:28:14,461 error [hconnection-0x11db8653-shared--pool24-t8] protobuf.responseconverter(432): exception while reading cells from result.resetting the scanner to    scan again. org.apache.hadoop.hbase.donotretryioexception: results sent from server=703. but only got 0 results completely at client. resetting the scanner to scan again.   at org.apache.hadoop.hbase.protobuf.responseconverter.getresults(responseconverter.java:426)   at org.apache.hadoop.hbase.client.scannercallable.call(scannercallable.java:284)   at org.apache.hadoop.hbase.client.scannercallable.call(scannercallable.java:62)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithoutretries(rpcretryingcaller.java:219)   at org.apache.hadoop.hbase.client.scannercallablewithreplicas$retryingrpc.call(scannercallablewithreplicas.java:388)   at org.apache.hadoop.hbase.client.scannercallablewithreplicas$retryingrpc.call(scannercallablewithreplicas.java:362)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:142)   at org.apache.hadoop.hbase.client.resultboundedcompletionservice$queueingfuture.run(resultboundedcompletionservice.java:80)   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)   at java.lang.thread.run(thread.java:745) 2017-11-18 19:28:14,464 error [hconnection-0x11db8653-shared--pool24-t2] protobuf.responseconverter(432): exception while reading cells from result.resetting the scanner to    scan again. java.io.eofexception: partial cell read   at org.apache.hadoop.hbase.codec.basedecoder.rethroweofexception(basedecoder.java:86)   at org.apache.hadoop.hbase.codec.basedecoder.advance(basedecoder.java:70)   at org.apache.hadoop.hbase.protobuf.responseconverter.getresults(responseconverter.java:419)   at org.apache.hadoop.hbase.client.scannercallable.call(scannercallable.java:284)   at org.apache.hadoop.hbase.client.scannercallable.call(scannercallable.java:62)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithoutretries(rpcretryingcaller.java:219)   at org.apache.hadoop.hbase.client.scannercallablewithreplicas$retryingrpc.call(scannercallablewithreplicas.java:388)   at org.apache.hadoop.hbase.client.scannercallablewithreplicas$retryingrpc.call(scannercallablewithreplicas.java:362)   at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:142)   at org.apache.hadoop.hbase.client.resultboundedcompletionservice$queueingfuture.run(resultboundedcompletionservice.java:80)   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)   at java.lang.thread.run(thread.java:745) caused by: java.io.ioexception: premature eof from inputstream   at org.apache.hadoop.io.ioutils.readfully(ioutils.java:202)   at org.apache.hadoop.hbase.keyvalueutil.iscreate(keyvalueutil.java:611)   at org.apache.hadoop.hbase.codec.keyvaluecodec$keyvaluedecoder.parsecell(keyvaluecodec.java:69)   at org.apache.hadoop.hbase.codec.basedecoder.advance(basedecoder.java:67)   ... 11 more ",
        "label": 441
    },
    {
        "text": "  testmetareadereditor intermittently hangs  from 0.92 build #304: running org.apache.hadoop.hbase.catalog.testmetareadereditor running org.apache.hadoop.hbase.catalog.testcatalogtrackeroncluster tests run: 1, failures: 0, errors: 0, skipped: 0, time elapsed: 149.104 sec ",
        "label": 38
    },
    {
        "text": "don't delete hfiles when in  backup mode   this came up in a discussion i had with stack.  it would be nice if hbase could be notified that a backup is in progress (via a znode for example) and in that case either:  1. rename hfiles to be delete to <file>.bck  2. rename the hfiles into a special directory  3. rename them to a general trash directory (which would not need to be tied to backup mode). that way it should be able to get a consistent backup based on hfiles (hdfs snapshots or hard links would be better options here, but we do not have those). #1 makes cleanup a bit harder. ",
        "label": 236
    },
    {
        "text": "server not shutting down after losing log lease  ran into this bug testing 0.90rc2. i kill -stoped a server, and then -cont it after its logs had been split. it correctly decided it should abort, but got stuck during the shutdown process. ",
        "label": 314
    },
    {
        "text": "tableinputformat   support multi column family scan  currently hbase tableinputformat class has scan_column_family and scan_columns. scan_column_family can only scan single column family. if we need to scan multiple column families from a table then we must use scan_columns where we must provide both columns and column families which is not a convenient. can we have a scan_column_families which supports scan of multiple column families. ",
        "label": 495
    },
    {
        "text": "testtableinputformatscan2 testscanfromconfiguration fails on hadoop2 profile  this is essentially a port of hbase-8342 after the hbase-8326 ",
        "label": 248
    },
    {
        "text": "hclient still seems to depend on master  during a master down, but cluster up event, my clients seem to not work. clients shouldnt need to talk to the master anymore in 0.20. we should double check this. ",
        "label": 229
    },
    {
        "text": "replication gets stuck for empty wals  replication assumes that only the last wal of a recovered queue can be empty. but, intermittent dfs issues may cause empty wals being created (without the pwal magic), and a roll of wal to happen without a regionserver crash. this will cause recovered queues to have empty wals in the middle. this cause replication to get stuck: trace regionserver.replicationsource: opening log <wal_file> warn regionserver.replicationsource: <peer_cluster_id>-<recovered_queue> got:  java.io.eofexception at java.io.datainputstream.readfully(datainputstream.java:197) at java.io.datainputstream.readfully(datainputstream.java:169) at org.apache.hadoop.io.sequencefile$reader.init(sequencefile.java:1915) at org.apache.hadoop.io.sequencefile$reader.initialize(sequencefile.java:1880) at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1829) at org.apache.hadoop.io.sequencefile$reader.<init>(sequencefile.java:1843) at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader$walreader.<init>(sequencefilelogreader.java:70) at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.reset(sequencefilelogreader.java:168) at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogreader.initreader(sequencefilelogreader.java:177) at org.apache.hadoop.hbase.regionserver.wal.readerbase.init(readerbase.java:66) at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:312) at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:276) at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:264) at org.apache.hadoop.hbase.wal.walfactory.createreader(walfactory.java:423) at org.apache.hadoop.hbase.replication.regionserver.replicationwalreadermanager.openreader(replicationwalreadermanager.java:70) at org.apache.hadoop.hbase.replication.regionserver.replicationsource$replicationsourceworkerthread.openreader(replicationsource.java:830) at org.apache.hadoop.hbase.replication.regionserver.replicationsource$replicationsourceworkerthread.run(replicationsource.java:572) the wal in question was completely empty but there were other wals in the recovered queue which were newer and non-empty. ",
        "label": 471
    },
    {
        "text": "refuse operations from admin before master is initialized  in our testing environment,  when master is initializing, we found conflict problems between master#assignalluserregions and enabletable event, causing assigning region throw exception so that master abort itself. we think we'd better refuse operations from admin, such as createtable, enabletable,etc, it could reduce error. ",
        "label": 107
    },
    {
        "text": " replication  source shouldn't update zk if it didn't progress  a relatively minor optimization to be done in replicationsource, currently it calls replicationsourcemanager.logpositionandcleanoldlogs whether it made progress or not, generating more load on zk than necessary. the last position should be kept around so that we can compare. ",
        "label": 103
    },
    {
        "text": "generate changes md and releasenotes md for  ",
        "label": 149
    },
    {
        "text": "hbase broke the ability to manage non regionserver start up shut down  ie  you cant start stop thrift on a cluster anymore  i used to be able to do: bin/hbase-daemons.sh stop thrift  bin/hbase-daemons.sh start thrift i can't anymore. we can't manage anything but regionservers and zookeeper with this script now. ",
        "label": 314
    },
    {
        "text": "tableinputformat does not setup the configuration for hbase mapreduce jobs correctly  in 0.20.x and earlier tablemapreduceutil (and other input/outputformat classes) used to setup the htable with a hbaseconfiguration object, now that has been deprecated in #hbase-2036 they are constructed with hadoop configuration objects which do not contain the configuration xml file resources required to setup hbase. i think it is currently expected this is done when constructing the job but as this needs to be done for every hbase mapreduce job it would be cleaner if the tablemapreduceutil class did this whilst setting up the tableinput/outputformat classes. ",
        "label": 118
    },
    {
        "text": "fanoutoneblockasyncdfsoutputhelper fails to compile against hadoop  after hdfs-10996 clientprotocol#create() needs to specify the erasure code policy to use. in the meantime we should add a workaround to fanoutoneblockasyncdfsoutputhelper to be able to compile against hadoop 3 and hadoop 2. ",
        "label": 320
    },
    {
        "text": "fix test testtablemapreduce against   as reported by andrew on the hadoop mailing list, mvn -dhadoop.profile=23 clean test -dtest=org.apache.hadoop.hbase.mapreduce.testtablemapreduce fails on 0.92 branch. there are minor changes to hbase poms required to fix that. ",
        "label": 186
    },
    {
        "text": "list backup masters in ui   right now only the active master shows any information on the web ui. it would be nice to see that there are backup masters waiting. ",
        "label": 233
    },
    {
        "text": "second start after migration from to trunk crashes  i started a trunk cluster to upgrade from 90, inserted a ton of data, then did a clean shutdown. when i started again, i got the following exception: 11/09/13 12:29:09 info master.hmaster: meta has hri with htds. updating meta now.  11/09/13 12:29:09 fatal master.hmaster: unhandled exception. starting shutdown.  java.lang.negativearraysizeexception: -102  at org.apache.hadoop.hbase.util.bytes.readbytearray(bytes.java:147)  at org.apache.hadoop.hbase.htabledescriptor.readfields(htabledescriptor.java:606)  at org.apache.hadoop.hbase.migration.hregioninfo090x.readfields(hregioninfo090x.java:641)  at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:133)  at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:103)  at org.apache.hadoop.hbase.util.writables.gethregioninfoformigration(writables.java:228)  at org.apache.hadoop.hbase.catalog.metaeditor.gethregioninfoformigration(metaeditor.java:350)  at org.apache.hadoop.hbase.catalog.metaeditor$1.visit(metaeditor.java:273)  at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:633)  at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:255)  at org.apache.hadoop.hbase.catalog.metareader.fullscan(metareader.java:235)  at org.apache.hadoop.hbase.catalog.metaeditor.updatemetawithnewregioninfo(metaeditor.java:284)  at org.apache.hadoop.hbase.catalog.metaeditor.migraterootandmeta(metaeditor.java:298)  at org.apache.hadoop.hbase.master.hmaster.updatemetawithnewhri(hmaster.java:529)  at org.apache.hadoop.hbase.master.hmaster.finishinitialization(hmaster.java:472)  at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:309) ",
        "label": 314
    },
    {
        "text": "forward port hbase to and trunk  hbase-3848, request is always zero in webui for region server, was integrated to 0.90 this jira is a forward port. ",
        "label": 529
    },
    {
        "text": "hangup by regionserver causes write to fail  root cause is oome on the region server. nonetheless a hangup during ipc causes the client to fail the write, currently causing data loss. should the application catch and retry? or should the client libraries try harder? dec 4, 2008 5:25:30 pm com.powerset.heritrix.writer.hbasewriterprocessor innerprocessresult  severe: failed write of record: http://www.publicrecordslocal.com/georgia.htm (in thread 'toethread #9: http://www.publicrecordslocal.com/georgia.htm'; in processor 'archiver')  java.io.ioexception: java.io.ioexception: call to /10.30.94.38:60020 failed on local exception: connection refused  at com.powerset.heritrix.writer.hbasewriter.write(unknown source)  at com.powerset.heritrix.writer.hbasewriterprocessor.write(unknown source)  at com.powerset.heritrix.writer.hbasewriterprocessor.innerprocessresult(unknown source)  at org.archive.modules.processor.process(processor.java:123)  at org.archive.crawler.framework.toethread.processcrawluri(toethread.java:310)  at org.archive.crawler.framework.toethread.run(toethread.java:157)  caused by: java.io.ioexception: call to /10.30.94.38:60020 failed on local exception: connection refused  at org.apache.hadoop.ipc.client.call(client.java:699)  at org.apache.hadoop.hbase.ipc.hbaserpc$invoker.invoke(hbaserpc.java:323)  at $proxy12.batchupdates(unknown source)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers$2.call(hconnectionmanager.java:919)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers$2.call(hconnectionmanager.java:917)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.getregionserverforwithoutretries(hconnectionmanager.java:875)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.processbatchofrows(hconnectionmanager.java:916)  at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1267)  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1238)  at org.apache.hadoop.hbase.client.htable.commit(htable.java:1218)  at net.iridiant.content.content.storeurlinfo(unknown source)  ... 6 more  caused by: java.net.connectexception: connection refused  at sun.nio.ch.socketchannelimpl.checkconnect(native method)  at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:574)  at sun.nio.ch.socketadaptor.connect(socketadaptor.java:100)  at org.apache.hadoop.ipc.client$connection.setupiostreams(client.java:299)  at org.apache.hadoop.ipc.client$connection.access$1700(client.java:176)  at org.apache.hadoop.ipc.client.getconnection(client.java:772)  at org.apache.hadoop.ipc.client.call(client.java:685)  ... 16 more ",
        "label": 229
    },
    {
        "text": "persistentmetricstimevaryingrate gets used for non time based metrics  persistentmetricstimevaryingrate gets used for metrics that are not time-based, leading to confusing names such as \"avg_time\" for compaction size, etc. you hav to read the code in order to understand that this is actually referring to bytes, not seconds. ",
        "label": 348
    },
    {
        "text": " testwriteswhilegetting  unit test needs to be fixed   the unit test \"testwriteswhilegetting\" in the org.apache.hadoop.hbase.regionserver.testhregion test needs to be corrected. it is current using the table name and method name for initializing a hregion as \"testwriteswhilescanning\". it should be \"testwriteswhilegetting\". due to this, the test fails as the \"inithregion\" method fails in creating a new hregion for the test. ",
        "label": 462
    },
    {
        "text": "expand how table coprocessor jar and dependency path can be specified  currently you can specify the location of the coprocessor jar in the table coprocessor attribute.  the problem is that it only allows you to specify one jar that implements the coprocessor. you will need to either bundle all the dependencies into this jar, or you will need to copy the dependencies into hbase lib dir.  the first option may not be ideal sometimes. the second choice can be troublesome too, particularly when the hbase region sever node and dirs are dynamically added/created. there are a couple things we can expand here. we can allow the coprocessor attribute to specify a directory location, probably on hdfs.  we may even allow some wildcard in there. ",
        "label": 490
    },
    {
        "text": "split testrestartcluster  the logs for later tests are messed up with error messages, like 2019-04-09 09:41:11,717 warn  [leaserenewer:jenkins.hfs.12@localhost:41108] hdfs.leaserenewer(468): failed to renew lease for [dfsclient_nonmapreduce_400481390_21] for 55 seconds.  will retry shortly ... java.net.connectexception: call from asf918.gq1.ygridcore.net/67.195.81.138 to localhost:41108 failed on connection exception: java.net.connectexception: connection refused; for more details see:  http://wiki.apache.org/hadoop/connectionrefused at sun.reflect.generatedconstructoraccessor79.newinstance(unknown source) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:423) at org.apache.hadoop.net.netutils.wrapwithmessage(netutils.java:792) at org.apache.hadoop.net.netutils.wrapexception(netutils.java:732) at org.apache.hadoop.ipc.client.call(client.java:1480) at org.apache.hadoop.ipc.client.call(client.java:1413) at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:229) at com.sun.proxy.$proxy30.renewlease(unknown source) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocoltranslatorpb.renewlease(clientnamenodeprotocoltranslatorpb.java:595) at sun.reflect.generatedmethodaccessor154.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:191) at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:102) at com.sun.proxy.$proxy33.renewlease(unknown source) at sun.reflect.generatedmethodaccessor154.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:372) at com.sun.proxy.$proxy34.renewlease(unknown source) at sun.reflect.generatedmethodaccessor154.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:372) at com.sun.proxy.$proxy34.renewlease(unknown source) at org.apache.hadoop.hdfs.dfsclient.renewlease(dfsclient.java:901) at org.apache.hadoop.hdfs.leaserenewer.renew(leaserenewer.java:423) at org.apache.hadoop.hdfs.leaserenewer.run(leaserenewer.java:448) at org.apache.hadoop.hdfs.leaserenewer.access$700(leaserenewer.java:71) at org.apache.hadoop.hdfs.leaserenewer$1.run(leaserenewer.java:304) at java.lang.thread.run(thread.java:748) caused by: java.net.connectexception: connection refused at sun.nio.ch.socketchannelimpl.checkconnect(native method) at sun.nio.ch.socketchannelimpl.finishconnect(socketchannelimpl.java:717) at org.apache.hadoop.net.socketiowithtimeout.connect(socketiowithtimeout.java:206) at org.apache.hadoop.net.netutils.connect(netutils.java:531) at org.apache.hadoop.net.netutils.connect(netutils.java:495) at org.apache.hadoop.ipc.client$connection.setupconnection(client.java:615) at org.apache.hadoop.ipc.client$connection.setupiostreams(client.java:713) at org.apache.hadoop.ipc.client$connection.access$2900(client.java:376) at org.apache.hadoop.ipc.client.getconnection(client.java:1529) at org.apache.hadoop.ipc.client.call(client.java:1452) ... 26 more 2019-04-09 09:41:11,949 warn  [rs_open_region-regionserver/asf918:33671-1] regionserver.hstore(1062): failed flushing store file, retrying num=8 java.io.ioexception: filesystem closed at org.apache.hadoop.hdfs.dfsclient.checkopen(dfsclient.java:817) at org.apache.hadoop.hdfs.dfsclient.getfileinfo(dfsclient.java:2114) at org.apache.hadoop.hdfs.distributedfilesystem$22.docall(distributedfilesystem.java:1305) at org.apache.hadoop.hdfs.distributedfilesystem$22.docall(distributedfilesystem.java:1301) at org.apache.hadoop.fs.filesystemlinkresolver.resolve(filesystemlinkresolver.java:81) at org.apache.hadoop.hdfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:1301) at org.apache.hadoop.fs.filterfilesystem.getfilestatus(filterfilesystem.java:428) at org.apache.hadoop.fs.filesystem.exists(filesystem.java:1425) at org.apache.hadoop.hbase.regionserver.storefilewriter$builder.build(storefilewriter.java:528) at org.apache.hadoop.hbase.regionserver.hstore.createwriterintmp(hstore.java:1144) at org.apache.hadoop.hbase.regionserver.defaultstoreflusher.flushsnapshot(defaultstoreflusher.java:64) at org.apache.hadoop.hbase.regionserver.hstore.flushcache(hstore.java:1045) at org.apache.hadoop.hbase.regionserver.hstore$storeflusherimpl.flushcache(hstore.java:2325) at org.apache.hadoop.hbase.regionserver.hregion.internalflushcacheandcommit(hregion.java:2811) at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:2545) at org.apache.hadoop.hbase.regionserver.hregion.replayrecoverededitsifany(hregion.java:4655) at org.apache.hadoop.hbase.regionserver.hregion.initializeregioninternals(hregion.java:971) at org.apache.hadoop.hbase.regionserver.hregion.initialize(hregion.java:917) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7349) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7306) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7278) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7236) at org.apache.hadoop.hbase.regionserver.hregion.openhregion(hregion.java:7187) at org.apache.hadoop.hbase.regionserver.handler.assignregionhandler.process(assignregionhandler.java:133) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:104) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at java.lang.thread.run(thread.java:748) which makes it really hard to find out the actual problem... as we always restart the mini cluster for every test, it will not increase the time we execute these tests if we split it into several uts. ",
        "label": 149
    },
    {
        "text": "refresh our hadoop jar and update zookeeper to just released   ",
        "label": 314
    },
    {
        "text": "is root isnt needed in htabledescriptor   htabledescriptor has several vestigial references to the root region. we should clean these up. ",
        "label": 98
    },
    {
        "text": "junit dependency in main from htrace  htrace main depends on junit , it should be only test.  i created a junit in the github, that's https://github.com/cloudera/htrace/issues/1. if it's not fixed, we will be able to drop it in our pom, but let's wait a little before. ",
        "label": 340
    },
    {
        "text": "region count erratic in master ui  kill server hosting root or meta and see how count goes awry  make sure you have a bunch of reions in there  ",
        "label": 229
    },
    {
        "text": "intermittent testflushsnapshotfromclient testtakesnapshotaftermerge failure  from https://builds.apache.org/job/hbase-0.96/20/testreport/org.apache.hadoop.hbase.snapshot/testflushsnapshotfromclient/testtakesnapshotaftermerge/ : org.apache.hadoop.hbase.snapshot.hbasesnapshotexception: org.apache.hadoop.hbase.snapshot.hbasesnapshotexception: snapshot { ss=snapshotaftermerge table=test type=flush } had an error.  procedure snapshotaftermerge { waiting=[] done=[] } at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:526) at org.apache.hadoop.ipc.remoteexception.instantiateexception(remoteexception.java:95) at org.apache.hadoop.ipc.remoteexception.unwrapremoteexception(remoteexception.java:79) at org.apache.hadoop.hbase.client.rpcretryingcaller.translateexception(rpcretryingcaller.java:210) at org.apache.hadoop.hbase.client.rpcretryingcaller.translateexception(rpcretryingcaller.java:221) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:125) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:96) at org.apache.hadoop.hbase.client.hbaseadmin.executecallable(hbaseadmin.java:3120) at org.apache.hadoop.hbase.client.hbaseadmin.snapshot(hbaseadmin.java:2672) at org.apache.hadoop.hbase.client.hbaseadmin.snapshot(hbaseadmin.java:2605) at org.apache.hadoop.hbase.client.hbaseadmin.snapshot(hbaseadmin.java:2612) at org.apache.hadoop.hbase.snapshot.testflushsnapshotfromclient.testtakesnapshotaftermerge(testflushsnapshotfromclient.java:336) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:47) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:12) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:44) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:17) at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:74) caused by: org.apache.hadoop.hbase.ipc.remotewithextrasexception: org.apache.hadoop.hbase.snapshot.hbasesnapshotexception: snapshot { ss=snapshotaftermerge table=test type=flush } had an error.  procedure snapshotaftermerge { waiting=[] done=[] } at org.apache.hadoop.hbase.master.snapshot.snapshotmanager.issnapshotdone(snapshotmanager.java:365) at org.apache.hadoop.hbase.master.hmaster.issnapshotdone(hmaster.java:2947) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$2.callblockingmethod(masteradminprotos.java:32890) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2146) at org.apache.hadoop.hbase.ipc.rpcserver$handler.run(rpcserver.java:1851) caused by: org.apache.hadoop.hbase.snapshot.corruptedsnapshotexception via failed taking snapshot { ss=snapshotaftermerge table=test type=flush } due to exception:missing parent hfile for: 30b951996ef34885a2f5d64e4acb2467.6d1ed72bf95759cb606e8a6efdc6908e:org.apache.hadoop.hbase.snapshot.corruptedsnapshotexception: missing parent hfile for: 30b951996ef34885a2f5d64e4acb2467.6d1ed72bf95759cb606e8a6efdc6908e at org.apache.hadoop.hbase.errorhandling.foreignexceptiondispatcher.rethrowexception(foreignexceptiondispatcher.java:85) at org.apache.hadoop.hbase.master.snapshot.takesnapshothandler.rethrowexceptioniffailed(takesnapshothandler.java:318) at org.apache.hadoop.hbase.master.snapshot.snapshotmanager.issnapshotdone(snapshotmanager.java:355) ... 4 more caused by: org.apache.hadoop.hbase.snapshot.corruptedsnapshotexception: missing parent hfile for: 30b951996ef34885a2f5d64e4acb2467.6d1ed72bf95759cb606e8a6efdc6908e at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifystorefile(mastersnapshotverifier.java:224) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.access$000(mastersnapshotverifier.java:82) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier$1.storefile(mastersnapshotverifier.java:210) at org.apache.hadoop.hbase.util.fsvisitor.visitregionstorefiles(fsvisitor.java:115) at org.apache.hadoop.hbase.snapshot.snapshotreferenceutil.visitregionstorefiles(snapshotreferenceutil.java:123) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifyregion(mastersnapshotverifier.java:207) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifyregions(mastersnapshotverifier.java:175) at org.apache.hadoop.hbase.master.snapshot.mastersnapshotverifier.verifysnapshot(mastersnapshotverifier.java:120) at org.apache.hadoop.hbase.master.snapshot.takesnapshothandler.process(takesnapshothandler.java:189) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:131) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:724) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1425) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1629) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1686) at org.apache.hadoop.hbase.protobuf.generated.masteradminprotos$masteradminservice$blockingstub.issnapshotdone(masteradminprotos.java:34923) at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation$5.issnapshotdone(hconnectionmanager.java:2123) at org.apache.hadoop.hbase.client.hbaseadmin$24.call(hbaseadmin.java:2675) at org.apache.hadoop.hbase.client.hbaseadmin$24.call(hbaseadmin.java:2672) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:118) ",
        "label": 441
    },
    {
        "text": "hbase needs to defend against hbck operations  hbck needs updating to run against hbase2. meantime, if an hbck from hbase1 is run against hbck2, it may do damage. hbase2 should defend itself against hbck1 ops. ",
        "label": 459
    },
    {
        "text": "on split  if failure updating of  meta  table subsequently broke  on pset cluster \u2013 running 0.2.0 \u2013 i saw the following: + deadlock on server carrying .meta. made the .meta. table inaccessible (deadlock has been fixed)  + out on a regionserver, we split; two new daughters are created and parent region is closed.  + regionserver fails to update the .meta. with change in parent state and addition of two new daughter regions restarting the server carrying .meta. got us over the deadlock but subsequently, the parent region is no longer online nor its replacements. attempting restart of regionserver to see if parent will come back on line (since it was not 'offlined' in .meta. should come back on line again and resplit). ugly will be the fact that the filesystem has some trash in it \u2013 the new daughter regions. to consider: do not close the parent until the .meta. has been successfully updated. also, if .meta. update fails, remove daughter regions. ",
        "label": 314
    },
    {
        "text": "improve error message when hmaster can't bind to port  when the master fails to start becahse hbase.master.port is already taken, the log messages could make it easier to tell. 2015-07-14 13:10:02,667 info [main] regionserver.rsrpcservices: master/master01.example.com/10.20.188.121:16000 server-side hconnection retries=350  2015-07-14 13:10:02,879 info [main] ipc.simplerpcscheduler: using deadline as user call queue, count=3  2015-07-14 13:10:02,895 error [main] master.hmastercommandline: master exiting  java.lang.runtimeexception: failed construction of master: class org.apache.hadoop.hbase.master.hmaster  at org.apache.hadoop.hbase.master.hmaster.constructmaster(hmaster.java:2258)  at org.apache.hadoop.hbase.master.hmastercommandline.startmaster(hmastercommandline.java:234)  at org.apache.hadoop.hbase.master.hmastercommandline.run(hmastercommandline.java:140)  at org.apache.hadoop.util.toolrunner.run(toolrunner.java:70)  at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:126)  at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:2272)  caused by: java.net.bindexception: address already in use  at sun.nio.ch.net.bind0(native method)  at sun.nio.ch.net.bind(net.java:444)  at sun.nio.ch.net.bind(net.java:436)  at sun.nio.ch.serversocketchannelimpl.bind(serversocketchannelimpl.java:214)  at sun.nio.ch.serversocketadaptor.bind(serversocketadaptor.java:74)  at org.apache.hadoop.hbase.ipc.rpcserver.bind(rpcserver.java:2513)  at org.apache.hadoop.hbase.ipc.rpcserver$listener.<init>(rpcserver.java:599)  at org.apache.hadoop.hbase.ipc.rpcserver.<init>(rpcserver.java:2000)  at org.apache.hadoop.hbase.regionserver.rsrpcservices.<init>(rsrpcservices.java:919)  at org.apache.hadoop.hbase.master.masterrpcservices.<init>(masterrpcservices.java:211)  at org.apache.hadoop.hbase.master.hmaster.createrpcservices(hmaster.java:509)  at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:535)  at org.apache.hadoop.hbase.master.hmaster.<init>(hmaster.java:351)  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:57)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)  at java.lang.reflect.constructor.newinstance(constructor.java:526)  at org.apache.hadoop.hbase.master.hmaster.constructmaster(hmaster.java:2253)  ... 5 more i recognize that the \"rsrpcservices\" log message shows port 16000, but i don't know why a new operator would. additionally, it'd be nice to tell them that the port is controlled by hbase.master.port. maybe give a hint on how to see what's using the port. could be too os-dist specific? ",
        "label": 308
    },
    {
        "text": "test for creating a large number of regions  after hbase-7220, i think it will be good to write a unit test/it to create a large number of regions. we can put a reasonable timeout to the test. ",
        "label": 339
    },
    {
        "text": "package build for rpm and deb are broken  environment variable final.name was removed in hbase-3629, and this prevents rpm/deb packaging from building. ",
        "label": 160
    },
    {
        "text": "reimplement load balancing to be a background process and to not use heartbeats  ",
        "label": 247
    },
    {
        "text": "regionserver testpriorityrpc uses a fixed port    you can have this if the port is used.  testable by doing \"nc -l 60020\" before launching the test. ------------------------------------------------------------------------------- test set: org.apache.hadoop.hbase.regionserver.testpriorityrpc ------------------------------------------------------------------------------- tests run: 1, failures: 0, errors: 1, skipped: 0, time elapsed: 0.286 sec <<< failure! org.apache.hadoop.hbase.regionserver.testpriorityrpc  time elapsed: 0 sec  <<< error! java.lang.runtimeexception: failed construction of regionserver: class org.apache.hadoop.hbase.regionserver.hregionserver         at org.apache.hadoop.hbase.regionserver.hregionserver.constructregionserver(hregionserver.java:2376)         at org.apache.hadoop.hbase.regionserver.testpriorityrpc.onetimesetup(testpriorityrpc.java:53)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45)         at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15)         at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42)         at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:27)         at org.junit.runners.parentrunner.run(parentrunner.java:300)         at org.apache.maven.surefire.junit4.junit4testset.execute(junit4testset.java:53)         at org.apache.maven.surefire.junit4.junit4provider.executetestset(junit4provider.java:123)         at org.apache.maven.surefire.junit4.junit4provider.invoke(junit4provider.java:104)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.maven.surefire.util.reflectionutils.invokemethodwitharray(reflectionutils.java:164)         at org.apache.maven.surefire.booter.providerfactory$providerproxy.invoke(providerfactory.java:110)         at org.apache.maven.surefire.booter.surefirestarter.invokeprovider(surefirestarter.java:175)         at org.apache.maven.surefire.booter.surefirestarter.runsuitesinprocesswhenforked(surefirestarter.java:81)         at org.apache.maven.surefire.booter.forkedbooter.main(forkedbooter.java:68) caused by: java.lang.reflect.invocationtargetexception         at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)         at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)         at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)         at java.lang.reflect.constructor.newinstance(constructor.java:513)         at org.apache.hadoop.hbase.regionserver.hregionserver.constructregionserver(hregionserver.java:2374)         ... 22 more caused by: java.net.bindexception: problem binding to localhost/127.0.0.1:60020 : address already in use         at org.apache.hadoop.hbase.ipc.hbaseserver.bind(hbaseserver.java:295)         at org.apache.hadoop.hbase.ipc.hbaseserver$listener.<init>(hbaseserver.java:510)         at org.apache.hadoop.hbase.ipc.hbaseserver.<init>(hbaseserver.java:1922)         at org.apache.hadoop.hbase.ipc.protobufrpcengine$server.<init>(protobufrpcengine.java:247)         at org.apache.hadoop.hbase.ipc.protobufrpcengine.getserver(protobufrpcengine.java:85)         at org.apache.hadoop.hbase.ipc.protobufrpcengine.getserver(protobufrpcengine.java:57)         at org.apache.hadoop.hbase.ipc.hbaserpc.getserver(hbaserpc.java:400)         at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:516)         ... 27 more caused by: java.net.bindexception: address already in use         at sun.nio.ch.net.bind(native method)         at sun.nio.ch.serversocketchannelimpl.bind(serversocketchannelimpl.java:126)         at sun.nio.ch.serversocketadaptor.bind(serversocketadaptor.java:59)         at org.apache.hadoop.hbase.ipc.hbaseserver.bind(hbaseserver.java:293)         ... 34 more ",
        "label": 340
    },
    {
        "text": "add an lz4 compression option to hfile  hadoop-7657 adds support for lz4 compression to hadoop core. as with snappy, we should add reflection based support for this alternative to hfile.compression. ",
        "label": 38
    },
    {
        "text": "compilation errors when using non sun jdks to build hbase  when using ibm java 7 to build hbase-0.94.1, the following comilation error is seen. [info] -------------------------------------------------------------  [error] compilation error :   [info] -------------------------------------------------------------  [error] /home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/resourcechecker.java:[23,25] error: package com.sun.management does not exist  [error] /home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/resourcechecker.java:[46,25] error: cannot find symbol  [error] symbol: class unixoperatingsystemmxbean  location: class resourceanalyzer  /home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/resourcechecker.java:[75,29] error: cannot find symbol  [error] symbol: class unixoperatingsystemmxbean  location: class resourceanalyzer  /home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/resourcechecker.java:[76,23] error: cannot find symbol  [info] 4 errors   [info] -------------------------------------------------------------  [info] ------------------------------------------------------------------------  [info] build failure  [info] ------------------------------------------------------------------------  i have a patch available which should work for all jdks including sun.  i am in the process of testing this patch. preliminary tests indicate the build is working fine with this patch. i will post this patch when i am done testing. ",
        "label": 282
    },
    {
        "text": " snapshot merge  fix testmultiparallel  testmultiparallel has three tests that always fail on the merged branch: #testflushcommitswithabort, #testactivethreadscount and #testflushcommitsnoabort. there were some changes introduced in hbase-7299 which happend on trunk before the merge and are likely related. (that patch addresses problems on the same tests). ",
        "label": 441
    },
    {
        "text": "throttle online schema changes   throttle the open and close of the regions after an online schema change ",
        "label": 154
    },
    {
        "text": " pv2  filter out success procedures  on decent sized cluster  plethora overwhelms problems  trying to figure some problem procedures using nice ui tools. listing of procedures and locks shows success procedures and waiting/runnables all in the same list. can't find why a scp is stuck when it has hundreds of subprocedures all of which have succeeded but one especially when list includes other concurrent scps.... for now will just filter out success items. ",
        "label": 314
    },
    {
        "text": "testreplicawithcluster testreplicagetwithprimaryandmetadown failure in master  please see the flakey test list. https://builds.apache.org/job/hbase-find-flaky-tests/lastsuccessfulbuild/artifact/dashboard.html client.testreplicawithcluster 96.7% (29 / 30) 29 / 0 / 0 show/hide ",
        "label": 314
    },
    {
        "text": "testhregiononcluster testdatacorrectnessreplayingrecoverededits still fails occasionally   java.lang.reflect.undeclaredthrowableexception at $proxy20.move(unknown source) at org.apache.hadoop.hbase.client.hbaseadmin.move(hbaseadmin.java:1426) at org.apache.hadoop.hbase.regionserver.testhregiononcluster.testdatacorrectnessreplayingrecoverededits(testhregiononcluster.java:84) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:45) at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:42) at org.junit.internal.runners.statements.invokemethod.evaluate(invokemethod.java:20) at org.junit.internal.runners.statements.failontimeout$statementthread.run(failontimeout.java:62) caused by: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.hbase.unknownregionexception: 3775e6eba7718e4e4571942ae73b5851 at org.apache.hadoop.hbase.master.hmaster.move(hmaster.java:1159) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:364) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:1021) at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:150) ... 12 more ",
        "label": 286
    },
    {
        "text": "do not expose implementation classes to cp  for example, storefile. expose the implementation classes to cp will make it harder to implement new features or improve the old implementation. ",
        "label": 149
    },
    {
        "text": "inappropriate checking of logging mode in hregionserver  there is a condition check for debug mode logging in hregionserver.java . because of this the region server never closes the meta region while stopping hbase and thus never stops, if debug mode is not enable in logging. ",
        "label": 11
    },
    {
        "text": "testreplication failing up on builds a o because already running zk with new format root servername  when the test runs, i see this at the start of the output: 2011-09-21 04:56:35,282 info  [main] zookeeper.minizookeepercluster(122): failed binding zk server to client port: 21818 2011-09-21 04:56:35,344 info  [main] zookeeper.minizookeepercluster(136): started minizk server on client port: 21819 so, i think what is going on is that the test is connecting to the wrong zk ensemble because it does this on setup:   @beforeclass   public static void setupbeforeclass() throws exception {     conf1 = hbaseconfiguration.create();     conf1.set(hconstants.zookeeper_znode_parent, \"/1\");     // smaller block size and capacity to trigger more operations     // and test them     conf1.setint(\"hbase.regionserver.hlog.blocksize\", 1024*20);     conf1.setint(\"replication.source.size.capacity\", 1024);     conf1.setlong(\"replication.source.sleepforretries\", 100);     conf1.setint(\"hbase.regionserver.maxlogs\", 10);     conf1.setlong(\"hbase.master.logcleaner.ttl\", 10);     conf1.setboolean(hconstants.replication_enable_key, true);     conf1.setboolean(\"dfs.support.append\", true);     conf1.setlong(hconstants.thread_wake_frequency, 100);     utility1 = new hbasetestingutility(conf1);     utility1.startminizkcluster(); .... and we refer to conf1 subsequently rather than to htu.getconfiguration. ",
        "label": 314
    },
    {
        "text": "hdfs  breaks  testhbasetestingutility multiclusters  we upgraded our hadoop jar in trunk to latest on 0.20-append branch. testhbasetestingutility started failing reliably. if i back out hdfs-724, the test passes again. this issue is about figuring whats up here. ",
        "label": 192
    },
    {
        "text": "testchangingencoding failing sporadically in build  the test passes locally for me and elliott but takes a long time to run. timeout is only two minutes for the test though. ",
        "label": 314
    },
    {
        "text": "add a switch to dynamicclassloader to disable it  since hbase-1936 we have the option to load jars dynamically by default from hdfs or the local filesystem, however hbase.dynamic.jars.dir points to a directory that could be world writable it potentially opens a security problem in both the client side and the rs. we should consider to have a switch to enable or disable this option and it should be off by default. ",
        "label": 205
    },
    {
        "text": "deprecate disable and remove support for reading zookeeper zoo cfg files from the classpath  this issue was found by lars: http://search-hadoop.com/m/n04sthncji2/zoo.cfg+vs+hbase-site.xml&subj=re+zoo+cfg+vs+hbase+site+xml lets fix the inconsistency found and fix the places where we use non-zk attribute name for a zk attribute in hbase (there's only a few places that i remember \u2013 maximum client connections is one iirc) ",
        "label": 194
    },
    {
        "text": "add ga to hbase apache org  lets add the bit of script necessary tracking hbase.apache.org in google analytics. i was going to get it going first then open it to the pmc for viewing. ",
        "label": 314
    },
    {
        "text": "testcellacls failing  on1 builds  caught this in 1.7 builds: \"priorityrpcserver.handler=4,queue=0,port=42214\" daemon prio=10 tid=0x00007f08d5786000 nid=0x2eed in object.wait() [0x00007f08918d5000]    java.lang.thread.state: timed_waiting (on object monitor) at java.lang.object.wait(native method) at org.apache.hadoop.hbase.client.asyncprocess.waitformaximumcurrenttasks(asyncprocess.java:1659) - locked <0x00000007580e5f98> (a java.util.concurrent.atomic.atomiclong) at org.apache.hadoop.hbase.client.asyncprocess.waitforallpreviousopsandreset(asyncprocess.java:1688) at org.apache.hadoop.hbase.client.bufferedmutatorimpl.backgroundflushcommits(bufferedmutatorimpl.java:208) at org.apache.hadoop.hbase.client.bufferedmutatorimpl.flush(bufferedmutatorimpl.java:183) - locked <0x00000007580f36f8> (a org.apache.hadoop.hbase.client.bufferedmutatorimpl) at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1430) at org.apache.hadoop.hbase.client.htable.put(htable.java:1021) at org.apache.hadoop.hbase.security.access.accesscontrollists.adduserpermission(accesscontrollists.java:176) at org.apache.hadoop.hbase.security.access.accesscontroller$8.run(accesscontroller.java:2175) at org.apache.hadoop.hbase.security.access.accesscontroller$8.run(accesscontroller.java:2172) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:415) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1614) at org.apache.hadoop.security.securityutil.doasuser(securityutil.java:444) at org.apache.hadoop.security.securityutil.doasloginuser(securityutil.java:425) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.hbase.util.methods.call(methods.java:39) at org.apache.hadoop.hbase.security.user.runasloginuser(user.java:205) at org.apache.hadoop.hbase.security.access.accesscontroller.grant(accesscontroller.java:2172) at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice$1.grant(accesscontrolprotos.java:9933) at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice.callmethod(accesscontrolprotos.java:10097) at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:7650) at org.apache.hadoop.hbase.regionserver.rsrpcservices.execserviceonregion(rsrpcservices.java:1896) at org.apache.hadoop.hbase.regionserver.rsrpcservices.execservice(rsrpcservices.java:1878) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32590) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2120) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:106) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:107) at java.lang.thread.run(thread.java:745) \"priorityrpcserver.handler=3,queue=1,port=42214\" daemon prio=10 tid=0x00007f08d5784000 nid=0x2eec in object.wait() [0x00007f08919d7000]    java.lang.thread.state: timed_waiting (on object monitor) at java.lang.object.wait(native method) at org.apache.hadoop.hbase.ipc.rpcclientimpl.call(rpcclientimpl.java:1248) - locked <0x00000007cb61ecd8> (a org.apache.hadoop.hbase.ipc.call) at org.apache.hadoop.hbase.ipc.abstractrpcclient.callblockingmethod(abstractrpcclient.java:217) at org.apache.hadoop.hbase.ipc.abstractrpcclient$blockingrpcchannelimplementation.callblockingmethod(abstractrpcclient.java:295) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$blockingstub.get(clientprotos.java:33008) at org.apache.hadoop.hbase.client.htable$3.call(htable.java:862) at org.apache.hadoop.hbase.client.htable$3.call(htable.java:853) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:126) at org.apache.hadoop.hbase.client.htable.get(htable.java:870) at org.apache.hadoop.hbase.client.htable.get(htable.java:836) at org.apache.hadoop.hbase.security.access.accesscontrollists.getpermissions(accesscontrollists.java:464) at org.apache.hadoop.hbase.security.access.accesscontroller.updateacl(accesscontroller.java:268) at org.apache.hadoop.hbase.security.access.accesscontroller.postput(accesscontroller.java:1615) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$32.call(regioncoprocessorhost.java:956) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$regionoperation.call(regioncoprocessorhost.java:1673) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.execoperation(regioncoprocessorhost.java:1749) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.execoperation(regioncoprocessorhost.java:1705) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.postput(regioncoprocessorhost.java:952) at org.apache.hadoop.hbase.regionserver.hregion.dominibatchmutation(hregion.java:3221) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2841) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2783) at org.apache.hadoop.hbase.regionserver.rsrpcservices.dobatchop(rsrpcservices.java:697) at org.apache.hadoop.hbase.regionserver.rsrpcservices.dononatomicregionmutation(rsrpcservices.java:659) at org.apache.hadoop.hbase.regionserver.rsrpcservices.multi(rsrpcservices.java:2048) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32594) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2120) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:106) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:107) at java.lang.thread.run(thread.java:745) \"priorityrpcserver.handler=2,queue=0,port=42214\" daemon prio=10 tid=0x00007f08d5782000 nid=0x2eeb in object.wait() [0x00007f0891ad8000]    java.lang.thread.state: waiting (on object monitor) at java.lang.object.wait(native method) at java.lang.object.wait(object.java:503) at org.apache.hadoop.hbase.client.resultboundedcompletionservice.take(resultboundedcompletionservice.java:148) - locked <0x00000007cc351438> (a [lorg.apache.hadoop.hbase.client.resultboundedcompletionservice$queueingfuture;) at org.apache.hadoop.hbase.client.scannercallablewithreplicas.call(scannercallablewithreplicas.java:188) at org.apache.hadoop.hbase.client.scannercallablewithreplicas.call(scannercallablewithreplicas.java:59) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithoutretries(rpcretryingcaller.java:200) at org.apache.hadoop.hbase.client.clientsmallreversedscanner.loadcache(clientsmallreversedscanner.java:212) at org.apache.hadoop.hbase.client.clientsmallreversedscanner.next(clientsmallreversedscanner.java:186) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregioninmeta(connectionmanager.java:1276) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1182) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1166) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1123) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.getregionlocation(connectionmanager.java:958) at org.apache.hadoop.hbase.client.hregionlocator.getregionlocation(hregionlocator.java:83) at org.apache.hadoop.hbase.client.regionservercallable.prepare(regionservercallable.java:79) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:124) at org.apache.hadoop.hbase.client.htable.get(htable.java:870) at org.apache.hadoop.hbase.client.htable.get(htable.java:836) at org.apache.hadoop.hbase.security.access.accesscontrollists.getpermissions(accesscontrollists.java:464) at org.apache.hadoop.hbase.security.access.accesscontroller.updateacl(accesscontroller.java:268) at org.apache.hadoop.hbase.security.access.accesscontroller.postput(accesscontroller.java:1615) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$32.call(regioncoprocessorhost.java:956) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$regionoperation.call(regioncoprocessorhost.java:1673) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.execoperation(regioncoprocessorhost.java:1749) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.execoperation(regioncoprocessorhost.java:1705) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.postput(regioncoprocessorhost.java:952) at org.apache.hadoop.hbase.regionserver.hregion.dominibatchmutation(hregion.java:3221) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2841) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2783) at org.apache.hadoop.hbase.regionserver.rsrpcservices.dobatchop(rsrpcservices.java:697) at org.apache.hadoop.hbase.regionserver.rsrpcservices.dononatomicregionmutation(rsrpcservices.java:659) at org.apache.hadoop.hbase.regionserver.rsrpcservices.multi(rsrpcservices.java:2048) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32594) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2120) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:106) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:107) at java.lang.thread.run(thread.java:745) \"priorityrpcserver.handler=1,queue=1,port=42214\" daemon prio=10 tid=0x00007f08d5780000 nid=0x2eea in object.wait() [0x00007f0891bd8000]    java.lang.thread.state: timed_waiting (on object monitor) at java.lang.object.wait(native method) at org.apache.hadoop.hbase.client.asyncprocess.waitformaximumcurrenttasks(asyncprocess.java:1659) - locked <0x00000007581261c0> (a java.util.concurrent.atomic.atomiclong) at org.apache.hadoop.hbase.client.asyncprocess.waitforallpreviousopsandreset(asyncprocess.java:1688) at org.apache.hadoop.hbase.client.bufferedmutatorimpl.backgroundflushcommits(bufferedmutatorimpl.java:208) at org.apache.hadoop.hbase.client.bufferedmutatorimpl.flush(bufferedmutatorimpl.java:183) - locked <0x0000000758141d48> (a org.apache.hadoop.hbase.client.bufferedmutatorimpl) at org.apache.hadoop.hbase.client.htable.flushcommits(htable.java:1430) at org.apache.hadoop.hbase.client.htable.put(htable.java:1021) at org.apache.hadoop.hbase.security.access.accesscontrollists.adduserpermission(accesscontrollists.java:176) at org.apache.hadoop.hbase.security.access.accesscontroller$8.run(accesscontroller.java:2175) at org.apache.hadoop.hbase.security.access.accesscontroller$8.run(accesscontroller.java:2172) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:415) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1614) at org.apache.hadoop.security.securityutil.doasuser(securityutil.java:444) at org.apache.hadoop.security.securityutil.doasloginuser(securityutil.java:425) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at org.apache.hadoop.hbase.util.methods.call(methods.java:39) at org.apache.hadoop.hbase.security.user.runasloginuser(user.java:205) at org.apache.hadoop.hbase.security.access.accesscontroller.grant(accesscontroller.java:2172) at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice$1.grant(accesscontrolprotos.java:9933) at org.apache.hadoop.hbase.protobuf.generated.accesscontrolprotos$accesscontrolservice.callmethod(accesscontrolprotos.java:10097) at org.apache.hadoop.hbase.regionserver.hregion.execservice(hregion.java:7650) at org.apache.hadoop.hbase.regionserver.rsrpcservices.execserviceonregion(rsrpcservices.java:1896) at org.apache.hadoop.hbase.regionserver.rsrpcservices.execservice(rsrpcservices.java:1878) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32590) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2120) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:106) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:107) at java.lang.thread.run(thread.java:745) \"priorityrpcserver.handler=0,queue=0,port=42214\" daemon prio=10 tid=0x00007f08d577e000 nid=0x2ee8 in object.wait() [0x00007f0891ddb000]    java.lang.thread.state: waiting (on object monitor) at java.lang.object.wait(native method) at java.lang.object.wait(object.java:503) at org.apache.hadoop.hbase.client.resultboundedcompletionservice.take(resultboundedcompletionservice.java:148) - locked <0x00000007ca626188> (a [lorg.apache.hadoop.hbase.client.resultboundedcompletionservice$queueingfuture;) at org.apache.hadoop.hbase.client.scannercallablewithreplicas.call(scannercallablewithreplicas.java:188) at org.apache.hadoop.hbase.client.scannercallablewithreplicas.call(scannercallablewithreplicas.java:59) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithoutretries(rpcretryingcaller.java:200) at org.apache.hadoop.hbase.client.clientsmallreversedscanner.loadcache(clientsmallreversedscanner.java:212) at org.apache.hadoop.hbase.client.clientsmallreversedscanner.next(clientsmallreversedscanner.java:186) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregioninmeta(connectionmanager.java:1276) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1182) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1166) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.locateregion(connectionmanager.java:1123) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.getregionlocation(connectionmanager.java:958) at org.apache.hadoop.hbase.client.hregionlocator.getregionlocation(hregionlocator.java:83) at org.apache.hadoop.hbase.client.regionservercallable.prepare(regionservercallable.java:79) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithretries(rpcretryingcaller.java:124) at org.apache.hadoop.hbase.client.htable.get(htable.java:870) at org.apache.hadoop.hbase.client.htable.get(htable.java:836) at org.apache.hadoop.hbase.security.access.accesscontrollists.getpermissions(accesscontrollists.java:464) at org.apache.hadoop.hbase.security.access.accesscontroller.updateacl(accesscontroller.java:268) at org.apache.hadoop.hbase.security.access.accesscontroller.postput(accesscontroller.java:1615) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$32.call(regioncoprocessorhost.java:956) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost$regionoperation.call(regioncoprocessorhost.java:1673) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.execoperation(regioncoprocessorhost.java:1749) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.execoperation(regioncoprocessorhost.java:1705) at org.apache.hadoop.hbase.regionserver.regioncoprocessorhost.postput(regioncoprocessorhost.java:952) at org.apache.hadoop.hbase.regionserver.hregion.dominibatchmutation(hregion.java:3221) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2841) at org.apache.hadoop.hbase.regionserver.hregion.batchmutate(hregion.java:2783) at org.apache.hadoop.hbase.regionserver.rsrpcservices.dobatchop(rsrpcservices.java:697) at org.apache.hadoop.hbase.regionserver.rsrpcservices.dononatomicregionmutation(rsrpcservices.java:659) at org.apache.hadoop.hbase.regionserver.rsrpcservices.multi(rsrpcservices.java:2048) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:32594) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2120) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:106) at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:130) at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:107) at java.lang.thread.run(thread.java:745) ",
        "label": 314
    },
    {
        "text": "asyncprocess builddetailederrormessage concatenates strings using   in a loop  the method is building a string using lots of concatenation, including in a loop, which will be o(n^2) because a new array will be allocated each time. would be more efficient to use a stringbuilder for this. ",
        "label": 320
    },
    {
        "text": "increase zk maxclientcnxns to give us some head room  it's pretty easy to run out of zk connections on a single host if it's running a master, region server, and a tt with a few slots. just to make it easier for our users, we should set it to something like 100 by default. ",
        "label": 314
    },
    {
        "text": "add mob compact support for asyncadmin    private completablefuture<void> compact(tablename tablename, byte[] columnfamily, boolean major,       compacttype compacttype) {     if (compacttype.mob.equals(compacttype)) {       // todo support mob compact.       return failedfuture(new unsupportedoperationexception(\"mob compact does not support\"));     } we need to support it. ",
        "label": 60
    },
    {
        "text": "don't color the total rit line yellow if it's zero  right now if there are regions in transition, sometimes the rit over 60 seconds line is colored yellow. it shouldn't be colored yellow if there are no regions that have been in transition too long. ",
        "label": 345
    },
    {
        "text": "we no longer wait on hdfs to exit safe mode  we used wait on hdfs to exit safe mode before going on to startup hbase but this feature is broken since we moved out of hadoop contrib. now when you try start with hdfs in safe mode you get: 08/03/21 04:39:56 fatal hbase.hmaster: not starting hmaster because: org.apache.hadoop.ipc.remoteexception: org.apache.hadoop.dfs.safemodeexception: cannot create directory /hbase010. name node is in safe mode. safe mode will be turned off automatically.         at org.apache.hadoop.dfs.fsnamesystem.mkdirsinternal(fsnamesystem.java:1571)         at org.apache.hadoop.dfs.fsnamesystem.mkdirs(fsnamesystem.java:1559)         at org.apache.hadoop.dfs.namenode.mkdirs(namenode.java:422)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) if you are lucky, it appears on stdout/err but may just be stuffed into logs and all looks like its running properly. noticed first by lars george. ",
        "label": 241
    },
    {
        "text": " shell  count shell command to return ruby bindable value   currently if you have a table foo with 5 rows in it, you can run this command: hbase(main):001:0> x = count 'foo' 3100 row(s) in 0.7030 seconds hbase(main):001:0> x nil ideally, the 'x' variable should be 5. so something like this coudl be done: kernel.exit 127 if x != 5 this is useful for having scripts that can check a condition and act. ",
        "label": 248
    },
    {
        "text": "regionserver blocks because of waiting for offsetlock  my regionserver blocks, and all client rpc timeout. i print the regionserver's jstack, it seems a lot of threads were blocked for waiting offsetlock, detail infomation belows: ps: my table's block cache is off \"b.defaultrpcserver.handler=2,queue=2,port=60020\" #82 daemon prio=5 os_prio=0 tid=0x0000000001827000 nid=0x2cdc in object.wait() [0x00007f3831b72000]    java.lang.thread.state: waiting (on object monitor)         at java.lang.object.wait(native method)         at java.lang.object.wait(object.java:502)         at org.apache.hadoop.hbase.util.idlock.getlockentry(idlock.java:79)         - locked <0x0000000773af7c18> (a org.apache.hadoop.hbase.util.idlock$entry)         at org.apache.hadoop.hbase.io.hfile.hfilereaderv2.readblock(hfilereaderv2.java:352)         at org.apache.hadoop.hbase.io.hfile.hfileblockindex$blockindexreader.loaddatablockwithscaninfo(hfileblockindex.java:253)         at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$abstractscannerv2.seekto(hfilereaderv2.java:524)         at org.apache.hadoop.hbase.io.hfile.hfilereaderv2$abstractscannerv2.reseekto(hfilereaderv2.java:572)         at org.apache.hadoop.hbase.regionserver.storefilescanner.reseekatorafter(storefilescanner.java:257)         at org.apache.hadoop.hbase.regionserver.storefilescanner.reseek(storefilescanner.java:173)         at org.apache.hadoop.hbase.regionserver.nonlazykeyvaluescanner.dorealseek(nonlazykeyvaluescanner.java:55)         at org.apache.hadoop.hbase.regionserver.keyvalueheap.generalizedseek(keyvalueheap.java:313)         at org.apache.hadoop.hbase.regionserver.keyvalueheap.requestseek(keyvalueheap.java:269)         at org.apache.hadoop.hbase.regionserver.storescanner.reseek(storescanner.java:695)         at org.apache.hadoop.hbase.regionserver.storescanner.seekasdirection(storescanner.java:683)         at org.apache.hadoop.hbase.regionserver.storescanner.next(storescanner.java:533)         at org.apache.hadoop.hbase.regionserver.keyvalueheap.next(keyvalueheap.java:140)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.populateresult(hregion.java:3889)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.nextinternal(hregion.java:3969)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.nextraw(hregion.java:3847)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.next(hregion.java:3820)         - locked <0x00000005e5c55ad0> (a org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl)         at org.apache.hadoop.hbase.regionserver.hregion$regionscannerimpl.next(hregion.java:3807)         at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:4779)         at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:4753)         at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:2916)         at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:29583)         at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2027)         at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:108)         at org.apache.hadoop.hbase.ipc.rpcexecutor.consumerloop(rpcexecutor.java:114)         at org.apache.hadoop.hbase.ipc.rpcexecutor$1.run(rpcexecutor.java:94)         at java.lang.thread.run(thread.java:745)    locked ownable synchronizers:         - <0x00000005e5c55c08> (a java.util.concurrent.locks.reentrantlock$nonfairsync) ",
        "label": 198
    },
    {
        "text": "failover rpc's for scans  this is extension of hbase-10355 to add failover support for scans. ",
        "label": 139
    },
    {
        "text": "regionstates for regions under region in transition znode are not updated on startup  while testing hbase-11059, saw that if there are regions under region-in-transition znode their states are not updated in meta and master memory on startup. ",
        "label": 242
    },
    {
        "text": "use a random zk client port in unit tests so we can run them in parallel  the hard-coded zk client port has long been a problem for running hbase test suite in parallel. the mini zk cluster should run on a random free port, and that port should be passed to all parts of the unit tests that need to talk to the mini cluster. in fact, randomizing the port exposes a lot of places in the code where a new configuration is instantiated, and as a result the client tries to talk to the default zk client port and times out. the initial fix is for 0.89-fb, where it already allows to run unit tests in parallel in 10 minutes. a fix for the trunk will follow. ",
        "label": 324
    },
    {
        "text": "use slf4j instead of commons logging in new  just added peer procedure classes  ",
        "label": 149
    },
    {
        "text": "integration test and loadtesttool support for cell acls  cell level acls should have an integration test and loadtesttool support. ",
        "label": 544
    },
    {
        "text": "undo prefix tree module as dependency for mapreduce and for datablockencoding  fix dbe so it does lazy loading rather than hard loading of everything mentioned. client depends on prefix-tree module since hbase-7188 and mapreduce will after hbase-7934 ",
        "label": 441
    },
    {
        "text": "deprecate htable interface getroworbefore   htable's getroworbefore(...) internally calls into store.getrowkeyatorbefore. that method was created to allow our scanning of .meta. (see hbase-2600). store.getrowkeyatorbefore(...) lists a bunch of requirements for this to be performant that a user of htable will not be aware of. i propose deprecating this in the public interface in 0.92 and removing it from the public interface in 0.94. if we don't get to hbase-2600 in 0.94 it will still remain as internal interface for scanning meta. comments? ",
        "label": 286
    },
    {
        "text": "update jruby from to   ",
        "label": 314
    },
    {
        "text": "upgrade to curator  curator 4.0 is quite old, it's time to jump to 4.2.0. we should do it in hbase-connectors and hbase-filesystem too. http://curator.apache.org/zk-compatibility.html ",
        "label": 60
    },
    {
        "text": "use copy on write map for region location cache  internally a co-worker profiled their application that was talking to hbase. > 60% of the time was spent in locating a region. this was while the cluster was stable and no regions were moving. to figure out if there was a faster way to cache region location i wrote up a benchmark here: https://github.com/elliottneilclark/benchmark-hbase-cache this tries to simulate a heavy load on the location cache. 24 different threads. 2 deleting location data 2 adding location data using floor to get the result. to repeat my work just run ./run.sh and it should produce a result.csv results:  concurrentskiplistmap is a good middle ground. it's got equal speed for reading and writing. however most operations will not need to remove or add a region location. there will be potentially several orders of magnitude more reads for cached locations than there will be on clearing the cache. so i propose a copy on write tree map. ",
        "label": 154
    },
    {
        "text": "our jruby jar has  gpl jars in it  fix  the latest jruby's complete jar bundles *gpl jars (jna and jffi among others). it looks like the functionality we depend on \u2013 the shell in particular \u2013 makes use of these dirty jars so they are hard to strip. they came in because we (i!) just updated our jruby w/o checking in on what updates contained. jruby has been doing this for a while now (1.1.x added the first lgpl). you have to go all the ways back to the original hbase checkin, hbase-487, of jruby \u2013 1.0.3 \u2013 to get a jruby w/o *gpl jars. plan is to try and revert our jruby all the ways down to 1.0.3 before shipping 0.90.0. thats what this issue is about. we should also look into moving off jruby in the medium to long-term. its kinda awkward sticking on an old version that is no longer supported. i'll open an issue for that. ",
        "label": 314
    },
    {
        "text": "check for snapshot file cleaners on start  snapshots currently use the snaphothfilecleaner and snapshothlogcleaner to ensure that any hfiles or hlogs (respectively) that are currently part of a snapshot are not removed from their respective archive directories (.archive and .oldlogs). from matteo bertozzi: currently the snapshot cleaner is not in hbase-default.xml  and there's no warning/exception on snapshot/restore operation, if not enabled. even if we add the cleaner to the hbase-default.xml how do we ensure that the user doesn't remove it?  do we want to hardcode the cleaner at master startup?  do we want to add a check in snapshot/restore that throws an exception if the cleaner is not enabled? ",
        "label": 309
    },
    {
        "text": "multithreadedtablemapper doesn't work   mapreduce job using multithreadedtablemapper goes down throwing the following exception: java.io.ioexception: java.lang.nosuchmethodexception: org.apache.hadoop.mapreduce.mapper$context.<init>(org.apache.hadoop.conf.configuration, org.apache.hadoop.mapred.taskattemptid, org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$submaprecordreader, org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$submaprecordwriter, org.apache.hadoop.hbase.mapreduce.tableoutputcommitter, org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$submapstatusreporter, org.apache.hadoop.hbase.mapreduce.tablesplit) at org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$maprunner.<init>(multithreadedtablemapper.java:260) at org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper.run(multithreadedtablemapper.java:133) at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:764) at org.apache.hadoop.mapred.maptask.run(maptask.java:370) at org.apache.hadoop.mapred.child$4.run(child.java:255) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:396) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1083) at org.apache.hadoop.mapred.child.main(child.java:249) caused by: java.lang.nosuchmethodexception: org.apache.hadoop.mapreduce.mapper$context.<init>(org.apache.hadoop.conf.configuration, org.apache.hadoop.mapred.taskattemptid, org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$submaprecordreader, org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$submaprecordwriter, org.apache.hadoop.hbase.mapreduce.tableoutputcommitter, org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$submapstatusreporter, org.apache.hadoop.hbase.mapreduce.tablesplit) at java.lang.class.getconstructor0(class.java:2706) at java.lang.class.getconstructor(class.java:1657) at org.apache.hadoop.hbase.mapreduce.multithreadedtablemapper$maprunner.<init>(multithreadedtablemapper.java:241) ... 8 more this occured when the tasks are creating maprunner threads. ",
        "label": 436
    },
    {
        "text": "opening regions on dead server are not reassigned quickly  closed regions are not removed from assignments. i am not sure if it's a general state problem, or just a small bug; for now, one manifestation is that moved region is ignored by ssh of the target server if target server dies before updating zk. 2013-01-22 17:59:00,524 debug [ipc server handler 3 on 50658] master.assignmentmanager(1475): sent close to 10.11.2.92,51231,1358906285048 for region integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. 2013-01-22 17:59:00,997 debug [rs_close_region-10.11.2.92,51231,1358906285048-1] handler.closeregionhandler(167): set region closed state in zk successfully for region integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. sn name: 10.11.2.92,51231,1358906285048 2013-01-22 17:59:01,088 info  [master_close_region-10.11.2.92,50658,1358906192673-0] master.regionstates(242): region {name =&gt; &apos;integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.&apos;, startkey =&gt; &apos;66666660&apos;, endkey =&gt; &apos;7333332c&apos;, encoded =&gt; 0200b366bc37c5afd1185f7d487c7dfb,} transitioned from {integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. state=closed, ts=1358906341087, server=null} to {integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. state=offline, ts=1358906341088, server=null} 2013-01-22 17:59:01,128 info  [master_close_region-10.11.2.92,50658,1358906192673-0] master.assignmentmanager(1596): assigning region integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. to 10.11.2.92,50661,1358906192942 ... (50661 didn't update zk to open, only opening) 2013-01-22 17:59:06,605 info  [master_server_operations-10.11.2.92,50658,1358906192673-2] handler.servershutdownhandler(202): reassigning 7 region(s) that 10.11.2.92,50661,1358906192942 was carrying (skipping 0 regions(s) that are already in transition) 2013-01-22 17:59:06,605 debug [master_server_operations-10.11.2.92,50658,1358906192673-2] handler.servershutdownhandler(219): skip assigning region integrationtestrebalanceandkillserverstargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. because it has been opened in 10.11.2.92,51231,1358906285048 note the server in the last line - the one that has long closed the region. ",
        "label": 242
    },
    {
        "text": "testadmin hanging on hudson  testadmin is hanging in the enable/disable exercising code. ",
        "label": 314
    },
    {
        "text": "implement flush into stripes in stripe compactions  flush will be able to flush into multiple files under this design, avoiding l0 i/o amplification.  i have the patch which is missing just one feature - support for concurrent flushes and stripe changes. this can be done via extensive try-locking of stripe changes and flushes, or advisory flags without blocking flushes, dumping conflicting flushes into l0 in case of (very rare) collisions. for file loading for the latter, a set-cover-like problem needs to be solved to determine optimal stripes. that will also address jimmy's concern of getting rid of metadata, btw. however currently i don't have time for that. i plan to attach the try-locking patch first, but this won't happen for a couple weeks probably and should not block main reviews. hopefully this will be added on top of main reviews. ",
        "label": 406
    },
    {
        "text": "testheapsize failing for me on latest trunk  might be related to the fact i'm running 1.6.0_14 now. 2009-07-21 01:19:16,314 debug [main] util.classsize(239): primitives 33, arrays 0, references (includes 2 for object overhead) 22, refsize 8, size 216 testcase: testnativesizes took 0.004 sec testcase: testsizes took 0.065 sec failed expected:<216> but was:<224> junit.framework.assertionfailederror: expected:<216> but was:<224> at org.apache.hadoop.hbase.io.testheapsize.testsizes(testheapsize.java:266) testsuite: org.apache.hadoop.hbase.io.testheapsize tests run: 2, failures: 1, errors: 0, time elapsed: 0.073 sec ------------- standard error ----------------- 2009-07-21 01:19:16,301 debug [main] util.classsize(213): closed class java.util.concurrent.atomic.atomicboolean 2009-07-21 01:19:16,303 debug [main] util.classsize(213): closing class java.util.concurrent.atomic.atomicboolean 2009-07-21 01:19:16,303 debug [main] util.classsize(213): historian class org.apache.hadoop.hbase.regionhistorian 2009-07-21 01:19:16,304 debug [main] util.classsize(213): lockstorows interface java.util.map 2009-07-21 01:19:16,304 debug [main] util.classsize(213): stores interface java.util.map 2009-07-21 01:19:16,305 debug [main] util.classsize(213): memstoresize class java.util.concurrent.atomic.atomiclong 2009-07-21 01:19:16,306 debug [main] util.classsize(213): basedir class org.apache.hadoop.fs.path 2009-07-21 01:19:16,306 debug [main] util.classsize(213): log class org.apache.hadoop.hbase.regionserver.hlog 2009-07-21 01:19:16,307 debug [main] util.classsize(213): fs class org.apache.hadoop.fs.filesystem 2009-07-21 01:19:16,307 debug [main] util.classsize(213): conf class org.apache.hadoop.hbase.hbaseconfiguration 2009-07-21 01:19:16,308 debug [main] util.classsize(213): regioninfo class org.apache.hadoop.hbase.hregioninfo 2009-07-21 01:19:16,308 debug [main] util.classsize(213): regiondir class org.apache.hadoop.fs.path 2009-07-21 01:19:16,309 debug [main] util.classsize(213): regioncompactiondir class org.apache.hadoop.fs.path 2009-07-21 01:19:16,309 debug [main] util.classsize(213): comparator class org.apache.hadoop.hbase.keyvalue$kvcomparator 2009-07-21 01:19:16,310 debug [main] util.classsize(213): forcemajorcompaction boolean 2009-07-21 01:19:16,310 debug [main] util.classsize(213): writestate class org.apache.hadoop.hbase.regionserver.hregion$writestate 2009-07-21 01:19:16,311 debug [main] util.classsize(213): memstoreflushsize int 2009-07-21 01:19:16,311 debug [main] util.classsize(213): lastflushtime long 2009-07-21 01:19:16,312 debug [main] util.classsize(213): flushlistener interface org.apache.hadoop.hbase.regionserver.flushrequester 2009-07-21 01:19:16,312 debug [main] util.classsize(213): blockingmemstoresize int 2009-07-21 01:19:16,312 debug [main] util.classsize(213): threadwakefrequency long 2009-07-21 01:19:16,313 debug [main] util.classsize(213): splitsandcloseslock class java.util.concurrent.locks.reentrantreadwritelock 2009-07-21 01:19:16,313 debug [main] util.classsize(213): newscannerlock class java.util.concurrent.locks.reentrantreadwritelock 2009-07-21 01:19:16,313 debug [main] util.classsize(213): updateslock class java.util.concurrent.locks.reentrantreadwritelock 2009-07-21 01:19:16,314 debug [main] util.classsize(213): splitlock class java.lang.object 2009-07-21 01:19:16,314 debug [main] util.classsize(213): minsequenceid long 2009-07-21 01:19:16,314 debug [main] util.classsize(239): primitives 33, arrays 0, references (includes 2 for object overhead) 22, refsize 8, size 216 testcase: testnativesizes took 0.004 sec testcase: testsizes took 0.065 sec failed expected:<216> but was:<224> junit.framework.assertionfailederror: expected:<216> but was:<224> at org.apache.hadoop.hbase.io.testheapsize.testsizes(testheapsize.java:266) ",
        "label": 247
    },
    {
        "text": "prefix compression   trie data block encoding  hbase common and hbase server changes  these are the hbase-common and hbase-server changes for hbase-4676 prefix compression - trie data block encoding. ",
        "label": 307
    },
    {
        "text": "fix and reenable testlogrolling testlogrollonpipelinerestart  hbase-5984 disabled this flakey test (see the issue for more). this issue is about getting it enabled again. made a blocker on 0.96.0 so it gets attention. ",
        "label": 155
    },
    {
        "text": "hbaseadmin createtableasync  should check for invalid split keys   ",
        "label": 441
    },
    {
        "text": " replication  when transferring queues  check if the peer still exists before copying the znodes  right now it's a pain if you remove a peer and still have rogue queues because they get moved on and on and on. nodefailoverworker needs to run the check: if (!zkhelper.getpeerclusters().containskey(src.getpeerclusterid())) { before this: sortedmap<string, sortedset<string>> newqueues = zkhelper.copyqueuesfromrs(rsznode); and test. ",
        "label": 229
    },
    {
        "text": "hbase unsafeavailchecker returns false on arm64  arm64v8 supports unaligned access .  but unsafeavailchecker returns false due to a jdk bug.  the false of unsafeavailchecker return also causes the hbase unit tests(fuzzyrowfilter, testfuzzyrowfilterendtoend, testfuzzyrowandcolumnrangefilter) failures.   enable arm64 unaligned support by providing a hard-code workaround for the jdk bug. ",
        "label": 510
    },
    {
        "text": "when new master joins running cluster does  received report from unknown server   telling it to stop regionserver   working on it. ",
        "label": 314
    },
    {
        "text": " performance regression  appendnosync hbase doesn't take deferred log flush into account  since we upgraded to 0.94.1 from 0.92 i saw that our icvs are about twice as slow as they were. jstack'ing i saw that most of the time we are waiting on sync()... but those tables have deferred log flush turned on so they shouldn't even be calling it. htd.isdeferredlogflush is currently only called in the append() methods which are pretty much not in use anymore. ",
        "label": 229
    },
    {
        "text": "crash when attempting split with delimitedkeyprefixregionsplitpolicy  i am using delimitedkeyprefixregionsplitpolicy on my table. when i attempted to split the table from the web interface, it gives the error below. i believe the problem is org.apache.hadoop.hbase.regionserver.delimitedkeyprefixregionsplitpolicy.getsplitpoint:75 - the call from super.getsplitpoint() (line 71) returns null and this condition is not checked. ------- http error 500 problem accessing /table.jsp. reason:     java.io.ioexception: java.lang.nullpointerexception: array at com.google.common.base.preconditions.checknotnull(preconditions.java:204) at com.google.common.primitives.bytes.indexof(bytes.java:115) at org.apache.hadoop.hbase.regionserver.delimitedkeyprefixregionsplitpolicy.getsplitpoint(delimitedkeyprefixregionsplitpolicy.java:75) at org.apache.hadoop.hbase.regionserver.hregion.checksplit(hregion.java:5697) at org.apache.hadoop.hbase.regionserver.hregionserver.splitregion(hregionserver.java:3279) at sun.reflect.generatedmethodaccessor60.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:601) at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:320) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426) caused by: org.apache.hadoop.ipc.remoteexception: java.io.ioexception: java.lang.nullpointerexception: array at com.google.common.base.preconditions.checknotnull(preconditions.java:204) at com.google.common.primitives.bytes.indexof(bytes.java:115) at org.apache.hadoop.hbase.regionserver.delimitedkeyprefixregionsplitpolicy.getsplitpoint(delimitedkeyprefixregionsplitpolicy.java:75) at org.apache.hadoop.hbase.regionserver.hregion.checksplit(hregion.java:5697) at org.apache.hadoop.hbase.regionserver.hregionserver.splitregion(hregionserver.java:3279) at sun.reflect.generatedmethodaccessor60.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:601) at org.apache.hadoop.hbase.ipc.writablerpcengine$server.call(writablerpcengine.java:320) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1426) at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:995) at org.apache.hadoop.hbase.ipc.writablerpcengine$invoker.invoke(writablerpcengine.java:86) at com.sun.proxy.$proxy11.splitregion(unknown source) at org.apache.hadoop.hbase.client.hbaseadmin.split(hbaseadmin.java:1620) at org.apache.hadoop.hbase.client.hbaseadmin.split(hbaseadmin.java:1608) at org.apache.hadoop.hbase.client.hbaseadmin.split(hbaseadmin.java:1565) at org.apache.hadoop.hbase.client.hbaseadmin.split(hbaseadmin.java:1552) at org.apache.hadoop.hbase.generated.master.table_jsp._jspservice(table_jsp.java:94) at org.apache.jasper.runtime.httpjspbase.service(httpjspbase.java:98) at javax.servlet.http.httpservlet.service(httpservlet.java:820) at org.mortbay.jetty.servlet.servletholder.handle(servletholder.java:511) at org.mortbay.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1221) at org.apache.hadoop.http.httpserver$quotinginputfilter.dofilter(httpserver.java:835) at org.mortbay.jetty.servlet.servlethandler$cachedchain.dofilter(servlethandler.java:1212) at org.mortbay.jetty.servlet.servlethandler.handle(servlethandler.java:399) at org.mortbay.jetty.security.securityhandler.handle(securityhandler.java:216) at org.mortbay.jetty.servlet.sessionhandler.handle(sessionhandler.java:182) at org.mortbay.jetty.handler.contexthandler.handle(contexthandler.java:766) at org.mortbay.jetty.webapp.webappcontext.handle(webappcontext.java:450) at org.mortbay.jetty.handler.contexthandlercollection.handle(contexthandlercollection.java:230) at org.mortbay.jetty.handler.handlerwrapper.handle(handlerwrapper.java:152) at org.mortbay.jetty.server.handle(server.java:326) at org.mortbay.jetty.httpconnection.handlerequest(httpconnection.java:542) at org.mortbay.jetty.httpconnection$requesthandler.headercomplete(httpconnection.java:928) at org.mortbay.jetty.httpparser.parsenext(httpparser.java:549) at org.mortbay.jetty.httpparser.parseavailable(httpparser.java:212) at org.mortbay.jetty.httpconnection.handle(httpconnection.java:404) at org.mortbay.io.nio.selectchannelendpoint.run(selectchannelendpoint.java:410) at org.mortbay.thread.queuedthreadpool$poolthread.run(queuedthreadpool.java:582) ",
        "label": 379
    },
    {
        "text": " dac  authentication  follow what hadoop is doing. authentication via jaas:   http://issues.apache.org/jira/browse/hadoop-6299  http://java.sun.com/javase/6/docs/technotes/guides/security/jaas/jaasrefguide.html should support kerberos, unix, and ldap authentication options. integrate with authentication mechanisms for ipc and hdfs. ",
        "label": 180
    },
    {
        "text": " performance  make splits faster  regionservers run splits. they close the region to split, divide it, and then tell master about the two new regions. master then assigns new regions. new regions need to come up in new locations. both regions are offline during this time. instead, regionserver might run split as it does now but new, deploy the lower-half on the current regionserver immediately. it'd then inform master that it had split, and that it was serving the lower half. master would then take care of assigning the upper half. benefits would be that clients who were accessing the lower half of the split would not need to go through recalibration. they'd just keep working. there'd be disruption for those keys that landed in the top half of the split only. ",
        "label": 314
    },
    {
        "text": "hadoopqa not running findbugs  trunk   hadoopqa shows like  -1 findbugs. the patch appears to cause findbugs (version 1.3.9) to fail.  but not able to see any reports link when i checked the console output for the build i can see [info] --- findbugs-maven-plugin:2.4.0:findbugs (default-cli) @ hbase-common --- [info] fork value is true [info] ------------------------------------------------------------------------ [info] reactor summary: [info]  [info] hbase ............................................. success [1.890s] [info] hbase - common .................................... failure [2.238s] [info] hbase - server .................................... skipped [info] hbase - assembly .................................. skipped [info] hbase - site ...................................... skipped [info] ------------------------------------------------------------------------ [info] build failure [info] ------------------------------------------------------------------------ [info] total time: 4.856s [info] finished at: thu may 31 03:35:35 utc 2012 [info] final memory: 23m/154m [info] ------------------------------------------------------------------------ [error] could not find resource '${parent.basedir}/dev-support/findbugs-exclude.xml'. -> [help 1] [error]  because of this error findbugs is getting run! ",
        "label": 236
    },
    {
        "text": "move jruby jar so only on hbase shell module classpath  currently globally available  a suggestion that came up out of internal issue (filed by mr jan van besien) was to move the scope of the jruby include down so it is only a dependency for the hbase-shell. jruby jar brings in a bunch of dependencies (joda time for example) which can clash with the includes of others. our sean suggests that could be good to shut down exploit possibilities if jruby was not globally available. only downside i can think is that it may no longer be available to our bin/*rb scripts if we move the jar but perhaps these can be changed so they can find the ruby jar in new location. ",
        "label": 490
    },
    {
        "text": "boundedpriorityblockingqueue poll  should check the return value from awaitnanos   nanos represents the timeout value.       while (queue.size() == 0 && nanos > 0) {         notempty.awaitnanos(nanos);       } the return value from awaitnanos() should be checked - otherwise we may wait for period longer than the timeout value. ",
        "label": 413
    },
    {
        "text": "hbase shell move and online may be unusable if region name or server includes binary encoded data  similar to hbase-4115, this entails a conversion of org.apache.hadoop.hbase.utils.bytes.tobytes to a to_java_bytes call in the 'move' and 'online' call. ",
        "label": 248
    },
    {
        "text": " availability  skip recovered edits files with edits we know older than what region currently has  testing 0.92, i crashed all servers out. another bug makes it so wals are not getting cleaned so i had 7000 regions to replay. the distributed split code did a nice job and cluster came back but interesting is that some hot regions ended up having loads of recovered.edits files \u2013 tens if not hundreds \u2013 to replay against the region (can we bulk load recovered.edits instead of replaying them?). each recovered.edits file is taking about a second to process (though only about 30 odd edits per file it seems). the region is unavailable during this time. ",
        "label": 242
    },
    {
        "text": " c  the library should be named libhbaseclient  instead of libhbaseclient  some people get sticky about names, and it seems that the \"correct\" library name should be libh*b*aseclient. this seems really easy, and if it needs to be switched, we should just do it. ",
        "label": 155
    },
    {
        "text": "add more clarity to reference guide related to importing eclipse formatter  in hbase reference guide:   141.1.1. code formatting  in procedure bullet point 2: it is not clear what the menu item is.   it should be changed to the following:  \"in preferences, click java->code style->formatter\" ",
        "label": 390
    },
    {
        "text": "blockcache contents report  summarized block-cache report for a regionserver would be helpful. for example ... table1  cf1 100 blocks, totalbytes=yyyyy, averagetimeincache=xxxx hours  cf2 200 blocks, totalbytes=zzzzz, averagetimeincache=xxxx hours table2  cf1 75 blocks, totalbytes=yyyyy, averagetimeincache=xxxx hours  cf2 150 blocks, totalbytes=zzzzz, averagetimeincache=xxxx hours ... etc. the current metrics list blockcachesize and blockcachefree, but there is no way to know what's in there. any single block isn't really important, but the patterns of what cf/table they came from, how big are they, and how long (on average) they've been in the cache, are important. no such interface exists in hregioninterface. but i think it would be helpful from an operational perspective. updated (7-29): removing suggestion for ui. i would be happy just to get this report on a configured interval dumped to a log file. ",
        "label": 314
    },
    {
        "text": "remove replicationqueuesclient  use replicationqueuestorage directly  ",
        "label": 149
    },
    {
        "text": "hbase should run on both secure and vanilla versions of hadoop  there have been a couple cases recently of folks trying to run hbase trunk (or 0.89 drs) on cdh3b3 or secure hadoop. while hbase security is in the works, it currently only runs on secure hadoop versions. meanwhile hbase trunk won't compile on secure hadoop due to backward incompatible changes in org.apache.hadoop.security.usergroupinformation. this issue is to work out the minimal set of changes necessary to allow hbase to build and run on both secure and non-secure versions of hadoop. though, with secure hadoop, i don't even think it's important to target running with hdfs security enabled (and krb authentication). just allow hbase to build and run in both versions. i think mainly this amounts to abstracting usage of usergroupinformation and unixusergroupinformation. ",
        "label": 314
    },
    {
        "text": "revert hbase  it undoes hbc create  see this thread: http://search-hadoop.com/m/waxxv1oq1qy/what+is+hbaseconfiguration.create%2528configuration%2529+good+for%253f&subj=what+is+hbaseconfiguration+create+configuration+good+for+ ",
        "label": 314
    },
    {
        "text": "expose basic information about the master status through jmx beans  similar to the namenode and jobtracker, it would be good if the hbase master could expose some information through mbeans. ",
        "label": 201
    },
    {
        "text": "filter transform  always applies unconditionally  even when combined in a filterlist  filterlist.transform() applies all transformation from all filters unconditionally.  this leads to surprising results, eg. with combinations such as  (family=x and column=y and keyonlyfilter) or ...  the keyonlyfilter will strip the values from all keyvalues. ",
        "label": 104
    },
    {
        "text": " hbck2 hbase operator tools  create an assembly that builds an hbase operator tools tgz  was going to build a convenience binary tgz as part of the first release of hbase-operator-tools. not sure how just yet; best would be if it were a fatjar with all dependencies but that'd be kinda insane at same time since the tgz would be massive. ",
        "label": 314
    },
    {
        "text": "create an integration test for online schema change  with table locks in place it should be time to start really testing online table schema changes. ",
        "label": 154
    },
    {
        "text": "add metric on short circuit reads  got this from a colin message this afternoon: \"there are hdfs statistics that hbase could be checking by calling dfsinputstream#getreadstatistics. this tells you how many of your reads have been remote, local, short-circuit, etc. you could file an hbase jira for them to roll those up into the hbase stats. seems like a good idea to me.\" ",
        "label": 314
    },
    {
        "text": "remove replicationqueuesclient class replicationqueues class config and remove table based replicationqueuesclient replicationqueues implementation  when implement the procedure of replication admin operations, we abstract a replication storage interface in hbase-19543. so replicationqueues/replicationqueuesclient are not used anymore. these interface are ia.private. so it is ok to remove them. but there are two config: hbase.region.replica.replication.replicationqueues.class and hbase.region.replica.replication.replicationqueuesclient.class in replicationfactory. these configs were introduced by hbase-15867, which only in 2.0. and the feature development is not active now. in the future, we can implement the table based replication to replication storage interface. so let's remove them before release 2.0. see more details in the discussion of hbase-19573. ",
        "label": 187
    },
    {
        "text": "handle case where we open meta  root has been closed but znode location not deleted yet  and try to update meta location in root  carrying over from hbase-3159, i ran into a case with testrollingrestart with meta and root failing concurrently. this is how it played out: meta is closed on an rs that has been stopped: 2010-10-27 22:47:27,709 debug [rs_close_meta-192.168.0.44,59709,1288244829038-0] handler.closeregionhandler(128): closed region .meta.,,1.1028785192 then root is closed on a different rs that has been stopped: 2010-10-27 22:47:28,863 debug [rs_close_root-192.168.0.44,59662,1288244792380-0] handler.closeregionhandler(128): closed region -root-,,0.70236052 a running rs is assigned meta (the master isn't even aware yet that root has been closed, it is processing shutdown for rs 59709 but not yet received expired node for 59662): 2010-10-27 22:47:29,095 debug [master_meta_server_operations-192.168.0.44:59629-0] master.assignmentmanager(908): no previous transition plan for .meta.,,1.1028785192 so generated a random one; hri=.meta.,,1.1028785192, src=, dest=192.168.0.44,59735,1288244843993; 3 (online=3, exclude=null) available servers 2010-10-27 22:47:29,095 debug [master_meta_server_operations-192.168.0.44:59629-0] master.assignmentmanager(802): assigning region .meta.,,1.1028785192 to 192.168.0.44,59735,1288244843993 ... 2010-10-27 22:47:29,123 debug [rs_open_meta-192.168.0.44,59735,1288244843993-0] handler.openregionhandler(69): processing open of .meta.,,1.1028785192 after finishing the open of meta, the rs goes to update location in root and gets: 2010-10-27 22:47:29,208 error [rs_open_meta-192.168.0.44,59735,1288244843993-0] executor.eventhandler(154): caught throwable while processing event m_rs_open_meta java.lang.nullpointerexception: no server for -root- at org.apache.hadoop.hbase.catalog.metaeditor.updatemetalocation(metaeditor.java:127) at org.apache.hadoop.hbase.regionserver.hregionserver.postopendeploytasks(hregionserver.java:1271) at org.apache.hadoop.hbase.regionserver.handler.openregionhandler.process(openregionhandler.java:156) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:151) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:680) this doesn't actually kill the rs, it's just a caught exception up in the generic eventhandler. but we get left in a weird state. eventually master does the right thing and times-out the opening: 2010-10-27 22:48:15,694 info  [192.168.0.44:59629.timeoutmonitor] master.assignmentmanager$timeoutmonitor(1444): regions in transition timed out:  .meta.,,1.1028785192 state=pending_open, ts=1288244865700 2010-10-27 22:48:15,694 info  [192.168.0.44:59629.timeoutmonitor] master.assignmentmanager$timeoutmonitor(1454): region has been pending_open for too long, reassigning region=.meta.,,1.1028785192 but it chooses to assign it back to the same person because the plan is still there: 2010-10-27 22:48:15,702 debug [192.168.0.44:59629.timeoutmonitor] master.assignmentmanager(916): using preexisting plan=hri=.meta.,,1.1028785192, src=, dest=192.168.0.44,59735,1288244843993 2010-10-27 22:48:15,702 debug [192.168.0.44:59629.timeoutmonitor] master.assignmentmanager(802): assigning region .meta.,,1.1028785192 to 192.168.0.44,59735,1288244843993 but then the rs doesn't open it because it's actually already open on that server. we fail the root edit but then don't close the region out. 2010-10-27 22:48:15,805 info  [ipc server handler 3 on 59735] regionserver.hregionserver(1952): received request to open region: .meta.,,1.1028785192 2010-10-27 22:48:15,807 debug [rs_open_meta-192.168.0.44,59735,1288244843993-0] handler.openregionhandler(69): processing open of .meta.,,1.1028785192 2010-10-27 22:48:15,807 warn  [rs_open_meta-192.168.0.44,59735,1288244843993-0] handler.openregionhandler(84): attempting open of .meta.,,1.1028785192 but it's already online on this server this continues indefinitely, once every minute. 1. address the race condition when we get the connection to the root server (could exist for meta too). the blocking call thinks we have a location but then when we get the cached location and don't get one. 2. if we do get this npe writing to root (or maybe meta too), we should not just throw the exception all the way up to the eventhandler and log it and continue. that just stops our meta_open in it's tracks. we complete the open but just not the edit. we don't roll-back in any way. 3. if the master is assigning stuff out and a region says, hey, i'm already hosting this region... something must be up. in this case, it would not have been good for the rs to tell the master that it was already hosting it because it was missing the root edit. so maybe if this happens, the master asks the rs to close the region in question? dunno. probably more issues to think about around this this seems to be extremely rare. i have been running this testrollingrestart script constantly and this only happens when i do a concurrent kill of the server hosting root and then server hosting meta, and then only sometimes, it does work more times than not. ",
        "label": 247
    },
    {
        "text": " hbase  stuck replaying the edits of crashed machine  rapleaf master got stuck trying to replay the logs of the server holding the .meta. region. here are pertinent log excerpts: 2008-01-12 02:17:42,621 debug org.apache.hadoop.hbase.hlog: creating new log file writer for path /data/hbase1/hregion_1679905157/oldlogfile.log; map content {spider_pages,25_530417241,1200073704087=org.apache.hadoop.io.sequencefile$recordcompresswriter@24336556, spider_pages,6_74488371,1200029312876=org.apache.had oop.io.sequencefile$recordcompresswriter@2a4203ab, spider_pages,2_561473281,1200054637086=org.apache.hadoop.io.sequencefile$recordcompresswriter@b972625, .meta.,,1=org.apache.hadoop.io.sequencefile$recordcompresswriter@67a044a7, spider_pages,5_544278041,1199025825074=org.apache.hadoop.io.sequencefile$recordcompress writer@42be0008, spider_pages,49_567090611,1200028378117=org.apache.hadoop.io.sequencefile$recordcompresswriter@7bf4cbaa, spider_pages,5_566039401,1200058871594=org.apache.hadoop.io.sequencefile$recordcompresswriter@16479e88, spider_pages,59_360738971,1200073647952=org.apache.hadoop.io.sequencefile$recordcompresswr iter@70494d14, spider_pages,59_302628011,1200073647951=org.apache.hadoop.io.sequencefile$recordcompresswriter@654670a8} 2008-01-12 02:17:44,124 debug org.apache.hadoop.hbase.hlog: applied 20000 edits 2008-01-12 02:17:49,076 debug org.apache.hadoop.hbase.hlog: applied 30000 edits 2008-01-12 02:17:49,078 debug org.apache.hadoop.hbase.hlog: applied 30003 total edits 2008-01-12 02:17:49,078 debug org.apache.hadoop.hbase.hlog: splitting 1 of 2: hdfs://tf1:7276/data/hbase1/log_xx.xx.xx.32_1200011947645_60020/hlog.dat.003 2008-01-12 02:17:52,574 debug org.apache.hadoop.hbase.hlog: applied 10000 edits 2008-01-12 02:17:59,822 warn org.apache.hadoop.hbase.hmaster: processing pending operations: processservershutdown of xx.xx.xx.32:60020 java.io.eofexception         at java.io.datainputstream.readfully(datainputstream.java:180)         at org.apache.hadoop.io.dataoutputbuffer$buffer.write(dataoutputbuffer.java:56)         at org.apache.hadoop.io.dataoutputbuffer.write(dataoutputbuffer.java:90)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1763)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1663)         at org.apache.hadoop.io.sequencefile$reader.next(sequencefile.java:1709)         at org.apache.hadoop.hbase.hlog.splitlog(hlog.java:168)         at org.apache.hadoop.hbase.hmaster$processservershutdown.process(hmaster.java:2144)         at org.apache.hadoop.hbase.hmaster.run(hmaster.java:1056) 2008-01-12 02:17:59,822 debug org.apache.hadoop.hbase.hmaster: main processing loop: processservershutdown of xx.xx.xx.32:60020 it keeps doing the above over and over again. i suppose we could skip bad logs... or just shut down master w/ a reason why. odd is that we seem to be well into the file \u2013 we've run over 10000 edits... before we trip over the eof. i've asked for an fsck to see what that says. ",
        "label": 241
    },
    {
        "text": "disable testslabcache and testsinglesizedcache temporarily to see if these are cause of build box failure though all tests pass  ",
        "label": 289
    },
    {
        "text": "backport hbase to branch  in discussion of hbase-4552 / hbase-4677 there has been some discussion about whether and how to backport hbase-4552 to the 0.90 branch. this is a potentially compatibility breaking so several approaches hav ebeen suggested. 1) provide patch but do not integrate  2) integrate patch that extends and deprecates old api without removing old api. it has been argued that clients are supposed to use loadincrementalhfiles api and not at the internal hregionserver rpc api. ",
        "label": 248
    },
    {
        "text": "re enable testrsgroupskillrs testlowermetagroupversion  it is not allowed to change the table descriptor for system tables, especially that the table descriptor for meta table is hard coded. we need to find a way to deal with this. ",
        "label": 149
    },
    {
        "text": "region would be assigned twice easily with continually killing server and moving region in testing environment  before assigning region in servershutdownhandler#process, it will check whether region is in rit,  however, this checking doesn't work as the excepted in the following case:  1.move region a from server b to server c  2.kill server b  3.start server b immediately let's see what happen in the code for the above case for step1: 1.1 server b close the region a, 1.2 master setoffline for region a,(assignmentmanager#setoffline:this.regions.remove(regioninfo)) 1.3 server c start to open region a.(not completed) for step3: master servershutdownhandler#process() for server b { .. splitlog() ... list<regionstate> regionsintransition =         this.services.getassignmentmanager()         .processservershutdown(this.servername); ... skip regions that were in transition unless closing or pending_close ... assign region } in fact, when running servershutdownhandler#process()#this.services.getassignmentmanager().processservershutdown(this.servername), region a is in rit (step1.3 not completed), but the return list<regionstate> regionsintransition doesn't contain it, because region a has removed from assignmentmanager.regions by assignmentmanager#setoffline in step 1.2  therefore, region a will be assigned twice. actually, one server killed and started twice will also easily cause region assigned twice.  exclude the above reason, another probability :   when execute servershutdownhandler#process()#metareader.getserveruserregions ,region is included which is in rit now.  but after completing metareader.getserveruserregions, the region has been opened in other server and is not in rit now. in our testing environment where balancing,moving and killing are executed periodly, assigning region twice often happens, and it is hateful because it will affect other test cases. ",
        "label": 107
    },
    {
        "text": "javadoc cleanup of to reflect  meta  rename to hbase meta  ",
        "label": 330
    },
    {
        "text": "introduce metaeditor method that both adds and deletes rows in  meta  table  in review of hbase-7365, metaeditor.deleteregions() and metaeditor.addregionstometa() are used in restoresnapshothandler.java.handletableoperation() to apply changes to .meta. i made following suggestion: can we introduce new method in metaeditor which takes list of mutation's ?  the delete and put would be grouped and then written to .meta. table in one transaction. jon responded: i like that idea \u2013 then the todo/warning or follow on could refer to that method. when we fix it, it could get used in other multi row meta modifications like splits and table creation/deletion in general. see https://reviews.apache.org/r/8674/ ",
        "label": 309
    },
    {
        "text": "hbck should deal with partial failures better   the first cut of hbase-5128 failed fast and tried to avoid cases handling partial recovery failures. we can improve this. collecting faults and recovering from them handling rejectedexectionexception exceptions when using executor.execute. ",
        "label": 248
    },
    {
        "text": "separate the avro schema definition file from the code  the avro schema files are in the src/main/java path, but should be in /src/main/resources just like the hbase.thrift is. makes the separation the same and cleaner. ",
        "label": 21
    },
    {
        "text": "update the reference guide to reflect the changes in the security profile  the refguide needs to be updated to reflect the fact that there is no security profile anymore, etc. [follow up to hbase-5732] ",
        "label": 139
    },
    {
        "text": "admin  et al  not accurate with column vs  column family usage  consider the classes admin and hcolumndescriptor. hcolumndescriptor is really referring to a \"column family\" and not a \"column\" (i.e., family:qualifer). likewise, in admin there is a method called \"addcolumn\" that takes an hcolumndescriptor instance. i labeled this a bug in the sense that it produces conceptual confusion because there is a big difference between a column and column-family in hbase and these terms should be used consistently. the code works, though. ",
        "label": 284
    },
    {
        "text": "hbase shell truncate  hbase shell should allow for the truncation of tables, similar to the functionality provided by hql ",
        "label": 229
    },
    {
        "text": "components definition  are there any plans to separate them? i'd suggest, as described below.  i also think mapred package should be clarified their role and responsibilities to ensure their effectiveness. table. component list table cont'd component/s unknown documentation examples hql io ipc mapred rest thrift scripts test util ",
        "label": 241
    },
    {
        "text": "harmonize the get and delete operations  in my work on hbase-2400, implementing deletes for the avro server felt quite awkward. rather than the clean api of the get object, which allows restrictions on the result set from a row to be expressed with addcolumn, addfamily, settimestamp, settimerange, setmaxversions, and setfilters, the delete object hides these semantics behind various constructors to deletecolumn[s] an deletefamily. from my naive vantage point, i see no reason why it would be a bad idea to mimic the get api exactly, though i could quite possibly be missing something. thoughts? ",
        "label": 314
    },
    {
        "text": " replication  inconsistency between memstore and wal may result in data in remote cluster that is not in the origin  looks like the current write path can cause inconsistency between memstore/hfile and wal which cause the slave cluster has more data than the master cluster. the simplified write path looks like:  1. insert record into memstore  2. write record to wal  3. sync wal  4. rollback memstore if 3 fails it's possible that the hdfs sync rpc call fails, but the data is already (may partially) transported to the dns which finally get persisted. as a result, the handler will rollback the memstore and the later flushed hfile will also skip this record. ================================== this is a long lived issue. the above problem is solved by write path reorder, as now we will sync wal first before modifying memstore. but the problem may still exists as replication thread may read the new data before we return from hflush. see this document for more details: https://docs.google.com/document/d/11aywtghitqs6vslrix32pwtxmby3libxwgxi25obvey/edit# so we need to keep a sync length in wal and tell replication wal reader this is limit when you read this wal file. ",
        "label": 149
    },
    {
        "text": "python based compatiblity checker fails if git repo does not have a remote named 'origin'  the new python based compatibility checker will fail if the local git repo does not have a remote named \"origin\". i develop with multiple upstream repos and rename them according to a custom convention. if the requirement that an upstream named \"origin\" must be present could be removed, that would be good, or otherwise this should be documented next to the example usage in the python source. ",
        "label": 402
    },
    {
        "text": "add a costless notifications mechanism from master to regionservers   clients  t would be very useful to add a mechanism to distribute some information to the clients and regionservers. especially it would be useful to know globally (regionservers + clients apps) that some regionservers are dead. this would allow: to lower the load on the system, without clients using staled information and going on dead machines to make the recovery faster from a client point of view. it's common to use large timeouts on the client side, so the client may need a lot of time before declaring a region server dead and trying another one. if the client receives the information separatly about a region server states, it can take the right decision, and continue/stop to wait accordingly. we can also send more information, for example instructions like 'slow down' to instruct the client to increase the retries delay and so on.  technically, the master could send this information. to lower the load on the system, we should: have a multicast communication (i.e. the master does not have to connect to all servers by tcp), with once packet every 10 seconds or so. receivers should not depend on this: if the information is available great. if not, it should not break anything. it should be optional. so at the end we would have a thread in the master sending a protobuf message about the dead servers on a multicast socket. if the socket is not configured, it does not do anything. on the client side, when we receive an information that a node is dead, we refresh the cache about it. ",
        "label": 340
    },
    {
        "text": "unnecessary hqlclient object creation in a shell loop   shell.java while(....) {   hqlclient hql = new hqlclient(...); } ",
        "label": 152
    },
    {
        "text": "improve speed of table creation  in createtablehandler  for (int regionidx = 0; regionidx < this.newregions.length; regionidx++) {  hregioninfo newregion = this.newregions[regionidx];  // 1. create hregion  hregion region = hregion.createhregion(newregion,  this.filesystemmanager.getrootdir(), this.conf,  this.htabledescriptor, null, false, true);  regioninfos.add(region.getregioninfo());  if (regionidx % batchsize == 0) { // 2. insert into meta metaeditor.addregionstometa(this.catalogtracker, regioninfos); regioninfos.clear(); } // 3. close the new region to flush to disk. close log file too.  region.close();  }  all the region will be create serially.  if we have thousands of regions, that will be a huge cost.  we can improve it by create the region in parallel ",
        "label": 518
    },
    {
        "text": "visibility into zookeeper  currently, to the users of hbase zookeeper is some mysterious thing that is for some reason important but no one knows why. we need to provide more visibility into zookeeper from hbase. this issue is to start a discussion on what sorts of things we can/should provide. some possible ideas to get started: running a rest server. see zookeeper-36. fuse support. see zookeeper-25. admin/surgery tools in hbase shell. answer questions like \"in safe mode?\" status and information on hbase's data in zookeeper in the web ui. ",
        "label": 342
    },
    {
        "text": "hbase should check the issecurityenabled flag  hadoop 0.21.0's usergroupinfomation support the security check flag and always returns false.  hbase should check both the method existence and the return value. ",
        "label": 180
    },
    {
        "text": "lru style map for the block cache  we need to decide what structure to use to back the block cache. the primary decision is whether to continue using softreferences or to build our own structure. ",
        "label": 547
    },
    {
        "text": "getrowwithcolumnsts changed behavior  the method getrowwithtimestampts of the thrift interface changed behavior from version 0.19 to 0.20: in 0.19, it returned only cells with exactly the given timestamp, 0.20 it to returns cells with a timestamp before (not including) the given timestamp. it needs to be clearified, which one is the desired behavior. i attach a patch to make 0.20 conform with 0.19 (only return cells with exactly the given timestamp), if this is what is wanted. ",
        "label": 451
    },
    {
        "text": "handle disabled table for async client  now for async client we will not check if a table is disabled when retrying, so we will retry until the time or count limit is reached, and will not throw a tablenotenabledexception. we should have the same behavior as sync client, but the implementation should be carefully designed. for sync client, we will also check if a table is disabled if it is a retry, no matter what the exception is. this will double the pressure on meta table. we should try our best to eliminate unnecessary access to meta table. ",
        "label": 149
    },
    {
        "text": "regionservercallable   rpcretryingcaller clear meta cache on retries  when rpcretryingcaller.callwithretries() attempts a retry, it calls retryingcallable.prepare(tries != 0). for regionservercallable (and probably others), this will wind up calling regionlocator.getregionlocation(reload=true), which will drop the meta cache for the given region and always go back to meta. this is kind of silly, since in the case of exceptions, we already call retryingcallable.throwable(), which goes to great pains to only refresh the meta cache when necessary. since we are already doing this on failure, i don't really understand why we are doing duplicate work to refresh the meta cache on prepare() at all. ",
        "label": 180
    },
    {
        "text": "user with  create  permission can grant  but not revoke permissions on created table  a user that only has global or namespace \"create\" permission can grant permissions to another user on its created table, but cannot revoke them. this bug exists on branch-2.1, from 2.1.1  2.0, 2.1.0, master, and branch-2.2 are not effected. the bug can be triggered via hbase shell: #start hbase shell as superuse  #export hadoop_user_name=hbase  hbase shell grant 'regularuser1', 'c' exit #run hbase shell as regularuser1 #grant, then revoke 'rx' permission to regularuser2 #export hadoop_user_name=regularuser1 hbase shell create 'nunuke','nunuke' grant 'regularuser2', 'rx', 'nunuke' #this will fail on 2.1.1+ revoke 'regularuser2', 'nunuke' ",
        "label": 214
    },
    {
        "text": "write requesttoobigexception back to client for nettyrpcserver  for now we just close the connection so nettyrpcserver can not pass testipc. ",
        "label": 455
    },
    {
        "text": "hbaseserver   ipc reader threads are not daemons  doing a jstack on a region server process shows that the ipc reader threads are not created as daemon threads whereas ipc server threads (and other types are). this could cause the region server to not exit after the main method does if for some reason these non-daemon threads don't exit themselves. servers are daemon  \"ipc server handler 7 on 60020\" daemon prio=10 tid=0x00002aaabc998800 nid=0x7157 waiting on condition [0x0000000044b4e000]  \"ipc server handler 6 on 60020\" daemon prio=10 tid=0x00002aaabc996800 nid=0x7156 waiting on condition [0x0000000044a4d000]  \"ipc server handler 5 on 60020\" daemon prio=10 tid=0x00002aaabc995000 nid=0x7155 waiting on condition [0x000000004494c000] readers are not  \"ipc reader 5 on port 60020\" prio=10 tid=0x00002aaabc47d000 nid=0x712a runnable [0x0000000043033000]  \"ipc reader 4 on port 60020\" prio=10 tid=0x00002aaabc462000 nid=0x7129 runnable [0x0000000042f32000]  \"ipc reader 3 on port 60020\" prio=10 tid=0x00002aaabc447000 nid=0x7128 runnable [0x0000000042e31000] ",
        "label": 147
    },
    {
        "text": "hbase broke thrift  on hudson java.lang.nullpointerexception at org.apache.hadoop.hbase.thrift.thriftutilities.rowresultfromhbase(thriftutilities.java:107) at org.apache.hadoop.hbase.thrift.thriftserver$hbasehandler.getrowwithcolumnsts(thriftserver.java:328) at org.apache.hadoop.hbase.thrift.thriftserver$hbasehandler.getrow(thriftserver.java:303) at org.apache.hadoop.hbase.thrift.testthriftserver.dotesttablemutations(testthriftserver.java:169) at org.apache.hadoop.hbase.thrift.testthriftserver.testall(testthriftserver.java:65) ",
        "label": 229
    },
    {
        "text": "reenable testwalprocedurestoreonhdfs testwalrollonlowreplication  ",
        "label": 198
    },
    {
        "text": "major compaction period is not checked periodically  the major compaction period, hbase.hregion.majorcompaction, is not checked periodically. currently, we only request major compaction when the region is open or split at which point we check whether the major compaction period is due. ",
        "label": 314
    },
    {
        "text": "enable multi thread for memstore flush  if the kv is large or hlog is closed with high-pressure putting, we found memstore is often above the high water mark and block the putting. so should we enable multi-thread for memstore flush? some performance test data for reference, 1.test environment \uff1a   random writting\uff1bupper memstore limit 5.6gb;lower memstore limit 4.8gb;400 regions per regionserver\uff1brow len=50 bytes, value len=1024 bytes;5 regionserver, 300 ipc handler per regionserver;5 client, 50 thread handler per client for writing 2.test results: one cacheflush handler, tps: 7.8k/s per regionserver, flush:10.1mb/s per regionserver, appears many aboveglobalmemstorelimit blocking two cacheflush handlers, tps: 10.7k/s per regionserver, flush:12.46mb/s per regionserver, 200 thread handler per client & two cacheflush handlers, tps:16.1k/s per regionserver, flush:18.6mb/s per regionserver ",
        "label": 107
    },
    {
        "text": "tests in basetesthbasefsck are run by its subclasses redundantly  several tests extend basetesthbasefsck : public class testhbasefsckmob extends basetesthbasefsck {  hbase-server/src/test//java/org/apache/hadoop/hbase/util/testhbasefsckmob.java  public class testhbasefsckoners extends basetesthbasefsck {  hbase-server/src/test//java/org/apache/hadoop/hbase/util/testhbasefsckoners.java  public class testhbasefsckreplicas extends basetesthbasefsck {  hbase-server/src/test//java/org/apache/hadoop/hbase/util/testhbasefsckreplicas.java  public class testhbasefscktwors extends basetesthbasefsck {  hbase-server/src/test//java/org/apache/hadoop/hbase/util/testhbasefscktwors.java basetesthbasefsck contains several tests, e.g. testhdfsregioninfomissing this means that the tests in basetesthbasefsck would be run multiple times in the test suite. ",
        "label": 198
    },
    {
        "text": "hbasefsck should clean up connection properly when repair is completed  at the end of exec() method, connections to the cluster are not properly released. connections should be released upon completion of repair. this was mentioned by jean-marc in the thread '[vote] the 1st hbase 0.94.16 release candidate is available for download' ",
        "label": 441
    },
    {
        "text": "snapshot restore of table which splits after snapshot was taken encounters 'region is not online'  take snapshot of a table ('tablethree' in the log).  put some data in the table and split the table.  restore snapshot.  table cannot be enabled due to: thu aug 22 19:37:20 utc 2013, org.apache.hadoop.hbase.client.rpcretryingcaller@47a6ac39, org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception: region is not online: c32e63d8c8a1a94b68966645b956d86d at org.apache.hadoop.hbase.regionserver.hregionserver.getregionbyencodedname(hregionserver.java:2557) at org.apache.hadoop.hbase.regionserver.hregionserver.getregion(hregionserver.java:3921) at org.apache.hadoop.hbase.regionserver.hregionserver.scan(hregionserver.java:2996) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$2.callblockingmethod(clientprotos.java:26847) ",
        "label": 309
    },
    {
        "text": "integrationtestbiglinkedlistwithchaosmonkey never exits if there is an error  the chaos monkey never finishes if there is an error in integrationtestbiglinkedlistwithchaosmonkey. so the java process never exits. ",
        "label": 154
    },
    {
        "text": "flushing canceled by coprocessor still leads to memstoresize set down  a coprocessor override \"public internalscanner preflush(final store store, final internalscanner scanner)\" and return null when calling this method, will cancel flush request, leaving snapshot un-flushed, and no new storefile created. but the hregion.internalflushcache still set down memstoresize to 0 by totalflushablesize.   if there's no write requests anymore, the memstoresize will remaining as 0, and no more flush quests will be processed because of the checking of memstoresize.get() <=0 at the beginning of internalflushcache.  this issue may not cause data loss, but it will confuse coprocessor users. if we argree with this, i'll apply a patch later. ",
        "label": 498
    },
    {
        "text": "error in regioncoprocessorhost class prescanner method documentation   prescanneropen() method of regioncoprocessorhost class is documented to return 'false' value in negative case (default operation should not be bypassed). actual implementation returns 'null' value. proposed solution is just to correct comment to match existing implementation. ",
        "label": 381
    },
    {
        "text": "backport hbase  'master won't go down'  to  place holder for lars for whether he wants hbase-8422 in 0.94 or not ",
        "label": 543
    },
    {
        "text": "allow a deferred sync option per mutation   won't have time for parent. but a deferred sync option on a per operation basis comes up quite frequently.  in 0.96 this can be handled cleanly via protobufs and 0.94 we can have a special mutation attribute. for batch operation we'd take the safest sync option of any of the mutations. i.e. if there is at least one that wants to be flushed we'd sync the batch, if there's none of those but at least one that wants deferred flush we defer flush the batch, etc. ",
        "label": 286
    },
    {
        "text": "clean up configuration keys used in hbase spark module  this should be considered a blocker for backport to branch-1 since it will impact our compatibility. the constants we expose in configuration should all start with \"hbase\". since our configurations keys for the spark integration all relate to that system, the prefix for all configuration keys (excluding those cases where we need to do something special due to restrictions in how properties are handled by e.g. spark) should be \"hbase.spark\". before publishing a public api labeled version of our spark integration we should review all of our configuration keys to make sure they either conform to the \"hbase.spark\" prefix or they have a comment documenting why they need to be otherwise. ",
        "label": 499
    },
    {
        "text": "add metrics for storefile age  in order to make sure that compactions are fully up to date it would be nice to have metrics on: max storefile age min storefile age average storefile age number of reference files if we could have those metrics per region and per regionserver it would be awesome. ",
        "label": 323
    },
    {
        "text": "replace intrinsic locking with explicit locks in storescanner  did some more profiling (this time with a sampling profiler) and storescanner.peek() showed up a lot in the samples. at first that was surprising, but peek is synchronized, so it seems a lot of the sync'ing cost is eaten there.  it seems the only reason we have to synchronize all these methods is because a concurrent flush or compaction can change the scanner stack, other than that only a single thread should access a storescanner at any given time.  so replaced updatereaders() with some code that just indicates to the scanner that the readers should be updated and then make it the using thread's responsibility to do the work.  the perf improvement from this is staggering. i am seeing somewhere around 3x scan performance improvement across all scenarios. now, the hard part is to reason about whether this is 100% correct. i ran testatomicoperation and testacidguarantees a few times in a loop, all still pass. will attach a sample patch. ",
        "label": 286
    },
    {
        "text": "hregionthriftserver' might have to indefinitely do redirtects  hregionthriftserver.getrowwithcolumnsts() redirects the request to the correct region server if it has landed on the wrong region-server. with this approach the smart-client will never get a notservingregionexception and it will never be able to invalidate its cache. it will indefinitely send the request to the wrong region server and the wrong region server will always be redirecting it. either redirects should be turned off and the client should react to notservingregionexceptions. or somehow a flag should be set in the response telling the client to refresh its cache. ",
        "label": 247
    },
    {
        "text": "copytable instructions could be improved   the book and the usage instructions could be improved to include more details, things caveats and to better explain usage. one example in particular, could be updated to refer to replicationregioninterface and replicationregionserver in thier current locations (o.a.h.h.client.replication and o.a.h.h.replication.regionserver), and better explain why one would use particular arguments. $ bin/hbase org.apache.hadoop.hbase.mapreduce.copytable --rs.class=org.apache.hadoop.hbase.ipc.replicationregioninterface --rs.impl=org.apache.hadoop.hbase.regionserver.replication.replicationregionserver --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase testtable ",
        "label": 330
    },
    {
        "text": "check the sloppiness of the region load before balancing  per our discussion at the hackathon today, it seems that it would be more helpful to add a sloppiness check before doing the normal balancing. the current situation is that the balancer always tries to get the region load even, meaning that there can be some very frequent regions movement. setting the balancer to run less often (like every 4 hours) isn't much better since the load could get out of whack easily. this is why running the normal balancer frequently, but first checking for some sloppiness in the region load across the rs, seems like a more viable option. ",
        "label": 441
    },
    {
        "text": "regionserver stuck when after all ipc server handlers fatal'd  the region server is stuck with the following jstack 2010-11-03 22:23:41  full thread dump java hotspot(tm) 64-bit server vm (14.0-b16 mixed mode): \"attach listener\" daemon prio=10 tid=0x00002aaeb6774000 nid=0x3974 waiting on condition [0x0000000000000000]  java.lang.thread.state: runnable \"rs_close_region-pumahbase028.snc5.facebook.com,60020,1288733355197-2\" prio=10 tid=0x00002aaeb8449000 nid=0x3bbc waiting on condition [0x0000000043f67000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab7fd1130> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"rs_close_region-pumahbase028.snc5.facebook.com,60020,1288733355197-1\" prio=10 tid=0x00002aaeb843f800 nid=0x3bbb waiting on condition [0x0000000043e66000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab7fd1130> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"rs_close_region-pumahbase028.snc5.facebook.com,60020,1288733355197-0\" prio=10 tid=0x00002aaeb8447800 nid=0x3bba waiting on condition [0x0000000044068000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab7fd1130> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"rmi scheduler(0)\" daemon prio=10 tid=0x00002aaeb48c4800 nid=0x1c97 waiting on condition [0x00000000580a7000]  java.lang.thread.state: timed_waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab773a118> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:198)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.awaitnanos(abstractqueuedsynchronizer.java:1963)  at java.util.concurrent.delayqueue.take(delayqueue.java:164)  at java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:583)  at java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue.take(scheduledthreadpoolexecutor.java:576)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"rs_open_region-pumahbase028.snc5.facebook.com,60020,1288733355197-2\" daemon prio=10 tid=0x00002aaeb4804800 nid=0x17a0 waiting on condition [0x00000000582a9000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab7fca538> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"rs_open_region-pumahbase028.snc5.facebook.com,60020,1288733355197-1\" daemon prio=10 tid=0x00002aaeb490a000 nid=0x179f waiting on condition [0x000000004345c000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab7fca538> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"rs_open_region-pumahbase028.snc5.facebook.com,60020,1288733355197-0\" daemon prio=10 tid=0x00002aaeb4909000 nid=0x179e waiting on condition [0x000000004335b000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab7fca538> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at java.util.concurrent.threadpoolexecutor.gettask(threadpoolexecutor.java:947)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619) \"lrublockcache.evictionthread\" daemon prio=10 tid=0x00002aaeb889b000 nid=0x1767 in object.wait() [0x0000000057ea5000]  java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  at java.lang.object.wait(object.java:485)  at org.apache.hadoop.hbase.io.hfile.lrublockcache$evictionthread.run(lrublockcache.java:514) locked <0x00002aaab7fdb728> (a org.apache.hadoop.hbase.io.hfile.lrublockcache$evictionthread) \"timer thread for monitoring jvm\" daemon prio=10 tid=0x00002aaebc0a5000 nid=0x1620 in object.wait() [0x0000000043c64000]  java.lang.thread.state: timed_waiting (on object monitor)  at java.lang.object.wait(native method)  at java.util.timerthread.mainloop(timer.java:509) locked <0x00002aaab7e89c38> (a java.util.taskqueue)  at java.util.timerthread.run(timer.java:462) \"timer thread for monitoring hbase\" daemon prio=10 tid=0x00002aaebc0a3800 nid=0x161f in object.wait() [0x0000000043b63000]  java.lang.thread.state: timed_waiting (on object monitor)  at java.lang.object.wait(native method)  at java.util.timerthread.mainloop(timer.java:509) locked <0x00002aaab7fe95d8> (a java.util.taskqueue)  at java.util.timerthread.run(timer.java:462) \"destroyjavavm\" prio=10 tid=0x00002aaebc092000 nid=0x15ce waiting on condition [0x0000000000000000]  java.lang.thread.state: runnable \"regionserver60020-eventthread\" daemon prio=10 tid=0x00002aaeb5276800 nid=0x1606 waiting on condition [0x000000004325a000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab6bf6bb8> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:477) \"regionserver60020-sendthread(pumahbasectrl001.snc5.facebook.com:2181)\" daemon prio=10 tid=0x00002aaeb5275800 nid=0x1605 runnable [0x0000000043159000]  java.lang.thread.state: runnable  at sun.nio.ch.epollarraywrapper.epollwait(native method)  at sun.nio.ch.epollarraywrapper.poll(epollarraywrapper.java:215)  at sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:65)  at sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:69) locked <0x00002aaab6c02078> (a sun.nio.ch.util$1) locked <0x00002aaab6c02090> (a java.util.collections$unmodifiableset) locked <0x00002aaab66713d0> (a sun.nio.ch.epollselectorimpl)  at sun.nio.ch.selectorimpl.select(selectorimpl.java:80)  at org.apache.zookeeper.clientcnxn$sendthread.run(clientcnxn.java:1066) \"regionserver60020\" prio=10 tid=0x00002aaeb858c800 nid=0x1604 waiting on condition [0x0000000043058000]  java.lang.thread.state: timed_waiting (sleeping)  at java.lang.thread.sleep(native method)  at org.apache.hadoop.hbase.util.threads.sleep(threads.java:126)  at org.apache.hadoop.hbase.regionserver.hregionserver.waitonallregionstoclose(hregionserver.java:645)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:611)  at java.lang.thread.run(thread.java:619) \"timer thread for monitoring rpc\" daemon prio=10 tid=0x00002aaeb8583000 nid=0x1602 in object.wait() [0x0000000042f57000]  java.lang.thread.state: timed_waiting (on object monitor)  at java.lang.object.wait(native method)  at java.util.timerthread.mainloop(timer.java:509) locked <0x00002aaab6be7038> (a java.util.taskqueue)  at java.util.timerthread.run(timer.java:462) \"main-eventthread\" daemon prio=10 tid=0x00002aaeb842d000 nid=0x15f7 waiting on condition [0x000000004244c000]  java.lang.thread.state: waiting (parking)  at sun.misc.unsafe.park(native method) parking to wait for <0x00002aaab6bc0720> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)  at java.util.concurrent.locks.locksupport.park(locksupport.java:158)  at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)  at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:358)  at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:477) \"main-sendthread(pumahbasectrl062.snc5.facebook.com:2181)\" daemon prio=10 tid=0x00002aaeb8451800 nid=0x15f6 runnable [0x000000004234b000]  java.lang.thread.state: runnable  at sun.nio.ch.epollarraywrapper.epollwait(native method)  at sun.nio.ch.epollarraywrapper.poll(epollarraywrapper.java:215)  at sun.nio.ch.epollselectorimpl.doselect(epollselectorimpl.java:65)  at sun.nio.ch.selectorimpl.lockanddoselect(selectorimpl.java:69) locked <0x00002aaab6bb08f0> (a sun.nio.ch.util$1) locked <0x00002aaab6bb0908> (a java.util.collections$unmodifiableset) locked <0x00002aaab764df90> (a sun.nio.ch.epollselectorimpl)  at sun.nio.ch.selectorimpl.select(selectorimpl.java:80)  at org.apache.zookeeper.clientcnxn$sendthread.run(clientcnxn.java:1066) \"rmi tcp accept-0\" daemon prio=10 tid=0x00002aaeb83b9800 nid=0x15f2 runnable [0x0000000042149000]  java.lang.thread.state: runnable  at java.net.plainsocketimpl.socketaccept(native method)  at java.net.plainsocketimpl.accept(plainsocketimpl.java:390) locked <0x00002aaab6b935e8> (a java.net.sockssocketimpl)  at java.net.serversocket.implaccept(serversocket.java:453)  at java.net.serversocket.accept(serversocket.java:421)  at sun.management.jmxremote.localrmiserversocketfactory$1.accept(localrmiserversocketfactory.java:34)  at sun.rmi.transport.tcp.tcptransport$acceptloop.executeacceptloop(tcptransport.java:369)  at sun.rmi.transport.tcp.tcptransport$acceptloop.run(tcptransport.java:341)  at java.lang.thread.run(thread.java:619) \"rmi tcp accept-8091\" daemon prio=10 tid=0x00002aaeb83b3000 nid=0x15f1 runnable [0x0000000042048000]  java.lang.thread.state: runnable  at java.net.plainsocketimpl.socketaccept(native method)  at java.net.plainsocketimpl.accept(plainsocketimpl.java:390) locked <0x00002aaab7659d50> (a java.net.sockssocketimpl)  at java.net.serversocket.implaccept(serversocket.java:453)  at java.net.serversocket.accept(serversocket.java:421)  at sun.rmi.transport.tcp.tcptransport$acceptloop.executeacceptloop(tcptransport.java:369)  at sun.rmi.transport.tcp.tcptransport$acceptloop.run(tcptransport.java:341)  at java.lang.thread.run(thread.java:619) \"rmi tcp accept-0\" daemon prio=10 tid=0x00002aaeb83a0000 nid=0x15f0 runnable [0x0000000041f47000]  java.lang.thread.state: runnable  at java.net.plainsocketimpl.socketaccept(native method)  at java.net.plainsocketimpl.accept(plainsocketimpl.java:390) locked <0x00002aaab765c140> (a java.net.sockssocketimpl)  at java.net.serversocket.implaccept(serversocket.java:453)  at java.net.serversocket.accept(serversocket.java:421)  at sun.rmi.transport.tcp.tcptransport$acceptloop.executeacceptloop(tcptransport.java:369)  at sun.rmi.transport.tcp.tcptransport$acceptloop.run(tcptransport.java:341)  at java.lang.thread.run(thread.java:619) \"low memory detector\" daemon prio=10 tid=0x00002aaeb8016000 nid=0x15eb runnable [0x0000000000000000]  java.lang.thread.state: runnable \"compilerthread1\" daemon prio=10 tid=0x00002aaeb8013800 nid=0x15ea waiting on condition [0x0000000000000000]  java.lang.thread.state: runnable \"compilerthread0\" daemon prio=10 tid=0x00002aaeb8011800 nid=0x15e9 waiting on condition [0x0000000000000000]  java.lang.thread.state: runnable \"jdwp event helper thread\" daemon prio=10 tid=0x00002aaeb800f800 nid=0x15e8 runnable [0x0000000000000000]  java.lang.thread.state: runnable \"jdwp transport listener: dt_socket\" daemon prio=10 tid=0x00002aaeb800c000 nid=0x15e7 runnable [0x0000000000000000]  java.lang.thread.state: runnable \"signal dispatcher\" daemon prio=10 tid=0x00002aaeb45d4800 nid=0x15e6 runnable [0x0000000000000000]  java.lang.thread.state: runnable \"surrogate locker thread (cms)\" daemon prio=10 tid=0x00002aaeb45d2800 nid=0x15e5 waiting on condition [0x0000000000000000]  java.lang.thread.state: runnable \"finalizer\" daemon prio=10 tid=0x00002aaeb440b000 nid=0x15e3 in object.wait() [0x000000004173f000]  java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  at java.lang.ref.referencequeue.remove(referencequeue.java:118) locked <0x00002aaab6663ac0> (a java.lang.ref.referencequeue$lock)  at java.lang.ref.referencequeue.remove(referencequeue.java:134)  at java.lang.ref.finalizer$finalizerthread.run(finalizer.java:159) \"reference handler\" daemon prio=10 tid=0x00002aaeb4409000 nid=0x15e2 in object.wait() [0x000000004163e000]  java.lang.thread.state: waiting (on object monitor)  at java.lang.object.wait(native method)  at java.lang.object.wait(object.java:485)  at java.lang.ref.reference$referencehandler.run(reference.java:116) locked <0x00002aaab6c01940> (a java.lang.ref.reference$lock) \"vm thread\" prio=10 tid=0x00002aaeb4402000 nid=0x15e1 runnable \"gang worker#0 (parallel gc threads)\" prio=10 tid=0x000000004011e800 nid=0x15cf runnable \"gang worker#1 (parallel gc threads)\" prio=10 tid=0x0000000040120800 nid=0x15d0 runnable \"gang worker#2 (parallel gc threads)\" prio=10 tid=0x0000000040122000 nid=0x15d1 runnable \"gang worker#3 (parallel gc threads)\" prio=10 tid=0x0000000040124000 nid=0x15d2 runnable \"gang worker#4 (parallel gc threads)\" prio=10 tid=0x0000000040126000 nid=0x15d3 runnable \"gang worker#5 (parallel gc threads)\" prio=10 tid=0x0000000040127800 nid=0x15d4 runnable \"gang worker#6 (parallel gc threads)\" prio=10 tid=0x0000000040129800 nid=0x15d5 runnable \"gang worker#7 (parallel gc threads)\" prio=10 tid=0x00002aaab2526800 nid=0x15d6 runnable \"gang worker#8 (parallel gc threads)\" prio=10 tid=0x00002aaab2528000 nid=0x15d7 runnable \"gang worker#9 (parallel gc threads)\" prio=10 tid=0x00002aaab252a000 nid=0x15d8 runnable \"gang worker#10 (parallel gc threads)\" prio=10 tid=0x00002aaab252c000 nid=0x15d9 runnable \"gang worker#11 (parallel gc threads)\" prio=10 tid=0x00002aaab252d800 nid=0x15da runnable \"gang worker#12 (parallel gc threads)\" prio=10 tid=0x00002aaab252f800 nid=0x15db runnable \"concurrent mark-sweep gc thread\" prio=10 tid=0x00002aaea3b8f000 nid=0x15e0 runnable  \"gang worker#0 (parallel cms threads)\" prio=10 tid=0x00002aaea3a7e800 nid=0x15dc runnable \"gang worker#1 (parallel cms threads)\" prio=10 tid=0x00002aaea3a80000 nid=0x15dd runnable \"gang worker#2 (parallel cms threads)\" prio=10 tid=0x00002aaea3acb800 nid=0x15de runnable \"gang worker#3 (parallel cms threads)\" prio=10 tid=0x00002aaea3acd000 nid=0x15df runnable \"vm periodic task thread\" prio=10 tid=0x00002aaeb83bc800 nid=0x15f3 waiting on condition jni global references: 4205 === in the rs logs all the ipc server handlers have died because hdfs failure to write to the log. first failure to append to the logs  2010-11-02 17:05:58,620 warn org.apache.hadoop.hdfs.dfsclient: packet 243111 [offsetinblock=241298432 pktlen=2173] of blk_-4978761844464053668_783579 is timed out  2010-11-02 17:05:58,623 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_-4978761844464053668_783579 bad datanode[2] 10.38.47.49:50010  2010-11-02 17:05:58,623 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_-4978761844464053668_783579 in pipeline 10.38.15.45:50010, 10.38.47.59:50010, 10.38.47.49:50010: bad datanode 10.38.47.49:50010  2010-11-02 17:05:58,755 warn org.apache.hadoop.hdfs.dfsclient: failed recovery attempt #0 from primary datanode 10.38.15.45:50010  org.apache.hadoop.ipc.rpc$versionmismatch: protocol org.apache.hadoop.hdfs.protocol.clientdatanodeprotocol version mismatch. (client = 5, server = 3)  at org.apache.hadoop.ipc.rpc.getproxy(rpc.java:452)  at org.apache.hadoop.hdfs.dfsclient.createclientdatanodeprotocolproxy(dfsclient.java:194)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.processdatanodeerror(dfsclient.java:3027)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$1800(dfsclient.java:2467)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2687)  ...  2010-11-02 17:11:09,667 warn org.apache.hadoop.hdfs.dfsclient: failed recovery attempt #0 from primary datanode 10.38.47.49:50010  java.net.sockettimeoutexception: call to /10.38.47.49:50020 failed on socket timeout exception: java.net.sockettimeoutexception: 300000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.socketchannel[connected local=/10.38.15.45:56492 remote=/10.38.47.49:50020]  at org.apache.hadoop.ipc.client.wrapexception(client.java:855)  at org.apache.hadoop.ipc.client.call(client.java:827)  at org.apache.hadoop.ipc.rpc$invoker.invoke(rpc.java:222)  at $proxy3.getprotocolversion(unknown source)  at org.apache.hadoop.ipc.rpc.getproxy(rpc.java:447)  at org.apache.hadoop.hdfs.dfsclient.createclientdatanodeprotocolproxy(dfsclient.java:194)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.processdatanodeerror(dfsclient.java:3027)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$1800(dfsclient.java:2467)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2687)  caused by: java.net.sockettimeoutexception: 300000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.socketchannel[connected local=/10.38.15.45:56492 remote=/10.38.47.49:50020]  at org.apache.hadoop.net.socketiowithtimeout.doio(socketiowithtimeout.java:164)  at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:155)  at org.apache.hadoop.net.socketinputstream.read(socketinputstream.java:128)  at java.io.filterinputstream.read(filterinputstream.java:116)  at org.apache.hadoop.ipc.client$connection$pinginputstream.read(client.java:325)  at java.io.bufferedinputstream.fill(bufferedinputstream.java:218)  at java.io.bufferedinputstream.read(bufferedinputstream.java:237)  at java.io.datainputstream.readint(datainputstream.java:370)  at org.apache.hadoop.ipc.client$connection.receiveresponse(client.java:572)  at org.apache.hadoop.ipc.client$connection.run(client.java:500)  2010-11-02 17:11:09,669 warn org.apache.hadoop.hdfs.dfsclient: error recovery for block blk_-4978761844464053668_783579 file /pumahbase001-snc5-hbase/.logs/pumahbase028.snc5.facebook.com,60020,1288733355197/10.38.15.45%3a60020.1288742259683 failed because recovery from primary datanode 10.38.47.49:50010 failed 1 times. pipeline was 10.38.47.59:50010, 10.38.47.49:50010. will retry... error recovery fails.  2010-11-02 17:36:20,919 fatal org.apache.hadoop.hbase.regionserver.wal.hlog: could not append. requesting close of hlog  java.io.ioexception: reflection  at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.sync(sequencefilelogwriter.java:147)  at org.apache.hadoop.hbase.regionserver.wal.hlog.hflush(hlog.java:1042)  at org.apache.hadoop.hbase.regionserver.wal.hlog$logsyncer.run(hlog.java:966)  caused by: java.lang.reflect.invocationtargetexception  at sun.reflect.generatedmethodaccessor22.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.sync(sequencefilelogwriter.java:145)  ... 2 more  caused by: java.io.ioexception: error recovery for block blk_-4978761844464053668_783579 file /pumahbase001-snc5-hbase/.logs/pumahbase028.snc5.facebook.com,60020,1288733355197/10.38.15.45%3a60020.1288742259683 failed because recovery from primary datanode 10.38.47.59:50010 failed 6 times. pipeline was 10.38.47.59:50010. aborting...  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.processdatanodeerror(dfsclient.java:3065)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.access$1800(dfsclient.java:2467)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream$datastreamer.run(dfsclient.java:2687) regions start getting aborted because hlog close fails because filesystem is not available 2010-11-02 17:36:24,055 fatal org.apache.hadoop.hbase.regionserver.hregionserver: aborting region server servername=pumahbase028.snc5.facebook.com,60020,1288733355197, load=(requests=492, regions=55, usedheap=3376, maxheap=15993): file system not available  java.io.ioexception: file system is not available  at org.apache.hadoop.hbase.util.fsutils.checkfilesystemavailable(fsutils.java:130)  at org.apache.hadoop.hbase.regionserver.hregionserver.checkfilesystem(hregionserver.java:949)  at org.apache.hadoop.hbase.regionserver.hregionserver.increment(hregionserver.java:2312)  at sun.reflect.generatedmethodaccessor21.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:561)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1025)  caused by: java.io.ioexception: filesystem closed  at org.apache.hadoop.hdfs.dfsclient.checkopen(dfsclient.java:282)  at org.apache.hadoop.hdfs.dfsclient.getfileinfo(dfsclient.java:875)  at org.apache.hadoop.hdfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:484)  at org.apache.hadoop.fs.filesystem.exists(filesystem.java:766)  at org.apache.hadoop.hbase.util.fsutils.checkfilesystemavailable(fsutils.java:119)  ... 7 more all the ipc server handlers exit 2010-11-02 17:36:24,097 warn org.apache.hadoop.ipc.hbaseserver: ipc server responder, call increment([b@aa32978, row=b4da93229cfc2cfb7d731f46da44f108info.compartilo1.59624 lfi 450313269132, families={(family=counters_0, columns={=1, a13g1=1, ccl+=1, les_la+=1}), (family=counters_3600, columns={7ffa89a0+=1}}) from 10.38.15.37:49593: output error  2010-11-02 17:36:24,094 fatal org.apache.hadoop.hbase.regionserver.wal.hlog: could not append. requesting close of hlog  java.io.ioexception: filesystem closed  at org.apache.hadoop.hdfs.dfsclient.checkopen(dfsclient.java:282)  at org.apache.hadoop.hdfs.dfsclient.access$800(dfsclient.java:71)  at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.writechunk(dfsclient.java:3465)  at org.apache.hadoop.fs.fsoutputsummer.writechecksumchunk(fsoutputsummer.java:150)  at org.apache.hadoop.fs.fsoutputsummer.flushbuffer(fsoutputsummer.java:132)  at org.apache.hadoop.fs.fsoutputsummer.flushbuffer(fsoutputsummer.java:121)  at org.apache.hadoop.fs.fsoutputsummer.write1(fsoutputsummer.java:112)  at org.apache.hadoop.fs.fsoutputsummer.write(fsoutputsummer.java:86)  at org.apache.hadoop.fs.fsdataoutputstream$positioncache.write(fsdataoutputstream.java:49)  at java.io.dataoutputstream.write(dataoutputstream.java:90)  at org.apache.hadoop.io.sequencefile$writer.append(sequencefile.java:1073)  at org.apache.hadoop.io.sequencefile$writer.append(sequencefile.java:1037)  at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.append(sequencefilelogwriter.java:133)  at org.apache.hadoop.hbase.regionserver.wal.hlog.dowrite(hlog.java:1129)  at org.apache.hadoop.hbase.regionserver.wal.hlog.append(hlog.java:914)  at org.apache.hadoop.hbase.regionserver.hregion.increment(hregion.java:3038)  at org.apache.hadoop.hbase.regionserver.hregionserver.increment(hregionserver.java:2309)  at sun.reflect.generatedmethodaccessor21.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:561)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1025)  2010-11-02 17:36:24,108 warn org.apache.hadoop.ipc.hbaseserver: ipc server handler 69 on 60020 caught: java.nio.channels.closedchannelexception  at sun.nio.ch.socketchannelimpl.ensurewriteopen(socketchannelimpl.java:126)  at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:324)  at org.apache.hadoop.hbase.ipc.hbaseserver.channelwrite(hbaseserver.java:1282)  at org.apache.hadoop.hbase.ipc.hbaseserver$responder.processresponse(hbaseserver.java:713)  at org.apache.hadoop.hbase.ipc.hbaseserver$responder.dorespond(hbaseserver.java:778)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1053) 2010-11-02 17:36:24,113 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 69 on 60020: exiting === if all the ipc server handlers exit for whatever reason then the region server should cleanly exit. i think my clients are getting stuck in locateregioninmeta() because of this stuck region server. === ",
        "label": 247
    },
    {
        "text": "testassignmentmanager sometimes fails  from https://builds.apache.org/job/hbase-trunk/3432/testreport/junit/org.apache.hadoop.hbase.master/testassignmentmanager/testbalanceonmasterfailoverscenariowithopenednode/ : stacktrace java.lang.exception: test timed out after 5000 milliseconds at java.lang.system.arraycopy(native method) at java.lang.threadgroup.remove(threadgroup.java:969) at java.lang.threadgroup.threadterminated(threadgroup.java:942) at java.lang.thread.exit(thread.java:732) ... 2012-10-06 00:46:12,521 debug [master_close_region-mockedamexecutor-0] zookeeper.zkutil(1141): mockedserver-0x13a33892de7000e retrieved 81 byte(s) of data from znode /hbase/unassigned/dc01abf9cd7fd0ea256af4df02811640 and set watcher; region=t,,1349484359011.dc01abf9cd7fd0ea256af4df02811640., state=m_zk_region_offline, servername=master,1,1, createtime=1349484372509, payload.length=0 2012-10-06 00:46:12,522 error [master_close_region-mockedamexecutor-0] executor.eventhandler(205): caught throwable while processing event rs_zk_region_closed java.lang.nullpointerexception at org.apache.hadoop.hbase.master.testassignmentmanager$mockedloadbalancer.randomassignment(testassignmentmanager.java:773) at org.apache.hadoop.hbase.master.assignmentmanager.getregionplan(assignmentmanager.java:1709) at org.apache.hadoop.hbase.master.assignmentmanager.getregionplan(assignmentmanager.java:1666) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1435) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1155) at org.apache.hadoop.hbase.master.testassignmentmanager$assignmentmanagerwithextrasfortesting.assign(testassignmentmanager.java:1035) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1130) at org.apache.hadoop.hbase.master.assignmentmanager.assign(assignmentmanager.java:1125) at org.apache.hadoop.hbase.master.handler.closedregionhandler.process(closedregionhandler.java:106) at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:202) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) 2012-10-06 00:46:12,522 debug [pool-1-thread-1-eventthread] master.assignmentmanager(670): handling transition=m_zk_region_offline, server=master,1,1, region=dc01abf9cd7fd0ea256af4df02811640, current state from region state map ={t,,1349484359011.dc01abf9cd7fd0ea256af4df02811640. state=offline, ts=1349484372508, server=null} looks like npe happened on this line:       this.gate.set(true); ",
        "label": 242
    },
    {
        "text": " stargate  unable to delete column families  when trying to delete a column family using stargate, the following occurs (curl command + stargate logging): > curl http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute/5426359469582345882 -x delete  \u2014  10/01/13 18:57:38 debug stargate.rowresource: delete http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute/5426359469582345882  10/01/13 18:57:38 debug stargate.rowresource: delete row=ff417a5b-c4d0-4a43-b1c7-94c356fe0b72, ts=9223372036854775807, families= {(family=attribute, keyvalues=(ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882/deletecolumn/vlen=0)} \u2014 > curl http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882 -x delete  \u2014  10/01/13 18:57:49 debug stargate.rowresource: delete http://localhost:8080/books/ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882  10/01/13 18:57:49 debug stargate.rowresource: delete row=ff417a5b-c4d0-4a43-b1c7-94c356fe0b72, ts=9223372036854775807, families= {(family=attribute, keyvalues=(ff417a5b-c4d0-4a43-b1c7-94c356fe0b72/attribute:/5426359469582345882/deletecolumn/vlen=0)} \u2014 both are attempting to delete columns instead of the intended action of deleting families. the problem occurs because rowresource.java (line 282) will always return a split of length 2, since rowspec.java (line 122) appends a colon if it's missing. i've patched it so that a check will occur if the second split's (split[1]) length is 0 and acts accordingly. i'll attach the patch after i've run it against the test suite. p.s. it's my first time submitting a patch, so let me know if i screwed anything up. ",
        "label": 185
    },
    {
        "text": "assign system tables first  priority   need this for stuff like the rsgroup table, etc. assign these ahead of user-space regions. from 'handle sys table assignment first (e.g. acl, namespace, rsgroup); currently only hbase:meta is first.' of https://docs.google.com/document/d/1evka7fhdeoj1-9o8yzcotaqbv0u0bblblcczvsin69g/edit#heading=h.oefcyphs0v0x ",
        "label": 499
    },
    {
        "text": "rest servlet should allow content bodies to be gzipped  incoming or outgoing  we could probably save a lot of network traffic by gzipping our requests and responses to rest when possible. a fair number of client libraries support this feature out of the box, and others could be fairly easily coded to work that way. our content on requests and responses is probably very compressible, seeing as it is base64 encoded. ",
        "label": 38
    },
    {
        "text": "update thrift to use compact framed protocol  tcompactprotocol/tframedtransport and nonblocking server option promises better efficiency and performance improvements. consider moving hbase thrift bits to this when full platform support is ready for tcompactprotocol. ",
        "label": 284
    },
    {
        "text": " replication  empty znodes created during queue failovers aren't deleted  please check code below replicationsourcemanager.java // nodefailoverworker class public void run() { {     ...       log.info(\"moving \" + rsznode + \"'s hlogs to my queue\");       sortedmap<string, sortedset<string>> newqueues =           zkhelper.copyqueuesfromrs(rsznode);   // node create here*       zkhelper.deletersqueues(rsznode);        if (newqueues == null || newqueues.size() == 0) {         return;         }     ... }   public void closerecoveredqueue(replicationsourceinterface src) {     log.info(\"done with the recovered queue \" + src.getpeerclusterznode());     this.oldsources.remove(src);     this.zkhelper.deletesource(src.getpeerclusterznode(), false);  // node delete here*   } so from code we can see if newqueues == null or newqueues.size() == 0, failover replication source will never start and the failover zk node will never deleted. eg below failover node will never be delete: [zk: 10.232.98.77:2181(connected) 16] ls /hbase-test3-repl/replication/rs/dw93.kgb.sqa.cm4,60020,1346337383956/1-dw93.kgb.sqa.cm4,60020,1346309263932-dw91.kgb.sqa.cm4,60020,1346307150041-dw89.kgb.sqa.cm4,60020,1346307911711-dw93.kgb.sqa.cm4,60020,1346312019213-dw88.kgb.sqa.cm4,60020,1346311774939-dw89.kgb.sqa.cm4,60020,1346312314229-dw93.kgb.sqa.cm4,60020,1346312524307-dw88.kgb.sqa.cm4,60020,1346313203367-dw89.kgb.sqa.cm4,60020,1346313944402-dw88.kgb.sqa.cm4,60020,1346314214286-dw91.kgb.sqa.cm4,60020,1346315119613-dw93.kgb.sqa.cm4,60020,1346314186436-dw88.kgb.sqa.cm4,60020,1346315594396-dw89.kgb.sqa.cm4,60020,1346315909491-dw92.kgb.sqa.cm4,60020,1346315315634-dw89.kgb.sqa.cm4,60020,1346316742242-dw93.kgb.sqa.cm4,60020,1346317604055-dw92.kgb.sqa.cm4,60020,1346318098972-dw91.kgb.sqa.cm4,60020,1346317855650-dw93.kgb.sqa.cm4,60020,1346318532530-dw92.kgb.sqa.cm4,60020,1346318573238-dw89.kgb.sqa.cm4,60020,1346321299040-dw91.kgb.sqa.cm4,60020,1346321304393-dw92.kgb.sqa.cm4,60020,1346325755894-dw89.kgb.sqa.cm4,60020,1346326520895-dw91.kgb.sqa.cm4,60020,1346328246992-dw92.kgb.sqa.cm4,60020,1346327290653-dw93.kgb.sqa.cm4,60020,1346337303018-dw91.kgb.sqa.cm4,60020,1346337318929  [] // empty node will never be deleted ",
        "label": 552
    },
    {
        "text": "allow to specify a user supplied row key comparator for a table  now that row keys are byte arrays, users should be able to specify a comparator at table creation time. my use case for this is to implement secondary indexes. in this case, row keys for the index tables will be constructed from an optional prefix of the original row key as well as the content of column that is being indexed. then the comparator will first compare based on the key prefix, and break ties by deserializing the column values and using the deserialized type's compareto method. ",
        "label": 110
    },
    {
        "text": "the delete key in the hbase shell deletes the wrong character  within the hbase shell, if you type \"monkey\", then place the cursor on the letter 'k' and hit the key 'delete' several times it will do this:  monkey  mokey  mkey  key  key  key  ... ",
        "label": 451
    },
    {
        "text": "add metrics for region normalization plans  currently there is no metric for region normalization plans. this jira would add metrics for the following: number of region split plans executed  number of region merge plans executed these metrics would allow admin to know the impact of region normalization on production cluster ",
        "label": 441
    },
    {
        "text": " hbck2  add more log for hbck operations at master side  ",
        "label": 187
    },
    {
        "text": "hmaster createtable could be heavily optimized  looking at the createtable method in hmaster (the one that's private), we seem to be very inefficient: we set the enabled flag for the table for every region (should be done only once). every time we create a new region we create a new hlog and then close it (reuse one instead or see if it's really necessary). we do one rpc to .meta. per region (we should batch put). this should provide drastic speedups even for those creating tables with just 50 regions. ",
        "label": 441
    },
    {
        "text": "add enforcer rule to make sure hbase spark   scala aren't dependencies of unexpected modules  we should have an enforcer plugin rule that makes sure we don't have scala and/or hbase-spark showing up in new modules. (based on prior discussions about limiting the scope of where those things show up in our classpath, esp given scala's poor history on binary compatibility) ",
        "label": 320
    },
    {
        "text": "replicationpeerconfig should be builder style  similar to htd, hcd, replicationpeerconfig should be builder-style: replicationpeerconfig().setfoo(foo).setbar(bar);  see testhtabledescriptor.testclassmethodsarebuilderstyle(). ",
        "label": 177
    },
    {
        "text": "per store entries in compaction queue  although compaction is decided on a per-store basis, right now the compactsplitthread only deals at the region level for queueing. store-level compaction queue entries will give us more visibility into compaction workload + allow us to stop summarizing priorities. ",
        "label": 341
    },
    {
        "text": "bring in code constants in line with default xml's  after the defaults were changed in the xml some constants were left the same. default_hbase_client_pause for example. ",
        "label": 154
    },
    {
        "text": " fb  avoid acquiring the same row lock repeatedly  when processing the multiput, multimutations or multidelete operations, each ipc handler thread tries to acquire a lock for each row key in these batches. if there are duplicated row keys in these batches, previously the ipc handler thread will repeatedly acquire the same row key again and again. so the optimization is to sort each batch operation based on the row key in the client side, and skip acquiring the same row lock repeatedly in the server side. ",
        "label": 302
    },
    {
        "text": "regionserver's logsyncer thread hangs on dfsclient dfsoutputstream waitforackedseqno  during loads into hbase, we are noticing that a rs is sometimes getting stuck. the logsyncer thread:       at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.waitforackedseqno(dfsclient.java:3367)         - locked <0x00002aaac7fef748> (a java.util.linkedlist)         at org.apache.hadoop.hdfs.dfsclient$dfsoutputstream.sync(dfsclient.java:3301)         at org.apache.hadoop.fs.fsdataoutputstream.sync(fsdataoutputstream.java:97)         at org.apache.hadoop.io.sequencefile$writer.syncfs(sequencefile.java:944)         at sun.reflect.generatedmethodaccessor3.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.regionserver.wal.sequencefilelogwriter.sync(sequencefilelogwriter.java:124)         at org.apache.hadoop.hbase.regionserver.wal.hlog.hflush(hlog.java:949) a lot of other threads are stuck on:         at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1925)         at org.apache.hadoop.hbase.regionserver.wal.hlog$logsyncer.addtosyncqueue(hlog.java:916)         at org.apache.hadoop.hbase.regionserver.wal.hlog.sync(hlog.java:936)         at org.apache.hadoop.hbase.regionserver.wal.hlog.append(hlog.java:828)         at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1657)         at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1425)         at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1393)         at org.apache.hadoop.hbase.regionserver.hregionserver.put(hregionserver.java:1665)         at org.apache.hadoop.hbase.regionserver.hregionserver.multiput(hregionserver.java:2326) subsequently, trying to disable the table, which in turn attempts to close the region(s), caused internalflushcache() also to get stuck here:       at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(abstractqueuedsynchronizer.java:1114)         at java.util.concurrent.locks.reentrantreadwritelock$writelock.lock(reentrantreadwritelock.java:807)         at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:974)         at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:511)         - locked <0x00002aaab76af670> (a java.lang.object)         at org.apache.hadoop.hbase.regionserver.hregion.close(hregion.java:463)         at org.apache.hadoop.hbase.regionserver.hregionserver.closeregion(hregionserver.java:1468)         at org.apache.hadoop.hbase.regionserver.hregionserver$worker.run(hregionserver.java:1329) i'll attach the full jstack trace soon. ",
        "label": 192
    },
    {
        "text": "wait until regions are assigned in testsplittransactiononcluster  i've seen various failures where a table is created in the tests and then all regions are retrieved from the cluster, where the number of returned regions is 0, because the region have not been assigned, yet, or the am does not know about them, yet. ",
        "label": 286
    },
    {
        "text": " snapshot merge  tests with sleep after minicluster shutdown fail due to interrupt flag   something in the merge has set the interrupted flag on the main test threads of testreplicationdisabledinactivepeer, testrestartcluster, and testcatalogtrackeroncluster. these unacceptable hacks make the tests run and pass: diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/testcatalogtrackeroncluster.java b/hbase-server/src/test/java/or index f3e57d6..a8d2ef7 100644 --- a/hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/testcatalogtrackeroncluster.java +++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/testcatalogtrackeroncluster.java @@ -47,6 +47,7 @@ public class testcatalogtrackeroncluster {      // shutdown hbase.      util.shutdownminihbasecluster();      // give the various zkwatchers some time to settle their affairs. +    thread.interrupted(); // hack clear interrupt state.      thread.sleep(1000);        // mess with the root location in the running zk.  set it to be nonsense. diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/testrestartcluster.java b/hbase-server/src/test/java/org/apache/h index 15225e1..9f7f526 100644 --- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/testrestartcluster.java +++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/testrestartcluster.java @@ -108,6 +108,7 @@ public class testrestartcluster {      util.shutdownminihbasecluster();        log.info(\"\\n\\nsleeping a bit\"); +    thread.interrupted(); // hack clear interrupt state.      thread.sleep(2000);        log.info(\"\\n\\nstarting cluster the second time\"); diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/testreplicationdisableinactivepeer.java b/hbase-server/src/t index b089fbe..8162f4b 100644 --- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/testreplicationdisableinactivepeer.java +++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/testreplicationdisableinactivepeer.java @@ -50,6 +50,7 @@ public class testreplicationdisableinactivepeer extends testreplicationbase {      // enabling and shutdown the peer      admin.enablepeer(\"2\");      utility2.shutdownminihbasecluster(); +    thread.interrupted(); // hack clear interrupted flag.        byte[] rowkey = bytes.tobytes(\"disable inactive peer\");      put put = new put(rowkey); on the snapshot branch and on the trunk branch before the merge, these tests passed. need to figure out how they combination caused this behavior change. ",
        "label": 248
    },
    {
        "text": "when copying tables cfs  allow cf names to be changed  the copytable mapreduce job uses the import class to read cfs in one table and output them to a different table. here is a patch that allows different cf names to be used in the source and destination. ",
        "label": 126
    },
    {
        "text": "implement in memory column  hcd already talks of in-memory columns; its just not implemented. with hfile this should now be possible \u2013 if set, read in the whole storefile and serve from the blockbuffer. ",
        "label": 247
    },
    {
        "text": "enable and disable of table needs a bit of loving in new master  the tools are in place to do a more reliable enable/disable of tables. some work has been done to hack in a basic enable/disable but its not enough \u2013 see the test avro/thrift tests where a disable/enable/disable switchback can confuse the table state (and has been disabled until this issue addressed). this issue is about finishing off enable/disable in the new master. i think we need to add to the table znode an enabling/disabling state rather than have them binary with a watcher that will stop an enable (or disable) starting until the previous completes (currently we atomically switch the state though the region close/open lags \u2013 some work in enable/disable handlers helps in that they won't complete till all regions have transitioned.. but its not enough). need to add tests too. marking issue critical bug because loads of the questions we get on lists are about enable/disable probs. ",
        "label": 314
    },
    {
        "text": "implement client push back for async client  ",
        "label": 149
    },
    {
        "text": "bloomfilter's use of bitset is too inefficient  from the logfile run of testbloomfilter with special sizeof agent jar: writing bloom filter for: hdfs://localhost:64003/user/ryan/testcomputedparameters/1278366260/contents/6159869037185296839 for size: 100  2009-03-06 01:54:25,491 debug [regionserver:0.cacheflusher] regionserver.storefile$storefilewriter(319): new bloom filter: vectorsize: 1175 hash_count: 5 numkeys: 100  serialized bloomfilter size: 160  in memory bf size: 1248 as we can see, the bit vector is 1175 bits, and the serialized size is fairly compact - 160 bytes. but the in-memory size is nearly 10x bigger than it has to be. looking in bloomfilter we see:  bitset bits; is the only field. clearly it seems the bitset is using 1 byte = 1 bit. that is an 8 time expansion of where we should be. considering every hfile could potentially have a bloom filter, and bloom filters are more likely to have bit vector sizes of 10,000-100,000, we should do something about this. aka: write our own bit-set that uses byte[] and bit ops. ",
        "label": 341
    },
    {
        "text": " coprocessors  improve region server metrics to report loaded coprocessors to master  hbase-3512 is about listing loaded cp classes at shell. to make it more generic, we need a way to report this piece of information from region to master (or just at region server level). so later on, we can display the loaded class names at shell as well as web console. ",
        "label": 164
    },
    {
        "text": " replication  remove replicationzookeeper class  once all of the logic in replicationzookeeper has been refactored into three interfaces (for status, queues, and peers), there is almost no logic in replicationzookeeper. it can now be removed and classes that call it should be refactored to call the state interfaces directly. ",
        "label": 103
    },
    {
        "text": "allow different htable instances to share one executorservice  this came out of lily 1.1.1 release: use a shared executorservice for all htable instances, leading to better (or actual) thread reuse ",
        "label": 286
    },
    {
        "text": "hbase it tests failing with oome  permgen  let me up the heap used when failsafe forks. here is example oome doing itbll: 2015-11-16 03:09:15,073 info  [thread-694] actions.batchrestartrsaction(69): starting region server:asf905.gq1.ygridcore.net 2015-11-16 03:09:15,099 info  [thread-694] client.connectionutils(104): regionserver/asf905.gq1.ygridcore.net/67.195.81.149:0 server-side hconnection retries=350 2015-11-16 03:09:15,099 info  [thread-694] ipc.simplerpcscheduler(128): using deadline as user call queue, count=1 2015-11-16 03:09:15,101 info  [thread-694] ipc.rpcserver$listener(607): regionserver/asf905.gq1.ygridcore.net/67.195.81.149:0: started 3 reader(s) listening on port=36114 2015-11-16 03:09:15,103 info  [thread-694] fs.hfilesystem(252): added intercepting call to namenode#getblocklocations so can do block reordering using class class org.apache.hadoop.hbase.fs.hfilesystem$reorderwalblocks 2015-11-16 03:09:15,104 info  [thread-694] zookeeper.recoverablezookeeper(120): process identifier=regionserver:36114 connecting to zookeeper ensemble=localhost:50139 2015-11-16 03:09:15,117 debug [thread-694-eventthread] zookeeper.zookeeperwatcher(554): regionserver:361140x0, quorum=localhost:50139, baseznode=/hbase received zookeeper event, type=none, state=syncconnected, path=null 2015-11-16 03:09:15,118 debug [thread-694] zookeeper.zkutil(492): regionserver:361140x0, quorum=localhost:50139, baseznode=/hbase set watcher on existing znode=/hbase/master 2015-11-16 03:09:15,119 debug [thread-694] zookeeper.zkutil(492): regionserver:361140x0, quorum=localhost:50139, baseznode=/hbase set watcher on existing znode=/hbase/running 2015-11-16 03:09:15,119 debug [thread-694-eventthread] zookeeper.zookeeperwatcher(638): regionserver:36114-0x1510e2c6f1d0029 connected 2015-11-16 03:09:15,120 info  [rpcserver.responder] ipc.rpcserver$responder(926): rpcserver.responder: starting 2015-11-16 03:09:15,121 info  [rpcserver.listener,port=36114] ipc.rpcserver$listener(738): rpcserver.listener,port=36114: starting 2015-11-16 03:09:15,121 debug [thread-694] ipc.rpcexecutor(115): b.default start handler index=0 queue=0 2015-11-16 03:09:15,121 debug [thread-694] ipc.rpcexecutor(115): b.default start handler index=1 queue=0 2015-11-16 03:09:15,121 debug [thread-694] ipc.rpcexecutor(115): b.default start handler index=2 queue=0 2015-11-16 03:09:15,122 debug [thread-694] ipc.rpcexecutor(115): b.default start handler index=3 queue=0 2015-11-16 03:09:15,122 debug [thread-694] ipc.rpcexecutor(115): b.default start handler index=4 queue=0 2015-11-16 03:09:15,122 debug [thread-694] ipc.rpcexecutor(115): priority start handler index=0 queue=0 2015-11-16 03:09:15,123 debug [thread-694] ipc.rpcexecutor(115): priority start handler index=1 queue=1 2015-11-16 03:09:15,123 debug [thread-694] ipc.rpcexecutor(115): priority start handler index=2 queue=0 2015-11-16 03:09:15,123 debug [thread-694] ipc.rpcexecutor(115): priority start handler index=3 queue=1 2015-11-16 03:09:15,124 debug [thread-694] ipc.rpcexecutor(115): priority start handler index=4 queue=0 2015-11-16 03:09:15,124 debug [thread-694] ipc.rpcexecutor(115): replication start handler index=0 queue=0 2015-11-16 03:09:15,124 debug [thread-694] ipc.rpcexecutor(115): replication start handler index=1 queue=0 2015-11-16 03:09:15,124 debug [thread-694] ipc.rpcexecutor(115): replication start handler index=2 queue=0 2015-11-16 03:09:15,761 debug [rs:0;asf905:36114] client.connectionmanager$hconnectionimplementation(715): connection construction failed java.io.ioexception: java.lang.outofmemoryerror: permgen space at org.apache.hadoop.hbase.client.registryfactory.getregistry(registryfactory.java:43) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.setupregistry(connectionmanager.java:886) at org.apache.hadoop.hbase.client.connectionmanager$hconnectionimplementation.<init>(connectionmanager.java:692) at org.apache.hadoop.hbase.client.connectionutils$2.<init>(connectionutils.java:154) at org.apache.hadoop.hbase.client.connectionutils.createshortcircuitconnection(connectionutils.java:154) at org.apache.hadoop.hbase.regionserver.hregionserver.createclusterconnection(hregionserver.java:689) at org.apache.hadoop.hbase.regionserver.hregionserver.setupclusterconnection(hregionserver.java:720) at org.apache.hadoop.hbase.regionserver.hregionserver.preregistrationinitialization(hregionserver.java:733) at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:889) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.runregionserver(minihbasecluster.java:156) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.access$000(minihbasecluster.java:108) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver$1.run(minihbasecluster.java:140) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:356) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1594) at org.apache.hadoop.hbase.security.user$securehadoopuser.runas(user.java:334) at org.apache.hadoop.hbase.minihbasecluster$minihbaseclusterregionserver.run(minihbasecluster.java:138) at java.lang.thread.run(thread.java:745) caused by: java.lang.outofmemoryerror: permgen space at sun.misc.unsafe.defineclass(native method) at sun.reflect.classdefiner.defineclass(classdefiner.java:63) at sun.reflect.methodaccessorgenerator$1.run(methodaccessorgenerator.java:399) at sun.reflect.methodaccessorgenerator$1.run(methodaccessorgenerator.java:396) at java.security.accesscontroller.doprivileged(native method) at sun.reflect.methodaccessorgenerator.generate(methodaccessorgenerator.java:395) at sun.reflect.methodaccessorgenerator.generateconstructor(methodaccessorgenerator.java:94) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:48) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.lang.reflect.constructor.newinstance(constructor.java:526) at java.lang.class.newinstance(class.java:383) at org.apache.hadoop.hbase.client.registryfactory.getregistry(registryfactory.java:41) ... 17 more ",
        "label": 314
    },
    {
        "text": "a canary monitoring program specifically for regionserver  motivation  this ticket is to provide a canary monitoring tool specifically for hregionserver, details as follows  1. this tool is required by operation team due to they thought that the canary for each region of a hbase is too many for them, so i implemented this coarse-granular one based on the original o.a.h.h.tool.canary for them  2. and this tool is implemented by multi-threading, which means the each get request sent by a thread. the reason i use this way is due to we suffered the region server hung issue by now the root cause is still not clear. so this tool can help operation team to detect hung region server if any. example  1. the tool docs  ./bin/hbase org.apache.hadoop.hbase.tool.regionservercanary -help  usage: [opts] [regionservername 1 [regionservrname 2...]]  regionservername - fqdn servername, can use linux command:hostname -f to check your servername  where [-opts] are:  -help show this help and exit.  -e use regionservername as regular expression  which means the regionservername is regular expression pattern  -f <b> stop whole program if first error occurs, default is true  -t <n> timeout for a check, default is 600000 (milisecs)  -daemon continuous check at defined intervals.  -interval <n> interval between checks (sec) 2. will send a request to each regionserver in a hbase cluster  ./bin/hbase org.apache.hadoop.hbase.tool.regionservercanary 3. will send a request to a regionserver by given name  ./bin/hbase org.apache.hadoop.hbase.tool.regionservercanary rs1.domainname 4. will send a request to regionserver(s) by given regular-expression  /opt/trend/circus-opstool/bin/hbase-canary-monitor-each-regionserver.sh -e rs1.domainname.pattern  // another example  ./bin/hbase org.apache.hadoop.hbase.tool.regionservercanary -e tw-poc-tm-puppet-hdn[0-9]{1,2}.client.tw.trendnet.org 5. will send a request to a regionserver and also set a timeout limit for this test  // query regionserver:rs1.domainname with timeout limit 10sec  // -f false, means that will not exit this program even test failed  ./bin/hbase org.apache.hadoop.hbase.tool.regionservercanary -f false -t 10000 rs1.domainname  // echo \"1\" if timeout  echo \"$?\" 6. will run as daemon mode, which means it will send request to each regionserver periodically  ./bin/hbase org.apache.hadoop.hbase.tool.regionservercanary -daemon ",
        "label": 551
    },
    {
        "text": "backport bug fixes which were fixed in x and x versions but not in x version  backport the bug fixes which were fixed in 1.2.x and 1.4.x branch but missed in 1.3.x branch.   i have used the below query to those jiras.  project = hbase and status in (resolved, closed) and fixversion in (1.4.0, 1.4.1) and fixversion in (1.2.7, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6) and fixversion not in (1.3.0, 1.3.1, 1.3.2) ",
        "label": 53
    },
    {
        "text": "hbck  fix clearinmaster is commented out  in 0.90 rc the actual guts of the clearinmaster function are commented out, but it logs that it's doing something. we should either make the function do something, or log something like: \"hbck currently unable to fix region stuck in transition\", rather than indicate we're doing something when in fact it's a noop. ",
        "label": 248
    },
    {
        "text": "provide api for retrieving info port when hbase master info port is set to  when hbase.master.info.port is set to 0, info port is dynamically determined. an api should be provided so that client can retrieve the actual info port. ",
        "label": 411
    },
    {
        "text": "scan acid problem with concurrent puts   when scanning a table sometimes rows that have multiple column families get split into two rows if there are concurrent writes. in this particular case we are overwriting the contents of a get directly back onto itself as a put. for example, this is a two cf row (with \"f1\", \"f2\", .. \"f9\" cfs). it is actually returned as two rows (#55 and #56). interestingly if the two were merged we would have a single proper row. row row0000024461 had time stamps: [55: keyvalues= {row0000024461/f0:data/1318200440867/put/vlen=1000, row0000024461/f0:qual/1318200440867/put/vlen=10, row0000024461/f1:data/1318200440867/put/vlen=1000, row0000024461/f1:qual/1318200440867/put/vlen=10, row0000024461/f2:data/1318200440867/put/vlen=1000, row0000024461/f2:qual/1318200440867/put/vlen=10, row0000024461/f3:data/1318200440867/put/vlen=1000, row0000024461/f3:qual/1318200440867/put/vlen=10, row0000024461/f4:data/1318200440867/put/vlen=1000, row0000024461/f4:qual/1318200440867/put/vlen=10} ,   56: keyvalues= {row0000024461/f5:data/1318200440867/put/vlen=1000, row0000024461/f5:qual/1318200440867/put/vlen=10, row0000024461/f6:data/1318200440867/put/vlen=1000, row0000024461/f6:qual/1318200440867/put/vlen=10, row0000024461/f7:data/1318200440867/put/vlen=1000, row0000024461/f7:qual/1318200440867/put/vlen=10, row0000024461/f8:data/1318200440867/put/vlen=1000, row0000024461/f8:qual/1318200440867/put/vlen=10, row0000024461/f9:data/1318200440867/put/vlen=1000, row0000024461/f9:qual/1318200440867/put/vlen=10} ] i've only tested this on 0.90.1+patches and 0.90.3+patches, but it is consistent and duplicatable. ",
        "label": 248
    },
    {
        "text": "add zktop like output to hbase's master ui  zk jsp   it would be nice to add a zktop (http://github.com/phunt/zktop#readme) like output to the master's zk.jsp. that way one could quickly gather important insights on the zk status. ",
        "label": 285
    },
    {
        "text": "requestspersecond counter stuck at  running trunk @ r1163343, all of the requestspersecond counters are showing 0 both in the master ui and in the rs ui. the writerequestscount metric is properly updating in the rs ui. ",
        "label": 289
    },
    {
        "text": "backport hbase and hbase to  ",
        "label": 543
    },
    {
        "text": "serve writes during log split  for tracking the \"serve writes\" portion of hbase-6752. ",
        "label": 233
    },
    {
        "text": "make assignmentmanager standalone testable by having its constructor take interfaces rather than a catalogtracker and a servermanager  if we could stand up an instance of assignmentmanager, a core fat class that has a bunch of critical logic managing state transitions, then it'd be easier writing unit tests around its logic. currently its hard because it takes a servermanager and a catalogtracker, but a little bit of work could turn these into interfaces. sm looks easy to do. changing ct into an interface instead might ripple a little through the code base but it'd probably be well worth it. ",
        "label": 111
    },
    {
        "text": "provide new non copy mechanism to assure atomic reads in get and scan  hbase-2037 introduced a new memstorescanner which triggers a concurrentskiplistmap.buildfromsorted clone of the memstore and snapshot when starting a scan. after upgrading to 0.20.3, we noticed a big slowdown in our use of short scans. some of our data repesent a time series. the data is stored in time series order, mr jobs often insert/update new data at the end of the series, and queries usually have to pick up some or all of the series. these are often scans of 0-100 rows at a time. to load one page, we'll observe about 20 such scans being triggered concurrently, and they take 2 seconds to complete. doing a thread dump of a region server shows many threads in concurrentskiplistmap.biuldfromsorted which traverses the entire map of key values to copy it. ",
        "label": 547
    },
    {
        "text": "move up to thrift  move hbase thrift bits up to thrift 0.2.0 when it is released. ",
        "label": 266
    },
    {
        "text": "snapshotfilecache may fail to load the correct snapshot file list when there is an on going snapshot operation  and it seems that it is not only a test issue, we do delete the files under the archive directory, which is incorrect. need to find out why, this maybe a serious bug. ",
        "label": 149
    },
    {
        "text": "backport hbase  hbtop  to branch  backport parent issue to branch-1. ",
        "label": 455
    },
    {
        "text": "add namespace help info in table related shell commands  currently in the help info of table related shell command, we don't mention or give namespace as part of the table name.   for example, to create table: hbase(main):001:0> help 'create' creates a table. pass a table name, and a set of column family specifications (at least one), and, optionally, table configuration. column specification can be a simple string (name), or a dictionary (dictionaries are described below in main help output), necessarily including name attribute. examples:   hbase> create 't1', {name => 'f1', versions => 5}   hbase> create 't1', {name => 'f1'}, {name => 'f2'}, {name => 'f3'}   hbase> # the above in shorthand would be the following:   hbase> create 't1', 'f1', 'f2', 'f3'   hbase> create 't1', {name => 'f1', versions => 1, ttl => 2592000, blockcache => true}   hbase> create 't1', {name => 'f1', configuration => {'hbase.hstore.blockingstorefiles' => '10'}} table configuration options can be put at the end. examples:   hbase> create 't1', 'f1', splits => ['10', '20', '30', '40']   hbase> create 't1', 'f1', splits_file => 'splits.txt', owner => 'johndoe'   hbase> create 't1', {name => 'f1', versions => 5}, metadata => { 'mykey' => 'myvalue' }   hbase> # optionally pre-split the table into numregions, using   hbase> # splitalgo (\"hexstringsplit\", \"uniformsplit\" or classname)   hbase> create 't1', 'f1', {numregions => 15, splitalgo => 'hexstringsplit'}   hbase> create 't1', 'f1', {numregions => 15, splitalgo => 'hexstringsplit', configuration => {'hbase.hregion.scan.loadcolumnfamiliesondemand' => 'true'}} you can also keep around a reference to the created table:   hbase> t1 = create 't1', 'f1' which gives you a reference to the table named 't1', on which you can then call methods. we should document the usage of namespace in these commands.  for example: #namespace=foo and table qualifier=bar  create 'foo:bar', 'fam' #namespace=default and table qualifier=bar  create 'bar', 'fam' ",
        "label": 234
    },
    {
        "text": "backport hbase to branch  there seems to be need of hbase-512's capabilities in the 0.1 branch. people can't write as much data as they actually want to without oomeing. let's backport the features of 512 into 0.1. it might be a little uglier, but it's worth it. ",
        "label": 241
    },
    {
        "text": "revisit our jmx view  see hbase-5552 for some notes. as is, its hard to make sense of what we are publishing and its not amenable to new beans showing up \u2013 it'll be hard to fit them in to give a nice logical jmx view. fix. entertain doing this at the singularity. ",
        "label": 154
    },
    {
        "text": "snapshot restore may fail due to nullpointerexception  in our qa run, certain snapshot restore request seemed to hang.  the following was found in master log: 2013-11-11 08:52:43,887 info org.apache.hadoop.hbase.util.fsvisitor: no logs under directory:hdfs://qeempress21:8020/apps/hbase/data/.hbase-snapshot/snapshot_tablethree_mod/wals^m 2013-11-11 08:52:43,887 info org.apache.hadoop.hbase.master.regionstates: transitioned {e048f867bbb31702dc1dd0498db53b82 state=split, ts=1384159894259, server=qeempress23,60020,1384146414618} to {e048f867bbb31702dc1dd0498db53b82 state=offline, ts=1384159963887, server=null}^m 2013-11-11 08:52:43,896 info org.apache.hadoop.hbase.catalog.metaeditor: deleted [{encoded => 214a354a9e9f07d3c4e9420d2fdd8d28, name => 'tablethree_mod,,1384159891126.214a354a9e9f07d3c4e9420d2fdd8d28.', startkey => '', endkey => '\\x5c011'}, {encoded => 4dad288061dac82bb420fb6dd526eea5, name => 'tablethree_mod,\\x5c011,1384159900704.4dad288061dac82bb420fb6dd526eea5.', startkey => '\\x5c011', endkey => '\\x5c100'}, {encoded => e98b84924307f22d4318cfe235445ba7, name => 'tablethree_mod,\\x5c100,1384159900704.e98b84924307f22d4318cfe235445ba7.', startkey => '\\x5c100', endkey => ''}]^m 2013-11-11 08:52:43,900 info org.apache.hadoop.hbase.catalog.metaeditor: added 1^m 2013-11-11 08:52:43,913 debug org.apache.hadoop.hbase.zookeeper.lock.zkinterprocesslockbase: released /hbase/table-lock/tablethree_mod/write-master:600000000000006^m 2013-11-11 08:52:43,913 error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event c_m_restore_snapshot^m java.lang.nullpointerexception^m   at org.apache.hadoop.hbase.catalog.metaeditor.deleteregions(metaeditor.java:488)^m   at org.apache.hadoop.hbase.catalog.metaeditor.overwriteregions(metaeditor.java:534)^m   at org.apache.hadoop.hbase.master.snapshot.restoresnapshothandler.handletableoperation(restoresnapshothandler.java:160)^m   at org.apache.hadoop.hbase.master.handler.tableeventhandler.process(tableeventhandler.java:129)^m   at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:128)^m   at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)^m   at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)^m looks like null was passed in the following call metaeditor#overwriteregions():     deleteregions(catalogtracker, regioninfos); ",
        "label": 155
    },
    {
        "text": "npe when executors are down but events are still coming in  minor annoyance when shutting down a cluster and the master is still receiving events from zookeeper: 2011-09-22 23:53:01,552 debug org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: master:60000-0x3292d87deb004f received interruptedexception, doing nothing here  java.lang.interruptedexception  at java.lang.object.wait(native method)  at java.lang.object.wait(object.java:485)  at org.apache.zookeeper.clientcnxn.submitrequest(clientcnxn.java:1317)  at org.apache.zookeeper.zookeeper.delete(zookeeper.java:726)  at org.apache.hadoop.hbase.zookeeper.zkutil.deletenode(zkutil.java:938)  at org.apache.hadoop.hbase.zookeeper.zkassign.deletenode(zkassign.java:407)  at org.apache.hadoop.hbase.zookeeper.zkassign.deleteopenednode(zkassign.java:284)  at org.apache.hadoop.hbase.master.handler.openedregionhandler.process(openedregionhandler.java:88)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:156)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  ...  2011-09-22 23:53:01,558 debug org.apache.hadoop.hbase.executor.executorservice: executor service [master_open_region-sv2borg170:60000] not found in {}  2011-09-22 23:53:01,558 error org.apache.zookeeper.clientcnxn: error while calling watcher  java.lang.nullpointerexception  at org.apache.hadoop.hbase.executor.executorservice.submit(executorservice.java:220)  at org.apache.hadoop.hbase.master.assignmentmanager.handleregion(assignmentmanager.java:447)  at org.apache.hadoop.hbase.master.assignmentmanager.nodedatachanged(assignmentmanager.java:546)  at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:281)  at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:530)  at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:506) it's annoying because it then spams you with a bunch of npes that have nothing to do with the reason the master is shutting down. googling i saw someone also had that issue in june: http://pastebin.com/5tqrj0nq ",
        "label": 229
    },
    {
        "text": "implement secure async protobuf wal writer  ",
        "label": 149
    },
    {
        "text": "rest tests don't use ephemeral ports  saw this failure on my hudson: java.net.bindexception: address already in use  ...elided...  at org.apache.hadoop.hbase.rest.hbaserestclustertestbase.startservletcontainer(hbaserestclustertestbase.java:60)  at org.apache.hadoop.hbase.rest.hbaserestclustertestbase.setup(hbaserestclustertestbase.java:27)  at org.apache.hadoop.hbase.rest.testtableresource.setup(testtableresource.java:63) ",
        "label": 38
    },
    {
        "text": "cleanup htable public interface  hbase-6580 replaced the preferred means of htableinterface acquisition to the hconnection#gettable factory methods. hbase-9117 removes the hconnection cache, placing the burden of responsible connection cleanup on whomever acquires it. the remaining htable constructors use a connection instance and manage their own hconnection on the callers behalf. this is convenient but also a surprising source of poor performance for anyone accustomed to the previous connection caching behavior. i propose deprecating those remaining constructors for 0.98/0.96 and removing them for 1.0. while i'm at it, i suggest we pursue some api hygiene in general and convert htable into an interface. i'm sure there are method overloads for accepting string/byte[]/tablename where just tablename is sufficient. can that be done for 1.0 as well? ",
        "label": 155
    },
    {
        "text": "flushsnapshotsubprocedure should wait for concurrent region flush  to finish  in the following thread:  http://search-hadoop.com/m/hbase/ygbbmxkehli9zo  jacob described the scenario where data from certain region were missing in the snapshot. here was related region server log:  https://pastebin.com/1ecxjhrp he pointed out that concurrent flush from memstoreflusher.1 thread was not initiated from the thread pool for snapshot. in regionsnapshottask#call() method there is this:           region.flush(true); the return value is not checked. in hregion#flushcache(), result.cannot_flush may be returned due to:           string msg = \"not flushing since \"               + (writestate.flushing ? \"already flushing\"               : \"writes not enabled\"); this implies that flushsnapshotsubprocedure may incorrectly skip waiting for the concurrent flush to complete. ",
        "label": 441
    },
    {
        "text": "take last version of the hbase design doc  and make documentation out of it  hbase-1249 finishes with the api design we all voted on for hbase-880. this issue is about adding documentation of the new api into javadoc packages and wiki. ",
        "label": 161
    },
    {
        "text": "fix precommit javadoc warnings  we've been getting 2 new javadoc warnings. here's what are the warnings from a recent precommit build: // hbase-common [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[51,15] sun.misc.unsafe is sun proprietary api and may be removed in a future release [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[1095,19] sun.misc.unsafe is sun proprietary api and may be removed in a future release [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[1101,21] sun.misc.unsafe is sun proprietary api and may be removed in a future release [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:[1106,28] sun.misc.unsafe is sun proprietary api and may be removed in a future release ... 2 warnings [warning] javadoc warnings [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:51: warning: sun.misc.unsafe is sun proprietary api and may be removed in a future release [warning] import sun.misc.unsafe; [warning] ^ [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes.java:1095: warning: sun.misc.unsafe is sun proprietary api and may be removed in a future release [warning] static final unsafe theunsafe; [warning] ^ ... // hbase-protocol 2 warnings [warning] javadoc warnings [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/masteradminprotos.java:33377: warning - @return tag cannot be used in method with void return type. [warning] /home/jenkins/jenkins-slave/workspace/precommit-hbase-build/trunk/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/masteradminprotos.java:32381: warning - @return tag cannot be used in method with void return type. ... i believe the new ones are the protobuf related warnings. ",
        "label": 314
    },
    {
        "text": "rest support for namespaces  expand rest services to include addition features: create namespace alter namespace describe namespace drop namespace list tables in a specific namespace list all namespaces. ",
        "label": 308
    },
    {
        "text": "integrate hdfs request profiling with hbase request profiling  since the building blocks to retrieve the regionserver/datanode profiling data is done (in facebook's hdfs branch \u2013 the changes are/will be posted to github soon), it would be great to integrate them together, so that the hbase client can not only get the regionserver metrics but also the datanode status. it will offer the client a much clear view from end to end perspective including the disk/network level detail information for each request. ",
        "label": 154
    },
    {
        "text": "remove filter filterrow list   0.96+ the filterrow(list) method is deprecated:    * warning: please to not override this method.  instead override {@link #filterrowcells(list)}.    * this is for transition from 0.94 -> 0.96    **/   @deprecated   abstract public void filterrow(list<keyvalue> kvs) throws ioexception; this method should be removed from filter classes for 1.0 ",
        "label": 441
    },
    {
        "text": "zookeeper session got closed while trying to assign the region to rs using hbck  fix  after running the hbck in the cluster ,it is found that one region is not assigned  so the hbck -fix is used to fix this   but the assignment didnt happen since the zookeeper session is closed  please find the attached trace for more details  -----------------------------------------  trying to fix unassigned region...  12/04/03 11:02:57 info util.hbasefsckrepair: region still in transition, waiting for it to become assigned: {name => 'ufdr,002300,1333379123498.00871fbd7583512e12c4eb38e900be8d.', startkey => '002300', endkey => '002311', encoded => 00871fbd7583512e12c4eb38e900be8d,} 12/04/03 11:02:58 info client.hconnectionmanager$hconnectionimplementation: closed zookeeper sessionid=0x236738a2630000a  12/04/03 11:02:58 info zookeeper.zookeeper: session: 0x236738a2630000a closed  error: region { meta => ufdr,010444,1333379123857.01594219211d0035b9586f98954462e1., hdfs => hdfs://10.18.40.25:9000/hbase/ufdr/01594219211d0035b9586f98954462e1, deployed => } not deployed on any region server.  trying to fix unassigned region...  12/04/03 11:02:58 info zookeeper.clientcnxn: eventthread shut down  12/04/03 11:02:58 warn zookeeper.zkutil: hconnection-0x236738a2630000a unable to set watcher on znode (/hbase)  org.apache.zookeeper.keeperexception$sessionexpiredexception: keepererrorcode = session expired for /hbase  at org.apache.zookeeper.keeperexception.create(keeperexception.java:127)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)  at org.apache.zookeeper.zookeeper.exists(zookeeper.java:1021)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:150)  at org.apache.hadoop.hbase.zookeeper.zkutil.checkexists(zkutil.java:263)  at org.apache.hadoop.hbase.zookeeper.zookeepernodetracker.checkifbasenodeavailable(zookeepernodetracker.java:208)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.checkifbasenodeavailable(hconnectionmanager.java:695)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getmaster(hconnectionmanager.java:626)  at org.apache.hadoop.hbase.client.hbaseadmin.getmaster(hbaseadmin.java:211)  at org.apache.hadoop.hbase.client.hbaseadmin.assign(hbaseadmin.java:1325)  at org.apache.hadoop.hbase.util.hbasefsckrepair.forceofflineinzk(hbasefsckrepair.java:109)  at org.apache.hadoop.hbase.util.hbasefsckrepair.fixunassigned(hbasefsckrepair.java:92)  at org.apache.hadoop.hbase.util.hbasefsck.tryassignmentrepair(hbasefsck.java:1235)  at org.apache.hadoop.hbase.util.hbasefsck.checkregionconsistency(hbasefsck.java:1351)  at org.apache.hadoop.hbase.util.hbasefsck.checkandfixconsistency(hbasefsck.java:1114)  at org.apache.hadoop.hbase.util.hbasefsck.onlineconsistencyrepair(hbasefsck.java:356)  at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:375)  at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:2894)  12/04/03 11:02:58 error zookeeper.zookeeperwatcher: hconnection-0x236738a2630000a received unexpected keeperexception, re-throwing exception  org.apache.zookeeper.keeperexception$sessionexpiredexception: keepererrorcode = session expired for /hbase  at org.apache.zookeeper.keeperexception.create(keeperexception.java:127)  at org.apache.zookeeper.keeperexception.create(keeperexception.java:51)  at org.apache.zookeeper.zookeeper.exists(zookeeper.java:1021)  at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:150)  at org.apache.hadoop.hbase.zookeeper.zkutil.checkexists(zkutil.java:263)  at org.apache.hadoop.hbase.zookeeper.zookeepernodetracker.checkifbasenodeavailable(zookeepernodetracker.java:208)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.checkifbasenodeavailable(hconnectionmanager.java:695)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getmaster(hconnectionmanager.java:626)  at org.apache.hadoop.hbase.client.hbaseadmin.getmaster(hbaseadmin.java:211)  at org.apache.hadoop.hbase.client.hbaseadmin.assign(hbaseadmin.java:1325)  at org.apache.hadoop.hbase.util.hbasefsckrepair.forceofflineinzk(hbasefsckrepair.java:109)  at org.apache.hadoop.hbase.util.hbasefsckrepair.fixunassigned(hbasefsckrepair.java:92)  at org.apache.hadoop.hbase.util.hbasefsck.tryassignmentrepair(hbasefsck.java:1235)  at org.apache.hadoop.hbase.util.hbasefsck.checkregionconsistency(hbasefsck.java:1351)  at org.apache.hadoop.hbase.util.hbasefsck.checkandfixconsistency(hbasefsck.java:1114)  at org.apache.hadoop.hbase.util.hbasefsck.onlineconsistencyrepair(hbasefsck.java:356)  at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:375)  at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:2894)  12/04/03 11:02:58 info client.hconnectionmanager$hconnectionimplementation: this client just lost it's session with zookeeper, trying to reconnect.  12/04/03 11:02:58 info client.hconnectionmanager$hconnectionimplementation: trying to reconnect to zookeeper  12/04/03 11:02:58 info zookeeper.zookeeper: initiating client connection, connectstring=10.18.40.21:2181,10.18.40.25:2181,10.18.40.93:2181 sessiontimeout=60000 watcher=hconnection  12/04/03 11:02:58 info zookeeper.clientcnxn: opening socket connection to server /10.18.40.93:2181  12/04/03 11:02:58 info zookeeper.recoverablezookeeper: the identifier of this process is 18333@host-10-18-40-93  12/04/03 11:02:58 warn client.zookeepersaslclient: securityexception: java.lang.securityexception: unable to locate a login configuration occurred when trying to find jaas configuration.  12/04/03 11:02:58 info client.zookeepersaslclient: client will not sasl-authenticate because the default jaas configuration section 'client' could not be found. if you are not using sasl, you may ignore this. on the other hand, if you expected sasl to work, please fix your jaas configuration.  12/04/03 11:02:58 info zookeeper.clientcnxn: socket connection established to host-10-18-40-93/10.18.40.93:2181, initiating session  12/04/03 11:02:58 info zookeeper.clientcnxn: session establishment complete on server host-10-18-40-93/10.18.40.93:2181, sessionid = 0x3367392d5140018, negotiated timeout = 40000  12/04/03 11:02:58 info client.hconnectionmanager$hconnectionimplementation: reconnected successfully. this disconnect could have been caused by a network partition or a long-running gc pause, either way it's recommended that you verify your environment.  exception in thread \"main\" org.apache.hadoop.hbase.masternotrunningexception  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.getmaster(hconnectionmanager.java:686)  at org.apache.hadoop.hbase.client.hbaseadmin.getmaster(hbaseadmin.java:211)  at org.apache.hadoop.hbase.client.hbaseadmin.assign(hbaseadmin.java:1325)  at org.apache.hadoop.hbase.util.hbasefsckrepair.forceofflineinzk(hbasefsckrepair.java:109)  at org.apache.hadoop.hbase.util.hbasefsckrepair.fixunassigned(hbasefsckrepair.java:92)  at org.apache.hadoop.hbase.util.hbasefsck.tryassignmentrepair(hbasefsck.java:1235)  at org.apache.hadoop.hbase.util.hbasefsck.checkregionconsistency(hbasefsck.java:1351)  at org.apache.hadoop.hbase.util.hbasefsck.checkandfixconsistency(hbasefsck.java:1114)  at org.apache.hadoop.hbase.util.hbasefsck.onlineconsistencyrepair(hbasefsck.java:356)  at org.apache.hadoop.hbase.util.hbasefsck.onlinehbck(hbasefsck.java:375)  at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:2894)  please find the attached file for more details.. ",
        "label": 248
    },
    {
        "text": "math max  on syncedtillhere lacks synchronization  in fshlog#syncer(), around line 1080:       this.syncedtillhere = math.max(this.syncedtillhere, doneupto); assignment to syncedtillhere after computing max value is not protected by proper synchronization. ",
        "label": 441
    },
    {
        "text": "filter filterrow is called too often  filters rows it shouldn't have  filter#filterrow is called from scanquerymatcher#filterentirerow which is called from storescanner.next. however, if i understood the code correctly, storescanner processes keyvalue-s in a column-oriented order (i.e. after row1-col1 comes row2-col1, not row1-col2). thus, when filterentirerow is called, in reality, the filter only processed (via filterkeyvalue) only one column of a row. ",
        "label": 547
    },
    {
        "text": "hbase daemon sh should clean up pid files on process stop  i just did a simple watchdog script for internal use to enable hbase process restarts on failure. while most of it was easy, i found no way to distinguish between a process failure and intentional stop, since the hbase scripts leave pid files sitting around after shutdown. i think it would be generally useful for hbase-daemon.sh to cleanup pid files on successful process stop. this is really just a matter of adding \"rm $pid\" after stop completes. any concerns/objections? ",
        "label": 180
    },
    {
        "text": "narrow getclosestrowbefore by passing column family  currently when we do getclosestrowbefore, we're usually just interested in catalog table edits to the info family. as its written, we'll also go trawl the region historian column family though its irrelevant and worse, if we got a row out of here with no corresponding info entry, we'd be hosed. add being able to narrow the scope of the getclosestrowbefore by passing column family to dive in. ",
        "label": 314
    },
    {
        "text": "backup  export import  contrib tool for  add a new result/keyvalue based export mapreduce job to contrib for 0.20. make it in the hadoop 0.20 and hbase 0.20 mr api, and hbase 0.20 api (result/put). ",
        "label": 314
    },
    {
        "text": "dropping a 1k  regions table likely ends in a client socket timeout and it's very confusing  i tried truncating a 1.6k regions table from the shell and, after the usual disabling timeout, i then got a socket timeout on the second invocation while it was dropping. it looked like this: error: java.net.sockettimeoutexception: call to sv2borg180/10.20.20.180:61000 failed on socket timeout exception:  java.net.sockettimeoutexception: 60000 millis timeout while waiting for channel to be ready for read. ch :  java.nio.channels.socketchannel[connected local=/10.20.20.180:59153 remote=sv2borg180/10.20.20.180:61000] at first i thought that was coming from the master because hdfs was somehow slow, but then understood that it was my socket that timed out meaning that the master was still dropping the table. calling truncate again, i got: error: unknown table testtable! which means that the table would be deleted... i learned later that it wasn't totally deleted after i shut down the cluster. so it leaves me in a situation where i have to manually delete the files on the fs and the remaining .meta. entries. since i expect a few people will hit this issue rather soon, for 0.90.0, i propose we just set the socket timeout really high in the shell. for 0.90.1, or 0.92, we should do for drop what we do for disabling. ",
        "label": 314
    },
    {
        "text": "double assignment around split  its looking like we have a split updating .meta. with daughter regions and then before we process the split in master, one of the daughters has already been assigned. on processing of the split, we assign daughter again. i thought this had been fixed previously? doesn't seem so. need to look again. here is evidence for region named: summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 first master-side: 2010-03-15 16:06:52,153 info org.apache.hadoop.hbase.master.regionmanager: assigning region summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 to cactus208,60020,12686305486412010-03-15 16:06:52,156 info org.apache.hadoop.hbase.master.basescanner: regionmanager.metascanner scan of 245 row(s) of meta region {server: 172.16.1.209:60020, regionname: .meta.,,1, startkey: <>} complete2010-03-15 16:06:52,156 info org.apache.hadoop.hbase.master.basescanner: all 1 .meta. region(s) scanned2010-03-15 16:06:52,841 info org.apache.hadoop.hbase.master.regionmanager: assigning region summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e5\\x25a4\\x25a7\\x25e8\\x25bf\\x259e\\x25e5\\x2588\\x2598\\x25e5\\x25b8\\x2588\\x25e5\\x2582\\x2585\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e5\\x2587\\x25b9\\x25e9\\x2599\\x25b7\\x25e4\\x25bf\\x25ae\\x25e5\\x25a4\\x258d\\x25e6\\x2596\\x25bd\\x25e5\\x25b7\\x25a5\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e7\\x2594\\x25a8\\x25e5\\x2593\\x2581\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 to cactus210,60020,12686305508862010-03-15 16:06:54,377 info org.apache.hadoop.hbase.master.servermanager: processing msg_report_split: summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e5\\x25a4\\x25a7\\x25e8\\x25bf\\x259e\\x25e5\\x2588\\x2598\\x25e5\\x25b8\\x2588\\x25e5\\x2582\\x2585\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e5\\x2587\\x25b9\\x25e9\\x2599\\x25b7\\x25e4\\x25bf\\x25ae\\x25e5\\x25a4\\x258d\\x25e6\\x2596\\x25bd\\x25e5\\x25b7\\x25a5\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e7\\x2594\\x25a8\\x25e5\\x2593\\x2581\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268611349836: daughters; summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e5\\x25a4\\x25a7\\x25e8\\x25bf\\x259e\\x25e5\\x2588\\x2598\\x25e5\\x25b8\\x2588\\x25e5\\x2582\\x2585\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e5\\x2587\\x25b9\\x25e9\\x2599\\x25b7\\x25e4\\x25bf\\x25ae\\x25e5\\x25a4\\x258d\\x25e6\\x2596\\x25bd\\x25e5\\x25b7\\x25a5\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e7\\x2594\\x25a8\\x25e5\\x2593\\x2581\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017, summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 from cactus209,60020,1268630548451; 1 of 32010-03-15 16:06:54,388 info org.apache.hadoop.hbase.master.regionmanager: assigning region summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 to cactus209,60020,1268630548451 its hard to read but above is an assignment, the split message, then what seems to be same region being assigned again. here is rs side on 209 server: 2010-03-15 16:06:29,727 info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 2010-03-15 16:06:29,792 info org.apache.hadoop.hbase.regionserver.hregion: region summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017/1011052036 available; sequence id is 199443346 2010-03-15 16:06:29,792 info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e5\\x25a4\\x25a7\\x25e8\\x25bf\\x259e\\x25e5\\x2588\\x2598\\x25e5\\x25b8\\x2588\\x25e5\\x2582\\x2585\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e5\\x2587\\x25b9\\x25e9\\x2599\\x25b7\\x25e4\\x25bf\\x25ae\\x25e5\\x25a4\\x258d\\x25e6\\x2596\\x25bd\\x25e5\\x25b7\\x25a5\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e7\\x2594\\x25a8\\x25e5\\x2593\\x2581\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 2010-03-15 16:06:29,793 info org.apache.hadoop.hbase.regionserver.hregion: starting compaction on region summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017 2010-03-15 16:06:29,944 info org.apache.hadoop.hbase.regionserver.hregion: region summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e5\\x25a4\\x25a7\\x25e8\\x25bf\\x259e\\x25e5\\x2588\\x2598\\x25e5\\x25b8\\x2588\\x25e5\\x2582\\x2585\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e5\\x2587\\x25b9\\x25e9\\x2599\\x25b7\\x25e4\\x25bf\\x25ae\\x25e5\\x25a4\\x258d\\x25e6\\x2596\\x25bd\\x25e5\\x25b7\\x25a5\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e7\\x2594\\x25a8\\x25e5\\x2593\\x2581\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017/1971466363 available; sequence id is 199443345 2010-03-15 16:06:32,750 info org.apache.hadoop.hbase.regionserver.hregionserver: msg_region_close_without_report: summary,site_0000000032\\x01pt\\x0120100314000000\\x01\\x25e7\\x258c\\x25ae\\x25e5\\x258e\\x25bf\\x25e5\\x2586\\x2580\\x25e9\\x25b9\\x25b0\\x25e6\\x2591\\x25a9\\x25e6\\x2593\\x25a6\\x25e6\\x259d\\x2590\\x25e6\\x2596\\x2599\\x25e5\\x258e\\x2582\\x2b\\x25e6\\x25b1\\x25bd\\x25e8\\x25bd\\x25a6\\x25e9\\x2585\\x258d\\x25e4\\x25bb\\x25b6\\x25ef\\x25bc\\x258c\\x25e5\\x2598\\x2580\\x25e9\\x2593\\x2583\\x25e9\\x2593\\x2583--\\x25e7\\x259c\\x259f\\x25e5\\x25ae\\x259e\\x25e5\\x25ae\\x2589\\x25e5\\x2585\\x25a8\\x25e7\\x259a\\x2584\\x25e7\\x2594\\x25b5\\x25e8\\x25af\\x259d\\x25e3\\x2580\\x2581\\x25e7\\x25bd\\x2591\\x25e7\\x25bb\\x259c\\x25e4\\x25ba\\x2592\\x25e5\\x258a\\x25a8\\x25e4\\x25ba\\x25a4\\x25e5\\x258f\\x258b\\x25e7\\x25a4\\x25be\\x25e5\\x258c\\x25ba\\x25ef\\x25bc\\x2581,1268640385017: duplicate assignment see how we end with 'duplicate assignment' message? ",
        "label": 314
    },
    {
        "text": "list the stuffs which are using the patent grant license  patents file  of facebook  and then discuss and remove them   see \"apache foundation disallows use of the facebook \u201cbsd+patent\u201d license\" ",
        "label": 98
    },
    {
        "text": "hbase read high availability using timeline consistent region replicas  in the present hbase architecture, it is hard, probably impossible, to satisfy constraints like 99th percentile of the reads will be served under 10 ms. one of the major factors that affects this is the mttr for regions. there are three phases in the mttr process - detection, assignment, and recovery. of these, the detection is usually the longest and is presently in the order of 20-30 seconds. during this time, the clients would not be able to read the region data. however, some clients will be better served if regions will be available for reads during recovery for doing eventually consistent reads. this will help with satisfying low latency guarantees for some class of applications which can work with stale reads. for improving read availability, we propose a replicated read-only region serving design, also referred as secondary regions, or region shadows. extending current model of a region being opened for reads and writes in a single region server, the region will be also opened for reading in region servers. the region server which hosts the region for reads and writes (as in current case) will be declared as primary, while 0 or more region servers might be hosting the region as secondary. there may be more than one secondary (replica count > 2). will attach a design doc shortly which contains most of the details and some thoughts about development approaches. reviews are more than welcome. we also have a proof of concept patch, which includes the master and regions server side of changes. client side changes will be coming soon as well. ",
        "label": 155
    },
    {
        "text": "addition of renaming operation  \u03c1 s(b1, b2, ..., bn) (r) - renames the table r to s and columnfamilies ai to bi  \u03c1 s (r) - renames the table r to s  \u03c1 (b1, b2, ..., bn) (r) - renames columnfamilies ai to bi ",
        "label": 152
    },
    {
        "text": "inmemory compaction optimizations  segment structure  memory optimizations including compressed format representation and offheap allocations ",
        "label": 35
    },
    {
        "text": "remove  ignore for testlogrollaftersplitstart  we fixed a data loss bug in hbase-2312 by adding non-recursive creates to hdfs. although a number of hdfs versions have this fix, the official hdfs 0.20.205 branch currently doesn't, so we needed to mark the test as ignored. please revisit before the rc of 0.94, which should have 0.20.205.1 or later & the necessary hdfs patches. ",
        "label": 402
    },
    {
        "text": "update the mapred package document examples so they work with trunk   the examples in package doc. are old making mention of the long deprecated text, etc. update them. ",
        "label": 314
    },
    {
        "text": "fix infer issues in hbase common  ",
        "label": 154
    },
    {
        "text": "hbase daemon sh fails to execute with 'sh' command  hostname:hbase_home/bin # sh hbase-daemon.sh restart master  hbase-daemon.sh: line 188: hbase-daemon.sh: command not found  hbase-daemon.sh: line 196: hbase-daemon.sh: command not found ",
        "label": 53
    },
    {
        "text": "hbck should disable the balancer using synchronousbalanceswitch   hbck disable the balancer using admin.balanceswith(bool) when it would be preferable to use the newer synchronusbalanceswitch method found in 0.94 and trunk branches. ",
        "label": 186
    },
    {
        "text": "add ability for get operations to enable disable use of block caching  the public get api does not currently support enabling/disabling the block cache like scans do. no reason they shouldn't and now that they get converted into scans this should be fairly trivial. ",
        "label": 247
    },
    {
        "text": "create htable pooler  a client class that takes care of properly pooling htable references for use in multi-threaded, low-latency java clients. ",
        "label": 38
    },
    {
        "text": "inconsistency between the  regions  map and the  servers  map in assignmentmanager  there are occurrences in am where this.servers is not kept consistent with this.regions. this might cause balancer to offline a region from the rs that already returned notservingregionexception at a previous offline attempt. in assignmentmanager.unassign(hregioninfo, boolean)  try {  // todo: we should consider making this look more like it does for the  // region open where we catch all throwables and never abort  if (servermanager.sendregionclose(server, state.getregion(),  versionofclosingnode)) { log.debug(\"sent close to \" + server + \" for region \" + region.getregionnameasstring()); return; } // this never happens. currently regionserver close always return true.  log.warn(\"server \" + server + \" region close rpc returned false for \" +  region.getregionnameasstring());  } catch (notservingregionexception nsre) { log.info(\"server \" + server + \" returned \" + nsre + \" for \" + region.getregionnameasstring()); // presume that master has stale data. presume remote side just split. // presume that the split message when it comes in will fix up the master's // in memory cluster state. } catch (throwable t) {  if (t instanceof remoteexception) {  t = ((remoteexception)t).unwrapremoteexception();  if (t instanceof notservingregionexception) {  if (checkifregionbelongstodisabling(region)) {  // remove from the regionsintransition map  log.info(\"while trying to recover the table \"  + region.gettablenameasstring()  + \" to disabled state the region \" + region  + \" was offlined but the table was in disabling state\");  synchronized (this.regionsintransition) { this.regionsintransition.remove(region.getencodedname()); } // remove from the regionsmap  synchronized (this.regions) { this.regions.remove(region); } deleteclosingorclosednode(region);  }  }  // rs is already processing this region, only need to update the timestamp  if (t instanceof regionalreadyintransitionexception) { log.debug(\"update \" + state + \" the timestamp.\"); state.update(state.getstate()); } } in assignmentmanager.assign(hregioninfo, regionstate, boolean, boolean, boolean)  synchronized (this.regions) { this.regions.put(plan.getregioninfo(), plan.getdestination()); } ",
        "label": 482
    },
    {
        "text": "binary values are formatted wrong in shell  binary values in the shell don't seem to be formatted correctly. for example: hbase(main):007:0> put 't1', 'r1', 'f1:q1', \"\\x91\", 1000 0 row(s) in 0.0160 seconds hbase(main):008:0> scan 't1' row                          column+cell  r1                          column=f1:q1, timestamp=1260417826655, value=\\357\\277\\275 1 row(s) in 0.1090 seconds in this case we insert a single byte (double quotes needed for it to interpret the hex value correctly), but when formatted, it appears as 3 bytes in octal. the same thing happens when the data is inserted via the java api. for example, this code: htabledescriptor tabledesc = new htabledescriptor(\"t2\"); tabledesc.addfamily(new hcolumndescriptor(\"f1\")); hbaseadmin admin = new hbaseadmin(new hbaseconfiguration()); admin.createtable(tabledesc); htable table = new htable(\"t2\"); put put = new put(bytes.tobytes(\"r1\")); put.add(bytes.tobytes(\"f1\"), bytes.tobytes(\"q1\"), new byte[] {(byte) 0x91}); table.put(put); result result = table.get(new get(bytes.tobytes(\"r1\"))); system.out.println(bytes.tostringbinary(result.raw()[0].getvalue())); prints out \\x91  and then accessing via shell gives: hbase(main):009:0> scan 't2' row                          column+cell  r1                          column=f1:q1, timestamp=1260418531959, value=\\357\\277\\275 1 row(s) in 0.1100 seconds ",
        "label": 314
    },
    {
        "text": "up zk maxclientcnxns from default of to or or so  in irc: 17:47 < riz0d> 2009-06-29 14:28:19,368 warn org.apache.zookeeper.server.nioservercnxn: too many connections from /127.0.0.1 - max is 10 ... in pseudo-distributed mode. ",
        "label": 314
    },
    {
        "text": "hadoop compilation broken due to tests introduced in hbase  when attempting to compile hbase 0.94rc1 against hadoop 23, i got this set of compilation error messages: jon@swoop:~/proj/hbase-0.94$ mvn clean test -dhadoop.profile=23 -dskiptests ... [info] ------------------------------------------------------------------------ [info] build failure [info] ------------------------------------------------------------------------ [info] total time: 18.926s [info] finished at: mon apr 23 10:38:47 pdt 2012 [info] final memory: 55m/555m [info] ------------------------------------------------------------------------ [error] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testcompile (default-testcompile) on project hbase: compilation failure: compilation failure: [error] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/testhlogrecordreader.java:[147,46] org.apache.hadoop.mapreduce.jobcontext is abstract; cannot be instantiated [error]  [error] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/testhlogrecordreader.java:[153,29] org.apache.hadoop.mapreduce.jobcontext is abstract; cannot be instantiated [error]  [error] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/testhlogrecordreader.java:[194,46] org.apache.hadoop.mapreduce.jobcontext is abstract; cannot be instantiated [error]  [error] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/testhlogrecordreader.java:[206,29] org.apache.hadoop.mapreduce.jobcontext is abstract; cannot be instantiated [error]  [error] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/testhlogrecordreader.java:[213,29] org.apache.hadoop.mapreduce.jobcontext is abstract; cannot be instantiated [error]  [error] /home/jon/proj/hbase-0.94/src/test/java/org/apache/hadoop/hbase/mapreduce/testhlogrecordreader.java:[226,29] org.apache.hadoop.mapreduce.taskattemptcontext is abstract; cannot be instantiated [error] -> [help 1] upon further investigation this issue is due to code introduced in hbase-5064 and is also present in trunk. ",
        "label": 248
    },
    {
        "text": "improve documentation around keep deleted cells  time range scans  and delete markers  without keep_deleted_cells all timerange queries are broken if their range covers a delete marker.  as some internal discussions with colleagues showed, this feature is not well understand and documented. ",
        "label": 330
    },
    {
        "text": "snapshotmanager restoresnapshot not update table and region count quota correctly when encountering exception  in snapshotmanager#restoresnapshot, the table and region quota will be checked and updated as:       try {         // table already exist. check and update the region quota for this table namespace         checkandupdatenamespaceregionquota(manifest, tablename);         restoresnapshot(snapshot, snapshottabledesc);       } catch (ioexception e) {         this.master.getmasterquotamanager().removetablefromnamespacequota(tablename);         log.error(\"exception occurred while restoring the snapshot \" + snapshot.getname()             + \" as table \" + tablename.getnameasstring(), e);         throw e;       } the 'checkandupdatenamespaceregionquota' will fail if regions in the snapshot make the region count quota exceeded, then, the table will be removed in the 'catch' block. this will make the current table count and region count decrease, following table creation or region split will succeed even if the actual quota is exceeded. ",
        "label": 238
    },
    {
        "text": "add cm action for online compression algorithm change  we need to add a cm action for online compression algorithm change and make sure itbll is ok with it. ",
        "label": 242
    },
    {
        "text": "expose progress of a major compaction in ui and or in shell  a general recommendation is to turn off major compactions and run them externally only currently the only way to follow progress of a major compaction is by study of regionserver logs. add a feature that gives a percentage complete of major compaction up in the ui or in shell. ",
        "label": 77
    },
    {
        "text": "improve hbase   avoid copying bytes for regexfilter unless necessary  parent patch copies input for regexfilter unconditionally.  we should only do this if the kv portion into the passed byte[] is < 1/2 of the passed byte[]. otherwise we waste cycles.  patch is trivial and will be coming momentarily. ",
        "label": 286
    },
    {
        "text": "add back metascanner alltableregions configuration conf byte  tablename boolean offlined  method  the metascanner.alltableregions(configuration conf,byte[] tablename,boolean offlined) method was removed in 0.94.11. please add it back for backward compatibility. ",
        "label": 286
    },
    {
        "text": "remove multiple acls for the same user in kerberos  ",
        "label": 415
    },
    {
        "text": "reenable testshellrsgroups  it was disabled by the parent issue because rsgroups was failing. rsgroups now works but this test is still failling. need to dig in (signal from these jruby tests is murky). ",
        "label": 188
    },
    {
        "text": "hbase shell doesn't trap ctrl c signal  from withing the hbase shell, when there's a io problem, the hbase client code tries to recover automatically but sometimes we know what's going on and all we want is to cancel the operation by pressing ctrl-c but the shell doesn't catch it and we need to either wait for the operation to timeout or close the terminal and open another one. ",
        "label": 229
    },
    {
        "text": "branch needs a jenkins commit build job  the current hbase-1.4 job is actually branch-1.  https://builds.apache.org/job/hbase-1.4/ need a separate job for branch-1.4. and rename the current job to hbase-1.5. ",
        "label": 441
    },
    {
        "text": "am zk workers stuck doing  cpu on hashmap put  i just noticed on my test cluster that my master is using all my cpus even though it's completely idle. 5 threads are doing this: \"am.zk.worker-pool2-t34\" daemon prio=10 tid=0x00007f68ac176800 nid=0x5251 runnable [0x00007f688cc83000]    java.lang.thread.state: runnable at java.util.hashmap.put(hashmap.java:374) at org.apache.hadoop.hbase.master.assignmentmanager.handleregion(assignmentmanager.java:954) at org.apache.hadoop.hbase.master.assignmentmanager$6.run(assignmentmanager.java:1419) at org.apache.hadoop.hbase.master.assignmentmanager$3.run(assignmentmanager.java:1247) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) looking at the code, i see hbase-9095 introduced two hashmaps for tests only but they end up being used concurrently in the am and are never cleaned up. it seems to me that any master running since that patch was committed has a time bomb in it. i'm marking this as a blocker. devaraj kavali and jimmy xiang, you guys wanna take a look at this? ",
        "label": 139
    },
    {
        "text": "define replication interface  hbase has replication. fellas have been hijacking the replication apis to do all kinds of perverse stuff like indexing hbase content (hbase-indexer https://github.com/ngdata/hbase-indexer) and our francis christopher liu just showed up w/ overrides that replicate via an alternate channel (over a secure thrift channel between dcs over on hbase-9360). this issue is about surfacing these apis as public with guarantees to downstreamers similar to those we have on our public client-facing apis (and so we don't break them for downstreamers). any input patrick d. hunt or gabriel reid or francis christopher liu? thanks. ",
        "label": 155
    },
    {
        "text": "testcatalogtracker hangs occasionally  ",
        "label": 286
    },
    {
        "text": "getmaximumallowedtimebetweenruns in scheduledchore ignores the timeunit  i was running integrationtestingestwithmob test.  i lower the mob compaction chore interval to this value:     <property>       <name>hbase.mob.compaction.chore.period</name>       <value>6000</value>     </property> after whole night, there was no indication from master log that mob compaction ran.  all i found was: 2016-03-09 04:18:52,194 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_1] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 05:58:52,516 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_1] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 07:38:52,847 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_2] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 09:18:52,848 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_1] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 10:58:52,932 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_2] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 12:38:52,932 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_1] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 14:18:52,933 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_2] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 15:58:52,957 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_1] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time 2016-03-09 17:38:52,960 info  [tyu-hbase-rhel-re-2.novalocal,20000,1457491115327_choreservice_2] hbase.scheduledchore: chore: tyu-hbase-rhel-re-2.novalocal,20000,1457491115327-  mobcompactionchore missed its start time ",
        "label": 243
    },
    {
        "text": "allow scanner setcaching to specify size instead of number of rows  currently, we have the following api's to customize the behavior of scans:  setcaching() - how many rows to cache on client to speed up scans  setbatch() - max columns per row to return per row to prevent a very large response. ideally, we should be able to specify a memory buffer size because:  1. that would take care of both of these use cases.  2. it does not need any knowledge of the size of the rows or cells, as the final thing we are worried about is the available memory. ",
        "label": 93
    },
    {
        "text": "handle potential data loss due to concurrent processing of processfaileover and servershutdownhandler  this jira continues the effort from hbase-5179. starting with stack's comments about patches for 0.92 and trunk: reviewing 0.92v17 isdeadserverinprogress is a new public method in servermanager but it does not seem to be used anywhere. does isdeadrootserverinprogress need to be public? ditto for meta version. this method param names are not right 'definitiverootserver'; what is meant by definitive? do they need this qualifier? is there anything in place to stop us expiring a server twice if its carrying root and meta? what is difference between asking assignment manager iscarryingroot and this variable that is passed in? should be doc'd at least. ditto for meta. i think i've asked for this a few times - onlineservers needs to be explained... either in javadoc or in comment. this is the param passed into joincluster. how does it arise? i think i know but am unsure. god love the poor noob that comes awandering this code trying to make sense of it all. it looks like we get the list by trawling zk for regionserver znodes that have not checked in. don't we do this operation earlier in master setup? are we doing it again here? though distributed split log is configured, we will do in master single process splitting under some conditions with this patch. its not explained in code why we would do this. why do we think master log splitting 'high priority' when it could very well be slower. should we only go this route if distributed splitting is not going on. do we know if concurrent distributed log splitting and master splitting works? why would we have dead servers in progress here in master startup? because a servershutdownhandler fired? this patch is different to the patch for 0.90. should go into trunk first with tests, then 0.92. should it be in this issue? this issue is really hard to follow now. maybe this issue is for 0.90.x and new issue for more work on this trunk patch? this patch needs to have the v18 differences applied. ",
        "label": 107
    },
    {
        "text": "hrs closeregion should be able to close regions with only the encoded name  we had some sort of an outage this morning due to a few racks losing power, and some regions were left in the following state: error: region unknown_region on sv4r17s9:60020, key=e32bbe1f48c9b3633c557dc0291b90a3, not on hdfs or in meta but deployed on sv4r17s9:60020 that region was deleted by the master but the region server never got the memo. right now there's no way to force close it because hrs.closeregion requires an hri and the only way to create one is to get it from .meta. which in our case doesn't contain a row for that region. basically we have to wait until that server is dead to get rid of the region and make hbck happy. the required change is to have closeregion accept an encoded name in both hba (when the rs address is provided) and hrs since it's able to find it anyways from it's list of live regions. ",
        "label": 544
    },
    {
        "text": "port hbase 'replication can overrun  meta  scans on cluster re start' to  ",
        "label": 441
    },
    {
        "text": "bad compaction priority behavior in queue can cause store to be blocked  note that this can be solved by bumping up the number of compaction threads but still it seems like this priority \"inversion\" should be dealt with.  there's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). this compaction now has higher priority than the first one. after that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that i've seen). i wonder why we do thing thing where we queue compaction and compact separately. perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. this will need starvation safeguard too but should probably be better. btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqnums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). today i see the case that would also apply to old policy, but yesterday i saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted. ",
        "label": 406
    },
    {
        "text": "update hadoop in    we ship with branch-0.20-append a few versions back from the tip. if 205 comes out and hbase works on it, we should ship 0.92 with it (while also ensuring it work w/ 0.22 and 0.23 branches). ",
        "label": 314
    },
    {
        "text": "find a way to handle the corrupt recovered hfiles  copy the comment from pr review.   if the file is a corrupt hfile, an exception will be thrown here, which will cause the region to fail to open.  maybe we can add a new parameter to control whether to skip the exception, similar to recover edits which has a parameter \"hbase.hregion.edits.replay.skip.errors\";   regions that can't be opened because of detached references or corrupt hfiles are a fact-of-life. we need work on this issue. this will be a new variant on the problem \u2013 i.e. bad recovered hfiles. on adding a config to ignore bad files and just open, thats a bit dangerous as per @infraio .... as it could mean silent data loss. ",
        "label": 346
    },
    {
        "text": "getclientport method of minizookeepercluster does not always return the correct value  //starting 5 zk servers minizookeepercluster cluster = hbt.startminizkcluster(5); int defaultclientport = 21818; cluster.setdefaultclientport(defaultclientport); cluster.killcurrentactivezookeeperserver(); cluster.getclientport(); //still returns the port of the zk server that was killed in the previous step ",
        "label": 464
    },
    {
        "text": "unable to set modify ttl on a column family using the shell   when attempting to set the ttl parameter on a column family using the hbase shell, the following error is reported and the parameter is not modified: hbase(main):042:0> create 't1', {name => 'f1', versions => 1, ttl => 2592000, blockcache => true} error: uninitialized constant hbase::admin::hcolumndescriptor ",
        "label": 314
    },
    {
        "text": "testimporttsv failed with hadoop  java.io.filenotfoundexception: file does not exist: /home/henkins/.m2/repository/org/apache/hadoop/hadoop-mapred/0.22-snapshot/hadoop-mapred-0.22-snapshot.jar  at org.apache.hadoop.hdfs.distributedfilesystem.getfilestatus(distributedfilesystem.java:742)  at org.apache.hadoop.mapreduce.filecache.trackerdistributedcachemanager.getfilestatus(trackerdistributedcachemanager.java:331)  at org.apache.hadoop.mapreduce.filecache.trackerdistributedcachemanager.determinetimestamps(trackerdistributedcachemanager.java:711)  at org.apache.hadoop.mapreduce.jobsubmitter.copyandconfigurefiles(jobsubmitter.java:245)  at org.apache.hadoop.mapreduce.jobsubmitter.copyandconfigurefiles(jobsubmitter.java:283)  at org.apache.hadoop.mapreduce.jobsubmitter.submitjobinternal(jobsubmitter.java:350)  at org.apache.hadoop.mapreduce.job$2.run(job.java:1045)  at org.apache.hadoop.mapreduce.job$2.run(job.java:1042)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1153)  at org.apache.hadoop.mapreduce.job.submit(job.java:1042)  at org.apache.hadoop.mapreduce.job.waitforcompletion(job.java:1062)  at org.apache.hadoop.hbase.mapreduce.testimporttsv.domrontabletest(testimporttsv.java:215)  at org.apache.hadoop.hbase.mapreduce.testimporttsv.testmrontable(testimporttsv.java:165)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method) ",
        "label": 326
    },
    {
        "text": "essential column family optimization is broken  when using the filter singlecolumnvaluefilter, and depending of the columns specified in the scan (filtering column always specified), the results can be different. here is an example.  suppose the following table: key a:foo a:bar b:foo b:bar 1 false flag flag flag 2 true flag flag flag 3   flag flag flag with this filter: singlecolumnvaluefilter filter = new singlecolumnvaluefilter(bytes.tobytes(\"a\"), bytes.tobytes(\"foo\"), compareop.equal, new binarycomparator(bytes.tobytes(\"false\"))); filter.setfilterifmissing(true); depending of how i specify the list of columns to add in the scan, the result is different. yet, all examples below should always return only the first row (key '1'): ok: scan.addfamily(bytes.tobytes(\"a\")); ko (2 results returned, row '3' without 'a:foo' qualifier is returned): scan.addfamily(bytes.tobytes(\"a\")); scan.addfamily(bytes.tobytes(\"b\")); ko (2 results returned, row '3' without 'a:foo' qualifier is returned): scan.addcolumn(bytes.tobytes(\"a\"), bytes.tobytes(\"foo\")); scan.addcolumn(bytes.tobytes(\"a\"), bytes.tobytes(\"bar\")); scan.addcolumn(bytes.tobytes(\"b\"), bytes.tobytes(\"foo\")); ok: scan.addcolumn(bytes.tobytes(\"a\"), bytes.tobytes(\"foo\")); scan.addcolumn(bytes.tobytes(\"b\"), bytes.tobytes(\"bar\")); ok: scan.addcolumn(bytes.tobytes(\"a\"), bytes.tobytes(\"foo\")); scan.addcolumn(bytes.tobytes(\"a\"), bytes.tobytes(\"bar\")); this is a regression as it was working properly on hbase 0.92.  you will find in attachement the unit tests reproducing the issue. the analysis of this issue lead us to 2 critical bugs induced in 96 and above versions  1. the essential family optimization is broken in some cases. in case of condition on some families, we 1st will read those kvs and apply condition on those, when the condition says to filter out that row, we will not go ahead and fetch data from remaining non essential cfs. but now in most of the cases we will do this unwanted data read which is fully against this optimization  2. we have a cp hook postfilterrow() which will be called when a row is getting filtered out by the filter. this gives the cp to do a reseek to the next known row which it thinks can evaluate the condition to true. but currently in 96+ code , this hook is not getting called. ",
        "label": 441
    },
    {
        "text": "print warn message if hbase replication sizeoflogqueue is too big  a metric hbase.replication.sizeoflogqueue may become big when replication is delaying.  it would be useful if hbase prints warn log which tells hbase.replication.sizeoflogqueue is too big. ",
        "label": 315
    },
    {
        "text": "testzkinterprocessreadwritelock fails occasionally in qa test run  testreadlockexcludeswriters(org.apache.hadoop.hbase.zookeeper.lock.testzkinterprocessreadwritelock)  time elapsed: 0.003 sec  <<< error! java.lang.exception: test timed out after 3000 milliseconds at sun.misc.unsafe.park(native method) you can find the test output here: https://builds.apache.org/job/precommit-hbase-build/4634/artifact/trunk/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.zookeeper.lock.testzkinterprocessreadwritelock-output.txt ",
        "label": 155
    },
    {
        "text": "usage for rowcounter in refguide is out of sync with code  src/main/asciidoc/_chapters/troubleshooting.adoc: hadoop_classpath=`hbase classpath` hadoop jar $hbase_home/hbase-server-version.jar rowcounter usertable the class is no longer in hbase-server jar. it is in hbase-mapreduce jar. ",
        "label": 435
    },
    {
        "text": "change in filterallremaining  impl for filterlistwithor  filterallremaining() in filterlistwithor will now return false if the filter list is empty whereas earlier it used to return true for operator.must_pass_one.  in hbase-2.0  @override   public boolean filterallremaining() throws ioexception {     if (isempty()) {       return super.filterallremaining();//false     }     for (int i = 0, n = filters.size(); i < n; i++) {       if (!filters.get(i).filterallremaining()) {         return false;       }     }     return true;   } earlier versions(1.3.1 atleast):- @override   public boolean filterallremaining() throws ioexception {     int listize = filters.size();     for (int i = 0; i < listize; i++) {       if (filters.get(i).filterallremaining()) {         if (operator == operator.must_pass_all) {           return true;         }       } else {         if (operator == operator.must_pass_one) {           return false;         }       }     }     return operator == operator.must_pass_one;   } imo, the current implementation seems to be right, but probably this change requires a release note at least as some people might have implemented the filters considering the old semantics. ",
        "label": 514
    },
    {
        "text": "fix all javadoc warnings in all modules but hbase server  ",
        "label": 314
    },
    {
        "text": "example command in the  getting started  documentation doesn't work  the \"put\" command listed in the example in the \"running and confirming your installation\" section doesn't work. ",
        "label": 70
    },
    {
        "text": "upgrade hadoop to  ",
        "label": 314
    },
    {
        "text": "enhance the apache hbase reference guide  i am reading the guide here: http://hbase.apache.org/book.html#regions.arch i have noticed the following: there is lack of coverage on scan. note that lars has a very good presentation from last year's hbasecon: http://www.slideshare.net/cloudera/3-learning-h-base-internals-lars-hofhansl-salesforce-final under section 9.7.6.1, there is no mentioning of flush per region if a memstore reached the size threshold (and a possible flush of all memstores if over all memory usage of all the memstores reached memstore.upperlimit). note that people need to combine the information provided in section 9 with section 2.3.1.1 to fully understand the situation. compaction tuning strategies. both nicolas from facebook and sergey from hortonworks have presented various compaction algorithms. sometime, once all the necessary code is in master, we need to have the coverage. let me know how i can help on the matter. ",
        "label": 330
    },
    {
        "text": "increment class  these increment objects are used by the table implementation to perform increment operation. ",
        "label": 441
    },
    {
        "text": "add to admin create table start and end key params and desired number of regions  this would be an adornment on create table that pre-creates n regions in the new table. it came up yesterday at the hbase hackathon3. ",
        "label": 247
    },
    {
        "text": "hlog numbers should wrap around when they reach  question about log numbers -> closing current log writer hdfs://10.0.0.1:9000/gfs_storage/hadoop-root/hbase/log_10.0.0.3_1201900440436_60020/hlog.dat.024  what happens when the log get to hlog.dat.999 ",
        "label": 86
    },
    {
        "text": "in master  there are a load of places where no sleep between retries  here is an example:  270308 2008-03-12 14:10:02,054 debug org.apache.hadoop.hbase.hmaster: numberofmetaregions: 1, onlinemetaregions.size(): 1                                                                                                                                              270309 2008-03-12 14:10:02,054 debug org.apache.hadoop.hbase.hmaster: process server shutdown scanning .meta.,,1 on xx.xx.xx.184:60020 hmaster                                                                                                                         270310 2008-03-12 14:10:02,056 debug org.apache.hadoop.hbase.hmaster: process server shutdown scanning .meta.,,1 on xx.xx.xx.184:60020 hmaster                                                                                                                         270311 2008-03-12 14:10:02,057 debug org.apache.hadoop.hbase.hmaster: process server shutdown scanning .meta.,,1 on xx.xx.xx.184:60020 hmaster                                                                                                                         270312 2008-03-12 14:10:02,059 debug org.apache.hadoop.hbase.hmaster: process server shutdown scanning .meta.,,1 on xx.xx.xx.184:60020 hmaster 270313 2008-03-12 14:10:02,060 debug org.apache.hadoop.hbase.hmaster: process server shutdown scanning .meta.,,1 on xx.xx.xx.184:60020 hmaster 270314 2008-03-12 14:10:02,062 warn org.apache.hadoop.hbase.hmaster: processing pending operations: processservershutdown of xx.xx.xx.180:60020                                                                                                                        270315 org.apache.hadoop.hbase.notservingregionexception: org.apache.hadoop.hbase.notservingregionexception .meta.,,1                                                                                                                                                  270316         at org.apache.hadoop.hbase.hregionserver.getregion(hregionserver.java:1606)  ... whats actually going on here is 5 retries without a wait in between (logging should include index numbering retry. seems to be a bunch of duplicated code around retrying that we might be able to fix with a callable. jim firby today suggested we do expotential backoffs in our retries. ",
        "label": 241
    },
    {
        "text": "a new binary component comparator binarycomponentcomparator  to perform comparison of arbitrary length and position  lets say you have composite key: a+b+c+d. and for simplicity assume that a,b,c, and d all are 4 byte integers. now, if you want to execute a query which is semantically same to following sql: \"select * from table where a=1 and b > 10 and b < 20 and c > 90 and c < 100 and d=1\" the only choice you have is to do client side filtering. that could be lots of unwanted data going through various software components and network. solution: we can create a \"component\" comparator which takes the value of the \"component\" and its relative position in the key to pass the 'filter' subsystem of the server:     filterlist filterlist = new filterlist(filterlist.operator.must_pass_all);     int boffset = 4;     byte[] b10 = bytes.tobytes(10);      filter b10filter = new rowfilter(comparefilter.compareop.greater,             new binarycomponentcomparator(b10,boffset));     filterlist.addfilter(b10filter);     byte[] b20  = bytes.tobytes(20);     filter b20filter = new rowfilter(comparefilter.compareop.less,             new binarycomponentcomparator(b20,boffset));     filterlist.addfilter(b20filter);     int coffset = 8;     byte[] c90  = bytes.tobytes(90);     filter c90filter = new rowfilter(comparefilter.compareop.greater,             new binarycomponentcomparator(c90,coffset));     filterlist.addfilter(c90filter);     byte[] c100  = bytes.tobytes(100);     filter c100filter = new rowfilter(comparefilter.compareop.less,             new binarycomponentcomparator(c100,coffset));     filterlist.addfilter(c100filter);     in doffset = 12;     byte[] d1   = bytes.tobytes(1);     filter dfilter  = new rowfilter(comparefilter.compareop.equal,             new binarycomponentcomparator(d1,doffset));     filterlist.addfilter(dfilter);     //build start and end key for scan     int aoffset = 0;     byte[] startkey = new byte[16]; //key size with four ints     bytes.putint(startkey,aoffset,1); //a=1     bytes.putint(startkey,boffset,11); //b=11, takes care of b > 10     bytes.putint(startkey,coffset,91); //c=91,      bytes.putint(startkey,doffset,1); //d=1,      byte[] endkey = new byte[16];     bytes.putint(endkey,aoffset,1); //a=1     bytes.putint(endkey,boffset,20); //b=20, takes care of b < 20     bytes.putint(endkey,coffset,100); //c=100,      bytes.putint(endkey,doffset,1); //d=1,      //setup scan     scan scan = new scan(startkey,endkey);     scan.setfilter(filterlist);     //the scanner below now should give only desired rows.     //no client side filtering is required.      resultscanner scanner = table.getscanner(scan); the comparator can be used with any filter which makes use of bytearraycomparable. most notably it can be used with valuefilter to filter out kv based on partial comparison of 'values' :     byte[] partialvalue = bytes.tobytes(\"partial_value\");     int partialvalueoffset =      filter partialvaluefilter = new valuefilter(comparefilter.compareop.greater,             new binarycomponentcomparator(partialvalue,partialvalueoffset)); which in turn can be combined with rowfilter to create a poweful predicate:     rowfilter rowfilter = new rowfilter(greater, new binarycomponentcomparator(bytes.tobytes(\"a\"),1);     filterliost fl = new filterlist (must_pass_all,rowfilter,partialvaluefilter); ",
        "label": 457
    },
    {
        "text": "port hbase 'add bigdecimalcolumninterpreter for doing aggregations using aggregationclient' to trunk  columninterpreter implementation in trunk is different from that in 0.94 this issue ports bigdecimalcolumninterpreter to trunk ",
        "label": 254
    },
    {
        "text": "replace hql w  a hbase friendly jirb or jython shell  the hbase shell is a useful admin and debugging tool but it has a couple of downsides. to extend, a fragile parser definition needs tinkering-with and new java classes must be added. the current test suite for hql is lacking coverage and the current code could do with a rewrite having evolved piecemeal. another downside is that the presence of an hql interpreter gives the mis-impression that hbase is like a sql database. this 'wish' issue suggests that we jettison hql and instead offer users a jirb or jython command line. we'd ship with some scripts and jruby/jython classes that we'd source on startup to do things like import base client classes \u2013 so folks wouldn't have to remember all the packages stuff sat in \u2013 and added a pretty-print for scanners and getters outputting text, xhtml or binary. they would also make it easy to do hql-things in jruby/python script. advantages: already-written parser with no need of extension probing deeper into hbase: i.e. better for debugging than hql could ever be. easy extension adding scripts/modules rather than java code. less likely hbase could be confused for a sql db. downsides: probably more verbose. requires ruby or python knowledge (\"everyone knows some sql\"). big? (jruby lib is 24m). i was going to write security as downside but hql suffers this at the moment too \u2013 though it has been possible to sort the updates from the selects in the ui to prevent modification of the db from the ui, something that would be hard to do in a jruby/jython parser. what do others think? ",
        "label": 314
    },
    {
        "text": "testdrainingserver  testdrainingserverwithabort fails  see here https://builds.apache.org/job/hbase-0.95-on-hadoop2/179/testreport/org.apache.hadoop.hbase/testdrainingserver/testdrainingserverwithabort/ and here http://54.241.6.143/job/hbase-0.95-hadoop-2/org.apache.hbase$hbase-server/600/testreport/org.apache.hadoop.hbase/testdrainingserver/testdrainingserverwithabort/ it looks like .meta. is not being assigned. am digging in here now. ",
        "label": 314
    },
    {
        "text": "testassignmentmanager testdisablingtableregionsassignmentduringcleanclusterstartup fails due to port already in use   ",
        "label": 543
    },
    {
        "text": " hbck2 hbase operator tools  make first release   make our first release of hbck2/hbase-operator-tools. first release should have the coverage hbck1 had at least. when the parent for this issue is done, we'll be at hbck1+. let us release then (week or two?). a release will help operators who have been struggling having to build hbck2 against different hbase versions. the release should be a \"fat jar\"/completely contained with all dependency satisfied so operator can just fire up hbck2 w/o having to build against an hbase or provide some magic mix of jars to satisfy hbck2 tool needs. ",
        "label": 352
    },
    {
        "text": "bulkload should update the store storesize  after bulkloading some hfiles into the table, we found the force-split didn't work because of the midkey == null. only if we re-booted the hbase service, the force-split can work normally. ",
        "label": 239
    },
    {
        "text": "change logging for coprocessor exec call to trace  currently both regionserver and hmaster log (debug) this  \"received dynamic protocol exec call with protocolname <class>\"  on each coprocessor exec call. we just filled our regionserver log with 160gb of these since we're making heavy use of coprocessors. i would like to change this to trace. any objections? ",
        "label": 286
    },
    {
        "text": "regioncoprocessorhost system nanotime  performance bottleneck  the tracking of execution time of coprocessor methods introduced in hbase-11516 introduces 2 calls to system.nanotime() per coprocessor method per coprocessor. this is resulting in a serious performance bottleneck in certain scenarios. for example consider the scenario where many rows are being ingested (put) in a table which has multiple coprocessors (we have up to 20 coprocessors). this results in 8 extra calls to system.nanotime() per coprocessor (preput, postput, poststartregionoperation and postcloseregionoperation) which has in total (i.e. times 20) been seen to result in a 50% increase of execution time. i think it is generally considered bad practice to measure execution times on such a small scale (per single operation). also note that measurements are taken even for coprocessors that do not even have an actual implementation for certain operations, making the problem worse. ",
        "label": 38
    },
    {
        "text": "indefinite open close wait on busy regionservers  we observed a case where, when a specific rs got bombarded by a large amount of regular requests, spiking and filling up its rpc queue, the balancer's invoked unassigns and assigns for regions that dealt with this server entered into an indefinite retry loop. the regions specifically began waiting in pending_close/pending_open states indefinitely cause of the hbase client rpc from the servermanager at the master was running into sockettimeouts. this caused a region unavailability in the server for the affected regions. the timeout monitor retry default of 30m in 0.94's am compounded the waiting gap further a bit more (this is now 10m in 0.95+'s new am, and has further retries before we get there, which is good). wonder if there's a way to improve this situation generally. pending_opens may be easy to handle - we can switch them out and move them elsewhere. pending_closes may be a bit more tricky, but there must perhaps at least be a way to \"give up\" permanently on a movement plan, and letting things be for a while hoping for the rs to recover itself on its own (such that clients also have a chance of getting things to work in the meantime)? ",
        "label": 163
    },
    {
        "text": " findbugs  fix perf warnings  see https://builds.apache.org/job/precommit-hbase-build/1313//artifact/trunk/patchprocess/newpatchfindbugswarnings.html#warnings_performance ",
        "label": 458
    },
    {
        "text": "fix javadoc warnings in snapshot classes  ",
        "label": 441
    },
    {
        "text": "hbck2 setregionstate command  among some of the current amv2 issues, we faced situation where some regions had state as opening in meta, with an rs startcode that was not valid anymore. there was no ap running, the region stays permanently being logged as in-transition on master logs, yet no procedure is really trying to bring it online. current hbck2 unassigns/assigns commands didn't work either, as per the exception shown, it expects regions to be in state splitting, split, merging, open, or closing: warn org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure: failed transition, suspend 1secs pid=7093, state=runnable:region_transition_dispatch, locked=true; unassignprocedure table=rc_accounts, region=db85127b77fa56f7ad44e2c988e53925, server=server1.example.com,16020,1552682193324; rit=opening, location=server1.example.com,16020,1552682193324; waiting on rectified condition fixed by other procedure or operator intervention org.apache.hadoop.hbase.exceptions.unexpectedstateexception: expected [splitting, split, merging, open, closing] so could move to closing but current state=opening at org.apache.hadoop.hbase.master.assignment.regionstates$regionstatenode.transitionstate(regionstates.java:166) at org.apache.hadoop.hbase.master.assignment.assignmentmanager.markregionasclosing(assignmentmanager.java:1479) at org.apache.hadoop.hbase.master.assignment.unassignprocedure.updatetransition(unassignprocedure.java:212) at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:369) at org.apache.hadoop.hbase.master.assignment.regiontransitionprocedure.execute(regiontransitionprocedure.java:97) at org.apache.hadoop.hbase.procedure2.procedure.doexecute(procedure.java:957) at org.apache.hadoop.hbase.procedure2.procedureexecutor.execprocedure(procedureexecutor.java:1835) at org.apache.hadoop.hbase.procedure2.procedureexecutor.executeprocedure(procedureexecutor.java:1595) in this specific case, since we know the region is not actually being operated by any proc and is not really open anywhere, it's ok to manually set it's state to one of those assigns/unassigns can operate on, so this jira proposes a new hbck2 command that allows for arbitrarily set a region to a given state. ",
        "label": 486
    },
    {
        "text": "make hbase export metrics to jmx by default  instead of to nullcontext  i was debugging something in the swift branch, and found that hbase doesn't export to jmx by default. the jmx server is being spun-up anyways in single node setup, we might as well export the metrics to it. ",
        "label": 154
    },
    {
        "text": "deadlock between hregion icv and hregion close  hregion.icv gets a row lock then gets a newscanner lock. hregion.close gets a newscanner lock, slitcloselock and finally waits for all row locks to finish. if the icv got the row lock and then close got the newscannerlock, both end up waiting on the other. this was introduced when get became a scan. stack thinks we can get rid of the newscannerlock in close since we setclosing to true. ",
        "label": 229
    },
    {
        "text": "the 'assert highestunsyncedtxid   entry gettxid ' in abstractfwal append may fail when using asyncfswal  ",
        "label": 149
    },
    {
        "text": "improve assignmentmanager handleregion so that it can process certain zk state in the case of rs offline  currently assignmentmanager.handleregion skips processing of zk event change if the rs is offline. it relies on timeoutmonitor and servershutdownhandler to process rit.  // verify this is a known server  if (!servermanager.isserveronline(sn) &&  !this.master.getservername().equals(sn)) { log.warn(\"attempted to handle region transition for server but \" + \"server is not online: \" + bytes.tostring(data.getregionname())); return; } for certain states like opened, opening, failed_open, closed, it can continue the progressing even if the rs is offline. ",
        "label": 544
    },
    {
        "text": "find a way to set sequenceid on cells on the server  over in hbase-11591 there was a need to set the sequenceid of the hfile to the bulk loaded kvs. since we are trying to use the concept of cells in the read path if we need to use setsequenceid(), then the cell has to be converted to kv and only keyvalue impl has the accessor setsequenceid().  anoop sam john suggested if we can use a server side impl of cell and have these accessors in them.  this jira aims to solve this and see the related code changes that needs to be carried out for this. ",
        "label": 46
    },
    {
        "text": "master's jmx clusterrequests could be negative in branch  in 1.x branch, regionserver could report a negative  (overflow) requestcount to master, causing the master's jmx.clusterrequests to become smaller even negative hbase-12444 fixed, but missed a little when backport to branch-1 ",
        "label": 542
    },
    {
        "text": "add timestamping to gc logging options  http://forums.sun.com/thread.jspa?threadid=5165451 ",
        "label": 314
    },
    {
        "text": "hregion get never validates row  if a client gets confused (possibly by a hole in .meta., see hbase-4333), it may send a request to the wrong region. paths through put, delete, incrementcolumnvalue, and checkandmutate all call checkrow either directly or indirectly (through getlock). but get apparently does not. this can result in returning an incorrect empty result instead of a wrongregionexception. ",
        "label": 286
    },
    {
        "text": "port hbase to   and trunk  this jira ports hbase-5155 (servershutdownhandler and disable/delete should not happen parallely leading to recreation of regions that were deleted) to 0.92 and trunk ",
        "label": 56
    },
    {
        "text": "broken link f  hbase videos  broken link to first introduction to hbase video [0] second introduction video works, so suspect a redirect at the other end is broken or it's being changed in which case the second may stop working as well. have supplied a patch [0]https://hbase.apache.org/book.html#other.info.videos ",
        "label": 374
    },
    {
        "text": "hbase shell hcd  method is broken by the replication scope parameter  since an additional hcolumndescriptor constructor parameter (scope) was introduced, hbase shell hcd() method fails to create a hcolumndescriptor object: hbase(main):007:0> alter 'doc_total_stats', {name => 'country_views', versions => 1} argumenterror: wrong # of arguments(8 for 9) ",
        "label": 25
    },
    {
        "text": "unify admin and asyncadmin  admin and asyncadmin differ some places: some methods missing from asyncadmin (e.g. methods with string regex), some methods have different names (listtables vs listtabledescriptors), some method parameters are different (e.g. asyncadmin has optional<> parameters), asyncadmin returns lists instead of arrays (e.g. listtablenames), unify javadoc comments, ... ",
        "label": 187
    },
    {
        "text": "deadlock if wrongregionexception is thrown from getlock in hregion delete  in the hregion.delete function, if getlock throws a wrongregionexception, no lock id is ever returned, yet in the finally block, it tries to release the row lock using that lock id (which is null). this causes an npe in the finally clause, and the closeregionoperation() to never execute, keeping a read lock open forever. error org.apache.hadoop.hbase.regionserver.hregionserver:   java.lang.nullpointerexception   at org.apache.hadoop.hbase.util.bytes.compareto(bytes.java:840)   at org.apache.hadoop.hbase.util.bytes$bytearraycomparator.compare(bytes.java:108)   at org.apache.hadoop.hbase.util.bytes$bytearraycomparator.compare(bytes.java:100)   at java.util.treemap.getentryusingcomparator(treemap.java:351)   at java.util.treemap.getentry(treemap.java:322)   at java.util.treemap.remove(treemap.java:580)   at java.util.treeset.remove(treeset.java:259)   at org.apache.hadoop.hbase.regionserver.hregion.releaserowlock(hregion.java:2145)   at org.apache.hadoop.hbase.regionserver.hregion.delete(hregion.java:1174)   at org.apache.hadoop.hbase.regionserver.hregionserver.delete(hregionserver.java:1914)   at sun.reflect.generatedmethodaccessor22.invoke(unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)   at java.lang.reflect.method.invoke(method.java:597)   at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570)   at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1039) when the region later attempts to close, the write lock can never be acquired, and the region remains in transition forever. ",
        "label": 5
    },
    {
        "text": "incorrect use of positional read api in hfileblock  considering that read() is not guaranteed to read all bytes,   i'm interested to understand this particular piece of code and why is partial read treated as an error : https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/hfileblock.java#l1446-l1450 particularly, if hbase were to use a different filesystem, say webhdfsfilesystem, this would not work, please also see https://issues.apache.org/jira/browse/hdfs-8943 for discussion around this. ",
        "label": 102
    },
    {
        "text": "rest gateway reports insufficient permissions exceptions as not found  when a row access is denied due to insufficient permissions, the error thrown to the client is 404 not found, instead of the proper 403 forbidden exception. ",
        "label": 454
    },
    {
        "text": "typing 'help shutdown' in the shell shouldn't shutdown the cluster  hp on irc found out the bad way that typing 'help shutdown' actually gives you the full help... and shuts down the cluster. i don't really understand why we process both commands, putting against 0.90.0 if anyone has an idea. ",
        "label": 314
    },
    {
        "text": "methods missing in htableinterface  dear all, i found some methods existed in htable were not in htableinterface.  setautoflush  setwritebuffersize  ... in most cases, i manipulate hbase through htableinterface from htablepool. if i need to use the above methods, how to do that? i am considering writing my own table pool if no proper ways. is it fine? thanks so much! best regards,  bing ",
        "label": 242
    },
    {
        "text": "backport hbase to where splitlogworker exits due to concurrentmodificationexception  today we found the following error in our tests. later i found we already fixed the issue in trunk. i think we should backpor the fix because the consequence of the issue is high and the fix isn't complicated. 2013-04-01 21:23:21,864 info org.apache.hadoop.hbase.regionserver.splitlogworker: worker ip-10-143-160-121.ec2.internal,60020,1364849529986 done with task /hbase/splitlog/hdfs%3a%2f%2fip-10-137-16-140.ec2.internal%3a8020%2fapps%2fhbase%2fdata%2f.logs%2fip-10-137-20-188.ec2.internal%2c60020%2c1364849530779-splitting%2fip-10-137-20-188.ec2.internal%252c60020%252c1364849530779.1364865556657 in 67129ms 2013-04-01 21:23:21,864 error org.apache.hadoop.hbase.regionserver.splitlogworker: unexpected error java.util.concurrentmodificationexception         at java.util.treemap$privateentryiterator.nextentry(treemap.java:1100)         at java.util.treemap$valueiterator.next(treemap.java:1145)         at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter$outputsink.closelogwriters(hlogsplitter.java:1279)         at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter$outputsink.finishwritingandclose(hlogsplitter.java:1170)         at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlogfile(hlogsplitter.java:475)         at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlogfile(hlogsplitter.java:403)         at org.apache.hadoop.hbase.regionserver.splitlogworker$1.exec(splitlogworker.java:111)         at org.apache.hadoop.hbase.regionserver.splitlogworker.grabtask(splitlogworker.java:264)         at org.apache.hadoop.hbase.regionserver.splitlogworker.taskloop(splitlogworker.java:195)         at org.apache.hadoop.hbase.regionserver.splitlogworker.run(splitlogworker.java:163)         at java.lang.thread.run(thread.java:662) 2013-04-01 21:23:21,865 info org.apache.hadoop.hbase.regionserver.splitlogworker: splitlogworker ip-10-143-160-121.ec2.internal,60020,1364849529986 exiting the impact of this issue is that splitlogworker exits so does the region server recovering mechanism of hbase. if any rs failed after all splitlogworkers in te cluster exit due to the issue, you'll see a hang log splitting job and the failed rs won't be recovered. ",
        "label": 441
    },
    {
        "text": "procedure wals are archived but not cleaned  fix  the procedure wal files used to be deleted when done. hbase-14614 keeps them around in case issue but what is missing is a gc for no-longer-needed wal files. this one is pretty important. from walprocedurestore cleaner todo in https://docs.google.com/document/d/1evka7fhdeoj1-9o8yzcotaqbv0u0bblblcczvsin69g/edit#heading=h.r2pc835nb7vi ",
        "label": 352
    },
    {
        "text": "test that we always cache index and bloom blocks  this is a unit test that should have been part of hbase-4683 but was not committed. the original test was reviewed as part of https://reviews.facebook.net/d807. submitting unit test as a separate jira and patch, and extending the scope of the test to also handle the case when block cache is enabled for the column family. the new review is at https://reviews.facebook.net/d1695. ",
        "label": 324
    },
    {
        "text": "stuck on memcache flush  latest trunk, regionserver gets stuck doing below: 2009-05-13 03:05:28,126 [regionserver/0:0:0:0:0:0:0:0:60021.logroller] info org.apache.hadoop.hbase.regionserver.hlog: closed hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/.logs/aa0-000-13.u.powerset.com_1242178588540_60021/hlog.dat.1242183899413, entries=10051 2009-05-13 03:05:28,126 [regionserver/0:0:0:0:0:0:0:0:60021.logroller] debug org.apache.hadoop.hbase.regionserver.hlog: found 0 logs to remove  out of total 53; oldest outstanding seqnum is 6838308 from region testtable,0842399686,1242180442452 2009-05-13 03:05:28,737 [ipc server handler 9 on 60021] info org.apache.hadoop.hbase.regionserver.memcacheflusher: forced flushing of testtable,0107641622,1242181965182 because global memcache limit of 497.8m exceeded; currently 513.3m and flushing till 311.1m 2009-05-13 03:05:28,737 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.memcacheflusher: too many store files in store info: 5, waiting 2009-05-13 03:05:28,737 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region testtable,0107641622,1242181965182/614435386 because: regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher 2009-05-13 03:06:07,143 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.store: completed compaction of info; store size is 168.0m 2009-05-13 03:06:07,143 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region testtable,0092188538,1242183375224 in 3mins, 58sec 2009-05-13 03:06:58,757 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.memcacheflusher: too many store files in store info: 5, waiting 2009-05-13 03:06:58,757 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region testtable,0107641622,1242181965182/614435386 because: regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher 2009-05-13 03:08:28,767 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.memcacheflusher: too many store files in store info: 5, waiting 2009-05-13 03:08:28,767 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region testtable,0107641622,1242181965182/614435386 because: regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher 2009-05-13 03:09:58,777 [ipc server handler 9 on 60021] debug org.apache.hadoop.hbase.regionserver.memcacheflusher: too many store files in store info: 5, waiting ... thread dumping i see a few threads blocked here: \"ipc server handler 4 on 60021\" daemon prio=10 tid=0x00007f42982ee000 nid=0x599b waiting for monitor entry [0x000000004380b000..0x000000004380bb00]    java.lang.thread.state: blocked (on object monitor)         at org.apache.hadoop.hbase.regionserver.memcacheflusher.reclaimmemcachememory(memcacheflusher.java:294)         - waiting to lock <0x00007f42aac8cbd8> (a org.apache.hadoop.hbase.regionserver.memcacheflusher)         at org.apache.hadoop.hbase.regionserver.hregionserver.batchupdates(hregionserver.java:1800)         at sun.reflect.generatedmethodaccessor6.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)         at java.lang.reflect.method.invoke(method.java:597)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:642)         at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:911) .. and this is a good while after my mr upload failed. assigning myself since i can reproduce easy enough. ",
        "label": 38
    },
    {
        "text": "new ui should space detailed latency columns equally  spacing between the columns of the detailed latencies tab should be roughly equal. round latencies to two digits right of decimal point. ",
        "label": 154
    },
    {
        "text": "add a table attribute to make user scan snapshot feature configurable for table  if a cluster enable user scan snapshot feature, it will work for all tables. since this feature will make some operations such as grant, revoke, snapshot... slower and some tables don't use scan snaphot to make scan faster. so add a table attribute to make it configurable at table level, in general, the feature is disabled by default, and if someone use feature, must enable the attribute of the specific table firstly. ",
        "label": 500
    },
    {
        "text": "a logic mistake in hregionserver ishealthy  after visiting the ishealthy in hregionserver, i think there is a logic mistake.     // verify that all threads are alive     if (!(leases.isalive()         && cacheflusher.isalive() && hlogroller.isalive()         && this.compactionchecker.isalive())   <---- logic wrong here         && this.periodicflusher.isalive()) {       stop(\"one or more threads are no longer alive -- stop\");       return false;     } which should be     // verify that all threads are alive     if (!(leases.isalive()         && cacheflusher.isalive() && hlogroller.isalive()         && this.compactionchecker.isalive()         && this.periodicflusher.isalive())) {       stop(\"one or more threads are no longer alive -- stop\");       return false;     } please finger out if i am wrong. thx ",
        "label": 411
    },
    {
        "text": "deadlock in memstore flusher due to jdk internally synchronizing on current thread  we observed a deadlock in production between the following threads: ipc handler thread holding the monitor lock on memstoreflusher inside reclaimmemstorememory, waiting to obtain memstoreflusher.lock (the reentrant lock member) cacheflusher thread inside flushregion holds memstoreflusher.lock, and then calls prioritycompactionqueue.add, which calls prioritycompactionqueue.addtoregionsinqueue, which calls compactionrequest.tostring(), which calls date.tostring. if this occurs just after a gc under memory pressure, date.tostring needs to reload locale information (stored in a soft reference), so it calls resourcebundle.loadbundle, which uses thread.currentthread() as a synchronizer (see sun bug http://bugs.sun.com/view_bug.do?bug_id=6915621). since the current thread is the memstoreflusher itself, we have a lock order inversion and a deadlock. ",
        "label": 453
    },
    {
        "text": " hbase  hregioninfo cell empty in meta table  when we notice one of these, instead of reporting on it over and over \u2013 see below \u2013 lets just axe the whole row. its never going to get better on its own. we should also figure how these horked rows get manufactured. below is about split cells but also instances where servercode and servername are all thats left in a row. 2008-01-24 02:01:02,761 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,761 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,762 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,762 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,762 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,763 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,763 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,763 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,764 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,764 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,764 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,765 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,765 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,765 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,766 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,766 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,766 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,767 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,767 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,767 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,768 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,768 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,768 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,769 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,769 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,769 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,770 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,770 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] 2008-01-24 02:01:02,771 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,771 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splitb] 2008-01-24 02:01:02,771 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita, info:splitb] 2008-01-24 02:01:02,772 warn org.apache.hadoop.hbase.hmaster: info:regioninfo is empty; has keys: [info:splita] ",
        "label": 241
    },
    {
        "text": "increase unit test coverage of package org apache hadoop hbase coprocessor  increase unit-test coverage of package org.apache.hadoop.hbase.coprocessor up to 80%. ",
        "label": 217
    },
    {
        "text": "bin graceful stop sh logs nothing as a balancer state to be stored  in bin/graceful_stop.sh log() {   echo `date +%y-%m-%dt%h:%m:%s` $1 } ...skip... if [ $hbase_balancer_state != \"false\" ]; then   log \"restoring balancer state to \" $hbase_balancer_state the position of the above last double quotation mark is wrong, and the balancer state will be not shown. ",
        "label": 200
    },
    {
        "text": "rest service ignores misspelled  check  parameter  causing unexpected mutations  in rest.rowresource.update(), this code keeps executing a request if a misspelled check= parameter is provided.     if (check_put.equalsignorecase(check)) {       return checkandput(model);     } else if (check_delete.equalsignorecase(check)) {       return checkanddelete(model);     } else if (check != null && check.length() > 0) {       log.warn(\"unknown check value: \" + check + \", ignored\");     } by my reading of the code, this results in the provided cell value that was intended as a check instead being treated as a mutation, which is sure to destroy user data. thus the priority of this bug, as it can cause corruption. i suggest that a better reaction than a warning would be, approximately: return response.status(response.status.bad_request)         .type(mimetype_text).entity(\"invalid check value '\" + check + \"'\")         .build(); ",
        "label": 101
    },
    {
        "text": "remove unnecessary getrow overloads in hrs  hrs currently contains:  public rowresult getrow(final byte [] regionname, final byte [] row, final long ts)  public rowresult getrow(final byte [] regionname, final byte [] row, final byte [][] columns)  public rowresult getrow(final byte [] regionname, final byte [] row, final byte [][] columns, final long ts) the first two call the last one which calls hr.getfull. changes will be made to htable to map all getrow calls to a single getrow hrs method. ",
        "label": 241
    },
    {
        "text": "region historian  whenever we try to debug region splitting, assignment, compaction, etc. issues, we always end up having to look in 1-20 different log files for cryptic names of regions and try to piece together the chain of events ourselves. this is a challenging at best effort most of the time. what would be very useful would be a new utility i've nicknamed the region historian. you give it the text name of a region, and it will track down the log messages relevant to it in the master and regionserver logs. then, it will interleave the messages in such a way that the timestamps correctly list the order of events. the result is a log summary that accurately describes what happened to a region during it's lifetime, making it much easier to try and figure out where something went wrong. other things it could do would be replace cryptic log messages with simple events like \"the region was split into a and b\", \"the region was assigned to server x\", and trace the lineage of a region backwards to its parent before it came into existence. i'm sure there are other things we would think up that would be useful as well. ",
        "label": 229
    },
    {
        "text": "fix wrapping of requests counts regionserver level metrics  for aggregating the metrics from all of its regions hregionserver is using int, while the underlying region level metrics use long. due to this they get wrapped around giving out negative values if the regionserver is not restarted for a long time. ",
        "label": 524
    },
    {
        "text": "port hfileblockindex improvement from hbase  excerpt from hbase-5987:  first, we propose to lookahead for one more block index so that the hfilescanner would know the start key value of next data block. so if the target key value for the scan(reseekto) is \"smaller\" than that start kv of next data block, it means the target key value has a very high possibility in the current data block (if not in current data block, then the start kv of next data block should be returned. indexing on the start key has some defects here) and it shall not query the hfileblockindex in this case. on the contrary, if the target key value is \"bigger\", then it shall query the hfileblockindex. this improvement shall help to reduce the hotness of hfileblockindex and avoid some unnecessary idlock contention or index block cache lookup. this jira is to port the fix to hbase trunk, etc. ",
        "label": 441
    },
    {
        "text": "stop from resolving hregionserver addresses to names using dns on every heartbeat  over the time many parts of the code have evolved in different ways and one issue is that addresses are handled differently in different parts of the code. we need to set a standard and correct any inconsistencies. ",
        "label": 268
    },
    {
        "text": "coprocessors  flag the presence of coprocessors in logged exceptions  for some initial triage of bug reports for core versus for deployments with loaded coprocessors, we need something like the linux kernel's taint flag, and list of linked in modules that show up in the output of every oops, to appear above or below exceptions that appear in the logs. ",
        "label": 164
    },
    {
        "text": "job configuration string  deprecated  fix deprecations: job(configuration,string) deprecated by job.getinstance(cluster, configuration)   hbaseconfiguration() deprecated by hbaseconfiguration.create() ",
        "label": 266
    },
    {
        "text": "build on hudson failing compiling jsp  fix the build up on hudson. its failing compiling jsps. init:      [copy] copying 2 files to /zonestorage/hudson/home/hudson/hudson/jobs/hbase-patch/workspace/trunk/build/webapps jspc: build failed /zonestorage/hudson/home/hudson/hudson/jobs/hbase-patch/workspace/trunk/build.xml:178: java.lang.exceptionininitializererror total time: 14 seconds error: no artifacts found that match the file pattern \"trunk/build/*.tar.gz\". configuration error? error: 'trunk/build/*.tar.gz' doesn't match anything: 'trunk' exists but not 'trunk/build/*.tar.gz' recording test results i think upgrading ant will fix above. ",
        "label": 314
    },
    {
        "text": "icms as default jvm  ",
        "label": 314
    },
    {
        "text": "address some recent random test failures  i've seen many unspecific test failures recently that cannot be reproduced locally even when running these test is a loop for a very long time. many of these test one way or the other make assumption w.r.t. wall clock time. while i cannot fix that, an option to increase some of these timeout a bit. this issue is to remind me to do that. ",
        "label": 286
    },
    {
        "text": "upgrade surefire plugin to m4  ",
        "label": 363
    },
    {
        "text": "try to get rid of unused hconnection instance  after hbase-14361, we get rid of hconnection instance in replicationsink in standalone mode. but there are still three hconnection instance, i think the unused ones should be removed. the three instances are created below     6 2015-09-21 16:05:36,401 info  [10.0.3.80:60429.activemastermanager] client.connectionimplementation:.     7 java.lang.throwable     8 >---at org.apache.hadoop.hbase.client.connectionimplementation.<init>(connectionimplementation.java:217)     9 >---at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)    10 >---at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:62)    11 >---at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45)    12 >---at java.lang.reflect.constructor.newinstance(constructor.java:422)    13 >---at org.apache.hadoop.hbase.client.connectionfactory.createconnection(connectionfactory.java:231)    14 >---at org.apache.hadoop.hbase.client.connectionfactory.createconnection(connectionfactory.java:118)    15 >---at org.apache.hadoop.hbase.master.servermanager.<init>(servermanager.java:222)    16 >---at org.apache.hadoop.hbase.master.servermanager.<init>(servermanager.java:212)    17 >---at org.apache.hadoop.hbase.master.hmaster.createservermanager(hmaster.java:853)    18 >---at org.apache.hadoop.hbase.master.hmaster.finishactivemasterinitialization(hmaster.java:661)    19 >---at org.apache.hadoop.hbase.master.hmaster.access$500(hmaster.java:180)    20 >---at org.apache.hadoop.hbase.master.hmaster$1.run(hmaster.java:1670)    21 >---at java.lang.thread.run(thread.java:745)    22 2015-09-21 16:05:36,401 info  [m:0;10.0.3.80:60429] client.connectionimplementation:.    23 java.lang.throwable    24 >---at org.apache.hadoop.hbase.client.connectionimplementation.<init>(connectionimplementation.java:217)    25 >---at org.apache.hadoop.hbase.client.connectionutils$1.<init>(connectionutils.java:128)    26 >---at org.apache.hadoop.hbase.client.connectionutils.createshortcircuitconnection(connectionutils.java:128)    27 >---at org.apache.hadoop.hbase.regionserver.hregionserver.createclusterconnection(hregionserver.java:686)    28 >---at org.apache.hadoop.hbase.regionserver.hregionserver.setupclusterconnection(hregionserver.java:717)    29 >---at org.apache.hadoop.hbase.regionserver.hregionserver.preregistrationinitialization(hregionserver.java:730)    30 >---at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:885)    31 >---at org.apache.hadoop.hbase.master.hmastercommandline$localhmaster.run(hmastercommandline.java:311)    32 >---at java.lang.thread.run(thread.java:745)    33 2015-09-21 16:05:36,401 info  [rs:0;10.0.3.80:60431] client.connectionimplementation:.    34 java.lang.throwable    35 >---at org.apache.hadoop.hbase.client.connectionimplementation.<init>(connectionimplementation.java:217)    36 >---at org.apache.hadoop.hbase.client.connectionutils$1.<init>(connectionutils.java:128)    37 >---at org.apache.hadoop.hbase.client.connectionutils.createshortcircuitconnection(connectionutils.java:128)    38 >---at org.apache.hadoop.hbase.regionserver.hregionserver.createclusterconnection(hregionserver.java:686)    39 >---at org.apache.hadoop.hbase.regionserver.hregionserver.setupclusterconnection(hregionserver.java:717)    40 >---at org.apache.hadoop.hbase.regionserver.hregionserver.preregistrationinitialization(hregionserver.java:730)    41 >---at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:885)    42 >---at java.lang.thread.run(thread.java:745) ",
        "label": 198
    },
    {
        "text": "be able to block on htablemultiplexer puts when the buffer is full  so far, when we try to insert to htablemultiplexer, it returns false if the buffer is full. this forces one to write wrapper code to retry inserting into the buffer. if someone is okay with the put getting blocked while the buffer is flushed, we should block. ",
        "label": 154
    },
    {
        "text": "testadmin failing in because  tableinfo not found  i've been running tests before commit and found the following happens with some regularity, sporadic of course, but they fail fairly frequently: failed tests:   testonlinechangetableschema(org.apache.hadoop.hbase.client.testadmin)   testforcesplit(org.apache.hadoop.hbase.client.testadmin): expected:<2> but was:<1>   testforcesplitmultifamily(org.apache.hadoop.hbase.client.testadmin): expected:<2> but was:<1> looking, it seems like we fail to find .tableinfo in the tests that modify table schema while table is online. the update of a table schema just does an overwrite. in the tests we sometimes fail to find the newly written file or we get eofe reading it. ",
        "label": 314
    },
    {
        "text": "allow shared regionobserver state  ",
        "label": 286
    },
    {
        "text": "bulk loading script makes regions incorrectly  loadtable rb   from mailing list, murali krishna found that loadtable.rb is not making regions correctly from the passed hfiles. ",
        "label": 314
    },
    {
        "text": "still a 'hole' in scanners  even after hbase  before hbase-532, as soon as a flush started, we called snapshot. snapshot used to copy current live memcache into a 'snapshot' treemap inside in memcache. this snapshot treemap was an accumulation of all snapshots since last flush. whenever we took out a scanner, we'd do a copy of this snapshot into a new backing map carried by the scanner (every outstanding scanner had complete copy). memcache snapshots were cleared when a flush started. flushing could take near no time to up to tens of seconds during which an scanners taken out meantime would not see the edits in the snapshot currently being flushed and gets or getfull would also return incorrect answers because the content of the snapshot was not available to them. hbase-532 made it so the snapshot was available until flush was done \u2013 until a file had made it out to disk. this fixed gets and getfull and any scanners taken out during flushing. but there is still a hole. any outstanding scanners will be going against the state of store readers at time scanner was opened; they will not see the new flush file. chatting about this on irc, jim suggests that we pass either memcache or current snapshot to each scanner (pass the snapshot if not empty). the notion is that the scanner would hold on to the scanner reference should it be cleared by flushing. upside is that scanner wouldn't have to be concerned with the new flush that has been put out to disk. downsides are that scanner data could be way stale if for instance the memcache was near to flushing but we hadn't done it yet. and we wouldn't be clearing the snapshot promptly so would be some memory pressure. another suggestion is that flushing send an event. listeners such as outstanding scanners would notice event and open the new reader. would have to skip forward in the new reader to catch up with the current set but shouldn't be bad. same mechanism could be used to let compactions be moved into place while scanners were outstanding closing down all existing readers skipping to the current 'next' location in the new compacted store file. ",
        "label": 314
    },
    {
        "text": "new ui should color task list entries  the old ui changed the background color of tasklist entries according to their final status: green if successful, yellow if aborted, red if failed. bring this back in the new ui. ",
        "label": 154
    },
    {
        "text": "add histogram to metricsconnection to track concurrent calls per server  hbase-16388 introduced a new configuration setting \"hbase.client.perserver.requests.threshold \" to deal with slow region servers. i have back-ported the code for the new config setting to our environment, but i don't feel comfortable setting it in production without visibility into how the number of concurrent calls per server varies (especially the current high water mark or max in production when the cluster is healthy). it is straightforward to pass the value for the concurrent calls per server to a new histogram in metricsconnection. i will attach a patch that i am using to gain a better understanding of how setting \"hbase.client.perserver.requests.threshold\" will affect our production environment. ",
        "label": 380
    },
    {
        "text": "servername is created using getlocalsocketaddress  breaks binding to the wildcard address  revert hbase  in hbase-8148 i added a way to bind to specific addresses, like 0.0.0.0, but right now in 0.95/trunk the servername is created using getlocalsocketaddress in rpcserver so 0.0.0.0 gets published in zk. ",
        "label": 229
    },
    {
        "text": "documentation has incorrect default number of versions  reference guide has this section under compaction. compaction and versions  when you create a column family, you can specify the maximum number of versions to keep, by specifying hcolumndescriptor.setmaxversions(int versions). the default value is 3. if more versions than the specified maximum exist, the excess versions are filtered out and not written back to the compacted storefile. this is incorrect, the default value is 1. additionally, hcolumndescriptor is deprecated and the example should use columnfamilydescriptorbuilder$setmaxversions(int) instead. ",
        "label": 398
    },
    {
        "text": "hfile quarantine fails with missing files in hadoop  trunk/0.96 has a specific issue mentioned in hbase-6686 when run against hadoop 2.0. this addresses this problem. 2012-08-29 12:55:26,031 error [ipc server handler 0 on 41070] security.usergroupinformation(1235): priviledgedactionexception as:jon (auth:simple) cause:java.io.filenotfoundexception: file does not exist: /user/jon/hbase/testquarantinemissinghfile/4332ea87d02d33e443550537722ff4fc/fam/befbe65ff30e4a46866f04a5671f0e44 2012-08-29 12:55:26,085 warn  [thread-2994] hbck.hfilecorruptionchecker(253): failed to quaratine an hfile in regiondir hdfs://localhost:41070/user/jon/hbase/testquarantinemissinghfile/4332ea87d02d33e443550537722ff4fc java.lang.reflect.undeclaredthrowableexception at $proxy23.getblocklocations(unknown source) at org.apache.hadoop.hdfs.dfsclient.callgetblocklocations(dfsclient.java:882) at org.apache.hadoop.hdfs.dfsinputstream.fetchlocatedblocksandgetlastblocklength(dfsinputstream.java:152) at org.apache.hadoop.hdfs.dfsinputstream.openinfo(dfsinputstream.java:119) at org.apache.hadoop.hdfs.dfsinputstream.&lt;init&gt;(dfsinputstream.java:112) at org.apache.hadoop.hdfs.dfsclient.open(dfsclient.java:955) at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:212) at org.apache.hadoop.hdfs.distributedfilesystem.open(distributedfilesystem.java:75) at org.apache.hadoop.fs.filesystem.open(filesystem.java:664) at org.apache.hadoop.hbase.io.hfile.hfile.createreaderwithencoding(hfile.java:575) at org.apache.hadoop.hbase.io.hfile.hfile.createreader(hfile.java:605) at org.apache.hadoop.hbase.util.hbck.hfilecorruptionchecker.checkhfile(hfilecorruptionchecker.java:94) at org.apache.hadoop.hbase.util.testhbasefsck$1$1.checkhfile(testhbasefsck.java:1401) at org.apache.hadoop.hbase.util.hbck.hfilecorruptionchecker.checkcolfamdir(hfilecorruptionchecker.java:175) at org.apache.hadoop.hbase.util.hbck.hfilecorruptionchecker.checkregiondir(hfilecorruptionchecker.java:208) at org.apache.hadoop.hbase.util.hbck.hfilecorruptionchecker$regiondirchecker.call(hfilecorruptionchecker.java:290) at org.apache.hadoop.hbase.util.hbck.hfilecorruptionchecker$regiondirchecker.call(hfilecorruptionchecker.java:281) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.executors$runnableadapter.call(executors.java:441) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:98) at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:206) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.lang.reflect.invocationtargetexception at sun.reflect.generatedmethodaccessor27.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.fs.hfilesystem$1.invoke(hfilesystem.java:261) ... 27 more caused by: java.io.filenotfoundexception: file does not exist: /user/jon/hbase/testquarantinemissinghfile/4332ea87d02d33e443550537722ff4fc/fam/befbe65ff30e4a46866f04a5671f0e44 at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocationsupdatetimes(fsnamesystem.java:1133) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1095) at org.apache.hadoop.hdfs.server.namenode.fsnamesystem.getblocklocations(fsnamesystem.java:1067) at org.apache.hadoop.hdfs.server.namenode.namenoderpcserver.getblocklocations(namenoderpcserver.java:384) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocolserversidetranslatorpb.getblocklocations(clientnamenodeprotocolserversidetranslatorpb.java:165) at org.apache.hadoop.hdfs.protocol.proto.clientnamenodeprotocolprotos$clientnamenodeprotocol$2.callblockingmethod(clientnamenodeprotocolprotos.java:42586) at org.apache.hadoop.ipc.protobufrpcengine$server$protobufrpcinvoker.call(protobufrpcengine.java:427) at org.apache.hadoop.ipc.rpc$server.call(rpc.java:916) at org.apache.hadoop.ipc.server$handler$1.run(server.java:1692) at org.apache.hadoop.ipc.server$handler$1.run(server.java:1688) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:396) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1232) at org.apache.hadoop.ipc.server$handler.run(server.java:1686) at org.apache.hadoop.ipc.client.call(client.java:1161) at org.apache.hadoop.ipc.protobufrpcengine$invoker.invoke(protobufrpcengine.java:184) at $proxy17.getblocklocations(unknown source) at sun.reflect.generatedmethodaccessor26.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.io.retry.retryinvocationhandler.invokemethod(retryinvocationhandler.java:165) at org.apache.hadoop.io.retry.retryinvocationhandler.invoke(retryinvocationhandler.java:84) at $proxy17.getblocklocations(unknown source) at org.apache.hadoop.hdfs.protocolpb.clientnamenodeprotocoltranslatorpb.getblocklocations(clientnamenodeprotocoltranslatorpb.java:149) ... 31 more ",
        "label": 248
    },
    {
        "text": "don't close region if message is not from server that opened it or is opening it  we assign a region to a server. it takes too long to open (hbase-505). region gets assigned to another server. meantime original host returns a msg_report_close (because other regions opening messes it up moving files on disk out from under it). we queue a shutdown which marks the region as needing reassignment. second server reports in that it successfully opened the region. master tells it it should not have opened it. churn ensues. fix is to ignore the close if its reported server/startcode does not match that of the server currently trying to open region. fix is not easy because currently we don't keep list of server info in unassigned regions. here's master log snippet showing problem: ... 2008-03-25 19:16:43,711 info org.apache.hadoop.hbase.hmaster: assigning region enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 to server xx.xx.xx.220:60020 2008-03-25 19:16:46,725 debug org.apache.hadoop.hbase.hmaster: received msg_report_process_open : enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 from xx.xx.xx.220:60020 2008-03-25 19:18:06,411 debug org.apache.hadoop.hbase.hmaster: shutdown scanner looking at enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:18:06,811 debug org.apache.hadoop.hbase.hmaster: shutdown scanner looking at enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:19:46,841 info org.apache.hadoop.hbase.hmaster: assigning region enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 to server xx.xx.xx.221:60020 2008-03-25 19:19:49,849 debug org.apache.hadoop.hbase.hmaster: received msg_report_process_open : enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 from xx.xx.xx.221:60020 2008-03-25 19:19:56,883 debug org.apache.hadoop.hbase.hmaster: received msg_report_close : enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 from xx.xx.xx.220:60020 2008-03-25 19:19:56,883 info org.apache.hadoop.hbase.hmaster: xx.xx.xx.220:60020 no longer serving regionname: enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482, startkey: <ilstz0ytnfvuziycnvvxwv==>, endkey: <jlb27q4hkls4tsvp64rjff== >, encodedname: 1857033608, tabledesc: {name: enwiki_080103, families: {alternate_title:={name: alternate_title, max versions: 3, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, alternate_url:={name: al ternate_url, max versions: 3, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, anchor:={name: anchor, max versions: 3, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, mi sc:={name: misc, max versions: 3, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, page:={name: page, max versions: 3, compression: none, in memory: false, max length: 2147483647, bloom filter: none}, re direct:={name: redirect, max versions: 3, compression: none, in memory: false, max length: 2147483647, bloom filter: none}}} 2008-03-25 19:19:56,885 debug org.apache.hadoop.hbase.hmaster: main processing loop: processregionclose of enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482, true, false 2008-03-25 19:19:56,885 info org.apache.hadoop.hbase.hmaster: region closed: enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:19:56,887 info org.apache.hadoop.hbase.hmaster: reassign region: enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:19:57,288 info org.apache.hadoop.hbase.hmaster: assigning region enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 to server xx.xx.xx.189:60020 2008-03-25 19:20:00,296 debug org.apache.hadoop.hbase.hmaster: received msg_report_process_open : enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 from xx.xx.xx.189:60020 2008-03-25 19:20:16,885 debug org.apache.hadoop.hbase.hmaster: received msg_report_open : enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 from xx.xx.xx.221:60020 2008-03-25 19:20:16,885 debug org.apache.hadoop.hbase.hmaster: region server xx.xx.xx.221:60020 should not have opened region enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:23:51,707 debug org.apache.hadoop.hbase.hmaster: shutdown scanner looking at enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:23:51,834 debug org.apache.hadoop.hbase.hmaster: shutdown scanner looking at enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 2008-03-25 19:23:53,947 info org.apache.hadoop.hbase.hmaster: assigning region enwiki_080103,ilstz0ytnfvuziycnvvxwv==,1205393076482 to server xx.xx.xx.97:60020 ... ",
        "label": 314
    },
    {
        "text": "rollingbatchrestartrsaction aborts if timeout  in our test rigs, we see following quiet frequently: 2013-10-10 05:04:09,367 info  [thread-6] actions.action: killing region server:a1809.halxg.cloudera.com,60020,1381404629253 2013-10-10 05:04:09,367 info  [thread-6] hbase.hbasecluster: aborting rs: a1809.halxg.cloudera.com,60020,1381404629253 2013-10-10 05:04:09,367 info  [thread-6] hbase.clustermanager: executing remote command: ps aux | grep proc_regionserver | grep -v grep | tr -s ' ' | cut -d ' ' -f2 | xargs kill -s sigkill , hostname:a1809.halxg.cloudera.com 2013-10-10 05:04:09,367 info  [thread-6] util.shell: executing full command [/usr/bin/ssh -o connecttimeout=1 -o stricthostkeychecking=no a1809.halxg.cloudera.com \"ps aux | grep proc_regionserver | grep -v grep | tr -s ' ' | cut -d ' ' -f2 | xargs kill -s sigkill\"] 2013-10-10 05:04:09,621 debug [thread-5] client.hbaseadmin: getting current status of snapshot from master... 2013-10-10 05:04:09,623 debug [thread-5] client.hbaseadmin: (#6) sleeping: 1714ms while waiting for snapshot completion. 2013-10-10 05:04:10,381 warn  [thread-6] policies.policy: exception occured during performing action: org.apache.hadoop.util.shell$exitcodeexception: connection timed out during banner exchange at org.apache.hadoop.util.shell.runcommand(shell.java:458) at org.apache.hadoop.util.shell.run(shell.java:373) at org.apache.hadoop.util.shell$shellcommandexecutor.execute(shell.java:578) at org.apache.hadoop.hbase.hbaseclustermanager$remoteshell.execute(hbaseclustermanager.java:111) at org.apache.hadoop.hbase.hbaseclustermanager.exec(hbaseclustermanager.java:187) at org.apache.hadoop.hbase.hbaseclustermanager.signal(hbaseclustermanager.java:216) at org.apache.hadoop.hbase.clustermanager.kill(clustermanager.java:97) at org.apache.hadoop.hbase.distributedhbasecluster.killregionserver(distributedhbasecluster.java:110) at org.apache.hadoop.hbase.chaos.actions.action.killrs(action.java:84) at org.apache.hadoop.hbase.chaos.actions.rollingbatchrestartrsaction.perform(rollingbatchrestartrsaction.java:60) at org.apache.hadoop.hbase.chaos.policies.periodicrandomactionpolicy.runoneiteration(periodicrandomactionpolicy.java:59) at org.apache.hadoop.hbase.chaos.policies.periodicpolicy.run(periodicpolicy.java:41) at org.apache.hadoop.hbase.chaos.policies.compositesequentialpolicy.run(compositesequentialpolicy.java:42) at java.lang.thread.run(thread.java:724) ... so, we went to kill a rs and we timed out. server was busy at the time. we see the kill usually going through. when above happens in a rollingbatchrestartrsaction, we'll usually 'lose' a server for the rest of the test. that is at a minimum. we've also seen case where we kill near all servers in cluster and then the above timeout happens and we are left w/ a test limping along running real slow eventually failing. ",
        "label": 314
    },
    {
        "text": "testhmasterrpcexception in failed twice on socket timeout  #502 and #498 0.92 builds have testhmasterrpcexception failing because of socket timeout when servernotrunning is expected. socket timeout is 100ms only. ",
        "label": 314
    },
    {
        "text": "rit map in rs not getting cleared while region opening  while opening the region in rs after adding the region to regionsintransitioninrs if tabledescriptors.get() throws exception the region wont be cleared from regionsintransitioninrs. so next time if it tries to open the region in the same rs it will throw the regionalreadyintransitionexception. if swap the below statement this issue wont come. this.regionsintransitioninrs.putifabsent(region.getencodednameasbytes(),true); htabledescriptor htd = this.tabledescriptors.get(region.gettablename()); ",
        "label": 100
    },
    {
        "text": "avoid temp bytebuffer allocation in fileioengine read  a temp bytebuffer was allocated each time fileioengine#read was called public cacheable read(bucketentry be) throws ioexception {   long offset = be.offset();   int length = be.getlength();   preconditions.checkargument(length >= 0, \"length of read can not be less than 0.\");   bytebuffer dstbuffer = bytebuffer.allocate(length);   ... } we can avoid this by use of bytebuffallocator#allocate(length) after hbase-21879 ",
        "label": 521
    },
    {
        "text": "save one call to keyvalueheap peek per row  another one of my micro optimizations.  in storescanner.next(...) we can actually save a call to keyvalueheap.peek, which in my runs of scan heavy loads shows up at top. based on the run and data this can safe between 3 and 10% of runtime. ",
        "label": 286
    },
    {
        "text": "server side scanner doesn't honor stop row  i have a large table. if i create a scanner with a stop row near the beginning of the table, the last hasnext call hangs for a while. if i do the same with the stop row near the end of the table, the last hasnext call is pretty quick. i suspect that the server side scanner isn't terminating early, and is actually scanning through the whole table returning nothing. ",
        "label": 314
    },
    {
        "text": "bin local master backup regionservers sh doesn't take  config arg  i needed backup masters because the chaos monkey keeps killing them. i tried bin/local-master-backup.sh because has nice doc in refguide. my conf is elsewhere than default and new master just crashes out if i pass --config. ",
        "label": 543
    },
    {
        "text": "can not set coprocessor from shell after hbase  i was able to set coprocessors for table by shell normally, but it didn't worked now.  here's the shell output (omit really jar path and coprocessor classname)\uff1a hbase shell; enter 'help<return>' for list of supported commands. type \"exit<return>\" to leave the hbase shell version 1.3.0-snapshot, 642273bc2a5a415eba6f1592a439a6b2b53a70a9, tue sep 29 15:58:28 cst 2015 hbase(main):001:0> describe 'test' table test is enabled test column families description {name => 'f', data_block_encoding => 'none', bloomfilter => 'row', replication_scope => '0', versions => '1', compression => 'none', min_versions => '0', ttl => 'forever', keep_deleted_cells => 'false', b locksize => '65536', in_memory => 'false', blockcache => 'true'} 1 row(s) in 0.4370 seconds hbase(main):002:0> alter 'test', 'coprocessor'=>'hdfs:///some.jar|com.somepackage.someobserver|1001' updating all regions with the new schema... 1/1 regions updated. done. 0 row(s) in 1.9760 seconds hbase(main):003:0> describe 'test' table test is enabled test, {table_attributes => {coprocessor$1 => '|hdfs:///some.jar|com.somepackage.someobserver|1001|1073741823|'} column families description {name => 'f', data_block_encoding => 'none', bloomfilter => 'row', replication_scope => '0', versions => '1', compression => 'none', min_versions => '0', ttl => 'forever', keep_deleted_cells => 'false', b locksize => '65536', in_memory => 'false', blockcache => 'true'} 1 row(s) in 0.0180 seconds i checked the recent commits and found hbase-14224 is related to. it's an important improvement, but made a mistake in alter() of admin.rb, line 587. as the value is coprocess spec string but not only class name, here should use htd.setcoprocessorwithspec instead of htd.setcoprocessor. ",
        "label": 498
    },
    {
        "text": "fifocompactionpolicy pre check uses wrong scope  see https://issues.apache.org/jira/browse/hbase-14468 it adds this check to hmaster.checkcompactionpolicy(): // 1. check ttl if (hcd.gettimetolive() == hcolumndescriptor.default_ttl) {   message = \"default ttl is not supported for fifo compaction\";   throw new ioexception(message); } // 2. check min versions if (hcd.getminversions() > 0) {   message = \"min_version > 0 is not supported for fifo compaction\";   throw new ioexception(message); } // 3. blocking file count string sbfc = htd.getconfigurationvalue(hstore.blocking_storefiles_key); if (sbfc != null) {   blockingfilecount = integer.parseint(sbfc); } if (blockingfilecount < 1000) {   message =       \"blocking file count '\" + hstore.blocking_storefiles_key + \"' \" + blockingfilecount           + \" is below recommended minimum of 1000\";   throw new ioexception(message); } why does it only check the blocking file count on the htd level, while  others are check on the hcd level? doing this for example fails  because of it: hbase(main):008:0> create 'ttltable', { name => 'cf1', ttl => 300, configuration => { 'hbase.hstore.defaultengine.compactionpolicy.class' => 'org.apache.hadoop.hbase.regionserver.compactions.fifocompactionpolicy', 'hbase.hstore.blockingstorefiles' => 2000 } } error: org.apache.hadoop.hbase.donotretryioexception: blocking file count 'hbase.hstore.blockingstorefiles' 10 is below recommended minimum of 1000 set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks at org.apache.hadoop.hbase.master.hmaster.warnorthrowexceptionforfailure(hmaster.java:1782) at org.apache.hadoop.hbase.master.hmaster.sanitychecktabledescriptor(hmaster.java:1663) at org.apache.hadoop.hbase.master.hmaster.createtable(hmaster.java:1545) at org.apache.hadoop.hbase.master.masterrpcservices.createtable(masterrpcservices.java:469) at org.apache.hadoop.hbase.protobuf.generated.masterprotos$masterservice$2.callblockingmethod(masterprotos.java:58549) at org.apache.hadoop.hbase.ipc.rpcserver.call(rpcserver.java:2339) at org.apache.hadoop.hbase.ipc.callrunner.run(callrunner.java:123) at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:188) at org.apache.hadoop.hbase.ipc.rpcexecutor$handler.run(rpcexecutor.java:168) caused by: java.io.ioexception: blocking file count 'hbase.hstore.blockingstorefiles' 10 is below recommended minimum of 1000 at org.apache.hadoop.hbase.master.hmaster.checkcompactionpolicy(hmaster.java:1773) at org.apache.hadoop.hbase.master.hmaster.sanitychecktabledescriptor(hmaster.java:1661) ... 7 more the check should be performed on the column family level instead. ",
        "label": 478
    },
    {
        "text": "hbase increments from old value after delete and write to disk  deleted row values are sometimes used for starting points on new increments. to reproduce:  create a row \"r\". set column \"x\" to some default value.  force hbase to write that value to the file system (such as restarting the cluster).  delete the row.  call table.incrementcolumnvalue with \"some_value\"  get the row.  the returned value in the column was incremented from the old value before the row was deleted instead of being initialized to \"some_value\". code to reproduce: import java.io.ioexception; import org.apache.hadoop.conf.configuration; import org.apache.hadoop.hbase.hbaseconfiguration; import org.apache.hadoop.hbase.hcolumndescriptor; import org.apache.hadoop.hbase.htabledescriptor; import org.apache.hadoop.hbase.client.delete; import org.apache.hadoop.hbase.client.get; import org.apache.hadoop.hbase.client.hbaseadmin; import org.apache.hadoop.hbase.client.htableinterface; import org.apache.hadoop.hbase.client.htablepool; import org.apache.hadoop.hbase.client.increment; import org.apache.hadoop.hbase.client.result; import org.apache.hadoop.hbase.util.bytes; public class hbasetestincrement { static string tablename  = \"testincrement\"; static byte[] infocf = bytes.tobytes(\"info\"); static byte[] rowkey = bytes.tobytes(\"test-rowkey\"); static byte[] newinc = bytes.tobytes(\"new\"); static byte[] oldinc = bytes.tobytes(\"old\"); /**  * this code reproduces a bug with increment column values in hbase  * usage: first run part one by passing '1' as the first arg  *        then restart the hbase cluster so it writes everything to disk  *   run part two by passing '2' as the first arg  *  * this will result in the old deleted data being found and used for the increment calls  *  * @param args  * @throws ioexception  */ public static void main(string[] args) throws ioexception { if(\"1\".equals(args[0])) partone(); if(\"2\".equals(args[0])) parttwo(); if (\"both\".equals(args[0])) { partone(); parttwo(); } } /**  * creates a table and increments a column value 10 times by 10 each time.  * results in a value of 100 for the column  *  * @throws ioexception  */ static void partone()throws ioexception { configuration conf = hbaseconfiguration.create(); hbaseadmin admin = new hbaseadmin(conf); htabledescriptor tabledesc = new htabledescriptor(tablename); tabledesc.addfamily(new hcolumndescriptor(infocf)); if(admin.tableexists(tablename)) { admin.disabletable(tablename); admin.deletetable(tablename); } admin.createtable(tabledesc); htablepool pool = new htablepool(conf, integer.max_value); htableinterface table = pool.gettable(bytes.tobytes(tablename)); //increment unitialized column for (int j = 0; j < 10; j++) { table.incrementcolumnvalue(rowkey, infocf, oldinc, (long)10); increment inc = new increment(rowkey); inc.addcolumn(infocf, newinc, (long)10); table.increment(inc); } get get = new get(rowkey); result r = table.get(get); system.out.println(\"initial values: new \" + bytes.tolong(r.getvalue(infocf, newinc)) + \" old \" + bytes.tolong(r.getvalue(infocf, oldinc))); } /**  * first deletes the data then increments the column 10 times by 1 each time  *  * should result in a value of 10 but it doesn't, it results in a values of 110  *  * @throws ioexception  */ static void parttwo()throws ioexception { configuration conf = hbaseconfiguration.create(); htablepool pool = new htablepool(conf, integer.max_value); htableinterface table = pool.gettable(bytes.tobytes(tablename)); delete delete = new delete(rowkey); table.delete(delete); //increment columns for (int j = 0; j < 10; j++) { table.incrementcolumnvalue(rowkey, infocf, oldinc, (long)1); increment inc = new increment(rowkey); inc.addcolumn(infocf, newinc, (long)1); table.increment(inc); } get get = new get(rowkey); result r = table.get(get); system.out.println(\"after delete values: new \" + bytes.tolong(r.getvalue(infocf, newinc)) + \" old \" + bytes.tolong(r.getvalue(infocf, oldinc))); } } ",
        "label": 414
    },
    {
        "text": "testreplicationwithcompression fails intermittently in both precommit and trunk builds  testreplicationwithcompression has been failing often.  here are few examples:  https://builds.apache.org/job/precommit-hbase-build/3755/testreport/ https://builds.apache.org/job/hbase-trunk/3672/testreport/org.apache.hadoop.hbase.replication/testreplicationwithcompression/testdeletetypes/  https://builds.apache.org/job/hbase-0.94/677/testreport/junit/org.apache.hadoop.hbase.replication/testreplicationwithcompression/queuefailover/ ",
        "label": 233
    },
    {
        "text": "apply clusterstatus getclusterstatus enumset option  in code base  hbase-15511 enable us to get the cluster status by scope, and after refactoring in hbase-18621. we should apply it in code base so as to prevent the useless information. ",
        "label": 370
    },
    {
        "text": "update 'getting started' for including making  important configurations more visiable   place holder to update doc. ",
        "label": 229
    },
    {
        "text": "fixups to multithreadedtablemapper for hadoop   there are two issues: statusreporter has a new method getprogress() mapper and reducer context objects can no longer be directly instantiated. see attached patch. i'm not thrilled with the added reflection but it was the minimally intrusive change. raised the priority to critical because compilation fails. ",
        "label": 38
    },
    {
        "text": " backport  hbase to branch to avoid forkjoinpool to spawn thousands of threads  ",
        "label": 514
    },
    {
        "text": "reenable testregionmergetransactiononcluster testmergewithreplicas disable by hbase  ",
        "label": 314
    },
    {
        "text": "the initialization order for a fresh cluster is incorrect  the cluster id will set once we become the active master in finishactivemasterinitialization, but the blockuntilbecomingactivemaster and finishactivemasterinitialization are both called in a thread to make the constructor of hmaster return without blocking. and since hmaster itself is also a hregionserver, it will create a connection and then start calling reportforduty. and when creating the connectionimplementation, we will read the cluster id from zk, but the cluster id may have not been set yet since it is set in another thread, we will get an exception and use the default cluster id instead. i always get this when running uts which will start a mini cluster 2018-01-03 15:16:37,916 warn  [m:0;zhangduo-ubuntu:32848] client.connectionimplementation(528): retrieve cluster id failed java.util.concurrent.executionexception: org.apache.zookeeper.keeperexception$nonodeexception: keepererrorcode = nonode for /hbase/hbaseid at java.util.concurrent.completablefuture.reportget(completablefuture.java:357) at java.util.concurrent.completablefuture.get(completablefuture.java:1895) at org.apache.hadoop.hbase.client.connectionimplementation.retrieveclusterid(connectionimplementation.java:526) at org.apache.hadoop.hbase.client.connectionimplementation.<init>(connectionimplementation.java:286) at org.apache.hadoop.hbase.client.connectionutils$shortcircuitingclusterconnection.<init>(connectionutils.java:141) at org.apache.hadoop.hbase.client.connectionutils$shortcircuitingclusterconnection.<init>(connectionutils.java:137) at org.apache.hadoop.hbase.client.connectionutils.createshortcircuitconnection(connectionutils.java:185) at org.apache.hadoop.hbase.regionserver.hregionserver.createclusterconnection(hregionserver.java:781) at org.apache.hadoop.hbase.regionserver.hregionserver.setupclusterconnection(hregionserver.java:812) at org.apache.hadoop.hbase.regionserver.hregionserver.preregistrationinitialization(hregionserver.java:827) at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:938) at org.apache.hadoop.hbase.master.hmaster.run(hmaster.java:550) at java.lang.thread.run(thread.java:748) caused by: org.apache.zookeeper.keeperexception$nonodeexception: keepererrorcode = nonode for /hbase/hbaseid at org.apache.zookeeper.keeperexception.create(keeperexception.java:111) at org.apache.zookeeper.keeperexception.create(keeperexception.java:51) at org.apache.hadoop.hbase.zookeeper.readonlyzkclient$zktask$1.exec(readonlyzkclient.java:163) at org.apache.hadoop.hbase.zookeeper.readonlyzkclient.run(readonlyzkclient.java:311) ... 1 more ",
        "label": 314
    },
    {
        "text": "hbase io index interval doesn't seem to have an effect  interval is rather than the configured  this fact was discovered up on mailing list by sijie cai ",
        "label": 38
    },
    {
        "text": "issues with ui for table compact split operation completion  after split compaction operation using ui  the page is not automatically redirecting back using ie8 firefox   steps: 1. create table with regions.  2. insert some amount of data in such a way that make some hfiles which is lessthan min.compacted files size (say 3 hfiles are there but min compaction files 10)  3. from ui perform compact operation on the table.  \"table action request accepted\" page is displayed  4. operation is failing becoz compaction criteria is not meeting. but from ui compaction requests are continously sending to server. this happens using ie(history.back() seems to resend compact/split request). firefox seems ok in this case.  5. later no automatic redirection to main hamster page occurs. solution: table.jsp in hbase master. the meta tag for html contains refresh with javascript:history.back(). a javascript cannot execute in the meta refresh tag like above in table.jsp and snapshot.jsp <meta http-equiv=\"refresh\" content=\"5,javascript:history.back()\" />  this will fail. w3 school also suggests to use refresh in javascript rather than meta tag. if above meta is replaced as below, the behavior is ok verified for ie8/firefox.  <script type=\"text/javascript\">  <!--  settimeout(\"history.back()\",5000);  -->  </script> hence in table.jsp and snapshot.jsp, it should be modified as above. ",
        "label": 269
    },
    {
        "text": "see if compacting to local fs runs faster than compacting into hdfs  compacting into local fs and then copying the resulting file up into hdfs rather than writing compacted file directly to hdfs may run faster. try it. ",
        "label": 314
    },
    {
        "text": "set zk max connections much higher in  i think by now we can all acknowledge that 0.90 has an issue with zk connections, in that we create too many of them and it's also too easy for our users to shoot themselves in the foot. for 0.90.3, i think we should change the default configuration of 30 that we ship with and set it much much higher, i'm thinking of 32k. ",
        "label": 229
    },
    {
        "text": "hbase script assumes pre hadoop layout of jar files  the following in the bin/hbase: hadoopcppath=$(append_path \"${hadoopcppath}\" `ls ${hadoop_home}/hadoop-core*.jar`) assumes a pre-21 hadoop layout. it'll be better to dynamically account for   either hadoop-core* or hadoop-common*, hadoop-hdfs*, hadoop-mapreduce* ",
        "label": 382
    },
    {
        "text": "utilize multi get to speed up wal file checking in backuplogcleaner  currently backuplogcleaner#getdeletablefiles() issues one get per wal file:       for (filestatus file : files) {         string wal = file.getpath().tostring();         boolean loginsystemtable = table.iswalfiledeletable(wal); this is rather inefficient considering the number of wal files in production can get quite large. we should use multi-get to reduce the number of calls to backup table (which normally resides on another server). ",
        "label": 455
    },
    {
        "text": "scheduled tasks in distributed log splitting not in sync with zk  this is in continuation to hbase-3889: note that there must be more slightly off here. although the splitlogs znode is now empty the master is still stuck here: doing distributed log split in hdfs://localhost:8020/hbase/.logs/10.0.0.65,60020,1305406356765 - waiting for distributed tasks to finish. scheduled=2 done=1 error=0   4380s master startup - splitting logs after master startup   4388s there seems to be an issue with what is in zk and what the taskbatch holds. in my case it could be related to the fact that the task was already in zk after many faulty restarts because of the npe. maybe it was added once (since that is keyed by path, and that is unique on my machine), but the reference count upped twice? now that the real one is done, the done counter has been increased, but will never match the scheduled. the code could also check if zk is actually depleted, and therefore treat the scheduled task as bogus? this of course only treats the symptom, not the root cause of this condition. ",
        "label": 233
    },
    {
        "text": "ageoflastappliedop should update after cluster replication failures  the value of ageoflastappliedop in jmx doesn't update after replication starts failing, and it should. see: http://search-hadoop.com/m/jfpgf1hfnlc ",
        "label": 229
    },
    {
        "text": "add other delete type information into the delete bloom filter to optimize the time range query  to speed up time range scans we need to seek to the maximum timestamp of the requested range,instead of going to the first kv of the (row, column) pair and iterating from there. if we don't know the (row, column), e.g. if it is not specified in the query, we need to go to end of the current row/column pair first, get a kv from there, and do another seek to (row', column', timerange_max) from there. we can only skip over to the timerange_max timestamp when we know that there are no deletecolumn records at the top of that row/column with a higher timestamp. we can utilize another bloom filter keyed on (row, column) to quickly find that out. (from hbase-4962) so the motivation is to save seek ops for scanning time-range queries if we know there is no delete for this row/column. from the implementation perspective, we have already had a delete family bloom filter which contains all the delete family key values. so we can reuse the same bloom filter for all other kinds of delete information such as delete columns or delete. ",
        "label": 6
    },
    {
        "text": "support reseek  at regionscanner  reseek() is not supported currently at the regionscanner level. we can support the same.  this is created following the discussion under hbase-2038 ",
        "label": 544
    },
    {
        "text": "unknown metrics type  since the recent metric commits i see this on the master and rs at boot: 2010-01-16 11:24:59,730 info org.apache.hadoop.metrics.jvm.jvmmetrics: initializing jvm metrics with processname=regionserver, sessionid=regionserver/10.10.1.49:60020 2010-01-16 11:24:59,732 error org.apache.hadoop.metrics.metricsutil: unknown metrics type: org.apache.hadoop.hbase.metrics.metricsrate 2010-01-16 11:24:59,732 error org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.metricstimevaryingrate 2010-01-16 11:24:59,732 error org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.metricstimevaryingrate 2010-01-16 11:24:59,732 error org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.metricstimevaryingrate 2010-01-16 11:24:59,732 error org.apache.hadoop.hbase.metrics: unknown metrics instance: org.apache.hadoop.metrics.util.metricstimevaryingrate we need to clean that for 0.20.3 ",
        "label": 314
    },
    {
        "text": "clean checkout with empty hbase site xml  zk won't start  stack@connelly:~/checkouts/hbase/cleantrunk$ ./bin/start-hbase.sh  localhost: starting zookeeper, logging to /home/stack/checkouts/hbase/cleantrunk/bin/../logs/hbase-stack-zookeeper-connelly.out localhost: java.io.ioexception: could not find my address: connelly in list of zookeeper quorum servers localhost:  at org.apache.hadoop.hbase.zookeeper.hquorumpeer.writemyid(hquorumpeer.java:112) localhost:  at org.apache.hadoop.hbase.zookeeper.hquorumpeer.main(hquorumpeer.java:66) starting master, logging to /home/stack/checkouts/hbase/cleantrunk/bin/../logs/hbase-stack-master-connelly.out localhost: starting regionserver, logging to /home/stack/checkouts/hbase/cleantrunk/bin/../logs/hbase-stack-regionserver-connelly.out whats supposed to happen here in the default case? my machine name is connolly.. i don't have anything in hbase-site.xml ",
        "label": 342
    },
    {
        "text": "add cp hooks after  start close regionoperation  these hooks helps for checking resources(blocking memstore size) and necessary locking on index region while performing batch of mutations. ",
        "label": 543
    },
    {
        "text": " findbugs  synchronization on boxed primitive  this bug is reported by the findbugs tool, which is a static analysis tool. bug: synchronization on integer in org.apache.hadoop.hbase.regionserver.compactions.compactselection.emptyfilelist()  the code synchronizes on a boxed primitive constant, such as an integer. private static integer count = 0; ...   synchronized(count) {      count++;      } ... since integer objects can be cached and shared, this code could be synchronizing on the same object as other, unrelated code, leading to unresponsiveness and possible deadlock see cert con08-j. do not synchronize on objects that may be reused for more information. confidence: normal, rank: troubling (14)  pattern: dl_synchronization_on_boxed_primitive   type: dl, category: mt_correctness (multithreaded correctness) ",
        "label": 67
    },
    {
        "text": "the put object has no simple read methods for checking what has already been added   once data is added to a put object, there is no simple method equivalent to add() to tell what keyvalue objects have already been added to the familymap. it would make sense to add a getter and a boolean method or 2 for ease of use to read/search the familymap. ",
        "label": 388
    },
    {
        "text": "update to maven javadoc plugin and switch to non forking aggregate goals  mjavadoc-444 got into the 3.1.0 release of the maven-javadoc-plugin so now there are versions of the aggregate javadoc goals that don't include a forked build. update our build to make use of this new feature. (a before/after on build time would be nice to know as well) ",
        "label": 402
    },
    {
        "text": "hbase verifyreplication not working when security enabled  when security is enabled, hbase verifyreplication fails for two reasons:  1.mapreduce do not have the auth to read the replication paths \"/hbase/replication/*\" on zk;  2.verifyreplication does not get the token for slave cluster, it's different from hbase-7442, this verifyreplication does not have the output. warn [main] org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation: remoteexception connecting to rs javax.security.sasl.saslexception: digest-md5: digest response format violation. mismatched response.  at org.apache.hadoop.hbase.security.hbasesaslrpcclient.readstatus(hbasesaslrpcclient.java:112)  at org.apache.hadoop.hbase.security.hbasesaslrpcclient.saslconnect(hbasesaslrpcclient.java:174)  at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.setupsaslconnection(secureclient.java:177)  at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.access$500(secureclient.java:85)  at org.apache.hadoop.hbase.ipc.secureclient$secureconnection$2.run(secureclient.java:284)  at org.apache.hadoop.hbase.ipc.secureclient$secureconnection$2.run(secureclient.java:281)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1232)  at sun.reflect.generatedmethodaccessor3.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.util.methods.call(methods.java:37)  at org.apache.hadoop.hbase.security.user.call(user.java:586)  at org.apache.hadoop.hbase.security.user.access$700(user.java:50)  at org.apache.hadoop.hbase.security.user$securehadoopuser.runas(user.java:440)  at org.apache.hadoop.hbase.ipc.secureclient$secureconnection.setupiostreams(secureclient.java:280)  at org.apache.hadoop.hbase.ipc.hbaseclient.getconnection(hbaseclient.java:1150)  at org.apache.hadoop.hbase.ipc.hbaseclient.call(hbaseclient.java:1000)  at org.apache.hadoop.hbase.ipc.securerpcengine$invoker.invoke(securerpcengine.java:164)  at $proxy13.getprotocolversion(unknown source)  at org.apache.hadoop.hbase.ipc.securerpcengine.getproxy(securerpcengine.java:208)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:335)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:312)  at org.apache.hadoop.hbase.ipc.hbaserpc.getproxy(hbaserpc.java:364)  at org.apache.hadoop.hbase.ipc.hbaserpc.waitforproxy(hbaserpc.java:236)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:1313)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:1269)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.gethregionconnection(hconnectionmanager.java:1256)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregioninmeta(hconnectionmanager.java:965)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:860)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregioninmeta(hconnectionmanager.java:962)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:864)  at org.apache.hadoop.hbase.client.hconnectionmanager$hconnectionimplementation.locateregion(hconnectionmanager.java:821)  at org.apache.hadoop.hbase.client.htable.finishsetup(htable.java:234)  at org.apache.hadoop.hbase.client.htable.<init>(htable.java:174)  at org.apache.hadoop.hbase.client.htable.<init>(htable.java:133)  at org.apache.hadoop.hbase.mapreduce.replication.verifyreplication$verifier$1.connect(verifyreplication.java:117)  at org.apache.hadoop.hbase.mapreduce.replication.verifyreplication$verifier$1.connect(verifyreplication.java:110)  at org.apache.hadoop.hbase.client.hconnectionmanager.execute(hconnectionmanager.java:360)  at org.apache.hadoop.hbase.mapreduce.replication.verifyreplication$verifier.map(verifyreplication.java:110)  at org.apache.hadoop.hbase.mapreduce.replication.verifyreplication$verifier.map(verifyreplication.java:74)  at org.apache.hadoop.mapreduce.mapper.run(mapper.java:144)  at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:726)  at org.apache.hadoop.mapred.maptask.run(maptask.java:333)  at org.apache.hadoop.mapred.yarnchild$2.run(yarnchild.java:154)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1232)  at org.apache.hadoop.mapred.yarnchild.main(yarnchild.java:149) ",
        "label": 309
    },
    {
        "text": "warn illegalstateexception  cannot set a region as open if it has not been pending  2009-04-24 18:25:05,029 [regionserver/0:0:0:0:0:0:0:0:60021] warn org.apache.hadoop.hbase.regionserver.hregionserver: processing message (retry: 0) java.io.ioexception: java.io.ioexception: java.lang.illegalstateexception: cannot set a region as open if it has not been pending. state: name=testtable,1833989850,1240597495855, unassigned=true, pendingopen=false, open=false, closing=false, pendingclose=false, closed=false, offlined=false  at org.apache.hadoop.hbase.master.regionmanager$regionstate.setopen(regionmanager.java:1236)  at org.apache.hadoop.hbase.master.regionmanager.setopen(regionmanager.java:805)  at org.apache.hadoop.hbase.master.servermanager.processregionopen(servermanager.java:524)  at org.apache.hadoop.hbase.master.servermanager.processmsgs(servermanager.java:390)  at org.apache.hadoop.hbase.master.servermanager.processregionserverallswell(servermanager.java:361)  at org.apache.hadoop.hbase.master.servermanager.regionserverreport(servermanager.java:269)  at org.apache.hadoop.hbase.master.hmaster.regionserverreport(hmaster.java:601)  at sun.reflect.generatedmethodaccessor4.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:632)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:909)  at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)  at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)  at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)  at java.lang.reflect.constructor.newinstance(constructor.java:513)  at org.apache.hadoop.hbase.remoteexceptionhandler.decoderemoteexception(remoteexceptionhandler.java:94)  at org.apache.hadoop.hbase.remoteexceptionhandler.checkthrowable(remoteexceptionhandler.java:48)  at org.apache.hadoop.hbase.remoteexceptionhandler.checkioexception(remoteexceptionhandler.java:66)  at org.apache.hadoop.hbase.regionserver.hregionserver.run(hregionserver.java:491)  at java.lang.thread.run(thread.java:619) here is the regions' lifecycle: [stack@aa0-000-13 trunk]$ grep -e 'testtable,1833989850,1240597495855' logs/hbase-stack-regionserver-aa0-000-13.u.powerset.com.log                                                                                                                                     2009-04-24 18:24:59,640 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: opening region testtable,1833989850,1240597495855/1257748575                                                                                2009-04-24 18:24:59,666 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: next sequence id for region testtable,1833989850,1240597495855 is 183696121                                                                 2009-04-24 18:24:59,666 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: region testtable,1833989850,1240597495855/1257748575 available                                                                               2009-04-24 18:24:59,666 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: closing testtable,1833989850,1240597495855: compactions & flushes disabled                                                                  2009-04-24 18:24:59,666 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region, no outstanding scanners on testtable,1833989850,1240597495855                                                  2009-04-24 18:24:59,667 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: no more row locks outstanding on region testtable,1833989850,1240597495855                                                                  2009-04-24 18:24:59,667 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: closed testtable,1833989850,1240597495855                                                                                                    2009-04-24 18:24:59,672 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.compactsplitthread: region split, meta updated, and report to master all successful. old region=region => {name => 'testtable,1832962724,124059744833 1', startkey => '1832962724', endkey => '', encoded => 1889052331, offline => true, split => true, table => {{name => 'testtable', is_root => 'false', is_meta => 'false', families => [{name => 'info', bloomfilter => 'false', compression => 'none', versions => '3 ', length => '2147483647', ttl => '-1', blocksize => '65536', in_memory => 'false', blockcache => 'false'}], indexes => []}}, new regions: testtable,1832962724,1240597495855, testtable,1833989850,1240597495855. split took 3sec                                     2009-04-24 18:25:02,011 [regionserver/0:0:0:0:0:0:0:0:60021] info org.apache.hadoop.hbase.regionserver.hregionserver: msg_region_open: testtable,1833989850,1240597495855                                                                                              2009-04-24 18:25:02,011 [regionserver/0:0:0:0:0:0:0:0:60021.worker] info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: testtable,1833989850,1240597495855                                                                               2009-04-24 18:25:02,013 [regionserver/0:0:0:0:0:0:0:0:60021.worker] debug org.apache.hadoop.hbase.regionserver.hregion: opening region testtable,1833989850,1240597495855/1257748575                                                                                   2009-04-24 18:25:02,037 [regionserver/0:0:0:0:0:0:0:0:60021.worker] debug org.apache.hadoop.hbase.regionserver.hregion: next sequence id for region testtable,1833989850,1240597495855 is 183696121                                                                    2009-04-24 18:25:02,037 [regionserver/0:0:0:0:0:0:0:0:60021.worker] info org.apache.hadoop.hbase.regionserver.hregion: region testtable,1833989850,1240597495855/1257748575 available                                                                                  2009-04-24 18:25:02,037 [regionserver/0:0:0:0:0:0:0:0:60021.worker] debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region testtable,1833989850,1240597495855/1257748575 because: region open check                            2009-04-24 18:25:02,038 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: starting  compaction on region testtable,1833989850,1240597495855                                                                            java.io.ioexception: java.io.ioexception: java.lang.illegalstateexception: cannot set a region as open if it has not been pending. state: name=testtable,1833989850,1240597495855, unassigned=true, pendingopen=false, open=false, closing=false, pendingclose=false,  closed=false, offlined=false                                                                                                                                                                                                                                           2009-04-24 18:25:05,042 [regionserver/0:0:0:0:0:0:0:0:60021] info org.apache.hadoop.hbase.regionserver.hregionserver: msg_region_open: testtable,1833989850,1240597495855                                                                                              2009-04-24 18:25:05,043 [regionserver/0:0:0:0:0:0:0:0:60021.worker] info org.apache.hadoop.hbase.regionserver.hregionserver: worker: msg_region_open: testtable,1833989850,1240597495855                                                                               2009-04-24 18:25:07,628 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region testtable,1833989850,1240597495855 in 5sec                                                                    2009-04-24 18:25:18,196 [ipc server handler 1 on 60021] debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,1833989850,1240597495855                                                                                                      2009-04-24 18:25:18,196 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region testtable,1833989850,1240597495855. current region memcache size 64.0m                                 2009-04-24 18:25:24,320 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.store: added hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/testtable/1257748575/info/669389692274037086, entries=58120, sequenceid=183754265,  memsize=64.0m, filesize=57.6m to testtable,1833989850,1240597495855                                                                                                                                                                                                   2009-04-24 18:25:24,727 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.hregion: finished memcache flush of ~64.0m for region testtable,1833989850,1240597495855 in 6531ms, sequence id=183754265, compaction requested=f alse                                                                                                                                                                                                                                                                   2009-04-24 18:25:27,299 [ipc server handler 0 on 60021] debug org.apache.hadoop.hbase.regionserver.hregion: flush requested on testtable,1833989850,1240597495855                                                                                                      2009-04-24 18:25:27,299 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region testtable,1833989850,1240597495855. current region memcache size 64.0m                                 2009-04-24 18:25:32,722 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.store: added hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/testtable/1257748575/info/4755763594728033629, entries=58111, sequenceid=183812377 , memsize=64.0m, filesize=57.6m to testtable,1833989850,1240597495855                                                                                                                                                                                                  2009-04-24 18:25:32,729 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.hregion: finished memcache flush of ~64.0m for region testtable,1833989850,1240597495855 in 5430ms, sequence id=183812377, compaction requested=t rue                                                                                                                                                                                                                                                                    2009-04-24 18:25:32,729 [regionserver/0:0:0:0:0:0:0:0:60021.cacheflusher] debug org.apache.hadoop.hbase.regionserver.compactsplitthread: compaction requested for region testtable,1833989850,1240597495855/1257748575 because: regionserver/0:0:0:0:0:0:0:0:60021.cac heflusher                                                                                                                                                                                                                                                              2009-04-24 18:25:32,730 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: starting  compaction on region testtable,1833989850,1240597495855                                                                            2009-04-24 18:25:42,843 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: compaction completed on region testtable,1833989850,1240597495855 in 10sec                                                                   2009-04-24 18:25:42,846 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: starting split of region testtable,1833989850,1240597495855                                                                                  2009-04-24 18:25:42,849 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: closing testtable,1833989850,1240597495855: compactions & flushes disabled                                                                  2009-04-24 18:25:42,874 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: updates disabled for region, no outstanding scanners on testtable,1833989850,1240597495855                                                  2009-04-24 18:25:42,874 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: no more row locks outstanding on region testtable,1833989850,1240597495855                                                                  2009-04-24 18:25:42,874 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: started memcache flush for region testtable,1833989850,1240597495855. current region memcache size 58.8m                                    2009-04-24 18:25:45,045 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.store: added hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/testtable/1257748575/info/3148860415324952525, entries=53415, sequenceid=183865794, m emsize=58.8m, filesize=52.9m to testtable,1833989850,1240597495855                                                                                                                                                                                                     2009-04-24 18:25:45,046 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] debug org.apache.hadoop.hbase.regionserver.hregion: finished memcache flush of ~58.8m for region testtable,1833989850,1240597495855 in 2172ms, sequence id=183865794, compaction requested=true 2009-04-24 18:25:45,046 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.hregion: closed testtable,1833989850,1240597495855                                                                                                    2009-04-24 18:25:45,369 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] info org.apache.hadoop.hbase.regionserver.compactsplitthread: region split, meta updated, and report to master all successful. old region=region => {name => 'testtable,1833989850,124059749585 5', startkey => '1833989850', endkey => '', encoded => 1257748575, offline => true, split => true, table => {{name => 'testtable', is_root => 'false', is_meta => 'false', families => [{name => 'info', bloomfilter => 'false', compression => 'none', versions => '3 ', length => '2147483647', ttl => '-1', blocksize => '65536', in_memory => 'false', blockcache => 'false'}], indexes => []}}, new regions: testtable,1833989850,1240597542848, testtable,1835061689,1240597542848. split took 2sec  odd, is that it seems to go on in spite of ise... so just a warning? why an open w/o first being pending? ",
        "label": 314
    },
    {
        "text": "fix local master backup sh   parameter order wrong  i think this is a regression from hbase-3895 (if so, my bad!). the local-master-backup.sh needs the parameters adjusted to work. $ bin/local-master-backup.sh start 1           starting master, logging to /var/lib/hbase/logs/hbase-larsgeorge-1-master-de1-app-mbp-2.out usage: master [opts] start|stop  start  start master. if local mode, start master and regionserver in same jvm  stop   start cluster shutdown; master signals regionserver shutdown  where [opts] are:    --minservers=<servers>    minimum regionservers needed to host user tables.    --backup                  master should start in backup mode the log has 2011-06-25 10:26:34,864 error org.apache.hadoop.hbase.master.hmastercommandline: could not parse:  org.apache.commons.cli.unrecognizedoptionexception: unrecognized option: -d         at org.apache.commons.cli.parser.processoption(parser.java:363)         at org.apache.commons.cli.parser.parse(parser.java:199)         at org.apache.commons.cli.parser.parse(parser.java:85)         at org.apache.hadoop.hbase.master.hmastercommandline.run(hmastercommandline.java:73)         at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65)         at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:76)         at org.apache.hadoop.hbase.master.hmaster.main(hmaster.java:1346) fix: index: bin/local-master-backup.sh =================================================================== --- bin/local-master-backup.sh  (revision 1139502) +++ bin/local-master-backup.sh  (working copy) @@ -21,9 +21,9 @@    dn=$2    export hbase_ident_string=\"$user-$dn\"    hbase_master_args=\"\\ -    --backup \\      -d hbase.master.port=`expr 60000 + $dn` \\ -    -d hbase.master.info.port=`expr 60010 + $dn`\" +    -d hbase.master.info.port=`expr 60010 + $dn` \\ +    --backup\"    \"$bin\"/hbase-daemon.sh $1 master $hbase_master_args  } ",
        "label": 285
    },
    {
        "text": "master walprocs still never clean up  bin/hdfs dfs -ls /hbase/masterprocwals | wc -l 218631 ",
        "label": 309
    },
    {
        "text": "set branch version to alpha4 from alpha4 snapshot  ",
        "label": 314
    },
    {
        "text": "master should support close open reassignment enable disable operations on individual regions  the master should support close/open/reassignment/enable/disable operations on individual regions by way of a client api and corresponding shell command(s), and maybe also controls on the master ui. if one region out of 1000s is closed or offline, for example due to transient dfs problems, currently the whole table must be disabled, then reenabled to trigger reassignment and, hopefully, successful (re)open of the offline regions. the flurry of compactions this causes can exacerbate the underlying problem and actually make cluster state worse, as more dfs errors accumulate. ",
        "label": 314
    },
    {
        "text": "totalcompactingkvs may overflow  i happened to get a very large region (mistakely bulk loading tons of hfile into a signle region). when it's getting compacted, the webui shows a overflow totalcompactingkvs. i found this is due to compactor#filedetails#maxkeycount is int32. it is not a big deal that this variable is only used for displaying compaction progress and everywhere else uses long. totalcompactingkvs=1909276739, currentcompactedkvs=11308733425, ",
        "label": 92
    },
    {
        "text": "servlets generate xss request parameter to servlet writer findbugs warnings  in our jmxjsonservlet we are doing this:  jsonpcb = request.getparameter(callback_param);  if (jsonpcb != null) {  response.setcontenttype(\"application/javascript; charset=utf8\");  writer.write(jsonpcb + \"(\"); ... findbugs complains rightly. there are other instances in our servlets and then there are the pages generated by jamon excluded from findbugs checking (and findbugs volunteers that it is dumb in this regard finding only the most egregious of violations). we have no sanitizing tooling in hbase that i know of (correct me if i am wrong). i started to pull on this thread and it runs deep. our jamon templating (last updated in 2013 and before that, in 2011) engine doesn't seem to have sanitizing means either and there seems to be outstanding xss complaint against jamon that goes unaddressed. could pull in something like https://www.owasp.org/index.php/owasp_java_encoder_project and run all emissions via it or get a templating engine that has sanitizing built in. ",
        "label": 393
    },
    {
        "text": "include deletes in the scan  setraw  method does not respect the time range or the filter  if a row has been deleted at time stamp 't' and a scan with time range (0, t-1) is executed, it still returns the delete marker at time stamp 't'. it is because of the code in scanquerymatcher.java       if (retaindeletesinoutput           || (!isuserscan && (environmentedgemanager.currenttimemillis() - timestamp) <= timetopurgedeletes)           || kv.getmemstorets() > maxreadpointtotrackversions) {         // always include or it is not time yet to check whether it is ok         // to purge deltes or not         return matchcode.include;       } the assumption is scan (even with setraw is set to true) should respect the filters and the time range specified. please let me know if you think this behavior can be changed so that i can provide a patch for it. ",
        "label": 286
    },
    {
        "text": "testreplicationthrottler testthrottling failed on virtual boxes  during test runs testreplicationthrottler.testthrottling sometimes fails with assertion  testthrottling(org.apache.hadoop.hbase.replication.regionserver.testreplicationthrottler) time elapsed: 0.229 sec <<< failure!  java.lang.assertionerror: null  at org.junit.assert.fail(assert.java:86)  at org.junit.assert.asserttrue(assert.java:41)  at org.junit.assert.asserttrue(assert.java:52)  at org.apache.hadoop.hbase.replication.regionserver.testreplicationthrottler.testthrottling(testreplicationthrottler.java:69) ",
        "label": 426
    },
    {
        "text": "importtsv bulk output does not support tags with hfile format version  running the following command: hbase hbase org.apache.hadoop.hbase.mapreduce.importtsv \\  -dhfile.format.version=3 \\  -dmapreduce.map.combine.minspills=1 \\  -dimporttsv.separator=, \\  -dimporttsv.skip.bad.lines=false \\  -dimporttsv.columns=\"hbase_row_key,cf1:a,hbase_cell_ttl\" \\  -dimporttsv.bulk.output=/tmp/testttl/output/1 \\  testttl \\  /tmp/testttl/input  the content of input is like: row1,data1,00000060  row2,data2,00000660  row3,data3,00000060  row4,data4,00000660 when running hfile tool with the output hfile, there is no ttl tag. ",
        "label": 205
    },
    {
        "text": "hlog entry implements writable  change to pb  can we do this in a way that makes it so even after 0.96, we can read old wals? ",
        "label": 98
    },
    {
        "text": "tablemapreduceutil is inconsistent with other table related classes that accept byte  as a table name  minor gripe but we define our entire schema as a set of byte[] constants for tables and cfs. this works well with htable and htablepool but tablemapreduceutil requires conversion to a string, most table-related classes do not. ",
        "label": 162
    },
    {
        "text": "revert to older version of jetty  as discussed in hbase-19256 we will have to temporarily revert to jetty 9.3 due existing issues with 9.4 and hadoop3. once hbase-19256 is resolved we can revert to 9.4. ",
        "label": 320
    },
    {
        "text": "script to patch holes in  meta  table  i need a script to patch holes in the .meta. table, which was corrupted by earlier issue on the cluster. ",
        "label": 248
    },
    {
        "text": "thread contention over row locks set monitor  hregion maintains a set of row locks. whenever any thread attempts to lock or release a row it needs to acquire the monitor on that set. we've been encountering cases with 30 handler threads all contending for that monitor, blocked progress on the region server. clients timeout, and retry making it worse, and the region server stops responding to new clients almost entirely. ",
        "label": 125
    },
    {
        "text": "maven archetype  client application with shaded jars  add new archetype for generation of hbase-shaded-client dependent project. ",
        "label": 123
    },
    {
        "text": "audit log messages do not include column family   qualifier information consistently  the code related to this issue is in accesscontroller.java:permissiongranted(). when creating audit logs, that method will do one of the following: grant access, create audit log with table name only deny access because of table permission, create audit log with table name only deny access because of column family / qualifier permission, create audit log with specific family / qualifier so, in the case where more than one column family and/or qualifier are in the same request, there will be a loss of information. even in the case where only one column family and/or qualifier is involved, information may be lost. it would be better if this behavior consistently included all the information in the request; regardless of access being granted or denied, and regardless which permission caused the denial, the column family and qualifier info should be part of the audit log message. ",
        "label": 309
    },
    {
        "text": "testtablemapreduce broken up on hudson  so, test has failed for various reasons since fixed over last week or so. it is currently failing when the reducer starts up. its failing because its not picking up the zk servers location; its using stale config. ",
        "label": 314
    },
    {
        "text": "gethaseintegrationtestingutility  is misspelled  the function gethaseintegrationtestingutility() in chaosmonkey.java should be gethbaseintegrationtestingutility(), just a spelling mistake.   /**                                                                                                                                  * context for action's                                                                                                              */   public static class actioncontext {     private integrationtestingutility util;     public actioncontext(integrationtestingutility util) {       this.util = util;     }     public integrationtestingutility gethaseintegrationtestingutility() {       return util;     }     public hbasecluster gethbasecluster() {       return util.gethbaseclusterinterface();     }   } ",
        "label": 520
    },
    {
        "text": "recursive loop on keeperexception in authenticationtokensecretmanager zkleadermanager  looking through stack traces for testmasterfailover, i see a case where the leader authenticationtokensecretmanager can get into a recursive loop when a keeperexception is encountered: thread-1-eventthread\" daemon prio=10 tid=0x00007f9fb47b2800 nid=0x77f6 waiting on condition [0x00007f9fab376000]    java.lang.thread.state: timed_waiting (sleeping)         at java.lang.thread.sleep(native method)         at java.lang.thread.sleep(thread.java:302)         at java.util.concurrent.timeunit.sleep(timeunit.java:328)         at org.apache.hadoop.hbase.util.retrycounter.sleepuntilnextretry(retrycounter.java:55)         at org.apache.hadoop.hbase.zookeeper.recoverablezookeeper.exists(recoverablezookeeper.java:206)         at org.apache.hadoop.hbase.zookeeper.zkutil.createandfailsilent(zkutil.java:891)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.createbaseznodes(zookeeperwatcher.java:161)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.<init>(zookeeperwatcher.java:154)         at org.apache.hadoop.hbase.master.hmaster.tryrecoveringexpiredzksession(hmaster.java:1397)         at org.apache.hadoop.hbase.master.hmaster.abortnow(hmaster.java:1435)         at org.apache.hadoop.hbase.master.hmaster.abort(hmaster.java:1374)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.abort(zookeeperwatcher.java:450)         at org.apache.hadoop.hbase.zookeeper.zkleadermanager.stepdownasleader(zkleadermanager.java:166)         at org.apache.hadoop.hbase.security.token.authenticationtokensecretmanager$leaderelector.stop(authenticationtokensecretmanager.java:293)         at org.apache.hadoop.hbase.zookeeper.zkleadermanager.stepdownasleader(zkleadermanager.java:167)         at org.apache.hadoop.hbase.security.token.authenticationtokensecretmanager$leaderelector.stop(authenticationtokensecretmanager.java:293)         at org.apache.hadoop.hbase.zookeeper.zkleadermanager.stepdownasleader(zkleadermanager.java:167)         at org.apache.hadoop.hbase.security.token.authenticationtokensecretmanager$leaderelector.stop(authenticationtokensecretmanager.java:293)         at org.apache.hadoop.hbase.zookeeper.zkleadermanager.handleleaderchange(zkleadermanager.java:96)         at org.apache.hadoop.hbase.zookeeper.zkleadermanager.nodedeleted(zkleadermanager.java:78)         at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.process(zookeeperwatcher.java:286)         at org.apache.zookeeper.clientcnxn$eventthread.processevent(clientcnxn.java:521)         at org.apache.zookeeper.clientcnxn$eventthread.run(clientcnxn.java:497) the keeperexception causes zkleadermanager to call authenticationtokensecretmanager$leaderelector.stop(), which calls zkleadermanager.stepdownasleader(), which will encounter another keeperexception, and so on... ",
        "label": 180
    },
    {
        "text": "configurable bucket sizes in bucketcache  in the current implementation of bucketcache, the sizes of buckets created for different blocksizes is fixed. given that the all the blocksizes will not be used, it would be good to have the bucketcache configurable. the block sizes for which buckets need to be created and the size of the buckets for each block size should be configurable. ",
        "label": 339
    },
    {
        "text": "re enable testinstantschemachangefailover testinstantschemaoperationsinzkformasterfailover  testinstantschemachangefailover#testinstantschemaoperationsinzkformasterfailover was disabled for instant schema change (hbase-4213) after it failed on jenkins. we should enable it and make it pass on jenkins and dev enviroments. ",
        "label": 428
    },
    {
        "text": "unresponsive region server  potential deadlock  we have a 15-node (14rs+1master) hbase cluster. we just recently upgraded from 0.20.3 to 0.20.4. this cluster does have colocated hadoop mr, but we mostly use another mr cluster to hit it. upon start, the cluster runs the jobs fine for about an hour. afterwards, an rs seems to have locked up. doing a get for a row in region being served by that region server hangs (cannot even ctrl+c out of the hbase shell). attached is the thread dump. verified in ui that the affect server runs on 0.20.4 and not 0.20.3. ",
        "label": 453
    },
    {
        "text": "remove the concept of table owner  the table owner concept was a design simplification in the initial drop. first, the design changes under review means only a user with global create permission can create a table, which will probably be an administrator. then, granting implicit permissions may lead to oversights and it adds unnecessary conditionals to our code. so instead the administrator with global create permission should make the appropriate grants at table create time. ",
        "label": 287
    },
    {
        "text": "add mutaterow method support to thrift2  examples py: tput put = tput()  m1 = tmutation(put=put)  tdelete single_delete = tdelete()  m2 = tmutation(singledelete=single_delete)  row_mu = trowmutations(row,[m1,m2])  thbase_service.mutaterow(table,row_mu) # atomically on a single row ",
        "label": 193
    },
    {
        "text": "close scanners when at end in thrift  hbase-6073 adds the following to the overall patch: -      return resultsfromhbase(scanner.next(numrows)); +      list<tresult> results = resultsfromhbase(scanner.next(numrows)); +      if(results.size() < numrows) { +        removescanner(scannerid); +      } +      return results; we need to see if we have to add this to thrift2 as separate patch. ",
        "label": 285
    },
    {
        "text": " findbugs  rv  negating the result of compareto compare   finbugs warns that catalogjanitor.java, namespaceupgrade.java and fstabledescriptors.java have the following warning: rv: negating the result of compareto()/compare() (rv_negating_result_of_compareto) this code negatives the return value of a compareto or compare method. this is a questionable or bad programming practice, since if the return value is integer.min_value, negating the return value won't negate the sign of the result. you can achieve the same intended result by reversing the order of the operands rather than by negating the results. ",
        "label": 320
    },
    {
        "text": "todo handle stuck in transition  rit opening  location ve0538   a few of us trying migration from hbase1 to hbase2 have run into various assignment issues. in my case, i see that master coming online, reading hbase:meta, finding an issue in opening state and then just not assigning. my thought is that its expecting a callback to come in. eventually log is fills with this sort of stuff: 2017-11-02 15:02:54,237 warn [procexectimeout] assignment.assignmentmanager: todo handle stuck in transition: rit=opening, location=ve0528.halxg.cloudera.com,16020,1509657692581, table=integrationtestbiglinkedlist, region=919cc6636ffbed17f628f335d3a58726 let me get a cleaner run. this current log is polluted by a few restarts. ",
        "label": 314
    },
    {
        "text": "hbase jar exists in hbase root and in 'lib '  the official hbase 0.90.2 release contains a hbase-0.90.2.jar in'<hbase root>' and in '<hbase-root>/lib/'. ",
        "label": 314
    },
    {
        "text": "disable builds on eol'd branch  undo nightly build  i think all i have to do is remove the dev-support/jenkinsfile (iirc). ",
        "label": 314
    },
    {
        "text": "clean out load table rb and make sure all roads lead to completebulkload tool  up on list vidhya tried using load_table.rb with 0.90 and new master and it don't work any more now we assign differently. clean out this script. make sure all doc points at completebulkload tool instead. ",
        "label": 467
    },
    {
        "text": "documentation is lacking information about rolling restart sh script   current documentation is talking about graceful_stop.sh and how to do a rolling restart but is not talking about the rolling-restart.sh script. we need to document that. ",
        "label": 330
    },
    {
        "text": "region load balancing by table does not handle the case where a table's region count is lower than the number of the rs in the cluster  when the table's region count is less than the count of region servers, the region balance algorithm will not move the region. for example, the cluster has 100 rs, the table has 50 regions sitting on one rs, they will not be moved to any of the other 99 rs. this is because the algorithm did not calculate the under-loaded rs correctly. this is how the algorithm works with the above example: avg-regions-per-rs=0.5  min-rs-per-rs=0  max-rs-per-rs=1 when they calculate the under loaded rs, the code is as below. since regioncount=0, which is always >=min, so it will always skip, therefore, no underloaded rs are found. map<servername, integer> underloadedservers = new hashmap<servername, integer>();  for (map.entry<serverandload, list<hregioninfo>> server:  serversbyload.entryset()) {  int regioncount = server.getkey().getload();  if (regioncount >= min) { break; } underloadedservers.put(server.getkey().getservername(), min - regioncount);  } later the function returns since underloaded rs size is 0 if (serverunerloaded ==0) return regionstoreturn; ",
        "label": 441
    },
    {
        "text": "send sigstop to hang or sigcont to resume rs and add graceful rolling restart  add a chaos monkey action that uses sigstop and sigcont to hang and resume a ratio of region servers. add a chaos monkey action to simulate a rolling restart including graceful_stop like functionality that unloads the regions from the server before a restart and then places it under load again afterwards. add these actions to the relevant monkeys ",
        "label": 433
    },
    {
        "text": "add client support for atomic checkandmutate  currently hbase has support for atomic checkandput as well as checkanddelete operations on individual rows.   hbase also supports the atomic submission of multiple operations through mutaterow.   it would be nice to support atomic checkandmutate(rowmutations) as well. ",
        "label": 423
    },
    {
        "text": "tablemapreduceutil overwrites user supplied options for multiple tables scaners job  in tablemapreduceutil#inittablemapperjob, we have hbaseconfiguration.addhbaseresources(job.getconfiguration()); it should use merge instead. otherwise, user supplied options are overwritten. ",
        "label": 242
    },
    {
        "text": "should have htable deleteall string row  and htable deleteall text row   we have this:  deleteall(final byte [] row) deleteall(final byte [] row, final long ts)  deleteall(final string row, final long ts)  deleteall(final text row, final long ts) deleteall(final byte [] row, final byte [] column)  deleteall(final text row, final text column)  deleteall(final string row, final string column) deleteall(final byte [] row, final byte [] column, final long ts)  deleteall(final text row, final text column, final long ts)  deleteall(final string row, final string column, final long ts) so to be logical deleteall(final text row) and deleteall(final string row) are missing. ",
        "label": 229
    },
    {
        "text": "meta row with missing hri breaks ui  seeing this in the master log: 2009-07-26 06:01:55,373 warn org.apache.hadoop.hbase.master.basescanner: found 1 rows with empty hregioninfo while scanning meta region .meta.,,1  2009-07-26 06:01:55,373 warn org.apache.hadoop.hbase.master.hmaster: removed region: content,9e1f3dbffe50886f56d67044ca7177e,1248262776854 from meta region: .meta.,,1 because hregioninfo was empty seeing this on the master ui (http://test:60010/master.jsp): http error: 500 internal_server_error requesturi=/master.jsp  caused by: java.lang.nullpointerexception  at org.apache.hadoop.hbase.util.writables.getwritable(writables.java:74)  at org.apache.hadoop.hbase.util.writables.gethregioninfo(writables.java:118)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers$1.processrow(hconnectionmanager.java:356)  at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:63)  at org.apache.hadoop.hbase.client.metascanner.metascan(metascanner.java:28)  at org.apache.hadoop.hbase.client.hconnectionmanager$tableservers.listtables(hconnectionmanager.java:369)  at org.apache.hadoop.hbase.client.hbaseadmin.listtables(hbaseadmin.java:127)  at org.apache.hadoop.hbase.generated.master.master_jsp._jspservice(master_jsp.java:125)  at org.apache.jasper.runtime.httpjspbase.service(httpjspbase.java:97)  at javax.servlet.http.httpservlet.service(httpservlet.java:820)  at org.mortbay.jetty.servlet.servletholder.handle(servletholder.java:502)  at org.mortbay.jetty.servlet.servlethandler.handle(servlethandler.java:363)  at org.mortbay.jetty.security.securityhandler.handle(securityhandler.java:216)  at org.mortbay.jetty.servlet.sessionhandler.handle(sessionhandler.java:181)  at org.mortbay.jetty.handler.contexthandler.handle(contexthandler.java:766)  at org.mortbay.jetty.webapp.webappcontext.handle(webappcontext.java:417)  at org.mortbay.jetty.handler.contexthandlercollection.handle(contexthandlercollection.java:230)  at org.mortbay.jetty.handler.handlerwrapper.handle(handlerwrapper.java:152)  at org.mortbay.jetty.server.handle(server.java:324)  at org.mortbay.jetty.httpconnection.handlerequest(httpconnection.java:534)  at org.mortbay.jetty.httpconnection$requesthandler.headercomplete(httpconnection.java:864)  at org.mortbay.jetty.httpparser.parsenext(httpparser.java:533)  at org.mortbay.jetty.httpparser.parseavailable(httpparser.java:207)  at org.mortbay.jetty.httpconnection.handle(httpconnection.java:403)  at org.mortbay.io.nio.selectchannelendpoint.run(selectchannelendpoint.java:409)  at org.mortbay.thread.queuedthreadpool$poolthread.run(queuedthreadpool.java:522) ",
        "label": 38
    },
    {
        "text": "calltimeoutexception and callqueuetoobigexception should trigger pffe  if a region server is backed up enough that lots of calls are timing out then we should think about treating a server as failing. ",
        "label": 323
    },
    {
        "text": "improve replication metrics  we are trying to report on replication lag and find that there is no good single metric to do that.  ageoflastshippedop is close, but unfortunately it is increased even when there is nothing to ship on a particular regionserver. i would like discuss a few options here:  add a new metric: replicationqueuetime (or something) with the above meaning. i.e. if we have something to ship we set the age of that last shipped edit, if we fail we increment that last time (just like we do now). but if there is nothing to replicate we set it to current time (and hence that metric is reported to close to 0).  alternatively we could change the meaning of ageoflastshippedop to mean to do that. that might lead to surprises, but the current behavior is clearly weird when there is nothing to replicate. comments? jean-daniel cryans, michael stack. if approach sounds good, i'll make a patch for all branches. edit: also adds a new shippedkbs metric to track the amount of data that is shipped via replication. ",
        "label": 286
    },
    {
        "text": "hadoop hbase security visibility testvisibilitylabelswithdeletes fails  016-04-18 15:02:50,632 warn [member: '10.22.11.177,55156,1461016964801' subprocedure-pool5-thread-1] flush.regionserverflushtableproceduremanager$flushtablesubprocedurepool(275): got exception in flushsubprocedurepool  java.util.concurrent.executionexception: java.lang.illegalargumentexception: timestamp cannot be negative. minstamp:-9223372036854775808, maxstamp:128  at java.util.concurrent.futuretask.report(futuretask.java:122)  at java.util.concurrent.futuretask.get(futuretask.java:188)  at org.apache.hadoop.hbase.procedure.flush.regionserverflushtableproceduremanager$flushtablesubprocedurepool.waitforoutstandingtasks(regionserverflushtableproceduremanager.java:251)  at org.apache.hadoop.hbase.procedure.flush.flushtablesubprocedure.flushregions(flushtablesubprocedure.java:102)  at org.apache.hadoop.hbase.procedure.flush.flushtablesubprocedure.acquirebarrier(flushtablesubprocedure.java:113)  at org.apache.hadoop.hbase.procedure.subprocedure.call(subprocedure.java:166)  at org.apache.hadoop.hbase.procedure.subprocedure.call(subprocedure.java:1)  at java.util.concurrent.futuretask.run(futuretask.java:262)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:745)  caused by: java.lang.illegalargumentexception: timestamp cannot be negative. minstamp:-9223372036854775808, maxstamp:128  at org.apache.hadoop.hbase.io.timerange.<init>(timerange.java:81)  at org.apache.hadoop.hbase.regionserver.timerangetracker.totimerange(timerangetracker.java:204)  at org.apache.hadoop.hbase.regionserver.immutablesegment.<init>(immutablesegment.java:44)  at org.apache.hadoop.hbase.regionserver.segmentfactory.createimmutablesegment(segmentfactory.java:57)  at org.apache.hadoop.hbase.regionserver.defaultmemstore.snapshot(defaultmemstore.java:93)  at org.apache.hadoop.hbase.regionserver.hstore$storeflusherimpl.prepare(hstore.java:2151)  at org.apache.hadoop.hbase.regionserver.hregion.internalprepareflushcache(hregion.java:2324)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:2192)  at org.apache.hadoop.hbase.regionserver.hregion.internalflushcache(hregion.java:2163)  at org.apache.hadoop.hbase.regionserver.hregion.flushcache(hregion.java:2054)  at org.apache.hadoop.hbase.regionserver.hregion.flush(hregion.java:1980)  at org.apache.hadoop.hbase.procedure.flush.flushtablesubprocedure$regionflushtask.call(flushtablesubprocedure.java:68)  at org.apache.hadoop.hbase.procedure.flush.flushtablesubprocedure$regionflushtask.call(flushtablesubprocedure.java:1)  at java.util.concurrent.futuretask.run(futuretask.java:262)  at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ",
        "label": 46
    },
    {
        "text": "performance issue  don't use bufferedmutator for htable's put method  copied the test result from hbase-17994.  run start-hbase.sh in my local computer and use the default config to test with pe tool. ./bin/hbase org.apache.hadoop.hbase.performanceevaluation --rows=100000 --nomapred --autoflush=true randomwrite 1 ./bin/hbase org.apache.hadoop.hbase.performanceevaluation --rows=100000 --nomapred --autoflush=true asyncrandomwrite 1 mean latency test result.    test1   test2   test3   test4   test5 randomwrite   164.39   161.22   164.78   140.61   151.69 asyncrandomwrite   122.29   125.58   122.23   113.18   123.02 50th latency test result.    test1   test2   test3   test4   test5 randomwrite   130.00   125.00   123.00   112.00   121.00 asyncrandomwrite   95.00   97.00   95.00   88.00   95.00 99th latency test result.    test1   test2   test3   test4   test5 randomwrite   600.00   600.00   650.00   404.00   425.00 asyncrandomwrite   339.00   327.00   297.00   311.00   318.00 in our internal 0.98 branch, the pe test result shows the async write has the almost same latency with the blocking write. but for master branch, the result shows the async write has better latency than the blocking client. take a look about the code, i thought the difference is the bufferedmutator. for master branch, htable don't have a write buffer and all write request will be flushed directly. and user can use bufferedmutator when user want to perform client-side buffering of writes. for the performance issue (autoflush=true), i thought we can use rpc caller directly in htable's put method. thanks. review: https://reviews.apache.org/r/61454/ copy the comments from chia-ping tsai. remove the bufferdmutator brings four benefits.  1. correct the metrics (see hbase-18476)  2. make htable thread-safe (see hbase-17368)  3. reduce the latency  4. get rid of some deprecated methods in table ",
        "label": 187
    },
    {
        "text": "convert multirowmutationprotocol to protocol buffer service  with coprocessor endpoints now exposed as protobuf defined services, we should convert over all of our built-in endpoints to pb services. ",
        "label": 139
    },
    {
        "text": "testaccesscontroller testglobalauthorizationfornewregisteredrs  failed here https://builds.apache.org/job/hbase-0.95/323/testreport/org.apache.hadoop.hbase.security.access/testaccesscontroller/testglobalauthorizationfornewregisteredrs/ looks like it is not waiting long enough for single region to come online. at a minimum wait longer so can see more why it is not coming on line. ",
        "label": 314
    },
    {
        "text": " hbck  handle no version file and should not be deployed inconsistencies  the hbck tool can not fix the six scenarios.  1. version file does not exist in root dir.  fix: i try to create a version file by 'fsutils.setversion' method. 2. [regionname][key] on hdfs, but not listed in meta or deployed on any region server.  fix: i get region info form the hdfs file, this region info write to '.meta.' table. 3. [regionname][key] not in meta, but deployed on [servername]  fix: i get region info form the hdfs file, this region info write to '.meta.' table. 4. [regionname] should not be deployed according to meta, but is deployed on [servername]  fix: close this region. 5. first region should start with an empty key. you need to create a new region and regioninfo in hdfs to plug the hole.  fix: the region info is not in hdfs and .meta., so it create a empty region for this error. 6. there is a hole in the region chain between [key] and [key]. you need to create a new regioninfo and region dir in hdfs to plug the hole.  fix: the region info is not in hdfs and .meta., so it create a empty region for this hole. ",
        "label": 528
    },
    {
        "text": "add a good readme on the build   set up a good readme for the build and what's needed to get this running. ",
        "label": 154
    },
    {
        "text": "show catalogjanitor consistency complaints in new 'hbck report' page  hbase-22723 makes it so catalogjanitor looks for holes and overlaps when it scans hbase:meta. hbase-22709 added a new hbck report page to the master in which it notes findings its figured looking at regionserver reports and current state of master. this issue is about exposing the inconsistencies found scanning hbase:meta in the hbck report page. ",
        "label": 314
    },
    {
        "text": "hbase it tests are run when you ask to run all tests   it should be that you have to ask explicitly to run them  up on trunk and on 0.95 apache builds, sergey noticed that hbase-it tests are running. up to this, the convention was that you had to explicitly ask that they run but that changed somehow recently. these tests are heavyweight, take a long time to complete, and are very likely to fail up on the apache infra (which is what we want of them but not as part of the general proofing build). for now the tests have been artificially disabled up on builds.apache.org but their inclusion likely frustrates joe blow trying to do a local hbase packaging. ",
        "label": 340
    },
    {
        "text": "import filterkv does not call filter filterrowkey  the general contract of a filter is that filterrowkey is called before filterkeyvalue.  import is using filters for custom filtering but it does not called filterrowkey at all. that throws off some filters (such as rowfilter, and more recently prefixfilter, and inclusivestopfilter). see hbase-10493 and hbase-10485. ",
        "label": 286
    },
    {
        "text": "org apache hadoop hbase rest performanceevaluation is out of sync with org apache hadoop hbase performanceevaluation  here is list of jiras whose fixes might have gone into rest.performanceevaluation : ------------------------------------------------------------------------ r1527817 | mbertozzi | 2013-09-30 15:57:44 -0700 (mon, 30 sep 2013) | 1 line hbase-9663 performanceevaluation does not properly honor specified table name parameter ------------------------------------------------------------------------ r1526452 | mbertozzi | 2013-09-26 04:58:50 -0700 (thu, 26 sep 2013) | 1 line hbase-9662 performanceevaluation input do not handle tags properties ------------------------------------------------------------------------ r1525269 | ramkrishna | 2013-09-21 11:01:32 -0700 (sat, 21 sep 2013) | 3 lines hbase-8496 - implement tags and the internals of how a tag should look like (ram) ------------------------------------------------------------------------ r1524985 | nkeywal | 2013-09-20 06:02:54 -0700 (fri, 20 sep 2013) | 1 line hbase-9558  performanceevaluation is in hbase-server, and creates a dependency to minidfscluster ------------------------------------------------------------------------ r1523782 | nkeywal | 2013-09-16 13:07:13 -0700 (mon, 16 sep 2013) | 1 line hbase-9521  clean clearbufferonfail behavior and deprecate it ------------------------------------------------------------------------ r1518341 | jdcryans | 2013-08-28 12:46:55 -0700 (wed, 28 aug 2013) | 2 lines hbase-9330 refactor pe to create htable the correct way long term, we may consider consolidating the two performanceevaluation classes so that such maintenance work can be reduced. ",
        "label": 191
    },
    {
        "text": "remove option to create on heap bucket cache  since we are moving read and write paths to use direct memory, option to configure on heap bucket cache is irrelevant. we should remove that option. ",
        "label": 46
    },
    {
        "text": "get  and getscanner are different in how they treat column parameter  from the list, cure at xg dot pl     there are group of methods \"getrow\" and group \"getscanner\" - both get as param array of collumns     but in \"getrow\" methods we have to put it without \":\" at the end of column family name, and for \"getscanner\" the colon is necessary.     i think that it will be good to make it identical.  ",
        "label": 314
    },
    {
        "text": "add metrics to keep track of slow hlog appends  whenever an edit takes too long to be written to an hlog, hbase logs a warning such as this one: 2011-02-23 20:03:14,703 warn org.apache.hadoop.hbase.regionserver.wal.hlog: ipc server handler 21 on 60020 took 15065ms appending an edit to hlog; editcount=126050 i would like to have a counter incremented each time this happens and this counter exposed via the metrics stuff in hbase so i can collect it in my monitoring system. ",
        "label": 333
    },
    {
        "text": " hbck  refactor parallel workitem  to futures   this would convert workitem* logic (with low level notifies, and rough exception handling) into a more canonical futures pattern. currently there are two instances of this pattern (for loading hdfs dirs, for contacting regionservers for assignments, and soon \u2013 for loading hdfs .regioninfo files). ",
        "label": 39
    },
    {
        "text": "allow decoding more than getresponse from the server  hbase-15620 adds on call serialization and de-serialization. however the client-serialize-handler currently assumes that all responses will be getresponse. we should keep a call id to response type mapping. maybe the inner request should have a response factory inside it. ",
        "label": 154
    },
    {
        "text": "regexprowfilter behaves incorectly when there are multiple store files  i noticed that after running some table map/reduces, then using a  regexprowfilter to scan through the table, the scanner misses  rows when its columns are in different stores. this (rather convoluted) unit test provokes the behavior. set memcache flush size small to trigger multiple stores put in 10 row with 2 columns. each row has the same value for col1 (which the rowfilter wants to match) scan with and without the filter to be sure that we get all the rows with each run an identity table m/r 10 times to fill up the memcache and trigger flush. scan again. this time the filter does not pickup anything. attaching the log from this run as well. ",
        "label": 241
    },
    {
        "text": "shutdown and compactions  currently i thank the region server closes the region spits in a set order this same order is used in the compaction/spit thread after a startup and all regions need compaction.  this causes the shutdown time to take a vary long time depending on what region the server is compacting.  i have seen my cluster take more then 60 mins to shutdown waiting for several regions to finish compaction. the problem i am seeing is say the compaction thread is working on a on region 2 of 4 in the list.  when the region server get the msg_regionserver_quiesce from the master it closes in order from 1-4  so it closes region 1 and waits for region 2 to finish compaction before closing it. the problem is it never closes region split 3 or 4 while waiting for region 2 to complete the compaction. so example say 3 and 4 regions are waiting to be compacted also. when region 2 is done region 3 start compaction within milliseconds of 2's finished compaction.  this happens faster then the region server can close region 3 so the region server hangs around compacting regions until it run out of open regions needing compact.  with a lot of regions this could hang some region server a long time to shutdown the cluster. solutions 1 or 2 below will work i thank  1. close all regions not currently getting compaction so when the compaction thread complete it has no other regions open to compact   2. empty the compaction que so it has no other regions to compact when done with the current one. ",
        "label": 314
    },
    {
        "text": "create namespace command for existing namespace does not throw useful error message  create_namespace command for existing namespace does not throw an useful error message here we try to call create_namespace command twice for same namespace, and attempt to verify an error message  from test console log 2017-11-13 08:19:03,347|info|mainthread|hbase.py:191 - runshellcmds()|writing commands to file /hwqe/hadoopqe/artifacts/tmp-844452 2017-11-13 08:19:03,348|info|mainthread|hbase.py:194 - runshellcmds()| 'create_namespace 'testnamespace'' 2017-11-13 08:19:03,348|info|mainthread|hbase.py:194 - runshellcmds()| 'create_namespace 'testnamespace'' 2017-11-13 08:19:03,348|info|mainthread|hbase.py:194 - runshellcmds()| 'drop_namespace 'testnamespace'' 2017-11-13 08:19:03,348|info|mainthread|hbase.py:198 - runshellcmds()|done writing commands to file. will execute them now. 2017-11-13 08:19:03,348|info|mainthread|machine.py:145 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|running: /usr/hdp/current/hbase-client/bin/hbase shell /hwqe/hadoopqe/artifacts/tmp-844452 2017-11-13 08:19:11,593|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|2017-11-13 08:19:11,592 info  [main] beanutils.fluentpropertybeanintrospector: error when creating propertydescriptor for public final void org.apache.commons.configuration2.abstractconfiguration.setproperty(java.lang.string,java.lang.object)! ignoring this property. 2017-11-13 08:19:11,628|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|2017-11-13 08:19:11,628 info  [main] impl.metricsconfig: loaded properties from hadoop-metrics2-hbase.properties 2017-11-13 08:19:11,700|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|2017-11-13 08:19:11,700 info  [main] impl.metricssystemimpl: scheduled metric snapshot period at 10 second(s). 2017-11-13 08:19:11,700|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|2017-11-13 08:19:11,700 info  [main] impl.metricssystemimpl: hbase metrics system started 2017-11-13 08:19:12,655|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|took 0.6500 seconds 2017-11-13 08:19:12,784|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2| 2017-11-13 08:19:12,784|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|error: testnamespace 2017-11-13 08:19:12,784|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2| 2017-11-13 08:19:12,784|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|create namespace; pass namespace name, 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|and optionally a dictionary of namespace configuration. 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|examples: 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2| 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|hbase> create_namespace 'ns1' 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|hbase> create_namespace 'ns1', {'property_name'=>'property_value'} 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2| 2017-11-13 08:19:12,785|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|took 0.1284 seconds 2017-11-13 08:19:13,016|info|mainthread|machine.py:159 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|took 0.2307 seconds 2017-11-13 08:19:13,541|info|mainthread|machine.py:184 - run()||guid=29350822-e327-4a0d-97ec-fc8b0a7d1be2|exit code: 0 2017-11-13 08:19:13,575|info|mainthread|conftest.py:241 - pytest_report_teststatus()|test \"test_shellcreatenamespace[createexistentnamespace]\" failed in 10.22 seconds ",
        "label": 441
    },
    {
        "text": "add unit tests for org apache hadoop hbase util strings  i've analysed your codebase and noticed that org.apache.hadoop.hbase.util.strings is not fully tested.  i've written some tests for the methods in this class with the help of diffblue cover. hopefully, these tests will help you detect any regressions caused by future code changes. if you would find it useful to have additional tests written for this repository, i would be more than happy to look at other classes that you consider important in a subsequent pr. ",
        "label": 159
    },
    {
        "text": "move hbase 92rc1 on to hadoop 0rc2  it looks like i can just add staging repo to our list of repositories in pom and then we'll pick up rc2. running tests now and need to verify that what i am picking up is actually rc2. ",
        "label": 314
    },
    {
        "text": "allow hbase coprocessors to clean up when they fail  in the thread giving a chance to buggy coprocessors to clean up i brought up the issue that coprocessors currently don't have a chance to release their own resources (be they internal resources within the jvm, or external resources elsewhere) when they get forcefully removed due to an uncaught exception escaping. it would be nice to fix that, either by adding an api called by the coprocessorhost when killing a faulty coprocessor, or by guaranteeing that the coprocessor's stop() method will be invoked then. this feature request is actually pretty important due to bug hbase-9046, which means that it's not possible to properly clean up a coprocessor without restarting the regionserver (!!). ",
        "label": 70
    },
    {
        "text": "testreplication is flaky  see discussion at the end of hbase-5778.  testreplication failed in all recent 0.94 jenkins builds. ",
        "label": 286
    },
    {
        "text": "compaction file not cleaned up after a crash oome server  we do not clean up compaction files after a crash/oome of a region server. i am not sure how the compaction file naming is anymore if its not reproducable some how we   should let the master or the server with the root region check every so often and delete old files say   older then 24 hours in the compaction dir's of the tables ",
        "label": 167
    },
    {
        "text": "failed server shutdown processing when retrying hlog split  2011-01-04 01:14:17,353 warn org.apache.hadoop.hbase.master.masterfilesystem: retrying splitting because of:  org.apache.hadoop.hbase.regionserver.wal.orphanhlogaftersplitexception: discovered orphan hlog after split. maybe the hregionserver was not dead when we started  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlog(hlogsplitter.java:286)  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlog(hlogsplitter.java:187)  at org.apache.hadoop.hbase.master.masterfilesystem.splitlog(masterfilesystem.java:196)  at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:96)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:151)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  2011-01-04 01:14:17,353 error org.apache.hadoop.hbase.executor.eventhandler: caught throwable while processing event m_meta_server_shutdown  java.lang.illegalstateexception: an hlogsplitter instance may only be used once  at com.google.common.base.preconditions.checkstate(preconditions.java:145)  at org.apache.hadoop.hbase.regionserver.wal.hlogsplitter.splitlog(hlogsplitter.java:170)  at org.apache.hadoop.hbase.master.masterfilesystem.splitlog(masterfilesystem.java:199)  at org.apache.hadoop.hbase.master.handler.servershutdownhandler.process(servershutdownhandler.java:96)  at org.apache.hadoop.hbase.executor.eventhandler.run(eventhandler.java:151)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619) ",
        "label": 314
    },
    {
        "text": "precompact and preflush can bypass by returning null scanner  shut it down  noticed by anoop sam john during review of hbase-18770, precompact and preflush can bypass normal processing by returning null. they are not bypasable by ordained route. we should shut down this avenue. the precompact at least may be new coming in with: tree dbf13093842f85a713f023d7219caccf8f4eb05f parent a4dcf51415616772e462091ce93622f070ea8810 author zhangduo <zhangduo@apache.org> sat apr 9 16:18:08 2016 +0800 committer zhangduo <zhangduo@apache.org> sun apr 10 09:26:28 2016 +0800 hbase-15527 refactor compactor related classes would have to dig in more to figure for sure. ",
        "label": 314
    },
    {
        "text": "add a global read only property to turn off all writes for the cluster  as part of hbase-18477, we need a way to turn off all modification for a cluster. this patch extends the read only mode used by replication to disable all data and metadata operations. ",
        "label": 511
    },
    {
        "text": " site  add one page only version of hbase doc  ",
        "label": 314
    },
    {
        "text": "atomicappend  a put that appends to the latest version of a cell  i e  reads current value then adds the bytes offered by the client to the tail and writes out a new entry  its come up a few times that clients want to add to an existing cell rather than make a new cell each time. at our place, the frontend keeps a list of urls a user has visited \u2013 their md5s \u2013 and updates it as user progresses. rather than read, modify client-side, then write new value back to hbase, it would be sweet if could do it all in one operation in hbase server. tsdb aims to be space efficient. rather than pay the cost of the kv wrapper per metric, it would rather have a kv for an interval an in this kv have a value that is all the metrics for the period. it could be done as a coprocessor but this feels more like a fundamental feature. beno\u00eet suggests that atomicappend take a flag to indicate whether or not the client wants to see the resulting cell; often a client won't want to see the result and in this case, why pay the price formulating and delivering a response that client just drops. ",
        "label": 286
    },
    {
        "text": "cme in regionmanager ismetaserver  2010-03-05 14:26:17,990 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 84 on 60000, call regionserverreport(address: 98.137.30.50:60020, startcode: 1267824847596, load: (requests=0, regions=163, usedheap=3113, maxheap=5983), [lorg.apache.hadoop.hbase.hmsg;@5446315a, [lorg.apache.hadoop.hbase.hregioninfo;@4c5236ef) from 98.137.30.50:48101: error: java.io.ioexception: java.util.concurrentmodificationexception java.io.ioexception: java.util.concurrentmodificationexception        at java.util.treemap$privateentryiterator.nextentry(treemap.java:1100)        at java.util.treemap$valueiterator.next(treemap.java:1145)        at org.apache.hadoop.hbase.master.regionmanager.ismetaserver(regionmanager.java:832)        at org.apache.hadoop.hbase.master.regionmanager.regionsawaitingassignment(regionmanager.java:391)        at org.apache.hadoop.hbase.master.regionmanager.assignregions(regionmanager.java:196)        at org.apache.hadoop.hbase.master.servermanager.processmsgs(servermanager.java:488)        at org.apache.hadoop.hbase.master.servermanager.processregionserverallswell(servermanager.java:414)        at org.apache.hadoop.hbase.master.servermanager.regionserverreport(servermanager.java:323)        at org.apache.hadoop.hbase.master.hmaster.regionserverreport(hmaster.java:724)        at sun.reflect.generatedmethodaccessor2.invoke(unknown source)        at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)        at java.lang.reflect.method.invoke(method.java:597)        at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:657)        at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:915) ",
        "label": 229
    },
    {
        "text": "backport hbase  filter hfiles based on first last key  into  hbase-8063 was backported to 0.94 in hbase-8198. now i think we have to have it 0.96, so making a blocker. ",
        "label": 314
    },
    {
        "text": "online slow response log  today when an individual rpc exceeds a configurable time bound we log a complaint by way of the logging subsystem. these log lines look like: 2019-08-30 22:10:36,195 warn [,queue=15,port=60020] ipc.rpcserver - (responsetooslow): {\"call\":\"scan(org.apache.hadoop.hbase.protobuf.generated.clientprotos$scanrequest)\", \"starttimems\":1567203007549, \"responsesize\":6819737, \"method\":\"scan\", \"param\":\"region { type: region_name value: \\\"tsdb,\\\\000\\\\000\\\\215\\\\f)o\\\\\\\\\\\\024\\\\302\\\\220\\\\000\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\000\\\\000\\\\006\\\\000\\\\000\\\\000\\\\000\\\\000\\\\005\\\\000\\\\000<truncated>\", \"processingtimems\":28646, \"client\":\"10.253.196.215:41116\", \"queuetimems\":22453, \"class\":\"hregionserver\"} unfortunately we often truncate the request parameters, like in the above example. we do this because the human readable representation is verbose, the rate of too slow warnings may be high, and the combination of these things can overwhelm the log capture system. the truncation is unfortunate because it eliminates much of the utility of the warnings. for example, the region name, the start and end keys, and the filter hierarchy are all important clues for debugging performance problems caused by moderate to low selectivity queries or queries made at a high rate. we can maintain an in-memory ring buffer of requests that were judged to be too slow in addition to the responsetooslow logging. the in-memory representation can be complete and compressed. a new admin api and shell command can provide access to the ring buffer for online performance debugging. a modest sizing of the ring buffer will prevent excessive memory utilization for a minor performance debugging feature by limiting the total number of retained records. there is some chance a high rate of requests will cause information on other interesting requests to be overwritten before it can be read. this is the nature of a ring buffer and an acceptable trade off. the write request types do not require us to retain all information submitted in the request. we don't need to retain all key-values in the mutation, which may be too large to comfortably retain. we only need a unique set of row keys, or even a min/max range, and total counts. the consumers of this information will be debugging tools. we can afford to apply fast compression to ring buffer entries (if codec support is available), something like snappy or zstandard, and decompress on the fly when servicing the retrieval api request. this will minimize the impact of retaining more information about slow requests than we do today. this proposal is for retention of request information only, the same information provided by responsetooslow warnings. total size of response serialization, possibly also total cell or row counts, should be sufficient to characterize the response. optionally persist new entries added to the ring buffer into one or more files in hdfs in a write-behind manner. if the hdfs writer blocks or falls behind and we are unable to persist an entry before it is overwritten, that is fine. response too slow logging is best effort. if we can detect this make a note of it in the log file. provide a tool for parsing, dumping, filtering, and pretty printing the slow logs written to hdfs. the tool and the shell can share and reuse some utility classes and methods for accomplishing that. \u2014 new shell commands: get_slow_responses [ <server1> ... , <servern> ] [ , { <filter-attributes> } ] retrieve, decode, and pretty print the contents of the too slow response ring buffer maintained by the given list of servers; or all servers in the cluster if no list is provided. optionally provide a map of parameters for filtering as additional argument. the table filter, which expects a string containing a table name, will include only entries pertaining to that table. the region filter, which expects a string containing an encoded region name, will include only entries pertaining to that region. the client_ip filter, which expects a string containing an ip address, will include only entries pertaining to that client. the user filter, which expects a string containing a user name, will include only entries pertaining to that user. filters are additive, for example if both client_ip and user filters are given, entries matching either or both conditions will be included. the exception to this is if both table and region filters are provided, region will be preferred and table will be ignored. a server name is its host, port, and start code, e.g. \"host187.example.com,60020,1289493121758\". clear_slow_responses [ <server1> ... , <servern> ] clear the too slow response ring buffer maintained by the given list of servers; or all servers on the cluster if no argument is provided. a server name is its host, port, and start code, e.g. \"host187.example.com,60020,1289493121758\". \u2014 new admin apis: list<responsedetail> admin#getslowresponses(@nullable list<string> servers); list<responsedetail> admin#clearslowresponses(@nullable list<string> servers); ",
        "label": 473
    },
    {
        "text": "notservingregionexception shouldn't log a stack trace  notservingregionexception is not really an error condition, and so the logged stacktrace (see below) is unnecessary information that could be trimmed from the logs. 2010-07-12 14:41:44,239 info org.apache.hadoop.ipc.hbaseserver: ipc server handler 16 on 60020, call multiput(org.apache.hadoop.hbase.client.multiput@1f920f2) from 10.40.0.43:32905: error: org.apache.hadoop.hbase.notservingregionexception: person,0030706293:pa,1278958944701.e4097c109d5a8265198890b4bfa92f3c. is closed  org.apache.hadoop.hbase.notservingregionexception: person,0030706293:pa,1278958944701.e4097c109d5a8265198890b4bfa92f3c. is closed  at org.apache.hadoop.hbase.regionserver.hregion.internalobtainrowlock(hregion.java:2122)  at org.apache.hadoop.hbase.regionserver.hregion.getlock(hregion.java:2211)  at org.apache.hadoop.hbase.regionserver.hregion.dominibatchput(hregion.java:1493)  at org.apache.hadoop.hbase.regionserver.hregion.put(hregion.java:1447)  at org.apache.hadoop.hbase.regionserver.hregionserver.put(hregionserver.java:1703)  at org.apache.hadoop.hbase.regionserver.hregionserver.multiput(hregionserver.java:2361)  at sun.reflect.generatedmethodaccessor13.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:576)  at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:919)  2010-07-12 14:41:44,376 debug org.apache.hadoop.hbase.regionserver.hregion: creating region person,0030706293:pa,1278960103839.cfff1e6418e52aca6b766bc56baa3f53.  2010-07-12 14:41:44,378 debug org.apache.hadoop.hbase.regionserver.hregion: creating region person,0030896133:pa,1278960103839.427ac16dfbf3f747ff938de598a9a356. ",
        "label": 314
    },
    {
        "text": "testprocedurerecovery is flaky  encountered this when running master tests locally using 7u79: tests run: 8, failures: 0, errors: 1, skipped: 0, time elapsed: 12.28 sec <<< failure! - in org.apache.hadoop.hbase.procedure2.testprocedurerecovery testrunningprocwithsamenonce(org.apache.hadoop.hbase.procedure2.testprocedurerecovery)  time elapsed: 0.318 sec  <<< error! java.lang.illegalargumentexception: null at com.google.common.base.preconditions.checkargument(preconditions.java:76) at org.apache.hadoop.hbase.procedure2.procedureexecutor.submitprocedure(procedureexecutor.java:595) at org.apache.hadoop.hbase.procedure2.proceduretestingutility.submitandwait(proceduretestingutility.java:137) at org.apache.hadoop.hbase.procedure2.testprocedurerecovery.testrunningprocwithsamenonce(testprocedurerecovery.java:321) flaked tests:  org.apache.hadoop.hbase.procedure2.testprocedurerecovery.testrunningprocwithsamenonce(org.apache.hadoop.hbase.procedure2.testprocedurerecovery)   run 1: testprocedurerecovery.testrunningprocwithsamenonce:321 \u00bb illegalargument   run 2: pass ",
        "label": 309
    },
    {
        "text": "zookeeper keeperexception connectionlossexception is a  recoverable  exception  we should retry a while on server startup at least   on startup of daemons we'll sometimes fail (new master) with something like the following: 2010-09-30 23:38:36,979 info org.apache.hadoop.hbase.zookeeper.zkutil: regionserver60020 opening connection to zookeeper with quorum (sv2borg182:20001,sv2borg181:20001,sv2borg180:20001) 2010-09-30 23:38:36,979 info org.apache.zookeeper.zookeeper: initiating client connection, connectstring=sv2borg182:20001,sv2borg181:20001,sv2borg180:20001 sessiontimeout=60000 watcher=regionserver60020 2010-09-30 23:38:36,980 info org.apache.zookeeper.clientcnxn: opening socket connection to server sv2borg182/10.20.20.182:20001 2010-09-30 23:38:36,980 info org.apache.zookeeper.clientcnxn: socket connection established to sv2borg182/10.20.20.182:20001, initiating session 2010-09-30 23:38:36,981 info org.apache.zookeeper.clientcnxn: unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect 2010-09-30 23:38:37,083 error org.apache.hadoop.hbase.zookeeper.zookeeperwatcher: regionserver60020 unexpected keeperexception creating base node org.apache.zookeeper.keeperexception$connectionlossexception: keepererrorcode = connectionloss for /hbase     at org.apache.zookeeper.keeperexception.create(keeperexception.java:90)     at org.apache.zookeeper.keeperexception.create(keeperexception.java:42)     at org.apache.zookeeper.zookeeper.create(zookeeper.java:637)     at org.apache.hadoop.hbase.zookeeper.zkutil.createandfailsilent(zkutil.java:807)     at org.apache.hadoop.hbase.zookeeper.zookeeperwatcher.<init>(zookeeperwatcher.java:107)     at org.apache.hadoop.hbase.regionserver.hregionserver.initializezookeeper(hregionserver.java:438)     at org.apache.hadoop.hbase.regionserver.hregionserver.initialize(hregionserver.java:420)     at org.apache.hadoop.hbase.regionserver.hregionserver.<init>(hregionserver.java:305)     at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method)     at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39)     at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27)     at java.lang.reflect.constructor.newinstance(constructor.java:513)     at org.apache.hadoop.hbase.regionserver.hregionserver.constructregionserver(hregionserver.java:2436)     at org.apache.hadoop.hbase.regionserver.hregionservercommandline.start(hregionservercommandline.java:60)     at org.apache.hadoop.hbase.regionserver.hregionservercommandline.run(hregionservercommandline.java:75)     at org.apache.hadoop.util.toolrunner.run(toolrunner.java:65)     at org.apache.hadoop.hbase.util.servercommandline.domain(servercommandline.java:76)     at org.apache.hadoop.hbase.regionserver.hregionserver.main(hregionserver.java:2460) i'll see it over on master too the odd time. currently we fail out (though in above case, because of order in which we do startup the regionserver actually hung up because rpc was started before zk) we should retry this error some at least on startup because its 'recoverable' (see http://wiki.apache.org/hadoop/zookeeper/errorhandling). in fact, we should probably retry always until we get a session expired. ",
        "label": 314
    },
    {
        "text": "clean up coprocessor's handlings of table operations  couple fixes we can do w.r.t coprocessor's handlings of table operations. 1. honor masterobserver's requests to bypass default action.  2. fix up the function signatures for precreatetable to use hregioninfo as parameter instead.  3. invoke postenabletable, etc. methods after the operations are done. ",
        "label": 326
    },
    {
        "text": "flush task message may be confusing when region is recovered  in hregion.setrecovering() we have this code:     // force a flush only if region replication is set up for this region. otherwise no need.       boolean forceflush = gettabledesc().getregionreplication() > 1;       // force a flush first       monitoredtask status = taskmonitor.get().createstatus(         \"flushing region \" + this + \" because recovery is finished\");       try {         if (forceflush) {           internalflushcache(status);         } so we only optionally force flush after a recovery of a region, but the message always is set to \"flushing...\", which might be confusing. we should change the message based on forceflush. ",
        "label": 177
    },
    {
        "text": "deadlock while rolling wal log while finishing flush  found this in our cluster: found one java-level deadlock: ============================= \"ipc server handler 5 on 60020\":   waiting for ownable synchronizer 0x00007f5f659d4e90, (a java.util.concurrent.locks.reentrantlock$nonfairsync),   which is held by \"regionserver/0:0:0:0:0:0:0:0:60020.cacheflusher\" \"regionserver/0:0:0:0:0:0:0:0:60020.cacheflusher\":   waiting to lock monitor 0x00007f5f56312f78 (object 0x00007f5f659d4ec0, a java.lang.integer),   which is held by \"ipc server handler 5 on 60020\" java stack information for the threads listed above: =================================================== \"ipc server handler 5 on 60020\":         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x00007f5f659d4e90> (a java.util.concurrent.locks.reentrantlock$nonfairsync)         at java.util.concurrent.locks.locksupport.park(unknown source)         at java.util.concurrent.locks.abstractqueuedsynchronizer.parkandcheckinterrupt(unknown source)         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquirequeued(unknown source)         at java.util.concurrent.locks.abstractqueuedsynchronizer.acquire(unknown source)         at java.util.concurrent.locks.reentrantlock$nonfairsync.lock(unknown source)         at java.util.concurrent.locks.reentrantlock.lock(unknown source)         at org.apache.hadoop.hbase.hlog.rollwriter(hlog.java:219)         at org.apache.hadoop.hbase.hlog.append(hlog.java:390)         - locked <0x00007f5f659d4ec0> (a java.lang.integer)         at org.apache.hadoop.hbase.hregion.update(hregion.java:1633)         at org.apache.hadoop.hbase.hregion.batchupdate(hregion.java:1436)         at org.apache.hadoop.hbase.hregionserver.batchupdate(hregionserver.java:1552)         at sun.reflect.generatedmethodaccessor9.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)         at java.lang.reflect.method.invoke(unknown source)         at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:413)         at org.apache.hadoop.ipc.server$handler.run(server.java:901) \"regionserver/0:0:0:0:0:0:0:0:60020.cacheflusher\":         at org.apache.hadoop.hbase.hlog.completecacheflush(hlog.java:481)         - waiting to lock <0x00007f5f659d4ec0> (a java.lang.integer)         at org.apache.hadoop.hbase.hregion.internalflushcache(hregion.java:1133)         at org.apache.hadoop.hbase.hregion.flushcache(hregion.java:1020)         at org.apache.hadoop.hbase.hregionserver$flusher.flushregion(hregionserver.java:446)         - locked <0x00007f5f659d3ec8> (a java.util.hashset)         at org.apache.hadoop.hbase.hregionserver$flusher.run(hregionserver.java:390) found 1 deadlock. ",
        "label": 314
    },
    {
        "text": "timeouts for row lock and scan should be separate  apparently the timeout used for row locking and for scanning is global. it would be better to have two separate timeouts.  (opening the issue to make lars george happy) ",
        "label": 103
    },
    {
        "text": "servershutdownhandler and disable delete should not happen parallely leading to recreation of regions that were deleted  servershutdownhandler and disable/delete table handler races. this is not an issue due to tm.  -> a regionserver goes down. in our cluster the regionserver holds lot of regions.  -> a region r1 has two daughters d1 and d2.  -> the servershutdownhandler gets called and scans the meta and gets all the user regions  -> parallely a table is disabled. (no problem in this step).  -> delete table is done.  -> the tables and its regions are deleted including r1, d1 and d2.. (so meta is cleaned)  -> now servershutdownhandler starts to processthedeadregion  if (hri.isoffline() && hri.issplit()) {       log.debug(\"offlined and split region \" + hri.getregionnameasstring() +         \"; checking daughter presence\");       fixupdaughters(result, assignmentmanager, catalogtracker); as part of fixupdaughters as the daughers d1 and d2 is missing for r1     if (isdaughtermissing(catalogtracker, daughter)) {       log.info(\"fixup; missing daughter \" + daughter.getregionnameasstring());       metaeditor.adddaughter(catalogtracker, daughter, null);       // todo: log warn if the regiondir does not exist in the fs.  if its not       // there then something wonky about the split -- things will keep going       // but could be missing references to parent region.       // and assign it.       assignmentmanager.assign(daughter, true); we call assign of the daughers.   now after this we again start with the below code.         if (processdeadregion(e.getkey(), e.getvalue(),             this.services.getassignmentmanager(),             this.server.getcatalogtracker())) {           this.services.getassignmentmanager().assign(e.getkey(), true); now when the ssh scanned the meta it had r1, d1 and d2.  so as part of the above code d1 and d2 which where assigned by fixupdaughters  is again assigned by this.services.getassignmentmanager().assign(e.getkey(), true); thus leading to a zookeeper issue due to bad version and killing the master.  the important part here is the regions that were deleted are recreated which i think is more critical. ",
        "label": 544
    },
    {
        "text": "scanners not closed properly in certain circumstances  memory leak   scanners are sometimes leaked by the keyvalueheap class. the constructor adds each scanner to a heap, but only if the scanner's peek() method returns not null (line 58). otherwise the scanner is dropped without being closed. unfortunately some scanners (like storescanner and memstorescanner) register themselves to some global list when constructed and only deregister on close(). this can cause a memory leak, for example with memstorescanners on an empty memory store. the quick fix is to add an else clause to the if on line 58: } else {  scanner.close()  } the root cause is that ownership of the scanners is transferred from the caller to the keyvalueheap on construction. maybe this should be made clear in the documentation or changed. ",
        "label": 247
    },
    {
        "text": "correct regionserver metric of totalrequestcount  when i get the metric ,i found this three metric may be have some error as follow :  \"totalrequestcount\" : 17541,  \"readrequestcount\" : 17483,  \"writerequestcount\" : 1633, ",
        "label": 504
    },
    {
        "text": "drop the support for several hadoop releases due to cve  https://lists.apache.org/thread.html/3d6831c3893cd27b6850aea2feff7d536888286d588e703c6ffd2e82@%3cuser.hadoop.apache.org%3e versions affected:  3.0.0-alpha1 to 3.1.0, 2.9.0 to 2.9.1, 2.2.0 to 2.8.4 so maybe we should drop the several release for 2.8.x and 2.9.x, and drop the support for whole 3.0.x release line. ",
        "label": 149
    },
    {
        "text": "contention getting the current user in rpcclient connection writerequest  i've been running tests on clusters with \"lots\" of regions, about 400, and i'm seeing weird contention in the client. this one i see a lot, hundreds and sometimes thousands of threads are blocked like this: \"htable-pool4-t74\" daemon prio=10 tid=0x00007f2254114000 nid=0x2a99 waiting for monitor entry [0x00007f21f9e94000]    java.lang.thread.state: blocked (on object monitor) at org.apache.hadoop.security.usergroupinformation.getcurrentuser(usergroupinformation.java:466) - waiting to lock <0x00000000fb5ad000> (a java.lang.class for org.apache.hadoop.security.usergroupinformation) at org.apache.hadoop.hbase.ipc.rpcclient$connection.writerequest(rpcclient.java:1013) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1407) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1634) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1691) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$blockingstub.multi(clientprotos.java:27339) at org.apache.hadoop.hbase.client.multiservercallable.call(multiservercallable.java:105) at org.apache.hadoop.hbase.client.multiservercallable.call(multiservercallable.java:43) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithoutretries(rpcretryingcaller.java:183) while the holder is doing this: \"htable-pool17-t55\" daemon prio=10 tid=0x00007f2244408000 nid=0x2a98 runnable [0x00007f21f9f95000]    java.lang.thread.state: runnable at java.security.accesscontroller.getstackaccesscontrolcontext(native method) at java.security.accesscontroller.getcontext(accesscontroller.java:487) at org.apache.hadoop.security.usergroupinformation.getcurrentuser(usergroupinformation.java:466) - locked <0x00000000fb5ad000> (a java.lang.class for org.apache.hadoop.security.usergroupinformation) at org.apache.hadoop.hbase.ipc.rpcclient$connection.writerequest(rpcclient.java:1013) at org.apache.hadoop.hbase.ipc.rpcclient.call(rpcclient.java:1407) at org.apache.hadoop.hbase.ipc.rpcclient.callblockingmethod(rpcclient.java:1634) at org.apache.hadoop.hbase.ipc.rpcclient$blockingrpcchannelimplementation.callblockingmethod(rpcclient.java:1691) at org.apache.hadoop.hbase.protobuf.generated.clientprotos$clientservice$blockingstub.multi(clientprotos.java:27339) at org.apache.hadoop.hbase.client.multiservercallable.call(multiservercallable.java:105) at org.apache.hadoop.hbase.client.multiservercallable.call(multiservercallable.java:43) at org.apache.hadoop.hbase.client.rpcretryingcaller.callwithoutretries(rpcretryingcaller.java:183) ",
        "label": 242
    },
    {
        "text": "update maven documentation in book  the maven documentation is a little out of date and has recently led to some confusion about tests. this would cleanup the maven documents in the book to be more explicit about how maven should be used. ",
        "label": 236
    },
    {
        "text": "addition of start command for execute the  hql query file  $ cat queries.hql create table webtable (  contents in_memory max_versions=10 compression=block,  anchor max_length=256 bloomfilter=counting_bloomfilter  vector_size=1000000 num_hash=4); ); insert into webtable (contents, anchor:hadoop)  values ('html content', 'http://hadoop.apache.org')  where row='http://www.hadoop.co.kr'; ... ---- hql> start ./queries.hql; ... hql> desc webtable; .... ",
        "label": 152
    },
    {
        "text": "htable batch  should perform validation on empty put  raised by java8964 in this thread:  http://osdir.com/ml/general/2014-02/msg44384.html when an empty put is passed in the list to htable#batch(), there is no validation performed whereas illegalargumentexception would have been thrown if this empty put in the simple put api call. validation on empty put should be carried out in htable#batch(). ",
        "label": 441
    },
    {
        "text": "rssstub in hregionserver is not thread safe and should not directly be used  while working on a patch for hbase-22060, i noticed that a unit test started failing because region server crashed with npe during initialization and after aborting the master. it turned out that access to the rssstub was not synchronized. the existing code in hregionserver already takes care of this fact by copying and null checking in most places, but there are a couple ones that are not so careful. those are in reportforduty and abort methods.  ",
        "label": 58
    },
    {
        "text": "outofmemory could occur when using boundedbytebufferpool during rpc bursts  after hbase-13819 the system some times run out of direct memory whenever there is some network congestion or some client side issues.  this was because of pending rpcs in the rpcserver$connection.responsequeue and since all the responses in this queue hold a buffer for cellblock from boundedbytebufferpool this could takeup a lot of memory if the boundedbytebufferpool's moving average settles down towards a higher value see the discussion here hbase-13819-comment ",
        "label": 46
    },
    {
        "text": "make shell's  d and debug cmd behave the same  the -d option switches log4j to debug and leaves the backtrace level at the default. when using the supplied debug command we only switch the backtrace, but i would think this also should set the log4j levels: # debugging method def debug   if @shell.debug     @shell.debug = false     conf.back_trace_limit = 0   else     @shell.debug = true     conf.back_trace_limit = 100   end   debug? end could be something like # debugging method def debug   if @shell.debug     @shell.debug = false     conf.back_trace_limit = 0     log_level = org.apache.log4j.level::error   else     @shell.debug = true     conf.back_trace_limit = 100     log_level = org.apache.log4j.level::debug   end   org.apache.log4j.logger.getlogger(\"org.apache.zookeeper\").setlevel(log_level)   org.apache.log4j.logger.getlogger(\"org.apache.hadoop.hbase\").setlevel(log_level)   debug? end ",
        "label": 290
    },
    {
        "text": " hbck2  hbase hbck throws ioe  no filesystem for scheme  hdfs   input appreciated on this one. if i do the below, passing a config that is pointing at a hdfs, i get the below (if i run w/o, hbck just picks up the wrong fs \u2013 the local fs). $ /vagrant/hbase/bin/hbase --config hbase-conf  hbck 2019-08-30 05:04:54,467 warn  [main] util.nativecodeloader: unable to load native-hadoop library for your platform... using builtin-java classes where applicable exception in thread \"main\" java.io.ioexception: no filesystem for scheme: hdfs         at org.apache.hadoop.fs.filesystem.getfilesystemclass(filesystem.java:2799)         at org.apache.hadoop.fs.filesystem.createfilesystem(filesystem.java:2810)         at org.apache.hadoop.fs.filesystem.access$200(filesystem.java:100)         at org.apache.hadoop.fs.filesystem$cache.getinternal(filesystem.java:2849)         at org.apache.hadoop.fs.filesystem$cache.get(filesystem.java:2831)         at org.apache.hadoop.fs.filesystem.get(filesystem.java:389)         at org.apache.hadoop.fs.path.getfilesystem(path.java:356)         at org.apache.hadoop.hbase.util.commonfsutils.getrootdir(commonfsutils.java:361)         at org.apache.hadoop.hbase.util.hbasefsck.main(hbasefsck.java:3605) its because the classpath is carefully curated so as to use shaded client only; there are no hdfs classes on the classpath intentionally. so, how to fix? happens whether hbck1 or hbck2 (you have to do a hdfs operation for hbck2 to trigger same issue). could be careful in hbck2 and note that if fs operation, you need to add hdfs jars to classpath so hbck2 can go against hdfs. if add the ' --internal-classpath' flag, then all classes are put on the classpath for hbck(2) (including the hdfs client jar which got the hdfs implementation after 2.7.2 was released) and stuff 'works'. could edit the bin/hbase script and make it so hdfs classes are added to the hbck classpath? could see if could do hdfs client-only? anyways, putting this up for now. others may have opinions. thanks. ",
        "label": 314
    },
    {
        "text": "hlogsplitter should handle missing hlogs  in build #48 (https://hudson.apache.org/hudson/job/hbase-0.90/48/), testreplication failed because of missing rows on the slave cluster. the reason is that a region server that was killed was able to archive a log at the same time the master was trying to recover it: [master_meta_server_operations-vesta.apache.org:47907-0] util.fsutils(625):  recovering file hdfs://localhost:50121/user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3a58598.1294117406909 ... [regionserver:0;vesta.apache.org,58598,1294117333857.logroller] wal.hlog(740):  moving old hlog file /user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3a58598.1294117406909  whose highest sequenceid is 422 to /user/hudson/.oldlogs/vesta.apache.org%3a58598.1294117406909 ... [master_meta_server_operations-vesta.apache.org:47907-0] master.masterfilesystem(204):  failed splitting hdfs://localhost:50121/user/hudson/.logs/vesta.apache.org,58598,1294117333857  java.io.ioexception: failed to open hdfs://localhost:50121/user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3a58598.1294117406909 for append caused by: org.apache.hadoop.hdfs.server.namenode.leaseexpiredexception:  org.apache.hadoop.hdfs.server.namenode.leaseexpiredexception:  no lease on /user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3a58598.1294117406909  file does not exist. [lease.  holder: dfsclient_-986975908, pendingcreates: 1] we should probably just handle the fact that a file could have been archived (maybe even check in .oldlogs to be sure) and move on to the next log. ",
        "label": 229
    },
    {
        "text": "column family scoping and cluster identification  support column family scoping via a new hcd attribute. provide initial set of scoping constants and javadoc setting expectations for a simple default binary scoping policy: replicate, or do not. make the underlying type integer so more complex edit routing policies are possible. also identify every hlogkey with the original cluster's id. ",
        "label": 229
    },
    {
        "text": "set up jenkins job to build site documentation  we should add a jenkins job that builds the website with all docs (nightly? on change?) so that we can get early warning of breakage (e.g. tag typos) as well as an easy place to review the not-yet-published rendered form. ",
        "label": 330
    },
    {
        "text": "improve error message when using startrow for meta scans   while in shell, when doing one of these: get '.meta.','table,d110818d237e63f-b236-4c89-a2c7-5f6342110818' scan '.meta.', {limit => 10 , startrow=>'table,d110818d3f72650-4a61-4352-ab49-707b42110818'} the call fails with a cryptic message: error: org.apache.hadoop.hbase.client.retriesexhaustedexception: trying to contact region server hp-node02.phx1.mozilla.com:60020 for region .meta.,,1, row 'crash_reports,d110818d237e63f-b236-4c89-a2c7-5f6342110818', but failed after 7 attempts. exceptions: java.io.ioexception: java.io.ioexception: java.lang.illegalargumentexception: no 44 in <e9table,d110818d237e63f-b236-4c89-a2c7-5f6342110818\u007f\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd>, length=43, offset=24 at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:987) at org.apache.hadoop.hbase.regionserver.hregionserver.convertthrowabletoioe(hregionserver.java:976) at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1632) at sun.reflect.generatedmethodaccessor30.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.hadoop.hbase.ipc.hbaserpc$server.call(hbaserpc.java:570) at org.apache.hadoop.hbase.ipc.hbaseserver$handler.run(hbaseserver.java:1039) caused by: java.lang.illegalargumentexception: no 44 in <e9table,d110818d237e63f-b236-4c89-a2c7-5f6342110818\u007f\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd>, length=43, offset=24 at org.apache.hadoop.hbase.keyvalue.getrequireddelimiterinreverse(keyvalue.java:1281) at org.apache.hadoop.hbase.keyvalue$metakeycomparator.comparerows(keyvalue.java:1827) at org.apache.hadoop.hbase.keyvalue$keycomparator.compare(keyvalue.java:1866) at org.apache.hadoop.hbase.util.bytes.binarysearch(bytes.java:1159) at org.apache.hadoop.hbase.io.hfile.hfile$blockindex.blockcontainingkey(hfile.java:1622) at org.apache.hadoop.hbase.io.hfile.hfile$reader.blockcontainingkey(hfile.java:918) at org.apache.hadoop.hbase.io.hfile.hfile$reader$scanner.seekto(hfile.java:1300) at org.apache.hadoop.hbase.regionserver.storefilescanner.seekatorafter(storefilescanner.java:136) at org.apache.hadoop.hbase.regionserver.storefilescanner.seek(storefilescanner.java:96) at org.apache.hadoop.hbase.regionserver.storescanner.<init>(storescanner.java:77) at org.apache.hadoop.hbase.regionserver.store.getscanner(store.java:1345) at org.apache.hadoop.hbase.regionserver.hregion$regionscanner.<init>(hregion.java:2276) at org.apache.hadoop.hbase.regionserver.hregion.instantiateinternalscanner(hregion.java:1131) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1123) at org.apache.hadoop.hbase.regionserver.hregion.getscanner(hregion.java:1107) at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2998) at org.apache.hadoop.hbase.regionserver.hregion.get(hregion.java:2900) at org.apache.hadoop.hbase.regionserver.hregionserver.get(hregionserver.java:1630) ... 5 more apparently the solution is to have the proper number of commas in the query. get '.meta.','table,d110818d237e63f-b236-4c89-a2c7-5f6342110818,' scan '.meta.', {limit => 10 , startrow=>'table,d110818d3f72650-4a61-4352-ab49-707b42110818,'} a better actionable error message should be reported. ",
        "label": 248
    },
    {
        "text": "replicationlogcleaner stop  calls hconnectionmanager deleteconnection  unnecessarily  when inspecting log, i found the following: 2013-11-08 18:23:48,472 error [m:0;kiyo:42380.oldlogcleaner] client.hconnectionmanager(468): connection not found in the list, can't delete it (connection key=hconnectionkey{properties={hbase.rpc.timeout=60000, hbase.zookeeper.property.clientport=59832, hbase.client.pause=100, zookeeper.znode.parent=/hbase, hbase.client.retries.number=350, hbase.zookeeper.quorum=localhost}, username='zy'}). may be the key was modified? java.lang.exception         at org.apache.hadoop.hbase.client.hconnectionmanager.deleteconnection(hconnectionmanager.java:468)         at org.apache.hadoop.hbase.client.hconnectionmanager.deleteconnection(hconnectionmanager.java:404)         at org.apache.hadoop.hbase.replication.master.replicationlogcleaner.stop(replicationlogcleaner.java:141)         at org.apache.hadoop.hbase.master.cleaner.cleanerchore.cleanup(cleanerchore.java:276) the call to hconnectionmanager#deleteconnection() is not needed.  here is related code which has a comment for this effect:     // not sure why we're deleting a connection that we never acquired or used     hconnectionmanager.deleteconnection(this.getconf()); ",
        "label": 441
    },
    {
        "text": "deadlock occurs between hlog roller and hlog syncer  the hlog roller thread and hlog syncer thread may occur dead lock with the 'flushlock' and 'updatelock', and then cause all 'ipc server handler' thread blocked on hlog append. the jstack info is as follow :  \"regionserver60020.logroller\":  at org.apache.hadoop.hbase.regionserver.wal.hlog.syncer(hlog.java:1305) waiting to lock <0x000000067bf88d58> (a java.lang.object)  at org.apache.hadoop.hbase.regionserver.wal.hlog.syncer(hlog.java:1283)  at org.apache.hadoop.hbase.regionserver.wal.hlog.sync(hlog.java:1456)  at org.apache.hadoop.hbase.regionserver.wal.hlog.cleanupcurrentwriter(hlog.java:876)  at org.apache.hadoop.hbase.regionserver.wal.hlog.rollwriter(hlog.java:657) locked <0x000000067d54ace0> (a java.lang.object)  at org.apache.hadoop.hbase.regionserver.logroller.run(logroller.java:94)  at java.lang.thread.run(thread.java:662)  \"regionserver60020.logsyncer\":  at org.apache.hadoop.hbase.regionserver.wal.hlog.syncer(hlog.java:1314) waiting to lock <0x000000067d54ace0> (a java.lang.object) locked <0x000000067bf88d58> (a java.lang.object)  at org.apache.hadoop.hbase.regionserver.wal.hlog.syncer(hlog.java:1283)  at org.apache.hadoop.hbase.regionserver.wal.hlog.sync(hlog.java:1456)  at org.apache.hadoop.hbase.regionserver.wal.hlog$logsyncer.run(hlog.java:1235)  at java.lang.thread.run(thread.java:662) ",
        "label": 441
    },
    {
        "text": "remove hql unit test  hql unit test takes a long time and doesn't have that much actual coverage. we're deprecating hql in 0.2, so we should plan on removing the unit test soon to save ourselves some trouble and time. ",
        "label": 314
    },
    {
        "text": "rowresult containskey string  doesn't work  since one can do a rowresult.get(string) i think rowresult should also support checking a column's existence with a string too (i.e a rowresult.containskey(string) method) ",
        "label": 144
    },
    {
        "text": "address ruby static analysis for bin directory  ",
        "label": 320
    },
    {
        "text": "add ability to have multiple masters localhbasecluster for test writing  to really be able to unit test the new master properly, we need to be able to have multiple masters running at once within a single logical cluster. should expose some methods like getactivemaster() and isactivemaster() as well as a simple way to kill an individual master / kill the active master. ",
        "label": 247
    },
    {
        "text": "do not throw runtimeexceptions in rpc hbaseobjectwritable code  ensure we log and rethrow as ioe  when there are issues with rpc and hbaseobjectwritable, primarily when server and client have different jars, the only thing that happens is the client will receive an eof exception. the server does not log what happened at all and the client does not receive a server trace, rather the server seems to close the connection and the client gets an eof because it tries to read off of a closed stream. we need to ensure that we catch, log, and rethrow as ioe any exceptions that may occur because of an issue with rpc or hbaseobjectwritable. ",
        "label": 268
    },
    {
        "text": "fix inter cluster replication future ordering issues  replication fails with indexoutofboundsexception regionserver.replicationsource$replicationsourceworkerthread(939): org.apache.hadoop.hbase.replication.regionserver.hbaseinterclusterreplicationendpoint threw unknown exception:java.lang.indexoutofboundsexception: index: 1, size: 1 at java.util.arraylist.rangecheck(unknown source) at java.util.arraylist.remove(unknown source) at org.apache.hadoop.hbase.replication.regionserver.hbaseinterclusterreplicationendpoint.replicate(hbaseinterclusterreplicationendpoint.java:222) its happening due to incorrect removal of entries from the replication entries list. ",
        "label": 55
    },
    {
        "text": "documentation of hbase site xml parameters  there is nowhere on the site or wiki a detailed examination of the parameters behind hbase. you have to open hbase-default.xml to see them and their descriptions. a page on the wiki with descriptions would be nice ",
        "label": 314
    },
    {
        "text": "endtime won't be set in verifyreplication if starttime is not set  in verifyreplication, we may set starttime and endtime to restrict the data to verfiy. however, the endtime won't be set in the program if we only pass endtime without starttime in command line argument. the reason is the following code:         if (starttime != 0) {           scan.settimerange(starttime,               endtime == 0 ? hconstants.latest_timestamp : endtime);         } the code will ignore endtime setting when not passing starttime in command line argument. another place needs to improvement is the help message as follows:     system.err.println(\" stoprow      end of the row\"); however, the program actually use \"endrow\" to parse the arguments:         final string endtimeargkey = \"--endtime=\";         if (cmd.startswith(endtimeargkey)) {           endtime = long.parselong(cmd.substring(endtimeargkey.length()));           continue;         } ",
        "label": 238
    },
    {
        "text": "persist memstorets to disk  atomicity can be achieved in two ways \u2013 by using a multiversion concurrency system (mvcc), or (ii) by ensuring that \"new\" writes do not complete, until the \"old\" reads complete. currently, memstore uses something along the lines of mvcc (called rwcc for read-write-consistency-control). but, this mechanism is not incorporated for the key-values written to the disk, as they do not include the memstore ts. let us make the two approaches be similar, by persisting the memstorets along with the key-value when it is written to the disk. ",
        "label": 34
    },
    {
        "text": "constraints implementation and javadoc changes  this is continuation of hbase-4605 see stack's comments https://reviews.apache.org/r/2579/#review3980 ",
        "label": 236
    },
    {
        "text": "thrift not forwarding column missing exception  when connecting to thrift, using the mutaterow() or mutaterowts() functions, if the column specified belongs to a non-existent column family, thrift just hangs, and the caller eventually gets a texception due to socket timeout. i would have expected to get a more hbasey exception along the lines of \"column family 'foo' does not exist for table 'bar'\". ",
        "label": 86
    },
    {
        "text": "hregionserver updaterecoveringregionlastflushedsequenceid hregion  makes inefficient use of keyset iterator instead of entryset iterator      for (byte[] columnfamily : maxseqidinstores.keyset()) {       long storeseqidforreplay = maxseqidinstores.get(columnfamily); maxseqidinstores.entryset() should have been used. ",
        "label": 441
    },
    {
        "text": "document how and why to do a manual region split  ---------- forwarded message ----------  from: liu, ming (hpit-gadsc) <ming.liu2@hp.com>  date: tue, aug 5, 2014 at 11:28 pm  subject: why hbase need manual split?  to: \"user@hbase.apache.org\" <user@hbase.apache.org> hi, all, as i understand, hbase will automatically split a region when the region is too big.  so in what scenario, user needs to do a manual split? could someone kindly give me some examples that user need to do the region split explicitly via hbase shell or java api? thanks very much. regards,  ming ",
        "label": 330
    },
    {
        "text": "regionserver wouldn't go down because split happened exactly at same time we issued bulk user region close call on our way out  a regionserver wouldn't go down because it was waiting on a user region to close only the user-space region had just been opened as part of a split transaction \u2013 it was a new daughter \u2013 just as we'd issued the bulk close to all user regions on receipt of a cluster shutdown call. we need to add a check for this condition \u2013 user tables that did not get the close. ",
        "label": 314
    },
    {
        "text": "test org apache hadoop hbase backup example testzookeepertablearchiveclient testmultipletables flaps  https://builds.apache.org/job/hbase-trunk/3293/ error message archived hfiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/othertable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/othertable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c] stacktrace java.lang.assertionerror: archived hfiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/othertable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/othertable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]  at org.junit.assert.fail(assert.java:93)  at org.junit.assert.asserttrue(assert.java:43)  at org.junit.assert.assertnull(assert.java:551)  at org.apache.hadoop.hbase.backup.example.testzookeepertablearchiveclient.testmultipletables(testzookeepertablearchiveclient.java:291)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) ",
        "label": 236
    },
    {
        "text": "forward port to  hbase  fix merge of mvcc and sequenceid performance regression in branch for increments   ",
        "label": 314
    }
]