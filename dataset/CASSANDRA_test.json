[
    {
        "text": "java lang runtimeexception  failed to rename xxx to yyy <description> java.lang.runtimeexception: failed to rename build\\test\\cassandra\\data;0\\system\\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\\system-schema_keyspaces-tmp-ka-5-index.db to build\\test\\cassandra\\data;0\\system\\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\\system-schema_keyspaces-ka-5-index.db at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.java:170) ~[main/:na] at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.java:154) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.java:569) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.java:561) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.java:535) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.finish(sstablewriter.java:470) ~[main/:na] at org.apache.cassandra.io.sstable.sstablerewriter.finishandmaybethrow(sstablerewriter.java:349) ~[main/:na] at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewriter.java:324) ~[main/:na] at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewriter.java:304) ~[main/:na] at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:200) ~[main/:na] at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na] at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:75) ~[main/:na] at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[main/:na] at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:226) ~[main/:na] at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_45] at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_45] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_45] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_45] at java.lang.thread.run(thread.java:744) [na:1.7.0_45] caused by: java.nio.file.filesystemexception: build\\test\\cassandra\\data;0\\system\\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\\system-schema_keyspaces-tmp-ka-5-index.db -> build\\test\\cassandra\\data;0\\system\\schema_keyspaces-b0f2235744583cdb9631c43e59ce3676\\system-schema_keyspaces-ka-5-index.db: the process cannot access the file because it is being used by another process. at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.java:86) ~[na:1.7.0_45] at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.java:97) ~[na:1.7.0_45] at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0_45] at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.java:287) ~[na:1.7.0_45] at java.nio.file.files.move(files.java:1345) ~[na:1.7.0_45] at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileutils.java:184) ~[main/:na] at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.java:166) ~[main/:na] ... 18 common frames omitted<stacktrace> java.lang.runtimeexception: failed to rename build/test/cassandra/data;0/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-tmp-ka-5-index.db to build/test/cassandra/data;0/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-5-index.db at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.java:170) ~[main/:na] at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.java:154) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.java:569) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.java:561) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.java:535) ~[main/:na] at org.apache.cassandra.io.sstable.sstablewriter.finish(sstablewriter.java:470) ~[main/:na] at org.apache.cassandra.io.sstable.sstablerewriter.finishandmaybethrow(sstablerewriter.java:349) ~[main/:na] at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewriter.java:324) ~[main/:na] at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewriter.java:304) ~[main/:na] at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:200) ~[main/:na] at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na] at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:75) ~[main/:na] at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[main/:na] at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:226) ~[main/:na] at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_45] at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_45] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_45] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_45] at java.lang.thread.run(thread.java:744) [na:1.7.0_45] caused by: java.nio.file.filesystemexception: build/test/cassandra/data;0/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-tmp-ka-5-index.db -> build/test/cassandra/data;0/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-5-index.db: the process cannot access the file because it is being used by another process. at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.java:86) ~[na:1.7.0_45] at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.java:97) ~[na:1.7.0_45] at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0_45] at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.java:287) ~[na:1.7.0_45] at java.nio.file.files.move(files.java:1345) ~[na:1.7.0_45] at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileutils.java:184) ~[main/:na] at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.java:166) ~[main/:na] ... 18 common frames omitted<code> <text> ",
        "label": 280
    },
    {
        "text": "sstablemetadata command should print some more stuff <description> it would be nice if the sstablemetadata command printed out some more of the stuff we track. like the min/max column names and the min/max token in the file.<stacktrace> <code> <text> it would be nice if the sstablemetadata command printed out some more of the stuff we track. like the min/max column names and the min/max token in the file.",
        "label": 321
    },
    {
        "text": "lww bug in materialized views <description> materializedviewlongtest is flakey. turns out it happens when the same timestamp is used for two writes. mv doesn't resolve this properly so you get unexpected results<stacktrace> <code> <text> materializedviewlongtest is flakey. turns out it happens when the same timestamp is used for two writes. mv doesn't resolve this properly so you get unexpected results",
        "label": 521
    },
    {
        "text": "rangetombstonetest testoverwritestodeletedcolumns failed in <description> expected:<1> but was:<2> http://cassci.datastax.com/job/cassandra-2.2_utest/56/testreport/org.apache.cassandra.db/rangetombstonetest/testoverwritestodeletedcolumns/<stacktrace> <code> expected:<1> but was:<2> http://cassci.datastax.com/job/cassandra-2.2_utest/56/testreport/org.apache.cassandra.db/rangetombstonetest/testoverwritestodeletedcolumns/<text> ",
        "label": 52
    },
    {
        "text": "eclipse warnings on cassandra <description> cassandra-9425 commit triggered a few eclipse-warnings on http://cassci.datastax.com/job/trunk_eclipse-warnings/1181/ cc: [~iamaleksey] 23:50:23 eclipse-warnings: 23:50:23     [mkdir] created dir: /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/build/ecj 23:50:23      [echo] running eclipse code analysis.  output logged to /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/build/ecj/eclipse_compiler_checks.txt 23:50:34      [java] ---------- 23:50:34      [java] 1. error in /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/src/java/org/apache/cassandra/cache/ohcprovider.java (at line 138) 23:50:34      [java]  throw new runtimeexception(e); 23:50:34      [java]  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 23:50:34      [java] potential resource leak: 'dataoutput' may not be closed at this location 23:50:34      [java] ---------- 23:50:34      [java] 2. error in /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/src/java/org/apache/cassandra/cache/ohcprovider.java (at line 159) 23:50:34      [java]  throw new runtimeexception(e); 23:50:34      [java]  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 23:50:34      [java] potential resource leak: 'datainput' may not be closed at this location 23:50:34      [java] ---------- 23:50:34      [java] 3. error in /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/src/java/org/apache/cassandra/cache/ohcprovider.java (at line 163) 23:50:34      [java]  return new rowcachekey(tableid, indexname, key); 23:50:34      [java]  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 23:50:34      [java] potential resource leak: 'datainput' may not be closed at this location 23:50:34      [java] ---------- 23:50:34      [java] 3 problems (3 errors) 23:50:34  23:50:34 build failed<stacktrace> <code> 23:50:23 eclipse-warnings: 23:50:23     [mkdir] created dir: /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/build/ecj 23:50:23      [echo] running eclipse code analysis.  output logged to /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/build/ecj/eclipse_compiler_checks.txt 23:50:34      [java] ---------- 23:50:34      [java] 1. error in /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/src/java/org/apache/cassandra/cache/ohcprovider.java (at line 138) 23:50:34      [java]  throw new runtimeexception(e); 23:50:34      [java]  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 23:50:34      [java] potential resource leak: 'dataoutput' may not be closed at this location 23:50:34      [java] ---------- 23:50:34      [java] 2. error in /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/src/java/org/apache/cassandra/cache/ohcprovider.java (at line 159) 23:50:34      [java]  throw new runtimeexception(e); 23:50:34      [java]  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 23:50:34      [java] potential resource leak: 'datainput' may not be closed at this location 23:50:34      [java] ---------- 23:50:34      [java] 3. error in /var/lib/jenkins/jobs/trunk_eclipse-warnings/workspace/src/java/org/apache/cassandra/cache/ohcprovider.java (at line 163) 23:50:34      [java]  return new rowcachekey(tableid, indexname, key); 23:50:34      [java]  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 23:50:34      [java] potential resource leak: 'datainput' may not be closed at this location 23:50:34      [java] ---------- 23:50:34      [java] 3 problems (3 errors) 23:50:34  23:50:34 build failed cassandra-9425 commit triggered a few eclipse-warnings on http://cassci.datastax.com/job/trunk_eclipse-warnings/1181/ cc: [~iamaleksey]<text> ",
        "label": 453
    },
    {
        "text": "add mbean to adjust log4j output level at runtime <description> this should probably take string classqualifier and string level (use level.tolevel to convert to a level)<stacktrace> <code> <text> this should probably take string classqualifier and string level (use level.tolevel to convert to a level)",
        "label": 274
    },
    {
        "text": "dtest failure in write failures test testwritefailures test thrift <description> example failure: http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/14/testreport/write_failures_test/testwritefailures/test_thrift failure is unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,127 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-2-big-data.db as it does not exist unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,334 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-15-big-data.db as it does not exist unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,337 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-31-big-data.db as it does not exist unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,339 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-18-big-data.db as it does not exist<stacktrace> <code> unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,127 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-2-big-data.db as it does not exist unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,334 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-15-big-data.db as it does not exist unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,337 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-31-big-data.db as it does not exist unexpected error in node3 log, error:  error [nonperiodictasks:1] 2016-07-20 07:09:52,339 logtransaction.java:205 - unable to delete /tmp/dtest-cspefg/test/node3/data2/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-18-big-data.db as it does not exist http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/14/testreport/write_failures_test/testwritefailures/test_thrift <text> example failure: failure is",
        "label": 508
    },
    {
        "text": "tuple relation in select query is broken <description> cassandra version 2.011 (and 2.0.12) 1) login to cqlsh 2) create a keyspace as below: create table test1 (campid int,grpid int,itmtypeid int, itemid int, primary key((campid, grpid), itmtypeid, itemid)); 3) insert 10 rows 4) run select * from test1 and it works 5) run a select query as below: select * from test1 where token(campid, grpid) = token(987654321, 4) and (itmtypeid, itemid) > (12347, 1234567812); it fails with error 'tsocket read 0 bytes' 6) the actual error in the cassandra server log is as below: error [thrift:2] 2015-02-13 16:22:41,269 customtthreadpoolserver.java (line 222) error occurred during processing of message. java.lang.classcastexception: org.apache.cassandra.cql3.multicolumnrelation cannot be cast to org.apache.cassandra.cql3.singlecolumnrelation  at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.checktokenfunctionargumentsorder(selectstatement.java:1887) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.processpartitionkeyrestrictions(selectstatement.java:1873) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:1494) at org.apache.cassandra.cql3.queryprocessor.getstatement(queryprocessor.java:333) at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:171) at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1958) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4486) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4470) at org.apache.thrift.processfunction.process(processfunction.java:39) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:204) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:745)<stacktrace> error [thrift:2] 2015-02-13 16:22:41,269 customtthreadpoolserver.java (line 222) error occurred during processing of message. java.lang.classcastexception: org.apache.cassandra.cql3.multicolumnrelation cannot be cast to org.apache.cassandra.cql3.singlecolumnrelation  at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.checktokenfunctionargumentsorder(selectstatement.java:1887) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.processpartitionkeyrestrictions(selectstatement.java:1873) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:1494) at org.apache.cassandra.cql3.queryprocessor.getstatement(queryprocessor.java:333) at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:171) at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1958) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4486) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4470) at org.apache.thrift.processfunction.process(processfunction.java:39) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:204) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:745) <code> cassandra version 2.011 (and 2.0.12) 1) login to cqlsh create table test1 (campid int,grpid int,itmtypeid int, itemid int, primary key((campid, grpid), itmtypeid, itemid)); select * from test1 where token(campid, grpid) = token(987654321, 4) and (itmtypeid, itemid) > (12347, 1234567812); <text> 2) create a keyspace as below: 3) insert 10 rows 4) run select * from test1 and it works 5) run a select query as below: it fails with error 'tsocket read 0 bytes' 6) the actual error in the cassandra server log is as below:",
        "label": 538
    },
    {
        "text": "support custom secondary indexes in cql <description> through thrift users can add custom secondary indexes to the column metadata. the following syntax is used in plsql, and i think we could use something similar. create index <name> on <table> (<column>) [indextype is (<typename>) [parameters (<param>[, <param>])]<stacktrace> <code> <text> through thrift users can add custom secondary indexes to the column metadata. the following syntax is used in plsql, and i think we could use something similar. create index <name> on <table> (<column>) [indextype is (<typename>) [parameters (<param>[, <param>])]",
        "label": 18
    },
    {
        "text": "streamout fails to start an empty stream <description> streamout only starts a stream if there are actually files to transfer. this means callbacks will never get called for streams that don't actually have anything to transfer.<stacktrace> <code> <text> streamout only starts a stream if there are actually files to transfer. this means callbacks will never get called for streams that don't actually have anything to transfer.",
        "label": 274
    },
    {
        "text": "stop and restart fail in debian buster <description> debian 10 \"buster\" has an updated start-stop-daemon version that causes init script to fail to stop or restart cassandra daemon. stop option do not stops first launched instance and depending of previous operations, restart and start could launch a second cassandra daemon instance. start-stop-daemon since version 1.19.3 fail to stop matching only a pidfile that can be writen by an unprivileged (non-root) user because it is considered a security risk. adding a second matching option (user) solved this. this fix does not break compatibility with older versions and does not mess with pidfile's mode or permissions. related: cassandra-15099 https://github.com/vice/cassandra/tree/debian-buster<stacktrace> <code> related: cassandra-15099 https://github.com/vice/cassandra/tree/debian-buster<text> debian 10 'buster' has an updated start-stop-daemon version that causes init script to fail to stop or restart cassandra daemon. stop option do not stops first launched instance and depending of previous operations, restart and start could launch a second cassandra daemon instance. start-stop-daemon since version 1.19.3 fail to stop matching only a pidfile that can be writen by an unprivileged (non-root) user because it is considered a security risk. adding a second matching option (user) solved this. this fix does not break compatibility with older versions and does not mess with pidfile's mode or permissions. ",
        "label": 554
    },
    {
        "text": "nodetool repair on cassandra indexed tables returns java exception about creating snapshots <description> running a nodetool repair on cassandra 2.1.0 indexed tables returns java exception about creating snapshots: command line: [2014-09-29 11:25:24,945] repair session 73c0d390-47e4-11e4-ba0f-c7788dc924ec for range (-7298689860784559350,-7297558156602685286] failed with error java.io.ioexception: failed during snapshot creation. [2014-09-29 11:25:24,945] repair command #5 finished cassandra log: error [thread-49681] 2014-09-29 11:25:24,945 storageservice.java:2689 - repair session 73c0d390-47e4-11e4-ba0f-c7788dc924ec for range (-7298689860784559350,-7297558156602685286] failed with error java.io.ioexception: failed during snapshot creation. java.util.concurrent.executionexception: java.lang.runtimeexception: java.io.ioexception: failed during snapshot creation.         at java.util.concurrent.futuretask.report(futuretask.java:122) [na:1.7.0_67]         at java.util.concurrent.futuretask.get(futuretask.java:188) [na:1.7.0_67]         at org.apache.cassandra.service.storageservice$4.runmaythrow(storageservice.java:2680) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [apache-cassandra-2.1.0.jar:2.1.0]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) [na:1.7.0_67]         at java.lang.thread.run(thread.java:745) [na:1.7.0_67] caused by: java.lang.runtimeexception: java.io.ioexception: failed during snapshot creation.         at com.google.common.base.throwables.propagate(throwables.java:160) ~[guava-16.0.jar:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:32) [apache-cassandra-2.1.0.jar:2.1.0]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) [na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) ~[na:1.7.0_67]         ... 1 common frames omitted caused by: java.io.ioexception: failed during snapshot creation.         at org.apache.cassandra.repair.repairsession.failedsnapshot(repairsession.java:344) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.repair.repairjob$2.onfailure(repairjob.java:128) ~[apache-cassandra-2.1.0.jar:2.1.0]         at com.google.common.util.concurrent.futures$4.run(futures.java:1172) ~[guava-16.0.jar:na]         ... 3 common frames omitted if the index is dropped, the repair returns no error: cqlsh:test> drop index user_pass_idx ; root@test:~# nodetool repair test user [2014-09-29 11:27:29,668] starting repair command #6, repairing 743 ranges for keyspace test (seq=true, full=true) . . [2014-09-29 11:28:38,030] repair session e6d40e10-47e4-11e4-ba0f-c7788dc924ec for range (-7298689860784559350,-7297558156602685286] finished [2014-09-29 11:28:38,030] repair command #6 finished the test table: create table test.user (     login text primary key,     password text ) create index user_pass_idx on test.user (password) ;<stacktrace> error [thread-49681] 2014-09-29 11:25:24,945 storageservice.java:2689 - repair session 73c0d390-47e4-11e4-ba0f-c7788dc924ec for range (-7298689860784559350,-7297558156602685286] failed with error java.io.ioexception: failed during snapshot creation. java.util.concurrent.executionexception: java.lang.runtimeexception: java.io.ioexception: failed during snapshot creation.         at java.util.concurrent.futuretask.report(futuretask.java:122) [na:1.7.0_67]         at java.util.concurrent.futuretask.get(futuretask.java:188) [na:1.7.0_67]         at org.apache.cassandra.service.storageservice$4.runmaythrow(storageservice.java:2680) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [apache-cassandra-2.1.0.jar:2.1.0]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) [na:1.7.0_67]         at java.lang.thread.run(thread.java:745) [na:1.7.0_67] caused by: java.lang.runtimeexception: java.io.ioexception: failed during snapshot creation.         at com.google.common.base.throwables.propagate(throwables.java:160) ~[guava-16.0.jar:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:32) [apache-cassandra-2.1.0.jar:2.1.0]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) [na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) ~[na:1.7.0_67]         ... 1 common frames omitted caused by: java.io.ioexception: failed during snapshot creation.         at org.apache.cassandra.repair.repairsession.failedsnapshot(repairsession.java:344) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.repair.repairjob$2.onfailure(repairjob.java:128) ~[apache-cassandra-2.1.0.jar:2.1.0]         at com.google.common.util.concurrent.futures$4.run(futures.java:1172) ~[guava-16.0.jar:na]         ... 3 common frames omitted <code> [2014-09-29 11:25:24,945] repair session 73c0d390-47e4-11e4-ba0f-c7788dc924ec for range (-7298689860784559350,-7297558156602685286] failed with error java.io.ioexception: failed during snapshot creation. [2014-09-29 11:25:24,945] repair command #5 finished cqlsh:test> drop index user_pass_idx ; root@test:~# nodetool repair test user [2014-09-29 11:27:29,668] starting repair command #6, repairing 743 ranges for keyspace test (seq=true, full=true) . . [2014-09-29 11:28:38,030] repair session e6d40e10-47e4-11e4-ba0f-c7788dc924ec for range (-7298689860784559350,-7297558156602685286] finished [2014-09-29 11:28:38,030] repair command #6 finished create table test.user (     login text primary key,     password text ) create index user_pass_idx on test.user (password) ; <text> running a nodetool repair on cassandra 2.1.0 indexed tables returns java exception about creating snapshots: command line: cassandra log: if the index is dropped, the repair returns no error: the test table:",
        "label": 577
    },
    {
        "text": "columnfamilyinputformat demands orderpreservingpartitioner when specifying inputrange with tokens <description> when columnfamilyinputformat starts getting splits (via getsplits(...) [columnfamilyinputformat.java:101]) it checks to see if a `jobkeyrange` has been set. if it has been set it attempts to set the `jobrange`. however the if block (columnfamilyinputformat.java:124) looks to see if the `jobkeyrange` has tokens but asserts that the orderpreservingpartitioner must be in use. this if block should be looking for keys (not tokens). code further down (columnfamilyinputformat.java:147) already manages the range if tokens are used but can never be reached.<stacktrace> <code> when columnfamilyinputformat starts getting splits (via getsplits(...) [columnfamilyinputformat.java:101]) it checks to see if a `jobkeyrange` has been set. if it has been set it attempts to set the `jobrange`. however the if block (columnfamilyinputformat.java:124) looks to see if the `jobkeyrange` has tokens but asserts that the orderpreservingpartitioner must be in use. <text> this if block should be looking for keys (not tokens). code further down (columnfamilyinputformat.java:147) already manages the range if tokens are used but can never be reached.",
        "label": 274
    },
    {
        "text": "hints are not replayed unless node was marked down <description> if b drops a write from a because it is overwhelmed (but not dead), a will hint the write. but it will never get notified that b is back up (since it was never down), so it will never attempt hint delivery.<stacktrace> <code> <text> if b drops a write from a because it is overwhelmed (but not dead), a will hint the write. but it will never get notified that b is back up (since it was never down), so it will never attempt hint delivery.",
        "label": 274
    },
    {
        "text": "offline scrub should not abort when it hits corruption <description> hit a failure on startup due to corruption of some sstables in system keyspace. deleted the listed file and restarted - came down again with another file. figured that i may as well run scrub to clean up all the files. got following error: sstablescrub system compaction_history  error 17:21:34 exiting forcefully due to file system exception on startup, disk failure policy \"stop\"  org.apache.cassandra.io.sstable.corruptsstableexception: corrupted: /cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/system-compaction_history-ka-1936-compressioninfo.db  at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:131) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.compress.compressionmetadata.create(compressionmetadata.java:85) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.util.compressedsegmentedfile$builder.metadata(compressedsegmentedfile.java:79) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:72) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.util.segmentedfile$builder.complete(segmentedfile.java:169) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:741) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:692) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:480) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046] at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:376) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046] at org.apache.cassandra.io.sstable.sstablereader$4.run(sstablereader.java:523) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_79]  at java.util.concurrent.futuretask.run(futuretask.java:262) [na:1.7.0_79]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [na:1.7.0_79]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_79]  at java.lang.thread.run(thread.java:745) [na:1.7.0_79]  caused by: java.io.eofexception: null  at java.io.datainputstream.readunsignedshort(datainputstream.java:340) ~[na:1.7.0_79]  at java.io.datainputstream.readutf(datainputstream.java:589) ~[na:1.7.0_79]  at java.io.datainputstream.readutf(datainputstream.java:564) ~[na:1.7.0_79]  at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:106) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  ... 14 common frames omitted  i guess it might be by design - but i'd argue that i should at least have the option to continue and let it do it's thing. i'd prefer that sstablescrub ignored the disk failure policy.<stacktrace> sstablescrub system compaction_history  error 17:21:34 exiting forcefully due to file system exception on startup, disk failure policy 'stop'  org.apache.cassandra.io.sstable.corruptsstableexception: corrupted: /cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/system-compaction_history-ka-1936-compressioninfo.db  at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:131) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.compress.compressionmetadata.create(compressionmetadata.java:85) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.util.compressedsegmentedfile$builder.metadata(compressedsegmentedfile.java:79) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:72) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.util.segmentedfile$builder.complete(segmentedfile.java:169) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:741) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:692) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:480) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046] at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:376) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046] at org.apache.cassandra.io.sstable.sstablereader$4.run(sstablereader.java:523) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_79]  at java.util.concurrent.futuretask.run(futuretask.java:262) [na:1.7.0_79]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [na:1.7.0_79]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_79]  at java.lang.thread.run(thread.java:745) [na:1.7.0_79]  caused by: java.io.eofexception: null  at java.io.datainputstream.readunsignedshort(datainputstream.java:340) ~[na:1.7.0_79]  at java.io.datainputstream.readutf(datainputstream.java:589) ~[na:1.7.0_79]  at java.io.datainputstream.readutf(datainputstream.java:564) ~[na:1.7.0_79]  at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:106) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]  ... 14 common frames omitted  <code> <text> hit a failure on startup due to corruption of some sstables in system keyspace. deleted the listed file and restarted - came down again with another file. figured that i may as well run scrub to clean up all the files. got following error: i guess it might be by design - but i'd argue that i should at least have the option to continue and let it do it's thing. i'd prefer that sstablescrub ignored the disk failure policy.",
        "label": 577
    },
    {
        "text": "windows dtest  sstablesplit test py testsstablesplit split test <description> error text:  file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 358, in run     self.teardown()   file \"d:\\jenkins\\workspace\\cassandra-2.2_dtest_win32\\cassandra-dtest\\dtest.py\", line 513, in teardown     self._cleanup_cluster()   file \"d:\\jenkins\\workspace\\cassandra-2.2_dtest_win32\\cassandra-dtest\\dtest.py\", line 212, in _cleanup_cluster     self.cluster.remove()   file \"d:\\jenkins\\workspace\\cassandra-2.2_dtest_win32\\ccm\\ccmlib\\cluster.py\", line 223, in remove     common.rmdirs(self.get_path())   file \"d:\\jenkins\\workspace\\cassandra-2.2_dtest_win32\\ccm\\ccmlib\\common.py\", line 156, in rmdirs     shutil.rmtree(u\"\\\\\\\\?\\\\\" + path)   file \"c:\\tools\\python2\\lib\\shutil.py\", line 247, in rmtree     rmtree(fullname, ignore_errors, onerror)   file \"c:\\tools\\python2\\lib\\shutil.py\", line 247, in rmtree     rmtree(fullname, ignore_errors, onerror)   file \"c:\\tools\\python2\\lib\\shutil.py\", line 252, in rmtree     onerror(os.remove, fullname, sys.exc_info())   file \"c:\\tools\\python2\\lib\\shutil.py\", line 250, in rmtree     os.remove(fullname) \"[error 5] access is denied: u'\\\\\\\\\\\\\\\\?\\\\\\\\d:\\\\\\\\temp\\\\\\\\dtest-zsl_4c\\\\\\\\test\\\\\\\\node1\\\\\\\\commitlogs\\\\\\\\commitlog-5-1439489973883.log'\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: d:\\\\temp\\\\dtest-zsl_4c\\ndtest: debug: run stress to insert data\\ndtest: debug: compact sstables.\\ndtest: debug: number of sstables after compaction: 1\\ndtest: debug: run sstablesplit\\ndtest: debug: original sstable and sizes before split: [('d:\\\\\\\\temp\\\\\\\\dtest-zsl_4c\\\\\\\\test\\\\\\\\node1\\\\\\\\data\\\\\\\\keyspace1\\\\\\\\standard1-e596155041e611e5b337390aedd65e00\\\\\\\\la-25-big-data.db', 281000000l)]\\ndtest: debug: number of sstables after split: 27. expected 26.0\\ndtest: debug: compact sstables.\\ndtest: debug: number of sstables after compaction: 1\\ndtest: debug: run sstablesplit\\ndtest: debug: original sstable and sizes before split: [('d:\\\\\\\\temp\\\\\\\\dtest-zsl_4c\\\\\\\\test\\\\\\\\node1\\\\\\\\data\\\\\\\\keyspace1\\\\\\\\standard1-e596155041e611e5b337390aedd65e00\\\\\\\\la-54-big-data.db', 281000000l)]\\ndtest: debug: number of sstables after split: 27. expected 26.0\\ndtest: debug: run stress to ensure data is readable\\ndtest: debug: removing ccm cluster test at: d:\\\\temp\\\\dtest-zsl_4c\\n--------------------- >> end captured logging << ----- failure history env: ci only. passes locally.<stacktrace> <code> <text>  file 'c:/tools/python2/lib/unittest/case.py', line 358, in run     self.teardown()   file 'd:/jenkins/workspace/cassandra-2.2_dtest_win32/cassandra-dtest/dtest.py', line 513, in teardown     self._cleanup_cluster()   file 'd:/jenkins/workspace/cassandra-2.2_dtest_win32/cassandra-dtest/dtest.py', line 212, in _cleanup_cluster     self.cluster.remove()   file 'd:/jenkins/workspace/cassandra-2.2_dtest_win32/ccm/ccmlib/cluster.py', line 223, in remove     common.rmdirs(self.get_path())   file 'd:/jenkins/workspace/cassandra-2.2_dtest_win32/ccm/ccmlib/common.py', line 156, in rmdirs     shutil.rmtree(u'////?//' + path)   file 'c:/tools/python2/lib/shutil.py', line 247, in rmtree     rmtree(fullname, ignore_errors, onerror)   file 'c:/tools/python2/lib/shutil.py', line 247, in rmtree     rmtree(fullname, ignore_errors, onerror)   file 'c:/tools/python2/lib/shutil.py', line 252, in rmtree     onerror(os.remove, fullname, sys.exc_info())   file 'c:/tools/python2/lib/shutil.py', line 250, in rmtree     os.remove(fullname) '[error 5] access is denied: u'////////?////d:////temp////dtest-zsl_4c////test////node1////commitlogs////commitlog-5-1439489973883.log'/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: d://temp//dtest-zsl_4c/ndtest: debug: run stress to insert data/ndtest: debug: compact sstables./ndtest: debug: number of sstables after compaction: 1/ndtest: debug: run sstablesplit/ndtest: debug: original sstable and sizes before split: [('d:////temp////dtest-zsl_4c////test////node1////data////keyspace1////standard1-e596155041e611e5b337390aedd65e00////la-25-big-data.db', 281000000l)]/ndtest: debug: number of sstables after split: 27. expected 26.0/ndtest: debug: compact sstables./ndtest: debug: number of sstables after compaction: 1/ndtest: debug: run sstablesplit/ndtest: debug: original sstable and sizes before split: [('d:////temp////dtest-zsl_4c////test////node1////data////keyspace1////standard1-e596155041e611e5b337390aedd65e00////la-54-big-data.db', 281000000l)]/ndtest: debug: number of sstables after split: 27. expected 26.0/ndtest: debug: run stress to ensure data is readable/ndtest: debug: removing ccm cluster test at: d://temp//dtest-zsl_4c/n--------------------- >> end captured logging << ----- error text: failure history env: ci only. passes locally.",
        "label": 280
    },
    {
        "text": "cli should accept    and or some other char  as a comment character in files passed with  f <description> the cli doesn't allow comments in files passed in with -f<stacktrace> <code> <text> the cli doesn't allow comments in files passed in with -f",
        "label": 412
    },
    {
        "text": "oom due to heapbytebuffer instances <description> cassandra 3.0.2 fails with oom. the heapdump shows large number of heapbytebuffer instances, each retaining 1mb (see the details on the screenshot). overall retained size is ~2gb. we can provide the additional info and the whole heapdump if necessary.<stacktrace> <code> <text> cassandra 3.0.2 fails with oom. the heapdump shows large number of heapbytebuffer instances, each retaining 1mb (see the details on the screenshot). overall retained size is ~2gb. we can provide the additional info and the whole heapdump if necessary.",
        "label": 520
    },
    {
        "text": "add 'null' support to cql <description> dense composite supports adding records where only a prefix of all the component specifying the key is defined. in other words, with: create table connections (    userid int,    ip text,    port int,    protocol text,    time timestamp,    primary key (userid, ip, port, protocol) ) with compact storage you can insert insert into connections (userid, ip, port, time) values (2, '192.168.0.1', 80, 123456789); you cannot however select that column specifically (i.e, without selecting column (2, '192.168.0.1', 80, 'http') for instance).  this ticket proposes to allow that though 'null', i.e. to allow select * from connections where userid = 2 and ip = '192.168.0.1' and port = 80 and protocol = null; it would then also make sense to support: insert into connections (userid, ip, port, protocol, time) values (2, '192.168.0.1', 80, null, 123456789); as an equivalent to the insert query above.<stacktrace> <code> create table connections (    userid int,    ip text,    port int,    protocol text,    time timestamp,    primary key (userid, ip, port, protocol) ) with compact storage insert into connections (userid, ip, port, time) values (2, '192.168.0.1', 80, 123456789); select * from connections where userid = 2 and ip = '192.168.0.1' and port = 80 and protocol = null; insert into connections (userid, ip, port, protocol, time) values (2, '192.168.0.1', 80, null, 123456789); <text> dense composite supports adding records where only a prefix of all the component specifying the key is defined. in other words, with: you can insert you cannot however select that column specifically (i.e, without selecting column (2, '192.168.0.1', 80, 'http') for instance).  this ticket proposes to allow that though 'null', i.e. to allow it would then also make sense to support: as an equivalent to the insert query above.",
        "label": 352
    },
    {
        "text": "sstablereversediterator ignores range tombstones <description> <stacktrace> <code> <text> ",
        "label": 520
    },
    {
        "text": "request specific column families using streamin <description> streamin.requestranges only specifies a keyspace, meaning that requesting a range will request it for all column families: if you have a large number of cfs, this can cause quite a headache.<stacktrace> <code> <text> streamin.requestranges only specifies a keyspace, meaning that requesting a range will request it for all column families: if you have a large number of cfs, this can cause quite a headache.",
        "label": 274
    },
    {
        "text": "deletions using clustering keys not reflected in mv <description> i wrote a test to reproduce an issue reported on so and turns out this is easily reproducible. there seems to be a bug preventing deletes to be propagated to mvs in case a clustering key is used. see here for test case (testclusteringkeytombstone should fail). it seems materializedview.updateaffectsview() will not consider the delete relevant for the view as partition.deletioninfo().islive() will be true during the test. in other test cases islive will return false, which seems to be the actual problem here. i'm not even sure the root cause is mv specific, but wasn't able to dig much deeper as i'm not familiar with the slightly confusing semantics around deletioninfo.<stacktrace> <code> <text> i wrote a test to reproduce an issue reported on so and turns out this is easily reproducible. there seems to be a bug preventing deletes to be propagated to mvs in case a clustering key is used. see here for test case (testclusteringkeytombstone should fail). it seems materializedview.updateaffectsview() will not consider the delete relevant for the view as partition.deletioninfo().islive() will be true during the test. in other test cases islive will return false, which seems to be the actual problem here. i'm not even sure the root cause is mv specific, but wasn't able to dig much deeper as i'm not familiar with the slightly confusing semantics around deletioninfo.",
        "label": 98
    },
    {
        "text": "add cql support for batchlog <description> need to expose the equivalent of atomic_batch_mutate (cassandra-4542) to cql3.<stacktrace> <code> <text> need to expose the equivalent of atomic_batch_mutate (cassandra-4542) to cql3.",
        "label": 18
    },
    {
        "text": "datacentershardstrategytest testproperties fails <description> stacktrace  junit.framework.assertionfailederror  at org.apache.cassandra.locator.datacentershardstrategytest.testproperties(datacentershardstrategytest.java:36) standard error  error 12:39:35,968 could not find end point information for 127.0.0.1, will use default. http://hudson.zones.apache.org/hudson/job/cassandra/440/testreport/junit/org.apache.cassandra.locator/datacentershardstrategytest/testproperties/<stacktrace> stacktrace  junit.framework.assertionfailederror  at org.apache.cassandra.locator.datacentershardstrategytest.testproperties(datacentershardstrategytest.java:36) <code> http://hudson.zones.apache.org/hudson/job/cassandra/440/testreport/junit/org.apache.cassandra.locator/datacentershardstrategytest/testproperties/<text> standard error  error 12:39:35,968 could not find end point information for 127.0.0.1, will use default. ",
        "label": 274
    },
    {
        "text": "nodetool repair triggers oom <description> customer has a 3 node cluster with 500mb data on each node [cassandra@nbcqa-chc-a02 ~]$ nodetool status note: ownership information does not include topology; for complete information, specify a keyspace datacenter: ch2 =============== status=up/down |/ state=normal/leaving/joining/moving --  address        load       tokens  owns   host id                               rack un  162.150.4.234  255.26 mb  256     33.2%  4ad1b6a8-8759-4920-b54a-f059126900df  rac1 un  162.150.4.235  318.37 mb  256     32.6%  3eb0ec58-4b81-442e-bee5-4c91da447f38  rac1 un  162.150.4.167  243.7 mb   256     34.2%  5b2c1900-bf03-41c1-bb4e-82df1655b8d8  rac1 [cassandra@nbcqa-chc-a02 ~]$ after we run repair command, system runs into oom after some 45 minutes  nothing else is running [cassandra@nbcqa-chc-a02 ~]$ date fri sep 19 15:55:33 utc 2014 [cassandra@nbcqa-chc-a02 ~]$ nodetool repair -st -9220354588320251877 -et -9220354588320251873 sep 19, 2014 4:06:08 pm clientcommunicatoradmin checker-run warning: failed to check the connection: java.net.sockettimeoutexception: read timed out here is when we run oom error [readstage:28914] 2014-09-19 16:34:50,381 cassandradaemon.java (line 199) exception in thread thread[readstage:28914,5,main] java.lang.outofmemoryerror: java heap space         at org.apache.cassandra.io.util.randomaccessreader.<init>(randomaccessreader.java:69)         at org.apache.cassandra.io.compress.compressedrandomaccessreader.<init>(compressedrandomaccessreader.java:76)         at org.apache.cassandra.io.compress.compressedrandomaccessreader.open(compressedrandomaccessreader.java:43)         at org.apache.cassandra.io.util.compressedpoolingsegmentedfile.createreader(compressedpoolingsegmentedfile.java:48)         at org.apache.cassandra.io.util.poolingsegmentedfile.getsegment(poolingsegmentedfile.java:39)         at org.apache.cassandra.io.sstable.sstablereader.getfiledatainput(sstablereader.java:1195)         at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:57)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:65)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:42)         at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:167)         at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:62)         at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:250)         at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1547)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1376)         at org.apache.cassandra.db.keyspace.getrow(keyspace.java:333)         at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)         at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1363)         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1927)         at java.util.concurrent.threadpoolexecutor.runworker(unknown source)         at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)         at java.lang.thread.run(unknown source) cassandra process pegs 1 of the 8 cpu's 100% top - 16:50:07 up 11 days,  2:01,  2 users,  load average: 0.54, 0.60, 0.65 tasks: 175 total,   1 running, 174 sleeping,   0 stopped,   0 zombie cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu1  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu2  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu3  :  0.7%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu4  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu5  :  0.3%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.3%st cpu6  :  0.0%us,  0.3%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu7  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st mem:  16332056k total, 16167212k used,   164844k free,   149956k buffers swap:        0k total,        0k used,        0k free,  8360056k cached   pid user      pr  ni  virt  res  shr s %cpu %mem    time+  command  2161 cassandr  20   0 11.5g 6.9g 227m s 107.8 44.0 281:59.49 java  9942 root      20   0  106m 2320 1344 s  1.0  0.0   0:00.03 dhclient-script 28969 opscente  20   0 4479m 188m  12m s  0.7  1.2  56:24.24 java  5123 cassandr  20   0 1788m 107m  28m s  0.3  0.7   0:08.09 java     1 root      20   0 19228 1352 1072 s  0.0  0.0   0:00.82 init     2 root      20   0     0    0    0 s  0.0  0.0   0:00.02 kthreadd     3 root      rt   0     0    0    0 s  0.0  0.0   0:05.52 migration/0     4 root      20   0     0    0    0 s  0.0  0.0   0:13.15 ksoftirqd/0     5 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/0     6 root      rt   0     0    0    0 s  0.0  0.0   0:03.33 watchdog/0     7 root      rt   0     0    0    0 s  0.0  0.0   0:04.88 migration/1     8 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/1     9 root      20   0     0    0    0 s  0.0  0.0   0:19.21 ksoftirqd/1    10 root      rt   0     0    0    0 s  0.0  0.0   0:03.24 watchdog/1    11 root      rt   0     0    0    0 s  0.0  0.0   0:05.46 migration/2    12 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/2    13 root      20   0     0    0    0 s  0.0  0.0   0:16.87 ksoftirqd/2    14 root      rt   0     0    0    0 s  0.0  0.0   0:03.49 watchdog/2    15 root      rt   0     0    0    0 s  0.0  0.0   0:05.31 migration/3    16 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/3    17 root      20   0     0    0    0 s  0.0  0.0   0:19.33 ksoftirqd/3    18 root      rt   0     0    0    0 s  0.0  0.0   0:03.43 watchdog/3    19 root      rt   0     0    0    0 s  0.0  0.0   0:05.36 migration/4    20 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/4    21 root      20   0     0    0    0 s  0.0  0.0   0:17.64 ksoftirqd/4    22 root      rt   0     0    0    0 s  0.0  0.0   0:03.18 watchdog/4    23 root      rt   0     0    0    0 s  0.0  0.0   0:05.31 migration/5 there is a 12gb heapdump, the memory leak suspects show the following trace  thread stack rmi tcp connection(1621)-162.150.4.235   at org.apache.cassandra.service.storageservice.createrepairrangefrom(ljava/lang/string;ljava/lang/string;)ljava/util/collection; (storageservice.java:2612)   at org.apache.cassandra.service.storageservice.forcerepairrangeasync(ljava/lang/string;ljava/lang/string;ljava/lang/string;zljava/util/collection;ljava/util/collection;[ljava/lang/string;)i (storageservice.java:2541)   at sun.reflect.nativemethodaccessorimpl.invoke0(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (native method)   at sun.reflect.nativemethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at java.lang.reflect.method.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.misc.trampoline.invoke(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.generatedmethodaccessor8.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at java.lang.reflect.method.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.misc.methodutil.invoke(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(ljava/lang/object;ljava/lang/object;[ljava/lang/object;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(ljava/lang/object;ljava/lang/object;[ljava/lang/object;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.perinterface.invoke(ljava/lang/object;ljava/lang/string;[ljava/lang/object;[ljava/lang/string;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.mbeansupport.invoke(ljava/lang/string;[ljava/lang/object;[ljava/lang/string;)ljava/lang/object; (unknown source)   at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(ljavax/management/objectname;ljava/lang/string;[ljava/lang/object;[ljava/lang/string;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(ljavax/management/objectname;ljava/lang/string;[ljava/lang/object;[ljava/lang/string;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.dooperation(i[ljava/lang/object;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.access$300(ljavax/management/remote/rmi/rmiconnectionimpl;i[ljava/lang/object;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run()ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(i[ljava/lang/object;ljavax/security/auth/subject;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.invoke(ljavax/management/objectname;ljava/lang/string;ljava/rmi/marshalledobject;[ljava/lang/string;ljavax/security/auth/subject;)ljava/lang/object; (unknown source)   at sun.reflect.generatedmethodaccessor37.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at java.lang.reflect.method.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.rmi.server.unicastserverref.dispatch(ljava/rmi/remote;ljava/rmi/server/remotecall;)v (unknown source)   at sun.rmi.transport.transport$1.run()ljava/lang/void; (unknown source)   at sun.rmi.transport.transport$1.run()ljava/lang/object; (unknown source)   at java.security.accesscontroller.doprivileged(ljava/security/privilegedexceptionaction;ljava/security/accesscontrolcontext;)ljava/lang/object; (native method)   at sun.rmi.transport.transport.servicecall(ljava/rmi/server/remotecall;)z (unknown source)   at sun.rmi.transport.tcp.tcptransport.handlemessages(lsun/rmi/transport/connection;z)v (unknown source)   at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0()v (unknown source)   at sun.rmi.transport.tcp.tcptransport$connectionhandler.run()v (unknown source)   at java.util.concurrent.threadpoolexecutor.runworker(ljava/util/concurrent/threadpoolexecutor$worker;)v (unknown source)   at java.util.concurrent.threadpoolexecutor$worker.run()v (unknown source)   at java.lang.thread.run()v (unknown source) the file is way too large to be attached here.   it's currently held at my scp server  let us know if there is any other info you may need  i will be posting the other 2 system logs as soon as i get them<stacktrace> error [readstage:28914] 2014-09-19 16:34:50,381 cassandradaemon.java (line 199) exception in thread thread[readstage:28914,5,main] java.lang.outofmemoryerror: java heap space         at org.apache.cassandra.io.util.randomaccessreader.<init>(randomaccessreader.java:69)         at org.apache.cassandra.io.compress.compressedrandomaccessreader.<init>(compressedrandomaccessreader.java:76)         at org.apache.cassandra.io.compress.compressedrandomaccessreader.open(compressedrandomaccessreader.java:43)         at org.apache.cassandra.io.util.compressedpoolingsegmentedfile.createreader(compressedpoolingsegmentedfile.java:48)         at org.apache.cassandra.io.util.poolingsegmentedfile.getsegment(poolingsegmentedfile.java:39)         at org.apache.cassandra.io.sstable.sstablereader.getfiledatainput(sstablereader.java:1195)         at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:57)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:65)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:42)         at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:167)         at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:62)         at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:250)         at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1547)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1376)         at org.apache.cassandra.db.keyspace.getrow(keyspace.java:333)         at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)         at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1363)         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1927)         at java.util.concurrent.threadpoolexecutor.runworker(unknown source)         at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)         at java.lang.thread.run(unknown source)  thread stack rmi tcp connection(1621)-162.150.4.235   at org.apache.cassandra.service.storageservice.createrepairrangefrom(ljava/lang/string;ljava/lang/string;)ljava/util/collection; (storageservice.java:2612)   at org.apache.cassandra.service.storageservice.forcerepairrangeasync(ljava/lang/string;ljava/lang/string;ljava/lang/string;zljava/util/collection;ljava/util/collection;[ljava/lang/string;)i (storageservice.java:2541)   at sun.reflect.nativemethodaccessorimpl.invoke0(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (native method)   at sun.reflect.nativemethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at java.lang.reflect.method.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.misc.trampoline.invoke(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.generatedmethodaccessor8.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at java.lang.reflect.method.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.misc.methodutil.invoke(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(ljava/lang/reflect/method;ljava/lang/object;[ljava/lang/object;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(ljava/lang/object;ljava/lang/object;[ljava/lang/object;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(ljava/lang/object;ljava/lang/object;[ljava/lang/object;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.perinterface.invoke(ljava/lang/object;ljava/lang/string;[ljava/lang/object;[ljava/lang/string;ljava/lang/object;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.mbeansupport.invoke(ljava/lang/string;[ljava/lang/object;[ljava/lang/string;)ljava/lang/object; (unknown source)   at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(ljavax/management/objectname;ljava/lang/string;[ljava/lang/object;[ljava/lang/string;)ljava/lang/object; (unknown source)   at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(ljavax/management/objectname;ljava/lang/string;[ljava/lang/object;[ljava/lang/string;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.dooperation(i[ljava/lang/object;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.access$300(ljavax/management/remote/rmi/rmiconnectionimpl;i[ljava/lang/object;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run()ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(i[ljava/lang/object;ljavax/security/auth/subject;)ljava/lang/object; (unknown source)   at javax.management.remote.rmi.rmiconnectionimpl.invoke(ljavax/management/objectname;ljava/lang/string;ljava/rmi/marshalledobject;[ljava/lang/string;ljavax/security/auth/subject;)ljava/lang/object; (unknown source)   at sun.reflect.generatedmethodaccessor37.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.reflect.delegatingmethodaccessorimpl.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at java.lang.reflect.method.invoke(ljava/lang/object;[ljava/lang/object;)ljava/lang/object; (unknown source)   at sun.rmi.server.unicastserverref.dispatch(ljava/rmi/remote;ljava/rmi/server/remotecall;)v (unknown source)   at sun.rmi.transport.transport$1.run()ljava/lang/void; (unknown source)   at sun.rmi.transport.transport$1.run()ljava/lang/object; (unknown source)   at java.security.accesscontroller.doprivileged(ljava/security/privilegedexceptionaction;ljava/security/accesscontrolcontext;)ljava/lang/object; (native method)   at sun.rmi.transport.transport.servicecall(ljava/rmi/server/remotecall;)z (unknown source)   at sun.rmi.transport.tcp.tcptransport.handlemessages(lsun/rmi/transport/connection;z)v (unknown source)   at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0()v (unknown source)   at sun.rmi.transport.tcp.tcptransport$connectionhandler.run()v (unknown source)   at java.util.concurrent.threadpoolexecutor.runworker(ljava/util/concurrent/threadpoolexecutor$worker;)v (unknown source)   at java.util.concurrent.threadpoolexecutor$worker.run()v (unknown source)   at java.lang.thread.run()v (unknown source) <code> [cassandra@nbcqa-chc-a02 ~]$ nodetool status note: ownership information does not include topology; for complete information, specify a keyspace datacenter: ch2 =============== status=up/down |/ state=normal/leaving/joining/moving --  address        load       tokens  owns   host id                               rack un  162.150.4.234  255.26 mb  256     33.2%  4ad1b6a8-8759-4920-b54a-f059126900df  rac1 un  162.150.4.235  318.37 mb  256     32.6%  3eb0ec58-4b81-442e-bee5-4c91da447f38  rac1 un  162.150.4.167  243.7 mb   256     34.2%  5b2c1900-bf03-41c1-bb4e-82df1655b8d8  rac1 [cassandra@nbcqa-chc-a02 ~]$ [cassandra@nbcqa-chc-a02 ~]$ date fri sep 19 15:55:33 utc 2014 [cassandra@nbcqa-chc-a02 ~]$ nodetool repair -st -9220354588320251877 -et -9220354588320251873 sep 19, 2014 4:06:08 pm clientcommunicatoradmin checker-run warning: failed to check the connection: java.net.sockettimeoutexception: read timed out top - 16:50:07 up 11 days,  2:01,  2 users,  load average: 0.54, 0.60, 0.65 tasks: 175 total,   1 running, 174 sleeping,   0 stopped,   0 zombie cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu1  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu2  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu3  :  0.7%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu4  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu5  :  0.3%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.3%st cpu6  :  0.0%us,  0.3%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st cpu7  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st mem:  16332056k total, 16167212k used,   164844k free,   149956k buffers swap:        0k total,        0k used,        0k free,  8360056k cached   pid user      pr  ni  virt  res  shr s %cpu %mem    time+  command  2161 cassandr  20   0 11.5g 6.9g 227m s 107.8 44.0 281:59.49 java  9942 root      20   0  106m 2320 1344 s  1.0  0.0   0:00.03 dhclient-script 28969 opscente  20   0 4479m 188m  12m s  0.7  1.2  56:24.24 java  5123 cassandr  20   0 1788m 107m  28m s  0.3  0.7   0:08.09 java     1 root      20   0 19228 1352 1072 s  0.0  0.0   0:00.82 init     2 root      20   0     0    0    0 s  0.0  0.0   0:00.02 kthreadd     3 root      rt   0     0    0    0 s  0.0  0.0   0:05.52 migration/0     4 root      20   0     0    0    0 s  0.0  0.0   0:13.15 ksoftirqd/0     5 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/0     6 root      rt   0     0    0    0 s  0.0  0.0   0:03.33 watchdog/0     7 root      rt   0     0    0    0 s  0.0  0.0   0:04.88 migration/1     8 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/1     9 root      20   0     0    0    0 s  0.0  0.0   0:19.21 ksoftirqd/1    10 root      rt   0     0    0    0 s  0.0  0.0   0:03.24 watchdog/1    11 root      rt   0     0    0    0 s  0.0  0.0   0:05.46 migration/2    12 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/2    13 root      20   0     0    0    0 s  0.0  0.0   0:16.87 ksoftirqd/2    14 root      rt   0     0    0    0 s  0.0  0.0   0:03.49 watchdog/2    15 root      rt   0     0    0    0 s  0.0  0.0   0:05.31 migration/3    16 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/3    17 root      20   0     0    0    0 s  0.0  0.0   0:19.33 ksoftirqd/3    18 root      rt   0     0    0    0 s  0.0  0.0   0:03.43 watchdog/3    19 root      rt   0     0    0    0 s  0.0  0.0   0:05.36 migration/4    20 root      rt   0     0    0    0 s  0.0  0.0   0:00.00 migration/4    21 root      20   0     0    0    0 s  0.0  0.0   0:17.64 ksoftirqd/4    22 root      rt   0     0    0    0 s  0.0  0.0   0:03.18 watchdog/4    23 root      rt   0     0    0    0 s  0.0  0.0   0:05.31 migration/5 <text> customer has a 3 node cluster with 500mb data on each node after we run repair command, system runs into oom after some 45 minutes  nothing else is running here is when we run oom cassandra process pegs 1 of the 8 cpu's 100% there is a 12gb heapdump, the memory leak suspects show the following trace the file is way too large to be attached here.   it's currently held at my scp server  let us know if there is any other info you may need  i will be posting the other 2 system logs as soon as i get them",
        "label": 253
    },
    {
        "text": "cassandra stress shows the incorrect jmx port in settings output <description> cassandra-11914 introduces settings output for cassandra-stress; in that output, the jmx port is incorrectly reported. the attached patch fixes this.<stacktrace> <code> <text> cassandra-11914 introduces settings output for cassandra-stress; in that output, the jmx port is incorrectly reported. the attached patch fixes this.",
        "label": 196
    },
    {
        "text": "rename 'table' class to 'keyspace' in <description> with cql3 tables, j.o.a.c.d.table representing a keyspace is a major source of confusion for new contributors. in 2.0 we can finally rename it to j.o.a.c.d.keyspace. it's going to be a bit painful for 1.2 merges, but i believe it will pay off, eventually.<stacktrace> <code> <text> with cql3 tables, j.o.a.c.d.table representing a keyspace is a major source of confusion for new contributors. in 2.0 we can finally rename it to j.o.a.c.d.keyspace. it's going to be a bit painful for 1.2 merges, but i believe it will pay off, eventually.",
        "label": 244
    },
    {
        "text": "fix validation of indexes on composite column components of compact tables <description> cassandra-5125 added support of indexes on composite column components for non-compact tables (see cassandra-5125 comments for additional information).  this is a follow up for compact tables. using compact tables it is possible to create index on composite primary key columns, but queries returns no results for the tests below. create table users2 (    userid uuid,    fname text,    zip int,    state text,   primary key ((userid, fname)) ) with compact storage; create index on users2 (userid); create index on users2 (fname); insert into users2 (userid, fname, zip, state) values (b3e3bc33-b237-4b55-9337-3d41de9a5649, 'john', 10007, 'ny'); -- the following queries returns 0 rows, instead of 1 expected select * from users2 where fname='john';  select * from users2 where userid=b3e3bc33-b237-4b55-9337-3d41de9a5649; select * from users2 where userid=b3e3bc33-b237-4b55-9337-3d41de9a5649 and fname='john'; -- dropping 2ndary indexes restore normal behavior<stacktrace> <code> create table users2 (    userid uuid,    fname text,    zip int,    state text,   primary key ((userid, fname)) ) with compact storage; create index on users2 (userid); create index on users2 (fname); insert into users2 (userid, fname, zip, state) values (b3e3bc33-b237-4b55-9337-3d41de9a5649, 'john', 10007, 'ny'); -- the following queries returns 0 rows, instead of 1 expected select * from users2 where fname='john';  select * from users2 where userid=b3e3bc33-b237-4b55-9337-3d41de9a5649; select * from users2 where userid=b3e3bc33-b237-4b55-9337-3d41de9a5649 and fname='john'; -- dropping 2ndary indexes restore normal behavior create table users2 (    userid uuid,    fname text,    zip int,    state text,   primary key ((userid, fname)) ) with compact storage; create index on users2 (userid); create index on users2 (fname); insert into users2 (userid, fname, zip, state) values (b3e3bc33-b237-4b55-9337-3d41de9a5649, 'john', 10007, 'ny'); -- the following queries returns 0 rows, instead of 1 expected select * from users2 where fname='john';  select * from users2 where userid=b3e3bc33-b237-4b55-9337-3d41de9a5649; select * from users2 where userid=b3e3bc33-b237-4b55-9337-3d41de9a5649 and fname='john'; -- dropping 2ndary indexes restore normal behavior <text> cassandra-5125 added support of indexes on composite column components for non-compact tables (see cassandra-5125 comments for additional information).  this is a follow up for compact tables. using compact tables it is possible to create index on composite primary key columns, but queries returns no results for the tests below.",
        "label": 520
    },
    {
        "text": "hanging streaming sessions <description> i've started recently running repair using cassandra reaper (built-in nodetool repair doesn't work for me - cassandra-9935). it behaves fine but i've noticed hanging streaming sessions: root@db1:~# date sat jan  9 16:43:00 utc 2016 root@db1:~# nt netstats -h | grep total         receiving 5 files, 46.59 mb total. already received 1 files, 11.32 mb total         sending 7 files, 46.28 mb total. already sent 7 files, 46.28 mb total         receiving 6 files, 64.15 mb total. already received 1 files, 12.14 mb total         sending 5 files, 61.15 mb total. already sent 5 files, 61.15 mb total         receiving 4 files, 7.75 mb total. already received 3 files, 7.58 mb total         sending 4 files, 4.29 mb total. already sent 4 files, 4.29 mb total         receiving 12 files, 13.79 mb total. already received 11 files, 7.66 mb total         sending 5 files, 15.32 mb total. already sent 5 files, 15.32 mb total         receiving 8 files, 20.35 mb total. already received 1 files, 13.63 mb total         sending 38 files, 125.34 mb total. already sent 38 files, 125.34 mb total root@db1:~# date sat jan  9 17:45:42 utc 2016 root@db1:~# nt netstats -h | grep total         receiving 5 files, 46.59 mb total. already received 1 files, 11.32 mb total         sending 7 files, 46.28 mb total. already sent 7 files, 46.28 mb total         receiving 6 files, 64.15 mb total. already received 1 files, 12.14 mb total         sending 5 files, 61.15 mb total. already sent 5 files, 61.15 mb total         receiving 4 files, 7.75 mb total. already received 3 files, 7.58 mb total         sending 4 files, 4.29 mb total. already sent 4 files, 4.29 mb total         receiving 12 files, 13.79 mb total. already received 11 files, 7.66 mb total         sending 5 files, 15.32 mb total. already sent 5 files, 15.32 mb total         receiving 8 files, 20.35 mb total. already received 1 files, 13.63 mb total         sending 38 files, 125.34 mb total. already sent 38 files, 125.34 mb total such sessions are left even when repair job is long time done (confirmed by checking reaper's and cassandra's logs). streaming_socket_timeout_in_ms in cassandra.yaml is set to default value (3600000).<stacktrace> <code> root@db1:~# date sat jan  9 16:43:00 utc 2016 root@db1:~# nt netstats -h | grep total         receiving 5 files, 46.59 mb total. already received 1 files, 11.32 mb total         sending 7 files, 46.28 mb total. already sent 7 files, 46.28 mb total         receiving 6 files, 64.15 mb total. already received 1 files, 12.14 mb total         sending 5 files, 61.15 mb total. already sent 5 files, 61.15 mb total         receiving 4 files, 7.75 mb total. already received 3 files, 7.58 mb total         sending 4 files, 4.29 mb total. already sent 4 files, 4.29 mb total         receiving 12 files, 13.79 mb total. already received 11 files, 7.66 mb total         sending 5 files, 15.32 mb total. already sent 5 files, 15.32 mb total         receiving 8 files, 20.35 mb total. already received 1 files, 13.63 mb total         sending 38 files, 125.34 mb total. already sent 38 files, 125.34 mb total root@db1:~# date sat jan  9 17:45:42 utc 2016 root@db1:~# nt netstats -h | grep total         receiving 5 files, 46.59 mb total. already received 1 files, 11.32 mb total         sending 7 files, 46.28 mb total. already sent 7 files, 46.28 mb total         receiving 6 files, 64.15 mb total. already received 1 files, 12.14 mb total         sending 5 files, 61.15 mb total. already sent 5 files, 61.15 mb total         receiving 4 files, 7.75 mb total. already received 3 files, 7.58 mb total         sending 4 files, 4.29 mb total. already sent 4 files, 4.29 mb total         receiving 12 files, 13.79 mb total. already received 11 files, 7.66 mb total         sending 5 files, 15.32 mb total. already sent 5 files, 15.32 mb total         receiving 8 files, 20.35 mb total. already received 1 files, 13.63 mb total         sending 38 files, 125.34 mb total. already sent 38 files, 125.34 mb total <text> i've started recently running repair using cassandra reaper (built-in nodetool repair doesn't work for me - cassandra-9935). it behaves fine but i've noticed hanging streaming sessions: such sessions are left even when repair job is long time done (confirmed by checking reaper's and cassandra's logs). streaming_socket_timeout_in_ms in cassandra.yaml is set to default value (3600000).",
        "label": 409
    },
    {
        "text": "tarball contains duplicate entries <description> the tarball contains a lot of duplicate entries. one example is cassandra-stress.bat: tar -tvf /home/map/downloads/apache-cassandra-3.11.6-bin.tar.gz |grep \"cassandra-stress.bat\" -rw-r--r-- 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat -rwxr-xr-x 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat<stacktrace> <code> tar -tvf /home/map/downloads/apache-cassandra-3.11.6-bin.tar.gz |grep 'cassandra-stress.bat' -rw-r--r-- 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat -rwxr-xr-x 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat tar -tvf /home/map/downloads/apache-cassandra-3.11.6-bin.tar.gz |grep 'cassandra-stress.bat' -rw-r--r-- 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat -rwxr-xr-x 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat <text> the tarball contains a lot of duplicate entries. one example is cassandra-stress.bat:",
        "label": 335
    },
    {
        "text": "deleting a cf always produces an error and that cf remains in an unknown state <description> from the cli perspective: [default@disco] drop column family client;   null  org.apache.thrift.transport.ttransportexception  at org.apache.thrift.transport.tiostreamtransport.read(tiostreamtransport.java:132)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.transport.tframedtransport.readframe(tframedtransport.java:129)  at org.apache.thrift.transport.tframedtransport.read(tframedtransport.java:101)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.protocol.tbinaryprotocol.readall(tbinaryprotocol.java:378)  at org.apache.thrift.protocol.tbinaryprotocol.readi32(tbinaryprotocol.java:297)  at org.apache.thrift.protocol.tbinaryprotocol.readmessagebegin(tbinaryprotocol.java:204)  at org.apache.thrift.tserviceclient.receivebase(tserviceclient.java:69)  at org.apache.cassandra.thrift.cassandra$client.recv_system_drop_column_family(cassandra.java:1222)  at org.apache.cassandra.thrift.cassandra$client.system_drop_column_family(cassandra.java:1209)  at org.apache.cassandra.cli.cliclient.executedelcolumnfamily(cliclient.java:1301)  at org.apache.cassandra.cli.cliclient.executeclistatement(cliclient.java:234)  at org.apache.cassandra.cli.climain.processstatementinteractive(climain.java:219)  at org.apache.cassandra.cli.climain.main(climain.java:346) log:  info [migrationstage:1] 2012-05-09 11:25:35,686 columnfamilystore.java (line 634) enqueuing flush of memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)  info [flushwriter:3] 2012-05-09 11:25:35,687 memtable.java (line 266) writing memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)  info [flushwriter:3] 2012-05-09 11:25:35,748 memtable.java (line 307) completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-34-data.db (1041 bytes)  info [migrationstage:1] 2012-05-09 11:25:35,749 columnfamilystore.java (line 634) enqueuing flush of memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)  info [flushwriter:3] 2012-05-09 11:25:35,750 memtable.java (line 266) writing memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)  info [flushwriter:3] 2012-05-09 11:25:35,812 memtable.java (line 307) completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-data.db (649 bytes)  info [compactionexecutor:20] 2012-05-09 11:25:35,814 compactiontask.java (line 114) compacting [sstablereader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-27-data.db'), sstablereader  (path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-25-data.db'), sstablereader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-26-data.db'), sstablereader(path  ='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-data.db')]  info [migrationstage:1] 2012-05-09 11:25:35,918 columnfamilystore.java (line 634) enqueuing flush of memtable-client@864320066(372/465 serialized/live bytes, 6 ops)  info [flushwriter:3] 2012-05-09 11:25:35,919 memtable.java (line 266) writing memtable-client@864320066(372/465 serialized/live bytes, 6 ops)  info [compactionexecutor:20] 2012-05-09 11:25:35,945 compactiontask.java (line 225) compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-29-data.db,]. 22,486 to 20,621 (~91% of orig  inal) bytes for 2 keys at 0.150120mb/s. time: 131ms.  info [flushwriter:3] 2012-05-09 11:25:36,013 memtable.java (line 307) completed flushing /var/lib/cassandra/data/disco/client/disco-client-hc-5-data.db (407 bytes)  error [migrationstage:1] 2012-05-09 11:25:36,043 clibrary.java (line 158) unable to create hard link  com.sun.jna.lasterrorexception: errno was 17  at org.apache.cassandra.utils.clibrary.link(native method)  at org.apache.cassandra.utils.clibrary.createhardlink(clibrary.java:150)  at org.apache.cassandra.db.directories.snapshotleveledmanifest(directories.java:343)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1450)  at org.apache.cassandra.db.columnfamilystore.snapshot(columnfamilystore.java:1483)  at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:512)  at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:403)  at org.apache.cassandra.db.defstable.mergeschema(defstable.java:270)  at org.apache.cassandra.service.migrationmanager$1.call(migrationmanager.java:214)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) error [thrift:3] 2012-05-09 11:25:36,048 customtthreadpoolserver.java (line 204) error occurred during processing of message.  java.lang.runtimeexception: java.util.concurrent.executionexception: java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:372)  at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:191)  at org.apache.cassandra.service.migrationmanager.announcecolumnfamilydrop(migrationmanager.java:182)  at org.apache.cassandra.thrift.cassandraserver.system_drop_column_family(cassandraserver.java:948)  at org.apache.cassandra.thrift.cassandra$processor$system_drop_column_family.getresult(cassandra.java:3348)  at org.apache.cassandra.thrift.cassandra$processor$system_drop_column_family.getresult(cassandra.java:3336)  at org.apache.thrift.processfunction.process(processfunction.java:32)  at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:186)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.util.concurrent.executionexception: java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:368)  ... 11 more  caused by: java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1454)  at org.apache.cassandra.db.columnfamilystore.snapshot(columnfamilystore.java:1483)  at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:512)  at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:403)  at org.apache.cassandra.db.defstable.mergeschema(defstable.java:270)  at org.apache.cassandra.service.migrationmanager$1.call(migrationmanager.java:214)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more  caused by: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.utils.clibrary.createhardlink(clibrary.java:163)  at org.apache.cassandra.db.directories.snapshotleveledmanifest(directories.java:343)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1450)  ... 10 more  error [migrationstage:1] 2012-05-09 11:25:36,051 abstractcassandradaemon.java (line 134) exception in thread thread[migrationstage:1,5,main]  java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1454)  at org.apache.cassandra.db.columnfamilystore.snapshot(columnfamilystore.java:1483)  at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:512)  at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:403)  at org.apache.cassandra.db.defstable.mergeschema(defstable.java:270)  at org.apache.cassandra.service.migrationmanager$1.call(migrationmanager.java:214)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.utils.clibrary.createhardlink(clibrary.java:163)  at org.apache.cassandra.db.directories.snapshotleveledmanifest(directories.java:343)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1450)  ... 10 more  info [compactionexecutor:22] 2012-05-09 11:25:36,052 compactiontask.java (line 114) compacting [sstablereader(path='/var/lib/cassandra/data/disco/client/disco-client-hc-5-data.db'), sstablereader(path='/var/lib/cassandra/data/disco/client/disco-client-hc-4-data.db')]  info [compactionexecutor:22] 2012-05-09 11:25:36,187 compactiontask.java (line 225) compacted to [/var/lib/cassandra/data/disco/client/disco-client-hc-6-data.db,]. 728 to 458 (~62% of original) bytes for 8 keys at 0.003235mb/s. time: 135ms. schema: create column family client with  key_validation_class = uuidtype and  comparator = utf8type and  column_metadata = [ { column_name: key, validation_class: bytestype } { column_name: name, validation_class: utf8type } { column_name: userid, validation_class: uuidtype, index_type: keys } ] and  compression_options = { sstable_compression:snappycompressor, chunk_length_kb:64 } and  compaction_strategy = leveledcompactionstrategy and  compaction_strategy_options = { sstable_size_in_mb: 10 } and  gc_grace = 432000; state of data dir after deletion attempt: 1. ls -lah /var/lib/cassandra/data/disco/client/  total 76k  drwxr-xr-x 3 cassandra cassandra 4.0k may 9 11:25 .  drwxr-xr-x 17 cassandra cassandra 4.0k may 3 12:34 ..  rw-rr- 2 cassandra cassandra 420 may 9 11:25 client-old.json  rw-rr- 1 cassandra cassandra 418 may 7 18:04 client.client_userid_idx-old.json  rw-rr- 1 cassandra cassandra 418 may 7 18:04 client.client_userid_idx.json  rw-rr- 1 cassandra cassandra 418 may 9 11:25 client.json  rw-rr- 1 cassandra cassandra 46 may 9 11:25 disco-client-hc-6-compressioninfo.db  rw-rr- 1 cassandra cassandra 458 may 9 11:25 disco-client-hc-6-data.db  rw-rr- 1 cassandra cassandra 976 may 9 11:25 disco-client-hc-6-filter.db  rw-rr- 1 cassandra cassandra 208 may 9 11:25 disco-client-hc-6-index.db  rw-rr- 1 cassandra cassandra 4.3k may 9 11:25 disco-client-hc-6-statistics.db  rw-rr- 4 cassandra cassandra 46 may 7 18:04 disco-client.client_userid_idx-hc-2-compressioninfo.db  rw-rr- 4 cassandra cassandra 92 may 7 18:04 disco-client.client_userid_idx-hc-2-data.db  rw-rr- 4 cassandra cassandra 496 may 7 18:04 disco-client.client_userid_idx-hc-2-filter.db  rw-rr- 4 cassandra cassandra 26 may 7 18:04 disco-client.client_userid_idx-hc-2-index.db  rw-rr- 4 cassandra cassandra 4.3k may 7 18:04 disco-client.client_userid_idx-hc-2-statistics.db  drwxr-xr-x 6 cassandra cassandra 4.0k may 9 11:25 snapshots<stacktrace> [default@disco] drop column family client;   null  org.apache.thrift.transport.ttransportexception  at org.apache.thrift.transport.tiostreamtransport.read(tiostreamtransport.java:132)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.transport.tframedtransport.readframe(tframedtransport.java:129)  at org.apache.thrift.transport.tframedtransport.read(tframedtransport.java:101)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.protocol.tbinaryprotocol.readall(tbinaryprotocol.java:378)  at org.apache.thrift.protocol.tbinaryprotocol.readi32(tbinaryprotocol.java:297)  at org.apache.thrift.protocol.tbinaryprotocol.readmessagebegin(tbinaryprotocol.java:204)  at org.apache.thrift.tserviceclient.receivebase(tserviceclient.java:69)  at org.apache.cassandra.thrift.cassandra$client.recv_system_drop_column_family(cassandra.java:1222)  at org.apache.cassandra.thrift.cassandra$client.system_drop_column_family(cassandra.java:1209)  at org.apache.cassandra.cli.cliclient.executedelcolumnfamily(cliclient.java:1301)  at org.apache.cassandra.cli.cliclient.executeclistatement(cliclient.java:234)  at org.apache.cassandra.cli.climain.processstatementinteractive(climain.java:219)  at org.apache.cassandra.cli.climain.main(climain.java:346) info [migrationstage:1] 2012-05-09 11:25:35,686 columnfamilystore.java (line 634) enqueuing flush of memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)  info [flushwriter:3] 2012-05-09 11:25:35,687 memtable.java (line 266) writing memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)  info [flushwriter:3] 2012-05-09 11:25:35,748 memtable.java (line 307) completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-34-data.db (1041 bytes)  info [migrationstage:1] 2012-05-09 11:25:35,749 columnfamilystore.java (line 634) enqueuing flush of memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)  info [flushwriter:3] 2012-05-09 11:25:35,750 memtable.java (line 266) writing memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)  info [flushwriter:3] 2012-05-09 11:25:35,812 memtable.java (line 307) completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-data.db (649 bytes)  info [compactionexecutor:20] 2012-05-09 11:25:35,814 compactiontask.java (line 114) compacting [sstablereader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-27-data.db'), sstablereader  (path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-25-data.db'), sstablereader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-26-data.db'), sstablereader(path  ='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-data.db')]  info [migrationstage:1] 2012-05-09 11:25:35,918 columnfamilystore.java (line 634) enqueuing flush of memtable-client@864320066(372/465 serialized/live bytes, 6 ops)  info [flushwriter:3] 2012-05-09 11:25:35,919 memtable.java (line 266) writing memtable-client@864320066(372/465 serialized/live bytes, 6 ops)  info [compactionexecutor:20] 2012-05-09 11:25:35,945 compactiontask.java (line 225) compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-29-data.db,]. 22,486 to 20,621 (~91% of orig  inal) bytes for 2 keys at 0.150120mb/s. time: 131ms.  info [flushwriter:3] 2012-05-09 11:25:36,013 memtable.java (line 307) completed flushing /var/lib/cassandra/data/disco/client/disco-client-hc-5-data.db (407 bytes)  error [migrationstage:1] 2012-05-09 11:25:36,043 clibrary.java (line 158) unable to create hard link  com.sun.jna.lasterrorexception: errno was 17  at org.apache.cassandra.utils.clibrary.link(native method)  at org.apache.cassandra.utils.clibrary.createhardlink(clibrary.java:150)  at org.apache.cassandra.db.directories.snapshotleveledmanifest(directories.java:343)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1450)  at org.apache.cassandra.db.columnfamilystore.snapshot(columnfamilystore.java:1483)  at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:512)  at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:403)  at org.apache.cassandra.db.defstable.mergeschema(defstable.java:270)  at org.apache.cassandra.service.migrationmanager$1.call(migrationmanager.java:214)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) error [thrift:3] 2012-05-09 11:25:36,048 customtthreadpoolserver.java (line 204) error occurred during processing of message.  java.lang.runtimeexception: java.util.concurrent.executionexception: java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:372)  at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:191)  at org.apache.cassandra.service.migrationmanager.announcecolumnfamilydrop(migrationmanager.java:182)  at org.apache.cassandra.thrift.cassandraserver.system_drop_column_family(cassandraserver.java:948)  at org.apache.cassandra.thrift.cassandra$processor$system_drop_column_family.getresult(cassandra.java:3348)  at org.apache.cassandra.thrift.cassandra$processor$system_drop_column_family.getresult(cassandra.java:3336)  at org.apache.thrift.processfunction.process(processfunction.java:32)  at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:186)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.util.concurrent.executionexception: java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:368)  ... 11 more  caused by: java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1454)  at org.apache.cassandra.db.columnfamilystore.snapshot(columnfamilystore.java:1483)  at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:512)  at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:403)  at org.apache.cassandra.db.defstable.mergeschema(defstable.java:270)  at org.apache.cassandra.service.migrationmanager$1.call(migrationmanager.java:214)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more  caused by: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.utils.clibrary.createhardlink(clibrary.java:163)  at org.apache.cassandra.db.directories.snapshotleveledmanifest(directories.java:343)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1450)  ... 10 more  error [migrationstage:1] 2012-05-09 11:25:36,051 abstractcassandradaemon.java (line 134) exception in thread thread[migrationstage:1,5,main]  java.io.ioerror: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1454)  at org.apache.cassandra.db.columnfamilystore.snapshot(columnfamilystore.java:1483)  at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:512)  at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:403)  at org.apache.cassandra.db.defstable.mergeschema(defstable.java:270)  at org.apache.cassandra.service.migrationmanager$1.call(migrationmanager.java:214)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.ioexception: unable to create hard link from /var/lib/cassandra/data/disco/client/client.json to /var/lib/cassandra/data/disco/client/snapshots/1336559135918-client/client.json (errno 17)  at org.apache.cassandra.utils.clibrary.createhardlink(clibrary.java:163)  at org.apache.cassandra.db.directories.snapshotleveledmanifest(directories.java:343)  at org.apache.cassandra.db.columnfamilystore.snapshotwithoutflush(columnfamilystore.java:1450)  ... 10 more  info [compactionexecutor:22] 2012-05-09 11:25:36,052 compactiontask.java (line 114) compacting [sstablereader(path='/var/lib/cassandra/data/disco/client/disco-client-hc-5-data.db'), sstablereader(path='/var/lib/cassandra/data/disco/client/disco-client-hc-4-data.db')]  info [compactionexecutor:22] 2012-05-09 11:25:36,187 compactiontask.java (line 225) compacted to [/var/lib/cassandra/data/disco/client/disco-client-hc-6-data.db,]. 728 to 458 (~62% of original) bytes for 8 keys at 0.003235mb/s. time: 135ms. <code> create column family client with  key_validation_class = uuidtype and  comparator = utf8type and  column_metadata = [ ] and  compression_options = and  compaction_strategy = leveledcompactionstrategy and  compaction_strategy_options = and  gc_grace = 432000; <text> from the cli perspective: log: schema: state of data dir after deletion attempt:",
        "label": 412
    },
    {
        "text": "remove pre streaming compatibility code for <description> the cleanup method in streamreader.streamdeserializer does stuff in the cases when the field 'in' is a rewindabledatainputstreamplus typed object. given that it is a  this.in = new datainputplus.datainputstreamplus(in); that can never be. i'm assuming this was left over from some previous refactor or such. assuming we can just delete this?<stacktrace> <code> this.in = new datainputplus.datainputstreamplus(in); <text> the cleanup method in streamreader.streamdeserializer does stuff in the cases when the field 'in' is a rewindabledatainputstreamplus typed object. given that it is a that can never be. i'm assuming this was left over from some previous refactor or such. assuming we can just delete this?",
        "label": 409
    },
    {
        "text": "stress py's multiget option sends increasingly inefficient queries as more test data is inserted <description> multigetter's key list sizes should be broken up better for more efficient queries. setting an initial value that breaks up the key list into n sub lists (where n is the number of threads) yielded more efficient queries. (the choice of thread count here was a stop-gap for demonstration purposes. end result should probably be chunk-size config option with a sane default). pre patch:  \u2014  python stress.py -o multiget -t 25 -n 250000 -c 5  total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time  6,0,6000,8.6109764576,10  10,0,4000,18.6666852832,20  17,0,7000,27.4705835751,30  23,0,6000,36.6091703971,41  25,0,2000,41.8415510654,42 post patch:  \u2014  python mstress.py -o multiget -t 25 -n 250000 -c 5  total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time  172,17,6880,1.44215503127,10  314,14,5680,1.8667214538,20  466,15,6080,1.69888155084,31  624,15,6320,1.55442555947,41  625,0,40,0.0914790630341,41<stacktrace> <code> pre patch:  -  python stress.py -o multiget -t 25 -n 250000 -c 5  total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time  6,0,6000,8.6109764576,10  10,0,4000,18.6666852832,20  17,0,7000,27.4705835751,30  23,0,6000,36.6091703971,41  25,0,2000,41.8415510654,42 post patch:  -  python mstress.py -o multiget -t 25 -n 250000 -c 5  total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time  172,17,6880,1.44215503127,10  314,14,5680,1.8667214538,20  466,15,6080,1.69888155084,31  624,15,6320,1.55442555947,41  625,0,40,0.0914790630341,41<text> multigetter's key list sizes should be broken up better for more efficient queries. setting an initial value that breaks up the key list into n sub lists (where n is the number of threads) yielded more efficient queries. (the choice of thread count here was a stop-gap for demonstration purposes. end result should probably be chunk-size config option with a sane default). ",
        "label": 85
    },
    {
        "text": "rolescache should not be created for any authenticator that does not requireauthentication <description> the rolescache.initcache method currently checks the authenticator as follows:         if (databasedescriptor.getauthenticator() instanceof allowallauthenticator)             return null; this does not allow for any 3rd party authenticators that don't require authentication. it should check the requireauthentication method on the authenticator to see if it should continue.<stacktrace> <code>         if (databasedescriptor.getauthenticator() instanceof allowallauthenticator)             return null; <text> the rolescache.initcache method currently checks the authenticator as follows: this does not allow for any 3rd party authenticators that don't require authentication. it should check the requireauthentication method on the authenticator to see if it should continue.",
        "label": 357
    },
    {
        "text": "dtest failure in thrift tests testmutations test range tombstone eoc <description> example failure: http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testreport/thrift_tests/testmutations/test_range_tombstone_eoc_0 stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/thrift_tests.py\", line 2588, in test_range_tombstone_eoc_0     self.assertequals(2, len(ret))   file \"/usr/lib/python2.7/unittest/case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 506, in _baseassertequal     raise self.failureexception(msg) '2 != 0<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/thrift_tests.py', line 2588, in test_range_tombstone_eoc_0     self.assertequals(2, len(ret))   file '/usr/lib/python2.7/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file '/usr/lib/python2.7/unittest/case.py', line 506, in _baseassertequal     raise self.failureexception(msg) '2 != 0 http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testreport/thrift_tests/testmutations/test_range_tombstone_eoc_0<text> example failure: ",
        "label": 428
    },
    {
        "text": "isreadyforbootstrap doesn't compare schema uuid by timestamp as it should <description> cassandra-3629 introduced a wait to be sure the node is up to date on the schema before starting bootstrap. however, the isreadyforbootsrap() method compares schema version using uuid.compareto(), which doesn't compare uuid by timestamp, while the rest of the code does compare using timestamp (migrationmanager.updatehighestknown). during a test where lots of node were boostrapped simultaneously (and some schema change were done), we ended up having some node stuck in the isreadyforboostrap loop. restarting the node fixed it, so while i can't confirm it, i suspect this was the source of that problem.<stacktrace> <code> <text> cassandra-3629 introduced a wait to be sure the node is up to date on the schema before starting bootstrap. however, the isreadyforbootsrap() method compares schema version using uuid.compareto(), which doesn't compare uuid by timestamp, while the rest of the code does compare using timestamp (migrationmanager.updatehighestknown). during a test where lots of node were boostrapped simultaneously (and some schema change were done), we ended up having some node stuck in the isreadyforboostrap loop. restarting the node fixed it, so while i can't confirm it, i suspect this was the source of that problem.",
        "label": 520
    },
    {
        "text": "secondary index should report it's memory consumption <description> non-cfs backed secondary indexes will consume ram which should be reported back to cassandra to be factored into it's flush by ram amount.<stacktrace> <code> <text> non-cfs backed secondary indexes will consume ram which should be reported back to cassandra to be factored into it's flush by ram amount.",
        "label": 521
    },
    {
        "text": "partitiontest testcolumnstatsrecordsrowdeletescorrectly is flappy <description> this run of trunk-testall failed for partitiontest.testcolumnstatsrecordsrowdeletescorrectly. the attached patch should fix this (at least extend the probability to fail to more than one second).<stacktrace> <code> <text> this run of trunk-testall failed for partitiontest.testcolumnstatsrecordsrowdeletescorrectly. the attached patch should fix this (at least extend the probability to fail to more than one second).",
        "label": 520
    },
    {
        "text": "memtable throughput in mb can not support sizes over gigs because of an integer overflow  <description> if memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown. however, as soon as data starts being written it is almost immediately being flushed. several hundred sstables are created in minutes. i am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing. from memtable.java:  private final int threshold;  private final int threshold_count; ...  this.threshold = cfs.getmemtablethroughputinmb() * 1024 * 1024;  this.threshold_count = (int) (cfs.getmemtableoperationsinmillions() * 1024 * 1024); note:  i also think currentthroughput also needs to be changed from an int to a long. i'm not sure if it is as simple as this or if this also is used in other places.<stacktrace> <code> private final int threshold;  private final int threshold_count; ...  this.threshold = cfs.getmemtablethroughputinmb() * 1024 * 1024;  this.threshold_count = (int) (cfs.getmemtableoperationsinmillions() * 1024 * 1024); <text> if memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown. however, as soon as data starts being written it is almost immediately being flushed. several hundred sstables are created in minutes. i am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing. from memtable.java: note:  i also think currentthroughput also needs to be changed from an int to a long. i'm not sure if it is as simple as this or if this also is used in other places.",
        "label": 274
    },
    {
        "text": "windows dtest  clustering order test dtest fails <description> schema_metadata_test.testschemametadata.clustering_order_test fails on cassandra-3.0 under windows \u2013 the test expects a table's metadata object, represented as a cql query, to indicate its ordering.<stacktrace> <code> <text> schema_metadata_test.testschemametadata.clustering_order_test fails on cassandra-3.0 under windows - the test expects a table's metadata object, represented as a cql query, to indicate its ordering.",
        "label": 577
    },
    {
        "text": "login  request via thrift php fails with  unexpected authentication problem  in cassandra log    internal error processing login  in thrift <description> when issuing a login request via php thrift with the following parameters: $auth_request = new cassandra_authenticationrequest;  $auth_request->credentials = array (  \"username\" => \"jsmith\",  \"password\" => \"havebadpass\",  );  $client->login(\"keyspace1\", $auth_request); i get an exception, with the following details php exception:  php fatal error: uncaught exception 'tapplicationexception' with message 'internal error processing login' in /home/redsolar/html/includes/thrift/packages/cassandra/cassandra.php:73 cassandra log: error 13:00:53,823 internal error processing login  java.lang.runtimeexception: unexpected authentication problem  at org.apache.cassandra.auth.simpleauthenticator.login(simpleauthenticator.java:113)  at org.apache.cassandra.thrift.cassandraserver.login(cassandraserver.java:651)  at org.apache.cassandra.thrift.cassandra$processor$login.process(cassandra.java:1147)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:1125)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:636)  caused by: java.lang.nullpointerexception  at java.io.fileinputstream.<init>(fileinputstream.java:133)  at java.io.fileinputstream.<init>(fileinputstream.java:96)  at org.apache.cassandra.auth.simpleauthenticator.login(simpleauthenticator.java:82)  ... 7 more file contents (all chmod 777 for testing): \"conf/access.properties\"  keyspace1=jsmith,elvis presley,dilbert \"conf/password.properties\"  jsmith=havebadpass  elvis\\ presley=graceland4evar  dilbert=nomoovertime<stacktrace> error 13:00:53,823 internal error processing login  java.lang.runtimeexception: unexpected authentication problem  at org.apache.cassandra.auth.simpleauthenticator.login(simpleauthenticator.java:113)  at org.apache.cassandra.thrift.cassandraserver.login(cassandraserver.java:651)  at org.apache.cassandra.thrift.cassandra$processor$login.process(cassandra.java:1147)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:1125)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:636)  caused by: java.lang.nullpointerexception  at java.io.fileinputstream.<init>(fileinputstream.java:133)  at java.io.fileinputstream.<init>(fileinputstream.java:96)  at org.apache.cassandra.auth.simpleauthenticator.login(simpleauthenticator.java:82)  ... 7 more <code> $auth_request = new cassandra_authenticationrequest;  $auth_request->credentials = array (  'username' => 'jsmith',  'password' => 'havebadpass',  );  $client->login('keyspace1', $auth_request); php exception:  php fatal error: uncaught exception 'tapplicationexception' with message 'internal error processing login' in /home/redsolar/html/includes/thrift/packages/cassandra/cassandra.php:73 'conf/access.properties'  keyspace1=jsmith,elvis presley,dilbert <text> when issuing a login request via php thrift with the following parameters: i get an exception, with the following details cassandra log: file contents (all chmod 777 for testing): 'conf/password.properties'  jsmith=havebadpass  elvis/ presley=graceland4evar  dilbert=nomoovertime",
        "label": 455
    },
    {
        "text": "looks like serializing cache broken in <description> i get the following error while setting the row cache to be 1500 mb info 23:27:25,416 initializing row cache with capacity of 1500 mbs and provider org.apache.cassandra.cache.serializingcacheprovider  java.lang.outofmemoryerror: java heap space  dumping heap to java_pid26402.hprof ... havent spend a lot of time looking into the issue but looks like sc constructor has .initialcapacity(capacity)  .maximumweightedcapacity(capacity)  which 1500mb<stacktrace> <code> info 23:27:25,416 initializing row cache with capacity of 1500 mbs and provider org.apache.cassandra.cache.serializingcacheprovider  java.lang.outofmemoryerror: java heap space  dumping heap to java_pid26402.hprof ... .initialcapacity(capacity)  .maximumweightedcapacity(capacity) <text> i get the following error while setting the row cache to be 1500 mb havent spend a lot of time looking into the issue but looks like sc constructor has which 1500mb",
        "label": 555
    },
    {
        "text": "storageproxy slow down and memory leak <description> i am consistently observing slow-downs in storageproxy caused by the nonblockinghashmap used indirectly by messagingservice via the callbacks expiringmap. this seems do be due to nbhm having unbounded memory usage in the face of workloads with high key churn. as monotonically increasing integers are used as callback id's by messagingservice, the backing nbhm eventually ends up growing the backing store unboundedly. this causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. this behavior is especially noticable for high throughput workloads where the dataset is completely in ram and i'm doing up to a hundred thousand reads per second. replacing nbhm in expiringmap with the java standard library concurrenthashmap resolved the issue and allowed me to keep a consistent high throughput. an open issue on nbhm can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362<stacktrace> <code> an open issue on nbhm can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362<text> i am consistently observing slow-downs in storageproxy caused by the nonblockinghashmap used indirectly by messagingservice via the callbacks expiringmap. this seems do be due to nbhm having unbounded memory usage in the face of workloads with high key churn. as monotonically increasing integers are used as callback id's by messagingservice, the backing nbhm eventually ends up growing the backing store unboundedly. this causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. this behavior is especially noticable for high throughput workloads where the dataset is completely in ram and i'm doing up to a hundred thousand reads per second. replacing nbhm in expiringmap with the java standard library concurrenthashmap resolved the issue and allowed me to keep a consistent high throughput. ",
        "label": 133
    },
    {
        "text": "add missing license headers <description> a number of files are missing the license headers<stacktrace> <code> <text> a number of files are missing the license headers",
        "label": 264
    },
    {
        "text": "replace lib high scale lib jar with equivalent from maven central repository <description> as part of my effort to get cassandra published to maven central, there are a number of libraries which cassandra depends on but which are not available in maven central. perhaps the most interesting of these is the public domain high-scale-lib.jar the author is an xml build tool hater (and that includes ant), and the artifact itself contains a lot of unusual cruft... .cvs folders, etc. the build process uses a build.java, that effectively is a rewrite of make in java with the makefile embedded in the build.java. i have rebuilt the artifacts and published them to the maven central repository. as part of the requirements for publishing to maven central are to publish a javadoc.jar and a sources.jar with gpg signatures, etc. it was easier to take the source code and transform it into a maven project. the project is hosted at github: http://stephenc.github.com/high-scale-lib i have published the following versions, all signed with by stephenc@apache.org pgp key  1.0.0  1.0.1  1.1.0  1.1.1  1.1.2 these should all be equivalent to the releases by cliff click, with the only exception being 1.1.1. for 1.1.1 cliff's original build script did not run the unit tests correctly, one of the unit tests consistently fails even on his build process due to an invalid assumption that element ordering is preserved across serialization for nonblockingidentityhashmap. he fixed the test in 1.1.2, so i back-ported the test change. the code however remains as is. in any case, can we change the version of high-scale-lib.jar in the lib directory to the version from maven central  http://repo1.maven.org/maven2/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar  [the current version used by cassandra is 1.1.1] or if perhaps even consider upgrading to 1.1.2 [though i can appreciate that this could be considered riskier] my justification for the change is so that i can be sure that consumers of a maven central distribution of cassandra will have exactly the same dependencies, which have been tested as part of the cassandra release process, and not just the \"stephen's very damn sure they are the same\" dependencies<stacktrace> <code> perhaps the most interesting of these is the public domain high-scale-lib.jar the author is an xml build tool hater (and that includes ant), and the artifact itself contains a lot of unusual cruft... .cvs folders, etc. the build process uses a build.java, that effectively is a rewrite of make in java with the makefile embedded in the build.java. i have rebuilt the artifacts and published them to the maven central repository. as part of the requirements for publishing to maven central are to publish a javadoc.jar and a sources.jar with gpg signatures, etc. it was easier to take the source code and transform it into a maven project. the project is hosted at github: http://stephenc.github.com/high-scale-lib i have published the following versions, all signed with by stephenc@apache.org pgp key  1.0.0  1.0.1  1.1.0  1.1.1  1.1.2 in any case, can we change the version of high-scale-lib.jar in the lib directory to the version from maven central  http://repo1.maven.org/maven2/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar  [the current version used by cassandra is 1.1.1] <text> as part of my effort to get cassandra published to maven central, there are a number of libraries which cassandra depends on but which are not available in maven central. these should all be equivalent to the releases by cliff click, with the only exception being 1.1.1. for 1.1.1 cliff's original build script did not run the unit tests correctly, one of the unit tests consistently fails even on his build process due to an invalid assumption that element ordering is preserved across serialization for nonblockingidentityhashmap. he fixed the test in 1.1.2, so i back-ported the test change. the code however remains as is. or if perhaps even consider upgrading to 1.1.2 [though i can appreciate that this could be considered riskier] my justification for the change is so that i can be sure that consumers of a maven central distribution of cassandra will have exactly the same dependencies, which have been tested as part of the cassandra release process, and not just the 'stephen's very damn sure they are the same' dependencies",
        "label": 510
    },
    {
        "text": "cql doc  correct link text in alter keyspace section <description> the text in alter keyspace says: \"the supported <properties> are the same that for the create table statement\". should be create keyspace of course.<stacktrace> <code> <text> the text in alter keyspace says: 'the supported <properties> are the same that for the create table statement'. should be create keyspace of course.",
        "label": 272
    },
    {
        "text": "improve performance of foldersize function <description> fileutils.foldersize function recursively traverses the directory tree using listfiles method. this is no longer efficient as java 7 offers much better files.walkfiletree method. it makes the method work twice faster according to my tests.<stacktrace> <code> <text> fileutils.foldersize function recursively traverses the directory tree using listfiles method. this is no longer efficient as java 7 offers much better files.walkfiletree method. it makes the method work twice faster according to my tests.",
        "label": 92
    },
    {
        "text": "update cqlsh for udfs <description> once cassandra-7395 and cassandra-7526 are complete, we'll want to add cqlsh support for user defined functions. this will include: completion for create function and drop function tolerating (almost) arbitrary text inside function bodies describe type support including types in describe keyspace output possibly grant completion for any new privileges<stacktrace> <code> <text> once cassandra-7395 and cassandra-7526 are complete, we'll want to add cqlsh support for user defined functions. this will include:",
        "label": 453
    },
    {
        "text": "flakey test   testbootstrap test resumable bootstrap <description> fails 2 out of 10 times on mac/java 8 ================================================================================================================== failures ================================================================================================================== *___________________________________________________________________________________________________ testbootstrap.test_resumable_bootstrap ___________________________________________________________________________________________________*   self = <bootstrap_test.testbootstrap object at 0x10fd488d0>       *@since('2.2')*     *def test_resumable_bootstrap(self):*         *\"\"\"*             *test resuming bootstrap after data streaming failure*             *\"\"\"*         *cluster = self.cluster*         *cluster.populate(2)*     **             *node1 = cluster.nodes['node1']*         *# set up byteman*         *node1.byteman_port = '8100'*         *node1.import_config_files()*     **             *cluster.start(wait_other_notice=true)*         *# kill stream to node3 in the middle of streaming to let it fail*         *if cluster.version() < '4.0':*             *node1.byteman_submit([self.byteman_submit_path_pre_4_0])*         *else:*             *node1.byteman_submit([self.byteman_submit_path_4_0])*         *node1.stress(['write', 'n=1k', 'no-warmup', 'cl=two', '-schema', 'replication(factor=2)', '-rate', 'threads=50'])*         *cluster.flush()*     **             *# start bootstrapping node3 and wait for streaming*         *node3 = new_node(cluster)*         *node3.start(wait_other_notice=false)*     **             *# let streaming fail as we expect* *>       node3.watch_log_for('some data streaming failed')*   *bootstrap_test.py*:365:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    self = <ccmlib.node.node object at 0x10fdf1810>, exprs = 'some data streaming failed', from_mark = none, timeout = 600, process = none, verbose = false, filename = 'system.log'       *def watch_log_for(self, exprs, from_mark=none, timeout=600, process=none, verbose=false, filename='system.log'):*         *\"\"\"*             *watch the log until one or more (regular) expression are found.*             *this methods when all the expressions have been found or the method*             *timeouts (a timeouterror is then raised). on successful completion,*             *a list of pair (line matched, match object) is returned.*             *\"\"\"*         *start = time.time()*         *tofind = [exprs] if isinstance(exprs, string_types) else exprs*         *tofind = [re.compile(e) for e in tofind]*         *matchings = []*         *reads = \"\"*         *if len(tofind) == 0:*             *return none*     **             *log_file = os.path.join(self.get_path(), 'logs', filename)*         *output_read = false*         *while not os.path.exists(log_file):*             *time.sleep(.5)*             *if start + timeout < time.time():*                 *raise timeouterror(time.strftime(\"%d %b %y %h:%m:%s\", time.gmtime()) + \" [\" + self.name + \"] timed out waiting for {} to be created.\".format(log_file))*             *if process and not output_read:*                 *process.poll()*                 *if process.returncode is not none:*                     *self.print_process_output(self.name, process, verbose)*                     *output_read = true*                     *if process.returncode != 0:*                         *raise runtimeerror()  # shouldn't reuse runtimeerror but i'm lazy*     **             *with open(log_file) as f:*             *if from_mark:*                 *f.seek(from_mark)*     **                 *while true:*                 *# first, if we have a process to check, then check it.*                 *# skip on windows - stdout/stderr is cassandra.bat*                 *if not common.is_win() and not output_read:*                     *if process:*                         *process.poll()*                         *if process.returncode is not none:*                             *self.print_process_output(self.name, process, verbose)*                             *output_read = true*                             *if process.returncode != 0:*                                 *raise runtimeerror()  # shouldn't reuse runtimeerror but i'm lazy*     **                     *line = f.readline()*                 *if line:*                     *reads = reads + line*                     *for e in tofind:*                         *m = e.search(line)*                         *if m:*                             *matchings.append((line, m))*                             *tofind.remove(e)*                             *if len(tofind) == 0:*                                 *return matchings[0] if isinstance(exprs, string_types) else matchings*                 *else:*                     *# yep, it's ugly*                     *time.sleep(1)*                     *if start + timeout < time.time():* *>                       raise timeouterror(time.strftime(\"%d %b %y %h:%m:%s\", time.gmtime()) + \" [\" + self.name + \"] missing: \" + str([e.pattern for e in tofind]) + \":\\n\" + reads[:50] + \".....\\nsee {} for remainder\".format(filename))* *e                       ccmlib.node.timeouterror: 02 mar 2020 17:16:34 [node3] missing: ['some data streaming failed']:* *e                       info  [main] 2020-03-02 12:06:34,963 yamlconfigura.....* *e                       see system.log for remainder*   *../../dtest-new2/src/ccm/ccmlib/node.py*:536: timeouterror ----------------------------------------------------------------------------------------------------------- captured stdout setup ------------------------------------------------------------------------------------------------------------ 11:55:37,343 ccm debug log-watching thread starting. ------------------------------------------------------------------------------------------------------------- captured log setup ------------------------------------------------------------------------------------------------------------- 11:55:37,239 conftest info starting execution of test_resumable_bootstrap at 2020-03-02 11:55:37.239467 11:55:37,240 dtest_setup info cluster ccm directory: /var/folders/ql/nvcz74bd67d3vhw7227mpjm40000gp/t/dtest-fzxwwt9c ------------------------------------------------------------------------------------------------------------ captured stdout call ------------------------------------------------------------------------------------------------------------ install rule inject stream failure   ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:06:04,870 ccm debug log-watching thread exiting. ----------------------------------------------------------------------------------------------------------- captured stdout setup ------------------------------------------------------------------------------------------------------------ 12:06:06,20 ccm debug log-watching thread starting. ------------------------------------------------------------------------------------------------------------- captured log setup ------------------------------------------------------------------------------------------------------------- 12:06:05,887 conftest info starting execution of test_resumable_bootstrap at 2020-03-02 12:06:05.887592 12:06:05,888 dtest_setup info cluster ccm directory: /var/folders/ql/nvcz74bd67d3vhw7227mpjm40000gp/t/dtest-yntcltkx ------------------------------------------------------------------------------------------------------------ captured stdout call ------------------------------------------------------------------------------------------------------------ install rule inject stream failure   ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:06:04,870 ccm debug log-watching thread exiting. ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:06:04,870 ccm debug log-watching thread exiting. ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:16:34,818 ccm debug log-watching thread exiting. ===flaky test report===   test_resumable_bootstrap failed (1 runs remaining out of 2). <class 'ccmlib.node.timeouterror'> 02 mar 2020 17:06:04 [node3] missing: ['some data streaming failed']: info  [main] 2020-03-02 11:56:05,081 yamlconfigura..... see system.log for remainder [<tracebackentry /users/ekaterina.dimitri/ideaprojects/cassandra-dtest-d/bootstrap_test.py:365>, <tracebackentry /users/ekaterina.dimitri/dtest-new2/src/ccm/ccmlib/node.py:536>] test_resumable_bootstrap failed; it passed 0 out of the required 1 times. <class 'ccmlib.node.timeouterror'> 02 mar 2020 17:16:34 [node3] missing: ['some data streaming failed']: info  [main] 2020-03-02 12:06:34,963 yamlconfigura..... see system.log for remainder [<tracebackentry /users/ekaterina.dimitri/ideaprojects/cassandra-dtest-d/bootstrap_test.py:365>, <tracebackentry /users/ekaterina.dimitri/dtest-new2/src/ccm/ccmlib/node.py:536>]   ===end flaky test report=== *======================================================================================================== 1 failed in 1258.32 seconds =========================================================================================================*<stacktrace> <code> ================================================================================================================== failures ================================================================================================================== *___________________________________________________________________________________________________ testbootstrap.test_resumable_bootstrap ___________________________________________________________________________________________________*   self = <bootstrap_test.testbootstrap object at 0x10fd488d0>       *@since('2.2')*     *def test_resumable_bootstrap(self):*         *'''*             *test resuming bootstrap after data streaming failure*             *'''*         *cluster = self.cluster*         *cluster.populate(2)*     **             *node1 = cluster.nodes['node1']*         *# set up byteman*         *node1.byteman_port = '8100'*         *node1.import_config_files()*     **             *cluster.start(wait_other_notice=true)*         *# kill stream to node3 in the middle of streaming to let it fail*         *if cluster.version() < '4.0':*             *node1.byteman_submit([self.byteman_submit_path_pre_4_0])*         *else:*             *node1.byteman_submit([self.byteman_submit_path_4_0])*         *node1.stress(['write', 'n=1k', 'no-warmup', 'cl=two', '-schema', 'replication(factor=2)', '-rate', 'threads=50'])*         *cluster.flush()*     **             *# start bootstrapping node3 and wait for streaming*         *node3 = new_node(cluster)*         *node3.start(wait_other_notice=false)*     **             *# let streaming fail as we expect* *>       node3.watch_log_for('some data streaming failed')*   *bootstrap_test.py*:365:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    self = <ccmlib.node.node object at 0x10fdf1810>, exprs = 'some data streaming failed', from_mark = none, timeout = 600, process = none, verbose = false, filename = 'system.log'       *def watch_log_for(self, exprs, from_mark=none, timeout=600, process=none, verbose=false, filename='system.log'):*         *'''*             *watch the log until one or more (regular) expression are found.*             *this methods when all the expressions have been found or the method*             *timeouts (a timeouterror is then raised). on successful completion,*             *a list of pair (line matched, match object) is returned.*             *'''*         *start = time.time()*         *tofind = [exprs] if isinstance(exprs, string_types) else exprs*         *tofind = [re.compile(e) for e in tofind]*         *matchings = []*         *reads = ''*         *if len(tofind) == 0:*             *return none*     **             *log_file = os.path.join(self.get_path(), 'logs', filename)*         *output_read = false*         *while not os.path.exists(log_file):*             *time.sleep(.5)*             *if start + timeout < time.time():*                 *raise timeouterror(time.strftime('%d %b %y %h:%m:%s', time.gmtime()) + ' [' + self.name + '] timed out waiting for {} to be created.'.format(log_file))*             *if process and not output_read:*                 *process.poll()*                 *if process.returncode is not none:*                     *self.print_process_output(self.name, process, verbose)*                     *output_read = true*                     *if process.returncode != 0:*                         *raise runtimeerror()  # shouldn't reuse runtimeerror but i'm lazy*     **             *with open(log_file) as f:*             *if from_mark:*                 *f.seek(from_mark)*     **                 *while true:*                 *# first, if we have a process to check, then check it.*                 *# skip on windows - stdout/stderr is cassandra.bat*                 *if not common.is_win() and not output_read:*                     *if process:*                         *process.poll()*                         *if process.returncode is not none:*                             *self.print_process_output(self.name, process, verbose)*                             *output_read = true*                             *if process.returncode != 0:*                                 *raise runtimeerror()  # shouldn't reuse runtimeerror but i'm lazy*     **                     *line = f.readline()*                 *if line:*                     *reads = reads + line*                     *for e in tofind:*                         *m = e.search(line)*                         *if m:*                             *matchings.append((line, m))*                             *tofind.remove(e)*                             *if len(tofind) == 0:*                                 *return matchings[0] if isinstance(exprs, string_types) else matchings*                 *else:*                     *# yep, it's ugly*                     *time.sleep(1)*                     *if start + timeout < time.time():* *>                       raise timeouterror(time.strftime('%d %b %y %h:%m:%s', time.gmtime()) + ' [' + self.name + '] missing: ' + str([e.pattern for e in tofind]) + ':/n' + reads[:50] + '...../nsee {} for remainder'.format(filename))* *e                       ccmlib.node.timeouterror: 02 mar 2020 17:16:34 [node3] missing: ['some data streaming failed']:* *e                       info  [main] 2020-03-02 12:06:34,963 yamlconfigura.....* *e                       see system.log for remainder*   *../../dtest-new2/src/ccm/ccmlib/node.py*:536: timeouterror ----------------------------------------------------------------------------------------------------------- captured stdout setup ------------------------------------------------------------------------------------------------------------ 11:55:37,343 ccm debug log-watching thread starting. ------------------------------------------------------------------------------------------------------------- captured log setup ------------------------------------------------------------------------------------------------------------- 11:55:37,239 conftest info starting execution of test_resumable_bootstrap at 2020-03-02 11:55:37.239467 11:55:37,240 dtest_setup info cluster ccm directory: /var/folders/ql/nvcz74bd67d3vhw7227mpjm40000gp/t/dtest-fzxwwt9c ------------------------------------------------------------------------------------------------------------ captured stdout call ------------------------------------------------------------------------------------------------------------ install rule inject stream failure   ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:06:04,870 ccm debug log-watching thread exiting. ----------------------------------------------------------------------------------------------------------- captured stdout setup ------------------------------------------------------------------------------------------------------------ 12:06:06,20 ccm debug log-watching thread starting. ------------------------------------------------------------------------------------------------------------- captured log setup ------------------------------------------------------------------------------------------------------------- 12:06:05,887 conftest info starting execution of test_resumable_bootstrap at 2020-03-02 12:06:05.887592 12:06:05,888 dtest_setup info cluster ccm directory: /var/folders/ql/nvcz74bd67d3vhw7227mpjm40000gp/t/dtest-yntcltkx ------------------------------------------------------------------------------------------------------------ captured stdout call ------------------------------------------------------------------------------------------------------------ install rule inject stream failure   ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:06:04,870 ccm debug log-watching thread exiting. ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:06:04,870 ccm debug log-watching thread exiting. ---------------------------------------------------------------------------------------------------------- captured stdout teardown ---------------------------------------------------------------------------------------------------------- 12:16:34,818 ccm debug log-watching thread exiting. ===flaky test report===   test_resumable_bootstrap failed (1 runs remaining out of 2). <class 'ccmlib.node.timeouterror'> 02 mar 2020 17:06:04 [node3] missing: ['some data streaming failed']: info  [main] 2020-03-02 11:56:05,081 yamlconfigura..... see system.log for remainder [<tracebackentry /users/ekaterina.dimitri/ideaprojects/cassandra-dtest-d/bootstrap_test.py:365>, <tracebackentry /users/ekaterina.dimitri/dtest-new2/src/ccm/ccmlib/node.py:536>] test_resumable_bootstrap failed; it passed 0 out of the required 1 times. <class 'ccmlib.node.timeouterror'> 02 mar 2020 17:16:34 [node3] missing: ['some data streaming failed']: info  [main] 2020-03-02 12:06:34,963 yamlconfigura..... see system.log for remainder [<tracebackentry /users/ekaterina.dimitri/ideaprojects/cassandra-dtest-d/bootstrap_test.py:365>, <tracebackentry /users/ekaterina.dimitri/dtest-new2/src/ccm/ccmlib/node.py:536>]   ===end flaky test report=== *======================================================================================================== 1 failed in 1258.32 seconds =========================================================================================================* <text> fails 2 out of 10 times on mac/java 8",
        "label": 165
    },
    {
        "text": "stop accepting emails to google groups for cassandra <description> what do people thing about setting the old google groups addresses to not accept emails any longer? there are still emails here and there that make it onto that list, and it's never good to turn away interested community members, but i can see the advantage of consolidating the lists soon. thoughts? thanks,  jeff<stacktrace> <code> <text> what do people thing about setting the old google groups addresses to not accept emails any longer? there are still emails here and there that make it onto that list, and it's never good to turn away interested community members, but i can see the advantage of consolidating the lists soon. thoughts? thanks,  jeff",
        "label": 239
    },
    {
        "text": "setting ttl on some columns seems to expire whole row <description> i create a table with 4 columns, set a ttl on 2 of the columns and when the ttl is up, the entire row disappears. cqlsh:myks> create table paging_test (         ...   id int,         ...   mytext text,         ...   anothervalue text,         ...   somevalue text,         ...   primary key (id, mytext)         ... ); cqlsh:myks> insert into paging_test (id, mytext, anothervalue, somevalue) values (1, 'foo', 'some', 'another'); cqlsh:myks> select * from paging_test;  id | mytext | anothervalue | somevalue ----+--------+--------------+-----------   1 |    foo |         some |   another (1 rows) cqlsh:myks> update paging_test using ttl 10         ...   set somevalue='one', anothervalue='two'         ...   where id = 1 and mytext = 'foo'; cqlsh:myks> select * from paging_test;  id | mytext | anothervalue | somevalue ----+--------+--------------+-----------   1 |    foo |          two |       one (1 rows) cqlsh:myks> -- wait for it.... cqlsh:myks> select * from paging_test; (0 rows)<stacktrace> <code> cqlsh:myks> create table paging_test (         ...   id int,         ...   mytext text,         ...   anothervalue text,         ...   somevalue text,         ...   primary key (id, mytext)         ... ); cqlsh:myks> insert into paging_test (id, mytext, anothervalue, somevalue) values (1, 'foo', 'some', 'another'); cqlsh:myks> select * from paging_test;  id | mytext | anothervalue | somevalue ----+--------+--------------+-----------   1 |    foo |         some |   another (1 rows) cqlsh:myks> update paging_test using ttl 10         ...   set somevalue='one', anothervalue='two'         ...   where id = 1 and mytext = 'foo'; cqlsh:myks> select * from paging_test;  id | mytext | anothervalue | somevalue ----+--------+--------------+-----------   1 |    foo |          two |       one (1 rows) cqlsh:myks> -- wait for it.... cqlsh:myks> select * from paging_test; (0 rows) <text> i create a table with 4 columns, set a ttl on 2 of the columns and when the ttl is up, the entire row disappears.",
        "label": 520
    },
    {
        "text": "databasedescriptor applyconfig should log stacktrace in case of eception during seed provider creation <description> the comment says that: \"see log for stacktrace.\", but with the the flag \"false\" stacktrace is not logged. logging stacktrace will save some time when someone (like me) mess up with the configuration.<stacktrace> <code> <text> the comment says that: 'see log for stacktrace.', but with the the flag 'false' stacktrace is not logged. logging stacktrace will save some time when someone (like me) mess up with the configuration.",
        "label": 43
    },
    {
        "text": "confighelper setinputcolumnfamily incompatible with upper cased keyspaces since <description> hi, we have a keyspace starting with an upper-case character: visitors.  we are trying to run a map reduce job on one of the column family of this keyspace. to specify the keyspace it seems we have to use:  org.apache.cassandra.hadoop.  confighelper.setinputcolumnfamily(conf, keyspace, columnfamily); if we do:  confighelper.setinputcolumnfamily(conf, \"visitors\", columnfamily); we get: com.datastax.driver.core.exceptions.invalidqueryexception: keyspace 'visitors' does not exist  at com.datastax.driver.core.exceptions.invalidqueryexception.copy(invalidqueryexception.java:35)  at com.datastax.driver.core.defaultresultsetfuture.extractcausefromexecutionexception(defaultresultsetfuture.java:256)  at com.datastax.driver.core.sessionmanager.setkeyspace(sessionmanager.java:335) ... and if we do:  confighelper.setinputcolumnfamily(conf, \"\\\"visitors\\\"\", columnfamily); we get:  exception in thread \"main\" java.lang.runtimeexception: invalidrequestexception(why:no such keyspace: \"visitors\")  at org.apache.cassandra.hadoop.abstractcolumnfamilyinputformat.getrangemap(abstractcolumnfamilyinputformat.java:339)  at org.apache.cassandra.hadoop.abstractcolumnfamilyinputformat.getsplits(abstractcolumnfamilyinputformat.java:125)  at org.apache.hadoop.mapred.jobclient.writenewsplits(jobclient.java:962)  at org.apache.hadoop.mapred.jobclient.writesplits(jobclient.java:979)  ... this is working just fine if the keyspace is lowercase.  and it was working just fine with cassandra 2.0.6. but with cassandra 2.0.7, and the addition of datastax's java driver in the dependencies, i am getting this error.<stacktrace> com.datastax.driver.core.exceptions.invalidqueryexception: keyspace 'visitors' does not exist  at com.datastax.driver.core.exceptions.invalidqueryexception.copy(invalidqueryexception.java:35)  at com.datastax.driver.core.defaultresultsetfuture.extractcausefromexecutionexception(defaultresultsetfuture.java:256)  at com.datastax.driver.core.sessionmanager.setkeyspace(sessionmanager.java:335) and if we do:  confighelper.setinputcolumnfamily(conf, '/'visitors/'', columnfamily); we get:  exception in thread 'main' java.lang.runtimeexception: invalidrequestexception(why:no such keyspace: 'visitors')  at org.apache.cassandra.hadoop.abstractcolumnfamilyinputformat.getrangemap(abstractcolumnfamilyinputformat.java:339)  at org.apache.cassandra.hadoop.abstractcolumnfamilyinputformat.getsplits(abstractcolumnfamilyinputformat.java:125)  at org.apache.hadoop.mapred.jobclient.writenewsplits(jobclient.java:962)  at org.apache.hadoop.mapred.jobclient.writesplits(jobclient.java:979)  ... <code> to specify the keyspace it seems we have to use:  org.apache.cassandra.hadoop.  confighelper.setinputcolumnfamily(conf, keyspace, columnfamily); ... <text> hi, we have a keyspace starting with an upper-case character: visitors.  we are trying to run a map reduce job on one of the column family of this keyspace. if we do:  confighelper.setinputcolumnfamily(conf, 'visitors', columnfamily); we get: this is working just fine if the keyspace is lowercase.  and it was working just fine with cassandra 2.0.6. but with cassandra 2.0.7, and the addition of datastax's java driver in the dependencies, i am getting this error.",
        "label": 22
    },
    {
        "text": "exception during startup  unable to gossip with any seeds <description> when opscenter 4.1.4 or 5.0.1 tries to provision a 2-node dsc 2.0.10 cluster in either ec2 or locally, an error occurs sometimes with one of the nodes refusing to start c*. the error in the /var/log/cassandra/system.log is: error [main] 2014-10-06 15:54:52,292 cassandradaemon.java (line 513) exception encountered during startup  java.lang.runtimeexception: unable to gossip with any seeds  at org.apache.cassandra.gms.gossiper.doshadowround(gossiper.java:1200)  at org.apache.cassandra.service.storageservice.checkforendpointcollision(storageservice.java:444)  at org.apache.cassandra.service.storageservice.preparetojoin(storageservice.java:655)  at org.apache.cassandra.service.storageservice.initserver(storageservice.java:609)  at org.apache.cassandra.service.storageservice.initserver(storageservice.java:502)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:378)  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:496)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:585)  info [storageserviceshutdownhook] 2014-10-06 15:54:52,326 gossiper.java (line 1279) announcing shutdown  info [storageserviceshutdownhook] 2014-10-06 15:54:54,326 messagingservice.java (line 701) waiting for messaging service to quiesce  info [accept-localhost/127.0.0.1] 2014-10-06 15:54:54,327 messagingservice.java (line 941) messagingservice has terminated the accept() thread this errors does not always occur when provisioning a 2-node cluster, but probably around half of the time on only one of the nodes. i haven't been able to reproduce this error with dsc 2.0.9, and there have been no code or definition file changes in opscenter. i can reproduce locally with the above steps. i'm happy to test any proposed fixes since i'm the only person able to reproduce reliably so far.<stacktrace> error [main] 2014-10-06 15:54:52,292 cassandradaemon.java (line 513) exception encountered during startup  java.lang.runtimeexception: unable to gossip with any seeds  at org.apache.cassandra.gms.gossiper.doshadowround(gossiper.java:1200)  at org.apache.cassandra.service.storageservice.checkforendpointcollision(storageservice.java:444)  at org.apache.cassandra.service.storageservice.preparetojoin(storageservice.java:655)  at org.apache.cassandra.service.storageservice.initserver(storageservice.java:609)  at org.apache.cassandra.service.storageservice.initserver(storageservice.java:502)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:378)  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:496)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:585)  info [storageserviceshutdownhook] 2014-10-06 15:54:52,326 gossiper.java (line 1279) announcing shutdown  info [storageserviceshutdownhook] 2014-10-06 15:54:54,326 messagingservice.java (line 701) waiting for messaging service to quiesce  info [accept-localhost/127.0.0.1] 2014-10-06 15:54:54,327 messagingservice.java (line 941) messagingservice has terminated the accept() thread <code> <text> when opscenter 4.1.4 or 5.0.1 tries to provision a 2-node dsc 2.0.10 cluster in either ec2 or locally, an error occurs sometimes with one of the nodes refusing to start c*. the error in the /var/log/cassandra/system.log is: this errors does not always occur when provisioning a 2-node cluster, but probably around half of the time on only one of the nodes. i haven't been able to reproduce this error with dsc 2.0.9, and there have been no code or definition file changes in opscenter. i can reproduce locally with the above steps. i'm happy to test any proposed fixes since i'm the only person able to reproduce reliably so far.",
        "label": 508
    },
    {
        "text": "make hinted handoff throttle in kb configurable via nodetool <description> transfer of stored hints can peg the cpu of the node performing the sending of the hints. we have a throttle \"hinted_handoff_throttle_delay_in_ms\", but it requires a restart. it would be helpful if this were configurable via nodetool to avoid the reboot.<stacktrace> <code> <text> transfer of stored hints can peg the cpu of the node performing the sending of the hints. we have a throttle 'hinted_handoff_throttle_delay_in_ms', but it requires a restart. it would be helpful if this were configurable via nodetool to avoid the reboot.",
        "label": 315
    },
    {
        "text": "allow cassandra to start on a solaris machine  <description> cassandra ($cassandra_home/bin/cassandra) fails to start with a series of errors, fixing one reveals the next. these are the errors:  bin/cassandra: syntax error at line 27: `system_memory_in_mb=$' unexpected  bin/cassandra: syntax error at line 100: `check_openjdk=$' unexpected  bin/cassandra: test: argument expected<stacktrace> <code> <text> cassandra ($cassandra_home/bin/cassandra) fails to start with a series of errors, fixing one reveals the next. these are the errors:  bin/cassandra: syntax error at line 27: `system_memory_in_mb=$' unexpected  bin/cassandra: syntax error at line 100: `check_openjdk=$' unexpected  bin/cassandra: test: argument expected",
        "label": 303
    },
    {
        "text": "threadpoolexecutor creates threads as non daemon and will block on shutdown by default <description> this is most obviously visible in optionaltasks which should not block shutdown, but often does.<stacktrace> <code> <text> this is most obviously visible in optionaltasks which should not block shutdown, but often does.",
        "label": 274
    },
    {
        "text": "get slice needs to support desc from last column <description> at the moment there's no way to ask for a slice starting with the last column and going desc<stacktrace> <code> <text> at the moment there's no way to ask for a slice starting with the last column and going desc",
        "label": 285
    },
    {
        "text": "group by select queries query results differ when using select   vs select fields <description> i get two different out with these 2 queries.  the only difference between the 2 queries is that one does \u2018select *\u2019 and other does \u2018select specific fields\u2019 without any aggregate functions. i am using apache cassandra 3.10. consistency level set to local_quorum.  cassandra@cqlsh> select * from wp.position where account_id = 'user_1'; {{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}  ---------------------------------------------------------------------------------+--------------------------------  {{ user_1 | amzn | 2 | 1239.2 | 0 | 1011 | null | 2018-01-25 17:18:07.158000+0000}}  {{ user_1 | amzn | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}} (2 rows)  cassandra@cqlsh> select * from wp.position where account_id = 'user_1' group by security_id; {{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}  ---------------------------------------------------------------------------------+--------------------------------  {{ user_1 | amzn | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}} (1 rows)  cassandra@cqlsh> select account_id,security_id, counter, avg_exec_price,quantity, update_time from wp.position where account_id = 'user_1' group by security_id ; {{ account_id | security_id | counter | avg_exec_price | quantity | update_time}}  ---------------------------------------------------+--------------------------------  {{ user_1 | amzn | 2 | 1239.2 | 1011 | 2018-01-25 17:18:07.158000+0000}} (1 rows) table description:  create table wp.position (  {{ account_id text,}}  {{ security_id text,}}  {{ counter bigint,}}  {{ avg_exec_price double,}}  {{ pending_quantity double,}}  {{ quantity double,}}  {{ transaction_id uuid,}}  {{ update_time timestamp,}}  {{ primary key (account_id, security_id, counter)}}  ) with clustering order by (security_id asc, counter desc) <stacktrace> <code> consistency level set to local_quorum.  cassandra@cqlsh> select * from wp.position where account_id = 'user_1'; {{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}  ---------------------------------------------------------------------------------+--------------------------------  {{ user_1 | amzn | 2 | 1239.2 | 0 | 1011 | null | 2018-01-25 17:18:07.158000+0000}}  {{ user_1 | amzn | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}} (2 rows)  cassandra@cqlsh> select * from wp.position where account_id = 'user_1' group by security_id; {{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}  ---------------------------------------------------------------------------------+--------------------------------  {{ user_1 | amzn | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}} (1 rows)  cassandra@cqlsh> select account_id,security_id, counter, avg_exec_price,quantity, update_time from wp.position where account_id = 'user_1' group by security_id ; {{ account_id | security_id | counter | avg_exec_price | quantity | update_time}}  ---------------------------------------------------+--------------------------------  {{ user_1 | amzn | 2 | 1239.2 | 1011 | 2018-01-25 17:18:07.158000+0000}} (1 rows) table description:  create table wp.position (  {{ account_id text,}}  {{ security_id text,}}  {{ counter bigint,}}  {{ avg_exec_price double,}}  {{ pending_quantity double,}}  {{ quantity double,}}  {{ transaction_id uuid,}}  {{ update_time timestamp,}}  {{ primary key (account_id, security_id, counter)}}  ) with clustering order by (security_id asc, counter desc) <text> i get two different out with these 2 queries.  the only difference between the 2 queries is that one does 'select *' and other does 'select specific fields' without any aggregate functions. i am using apache cassandra 3.10. ",
        "label": 69
    },
    {
        "text": "blobas  function results not validated <description> the results of the blobas*() functions are not validated. here are some examples: non-type1 uuid inserted into timeuuid column: create table foo (k int primary key, v timeuuid); insert into foo (0, blobastimeuuid(0x00000000000000000000000000000000)); blob with length > 4 inserted into an int column: create table bar (k int primary key, v int); insert into bar (k, v) values (0, blobasint(0x0000000000)); non-ascii characters inserted into an ascii column: create table baz (k int primary key, v ascii); insert into baz (k, v) values (0, blobasascii(0xffffffff)); some of these (like the int column) could cause issues that look like corruption.<stacktrace> <code> create table foo (k int primary key, v timeuuid); insert into foo (0, blobastimeuuid(0x00000000000000000000000000000000)); create table bar (k int primary key, v int); insert into bar (k, v) values (0, blobasint(0x0000000000)); create table baz (k int primary key, v ascii); insert into baz (k, v) values (0, blobasascii(0xffffffff)); <text> the results of the blobas*() functions are not validated. here are some examples: non-type1 uuid inserted into timeuuid column: blob with length > 4 inserted into an int column: non-ascii characters inserted into an ascii column: some of these (like the int column) could cause issues that look like corruption.",
        "label": 520
    },
    {
        "text": "dtest failure in snapshot test testarchivecommitlog test archive commitlog <description> example failure: http://cassci.datastax.com/job/trunk_dtest_win32/416/testreport/snapshot_test/testarchivecommitlog/test_archive_commitlog failed on cassci build trunk_dtest_win32 #416 relevant error is pasted. this is clearly a test problem. no idea why it only happens on windows, as of yet. affecting most tests in the testarchivecommitlog suite warn: failed to flush node: node1 on shutdown. unexpected error in node1 log, error:  error [main] 2016-05-13 21:15:02,701 cassandradaemon.java:729 - fatal configuration error org.apache.cassandra.exceptions.configurationexception: cannot change the number of tokens from 64 to 32 at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:1043) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:740) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) ~[main/:na] at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:368) [main/:na] at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:583) [main/:na] at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:712) [main/:na]<stacktrace> warn: failed to flush node: node1 on shutdown. unexpected error in node1 log, error:  error [main] 2016-05-13 21:15:02,701 cassandradaemon.java:729 - fatal configuration error org.apache.cassandra.exceptions.configurationexception: cannot change the number of tokens from 64 to 32 at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:1043) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:740) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) ~[main/:na] at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:368) [main/:na] at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:583) [main/:na] at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:712) [main/:na] <code> http://cassci.datastax.com/job/trunk_dtest_win32/416/testreport/snapshot_test/testarchivecommitlog/test_archive_commitlog failed on cassci build trunk_dtest_win32 #416 <text> example failure: relevant error is pasted. this is clearly a test problem. no idea why it only happens on windows, as of yet. affecting most tests in the testarchivecommitlog suite",
        "label": 252
    },
    {
        "text": " patch  make json date formatter thread safe <description> the timestampserializer.to_json_formatter (simpledateformat) is shared across thread which isn't thread safe. switch to using threadlocal<stacktrace> <code> <text> the timestampserializer.to_json_formatter (simpledateformat) is shared across thread which isn't thread safe. switch to using threadlocal",
        "label": 139
    },
    {
        "text": "refactoring to specialised functional interfaces <description> usage of specialised functional interfaces provided by jdk, will reduce the autoboxing overhead hence.<stacktrace> <code> <text> usage of specialised functional interfaces provided by jdk, will reduce the autoboxing overhead hence.",
        "label": 31
    },
    {
        "text": "upgrade metrics to <description> gh pr #123 indicates that metrics 3.1.5 will fix a reconnect bug: https://github.com/apache/cassandra/pull/123<stacktrace> <code> https://github.com/apache/cassandra/pull/123<text> gh pr #123 indicates that metrics 3.1.5 will fix a reconnect bug: ",
        "label": 241
    },
    {
        "text": "fix replaying old   commitlog in cassandra <description> our docs, and code, both explicitly say that you should drain a node before upgrading to a new major release. if you don't do what the docs explicitly tell you to do, however, cassandra won't scream at you. also, we do currently have logic to replay 1.2 commitlog in 2.0, but it seems to be slightly broken, unfortunately.<stacktrace> <code> <text> our docs, and code, both explicitly say that you should drain a node before upgrading to a new major release. if you don't do what the docs explicitly tell you to do, however, cassandra won't scream at you. also, we do currently have logic to replay 1.2 commitlog in 2.0, but it seems to be slightly broken, unfortunately.",
        "label": 18
    },
    {
        "text": "snitchproperties uses gossipingpropertyfilesnitch log factory <description>    $ git diff diff --git a/src/java/org/apache/cassandra/locator/snitchproperties.java b/src/java/org/apache/cassandra/locator/snitchproperties.java index ae27fbe..809a180 100644 --- a/src/java/org/apache/cassandra/locator/snitchproperties.java +++ b/src/java/org/apache/cassandra/locator/snitchproperties.java @@ -26,7 +26,7 @@ import org.slf4j.loggerfactory;    public class snitchproperties  { -    private static final logger logger = loggerfactory.getlogger(gossipingpropertyfilesnitch.class); +    private static final logger logger = loggerfactory.getlogger(snitchproperties.class);      public static final string rackdc_property_filename = \"cassandra-rackdc.properties\";      private static properties properties = new properties();<stacktrace> <code>    $ git diff diff --git a/src/java/org/apache/cassandra/locator/snitchproperties.java b/src/java/org/apache/cassandra/locator/snitchproperties.java index ae27fbe..809a180 100644 --- a/src/java/org/apache/cassandra/locator/snitchproperties.java +++ b/src/java/org/apache/cassandra/locator/snitchproperties.java @@ -26,7 +26,7 @@ import org.slf4j.loggerfactory;    public class snitchproperties  { -    private static final logger logger = loggerfactory.getlogger(gossipingpropertyfilesnitch.class); +    private static final logger logger = loggerfactory.getlogger(snitchproperties.class);      public static final string rackdc_property_filename = 'cassandra-rackdc.properties';      private static properties properties = new properties();<text> ",
        "label": 103
    },
    {
        "text": "indexoutofboundsexception with select json using in and order by <description> when running the following code: public class cassandrajsonorderingbug {     public static void main(string[] args) {         session session = cassandrafactory.getsession();         session.execute(\"create table thebug ( primary key (a, b), a int, b int)\");         try {             session.execute(\"insert into thebug (a, b) values (20, 30)\");             session.execute(\"insert into thebug (a, b) values (100, 200)\");             statement statement = new simplestatement(\"select json a, b from thebug where a in (20, 100) order by b\");             statement.setfetchsize(integer.max_value);             for (row w: session.execute(statement)) {                 system.out.println(w.tostring());             }         } finally {             session.execute(\"drop table thebug\");         }     } } the following exception is thrown server-side: java.lang.indexoutofboundsexception: index: 1, size: 1 at java.util.collections$singletonlist.get(collections.java:4815) ~[na:1.8.0_151] at org.apache.cassandra.cql3.statements.selectstatement$singlecolumncomparator.compare(selectstatement.java:1297) ~[apache-cassandra-3.11.1.jar:3.11.1] at org.apache.cassandra.cql3.statements.selectstatement$singlecolumncomparator.compare(selectstatement.java:1284) ~[apache-cassandra-3.11.1.jar:3.11.1] at java.util.timsort.countrunandmakeascending(timsort.java:355) ~[na:1.8.0_151] at java.util.timsort.sort(timsort.java:220) ~[na:1.8.0_151] at java.util.arrays.sort(arrays.java:1512) ~[na:1.8.0_151] at java.util.arraylist.sort(arraylist.java:1460) ~[na:1.8.0_151] at java.util.collections.sort(collections.java:175) ~[na:1.8.0_151] (full traceback attached) the accessed index is the index of the sorted column in the select json fields list.  similarly, if the select clause is changed to select json b, a from thebug where a in (20, 100) order by b then the query finishes, but the output is sorted incorrectly (by textual json representation): row[{\"b\": 200, \"a\": 100}] row[{\"b\": 30, \"a\": 20}]<stacktrace> java.lang.indexoutofboundsexception: index: 1, size: 1 at java.util.collections$singletonlist.get(collections.java:4815) ~[na:1.8.0_151] at org.apache.cassandra.cql3.statements.selectstatement$singlecolumncomparator.compare(selectstatement.java:1297) ~[apache-cassandra-3.11.1.jar:3.11.1] at org.apache.cassandra.cql3.statements.selectstatement$singlecolumncomparator.compare(selectstatement.java:1284) ~[apache-cassandra-3.11.1.jar:3.11.1] at java.util.timsort.countrunandmakeascending(timsort.java:355) ~[na:1.8.0_151] at java.util.timsort.sort(timsort.java:220) ~[na:1.8.0_151] at java.util.arrays.sort(arrays.java:1512) ~[na:1.8.0_151] at java.util.arraylist.sort(arraylist.java:1460) ~[na:1.8.0_151] at java.util.collections.sort(collections.java:175) ~[na:1.8.0_151] <code> public class cassandrajsonorderingbug {     public static void main(string[] args) {         session session = cassandrafactory.getsession();         session.execute('create table thebug ( primary key (a, b), a int, b int)');         try {             session.execute('insert into thebug (a, b) values (20, 30)');             session.execute('insert into thebug (a, b) values (100, 200)');             statement statement = new simplestatement('select json a, b from thebug where a in (20, 100) order by b');             statement.setfetchsize(integer.max_value);             for (row w: session.execute(statement)) {                 system.out.println(w.tostring());             }         } finally {             session.execute('drop table thebug');         }     } } row[{'b': 200, 'a': 100}] row[{'b': 30, 'a': 20}] (full traceback attached) select json b, a from thebug where a in (20, 100) order by b <text> when running the following code: the following exception is thrown server-side: the accessed index is the index of the sorted column in the select json fields list.  similarly, if the select clause is changed to then the query finishes, but the output is sorted incorrectly (by textual json representation):",
        "label": 182
    },
    {
        "text": "streaming session failures during node replace of same address <description> when using replace_address, gossiper applicationstate is set to hibernate, which is a down state. we are seeing that the peer nodes are seeing streaming plan request even before the gossiper on them marks the replacing node as dead. as a result, streaming on peer nodes convicts the replacing node by closing the stream handler.   i think, making the storageservice thread on the replacing node, sleep for broadcast_interval before bootstrapping, would avoid this scenario. relevant logs from peer node (see that the gossiper on peer node mark the replacing node as down, 2 secs after the streaming init request):  info [stream-init-/x.x.x.x:46436] 2014-01-26 20:42:24,388 streamresultfuture.java (line 116) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] received streaming plan for bootstrap ....  info [gossiptasks:1] 2014-01-26 20:42:25,240 streamresultfuture.java (line 181) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] session with /x.x.x.x is complete  warn [gossiptasks:1] 2014-01-26 20:42:25,240 streamresultfuture.java (line 210) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] stream failed  info [gossipstage:1] 2014-01-26 20:42:25,242 gossiper.java (line 850) inetaddress /x.x.x.x is now down error [stream-in-/x.x.x.x] 2014-01-26 20:42:25,766 streamsession.java (line 410) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] streaming error occurred java.lang.runtimeexception: outgoing stream handler has been closed         at org.apache.cassandra.streaming.connectionhandler.sendmessage(connectionhandler.java:175)         at org.apache.cassandra.streaming.streamsession.prepare(streamsession.java:436)         at org.apache.cassandra.streaming.streamsession.messagereceived(streamsession.java:358)         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:293)         at java.lang.thread.run(thread.java:722)  info [stream-in-/x.x.x.x] 2014-01-26 20:42:25,768 streamresultfuture.java (line 181) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] session with /x.x.x.x is complete  warn [stream-in-/x.x.x.x] 2014-01-26 20:42:25,768 streamresultfuture.java (line 210) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] stream failed<stacktrace>  info [stream-init-/x.x.x.x:46436] 2014-01-26 20:42:24,388 streamresultfuture.java (line 116) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] received streaming plan for bootstrap ....  info [gossiptasks:1] 2014-01-26 20:42:25,240 streamresultfuture.java (line 181) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] session with /x.x.x.x is complete  warn [gossiptasks:1] 2014-01-26 20:42:25,240 streamresultfuture.java (line 210) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] stream failed  info [gossipstage:1] 2014-01-26 20:42:25,242 gossiper.java (line 850) inetaddress /x.x.x.x is now down error [stream-in-/x.x.x.x] 2014-01-26 20:42:25,766 streamsession.java (line 410) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] streaming error occurred java.lang.runtimeexception: outgoing stream handler has been closed         at org.apache.cassandra.streaming.connectionhandler.sendmessage(connectionhandler.java:175)         at org.apache.cassandra.streaming.streamsession.prepare(streamsession.java:436)         at org.apache.cassandra.streaming.streamsession.messagereceived(streamsession.java:358)         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:293)         at java.lang.thread.run(thread.java:722)  info [stream-in-/x.x.x.x] 2014-01-26 20:42:25,768 streamresultfuture.java (line 181) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] session with /x.x.x.x is complete  warn [stream-in-/x.x.x.x] 2014-01-26 20:42:25,768 streamresultfuture.java (line 210) [stream #5c6cd940-86ca-11e3-90a0-411b913c0e88] stream failed <code> <text> when using replace_address, gossiper applicationstate is set to hibernate, which is a down state. we are seeing that the peer nodes are seeing streaming plan request even before the gossiper on them marks the replacing node as dead. as a result, streaming on peer nodes convicts the replacing node by closing the stream handler.   i think, making the storageservice thread on the replacing node, sleep for broadcast_interval before bootstrapping, would avoid this scenario. relevant logs from peer node (see that the gossiper on peer node mark the replacing node as down, 2 secs after the streaming init request):",
        "label": 85
    },
    {
        "text": " of compaction time spent in streaminghistogram update  <description> with the following table, that contains a lot of cells: create table biggraphite.datapoints_11520p_60s (     metric uuid,     time_start_ms bigint,     offset smallint,     count int,     value double,     primary key ((metric, time_start_ms), offset) ) with clustering order by (offset desc); and compaction = {'class': 'org.apache.cassandra.db.compaction.timewindowcompactionstrategy', 'compaction_window_size': '6', 'compaction_window_unit': 'hours', 'max_threshold': '32', 'min_threshold': '6'} keyspace : biggraphite         read count: 1822         read latency: 1.8870054884742042 ms.         write count: 2212271647         write latency: 0.027705127678653473 ms.         pending flushes: 0                 table: datapoints_11520p_60s                 sstable count: 47                 space used (live): 300417555945                 space used (total): 303147395017                 space used by snapshots (total): 0                 off heap memory used (total): 207453042                 sstable compression ratio: 0.4955200053039823                 number of keys (estimate): 16343723                 memtable cell count: 220576                 memtable data size: 17115128                 memtable off heap memory used: 0                 memtable switch count: 2872                 local read count: 0                 local read latency: nan ms                 local write count: 1103167888                 local write latency: 0.025 ms                 pending flushes: 0                 percent repaired: 0.0                 bloom filter false positives: 0                 bloom filter false ratio: 0.00000                 bloom filter space used: 105118296                 bloom filter off heap memory used: 106547192                 index summary off heap memory used: 27730962                 compression metadata off heap memory used: 73174888                 compacted partition minimum bytes: 61                 compacted partition maximum bytes: 51012                 compacted partition mean bytes: 7899                 average live cells per slice (last five minutes): nan                 maximum live cells per slice (last five minutes): 0                 average tombstones per slice (last five minutes): nan                 maximum tombstones per slice (last five minutes): 0                 dropped mutations: 0 it looks like a good chunk of the compaction time is lost in streaminghistogram.update() (which is used to store the estimated tombstone drop times). this could be caused by a huge number of different deletion times which would makes the bin huge but it this histogram should be capped to 100 keys. it's more likely caused by the huge number of cells. a simple solutions could be to only take into accounts part of the cells, the fact the this table has a twcs also gives us an additional hint that sampling deletion times would be fine.<stacktrace> <code> create table biggraphite.datapoints_11520p_60s (     metric uuid,     time_start_ms bigint,     offset smallint,     count int,     value double,     primary key ((metric, time_start_ms), offset) ) with clustering order by (offset desc); and compaction = {'class': 'org.apache.cassandra.db.compaction.timewindowcompactionstrategy', 'compaction_window_size': '6', 'compaction_window_unit': 'hours', 'max_threshold': '32', 'min_threshold': '6'} keyspace : biggraphite         read count: 1822         read latency: 1.8870054884742042 ms.         write count: 2212271647         write latency: 0.027705127678653473 ms.         pending flushes: 0                 table: datapoints_11520p_60s                 sstable count: 47                 space used (live): 300417555945                 space used (total): 303147395017                 space used by snapshots (total): 0                 off heap memory used (total): 207453042                 sstable compression ratio: 0.4955200053039823                 number of keys (estimate): 16343723                 memtable cell count: 220576                 memtable data size: 17115128                 memtable off heap memory used: 0                 memtable switch count: 2872                 local read count: 0                 local read latency: nan ms                 local write count: 1103167888                 local write latency: 0.025 ms                 pending flushes: 0                 percent repaired: 0.0                 bloom filter false positives: 0                 bloom filter false ratio: 0.00000                 bloom filter space used: 105118296                 bloom filter off heap memory used: 106547192                 index summary off heap memory used: 27730962                 compression metadata off heap memory used: 73174888                 compacted partition minimum bytes: 61                 compacted partition maximum bytes: 51012                 compacted partition mean bytes: 7899                 average live cells per slice (last five minutes): nan                 maximum live cells per slice (last five minutes): 0                 average tombstones per slice (last five minutes): nan                 maximum tombstones per slice (last five minutes): 0                 dropped mutations: 0 <text> with the following table, that contains a lot of cells: it looks like a good chunk of the compaction time is lost in streaminghistogram.update() (which is used to store the estimated tombstone drop times). this could be caused by a huge number of different deletion times which would makes the bin huge but it this histogram should be capped to 100 keys. it's more likely caused by the huge number of cells. a simple solutions could be to only take into accounts part of the cells, the fact the this table has a twcs also gives us an additional hint that sampling deletion times would be fine.",
        "label": 241
    },
    {
        "text": "add  nodetool cfstats  ks   cf  abilities <description> this way cfstats will only print information per keyspace/column family combinations. another related proposal as an alternative to this ticket: allow for `nodetool cfstats` to use --excludes or --includes to accept keyspace and column family arguments.<stacktrace> <code> <text> this way cfstats will only print information per keyspace/column family combinations. another related proposal as an alternative to this ticket: allow for `nodetool cfstats` to use --excludes or --includes to accept keyspace and column family arguments.",
        "label": 315
    },
    {
        "text": "option to leave omitted columns in insert json unset <description> cassandra-7304 introduced the ability to distinguish between null and unset prepared statement parameters. when inserting json objects it is not possible to profit from this as a prepared statement only has one parameter that is bound to the json object as a whole. there is no way to control null vs unset behavior for columns omitted from the json object. please extend on cassandra-7304 to include json support.  (my personal requirement is to be able to insert json objects with optional fields without incurring the overhead of creating a tombstone of every column not covered by the json object upon initial insert.)<stacktrace> <code> <text> cassandra-7304 introduced the ability to distinguish between null and unset prepared statement parameters. when inserting json objects it is not possible to profit from this as a prepared statement only has one parameter that is bound to the json object as a whole. there is no way to control null vs unset behavior for columns omitted from the json object. please extend on cassandra-7304 to include json support.  (my personal requirement is to be able to insert json objects with optional fields without incurring the overhead of creating a tombstone of every column not covered by the json object upon initial insert.)",
        "label": 391
    },
    {
        "text": "dtest failure in topology test testtopology simple decommission test <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1223/testreport/topology_test/testtopology/simple_decommission_test failed on cassci build trunk_dtest #1223 the problem is that node3 detected node2 as down before the stop call was made, so the wait_other_notice check fails. the fix here is almost certainly as simple as just changing that line to node2.stop()<stacktrace> <code> http://cassci.datastax.com/job/trunk_dtest/1223/testreport/topology_test/testtopology/simple_decommission_test <text> example failure: failed on cassci build trunk_dtest #1223 the problem is that node3 detected node2 as down before the stop call was made, so the wait_other_notice check fails. the fix here is almost certainly as simple as just changing that line to node2.stop()",
        "label": 428
    },
    {
        "text": "document users and permissions in cql docs <description> the cql3 docs don't cover create user, alter user, drop user, list users, grant, or revoke.<stacktrace> <code> <text> the cql3 docs don't cover create user, alter user, drop user, list users, grant, or revoke.",
        "label": 474
    },
    {
        "text": "do not allow use of collection with compact storage <description> you can define columnfamily with collection type and compact storage as follows: create table test (   user ascii primary key,   mails list<text> ) with compact storage; this does not make sense and end up error when inserting data to collection. insert into test (user, mails) values ('foo', ['foo@foo.org']); i think it is better not to allow defining such columnfamily.<stacktrace> <code> create table test (   user ascii primary key,   mails list<text> ) with compact storage; insert into test (user, mails) values ('foo', ['foo@foo.org']); <text> you can define columnfamily with collection type and compact storage as follows: this does not make sense and end up error when inserting data to collection. i think it is better not to allow defining such columnfamily.",
        "label": 520
    },
    {
        "text": "can't use rackinferringsnitch and cql jdbc's  create keyspace  with networktopologystrategy <description> if using the cql jdbc driver, there's a problem with using rackinferringsnitch 1. with rackinferringsnitch, the datacenter names are numeric  2. with cql and networktopologystrategy, the data center replicas are specified as strategy_options:<dc-name>=<#-of-replicas>  3. using a number for <dc-name> fails  4. using a quoted number for <dc-name> fails<stacktrace> <code> <text> if using the cql jdbc driver, there's a problem with using rackinferringsnitch 1. with rackinferringsnitch, the datacenter names are numeric  2. with cql and networktopologystrategy, the data center replicas are specified as strategy_options:<dc-name>=<#-of-replicas>  3. using a number for <dc-name> fails  4. using a quoted number for <dc-name> fails",
        "label": 412
    },
    {
        "text": "bootstrap test py testbootstrap resumable bootstrap test is failing <description> when running bootstrap_test.py:testbootstrap.resumable_bootstrap_test locally, the test is failing on cassandra-3.0. when i bisect the failure, i find that 87f5e2e39c100, the commit that merged cassandra-10557 into 3.0 is the first failing commit. i can reproduce this consistently locally, but cassci is only having intermittent failures.<stacktrace> <code> <text> when running bootstrap_test.py:testbootstrap.resumable_bootstrap_test locally, the test is failing on cassandra-3.0. when i bisect the failure, i find that 87f5e2e39c100, the commit that merged cassandra-10557 into 3.0 is the first failing commit. i can reproduce this consistently locally, but cassci is only having intermittent failures.",
        "label": 577
    },
    {
        "text": "commit log pending tasks incremented during getpendingtasks <description> after upgrading from cassandra 2.0.8 to 2.1.2 our monitoring based on jmx metrics shows increasing number of commit log pending tasks. from what we found out, this value is now incremented every time it is fetched. it is easy to reproduce using tool like jconsole by refreshing value of pendingtasks attribute. quick look at the code shows that this behavior was probably introduced in 2.1 with cassandra-3578 where getpendingtasks in abstractcommitlogservice was changed from returning queue.size() to returning pending.incrementandget(). as a result this counter is incremented when getting value of commitlog.pendingtasks from org.apache.cassandra.db (now deprecated) or commitlog.pendingtasks from org.apache.cassandra.metrics. could you please clarify if this is expected behavior?  this was already mentioned in cassandra-8531.<stacktrace> <code> <text> after upgrading from cassandra 2.0.8 to 2.1.2 our monitoring based on jmx metrics shows increasing number of commit log pending tasks. from what we found out, this value is now incremented every time it is fetched. it is easy to reproduce using tool like jconsole by refreshing value of pendingtasks attribute. quick look at the code shows that this behavior was probably introduced in 2.1 with cassandra-3578 where getpendingtasks in abstractcommitlogservice was changed from returning queue.size() to returning pending.incrementandget(). as a result this counter is incremented when getting value of commitlog.pendingtasks from org.apache.cassandra.db (now deprecated) or commitlog.pendingtasks from org.apache.cassandra.metrics. could you please clarify if this is expected behavior?  this was already mentioned in cassandra-8531.",
        "label": 274
    },
    {
        "text": "cassandra   cassandra udt not returning value for list type as udt <description> i using list and its data type is udt. udt: create type fieldmap (  key text,  value text ); table: create table entity (   entity_id uuid primary key,   begining int,   domain text,   domain_type text,   entity_template_name text,   field_values list<fieldmap>,   global_entity_type text,   revision_time timeuuid,   status_key int,   status_name text,   uuid timeuuid   )  index: create index entity_domain_idx_1 on galaxy_dev.entity (domain); create index entity_field_values_idx_1 on galaxy_dev.entity (field_values); create index entity_global_entity_type_idx_1 on galaxy_dev.entity (gen_type ); query select * from entity where status_key < 3 and field_values contains {key: 'username', value: 'sprint5_200002'} and gen_type = 'user' and domain = 's4_1017.abc.com' allow filtering; the above query return value for some row and not for many rows but those rows and data's are exist. observation:  if i execute query with other than field_maps, then it returns value. i suspect the problem with list with udt. i have single node cassadra db. please let me know why this strange behavior from cassandra.<stacktrace> <code> create type fieldmap (  key text,  value text ); create table entity (   entity_id uuid primary key,   begining int,   domain text,   domain_type text,   entity_template_name text,   field_values list<fieldmap>,   global_entity_type text,   revision_time timeuuid,   status_key int,   status_name text,   uuid timeuuid   )  create index entity_domain_idx_1 on galaxy_dev.entity (domain); create index entity_field_values_idx_1 on galaxy_dev.entity (field_values); create index entity_global_entity_type_idx_1 on galaxy_dev.entity (gen_type ); select * from entity where status_key < 3 and field_values contains {key: 'username', value: 'sprint5_200002'} and gen_type = 'user' and domain = 's4_1017.abc.com' allow filtering; query <text> i using list and its data type is udt. udt: table: index: the above query return value for some row and not for many rows but those rows and data's are exist. observation:  if i execute query with other than field_maps, then it returns value. i suspect the problem with list with udt. i have single node cassadra db. please let me know why this strange behavior from cassandra.",
        "label": 69
    },
    {
        "text": "leveledcompactionstrategy is broken because of generation pre allocation in leveledmanifest  <description> leveledmanifest constructor has the following code: for (int i = 0; i < generations.length; i++) {     generations[i] = new arraylist<sstablereader>();     lastcompactedkeys[i] = new decoratedkey(cfs.partitioner.getminimumtoken(), null); } but in the decoratedkey constructor we have: assert token != null && key != null && key.remaining() > 0; so when you tried to create a cf with leveledcompressionstrategy that will result in java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception at org.apache.cassandra.thrift.cassandraserver.applymigrationonstage(cassandraserver.java:865) at org.apache.cassandra.thrift.cassandraserver.system_add_keyspace(cassandraserver.java:953) at org.apache.cassandra.thrift.cassandra$processor$system_add_keyspace.process(cassandra.java:4103) at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:3078) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:188) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:680) caused by: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222) at java.util.concurrent.futuretask.get(futuretask.java:83) at org.apache.cassandra.thrift.cassandraserver.applymigrationonstage(cassandraserver.java:857) ... 7 more caused by: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception at org.apache.cassandra.config.cfmetadata.createcompactionstrategyinstance(cfmetadata.java:770) at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:209) at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:300) at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:281) at org.apache.cassandra.db.table.initcf(table.java:339) at org.apache.cassandra.db.table.<init>(table.java:288) at org.apache.cassandra.db.table.open(table.java:117) at org.apache.cassandra.db.migration.addkeyspace.applymodels(addkeyspace.java:72) at org.apache.cassandra.db.migration.migration.apply(migration.java:156) at org.apache.cassandra.thrift.cassandraserver$2.call(cassandraserver.java:850) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) ... 3 more caused by: java.lang.reflect.invocationtargetexception at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27) at java.lang.reflect.constructor.newinstance(constructor.java:513) at org.apache.cassandra.config.cfmetadata.createcompactionstrategyinstance(cfmetadata.java:752) ... 14 more caused by: java.lang.assertionerror at org.apache.cassandra.db.decoratedkey.<init>(decoratedkey.java:55) at org.apache.cassandra.db.compaction.leveledmanifest.<init>(leveledmanifest.java:79) at org.apache.cassandra.db.compaction.leveledmanifest.create(leveledmanifest.java:85) at org.apache.cassandra.db.compaction.leveledcompactionstrategy.<init>(leveledcompactionstrategy.java:74) ... 19 more error 19:52:44,029 fatal exception in thread thread[migrationstage:1,5,main]<stacktrace> java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception at org.apache.cassandra.thrift.cassandraserver.applymigrationonstage(cassandraserver.java:865) at org.apache.cassandra.thrift.cassandraserver.system_add_keyspace(cassandraserver.java:953) at org.apache.cassandra.thrift.cassandra$processor$system_add_keyspace.process(cassandra.java:4103) at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:3078) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:188) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:680) caused by: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222) at java.util.concurrent.futuretask.get(futuretask.java:83) at org.apache.cassandra.thrift.cassandraserver.applymigrationonstage(cassandraserver.java:857) ... 7 more caused by: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception at org.apache.cassandra.config.cfmetadata.createcompactionstrategyinstance(cfmetadata.java:770) at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:209) at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:300) at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:281) at org.apache.cassandra.db.table.initcf(table.java:339) at org.apache.cassandra.db.table.<init>(table.java:288) at org.apache.cassandra.db.table.open(table.java:117) at org.apache.cassandra.db.migration.addkeyspace.applymodels(addkeyspace.java:72) at org.apache.cassandra.db.migration.migration.apply(migration.java:156) at org.apache.cassandra.thrift.cassandraserver$2.call(cassandraserver.java:850) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) ... 3 more caused by: java.lang.reflect.invocationtargetexception at sun.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at sun.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:39) at sun.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:27) at java.lang.reflect.constructor.newinstance(constructor.java:513) at org.apache.cassandra.config.cfmetadata.createcompactionstrategyinstance(cfmetadata.java:752) ... 14 more caused by: java.lang.assertionerror at org.apache.cassandra.db.decoratedkey.<init>(decoratedkey.java:55) at org.apache.cassandra.db.compaction.leveledmanifest.<init>(leveledmanifest.java:79) at org.apache.cassandra.db.compaction.leveledmanifest.create(leveledmanifest.java:85) at org.apache.cassandra.db.compaction.leveledcompactionstrategy.<init>(leveledcompactionstrategy.java:74) ... 19 more error 19:52:44,029 fatal exception in thread thread[migrationstage:1,5,main] <code> for (int i = 0; i < generations.length; i++) {     generations[i] = new arraylist<sstablereader>();     lastcompactedkeys[i] = new decoratedkey(cfs.partitioner.getminimumtoken(), null); } assert token != null && key != null && key.remaining() > 0; <text> leveledmanifest constructor has the following code: but in the decoratedkey constructor we have: so when you tried to create a cf with leveledcompressionstrategy that will result in",
        "label": 520
    },
    {
        "text": "suggest avoiding broken openjdk6 on debian as build dep <description> i ran into this myself and then today someone was reporting having the same problem on irc; there is a packaging bug in openjdk6 in lenny:  http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=501487 the effect is that when ant tries to download files over ssl, it fails complaining about:  \"java.security.invalidalgorithmparameterexception: the trustanchors parameter must be non-empty\" it turns out this works fine with the sun jvm. i'm attaching a patch which makes cassandra build on both lenny and squeeze; however, i am not sure whether other platforms may be negatively affected. the patch just requires an openjdk sufficiently new that the lenny openjdk won't quality. if there are other platforms where we do want an older openjdk, this patch might break that. in addition, i removed the \"java6-sdk\" as a sufficient dependency because that resolved to openjdk-6-jdk on lenny. i think it's a good idea to consider changing this just to decrease the initial threshold of adoption for those trying to build from source. so: this does fix the build issue on lenny, and doesn't seem to break squeeze, but i cannot promise anything about e.g. ubuntu. for the record, i'm also attaching a small self-contained test case which, when run, tries to download one of the offending pom files. it can be used to easily test weather the ssl download with work with a particular jvm.<stacktrace> <code> http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=501487 'java.security.invalidalgorithmparameterexception: the trustanchors parameter must be non-empty' <text> i ran into this myself and then today someone was reporting having the same problem on irc; there is a packaging bug in openjdk6 in lenny: the effect is that when ant tries to download files over ssl, it fails complaining about: it turns out this works fine with the sun jvm. i'm attaching a patch which makes cassandra build on both lenny and squeeze; however, i am not sure whether other platforms may be negatively affected. the patch just requires an openjdk sufficiently new that the lenny openjdk won't quality. if there are other platforms where we do want an older openjdk, this patch might break that. in addition, i removed the 'java6-sdk' as a sufficient dependency because that resolved to openjdk-6-jdk on lenny. i think it's a good idea to consider changing this just to decrease the initial threshold of adoption for those trying to build from source. so: this does fix the build issue on lenny, and doesn't seem to break squeeze, but i cannot promise anything about e.g. ubuntu. for the record, i'm also attaching a small self-contained test case which, when run, tries to download one of the offending pom files. it can be used to easily test weather the ssl download with work with a particular jvm.",
        "label": 169
    },
    {
        "text": "ant codecoverage no longer works <description> code coverage does not run currently due to cobertura jdk incompatibility. fix is coming.<stacktrace> <code> <text> code coverage does not run currently due to cobertura jdk incompatibility. fix is coming.",
        "label": 362
    },
    {
        "text": "performing a  select count  when replication factor   node count causes assertion error and timeout <description> when performing a \"select count()\" query on a table belonging to a keyspace with a replication factor less than the total node count, the following error is encountered which ultimately results in an rpc_timeout for the request: error 18:47:54,660 exception in thread thread[thread-5,5,main]  java.lang.assertionerror  at org.apache.cassandra.db.filter.idiskatomfilter$serializer.deserialize(idiskatomfilter.java:116)  at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:247)  at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:156)  at org.apache.cassandra.net.messagein.read(messagein.java:99)  at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:148)  at org.apache.cassandra.net.incomingtcpconnection.handlemodernversion(incomingtcpconnection.java:125)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:73) the issue is not encountered when the replication factor is >= node count to replicate the issue:  1) create the keyspace: create keyspace demodb with replication = {'class' : 'simplestrategy', 'replication_factor': 1} ; 2) create the table create table users (  user_name varchar,  password varchar,  gender varchar,  session_token varchar,  state varchar,  birth_year bigint,  primary key (user_name)); 3) do a cql query: \"select count( * ) from demodb.users\" ; the issue is reproducible even if the table is empty. both cqlsh and client (astyanax) api calls are affected. tested on two different clusters (2-node and 8-node)<stacktrace> error 18:47:54,660 exception in thread thread[thread-5,5,main]  java.lang.assertionerror  at org.apache.cassandra.db.filter.idiskatomfilter$serializer.deserialize(idiskatomfilter.java:116)  at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:247)  at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:156)  at org.apache.cassandra.net.messagein.read(messagein.java:99)  at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:148)  at org.apache.cassandra.net.incomingtcpconnection.handlemodernversion(incomingtcpconnection.java:125)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:73) <code> the issue is not encountered when the replication factor is >= node count ; 2) create the table create table users (  user_name varchar,  password varchar,  gender varchar,  session_token varchar,  state varchar,  birth_year bigint,  primary key (user_name)); 3) do a cql query: 'select count( * ) from demodb.users' ; <text> when performing a 'select count()' query on a table belonging to a keyspace with a replication factor less than the total node count, the following error is encountered which ultimately results in an rpc_timeout for the request: to replicate the issue:  1) create the keyspace: create keyspace demodb with replication = the issue is reproducible even if the table is empty. both cqlsh and client (astyanax) api calls are affected. tested on two different clusters (2-node and 8-node)",
        "label": 520
    },
    {
        "text": "expose nodetool cfhistograms for secondary index cfs <description> with the objectname that nodeprobe uses, the jmx query can only match mbeans with type \"columnfamilies\". secondary index cfs have a type of \"indexcolumnfamilies\", so the query won't match them. the objectname documentation indicates that you can use wildcards, which would be the perfect solution if it actually worked. i'm not sure if it's some quoted vs non-quoted pattern issue, or if it's particular to the newmbeanproxy() method, but i could not get wildcards to match the secondary index cfs. explicitly setting the \"type\" field to \"indexcolumnfamilies\" did work.<stacktrace> <code> <text> with the objectname that nodeprobe uses, the jmx query can only match mbeans with type 'columnfamilies'. secondary index cfs have a type of 'indexcolumnfamilies', so the query won't match them. the objectname documentation indicates that you can use wildcards, which would be the perfect solution if it actually worked. i'm not sure if it's some quoted vs non-quoted pattern issue, or if it's particular to the newmbeanproxy() method, but i could not get wildcards to match the secondary index cfs. explicitly setting the 'type' field to 'indexcolumnfamilies' did work.",
        "label": 85
    },
    {
        "text": "secondaryindexmanager deletefromindexes  doesn't correctly retrieve column indexes <description> secondaryindexmanager#deletefromindexes() tries to retrieve the index class from indexesbycolumn by using the raw column name, but indexes are inserted in indexesbycolumn by using the columndefinition name.<stacktrace> <code> <text> secondaryindexmanager#deletefromindexes() tries to retrieve the index class from indexesbycolumn by using the raw column name, but indexes are inserted in indexesbycolumn by using the columndefinition name.",
        "label": 491
    },
    {
        "text": "fix trigger directory detection code <description> at least when building from source, cassandra determines the trigger directory wrong. c* calculates the trigger directory as 'build/triggers' instead of 'triggers'. fbutilities.cassandrahomedir() is to blame, and should be replaced with something more robust.<stacktrace> <code> <text> at least when building from source, cassandra determines the trigger directory wrong. c* calculates the trigger directory as 'build/triggers' instead of 'triggers'. fbutilities.cassandrahomedir() is to blame, and should be replaced with something more robust.",
        "label": 555
    },
    {
        "text": "add generic way of adding sstable components required custom compaction strategy <description> cfs compaction strategy coming up in the next dse release needs to store some important information in tombstones.db and removedkeys.db files, one per sstable. however, currently cassandra issues warnings when these files are found in the data directory. additionally, when switched to sizetieredcompactionstrategy, the files are left in the data directory after compaction. the attached patch adds new components to the component class so cassandra knows about those files.<stacktrace> <code> <text> cfs compaction strategy coming up in the next dse release needs to store some important information in tombstones.db and removedkeys.db files, one per sstable. however, currently cassandra issues warnings when these files are found in the data directory. additionally, when switched to sizetieredcompactionstrategy, the files are left in the data directory after compaction. the attached patch adds new components to the component class so cassandra knows about those files.",
        "label": 432
    },
    {
        "text": "indexes lost on upgrading to <description> how to reproduce: 1. create a 2.0.12 cluster 2. create the following keyspace/table (or something similar, it's primarily the indexes that matter to this case afaict) create keyspace tshirts with replication = {'class': 'networktopologystrategy', 'datacenter1': '1'}  and durable_writes = true; create table tshirts.tshirtorders (     store text,     order_time timestamp,     order_number uuid,     color text,     qty int,     size text,     primary key (store, order_time, order_number) ) with clustering order by (order_time asc, order_number asc)     and bloom_filter_fp_chance = 0.01     and caching = '{\"keys\":\"all\", \"rows_per_partition\":\"none\"}'     and comment = ''     and compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy', 'max_threshold': '32'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99.0percentile'; create index color on tshirts.tshirtorders (color); create index size on tshirts.tshirtorders (size); 3. load it with data 4. stop the node (one node cluster is enough to replicate) 5. upgrade the node to 2.1.4 6. start the node 7. optional: run nodetool upgradesstables 8. run the following queries: select * from tshirts.tshirtorders where store = 'store 65'; select store, color, qty, size from tshirts.tshirtorders where store = 'store 65' and color = 'red'; no rows containing will appear in the indexed table. sample output: cqlsh> select * from tshirts.tshirtorders where store = 'store 65';  store    | order_time               | order_number                         | color  | qty  | size ----------+--------------------------+--------------------------------------+--------+------+------  store 65 | 2000-01-03 18:20:20+0000 | 457e60e6-da39-11e4-add3-42010af08298 |    red | 1295 |    m  store 65 | 2000-01-04 01:29:21+0000 | 45947304-da39-11e4-add3-42010af08298 |   grey | 2805 |    m  store 65 | 2000-01-04 19:55:51+0000 | 45d69220-da39-11e4-add3-42010af08298 |  brown | 3380 | xxxl  store 65 | 2000-01-04 22:45:07+0000 | 45e16894-da39-11e4-add3-42010af08298 | yellow | 7000 |  xxl  store 65 | 2000-01-05 17:09:56+0000 | 46083bd6-da39-11e4-add3-42010af08298 | purple | 2440 |    s  store 65 | 2000-01-05 19:16:48+0000 | 460cadd8-da39-11e4-add3-42010af08298 |  green | 5690 |    l  store 65 | 2000-01-06 00:26:06+0000 | 461ccdbc-da39-11e4-add3-42010af08298 |  brown | 9890 |    p  store 65 | 2000-01-06 11:35:11+0000 | 4633aa00-da39-11e4-add3-42010af08298 |  black | 9350 |    p  store 65 | 2000-01-07 06:07:20+0000 | 4658e0ea-da39-11e4-add3-42010af08298 |  black | 1300 |    s  store 65 | 2000-01-07 06:47:40+0000 | 465be93e-da39-11e4-add3-42010af08298 | purple | 9630 |   xl  store 65 | 2000-01-09 12:42:38+0000 | 46bafdd4-da39-11e4-add3-42010af08298 | purple | 1470 |    m  store 65 | 2000-01-09 19:07:35+0000 | 46c43e08-da39-11e4-add3-42010af08298 |   pink | 6005 |    s  store 65 | 2000-01-10 04:47:56+0000 | 46d4b170-da39-11e4-add3-42010af08298 |    red |  345 |   xl  store 65 | 2000-01-10 20:25:44+0000 | 46ef7d52-da39-11e4-add3-42010af08298 |   pink |  420 |  xxl  store 65 | 2000-01-11 00:55:27+0000 | 46f7a84c-da39-11e4-add3-42010af08298 | purple | 9045 |    s  store 65 | 2000-01-11 17:54:25+0000 | 4724ea00-da39-11e4-add3-42010af08298 |  green | 5030 |  xxl  store 65 | 2000-01-12 08:21:15+0000 | 473c0370-da39-11e4-add3-42010af08298 |  white | 2860 |   xl  store 65 | 2000-01-12 17:09:19+0000 | 47497d2a-da39-11e4-add3-42010af08298 |    red | 6425 |    l  store 65 | 2000-01-14 07:27:37+0000 | 478662a8-da39-11e4-add3-42010af08298 |   pink |  330 | xxxl  store 65 | 2000-01-14 11:31:38+0000 | 478b43cc-da39-11e4-add3-42010af08298 |   pink | 3335 |  xxl  store 65 | 2000-01-14 18:55:59+0000 | 47955a24-da39-11e4-add3-42010af08298 | yellow |  500 |    p  store 65 | 2000-01-15 01:59:52+0000 | 479f0c5e-da39-11e4-add3-42010af08298 |    red | 8415 |   xl  store 65 | 2000-01-15 02:26:13+0000 | 47a00c08-da39-11e4-add3-42010af08298 |  green | 2265 |    p  store 65 | 2000-01-15 14:31:50+0000 | 47b28c34-da39-11e4-add3-42010af08298 |  green | 8165 |    m  store 65 | 2000-01-16 20:39:31+0000 | 47de6908-da39-11e4-add3-42010af08298 | purple | 1330 |  xxl  store 65 | 2000-01-17 06:02:33+0000 | 47eb832c-da39-11e4-add3-42010af08298 |  black | 9495 |    m  store 65 | 2000-01-17 12:32:09+0000 | 47f4ca18-da39-11e4-add3-42010af08298 |   grey | 4645 |    l  store 65 | 2000-01-18 02:46:05+0000 | 48080c9a-da39-11e4-add3-42010af08298 |   pink | 5220 |    l  store 65 | 2000-01-18 22:38:49+0000 | 48271c0c-da39-11e4-add3-42010af08298 |    red | 3515 |   xl  store 65 | 2000-01-19 05:14:17+0000 | 48311860-da39-11e4-add3-42010af08298 |  black | 8970 |    p  store 65 | 2000-01-19 09:15:57+0000 | 48368a8e-da39-11e4-add3-42010af08298 |    red | 6110 |    p  store 65 | 2000-01-20 07:48:34+0000 | 485a21ec-da39-11e4-add3-42010af08298 |  green | 9455 | xxxl  store 65 | 2000-01-22 23:03:05+0000 | 48b94686-da39-11e4-add3-42010af08298 |    red | 5370 |    p  store 65 | 2000-01-23 00:15:27+0000 | 48bb7a32-da39-11e4-add3-42010af08298 |  green | 2465 |    s  store 65 | 2000-01-23 03:40:35+0000 | 48c10eb6-da39-11e4-add3-42010af08298 |    red |  795 |    p  store 65 | 2000-01-23 20:52:24+0000 | 48db904c-da39-11e4-add3-42010af08298 |  brown | 9690 |   xl  store 65 | 2000-01-23 23:52:08+0000 | 48df5f24-da39-11e4-add3-42010af08298 |   blue | 9330 |    s  store 65 | 2000-01-25 06:10:55+0000 | 490ae194-da39-11e4-add3-42010af08298 |   pink | 2380 |    m  store 65 | 2000-01-25 14:04:42+0000 | 4914777c-da39-11e4-add3-42010af08298 | purple | 8425 |    s  store 65 | 2000-01-25 20:31:39+0000 | 491da19e-da39-11e4-add3-42010af08298 |  brown |   70 |    m  store 65 | 2000-01-26 09:33:27+0000 | 4932033c-da39-11e4-add3-42010af08298 |  green | 8150 |    l  store 65 | 2000-01-28 10:18:38+0000 | 4976c38c-da39-11e4-add3-42010af08298 |   pink | 3175 |  xxl  store 65 | 2000-01-31 00:22:38+0000 | 49d37fbe-da39-11e4-add3-42010af08298 |  black | 8310 |   xl  store 65 | 2000-01-31 11:21:25+0000 | 49e2fcdc-da39-11e4-add3-42010af08298 |  white | 6240 | xxxl  store 65 | 2000-01-31 16:22:50+0000 | 49e9904c-da39-11e4-add3-42010af08298 |  green | 8310 | xxxl  store 65 | 2000-01-31 17:12:11+0000 | 49eac6ce-da39-11e4-add3-42010af08298 |   grey | 4315 |   xl  store 65 | 2000-02-01 13:42:19+0000 | 4a086486-da39-11e4-add3-42010af08298 |  white | 6955 | xxxl  store 65 | 2000-02-03 09:21:18+0000 | 4a47abbe-da39-11e4-add3-42010af08298 |  white | 5360 |    p  store 65 | 2000-02-03 21:09:39+0000 | 4a58acde-da39-11e4-add3-42010af08298 |   pink | 8665 |    p  store 65 | 2000-02-04 17:57:51+0000 | 4a77aa76-da39-11e4-add3-42010af08298 |  brown | 8550 |    l  store 65 | 2000-02-04 21:37:13+0000 | 4a7cdc26-da39-11e4-add3-42010af08298 |  black | 9195 |    p  store 65 | 2000-02-05 20:35:33+0000 | 4a9fc81c-da39-11e4-add3-42010af08298 |  brown | 4460 |    m  store 65 | 2000-02-06 03:28:23+0000 | 4aaa30ae-da39-11e4-add3-42010af08298 |   pink | 4175 |    m  store 65 | 2000-02-06 07:45:29+0000 | 4ab0cd4c-da39-11e4-add3-42010af08298 | yellow | 5270 |    m  store 65 | 2000-02-06 07:47:06+0000 | 4ab100d2-da39-11e4-add3-42010af08298 |   grey |  165 |    p  store 65 | 2000-02-06 20:21:10+0000 | 4ac434e0-da39-11e4-add3-42010af08298 |  black | 5480 |    p  store 65 | 2000-02-07 01:49:04+0000 | 4acca18e-da39-11e4-add3-42010af08298 |  green | 7520 |    l  store 65 | 2000-02-07 17:02:03+0000 | 4ae1a2fa-da39-11e4-add3-42010af08298 | purple | 5630 |    p  store 65 | 2000-02-09 09:46:37+0000 | 4b1bcc28-da39-11e4-add3-42010af08298 | yellow | 6985 |    s  store 65 | 2000-02-09 19:08:30+0000 | 4b2a10bc-da39-11e4-add3-42010af08298 |   blue | 7505 |    m  store 65 | 2000-02-10 02:23:35+0000 | 4b35e428-da39-11e4-add3-42010af08298 |   blue |  730 |    m  store 65 | 2000-02-10 16:12:10+0000 | 4b49dad2-da39-11e4-add3-42010af08298 |  brown | 9940 |   xl  store 65 | 2000-02-12 05:10:22+0000 | 4b7c430a-da39-11e4-add3-42010af08298 | yellow | 3890 |    s  store 65 | 2000-02-14 20:16:52+0000 | 4bd9cb42-da39-11e4-add3-42010af08298 |  green | 5335 |   xl  store 65 | 2000-02-14 22:40:38+0000 | 4bdcfb3c-da39-11e4-add3-42010af08298 |  green | 2370 | xxxl  store 65 | 2000-02-15 17:30:08+0000 | 4bf6c45e-da39-11e4-add3-42010af08298 |    red | 6875 |    l  store 65 | 2000-02-16 15:19:40+0000 | 4c16783a-da39-11e4-add3-42010af08298 |   pink | 7880 |   xl  store 65 | 2000-02-17 05:01:18+0000 | 4c2aab20-da39-11e4-add3-42010af08298 |  white |  160 |   xl  store 65 | 2000-02-17 06:45:08+0000 | 4c2d66d0-da39-11e4-add3-42010af08298 |  brown | 6005 | xxxl  store 65 | 2000-02-17 08:43:02+0000 | 4c302f8c-da39-11e4-add3-42010af08298 | purple | 4970 |    l  store 65 | 2000-02-17 21:10:53+0000 | 4c44dea0-da39-11e4-add3-42010af08298 |  white | 9530 |    m  store 65 | 2000-02-18 01:57:35+0000 | 4c4c55e0-da39-11e4-add3-42010af08298 |   blue | 5695 |    m  store 65 | 2000-02-18 06:56:21+0000 | 4c53c50a-da39-11e4-add3-42010af08298 |    red | 9705 |    s  store 65 | 2000-02-18 07:52:10+0000 | 4c555f78-da39-11e4-add3-42010af08298 |  black | 5205 |  xxl  store 65 | 2000-02-20 07:01:49+0000 | 4ca0db9c-da39-11e4-add3-42010af08298 |   pink | 2645 |    s  store 65 | 2000-02-20 07:48:56+0000 | 4ca244be-da39-11e4-add3-42010af08298 |  brown | 2465 | xxxl  store 65 | 2000-02-21 03:55:08+0000 | 4cc08474-da39-11e4-add3-42010af08298 |    red | 4095 |    m  store 65 | 2000-02-21 07:25:29+0000 | 4cc5c736-da39-11e4-add3-42010af08298 |   pink | 7200 | xxxl  store 65 | 2000-02-21 23:08:29+0000 | 4cdba1b4-da39-11e4-add3-42010af08298 |  brown | 9190 |    m  store 65 | 2000-02-21 23:08:56+0000 | 4cdbd832-da39-11e4-add3-42010af08298 |  green | 7895 |  xxl  store 65 | 2000-02-23 00:17:30+0000 | 4d021ba0-da39-11e4-add3-42010af08298 |   pink | 3955 | xxxl  store 65 | 2000-02-23 04:40:20+0000 | 4d083b5c-da39-11e4-add3-42010af08298 |   blue | 6435 |  xxl  store 65 | 2000-02-23 13:23:49+0000 | 4d1651c4-da39-11e4-add3-42010af08298 | purple | 2595 |    s  store 65 | 2000-02-24 14:51:42+0000 | 4d3b4e3e-da39-11e4-add3-42010af08298 | purple | 5685 |    m  store 65 | 2000-02-24 15:07:04+0000 | 4d3b805c-da39-11e4-add3-42010af08298 |  brown | 5045 |    s  store 65 | 2000-02-24 22:04:02+0000 | 4d44af74-da39-11e4-add3-42010af08298 |   pink | 6780 |    l  store 65 | 2000-02-25 00:53:13+0000 | 4d4910be-da39-11e4-add3-42010af08298 |  green | 1285 |  xxl  store 65 | 2000-02-26 03:48:24+0000 | 4d729498-da39-11e4-add3-42010af08298 |    red | 1895 |  xxl  store 65 | 2000-02-26 08:17:34+0000 | 4d7a717c-da39-11e4-add3-42010af08298 |  brown | 6400 |    m  store 65 | 2000-02-26 17:41:23+0000 | 4d897d02-da39-11e4-add3-42010af08298 |  white | 5870 |    s  store 65 | 2000-02-26 21:18:35+0000 | 4d8fbfbe-da39-11e4-add3-42010af08298 |  green |  265 |    p  store 65 | 2000-02-26 23:51:47+0000 | 4d93bc40-da39-11e4-add3-42010af08298 |   grey | 3950 | xxxl  store 65 | 2000-02-27 09:34:06+0000 | 4da1b480-da39-11e4-add3-42010af08298 |  white | 1150 |  xxl  store 65 | 2000-02-27 13:22:07+0000 | 4da87716-da39-11e4-add3-42010af08298 |   pink | 3395 |  xxl  store 65 | 2000-02-27 16:49:46+0000 | 4dada0f6-da39-11e4-add3-42010af08298 |   blue | 1430 |  xxl (97 rows) cqlsh> select store, color, qty, size from tshirts.tshirtorders where store = 'store 65' and color = 'red';  store | order_time | order_number | color | qty | size -------+------------+--------------+-------+-----+------ (0 rows)<stacktrace> <code> create keyspace tshirts with replication = {'class': 'networktopologystrategy', 'datacenter1': '1'}  and durable_writes = true; create table tshirts.tshirtorders (     store text,     order_time timestamp,     order_number uuid,     color text,     qty int,     size text,     primary key (store, order_time, order_number) ) with clustering order by (order_time asc, order_number asc)     and bloom_filter_fp_chance = 0.01     and caching = '{'keys':'all', 'rows_per_partition':'none'}'     and comment = ''     and compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy', 'max_threshold': '32'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99.0percentile'; create index color on tshirts.tshirtorders (color); create index size on tshirts.tshirtorders (size); select * from tshirts.tshirtorders where store = 'store 65'; select store, color, qty, size from tshirts.tshirtorders where store = 'store 65' and color = 'red'; cqlsh> select * from tshirts.tshirtorders where store = 'store 65';  store    | order_time               | order_number                         | color  | qty  | size ----------+--------------------------+--------------------------------------+--------+------+------  store 65 | 2000-01-03 18:20:20+0000 | 457e60e6-da39-11e4-add3-42010af08298 |    red | 1295 |    m  store 65 | 2000-01-04 01:29:21+0000 | 45947304-da39-11e4-add3-42010af08298 |   grey | 2805 |    m  store 65 | 2000-01-04 19:55:51+0000 | 45d69220-da39-11e4-add3-42010af08298 |  brown | 3380 | xxxl  store 65 | 2000-01-04 22:45:07+0000 | 45e16894-da39-11e4-add3-42010af08298 | yellow | 7000 |  xxl  store 65 | 2000-01-05 17:09:56+0000 | 46083bd6-da39-11e4-add3-42010af08298 | purple | 2440 |    s  store 65 | 2000-01-05 19:16:48+0000 | 460cadd8-da39-11e4-add3-42010af08298 |  green | 5690 |    l  store 65 | 2000-01-06 00:26:06+0000 | 461ccdbc-da39-11e4-add3-42010af08298 |  brown | 9890 |    p  store 65 | 2000-01-06 11:35:11+0000 | 4633aa00-da39-11e4-add3-42010af08298 |  black | 9350 |    p  store 65 | 2000-01-07 06:07:20+0000 | 4658e0ea-da39-11e4-add3-42010af08298 |  black | 1300 |    s  store 65 | 2000-01-07 06:47:40+0000 | 465be93e-da39-11e4-add3-42010af08298 | purple | 9630 |   xl  store 65 | 2000-01-09 12:42:38+0000 | 46bafdd4-da39-11e4-add3-42010af08298 | purple | 1470 |    m  store 65 | 2000-01-09 19:07:35+0000 | 46c43e08-da39-11e4-add3-42010af08298 |   pink | 6005 |    s  store 65 | 2000-01-10 04:47:56+0000 | 46d4b170-da39-11e4-add3-42010af08298 |    red |  345 |   xl  store 65 | 2000-01-10 20:25:44+0000 | 46ef7d52-da39-11e4-add3-42010af08298 |   pink |  420 |  xxl  store 65 | 2000-01-11 00:55:27+0000 | 46f7a84c-da39-11e4-add3-42010af08298 | purple | 9045 |    s  store 65 | 2000-01-11 17:54:25+0000 | 4724ea00-da39-11e4-add3-42010af08298 |  green | 5030 |  xxl  store 65 | 2000-01-12 08:21:15+0000 | 473c0370-da39-11e4-add3-42010af08298 |  white | 2860 |   xl  store 65 | 2000-01-12 17:09:19+0000 | 47497d2a-da39-11e4-add3-42010af08298 |    red | 6425 |    l  store 65 | 2000-01-14 07:27:37+0000 | 478662a8-da39-11e4-add3-42010af08298 |   pink |  330 | xxxl  store 65 | 2000-01-14 11:31:38+0000 | 478b43cc-da39-11e4-add3-42010af08298 |   pink | 3335 |  xxl  store 65 | 2000-01-14 18:55:59+0000 | 47955a24-da39-11e4-add3-42010af08298 | yellow |  500 |    p  store 65 | 2000-01-15 01:59:52+0000 | 479f0c5e-da39-11e4-add3-42010af08298 |    red | 8415 |   xl  store 65 | 2000-01-15 02:26:13+0000 | 47a00c08-da39-11e4-add3-42010af08298 |  green | 2265 |    p  store 65 | 2000-01-15 14:31:50+0000 | 47b28c34-da39-11e4-add3-42010af08298 |  green | 8165 |    m  store 65 | 2000-01-16 20:39:31+0000 | 47de6908-da39-11e4-add3-42010af08298 | purple | 1330 |  xxl  store 65 | 2000-01-17 06:02:33+0000 | 47eb832c-da39-11e4-add3-42010af08298 |  black | 9495 |    m  store 65 | 2000-01-17 12:32:09+0000 | 47f4ca18-da39-11e4-add3-42010af08298 |   grey | 4645 |    l  store 65 | 2000-01-18 02:46:05+0000 | 48080c9a-da39-11e4-add3-42010af08298 |   pink | 5220 |    l  store 65 | 2000-01-18 22:38:49+0000 | 48271c0c-da39-11e4-add3-42010af08298 |    red | 3515 |   xl  store 65 | 2000-01-19 05:14:17+0000 | 48311860-da39-11e4-add3-42010af08298 |  black | 8970 |    p  store 65 | 2000-01-19 09:15:57+0000 | 48368a8e-da39-11e4-add3-42010af08298 |    red | 6110 |    p  store 65 | 2000-01-20 07:48:34+0000 | 485a21ec-da39-11e4-add3-42010af08298 |  green | 9455 | xxxl  store 65 | 2000-01-22 23:03:05+0000 | 48b94686-da39-11e4-add3-42010af08298 |    red | 5370 |    p  store 65 | 2000-01-23 00:15:27+0000 | 48bb7a32-da39-11e4-add3-42010af08298 |  green | 2465 |    s  store 65 | 2000-01-23 03:40:35+0000 | 48c10eb6-da39-11e4-add3-42010af08298 |    red |  795 |    p  store 65 | 2000-01-23 20:52:24+0000 | 48db904c-da39-11e4-add3-42010af08298 |  brown | 9690 |   xl  store 65 | 2000-01-23 23:52:08+0000 | 48df5f24-da39-11e4-add3-42010af08298 |   blue | 9330 |    s  store 65 | 2000-01-25 06:10:55+0000 | 490ae194-da39-11e4-add3-42010af08298 |   pink | 2380 |    m  store 65 | 2000-01-25 14:04:42+0000 | 4914777c-da39-11e4-add3-42010af08298 | purple | 8425 |    s  store 65 | 2000-01-25 20:31:39+0000 | 491da19e-da39-11e4-add3-42010af08298 |  brown |   70 |    m  store 65 | 2000-01-26 09:33:27+0000 | 4932033c-da39-11e4-add3-42010af08298 |  green | 8150 |    l  store 65 | 2000-01-28 10:18:38+0000 | 4976c38c-da39-11e4-add3-42010af08298 |   pink | 3175 |  xxl  store 65 | 2000-01-31 00:22:38+0000 | 49d37fbe-da39-11e4-add3-42010af08298 |  black | 8310 |   xl  store 65 | 2000-01-31 11:21:25+0000 | 49e2fcdc-da39-11e4-add3-42010af08298 |  white | 6240 | xxxl  store 65 | 2000-01-31 16:22:50+0000 | 49e9904c-da39-11e4-add3-42010af08298 |  green | 8310 | xxxl  store 65 | 2000-01-31 17:12:11+0000 | 49eac6ce-da39-11e4-add3-42010af08298 |   grey | 4315 |   xl  store 65 | 2000-02-01 13:42:19+0000 | 4a086486-da39-11e4-add3-42010af08298 |  white | 6955 | xxxl  store 65 | 2000-02-03 09:21:18+0000 | 4a47abbe-da39-11e4-add3-42010af08298 |  white | 5360 |    p  store 65 | 2000-02-03 21:09:39+0000 | 4a58acde-da39-11e4-add3-42010af08298 |   pink | 8665 |    p  store 65 | 2000-02-04 17:57:51+0000 | 4a77aa76-da39-11e4-add3-42010af08298 |  brown | 8550 |    l  store 65 | 2000-02-04 21:37:13+0000 | 4a7cdc26-da39-11e4-add3-42010af08298 |  black | 9195 |    p  store 65 | 2000-02-05 20:35:33+0000 | 4a9fc81c-da39-11e4-add3-42010af08298 |  brown | 4460 |    m  store 65 | 2000-02-06 03:28:23+0000 | 4aaa30ae-da39-11e4-add3-42010af08298 |   pink | 4175 |    m  store 65 | 2000-02-06 07:45:29+0000 | 4ab0cd4c-da39-11e4-add3-42010af08298 | yellow | 5270 |    m  store 65 | 2000-02-06 07:47:06+0000 | 4ab100d2-da39-11e4-add3-42010af08298 |   grey |  165 |    p  store 65 | 2000-02-06 20:21:10+0000 | 4ac434e0-da39-11e4-add3-42010af08298 |  black | 5480 |    p  store 65 | 2000-02-07 01:49:04+0000 | 4acca18e-da39-11e4-add3-42010af08298 |  green | 7520 |    l  store 65 | 2000-02-07 17:02:03+0000 | 4ae1a2fa-da39-11e4-add3-42010af08298 | purple | 5630 |    p  store 65 | 2000-02-09 09:46:37+0000 | 4b1bcc28-da39-11e4-add3-42010af08298 | yellow | 6985 |    s  store 65 | 2000-02-09 19:08:30+0000 | 4b2a10bc-da39-11e4-add3-42010af08298 |   blue | 7505 |    m  store 65 | 2000-02-10 02:23:35+0000 | 4b35e428-da39-11e4-add3-42010af08298 |   blue |  730 |    m  store 65 | 2000-02-10 16:12:10+0000 | 4b49dad2-da39-11e4-add3-42010af08298 |  brown | 9940 |   xl  store 65 | 2000-02-12 05:10:22+0000 | 4b7c430a-da39-11e4-add3-42010af08298 | yellow | 3890 |    s  store 65 | 2000-02-14 20:16:52+0000 | 4bd9cb42-da39-11e4-add3-42010af08298 |  green | 5335 |   xl  store 65 | 2000-02-14 22:40:38+0000 | 4bdcfb3c-da39-11e4-add3-42010af08298 |  green | 2370 | xxxl  store 65 | 2000-02-15 17:30:08+0000 | 4bf6c45e-da39-11e4-add3-42010af08298 |    red | 6875 |    l  store 65 | 2000-02-16 15:19:40+0000 | 4c16783a-da39-11e4-add3-42010af08298 |   pink | 7880 |   xl  store 65 | 2000-02-17 05:01:18+0000 | 4c2aab20-da39-11e4-add3-42010af08298 |  white |  160 |   xl  store 65 | 2000-02-17 06:45:08+0000 | 4c2d66d0-da39-11e4-add3-42010af08298 |  brown | 6005 | xxxl  store 65 | 2000-02-17 08:43:02+0000 | 4c302f8c-da39-11e4-add3-42010af08298 | purple | 4970 |    l  store 65 | 2000-02-17 21:10:53+0000 | 4c44dea0-da39-11e4-add3-42010af08298 |  white | 9530 |    m  store 65 | 2000-02-18 01:57:35+0000 | 4c4c55e0-da39-11e4-add3-42010af08298 |   blue | 5695 |    m  store 65 | 2000-02-18 06:56:21+0000 | 4c53c50a-da39-11e4-add3-42010af08298 |    red | 9705 |    s  store 65 | 2000-02-18 07:52:10+0000 | 4c555f78-da39-11e4-add3-42010af08298 |  black | 5205 |  xxl  store 65 | 2000-02-20 07:01:49+0000 | 4ca0db9c-da39-11e4-add3-42010af08298 |   pink | 2645 |    s  store 65 | 2000-02-20 07:48:56+0000 | 4ca244be-da39-11e4-add3-42010af08298 |  brown | 2465 | xxxl  store 65 | 2000-02-21 03:55:08+0000 | 4cc08474-da39-11e4-add3-42010af08298 |    red | 4095 |    m  store 65 | 2000-02-21 07:25:29+0000 | 4cc5c736-da39-11e4-add3-42010af08298 |   pink | 7200 | xxxl  store 65 | 2000-02-21 23:08:29+0000 | 4cdba1b4-da39-11e4-add3-42010af08298 |  brown | 9190 |    m  store 65 | 2000-02-21 23:08:56+0000 | 4cdbd832-da39-11e4-add3-42010af08298 |  green | 7895 |  xxl  store 65 | 2000-02-23 00:17:30+0000 | 4d021ba0-da39-11e4-add3-42010af08298 |   pink | 3955 | xxxl  store 65 | 2000-02-23 04:40:20+0000 | 4d083b5c-da39-11e4-add3-42010af08298 |   blue | 6435 |  xxl  store 65 | 2000-02-23 13:23:49+0000 | 4d1651c4-da39-11e4-add3-42010af08298 | purple | 2595 |    s  store 65 | 2000-02-24 14:51:42+0000 | 4d3b4e3e-da39-11e4-add3-42010af08298 | purple | 5685 |    m  store 65 | 2000-02-24 15:07:04+0000 | 4d3b805c-da39-11e4-add3-42010af08298 |  brown | 5045 |    s  store 65 | 2000-02-24 22:04:02+0000 | 4d44af74-da39-11e4-add3-42010af08298 |   pink | 6780 |    l  store 65 | 2000-02-25 00:53:13+0000 | 4d4910be-da39-11e4-add3-42010af08298 |  green | 1285 |  xxl  store 65 | 2000-02-26 03:48:24+0000 | 4d729498-da39-11e4-add3-42010af08298 |    red | 1895 |  xxl  store 65 | 2000-02-26 08:17:34+0000 | 4d7a717c-da39-11e4-add3-42010af08298 |  brown | 6400 |    m  store 65 | 2000-02-26 17:41:23+0000 | 4d897d02-da39-11e4-add3-42010af08298 |  white | 5870 |    s  store 65 | 2000-02-26 21:18:35+0000 | 4d8fbfbe-da39-11e4-add3-42010af08298 |  green |  265 |    p  store 65 | 2000-02-26 23:51:47+0000 | 4d93bc40-da39-11e4-add3-42010af08298 |   grey | 3950 | xxxl  store 65 | 2000-02-27 09:34:06+0000 | 4da1b480-da39-11e4-add3-42010af08298 |  white | 1150 |  xxl  store 65 | 2000-02-27 13:22:07+0000 | 4da87716-da39-11e4-add3-42010af08298 |   pink | 3395 |  xxl  store 65 | 2000-02-27 16:49:46+0000 | 4dada0f6-da39-11e4-add3-42010af08298 |   blue | 1430 |  xxl (97 rows) cqlsh> select store, color, qty, size from tshirts.tshirtorders where store = 'store 65' and color = 'red';  store | order_time | order_number | color | qty | size -------+------------+--------------+-------+-----+------ (0 rows) <text> how to reproduce: no rows containing will appear in the indexed table. sample output:",
        "label": 474
    },
    {
        "text": "inconsistent select count and select distinct <description> when performing select count( * ) from ... i expect the results to be consistent over multiple query executions if the table at hand is not written to / deleted from in the mean time. however, in my set-up it is not. the counts returned vary considerable (several percent). the same holds for select distinct partition-key-columns from .... i have a table in a keyspace with replication_factor = 1 which is something like: create table tbl (     id frozen<id_type>,     bucket bigint,     offset int,     value double,     primary key ((id, bucket), offset) ) the frozen udt is: create type id_type (     tags map<text, text> ); the table contains around 35k rows (i'm not trying to be funny here ...). the consistency level for the queries was one.<stacktrace> <code> create table tbl (     id frozen<id_type>,     bucket bigint,     offset int,     value double,     primary key ((id, bucket), offset) ) create type id_type (     tags map<text, text> ); <text> when performing select count( * ) from ... i expect the results to be consistent over multiple query executions if the table at hand is not written to / deleted from in the mean time. however, in my set-up it is not. the counts returned vary considerable (several percent). the same holds for select distinct partition-key-columns from .... i have a table in a keyspace with replication_factor = 1 which is something like: the frozen udt is: the table contains around 35k rows (i'm not trying to be funny here ...). the consistency level for the queries was one.",
        "label": 69
    },
    {
        "text": "give crr a default input cql statement <description> inorder to ease migration from cqlpagingrecordreader to cqlrecordreader, it would be helpful if crr input_cql defaulted to a select statement that would mirror the behavior of cprr. for example for a give table with partition key `((x,y,z),c1,c2)`  it would automatically generate input_cql = select * from ks.tab where token(x,y,z) > ? and token (x,y,z) <= ? <stacktrace> <code> input_cql = select * from ks.tab where token(x,y,z) > ? and token (x,y,z) <= ?  <text> inorder to ease migration from cqlpagingrecordreader to cqlrecordreader, it would be helpful if crr input_cql defaulted to a select statement that would mirror the behavior of cprr. for example for a give table with partition key `((x,y,z),c1,c2)`  it would automatically generate",
        "label": 357
    },
    {
        "text": "cassandra leaks fds <description> cassandra leaks file descriptors like crazy. i started getting these errors after a few hours of uptime: java.lang.runtimeexception: java.io.filenotfoundexception: /var/cassandra/data/digg-items-2-data.db (too many open files)  at org.apache.cassandra.service.cassandraserver.readcolumnfamily(cassandraserver.java:84)  at org.apache.cassandra.service.cassandraserver.get_slice(cassandraserver.java:181)  at org.apache.cassandra.service.cassandra$processor$get_slice.process(cassandra.java:859)  at org.apache.cassandra.service.cassandra$processor.process(cassandra.java:817)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:252)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.io.filenotfoundexception: /var/cassandra/data/digg-items-2-data.db (too many open files)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:141)  at org.apache.cassandra.io.sequencefile$bufferreader.init(sequencefile.java:811)  at org.apache.cassandra.io.sequencefile$reader.<init>(sequencefile.java:743)  at org.apache.cassandra.io.sequencefile$bufferreader.<init>(sequencefile.java:805)  at org.apache.cassandra.io.sequencefile$columngroupreader.<init>(sequencefile.java:248)  at org.apache.cassandra.io.sstablereader.getcolumngroupreader(sstablereader.java:346)  at org.apache.cassandra.db.sstablecolumniterator.<init>(columniterator.java:61)  at org.apache.cassandra.db.columnfamilystore.getslicefrom(columnfamilystore.java:1589)  at org.apache.cassandra.db.table.getrow(table.java:596)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:60)  at org.apache.cassandra.service.storageproxy.weakreadlocal(storageproxy.java:600)  at org.apache.cassandra.service.storageproxy.readprotocol(storageproxy.java:303)  at org.apache.cassandra.service.cassandraserver.readcolumnfamily(cassandraserver.java:80) i have an open file limit of 1024. examining the lsof output for cassandra shows 975 fds for the same file: /var/cassandra/data/digg-items-2-data.db clearly, these fds are leaking somewhere.<stacktrace> java.lang.runtimeexception: java.io.filenotfoundexception: /var/cassandra/data/digg-items-2-data.db (too many open files)  at org.apache.cassandra.service.cassandraserver.readcolumnfamily(cassandraserver.java:84)  at org.apache.cassandra.service.cassandraserver.get_slice(cassandraserver.java:181)  at org.apache.cassandra.service.cassandra$processor$get_slice.process(cassandra.java:859)  at org.apache.cassandra.service.cassandra$processor.process(cassandra.java:817)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:252)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.io.filenotfoundexception: /var/cassandra/data/digg-items-2-data.db (too many open files)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:141)  at org.apache.cassandra.io.sequencefile$bufferreader.init(sequencefile.java:811)  at org.apache.cassandra.io.sequencefile$reader.<init>(sequencefile.java:743)  at org.apache.cassandra.io.sequencefile$bufferreader.<init>(sequencefile.java:805)  at org.apache.cassandra.io.sequencefile$columngroupreader.<init>(sequencefile.java:248)  at org.apache.cassandra.io.sstablereader.getcolumngroupreader(sstablereader.java:346)  at org.apache.cassandra.db.sstablecolumniterator.<init>(columniterator.java:61)  at org.apache.cassandra.db.columnfamilystore.getslicefrom(columnfamilystore.java:1589)  at org.apache.cassandra.db.table.getrow(table.java:596)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:60)  at org.apache.cassandra.service.storageproxy.weakreadlocal(storageproxy.java:600)  at org.apache.cassandra.service.storageproxy.readprotocol(storageproxy.java:303)  at org.apache.cassandra.service.cassandraserver.readcolumnfamily(cassandraserver.java:80) <code> <text> cassandra leaks file descriptors like crazy. i started getting these errors after a few hours of uptime: i have an open file limit of 1024. examining the lsof output for cassandra shows 975 fds for the same file: /var/cassandra/data/digg-items-2-data.db clearly, these fds are leaking somewhere.",
        "label": 274
    },
    {
        "text": "exceptions encountered calling getseeds  breaks otc thread <description> outboundtcpconnection.connect() calls getseeds(). if getseeds() throws an exception (for example, dd/config invalid yaml error), messaging thread(s) break(s).<stacktrace> <code> <text> outboundtcpconnection.connect() calls getseeds(). if getseeds() throws an exception (for example, dd/config invalid yaml error), messaging thread(s) break(s).",
        "label": 241
    },
    {
        "text": "triggers <description> asynchronous triggers is a basic mechanism to implement various use cases of asynchronous execution of application code at database side. for example to support indexes and materialized views, online analytics, push-based data propagation. please find the motivation, triggers description and list of applications:  http://maxgrinev.com/2010/07/23/extending-cassandra-with-asynchronous-triggers/ an example of using triggers for indexing:  http://maxgrinev.com/2010/07/23/managing-indexes-in-cassandra-using-async-triggers/ implementation details are attached.<stacktrace> <code> <text> asynchronous triggers is a basic mechanism to implement various use cases of asynchronous execution of application code at database side. for example to support indexes and materialized views, online analytics, push-based data propagation. please find the motivation, triggers description and list of applications:  http://maxgrinev.com/2010/07/23/extending-cassandra-with-asynchronous-triggers/ an example of using triggers for indexing:  http://maxgrinev.com/2010/07/23/managing-indexes-in-cassandra-using-async-triggers/ implementation details are attached.",
        "label": 555
    },
    {
        "text": "cql delete does not delete <description> tested in 1.1 and trunk branch on a single node: cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;  cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');  cqlsh:test> select * from testcf_old;  username | id | name | stuff  ---------------+--------------------------  abc | 2 | rst | some other bunch of craps  abc | 4 | xyz | a bunch of craps cqlsh:test> delete from testcf_old where username = 'abc' and id =2;  cqlsh:test> select * from testcf_old;  username | id | name | stuff  ---------------+--------------------------  abc | 2 | rst | some other bunch of craps  abc | 4 | xyz | a bunch of craps same also when not using compact: cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));  cqlsh:test> select * from testcf;  username | id | name | stuff  ------------------------------------+-----------------  abc | 2 | some other bunch of craps | rst  abc | 4 | xyz | a bunch of craps cqlsh:test> delete from testcf where username = 'abc' and id =2;  cqlsh:test> select * from testcf;  username | id | name | stuff  ------------------------------------+-----------------  abc | 2 | some other bunch of craps | rst  abc | 4 | xyz | a bunch of craps<stacktrace> <code> cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;  cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');  cqlsh:test> select * from testcf_old;  username | id | name | stuff  ---------------+--------------------------  abc | 2 | rst | some other bunch of craps  abc | 4 | xyz | a bunch of craps cqlsh:test> delete from testcf_old where username = 'abc' and id =2;  cqlsh:test> select * from testcf_old;  username | id | name | stuff  ---------------+--------------------------  abc | 2 | rst | some other bunch of craps  abc | 4 | xyz | a bunch of craps cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));  cqlsh:test> select * from testcf;  username | id | name | stuff  ------------------------------------+-----------------  abc | 2 | some other bunch of craps | rst  abc | 4 | xyz | a bunch of craps cqlsh:test> delete from testcf where username = 'abc' and id =2;  cqlsh:test> select * from testcf;  username | id | name | stuff  ------------------------------------+-----------------  abc | 2 | some other bunch of craps | rst  abc | 4 | xyz | a bunch of craps<text> tested in 1.1 and trunk branch on a single node: same also when not using compact: ",
        "label": 520
    },
    {
        "text": "expose time spent waiting in thread pool queue <description> we are missing an important source of latency in our system, the time waiting to be processed by thread pools. we should add a metric for this so someone can easily see how much time is spent just waiting to be processed.<stacktrace> <code> <text> we are missing an important source of latency in our system, the time waiting to be processed by thread pools. we should add a metric for this so someone can easily see how much time is spent just waiting to be processed.",
        "label": 147
    },
    {
        "text": "thrift removal <description> thrift removal has been announced for 4.0. this ticket is meant to serve as a general task for that removal, but also to track issue related to that, either things that we should do in 3.x to make that removal as smooth as possible, or sub-tasks that it makes sense to separate.<stacktrace> <code> <text> thrift removal has been announced for 4.0. this ticket is meant to serve as a general task for that removal, but also to track issue related to that, either things that we should do in 3.x to make that removal as smooth as possible, or sub-tasks that it makes sense to separate.",
        "label": 520
    },
    {
        "text": "hive jdbc connections fail with invalidurlexception when both the c  and hive jdbc drivers are loaded <description> hive connections fail with invalidurlexception when both the c* and hive jdbc drivers are loaded, and it seems the url is being interpreted as a c* url. caused an error     [junit] invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra     [junit] org.apache.cassandra.cql.jdbc.invalidurlexception: invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra     [junit]  at org.apache.cassandra.cql.jdbc.cassandradriver.connect(cassandradriver.java:90)     [junit]  at java.sql.drivermanager.getconnection(drivermanager.java:582)     [junit]  at java.sql.drivermanager.getconnection(drivermanager.java:185)     [junit]  at com.datastax.bugrepros.repro_connection_error.test1_runhivebeforejdbc(repro_connection_error.java:34) code snippet: intended to illustrate the connection issues copy file to test directory change package declaration run: ant test -dtest.name=repro_conn_error package com.datastax.bugrepros; import java.sql.drivermanager; import java.sql.connection; import java.sql.sqlexception; import java.util.enumeration; import org.junit.test; public class repro_conn_error {     @test     public void jdbcconnectionerror() throws exception      {           // create hive jdbc connection - will succeed if               try          {             // uncomment loading c* driver to reproduce bug             class.forname(\"org.apache.cassandra.cql.jdbc.cassandradriver\");                          // load hive driver and connect             class.forname(\"org.apache.hadoop.hive.jdbc.hivedriver\");             connection hiveconn = drivermanager.getconnection(\"jdbc:hive://127.0.0.1:10000/default\", \"\", \"\");             hiveconn.close();               system.out.println(\"successful hive connection\");         } catch (sqlexception e) {             system.out.println(\"unsuccessful hive connection\");             e.printstacktrace();         }                  // create c* jdbc connection         try          {             class.forname(\"org.apache.cassandra.cql.jdbc.cassandradriver\");             connection jdbcconn = drivermanager.getconnection(\"jdbc:cassandra:root/root@127.0.0.1:9160/default\");                  jdbcconn.close();                 system.out.println(\"successful c* connection\");         } catch (sqlexception e) {             system.out.println(\"unsuccessful c* connection\");             e.printstacktrace();         }                  // print out all loaded jdbc drivers.         enumeration d = java.sql.drivermanager.getdrivers();                  while (d.hasmoreelements()) {             object driverasobject = d.nextelement();             system.out.println(\"jdbc driver=\" + driverasobject);         }     } } <stacktrace> caused an error     [junit] invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra     [junit] org.apache.cassandra.cql.jdbc.invalidurlexception: invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra     [junit]  at org.apache.cassandra.cql.jdbc.cassandradriver.connect(cassandradriver.java:90)     [junit]  at java.sql.drivermanager.getconnection(drivermanager.java:582)     [junit]  at java.sql.drivermanager.getconnection(drivermanager.java:185)     [junit]  at com.datastax.bugrepros.repro_connection_error.test1_runhivebeforejdbc(repro_connection_error.java:34) <code> package com.datastax.bugrepros; import java.sql.drivermanager; import java.sql.connection; import java.sql.sqlexception; import java.util.enumeration; import org.junit.test; public class repro_conn_error {     @test     public void jdbcconnectionerror() throws exception      {           // create hive jdbc connection - will succeed if               try          {             // uncomment loading c* driver to reproduce bug             class.forname('org.apache.cassandra.cql.jdbc.cassandradriver');                          // load hive driver and connect             class.forname('org.apache.hadoop.hive.jdbc.hivedriver');             connection hiveconn = drivermanager.getconnection('jdbc:hive://127.0.0.1:10000/default', '', '');             hiveconn.close();               system.out.println('successful hive connection');         } catch (sqlexception e) {             system.out.println('unsuccessful hive connection');             e.printstacktrace();         }                  // create c* jdbc connection         try          {             class.forname('org.apache.cassandra.cql.jdbc.cassandradriver');             connection jdbcconn = drivermanager.getconnection('jdbc:cassandra:root/root@127.0.0.1:9160/default');                  jdbcconn.close();                 system.out.println('successful c* connection');         } catch (sqlexception e) {             system.out.println('unsuccessful c* connection');             e.printstacktrace();         }                  // print out all loaded jdbc drivers.         enumeration d = java.sql.drivermanager.getdrivers();                  while (d.hasmoreelements()) {             object driverasobject = d.nextelement();             system.out.println('jdbc driver=' + driverasobject);         }     } } <text> hive connections fail with invalidurlexception when both the c* and hive jdbc drivers are loaded, and it seems the url is being interpreted as a c* url. code snippet: intended to illustrate the connection issues",
        "label": 449
    },
    {
        "text": "hex to bytes conversion accepts invalid inputs silently <description> fbutilities.hextobytes() has a minor bug - it copes with single-character inputs by prepending \"0\", which is ok - but it does this for any input with an odd number of characters, which is probably incorrect. if (str.length() % 2 == 1)     str = \"0\" + str; given 'fff' as an input, can we really assume that this should be '0fff'? isn't this just an error? add the following to fbutilitiestest to demonstrate: string[] badvalues = new string[]{\"000\", \"fff\"};         for (int i = 0; i < badvalues.length; i++)     try     {         fbutilities.hextobytes(badvalues[i]);         fail(\"invalid hex value accepted\"+badvalues[i]);     } catch (exception e){}<stacktrace> <code> if (str.length() % 2 == 1)     str = '0' + str; string[] badvalues = new string[]{'000', 'fff'};         for (int i = 0; i < badvalues.length; i++)     try     {         fbutilities.hextobytes(badvalues[i]);         fail('invalid hex value accepted'+badvalues[i]);     } catch (exception e){} <text> fbutilities.hextobytes() has a minor bug - it copes with single-character inputs by prepending '0', which is ok - but it does this for any input with an odd number of characters, which is probably incorrect. given 'fff' as an input, can we really assume that this should be '0fff'? isn't this just an error? add the following to fbutilitiestest to demonstrate:",
        "label": 520
    },
    {
        "text": "improve sstable discovery and allow for more components <description> a new 'statistics.db' component was added recently, and it is likely that more components will follow. due to our parsing code using a regex to detect 'orphan' files (rather than a checked type) a bug slipped through that prevents the new component from being detected as an orphan. to continue to be able to scale the components of sstables, we should formalize 'components'.<stacktrace> <code> <text> a new 'statistics.db' component was added recently, and it is likely that more components will follow. due to our parsing code using a regex to detect 'orphan' files (rather than a checked type) a bug slipped through that prevents the new component from being detected as an orphan. to continue to be able to scale the components of sstables, we should formalize 'components'.",
        "label": 515
    },
    {
        "text": "expunge concurrenthashmap <description> avinash says it is less memory-efficient than nonblockinghashmap. nbhm also has better performance numbers, although we don't use any of these instances in high-contention scenarios.<stacktrace> <code> <text> avinash says it is less memory-efficient than nonblockinghashmap. nbhm also has better performance numbers, although we don't use any of these instances in high-contention scenarios.",
        "label": 274
    },
    {
        "text": "consistency of append prepend on lists need to be improved or clarified <description> updates are idempotent in cassandra, this rule makes it simple for developers or client libraries to deal with retries on error. so far the only exception was counters, and we worked around it saying they were meant to be used for analytics use cases. now with list datatype to be added in cassandra 1.2 we have a similar issue as append and prepend operations that can be applied on them are not idempotent. the state of the list will be unknown whenever a timeout is received from the coordinator node saying that no acknowledge could be received on time from replicas or when the connection with the coordinator node is broken while a client wait for an update request to be acknowledged. of course the client can issue a read request on this list in the rare cases when such an unknown state appear, but this is not really elegant and such a check doesn't come with any visibility or atomicity guarantees. i can see 3 options: remove append and prepend operations. but this is a pity as they're really useful. make the behavior of these commands quasi-idempotent. i imagine that if we attach the list of timestamps and/or hashes of recent update requests to each list column stored in cassandra we would be able to avoid applying duplicate updates. explicitly document these operations as potentially unconsistent under these particular conditions.<stacktrace> <code> <text> updates are idempotent in cassandra, this rule makes it simple for developers or client libraries to deal with retries on error. so far the only exception was counters, and we worked around it saying they were meant to be used for analytics use cases. now with list datatype to be added in cassandra 1.2 we have a similar issue as append and prepend operations that can be applied on them are not idempotent. the state of the list will be unknown whenever a timeout is received from the coordinator node saying that no acknowledge could be received on time from replicas or when the connection with the coordinator node is broken while a client wait for an update request to be acknowledged. of course the client can issue a read request on this list in the rare cases when such an unknown state appear, but this is not really elegant and such a check doesn't come with any visibility or atomicity guarantees. i can see 3 options:",
        "label": 520
    },
    {
        "text": "stagemanager loadbalance stage is unnecessary  <description> it doesn't seem to be getting used anywhere. i think it can be taken out.<stacktrace> <code> <text> it doesn't seem to be getting used anywhere. i think it can be taken out.",
        "label": 274
    },
    {
        "text": "move all hints related tasks to hints private executor <description> we ran drivers 3-days endurance tests against cassandra 2.0.11 and c* crashed with an oome. this happened both with ruby-driver 1.0-beta and java-driver 2.0.8-snapshot. attached are : oome_node_system.log   the system.log of one cassandra node that crashed gc.log.gz   the gc log on the same node heap-usage-after-gc.png   the heap occupancy evolution after every gc cycle heap-usage-after-gc-zoom.png   a focus on when things start to go wrong workload :  our test executes 5 cql statements (select, insert, select, delete, select) for a given unique id, during 3 days, using multiple threads. there is not change in the workload during the test. symptoms :  in the attached log, it seems something starts in cassandra between 2014-11-06 10:29:22 and 2014-11-06 10:45:32. this causes an allocation that fills the heap. we eventually get stuck in a full gc storm and get an oome in the logs. i have run the java-driver tests against cassandra 1.2.19 and 2.1.1. the error does not occur. it seems specific to 2.0.11.<stacktrace> <code> <text> we ran drivers 3-days endurance tests against cassandra 2.0.11 and c* crashed with an oome. this happened both with ruby-driver 1.0-beta and java-driver 2.0.8-snapshot. attached are : workload :  our test executes 5 cql statements (select, insert, select, delete, select) for a given unique id, during 3 days, using multiple threads. there is not change in the workload during the test. symptoms :  in the attached log, it seems something starts in cassandra between 2014-11-06 10:29:22 and 2014-11-06 10:45:32. this causes an allocation that fills the heap. we eventually get stuck in a full gc storm and get an oome in the logs. i have run the java-driver tests against cassandra 1.2.19 and 2.1.1. the error does not occur. it seems specific to 2.0.11.",
        "label": 18
    },
    {
        "text": "clean up tools bin <description> right now we have most utils in bin/, but sstablemetadata is an outlier in tools/bin. for packaging, we don't care what the source directory is, we put everything in /usr/bin. we want to keep some things out of users' hands unless they know what they're doing (like sstable2json) so i propose that we break these things out into a companion cassandra-tools package that will depend on cassandra.<stacktrace> <code> <text> right now we have most utils in bin/, but sstablemetadata is an outlier in tools/bin. for packaging, we don't care what the source directory is, we put everything in /usr/bin. we want to keep some things out of users' hands unless they know what they're doing (like sstable2json) so i propose that we break these things out into a companion cassandra-tools package that will depend on cassandra.",
        "label": 348
    },
    {
        "text": "commitlogreplayer date time issue <description> hi, first off i am sorry if the component is not right for this. i am trying to get the point-in-time backup to work. and i ran into the following issues: 1. the documentation in the commitlog_archiving.properties seems to be out of date, as the example date format is no more valid and can't be parsed. 2. the restore_point_in_time property seems to differ from the actual maxtimestamp. i added additional logging to the codebase in the class commitlogreplayer like that: protected boolean pointintimeexceeded(rowmutation frm)  {  long restoretarget = commitlog.instance.archiver.restorepointintime;  logger.info(string.valueof(restoretarget));  for (columnfamily families : frm.getcolumnfamilies()) { logger.info(string.valueof(families.maxtimestamp())); if (families.maxtimestamp() > restoretarget) return true; } return false;  } the following output can be seen: the restoretarget timestamp is: 1377015783000  this has been correctly parsed as i added this date to the properties:   2013:08:20 17:23:03 the value for families.maxtimestamp() is: 1377009021033000  this date corresponds to: mon 45605-09-05 10:50:33 bst (44 millennia from now) it seems like the timestamp has 3 additional zeros. this also means that the code can never return false on the call, as the restoretarget will always be smaller then the maxtimestamp(). therefore the replayer can never replay any of my commitlog files.   the timestamp minus the 3 zeros corresponds to \"tue 2013-08-20 15:30:21 bst (23 hours ago)\" which makes more sense and would allow for the replay to work. my config: cassandra-1.2.4  java 1.6  ubuntu 12.04 64bit if you need any more information let me know and i'll be happy to suply whatever info i can. \u2013 artur<stacktrace> <code> 2. protected boolean pointintimeexceeded(rowmutation frm)  {  long restoretarget = commitlog.instance.archiver.restorepointintime;  logger.info(string.valueof(restoretarget));  for (columnfamily families : frm.getcolumnfamilies()) the value for families.maxtimestamp() is: 1377009021033000  this date corresponds to: mon 45605-09-05 10:50:33 bst (44 millennia from now) cassandra-1.2.4  java 1.6  ubuntu 12.04 64bit <text> hi, first off i am sorry if the component is not right for this. i am trying to get the point-in-time backup to work. and i ran into the following issues: 1. the documentation in the commitlog_archiving.properties seems to be out of date, as the example date format is no more valid and can't be parsed. the restore_point_in_time property seems to differ from the actual maxtimestamp. i added additional logging to the codebase in the class commitlogreplayer like that: return false;  } the following output can be seen: the restoretarget timestamp is: 1377015783000  this has been correctly parsed as i added this date to the properties:   2013:08:20 17:23:03 it seems like the timestamp has 3 additional zeros. this also means that the code can never return false on the call, as the restoretarget will always be smaller then the maxtimestamp(). therefore the replayer can never replay any of my commitlog files.   the timestamp minus the 3 zeros corresponds to 'tue 2013-08-20 15:30:21 bst (23 hours ago)' which makes more sense and would allow for the replay to work. my config: if you need any more information let me know and i'll be happy to suply whatever info i can. - artur",
        "label": 555
    },
    {
        "text": "concurrentmodificationexception during node recovery <description> testing some node recovery operations. in this case:  1. data is being added/updated as it would in production  2. repair is running on other nodes in the cluster  3. we wiped data on this node and started up again, but before repair was actually started on this node (but it had gotten data through the regular data feed) we got this error. i see no indication in the logs that outgoing streams has been started, but the node have finished one incoming stream before this (i guess from some other node doing repair).  info [compactionexecutor:11] 2011-06-14 14:15:09,078 sstablereader.java (line 155) opening /data/cassandra/node1/data/jp/test-g-8  info [compactionexecutor:13] 2011-06-14 14:15:09,079 sstablereader.java (line 155) opening /data/cassandra/node1/data/jp/test-g-10  info [hintedhandoff:1] 2011-06-14 14:15:26,623 hintedhandoffmanager.java (line 302) started hinted handoff for endpoint /1.10.42.216  info [hintedhandoff:1] 2011-06-14 14:15:26,623 hintedhandoffmanager.java (line 358) finished hinted handoff of 0 rows to endpoint /1.10.42.216  info [compactionexecutor:9] 2011-06-14 14:15:29,417 sstablereader.java (line 155) opening /data/cassandra/node1/data/jp/datetest-g-2  error [thread-84] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-84,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)  error [thread-79] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-79,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)  error [thread-83] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-83,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)  error [thread-85] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-85,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)<stacktrace> info [compactionexecutor:11] 2011-06-14 14:15:09,078 sstablereader.java (line 155) opening /data/cassandra/node1/data/jp/test-g-8  info [compactionexecutor:13] 2011-06-14 14:15:09,079 sstablereader.java (line 155) opening /data/cassandra/node1/data/jp/test-g-10  info [hintedhandoff:1] 2011-06-14 14:15:26,623 hintedhandoffmanager.java (line 302) started hinted handoff for endpoint /1.10.42.216  info [hintedhandoff:1] 2011-06-14 14:15:26,623 hintedhandoffmanager.java (line 358) finished hinted handoff of 0 rows to endpoint /1.10.42.216  info [compactionexecutor:9] 2011-06-14 14:15:29,417 sstablereader.java (line 155) opening /data/cassandra/node1/data/jp/datetest-g-2  error [thread-84] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-84,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)  error [thread-79] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-79,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)  error [thread-83] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-83,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)  error [thread-85] 2011-06-14 14:15:36,755 abstractcassandradaemon.java (line 113) fatal exception in thread thread[thread-85,5,main]  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.streaming.streaminsession.closeiffinished(streaminsession.java:132)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:63)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:155)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:93)<code> <text> testing some node recovery operations. in this case:  1. data is being added/updated as it would in production  2. repair is running on other nodes in the cluster  3. we wiped data on this node and started up again, but before repair was actually started on this node (but it had gotten data through the regular data feed) we got this error. i see no indication in the logs that outgoing streams has been started, but the node have finished one incoming stream before this (i guess from some other node doing repair). ",
        "label": 274
    },
    {
        "text": "windows utest  testdatecompatibility failing intermittently <description> 2/5 fail on windows, 0/5 fail on linux, i'm calling it windows-only:     [junit] testcase: testdatecompatibility(org.apache.cassandra.cql3.validation.entities.typetest):    failed     [junit] expected:<1> but was:<0>     [junit] junit.framework.assertionfailederror: expected:<1> but was:<0>     [junit]     at org.apache.cassandra.cql3.validation.entities.typetest.testdatecompatibility(typetest.java:49)<stacktrace>     [junit] testcase: testdatecompatibility(org.apache.cassandra.cql3.validation.entities.typetest):    failed     [junit] expected:<1> but was:<0>     [junit] junit.framework.assertionfailederror: expected:<1> but was:<0>     [junit]     at org.apache.cassandra.cql3.validation.entities.typetest.testdatecompatibility(typetest.java:49) <code> <text> 2/5 fail on windows, 0/5 fail on linux, i'm calling it windows-only:",
        "label": 409
    },
    {
        "text": "creating database resources automatically grants creator full permissions <description> i am developing a multi-tenant service.  every tenant has its own user, keyspace and can access only his keyspace.  as new tenants are provisioned there is a need to create new users and keyspaces.  only a superuser can issue create user requests, so we must have a super user account in the system. on the other hand super users have access to all the keyspaces, which poses a security risk.  for tenant provisioning i would like to have a restricted account which can only create new users, without read access to keyspaces.<stacktrace> <code> <text> i am developing a multi-tenant service.  every tenant has its own user, keyspace and can access only his keyspace.  as new tenants are provisioned there is a need to create new users and keyspaces.  only a superuser can issue create user requests, so we must have a super user account in the system. on the other hand super users have access to all the keyspaces, which poses a security risk.  for tenant provisioning i would like to have a restricted account which can only create new users, without read access to keyspaces.",
        "label": 474
    },
    {
        "text": "cassandra cli has backwards index status message <description> when a secondary index is building, the total bytes and processed bytes are swapped in the message. example:  currently building index cf1, completed 12052040551 of 18047343 bytes. the problem is a call to compactioninfo constructor with swapped parameters. patch to follow.<stacktrace> <code> <text> when a secondary index is building, the total bytes and processed bytes are swapped in the message. example:  currently building index cf1, completed 12052040551 of 18047343 bytes. the problem is a call to compactioninfo constructor with swapped parameters. patch to follow.",
        "label": 332
    },
    {
        "text": "preserve the names of query parameters in queryoptions <description> when writing a custom queryhandler that processes named parameters, the queryoptions arrive to the various processing methods with the values converted to positional arguments and the names unavailable. a custom queryhandler might want to make use of the names themselves so it would be helpful if they were preserved and exposed with their respective bytebuffer values. https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/cql3/queryoptions.java#l205<stacktrace> <code> https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/cql3/queryoptions.java#l205<text> when writing a custom queryhandler that processes named parameters, the queryoptions arrive to the various processing methods with the values converted to positional arguments and the names unavailable. a custom queryhandler might want to make use of the names themselves so it would be helpful if they were preserved and exposed with their respective bytebuffer values. ",
        "label": 69
    },
    {
        "text": "regular startup log has confusing  bootstrap replace move completed  without boostrap  replace  or move <description> a regular startup completes successfully, but it has a confusing message the end of the startup: \" info 15:19:29,137 bootstrap/replace/move completed! now serving reads.\" this happens despite no bootstrap, replace, or move. while purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?! it should simply read something like \"startup completed! now serving reads\" unless it actually has done one of the actions in the error message. complete log at the end: info 15:13:30,522 log replay complete, 6274 replayed mutations  info 15:13:30,527 cassandra version: 1.0.12  info 15:13:30,527 thrift api version: 19.20.0  info 15:13:30,527 loading persisted ring state  info 15:13:30,541 starting up server gossip  info 15:13:30,542 enqueuing flush of memtable-locationinfo@1828864224(29/36 serialized/live bytes, 1 ops)  info 15:13:30,543 writing memtable-locationinfo@1828864224(29/36 serialized/live bytes, 1 ops)  info 15:13:30,550 completed flushing /data2/data-cassandra/system/locationinfo-hd-274-data.db (80 bytes)  info 15:13:30,563 starting messaging service on port 7000  info 15:13:30,571 using saved token 31901471898837980949691369446728269823  info 15:13:30,572 enqueuing flush of memtable-locationinfo@294410307(53/66 serialized/live bytes, 2 ops)  info 15:13:30,573 writing memtable-locationinfo@294410307(53/66 serialized/live bytes, 2 ops)  info 15:13:30,579 completed flushing /data2/data-cassandra/system/locationinfo-hd-275-data.db (163 bytes)  info 15:13:30,581 node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal  info 15:13:30,598 bootstrap/replace/move completed! now serving reads.  info 15:13:30,600 will not load mx4j, mx4j-tools.jar is not in the classpath<stacktrace> <code> ' info 15:19:29,137 bootstrap/replace/move completed! now serving reads.' info 15:13:30,522 log replay complete, 6274 replayed mutations  info 15:13:30,527 cassandra version: 1.0.12  info 15:13:30,527 thrift api version: 19.20.0  info 15:13:30,527 loading persisted ring state  info 15:13:30,541 starting up server gossip  info 15:13:30,542 enqueuing flush of memtable-locationinfo@1828864224(29/36 serialized/live bytes, 1 ops)  info 15:13:30,543 writing memtable-locationinfo@1828864224(29/36 serialized/live bytes, 1 ops)  info 15:13:30,550 completed flushing /data2/data-cassandra/system/locationinfo-hd-274-data.db (80 bytes)  info 15:13:30,563 starting messaging service on port 7000  info 15:13:30,571 using saved token 31901471898837980949691369446728269823  info 15:13:30,572 enqueuing flush of memtable-locationinfo@294410307(53/66 serialized/live bytes, 2 ops)  info 15:13:30,573 writing memtable-locationinfo@294410307(53/66 serialized/live bytes, 2 ops)  info 15:13:30,579 completed flushing /data2/data-cassandra/system/locationinfo-hd-275-data.db (163 bytes)  info 15:13:30,581 node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal  info 15:13:30,598 bootstrap/replace/move completed! now serving reads.  info 15:13:30,600 will not load mx4j, mx4j-tools.jar is not in the classpath<text> a regular startup completes successfully, but it has a confusing message the end of the startup: this happens despite no bootstrap, replace, or move. while purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?! it should simply read something like 'startup completed! now serving reads' unless it actually has done one of the actions in the error message. complete log at the end: ",
        "label": 555
    },
    {
        "text": "cassandra version throttles and sometimes kills traffic to a node if you restart it  <description> from the cassandra user message board:   \"i just recently upgraded to latest in 0.5 branch, and i am running  into a serious issue. i have a cluster with 4 nodes, rackunaware  strategy, and using my own tokens distributed evenly over the hash  space. i am writing/reading equally to them at an equal rate of about  230 reads/writes per second(and cfstats shows that). the first 3 nodes  are seeds, the last one isn't. when i start all the nodes together at  the same time, they all receive equal amounts of reads/writes (about  230).  when i bring node 4 down and bring it back up again, node 4's load  fluctuates between the 230 it used to get to sometimes no traffic at  all. the other 3 still have the same amount of traffic. and no errors  what so ever seen in logs. \"<stacktrace> <code> <text> from the cassandra user message board:   'i just recently upgraded to latest in 0.5 branch, and i am running  into a serious issue. i have a cluster with 4 nodes, rackunaware  strategy, and using my own tokens distributed evenly over the hash  space. i am writing/reading equally to them at an equal rate of about  230 reads/writes per second(and cfstats shows that). the first 3 nodes  are seeds, the last one isn't. when i start all the nodes together at  the same time, they all receive equal amounts of reads/writes (about  230).  when i bring node 4 down and bring it back up again, node 4's load  fluctuates between the 230 it used to get to sometimes no traffic at  all. the other 3 still have the same amount of traffic. and no errors  what so ever seen in logs. '",
        "label": 186
    },
    {
        "text": "upgrade cassandra java driver to <description> udfs (cassandra-7563) requires java driver that supports user types and new collection features (at least java driver 2.1). may also be handled separately if e.g. hadoop stuff requires this (follow-up to cassandra-7618).<stacktrace> <code> <text> udfs (cassandra-7563) requires java driver that supports user types and new collection features (at least java driver 2.1). may also be handled separately if e.g. hadoop stuff requires this (follow-up to cassandra-7618).",
        "label": 453
    },
    {
        "text": "commented out lines that end in a semicolon cause an error  <description> commented-out lines that end in a semicolon cause an error. for example: /* describe keyspaces; */   this produces an error: syntaxexception: line 2:22 no viable alternative at input '<eof> (...*  describe keyspaces;...)   it works as expected if you use syntax: \u2013 describe keyspaces;   environment: python:3.7.7-slim-stretch (docker image)   i found that this was first seen here, and was patched, but the bug appears to have resurfaced: https://issues.apache.org/jira/browse/cassandra-2488<stacktrace> <code> /* describe keyspaces; */ syntaxexception: line 2:22 no viable alternative at input '<eof> (...*  describe keyspaces;...) - describe keyspaces; python:3.7.7-slim-stretch (docker image) https://issues.apache.org/jira/browse/cassandra-2488<text> commented-out lines that end in a semicolon cause an error. for example:   this produces an error:   it works as expected if you use syntax:   environment:   i found that this was first seen here, and was patched, but the bug appears to have resurfaced: ",
        "label": 445
    },
    {
        "text": "dtest failure in materialized views test testmaterializedviews clustering column test <description> recent failure, test has flapped before a while back. expecting 2 users, got 1 http://cassci.datastax.com/job/cassandra-3.0_dtest/688/testreport/materialized_views_test/testmaterializedviews/clustering_column_test failed on cassci build cassandra-3.0_dtest #688<stacktrace> <code> http://cassci.datastax.com/job/cassandra-3.0_dtest/688/testreport/materialized_views_test/testmaterializedviews/clustering_column_test failed on cassci build cassandra-3.0_dtest #688<text> expecting 2 users, got 1 recent failure, test has flapped before a while back. ",
        "label": 98
    },
    {
        "text": "abstractcolumnfamilyinputformat   abstractcolumnfamilyoutputformat throw npe if username is provided but password is null <description> if a username is provided to either of these classes but the password is null the thrift layer throws an npe because it can't handle null values for the login.<stacktrace> <code> <text> if a username is provided to either of these classes but the password is null the thrift layer throws an npe because it can't handle null values for the login.",
        "label": 357
    },
    {
        "text": "skip rows with empty columns when slicing entire row <description> we have been finding that range ghosts appear in results from hadoop via pig. this could also happen if rows don't have data for the slice predicate that is given. this leads to having to do a painful amount of defensive checking on the pig side, especially in the case of range ghosts. we would like to add an option to skip rows that have no column values in it. that functionality existed before in core cassandra but was removed because of the performance penalty of that checking. however with hadoop support in the recordreader, that is batch oriented anyway, so individual row reading performance isn't as much of an issue. also we would make it an optional config parameter for each job anyway, so people wouldn't have to incur that penalty if they are confident that there won't be those empty rows or they don't care. it could be parameter cassandra.skip.empty.rows and be true/false.<stacktrace> <code> <text> we have been finding that range ghosts appear in results from hadoop via pig. this could also happen if rows don't have data for the slice predicate that is given. this leads to having to do a painful amount of defensive checking on the pig side, especially in the case of range ghosts. we would like to add an option to skip rows that have no column values in it. that functionality existed before in core cassandra but was removed because of the performance penalty of that checking. however with hadoop support in the recordreader, that is batch oriented anyway, so individual row reading performance isn't as much of an issue. also we would make it an optional config parameter for each job anyway, so people wouldn't have to incur that penalty if they are confident that there won't be those empty rows or they don't care. it could be parameter cassandra.skip.empty.rows and be true/false.",
        "label": 521
    },
    {
        "text": "static columns mess up selects with ordering and clustering column ranges <description> starts off ok: cqlsh:test> create table test (p text, c text, v text, s text static, primary key (p, c)); cqlsh:test> insert into test (p, c, v, s) values ('p1', 'k1', 'v1', 'sv1'); cqlsh:test> select * from test where p = 'p1';  p  | c  | s   | v ----+----+-----+----  p1 | k1 | sv1 | v1 (1 rows) but try ordering, and we appear to get the static row instead: cqlsh:test> select * from test where p = 'p1' order by c desc;  p  | c    | s   | v ----+------+-----+------  p1 | null | sv1 | null (1 rows) now we add a clustering key range constraint, again works ok: cqlsh:test> select * from test where p = 'p1' and c >= 'a';  p  | c  | s   | v ----+----+-----+----  p1 | k1 | sv1 | v1 (1 rows) but, this causes assertion failure (which has a very nice comment explaining exactly why that might happen!): cqlsh:test> select * from test where p = 'p1' and c >= 'a' order by c desc; request did not complete within rpc_timeout. cause: java.lang.assertionerror: added column does not sort as the first column         at org.apache.cassandra.db.arraybackedsortedcolumns.addcolumn(arraybackedsortedcolumns.java:115)         at org.apache.cassandra.db.columnfamily.addcolumn(columnfamily.java:116)         at org.apache.cassandra.db.columnfamily.addifrelevant(columnfamily.java:110)         at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:205)         at org.apache.cassandra.db.filter.queryfilter.collatecolumns(queryfilter.java:122)         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:80)         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:72)         at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:297)         at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1547)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1376)         at org.apache.cassandra.db.keyspace.getrow(keyspace.java:333)         at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)         at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1363)         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1927)         at java.util.concurrent.executors$runnableadapter.call(executors.java:471)         at java.util.concurrent.futuretask.run(futuretask.java:262)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744)<stacktrace> java.lang.assertionerror: added column does not sort as the first column         at org.apache.cassandra.db.arraybackedsortedcolumns.addcolumn(arraybackedsortedcolumns.java:115)         at org.apache.cassandra.db.columnfamily.addcolumn(columnfamily.java:116)         at org.apache.cassandra.db.columnfamily.addifrelevant(columnfamily.java:110)         at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:205)         at org.apache.cassandra.db.filter.queryfilter.collatecolumns(queryfilter.java:122)         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:80)         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:72)         at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:297)         at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1547)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1376)         at org.apache.cassandra.db.keyspace.getrow(keyspace.java:333)         at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)         at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1363)         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1927)         at java.util.concurrent.executors$runnableadapter.call(executors.java:471)         at java.util.concurrent.futuretask.run(futuretask.java:262)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) <code> cqlsh:test> create table test (p text, c text, v text, s text static, primary key (p, c)); cqlsh:test> insert into test (p, c, v, s) values ('p1', 'k1', 'v1', 'sv1'); cqlsh:test> select * from test where p = 'p1';  p  | c  | s   | v ----+----+-----+----  p1 | k1 | sv1 | v1 (1 rows) cqlsh:test> select * from test where p = 'p1' order by c desc;  p  | c    | s   | v ----+------+-----+------  p1 | null | sv1 | null (1 rows) cqlsh:test> select * from test where p = 'p1' and c >= 'a';  p  | c  | s   | v ----+----+-----+----  p1 | k1 | sv1 | v1 (1 rows) cqlsh:test> select * from test where p = 'p1' and c >= 'a' order by c desc; request did not complete within rpc_timeout. <text> starts off ok: but try ordering, and we appear to get the static row instead: now we add a clustering key range constraint, again works ok: but, this causes assertion failure (which has a very nice comment explaining exactly why that might happen!): cause:",
        "label": 588
    },
    {
        "text": "upgradesstables optimisation <description> currently, if you run upgradesstables, cassandra will run through every single sstable within the scope of the request. where we have some large tables, an upgrade on a single sstable can take hours, even if its already sat on the same version. after upgrading to a new cassandra version, it would be ideal to be able to upgrade only sstables not sat in the latest version, as it seems like it just needs to do a massive amount of disk io, with nothing being achieved at the end of it. maybe its worth putting an option onto the nodetool command, or creating a new command for this type of upgrade<stacktrace> <code> <text> currently, if you run upgradesstables, cassandra will run through every single sstable within the scope of the request. where we have some large tables, an upgrade on a single sstable can take hours, even if its already sat on the same version. after upgrading to a new cassandra version, it would be ideal to be able to upgrade only sstables not sat in the latest version, as it seems like it just needs to do a massive amount of disk io, with nothing being achieved at the end of it. maybe its worth putting an option onto the nodetool command, or creating a new command for this type of upgrade",
        "label": 520
    },
    {
        "text": "fully expired sstable not dropped when running out of disk space <description> if a fully expired sstable is larger than the remaining disk space we won't run the compaction that can drop the sstable (ie, in our disk space check should not include the fully expired sstables)<stacktrace> <code> <text> if a fully expired sstable is larger than the remaining disk space we won't run the compaction that can drop the sstable (ie, in our disk space check should not include the fully expired sstables)",
        "label": 309
    },
    {
        "text": "remove target type from internal index metadata <description> as part of cassandra-6716 & in anticipation of cassandra-10124, a distinction was introduced between secondary indexes which target a fixed set of 1 or more columns in the base data, and those which are agnostic to the structure of the underlying rows. this distinction is manifested in indexmetadata.targettype and system_schema.indexes, in the target_type column. it could be argued that this distinction complicates the codebase without providing any tangible benefit, given that the target type is not actually used anywhere. it's only the impact on system_schema.indexes that makes puts this on the critical path for 3.0, any code changes are just implementation details.<stacktrace> <code> <text> as part of cassandra-6716 & in anticipation of cassandra-10124, a distinction was introduced between secondary indexes which target a fixed set of 1 or more columns in the base data, and those which are agnostic to the structure of the underlying rows. this distinction is manifested in indexmetadata.targettype and system_schema.indexes, in the target_type column. it could be argued that this distinction complicates the codebase without providing any tangible benefit, given that the target type is not actually used anywhere. it's only the impact on system_schema.indexes that makes puts this on the critical path for 3.0, any code changes are just implementation details.",
        "label": 474
    },
    {
        "text": "missing binary dependencies for running cassandra in embedded mode <description> when running cassandra in embedded mode (pulling the cassandra-all-3.3.jar from maven) and activating udf, i face the following exceptions when trying to create an udf: 18:13:57.922 [main] debug achilles_ddl_script -  script : create function converttolong(input text) returns null on null input returns bigint language java as $$return long.parselong(input);$$; 18:13:57.970 [sharedpool-worker-1] error o.apache.cassandra.transport.message - unexpected exception during request; channel = [id: 0x03f52731, /192.168.1.16:55224 => /192.168.1.16:9240] java.lang.noclassdeffounderror: org/objectweb/asm/classvisitor at org.apache.cassandra.cql3.functions.javabasedudfunction.<clinit>(javabasedudfunction.java:79) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.functions.udfunction.create(udfunction.java:223) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.statements.createfunctionstatement.announcemigration(createfunctionstatement.java:162) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.statements.schemaalteringstatement.execute(schemaalteringstatement.java:93) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:206) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:237) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:222) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [cassandra-all-3.3.jar:3.3] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [cassandra-all-3.3.jar:3.3] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_60-ea] at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:164) [cassandra-all-3.3.jar:3.3] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [cassandra-all-3.3.jar:3.3] at java.lang.thread.run(thread.java:745) [na:1.8.0_60-ea] caused by: java.lang.classnotfoundexception: org.objectweb.asm.classvisitor at java.net.urlclassloader.findclass(urlclassloader.java:381) ~[na:1.8.0_60-ea] at java.lang.classloader.loadclass(classloader.java:424) ~[na:1.8.0_60-ea] at sun.misc.launcher$appclassloader.loadclass(launcher.java:331) ~[na:1.8.0_60-ea] at java.lang.classloader.loadclass(classloader.java:357) ~[na:1.8.0_60-ea] ... 18 common frames omitted the stack-trace is quite explicit, some classes from the objectweb/asm are missing. by looking into the $cassandra_home/lib folder:  19:44:07 :/opt/apps/apache-cassandra-3.2/lib] % ll total 48768 -rw-r--r--@  1 archinnovinfo  wheel   234k jan  7 22:42 st4-4.0.8.jar -rw-r--r--@  1 archinnovinfo  wheel    85k jan  7 22:42 airline-0.6.jar -rw-r--r--@  1 archinnovinfo  wheel   164k jan  7 22:42 antlr-runtime-3.5.2.jar -rw-r--r--@  1 archinnovinfo  wheel   5.1m jan  7 22:42 apache-cassandra-3.2.jar -rw-r--r--@  1 archinnovinfo  wheel   189k jan  7 22:42 apache-cassandra-clientutil-3.2.jar -rw-r--r--@  1 archinnovinfo  wheel   1.8m jan  7 22:42 apache-cassandra-thrift-3.2.jar -rw-r--r--@  1 archinnovinfo  wheel    52k jan  7 22:42 asm-5.0.4.jar -rw-r--r--@  1 archinnovinfo  wheel   2.2m jan  7 22:42 cassandra-driver-core-3.0.0-beta1-bb1bce4-snapshot-shaded.jar -rw-r--r--@  1 archinnovinfo  wheel   224k jan  7 22:42 cassandra-driver-internal-only-3.0.0-6af642d.zip i can see there is a asm-5.0.4.jar. after adding the following dependency in maven, the issue is solved:             <dependency>                 <groupid>org.ow2.asm</groupid>                 <artifactid>asm</artifactid>                 <version>5.0.4</version>             </dependency> what is strange is that this dependency is not mentioned anywhere, neither in the cassandra-all-3.3.pom.xml nor in the build.xml ant file...<stacktrace> 18:13:57.922 [main] debug achilles_ddl_script -  script : create function converttolong(input text) returns null on null input returns bigint language java as $$return long.parselong(input);$$; 18:13:57.970 [sharedpool-worker-1] error o.apache.cassandra.transport.message - unexpected exception during request; channel = [id: 0x03f52731, /192.168.1.16:55224 => /192.168.1.16:9240] java.lang.noclassdeffounderror: org/objectweb/asm/classvisitor at org.apache.cassandra.cql3.functions.javabasedudfunction.<clinit>(javabasedudfunction.java:79) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.functions.udfunction.create(udfunction.java:223) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.statements.createfunctionstatement.announcemigration(createfunctionstatement.java:162) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.statements.schemaalteringstatement.execute(schemaalteringstatement.java:93) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:206) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:237) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:222) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[cassandra-all-3.3.jar:3.3] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [cassandra-all-3.3.jar:3.3] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [cassandra-all-3.3.jar:3.3] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_60-ea] at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:164) [cassandra-all-3.3.jar:3.3] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [cassandra-all-3.3.jar:3.3] at java.lang.thread.run(thread.java:745) [na:1.8.0_60-ea] caused by: java.lang.classnotfoundexception: org.objectweb.asm.classvisitor at java.net.urlclassloader.findclass(urlclassloader.java:381) ~[na:1.8.0_60-ea] at java.lang.classloader.loadclass(classloader.java:424) ~[na:1.8.0_60-ea] at sun.misc.launcher$appclassloader.loadclass(launcher.java:331) ~[na:1.8.0_60-ea] at java.lang.classloader.loadclass(classloader.java:357) ~[na:1.8.0_60-ea] ... 18 common frames omitted <code>             <dependency>                 <groupid>org.ow2.asm</groupid>                 <artifactid>asm</artifactid>                 <version>5.0.4</version>             </dependency>  19:44:07 :/opt/apps/apache-cassandra-3.2/lib] % ll total 48768 -rw-r--r--@  1 archinnovinfo  wheel   234k jan  7 22:42 st4-4.0.8.jar -rw-r--r--@  1 archinnovinfo  wheel    85k jan  7 22:42 airline-0.6.jar -rw-r--r--@  1 archinnovinfo  wheel   164k jan  7 22:42 antlr-runtime-3.5.2.jar -rw-r--r--@  1 archinnovinfo  wheel   5.1m jan  7 22:42 apache-cassandra-3.2.jar -rw-r--r--@  1 archinnovinfo  wheel   189k jan  7 22:42 apache-cassandra-clientutil-3.2.jar -rw-r--r--@  1 archinnovinfo  wheel   1.8m jan  7 22:42 apache-cassandra-thrift-3.2.jar -rw-r--r--@  1 archinnovinfo  wheel    52k jan  7 22:42 asm-5.0.4.jar -rw-r--r--@  1 archinnovinfo  wheel   2.2m jan  7 22:42 cassandra-driver-core-3.0.0-beta1-bb1bce4-snapshot-shaded.jar -rw-r--r--@  1 archinnovinfo  wheel   224k jan  7 22:42 cassandra-driver-internal-only-3.0.0-6af642d.zip             <dependency>                 <groupid>org.ow2.asm</groupid>                 <artifactid>asm</artifactid>                 <version>5.0.4</version>             </dependency> <text> when running cassandra in embedded mode (pulling the cassandra-all-3.3.jar from maven) and activating udf, i face the following exceptions when trying to create an udf: the stack-trace is quite explicit, some classes from the objectweb/asm are missing. by looking into the $cassandra_home/lib folder: i can see there is a asm-5.0.4.jar. after adding the following dependency in maven, the issue is solved: what is strange is that this dependency is not mentioned anywhere, neither in the cassandra-all-3.3.pom.xml nor in the build.xml ant file...",
        "label": 159
    },
    {
        "text": "marshalexception is thrown when cassandra cli creates the example keyspace specified by conf schema sample txt <description> use the following steps to recreate the bug: 1. checkout the source code from trunk. for my case, revision is 1085753.  2. run \"ant\" to build cassandra.  3. run \"bin/cassandra -f\" to start cassandra.  4. run \"bin/cassandra-cli -host localhost --file conf/schema-sample.txt\". then there is the following message: ... schemas agree across the cluster  line 9 => org.apache.cassandra.db.marshal.marshalexception: cannot parse 'birthdate' as hex bytes the root cause is bytestype's fromstring method. fbutilities's hextobytes method is invoked with \"birthdate\". numberformatexception is thrown since \"birthdate\" is not a hex string. bytestype.java     public bytebuffer fromstring(string source)     {         try         {             return bytebuffer.wrap(fbutilities.hextobytes(source));         }         catch (numberformatexception e)         {             throw new marshalexception(string.format(\"cannot parse '%s' as hex bytes\", source), e);         }     }<stacktrace> <code>     public bytebuffer fromstring(string source)     {         try         {             return bytebuffer.wrap(fbutilities.hextobytes(source));         }         catch (numberformatexception e)         {             throw new marshalexception(string.format('cannot parse '%s' as hex bytes', source), e);         }     } ... schemas agree across the cluster  line 9 => org.apache.cassandra.db.marshal.marshalexception: cannot parse 'birthdate' as hex bytes <text> use the following steps to recreate the bug: 1. checkout the source code from trunk. for my case, revision is 1085753.  2. run 'ant' to build cassandra.  3. run 'bin/cassandra -f' to start cassandra.  4. run 'bin/cassandra-cli -host localhost --file conf/schema-sample.txt'. then there is the following message: the root cause is bytestype's fromstring method. fbutilities's hextobytes method is invoked with 'birthdate'. numberformatexception is thrown since 'birthdate' is not a hex string.",
        "label": 274
    },
    {
        "text": "viewtest sstableinbounds is failing <description> cassandra-8568 introduced new tests to cover what was datatracker functionality in 2.1, and is now covered by the lifecycle package. this particular test indicates this method does not fulfil the expected contract, namely that more sstables are returned than should be. however while looking into it i noticed it also likely has a bug (which i have not updated the test to cover) wherein a wrapped range will only yield the portion at the end of the token range, not the beginning. it looks like we may have call sites using this function that do not realise this, so it could be a serious bug, especially for repair.<stacktrace> <code> <text> cassandra-8568 introduced new tests to cover what was datatracker functionality in 2.1, and is now covered by the lifecycle package. this particular test indicates this method does not fulfil the expected contract, namely that more sstables are returned than should be. however while looking into it i noticed it also likely has a bug (which i have not updated the test to cover) wherein a wrapped range will only yield the portion at the end of the token range, not the beginning. it looks like we may have call sites using this function that do not realise this, so it could be a serious bug, especially for repair.",
        "label": 52
    },
    {
        "text": "npe in streamingmessage type lookup <description> found while investigating https://issues.apache.org/jira/browse/cassandra-15965 java.lang.nullpointerexception: null at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:51) at org.apache.cassandra.streaming.async.streaminginboundhandler$streamdeserializingtask.run(streaminginboundhandler.java:172) at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) at java.lang.thread.run(thread.java:748) there is a null in the zero.index of the type map. we can clean this up, handle invalid ids in a uniform manner, and add a test.<stacktrace> java.lang.nullpointerexception: null at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:51) at org.apache.cassandra.streaming.async.streaminginboundhandler$streamdeserializingtask.run(streaminginboundhandler.java:172) at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) at java.lang.thread.run(thread.java:748) <code> found while investigating https://issues.apache.org/jira/browse/cassandra-15965 <text> there is a null in the zero.index of the type map. we can clean this up, handle invalid ids in a uniform manner, and add a test.",
        "label": 8
    },
    {
        "text": "fix weightedsize  for row cache reported by jmx and nodetool <description> row cache size is reported in entries but should be reported in bytes (as keycache do).  it happens because incorrect ohcprovider.ohcacheadapter.weightedsize method. currently it returns cache size but should return ohcache.memused()<stacktrace> <code> <text> row cache size is reported in entries but should be reported in bytes (as keycache do).  it happens because incorrect ohcprovider.ohcacheadapter.weightedsize method. currently it returns cache size but should return ohcache.memused()",
        "label": 184
    },
    {
        "text": "distributed test scripts not working with whirr <description> i suspect that our runurl based script execution is not working with whirr 0.4.0, which is causing distributed tests that kill/wipe nodes to timeout. see this faq entry for a description of the change.<stacktrace> <code> <text> i suspect that our runurl based script execution is not working with whirr 0.4.0, which is causing distributed tests that kill/wipe nodes to timeout. see this faq entry for a description of the change.",
        "label": 340
    },
    {
        "text": "upgrade the jna version to <description> could you please upgrade the jna version present in the github cassandra  location : https://github.com/apache/cassandra/blob/trunk/lib/jna-4.0.0.jar  to below latest version - 4.3.0 -  http://repo1.maven.org/maven2/net/java/dev/jna/jna/4.3.0/jna-4.3.0-javadoc.jar<stacktrace> <code> could you please upgrade the jna version present in the github cassandra  location : https://github.com/apache/cassandra/blob/trunk/lib/jna-4.0.0.jar  to below latest version - 4.3.0 -  http://repo1.maven.org/maven2/net/java/dev/jna/jna/4.3.0/jna-4.3.0-javadoc.jar<text> ",
        "label": 232
    },
    {
        "text": "nullpointerexception in org apache cassandra service antientropyservice when repair finds a keyspace with no cfs <description> 2012-03-01 21:38:09,039 [rmi tcp connection(142)-10.253.106.21] info storageservice - starting repair command #15, repairing 3 ranges.  2012-03-01 21:38:09,039 [antientropysessions:14] info antientropyservice - repair #d68369f0-63e6-11e1-0000-8add8b9398fd new session: will sync /10.253.106.21, /10.253.106.248, /10.253.106.247 on range (85070591730234615865843651857942052864,106338239662793269832304564822427566080] for personalizationdataservice2.[]  2012-03-01 21:38:09,039 [antientropysessions:14] error abstractcassandradaemon - fatal exception in thread thread[antientropysessions:14,5,rmi runtime]  java.lang.nullpointerexception  at org.apache.cassandra.service.antientropyservice$repairsession.runmaythrow(antientropyservice.java:691)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<stacktrace> 2012-03-01 21:38:09,039 [rmi tcp connection(142)-10.253.106.21] info storageservice - starting repair command #15, repairing 3 ranges.  2012-03-01 21:38:09,039 [antientropysessions:14] info antientropyservice - repair #d68369f0-63e6-11e1-0000-8add8b9398fd new session: will sync /10.253.106.21, /10.253.106.248, /10.253.106.247 on range (85070591730234615865843651857942052864,106338239662793269832304564822427566080] for personalizationdataservice2.[]  2012-03-01 21:38:09,039 [antientropysessions:14] error abstractcassandradaemon - fatal exception in thread thread[antientropysessions:14,5,rmi runtime]  java.lang.nullpointerexception  at org.apache.cassandra.service.antientropyservice$repairsession.runmaythrow(antientropyservice.java:691)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<code> <text> ",
        "label": 520
    },
    {
        "text": "cql  support for batch insert delete <description> test case to run // setup create columnfamily users (   key varchar primary key,   password varchar); //batch insert begin batch using quorum insert into users (key, password) values ('user2', 'ch@ngem3b'); insert into users (key, password) values ('user3', 'ch@ngem3c'); apply batch actual behavior cqlsh> begin batch using quorum    ... insert into users (key, password) values ('user2', 'ch@ngem3b'); bad request: line 0:-1 mismatched input '<eof>' expecting k_apply documentation sample begin batch [using <consistency>] update cf1 set name1 = value1, name2 = value2 where key = keyname1; update cf1 set name3 = value3 where key = keyname2; update cf2 set name4 = value4, name5 = value5 where key = keyname3; apply batch<stacktrace> <code> // setup create columnfamily users (   key varchar primary key,   password varchar); //batch insert begin batch using quorum insert into users (key, password) values ('user2', 'ch@ngem3b'); insert into users (key, password) values ('user3', 'ch@ngem3c'); apply batch cqlsh> begin batch using quorum    ... insert into users (key, password) values ('user2', 'ch@ngem3b'); bad request: line 0:-1 mismatched input '<eof>' expecting k_apply begin batch [using <consistency>] update cf1 set name1 = value1, name2 = value2 where key = keyname1; update cf1 set name3 = value3 where key = keyname2; update cf2 set name4 = value4, name5 = value5 where key = keyname3; apply batch <text> test case to run actual behavior documentation sample",
        "label": 412
    },
    {
        "text": "messagingservice should handle failures on remote nodes  <description> while going through the code of messagingservice, i discovered that we don't handle callbacks on failure very well. if a verb handler on the remote machine throws an exception, it goes right through uncaught exception handler. the machine which triggered the message will keep waiting and will timeout. on timeout, it will so some stuff hard coded in the ms like hints and add to latency. there is no way in iasynccallback to specify that to do on timeouts and also on failures. here are some examples which i found will help if we enhance this system to also propagate failures back. so iasynccallback will have methods like onfailure. 1) from activerepairservice.prepareforrepair  iasynccallback callback = new iasynccallback()  {  @override  public void response(messagein msg) { preparelatch.countdown(); } @override  public boolean islatencyforsnitch() { return false; } };  list<uuid> cfids = new arraylist<>(columnfamilystores.size());  for (columnfamilystore cfs : columnfamilystores)  cfids.add(cfs.metadata.cfid);  for(inetaddress neighbour : endpoints) { preparemessage message = new preparemessage(parentrepairsession, cfids, ranges); messageout<repairmessage> msg = message.createmessage(); messagingservice.instance().sendrr(msg, neighbour, callback); } try { preparelatch.await(1, timeunit.hours); } catch (interruptedexception e) { parentrepairsessions.remove(parentrepairsession); throw new runtimeexception(\"did not get replies from all endpoints.\", e); } 2) during snapshot phase in repair, if snapshotverbhandler throws an exception, we will wait forever.<stacktrace> <code> 1) from activerepairservice.prepareforrepair iasynccallback callback = new iasynccallback()  {  @override  public void response(messagein msg) @override  public boolean islatencyforsnitch() }; list<uuid> cfids = new arraylist<>(columnfamilystores.size());  for (columnfamilystore cfs : columnfamilystores)  cfids.add(cfs.metadata.cfid); for(inetaddress neighbour : endpoints) catch (interruptedexception e) <text> while going through the code of messagingservice, i discovered that we don't handle callbacks on failure very well. if a verb handler on the remote machine throws an exception, it goes right through uncaught exception handler. the machine which triggered the message will keep waiting and will timeout. on timeout, it will so some stuff hard coded in the ms like hints and add to latency. there is no way in iasynccallback to specify that to do on timeouts and also on failures. here are some examples which i found will help if we enhance this system to also propagate failures back. so iasynccallback will have methods like onfailure. try 2) during snapshot phase in repair, if snapshotverbhandler throws an exception, we will wait forever.",
        "label": 482
    },
    {
        "text": "simplestrategy w o replication factor <description> it is possible to create a new keyspace using simplestrategy without specifying the replication_factor option. things get more interesting if you shut the node down, since it will refuse to restart (throwing a configurationexception).<stacktrace> <code> <text> it is possible to create a new keyspace using simplestrategy without specifying the replication_factor option. things get more interesting if you shut the node down, since it will refuse to restart (throwing a configurationexception).",
        "label": 274
    },
    {
        "text": "provide max hint window as part of nodetool <description> currently it is not possible to get max_hint_window over nodetool. the information is available through storageproxymbean, though. since max hint window information is needed in order to asses what kind of failure recovery should be performed for a node that goes down (bootstrap or just restart), it would be handy if max hint window is easily accessible using nodetool. currently nodetool statushandoff output is: [centos@cassandra-node]$ nodetool statushandoff hinted handoff is running the output could be improved to look like this: [centos@cassandra-node]$ nodetool statushandoff hinted handoff is running with max hint window (ms): 10800000 implementation is quite trivial (fetch the info from the storageproxymbean from the statushandoff class). i can provide the patch for this, if it is agreed that this it right approach.<stacktrace> <code> [centos@cassandra-node]$ nodetool statushandoff hinted handoff is running [centos@cassandra-node]$ nodetool statushandoff hinted handoff is running with max hint window (ms): 10800000 <text> currently it is not possible to get max_hint_window over nodetool. the information is available through storageproxymbean, though. since max hint window information is needed in order to asses what kind of failure recovery should be performed for a node that goes down (bootstrap or just restart), it would be handy if max hint window is easily accessible using nodetool. currently nodetool statushandoff output is: the output could be improved to look like this: implementation is quite trivial (fetch the info from the storageproxymbean from the statushandoff class). i can provide the patch for this, if it is agreed that this it right approach.",
        "label": 552
    },
    {
        "text": "hints are not seekable <description> got the following error message on trunk. no idea how to reproduce. but the only thing the (not overridden) seek method does is throwing this exception. error [hintsdispatcher:2] 2016-06-05 18:51:09,397 cassandradaemon.java:222 - exception in thread thread[hintsdispatcher:2,1,main] java.lang.unsupportedoperationexception: hints are not seekable. at org.apache.cassandra.hints.hintsreader.seek(hintsreader.java:114) ~[main/:na] at org.apache.cassandra.hints.hintsdispatcher.seek(hintsdispatcher.java:79) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.deliver(hintsdispatchexecutor.java:257) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.dispatch(hintsdispatchexecutor.java:242) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.dispatch(hintsdispatchexecutor.java:220) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.run(hintsdispatchexecutor.java:199) ~[main/:na] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_91] at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_91] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_91] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_91] at java.lang.thread.run(thread.java:745) [na:1.8.0_91]<stacktrace> error [hintsdispatcher:2] 2016-06-05 18:51:09,397 cassandradaemon.java:222 - exception in thread thread[hintsdispatcher:2,1,main] java.lang.unsupportedoperationexception: hints are not seekable. at org.apache.cassandra.hints.hintsreader.seek(hintsreader.java:114) ~[main/:na] at org.apache.cassandra.hints.hintsdispatcher.seek(hintsdispatcher.java:79) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.deliver(hintsdispatchexecutor.java:257) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.dispatch(hintsdispatchexecutor.java:242) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.dispatch(hintsdispatchexecutor.java:220) ~[main/:na] at org.apache.cassandra.hints.hintsdispatchexecutor$dispatchhintstask.run(hintsdispatchexecutor.java:199) ~[main/:na] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_91] at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_91] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_91] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_91] at java.lang.thread.run(thread.java:745) [na:1.8.0_91] <code> <text> got the following error message on trunk. no idea how to reproduce. but the only thing the (not overridden) seek method does is throwing this exception.",
        "label": 507
    },
    {
        "text": "heuristic or hard cap to prevent fragmented commit logs from bringing down the server <description> widely divergent write rates on column families can cause the commit log segments to fragment. in some cases we have seen the commit log partition overrun. one solution here would be to create a heuristic for segment fragmentation to trigger a flush (commit log segments/memtable) or simply track the free disk space and force a global flush when the disk gets to 80% capacity.<stacktrace> <code> <text> widely divergent write rates on column families can cause the commit log segments to fragment. in some cases we have seen the commit log partition overrun. one solution here would be to create a heuristic for segment fragmentation to trigger a flush (commit log segments/memtable) or simply track the free disk space and force a global flush when the disk gets to 80% capacity.",
        "label": 397
    },
    {
        "text": "cql version is wrong either in doc or in code <description> the textile doc in 1.2.2+ for cql3 says it's version 3.0.2. the code says it's 3.0.1. in 1.2-branch and trunk it says it is 3.0.2 even though it should be 3.0.3 and 3.1.0 respectively. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/cql3/queryprocessor.java#l44  https://github.com/apache/cassandra/blob/cassandra-1.2/doc/cql3/cql.textile https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/queryprocessor.java#l43  https://github.com/apache/cassandra/blob/trunk/doc/cql3/cql.textile<stacktrace> <code> https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/cql3/queryprocessor.java#l44  https://github.com/apache/cassandra/blob/cassandra-1.2/doc/cql3/cql.textile https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/queryprocessor.java#l43  https://github.com/apache/cassandra/blob/trunk/doc/cql3/cql.textile<text> the textile doc in 1.2.2+ for cql3 says it's version 3.0.2. the code says it's 3.0.1. in 1.2-branch and trunk it says it is 3.0.2 even though it should be 3.0.3 and 3.1.0 respectively. ",
        "label": 18
    },
    {
        "text": " patch  make sure keystore streams get closed <description> code opens keystore streams and relies on the gc to close them. give the gc a break and just close them immediately.<stacktrace> <code> <text> code opens keystore streams and relies on the gc to close them. give the gc a break and just close them immediately.",
        "label": 139
    },
    {
        "text": "npe when cas encounters empty frozen collection <description> when a compare-and-set operation specifying an equality criterion with a non-null value encounters an empty collection (null cell), the server throws a nullpointerexception and the query fails. this does not happen for non-frozen collections. there's a self-contained test case at github. the stack trace for 3.11.0 is: error [native-transport-requests-1] 2017-11-27 12:59:26,924 querymessage.java:129 - unexpected error during query java.lang.nullpointerexception: null         at org.apache.cassandra.cql3.columncondition$collectionbound.appliesto(columncondition.java:546) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.cql3casrequest$columnsconditions.appliesto(cql3casrequest.java:324) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.cql3casrequest.appliesto(cql3casrequest.java:210) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.service.storageproxy.cas(storageproxy.java:265) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.modificationstatement.executewithcondition(modificationstatement.java:441) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.modificationstatement.execute(modificationstatement.java:416) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.44.final.jar:4.0.44.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:357) [netty-all-4.0.44.final.jar:4.0.44.final]         at io.netty.channel.abstractchannelhandlercontext.access$600(abstractchannelhandlercontext.java:35) [netty-all-4.0.44.final.jar:4.0.44.final]         at io.netty.channel.abstractchannelhandlercontext$7.run(abstractchannelhandlercontext.java:348) [netty-all-4.0.44.final.jar:4.0.44.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_151]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:162) [apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]         at java.lang.thread.run(thread.java:748) [na:1.8.0_151]<stacktrace> error [native-transport-requests-1] 2017-11-27 12:59:26,924 querymessage.java:129 - unexpected error during query java.lang.nullpointerexception: null         at org.apache.cassandra.cql3.columncondition$collectionbound.appliesto(columncondition.java:546) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.cql3casrequest$columnsconditions.appliesto(cql3casrequest.java:324) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.cql3casrequest.appliesto(cql3casrequest.java:210) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.service.storageproxy.cas(storageproxy.java:265) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.modificationstatement.executewithcondition(modificationstatement.java:441) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.statements.modificationstatement.execute(modificationstatement.java:416) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.44.final.jar:4.0.44.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:357) [netty-all-4.0.44.final.jar:4.0.44.final]         at io.netty.channel.abstractchannelhandlercontext.access$600(abstractchannelhandlercontext.java:35) [netty-all-4.0.44.final.jar:4.0.44.final]         at io.netty.channel.abstractchannelhandlercontext$7.run(abstractchannelhandlercontext.java:348) [netty-all-4.0.44.final.jar:4.0.44.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_151]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:162) [apache-cassandra-3.11.0.jar:3.11.0]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]         at java.lang.thread.run(thread.java:748) [na:1.8.0_151] <code> <text> when a compare-and-set operation specifying an equality criterion with a non-null value encounters an empty collection (null cell), the server throws a nullpointerexception and the query fails. this does not happen for non-frozen collections. there's a self-contained test case at github. the stack trace for 3.11.0 is:",
        "label": 305
    },
    {
        "text": "cli escaped single quote parsing gives errors <description> escaping quotes in cli commands causes parsing errors. some examples::::  no need to create columns etc, it doesn't get through parsing the expression:: cassandra-cli 1.   set column['key+vals'][value] = 'val\\'' ;  syntax error at position 41: mismatched character '<eof>' expecting ''' 2.  set column['key+val\\'s'][value] = 'val' ;  syntax error at position 41: mismatched character '<eof>' expecting ''' 3.  set column['key+vals\\''][value] = 'val\\'' ;  syntax error at position 38: unexpected \"\\\" for `set column['key+vals\\''][value] = 'val\\'' ;`.<stacktrace> <code> cassandra-cli 2.  set column['key+val/'s'][value] = 'val' ;  syntax error at position 41: mismatched character '<eof>' expecting ''' <text> escaping quotes in cli commands causes parsing errors. some examples::::  no need to create columns etc, it doesn't get through parsing the expression:: 1.   set column['key+vals'][value] = 'val/'' ;  syntax error at position 41: mismatched character '<eof>' expecting ''' 3.  set column['key+vals/''][value] = 'val/'' ;  syntax error at position 38: unexpected '/' for `set column['key+vals/''][value] = 'val/'' ;`.",
        "label": 412
    },
    {
        "text": "cannot start cassandra under windows <description> i get the following error when i try to launch the rc of cassandra from the command line: starting cassandra server  listening for transport dt_socket at address: 8888  exception in thread \"main\" java.lang.noclassdeffounderror: org/apache/cassandra/thrift/cassandradaemon  caused by: java.lang.classnotfoundexception: org.apache.cassandra.thrift.cassandradaemon  at java.net.urlclassloader$1.run(unknown source)  at java.security.accesscontroller.doprivileged(native method)  at java.net.urlclassloader.findclass(unknown source)  at java.lang.classloader.loadclass(unknown source)  at sun.misc.launcher$appclassloader.loadclass(unknown source)  at java.lang.classloader.loadclass(unknown source)  could not find the main class: org.apache.cassandra.thrift.cassandradaemon. program will exit.<stacktrace> starting cassandra server  listening for transport dt_socket at address: 8888  exception in thread 'main' java.lang.noclassdeffounderror: org/apache/cassandra/thrift/cassandradaemon  caused by: java.lang.classnotfoundexception: org.apache.cassandra.thrift.cassandradaemon  at java.net.urlclassloader$1.run(unknown source)  at java.security.accesscontroller.doprivileged(native method)  at java.net.urlclassloader.findclass(unknown source)  at java.lang.classloader.loadclass(unknown source)  at sun.misc.launcher$appclassloader.loadclass(unknown source)  at java.lang.classloader.loadclass(unknown source)  could not find the main class: org.apache.cassandra.thrift.cassandradaemon. program will exit.<code> <text> i get the following error when i try to launch the rc of cassandra from the command line: ",
        "label": 186
    },
    {
        "text": "fix flaky test incrementalsstableselection   org apache cassandra db streaming cassandrastreammanagertest <description> build link: https://app.circleci.com/pipelines/github/dcapwell/cassandra/287/workflows/06baf3db-7094-431f-920d-e8fcd1da9cce/jobs/1398 java.lang.runtimeexception: java.nio.file.nosuchfileexception: /tmp/cassandra/build/test/cassandra/data:2/ks_1589913975959/tbl-051c0a709a0111eab5fb6f52366536f8/na-4-big-statistics.db at org.apache.cassandra.io.util.channelproxy.openchannel(channelproxy.java:55) at org.apache.cassandra.io.util.channelproxy.<init>(channelproxy.java:66) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:315) at org.apache.cassandra.io.sstable.metadata.metadataserializer.deserialize(metadataserializer.java:126) at org.apache.cassandra.io.sstable.metadata.metadataserializer.deserialize(metadataserializer.java:136) at org.apache.cassandra.io.sstable.format.sstablereader.reloadsstablemetadata(sstablereader.java:2047) at org.apache.cassandra.db.streaming.cassandrastreammanagertest.mutaterepaired(cassandrastreammanagertest.java:128) at org.apache.cassandra.db.streaming.cassandrastreammanagertest.incrementalsstableselection(cassandrastreammanagertest.java:175) caused by: java.nio.file.nosuchfileexception: /tmp/cassandra/build/test/cassandra/data:2/ks_1589913975959/tbl-051c0a709a0111eab5fb6f52366536f8/na-4-big-statistics.db at sun.nio.fs.unixexception.translatetoioexception(unixexception.java:86) at sun.nio.fs.unixexception.rethrowasioexception(unixexception.java:102) at sun.nio.fs.unixexception.rethrowasioexception(unixexception.java:107) at sun.nio.fs.unixfilesystemprovider.newfilechannel(unixfilesystemprovider.java:177) at java.nio.channels.filechannel.open(filechannel.java:287) at java.nio.channels.filechannel.open(filechannel.java:335) at org.apache.cassandra.io.util.channelproxy.openchannel(channelproxy.java:51)<stacktrace> java.lang.runtimeexception: java.nio.file.nosuchfileexception: /tmp/cassandra/build/test/cassandra/data:2/ks_1589913975959/tbl-051c0a709a0111eab5fb6f52366536f8/na-4-big-statistics.db at org.apache.cassandra.io.util.channelproxy.openchannel(channelproxy.java:55) at org.apache.cassandra.io.util.channelproxy.<init>(channelproxy.java:66) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:315) at org.apache.cassandra.io.sstable.metadata.metadataserializer.deserialize(metadataserializer.java:126) at org.apache.cassandra.io.sstable.metadata.metadataserializer.deserialize(metadataserializer.java:136) at org.apache.cassandra.io.sstable.format.sstablereader.reloadsstablemetadata(sstablereader.java:2047) at org.apache.cassandra.db.streaming.cassandrastreammanagertest.mutaterepaired(cassandrastreammanagertest.java:128) at org.apache.cassandra.db.streaming.cassandrastreammanagertest.incrementalsstableselection(cassandrastreammanagertest.java:175) caused by: java.nio.file.nosuchfileexception: /tmp/cassandra/build/test/cassandra/data:2/ks_1589913975959/tbl-051c0a709a0111eab5fb6f52366536f8/na-4-big-statistics.db at sun.nio.fs.unixexception.translatetoioexception(unixexception.java:86) at sun.nio.fs.unixexception.rethrowasioexception(unixexception.java:102) at sun.nio.fs.unixexception.rethrowasioexception(unixexception.java:107) at sun.nio.fs.unixfilesystemprovider.newfilechannel(unixfilesystemprovider.java:177) at java.nio.channels.filechannel.open(filechannel.java:287) at java.nio.channels.filechannel.open(filechannel.java:335) at org.apache.cassandra.io.util.channelproxy.openchannel(channelproxy.java:51) <code> build link: https://app.circleci.com/pipelines/github/dcapwell/cassandra/287/workflows/06baf3db-7094-431f-920d-e8fcd1da9cce/jobs/1398<text> ",
        "label": 73
    },
    {
        "text": "cassandra cli doesn't allow hyphens in hostnames <description> it's not possible to use a hostname that contains a hyphen with the \"connect\" command interactively, (the parser does not accept hostnames that contain hyphens). note: it is still possible to connect to such hosts by passing it on the command line using -host.<stacktrace> <code> <text> it's not possible to use a hostname that contains a hyphen with the 'connect' command interactively, (the parser does not accept hostnames that contain hyphens). note: it is still possible to connect to such hosts by passing it on the command line using -host.",
        "label": 455
    },
    {
        "text": "after changing the compaction strategy  compression strategy always returning back to the  snappycompressor  through cql <description> faced very strange behaviour when changing compression_parameters of exisiting cf. after changing the compaction strategy, compression_strategy returning back to the \"snappycompressor\". using cassandra version 1.1.5.  [cqlsh 2.2.0 | cassandra 1.1.5 | cql spec 2.0.0 | thrift protocol 19.32.0]  i have one column family with following paramters: cqlsh > describe columnfamily auditlog_01;  create table auditlog_01 (  lid text primary key,  dscn text,  asid text,  soapa text  ) with  comment='' and  comparator=text and  read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  default_validation=text and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  replicate_on_write='true' and  compaction_strategy_class='sizetieredcompactionstrategy' and  compaction_strategy_options:sstable_size_in_mb='5' and  compression_parameters:sstable_compression='snappycompressor'; changing compression strategy to 'deflatecompressor cqlsh> alter table auditlog_01 with compression_parameters:sstabl  e_compression = 'deflatecompressor' and compression_parameters:chunk_length_kb =  64;  cqlsh> describe columnfamily auditlog_01; create table auditlog_01 (  lid text primary key,  dscn text,  asid text,  soapa text  ) with  comment='' and  comparator=text and  read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  default_validation=text and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  replicate_on_write='true' and  compaction_strategy_class='sizetieredcompactionstrategy' and  compaction_strategy_options:sstable_size_in_mb='5' and  compression_parameters:chunk_length_kb='64' and  compression_parameters:sstable_compression='deflatecompressor'; it's sucessfuly changed the compression strategy to 'deflatecompressor, after that when i am trying to change the compaction strategy, compression strategy returing back to \"snappycompressor\".  cqlsh> alter table auditlog_01 with compaction_strategy_class='le  veledcompactionstrategy' and compaction_strategy_options:sstable_size_in_mb=5;  cqlsh> describe columnfamily auditlog_01; create table auditlog_01 (  lid text primary key,  dscn text,  asid text,  soapa text  ) with  comment='' and  comparator=text and  read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  default_validation=text and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  replicate_on_write='true' and  compaction_strategy_class='sizetieredcompactionstrategy' and  compaction_strategy_options:sstable_size_in_mb='5' and  compression_parameters:sstable_compression='snappycompressor';<stacktrace> <code> cqlsh > describe columnfamily auditlog_01;  create table auditlog_01 (  lid text primary key,  dscn text,  asid text,  soapa text  ) with  comment='' and  comparator=text and  read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  default_validation=text and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  replicate_on_write='true' and  compaction_strategy_class='sizetieredcompactionstrategy' and  compaction_strategy_options:sstable_size_in_mb='5' and  compression_parameters:sstable_compression='snappycompressor'; changing compression strategy to 'deflatecompressor cqlsh> alter table auditlog_01 with compression_parameters:sstabl  e_compression = 'deflatecompressor' and compression_parameters:chunk_length_kb =  64;  cqlsh> describe columnfamily auditlog_01; create table auditlog_01 (  lid text primary key,  dscn text,  asid text,  soapa text  ) with  comment='' and  comparator=text and  read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  default_validation=text and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  replicate_on_write='true' and  compaction_strategy_class='sizetieredcompactionstrategy' and  compaction_strategy_options:sstable_size_in_mb='5' and  compression_parameters:chunk_length_kb='64' and  compression_parameters:sstable_compression='deflatecompressor'; it's sucessfuly changed the compression strategy to 'deflatecompressor, after that when i am trying to change the compaction strategy, compression strategy returing back to 'snappycompressor'.  cqlsh> alter table auditlog_01 with compaction_strategy_class='le  veledcompactionstrategy' and compaction_strategy_options:sstable_size_in_mb=5;  cqlsh> describe columnfamily auditlog_01; create table auditlog_01 (  lid text primary key,  dscn text,  asid text,  soapa text  ) with  comment='' and  comparator=text and  read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  default_validation=text and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  replicate_on_write='true' and  compaction_strategy_class='sizetieredcompactionstrategy' and  compaction_strategy_options:sstable_size_in_mb='5' and  compression_parameters:sstable_compression='snappycompressor';<text> faced very strange behaviour when changing compression_parameters of exisiting cf. after changing the compaction strategy, compression_strategy returning back to the 'snappycompressor'. using cassandra version 1.1.5.  [cqlsh 2.2.0 | cassandra 1.1.5 | cql spec 2.0.0 | thrift protocol 19.32.0]  i have one column family with following paramters: ",
        "label": 18
    },
    {
        "text": "faster index sampling <description> some discussion on cassandra-1526<stacktrace> <code> <text> some discussion on cassandra-1526",
        "label": 515
    },
    {
        "text": "invocationtargetexception concurrentmodificationexception at startup <description> i was starting up the new datastax ami where the seed starts first and 34 nodes would latch on together. so far things have been working decently for launching, but right now i just got this during startup. ubuntu@ip-10-40-190-143:~$ sudo cat /var/log/cassandra/output.log   info 09:24:38,453 jvm vendor/version: java hotspot(tm) 64-bit server vm/1.6.0_26  info 09:24:38,456 heap size: 1936719872/1937768448  info 09:24:38,457 classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar  info 09:24:39,891 jna mlockall successful  info 09:24:39,901 loading settings from file:/etc/cassandra/cassandra.yaml  info 09:24:40,057 diskaccessmode 'auto' determined to be mmap, indexaccessmode is mmap  info 09:24:40,069 global memtable threshold is enabled at 616mb  info 09:24:40,159 ec2snitch using region: us-east, zone: 1d.  info 09:24:40,475 creating new commitlog segment /raid0/cassandra/commitlog/commitlog-1319793880475.log  info 09:24:40,486 couldn't detect any schema definitions in local storage.  info 09:24:40,486 found table data in data directories. consider using the cli to define your schema.  info 09:24:40,497 no commitlog files found; skipping replay  info 09:24:40,501 cassandra version: 1.0.0  info 09:24:40,502 thrift api version: 19.18.0  info 09:24:40,502 loading persisted ring state  info 09:24:40,506 starting up server gossip  info 09:24:40,529 enqueuing flush of memtable-locationinfo@1388314661(190/237 serialized/live bytes, 4 ops)  info 09:24:40,530 writing memtable-locationinfo@1388314661(190/237 serialized/live bytes, 4 ops)  info 09:24:40,600 completed flushing /raid0/cassandra/data/system/locationinfo-h-1-data.db (298 bytes)  info 09:24:40,613 ec2snitch adding applicationstate ec2region=us-east ec2zone=1d  info 09:24:40,621 starting messaging service on /10.40.190.143:7000  info 09:24:40,628 joining: waiting for ring and schema information  info 09:24:43,389 inetaddress /10.194.29.156 is now dead.  info 09:24:43,391 inetaddress /10.85.11.38 is now dead.  info 09:24:43,392 inetaddress /10.34.42.28 is now dead.  info 09:24:43,393 inetaddress /10.77.63.49 is now dead.  info 09:24:43,394 inetaddress /10.194.22.191 is now dead.  info 09:24:43,395 inetaddress /10.34.74.58 is now dead.  info 09:24:43,395 node /10.34.33.16 is now part of the cluster  info 09:24:43,396 inetaddress /10.34.33.16 is now up  info 09:24:43,397 enqueuing flush of memtable-locationinfo@1629818866(20/25 serialized/live bytes, 1 ops)  info 09:24:43,398 writing memtable-locationinfo@1629818866(20/25 serialized/live bytes, 1 ops)  info 09:24:43,417 completed flushing /raid0/cassandra/data/system/locationinfo-h-2-data.db (74 bytes)  info 09:24:43,418 inetaddress /10.202.67.43 is now dead.  info 09:24:43,419 inetaddress /10.116.215.81 is now dead.  info 09:24:43,420 inetaddress /10.99.39.242 is now dead.  info 09:24:43,421 inetaddress /10.80.110.28 is now dead.  info 09:24:43,422 inetaddress /10.118.233.198 is now dead.  info 09:24:43,423 inetaddress /10.40.177.173 is now dead.  info 09:24:43,424 inetaddress /10.205.23.34 is now dead.  info 09:24:43,425 inetaddress /10.101.41.8 is now dead.  info 09:24:43,669 inetaddress /10.118.230.219 is now dead.  info 09:24:43,670 inetaddress /10.80.41.192 is now dead.  info 09:24:43,671 inetaddress /10.40.22.224 is now dead.  info 09:24:43,672 inetaddress /10.39.107.114 is now dead.  info 09:24:46,164 inetaddress /10.118.185.68 is now dead.  info 09:24:46,166 inetaddress /10.84.205.93 is now dead.  info 09:24:46,167 inetaddress /10.116.134.183 is now dead.  info 09:24:46,670 inetaddress /10.118.179.67 is now dead.  info 09:24:46,671 inetaddress /10.116.241.250 is now dead.  info 09:24:48,441 inetaddress /10.118.94.62 is now dead.  info 09:24:48,442 inetaddress /10.99.86.251 is now dead.  info 09:24:50,176 inetaddress /10.113.42.21 is now dead.  info 09:24:50,177 inetaddress /10.34.159.72 is now dead.  info 09:24:50,178 inetaddress /10.32.79.134 is now dead.  info 09:24:50,179 inetaddress /10.80.210.38 is now dead.  info 09:24:50,180 inetaddress /10.34.70.73 is now dead.  info 09:24:50,181 inetaddress /10.196.79.240 is now dead.  info 09:25:01,713 inetaddress /10.82.210.172 is now dead.  info 09:25:06,202 inetaddress /10.80.110.28 is now up  info 09:25:06,908 inetaddress /10.99.39.242 is now up  info 09:25:07,696 inetaddress /10.118.233.198 is now up  info 09:25:07,697 inetaddress /10.205.23.34 is now up  info 09:25:08,704 inetaddress /10.194.22.191 is now up  info 09:25:08,705 inetaddress /10.40.177.173 is now up  info 09:25:08,706 inetaddress /10.101.41.8 is now up  info 09:25:09,489 inetaddress /10.202.67.43 is now up  info 09:25:09,698 inetaddress /10.77.63.49 is now up  info 09:25:10,628 joining: getting bootstrap token  info 09:25:10,631 enqueuing flush of memtable-locationinfo@1733057335(36/45 serialized/live bytes, 1 ops)  info 09:25:10,631 writing memtable-locationinfo@1733057335(36/45 serialized/live bytes, 1 ops)  info 09:25:10,647 completed flushing /raid0/cassandra/data/system/locationinfo-h-3-data.db (87 bytes)  info 09:25:10,649 joining: sleeping 30000 ms for pending range setup  info 09:25:10,689 inetaddress /10.85.11.38 is now up  info 09:25:10,708 inetaddress /10.34.74.58 is now up  info 09:25:10,912 inetaddress /10.194.29.156 is now up  info 09:25:11,261 applying migration bb843dd0-0146-11e1-0000-b877c09da5ff add keyspace: opscenter, rep strategy:simplestrategy{org.apache.cassandra.config.cfmetadata@55e29b99[cfid=1000,ksname=opscenter,cfname=pdps,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=300.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=32,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@105585dc,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@5ec736e4[cfid=1004,ksname=opscenter,cfname=rollups86400,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=2,maxcompactionthreshold=8,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@68e4e358,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@b09dc35[cfid=1003,ksname=opscenter,cfname=rollups7200,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=2,maxcompactionthreshold=8,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@3458213c,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@5ee04fd[cfid=1002,ksname=opscenter,cfname=rollups300,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=16,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@4d898115,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@7e79b177[cfid=1005,ksname=opscenter,cfname=events,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=opscenter raw event storage,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=8,maxcompactionthreshold=12,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@67723c7f,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@540523be[cfid=1006,ksname=opscenter,cfname=events_timeline,cftype=standard,comparator=org.apache.cassandra.db.marshal.longtype,subcolumncomparator=<null>,comment=opscenter event timelines,rowcachesize=0.0,keycachesize=5.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=8,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=0,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@1d6dba0a,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@ed0f59e[cfid=1007,ksname=opscenter,cfname=settings,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=opscenter settings,rowcachesize=0.0,keycachesize=50.0,readrepairchance=1.0,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=8,maxcompactionthreshold=12,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@38ad5fab,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@7e63f09e[cfid=1001,ksname=opscenter,cfname=rollups60,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=32,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@534a55e5,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}]}, durable_writes: true  info 09:25:11,273 enqueuing flush of memtable-migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)  info 09:25:11,273 writing memtable-migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)  info 09:25:11,274 enqueuing flush of memtable-schema@1616586953(5820/7275 serialized/live bytes, 3 ops)  info 09:25:11,358 completed flushing /raid0/cassandra/data/system/migrations-h-1-data.db (12989 bytes)  info 09:25:11,358 writing memtable-schema@1616586953(5820/7275 serialized/live bytes, 3 ops)  info 09:25:11,390 completed flushing /raid0/cassandra/data/system/schema-h-1-data.db (5970 bytes)  info 09:25:11,727 inetaddress /10.116.215.81 is now up  info 09:25:11,744 inetaddress /10.34.42.28 is now up  info 09:25:11,750 inetaddress /10.40.22.224 is now up  info 09:25:12,023 inetaddress /10.80.41.192 is now up  info 09:25:12,712 inetaddress /10.39.107.114 is now up  info 09:25:12,717 inetaddress /10.118.185.68 is now up  info 09:25:12,721 inetaddress /10.116.134.183 is now up  info 09:25:13,322 inetaddress /10.118.230.219 is now up  info 09:25:13,632 inetaddress /10.84.205.93 is now up  info 09:25:14,713 inetaddress /10.118.179.67 is now up  info 09:25:14,717 inetaddress /10.116.241.250 is now up  info 09:25:17,468 inetaddress /10.34.159.72 is now up  info 09:25:17,476 inetaddress /10.118.94.62 is now up  info 09:25:17,480 inetaddress /10.80.210.38 is now up  info 09:25:17,716 inetaddress /10.32.79.134 is now up  info 09:25:17,721 inetaddress /10.99.86.251 is now up  info 09:25:18,717 inetaddress /10.196.79.240 is now up  info 09:25:18,727 inetaddress /10.34.70.73 is now up  info 09:25:19,596 inetaddress /10.113.42.21 is now up  info 09:25:25,750 inetaddress /10.82.210.172 is now up  info 09:25:37,743 enqueuing flush of memtable-locationinfo@288976631(35/43 serialized/live bytes, 1 ops)  info 09:25:37,744 writing memtable-locationinfo@288976631(35/43 serialized/live bytes, 1 ops)  info 09:25:37,764 completed flushing /raid0/cassandra/data/system/locationinfo-h-4-data.db (89 bytes)  info 09:25:37,773 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-1-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-3-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-4-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-2-data.db')]  info 09:25:37,776 enqueuing flush of memtable-locationinfo@1950702248(35/43 serialized/live bytes, 1 ops)  info 09:25:37,777 writing memtable-locationinfo@1950702248(35/43 serialized/live bytes, 1 ops)  info 09:25:37,821 completed flushing /raid0/cassandra/data/system/locationinfo-h-5-data.db (89 bytes)  info 09:25:37,869 compacted to [/raid0/cassandra/data/system/locationinfo-h-6-data.db,].  548 to 443 (~80% of original) bytes for 3 keys at 0.006500mb/s.  time: 65ms.  info 09:25:38,740 enqueuing flush of memtable-locationinfo@92917455(35/43 serialized/live bytes, 1 ops)  info 09:25:38,740 writing memtable-locationinfo@92917455(35/43 serialized/live bytes, 1 ops)  info 09:25:38,757 completed flushing /raid0/cassandra/data/system/locationinfo-h-8-data.db (89 bytes)  info 09:25:38,766 enqueuing flush of memtable-locationinfo@1096488363(35/43 serialized/live bytes, 1 ops)  info 09:25:38,767 writing memtable-locationinfo@1096488363(35/43 serialized/live bytes, 1 ops)  info 09:25:38,814 completed flushing /raid0/cassandra/data/system/locationinfo-h-9-data.db (89 bytes)  info 09:25:38,816 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-6-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-9-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-8-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-5-data.db')]  info 09:25:38,823 enqueuing flush of memtable-locationinfo@1734564525(35/43 serialized/live bytes, 1 ops)  info 09:25:38,823 writing memtable-locationinfo@1734564525(35/43 serialized/live bytes, 1 ops)  info 09:25:38,893 completed flushing /raid0/cassandra/data/system/locationinfo-h-10-data.db (89 bytes)  info 09:25:38,916 compacted to [/raid0/cassandra/data/system/locationinfo-h-11-data.db,].  710 to 548 (~77% of original) bytes for 3 keys at 0.005226mb/s.  time: 100ms.  info 09:25:39,538 enqueuing flush of memtable-locationinfo@811507066(35/43 serialized/live bytes, 1 ops)  info 09:25:39,539 writing memtable-locationinfo@811507066(35/43 serialized/live bytes, 1 ops)  info 09:25:39,555 completed flushing /raid0/cassandra/data/system/locationinfo-h-13-data.db (89 bytes)  info 09:25:39,578 enqueuing flush of memtable-locationinfo@1125690366(35/43 serialized/live bytes, 1 ops)  info 09:25:39,578 writing memtable-locationinfo@1125690366(35/43 serialized/live bytes, 1 ops)  info 09:25:39,594 completed flushing /raid0/cassandra/data/system/locationinfo-h-14-data.db (89 bytes)  info 09:25:39,596 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-11-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-10-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-14-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-13-data.db')]  info 09:25:39,613 enqueuing flush of memtable-locationinfo@1870148830(35/43 serialized/live bytes, 1 ops)  info 09:25:39,614 writing memtable-locationinfo@1870148830(35/43 serialized/live bytes, 1 ops)  info 09:25:39,652 completed flushing /raid0/cassandra/data/system/locationinfo-h-16-data.db (89 bytes)  info 09:25:39,692 compacted to [/raid0/cassandra/data/system/locationinfo-h-15-data.db,].  815 to 653 (~80% of original) bytes for 3 keys at 0.006487mb/s.  time: 96ms.  info 09:25:39,731 enqueuing flush of memtable-locationinfo@1279866611(35/43 serialized/live bytes, 1 ops)  info 09:25:39,731 writing memtable-locationinfo@1279866611(35/43 serialized/live bytes, 1 ops)  info 09:25:39,747 completed flushing /raid0/cassandra/data/system/locationinfo-h-18-data.db (89 bytes)  info 09:25:40,649 starting to bootstrap...  info 09:25:40,701 finished streaming session 304272969286 from /10.205.23.34  info 09:25:40,703 enqueuing flush of memtable-locationinfo@1868577756(53/66 serialized/live bytes, 2 ops)  info 09:25:40,703 writing memtable-locationinfo@1868577756(53/66 serialized/live bytes, 2 ops)  info 09:25:40,721 completed flushing /raid0/cassandra/data/system/locationinfo-h-19-data.db (163 bytes)  info 09:25:40,722 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-19-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-15-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-18-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-16-data.db')]  info 09:25:40,726 node /10.40.190.143 state jump to normal  info 09:25:40,734 enqueuing flush of memtable-locationinfo@641287650(35/43 serialized/live bytes, 1 ops)  info 09:25:40,735 writing memtable-locationinfo@641287650(35/43 serialized/live bytes, 1 ops) java.lang.reflect.invocationtargetexception     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.commons.daemon.support.daemonloader.load(daemonloader.java:160) caused by: java.util.concurrentmodificationexception     at java.util.hashmap$hashiterator.nextentry(hashmap.java:793)     at java.util.hashmap$entryiterator.next(hashmap.java:834)     at java.util.hashmap$entryiterator.next(hashmap.java:832)     at com.google.common.collect.abstractbimap$entryset$1.next(abstractbimap.java:301)     at com.google.common.collect.abstractbimap$entryset$1.next(abstractbimap.java:293)     at org.apache.cassandra.service.storageservice.calculatependingranges(storageservice.java:1127)     at org.apache.cassandra.service.storageservice.calculatependingranges(storageservice.java:1084)     at org.apache.cassandra.service.storageservice.handlestatenormal(storageservice.java:920)     at org.apache.cassandra.service.storageservice.onchange(storageservice.java:805)     at org.apache.cassandra.gms.gossiper.donotifications(gossiper.java:880)     at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1027)     at org.apache.cassandra.service.storageservice.settoken(storageservice.java:226)     at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:573)     at org.apache.cassandra.service.storageservice.initserver(storageservice.java:460)     at org.apache.cassandra.service.storageservice.initserver(storageservice.java:381)     at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:215)     at org.apache.cassandra.service.abstractcassandradaemon.init(abstractcassandradaemon.java:238)     ... 5 more cannot load daemon service exit with a return value of 3  info 09:35:35,156 jvm vendor/version: java hotspot(tm) 64-bit server vm/1.6.0_26  info 09:35:35,159 heap size: 1936719872/1937768448  info 09:35:35,160 classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar  info 09:35:36,626 jna mlockall successful  info 09:35:36,636 loading settings from file:/etc/cassandra/cassandra.yaml  info 09:35:36,757 diskaccessmode 'auto' determined to be mmap, indexaccessmode is mmap  info 09:35:36,769 global memtable threshold is enabled at 616mb  info 09:35:36,811 ec2snitch using region: us-east, zone: 1d.  info 09:35:37,030 opening /raid0/cassandra/data/system/schema-h-1 (5970 bytes)  info 09:35:37,067 opening /raid0/cassandra/data/system/migrations-h-1 (12989 bytes)  info 09:35:37,075 opening /raid0/cassandra/data/system/locationinfo-h-19 (163 bytes)  info 09:35:37,075 opening /raid0/cassandra/data/system/locationinfo-h-18 (89 bytes)  info 09:35:37,083 opening /raid0/cassandra/data/system/locationinfo-h-15 (653 bytes)  info 09:35:37,085 opening /raid0/cassandra/data/system/locationinfo-h-16 (89 bytes)  info 09:35:37,131 loading schema version bb843dd0-0146-11e1-0000-b877c09da5ff  info 09:35:37,372 creating new commitlog segment /raid0/cassandra/commitlog/commitlog-1319794537372.log  info 09:35:37,384 replaying /raid0/cassandra/commitlog/commitlog-1319793880475.log  info 09:35:37,416 finished reading /raid0/cassandra/commitlog/commitlog-1319793880475.log  info 09:35:37,422 enqueuing flush of memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)  info 09:35:37,423 writing memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)  info 09:35:37,424 enqueuing flush of memtable-versions@817138449(83/103 serialized/live bytes, 3 ops)  info 09:35:37,472 completed flushing /raid0/cassandra/data/opscenter/events-h-1-data.db (230 bytes)  info 09:35:37,479 writing memtable-versions@817138449(83/103 serialized/live bytes, 3 ops)  info 09:35:37,497 completed flushing /raid0/cassandra/data/system/versions-h-1-data.db (247 bytes)  info 09:35:37,497 log replay complete, 4 replayed mutations  info 09:35:37,509 cassandra version: 1.0.0  info 09:35:37,510 thrift api version: 19.18.0  info 09:35:37,510 loading persisted ring state  info 09:35:37,528 starting up server gossip  info 09:35:37,530 enqueuing flush of memtable-locationinfo@1655441108(29/36 serialized/live bytes, 1 ops)  info 09:35:37,530 writing memtable-locationinfo@1655441108(29/36 serialized/live bytes, 1 ops)  info 09:35:37,554 completed flushing /raid0/cassandra/data/system/locationinfo-h-20-data.db (80 bytes)  info 09:35:37,555 ec2snitch adding applicationstate ec2region=us-east ec2zone=1d  info 09:35:37,562 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-16-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-18-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-19-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-20-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-15-data.db')]  info 09:35:37,566 starting messaging service on /10.40.190.143:7000  info 09:35:37,592 using saved token 19444706681196483626478548996101040654  info 09:35:37,593 enqueuing flush of memtable-locationinfo@995684858(53/66 serialized/live bytes, 2 ops)  info 09:35:37,593 writing memtable-locationinfo@995684858(53/66 serialized/live bytes, 2 ops)  info 09:35:37,616 completed flushing /raid0/cassandra/data/system/locationinfo-h-22-data.db (163 bytes)  info 09:35:37,620 node /10.40.190.143 state jump to normal  info 09:35:37,639 bootstrap/replace/move completed! now serving reads.  info 09:35:37,640 will not load mx4j, mx4j-tools.jar is not in the classpath  info 09:35:37,684 binding thrift service to /0.0.0.0:9160  info 09:35:37,687 compacted to [/raid0/cassandra/data/system/locationinfo-h-21-data.db,].  1,074 to 799 (~74% of original) bytes for 4 keys at 0.007620mb/s.  time: 100ms.  info 09:35:37,688 using tfastframedtransport with a max frame size of 15728640 bytes.  info 09:35:37,692 using synchronous/threadpool thrift server on /0.0.0.0 : 9160  info 09:35:37,695 listening for thrift clients...  info 09:35:37,706 node /10.118.230.219 is now part of the cluster  info 09:35:37,707 inetaddress /10.118.230.219 is now up  info 09:35:37,708 enqueuing flush of memtable-locationinfo@2035487037(35/43 serialized/live bytes, 1 ops)  info 09:35:37,709 writing memtable-locationinfo@2035487037(35/43 serialized/live bytes, 1 ops)  info 09:35:37,725 completed flushing /raid0/cassandra/data/system/locationinfo-h-24-data.db (89 bytes)  info 09:35:37,726 node /10.34.42.28 is now part of the cluster  info 09:35:37,727 inetaddress /10.34.42.28 is now up  info 09:35:37,729 enqueuing flush of memtable-locationinfo@321887181(35/43 serialized/live bytes, 1 ops)  info 09:35:37,729 writing memtable-locationinfo@321887181(35/43 serialized/live bytes, 1 ops)  info 09:35:37,747 completed flushing /raid0/cassandra/data/system/locationinfo-h-25-data.db (89 bytes)  info 09:35:37,748 node /10.77.63.49 has restarted, now up  info 09:35:37,748 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-24-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-22-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-25-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-21-data.db')]  info 09:35:37,748 inetaddress /10.77.63.49 is now up  info 09:35:37,749 node /10.77.63.49 state jump to normal  info 09:35:37,750 node /10.34.70.73 is now part of the cluster  info 09:35:37,750 inetaddress /10.34.70.73 is now up  info 09:35:37,752 enqueuing flush of memtable-locationinfo@1354749546(35/43 serialized/live bytes, 1 ops)  info 09:35:37,752 writing memtable-locationinfo@1354749546(35/43 serialized/live bytes, 1 ops)  info 09:35:37,789 compacted to [/raid0/cassandra/data/system/locationinfo-h-26-data.db,].  1,140 to 877 (~76% of original) bytes for 4 keys at 0.020399mb/s.  time: 41ms.  info 09:35:37,801 completed flushing /raid0/cassandra/data/system/locationinfo-h-27-data.db (89 bytes)  info 09:35:37,801 node /10.99.86.251 is now part of the cluster  info 09:35:37,802 inetaddress /10.99.86.251 is now up  info 09:35:37,803 enqueuing flush of memtable-locationinfo@793374785(35/43 serialized/live bytes, 1 ops)  info 09:35:37,804 writing memtable-locationinfo@793374785(35/43 serialized/live bytes, 1 ops)  info 09:35:37,825 completed flushing /raid0/cassandra/data/system/locationinfo-h-29-data.db (89 bytes)  info 09:35:37,826 node /10.202.67.43 has restarted, now up  info 09:35:37,827 inetaddress /10.202.67.43 is now up  info 09:35:37,827 node /10.202.67.43 state jump to normal  info 09:35:37,828 node /10.116.134.183 is now part of the cluster  info 09:35:37,828 inetaddress /10.116.134.183 is now up  info 09:35:37,829 enqueuing flush of memtable-locationinfo@1728699027(35/43 serialized/live bytes, 1 ops)  info 09:35:37,830 writing memtable-locationinfo@1728699027(35/43 serialized/live bytes, 1 ops)  info 09:35:37,850 completed flushing /raid0/cassandra/data/system/locationinfo-h-30-data.db (89 bytes)  info 09:35:37,852 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-30-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-27-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-26-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-29-data.db')]  info 09:35:37,853 node /10.118.94.62 is now part of the cluster  info 09:35:37,853 inetaddress /10.118.94.62 is now up  info 09:35:37,855 enqueuing flush of memtable-locationinfo@2001229122(35/43 serialized/live bytes, 1 ops)  info 09:35:37,855 writing memtable-locationinfo@2001229122(35/43 serialized/live bytes, 1 ops)  info 09:35:37,885 completed flushing /raid0/cassandra/data/system/locationinfo-h-31-data.db (89 bytes)  info 09:35:37,886 node /10.116.215.81 is now part of the cluster  info 09:35:37,887 inetaddress /10.116.215.81 is now up  info 09:35:37,888 enqueuing flush of memtable-locationinfo@1748800276(35/43 serialized/live bytes, 1 ops)  info 09:35:37,888 writing memtable-locationinfo@1748800276(35/43 serialized/live bytes, 1 ops)  info 09:35:37,909 completed flushing /raid0/cassandra/data/system/locationinfo-h-33-data.db (89 bytes)  info 09:35:37,910 node /10.80.110.28 has restarted, now up  info 09:35:37,911 inetaddress /10.80.110.28 is now up  info 09:35:37,911 node /10.80.110.28 state jump to normal  info 09:35:37,912 node /10.80.210.38 is now part of the cluster  info 09:35:37,912 inetaddress /10.80.210.38 is now up  info 09:35:37,914 enqueuing flush of memtable-locationinfo@1761382005(35/43 serialized/live bytes, 1 ops)  info 09:35:37,914 writing memtable-locationinfo@1761382005(35/43 serialized/live bytes, 1 ops)  info 09:35:37,925 compacted to [/raid0/cassandra/data/system/locationinfo-h-32-data.db,].  1,144 to 982 (~85% of original) bytes for 4 keys at 0.014190mb/s.  time: 66ms.  info 09:35:37,927 completed flushing /raid0/cassandra/data/system/locationinfo-h-35-data.db (89 bytes)  info 09:35:37,928 node /10.40.177.173 has restarted, now up  info 09:35:37,929 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-31-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-32-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-33-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-35-data.db')]  info 09:35:37,929 inetaddress /10.40.177.173 is now up  info 09:35:37,929 node /10.40.177.173 state jump to normal  info 09:35:37,930 node /10.101.41.8 has restarted, now up  info 09:35:37,931 inetaddress /10.101.41.8 is now up  info 09:35:37,931 node /10.101.41.8 state jump to normal  info 09:35:37,931 node /10.205.23.34 has restarted, now up  info 09:35:37,932 inetaddress /10.205.23.34 is now up  info 09:35:37,932 node /10.205.23.34 state jump to normal  info 09:35:37,933 node /10.118.185.68 is now part of the cluster  info 09:35:37,933 inetaddress /10.118.185.68 is now up  info 09:35:37,934 enqueuing flush of memtable-locationinfo@260440278(35/43 serialized/live bytes, 1 ops)  info 09:35:37,935 writing memtable-locationinfo@260440278(35/43 serialized/live bytes, 1 ops)  info 09:35:37,970 completed flushing /raid0/cassandra/data/system/locationinfo-h-36-data.db (89 bytes)  info 09:35:37,971 node /10.116.241.250 is now part of the cluster  info 09:35:37,972 inetaddress /10.116.241.250 is now up  info 09:35:37,973 enqueuing flush of memtable-locationinfo@768673839(35/43 serialized/live bytes, 1 ops)  info 09:35:37,974 writing memtable-locationinfo@768673839(35/43 serialized/live bytes, 1 ops)  info 09:35:38,003 completed flushing /raid0/cassandra/data/system/locationinfo-h-38-data.db (89 bytes)  info 09:35:38,004 node /10.113.42.21 is now part of the cluster  info 09:35:38,005 inetaddress /10.113.42.21 is now up  info 09:35:38,007 enqueuing flush of memtable-locationinfo@1610335061(35/43 serialized/live bytes, 1 ops)  info 09:35:38,008 writing memtable-locationinfo@1610335061(35/43 serialized/live bytes, 1 ops)  info 09:35:38,014 compacted to [/raid0/cassandra/data/system/locationinfo-h-37-data.db,].  1,249 to 1,087 (~87% of original) bytes for 4 keys at 0.012196mb/s.  time: 85ms.  info 09:35:38,024 completed flushing /raid0/cassandra/data/system/locationinfo-h-40-data.db (89 bytes)  info 09:35:38,024 node /10.194.29.156 is now part of the cluster  info 09:35:38,025 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-37-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-40-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-36-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-38-data.db')]  info 09:35:38,025 inetaddress /10.194.29.156 is now up  info 09:35:38,026 enqueuing flush of memtable-locationinfo@1625488363(35/43 serialized/live bytes, 1 ops)  info 09:35:38,027 writing memtable-locationinfo@1625488363(35/43 serialized/live bytes, 1 ops)  info 09:35:38,042 completed flushing /raid0/cassandra/data/system/locationinfo-h-41-data.db (89 bytes)  info 09:35:38,043 node /10.85.11.38 has restarted, now up  info 09:35:38,044 inetaddress /10.85.11.38 is now up  info 09:35:38,044 node /10.85.11.38 state jump to normal  info 09:35:38,045 node /10.34.159.72 is now part of the cluster  info 09:35:38,046 inetaddress /10.34.159.72 is now up  info 09:35:38,047 enqueuing flush of memtable-locationinfo@747881713(35/43 serialized/live bytes, 1 ops)  info 09:35:38,048 writing memtable-locationinfo@747881713(35/43 serialized/live bytes, 1 ops)  info 09:35:38,065 completed flushing /raid0/cassandra/data/system/locationinfo-h-42-data.db (89 bytes)  info 09:35:38,067 node /10.194.22.191 is now part of the cluster  info 09:35:38,067 inetaddress /10.194.22.191 is now up  info 09:35:38,069 enqueuing flush of memtable-locationinfo@709926392(35/43 serialized/live bytes, 1 ops)  info 09:35:38,069 writing memtable-locationinfo@709926392(35/43 serialized/live bytes, 1 ops)  info 09:35:38,092 completed flushing /raid0/cassandra/data/system/locationinfo-h-44-data.db (89 bytes)  info 09:35:38,093 node /10.34.74.58 is now part of the cluster  info 09:35:38,097 inetaddress /10.34.74.58 is now up  info 09:35:38,098 enqueuing flush of memtable-locationinfo@1356841826(35/43 serialized/live bytes, 1 ops)  info 09:35:38,099 writing memtable-locationinfo@1356841826(35/43 serialized/live bytes, 1 ops)  info 09:35:38,105 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-43-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-41-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-44-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-42-data.db')]  info 09:35:38,106 compacted to [/raid0/cassandra/data/system/locationinfo-h-43-data.db,].  1,354 to 1,192 (~88% of original) bytes for 4 keys at 0.014034mb/s.  time: 81ms.  info 09:35:38,144 completed flushing /raid0/cassandra/data/system/locationinfo-h-46-data.db (89 bytes)  info 09:35:38,145 node /10.40.22.224 is now part of the cluster  info 09:35:38,146 inetaddress /10.40.22.224 is now up  info 09:35:38,147 enqueuing flush of memtable-locationinfo@422797318(35/43 serialized/live bytes, 1 ops)  info 09:35:38,148 writing memtable-locationinfo@422797318(35/43 serialized/live bytes, 1 ops)  info 09:35:38,155 compacted to [/raid0/cassandra/data/system/locationinfo-h-47-data.db,].  1,459 to 1,297 (~88% of original) bytes for 4 keys at 0.024738mb/s.  time: 50ms.  info 09:35:38,164 completed flushing /raid0/cassandra/data/system/locationinfo-h-49-data.db (89 bytes)  info 09:35:38,165 node /10.32.79.134 is now part of the cluster  info 09:35:38,166 inetaddress /10.32.79.134 is now up  info 09:35:38,167 enqueuing flush of memtable-locationinfo@1455093129(35/43 serialized/live bytes, 1 ops)  info 09:35:38,168 writing memtable-locationinfo@1455093129(35/43 serialized/live bytes, 1 ops)  info 09:35:38,199 completed flushing /raid0/cassandra/data/system/locationinfo-h-50-data.db (89 bytes)  info 09:35:38,200 node /10.118.179.67 is now part of the cluster  info 09:35:38,200 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-50-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-47-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-49-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-46-data.db')]  info 09:35:38,200 inetaddress /10.118.179.67 is now up  info 09:35:38,202 enqueuing flush of memtable-locationinfo@1105436908(35/43 serialized/live bytes, 1 ops)  info 09:35:38,202 writing memtable-locationinfo@1105436908(35/43 serialized/live bytes, 1 ops)  info 09:35:38,248 completed flushing /raid0/cassandra/data/system/locationinfo-h-51-data.db (89 bytes)  info 09:35:38,249 node /10.84.205.93 is now part of the cluster  info 09:35:38,249 inetaddress /10.84.205.93 is now up  info 09:35:38,251 enqueuing flush of memtable-locationinfo@1306980591(35/43 serialized/live bytes, 1 ops)  info 09:35:38,251 writing memtable-locationinfo@1306980591(35/43 serialized/live bytes, 1 ops)  info 09:35:38,262 compacted to [/raid0/cassandra/data/system/locationinfo-h-52-data.db,].  1,564 to 1,402 (~89% of original) bytes for 4 keys at 0.021919mb/s.  time: 61ms.  info 09:35:38,294 completed flushing /raid0/cassandra/data/system/locationinfo-h-54-data.db (89 bytes)  info 09:35:38,294 node /10.34.33.16 has restarted, now up  info 09:35:38,295 inetaddress /10.34.33.16 is now up  info 09:35:38,296 node /10.34.33.16 state jump to normal  info 09:35:38,296 node /10.39.107.114 is now part of the cluster  info 09:35:38,297 inetaddress /10.39.107.114 is now up  info 09:35:38,298 enqueuing flush of memtable-locationinfo@1038389338(35/43 serialized/live bytes, 1 ops)  info 09:35:38,299 writing memtable-locationinfo@1038389338(35/43 serialized/live bytes, 1 ops)  info 09:35:38,311 completed flushing /raid0/cassandra/data/system/locationinfo-h-55-data.db (89 bytes)  info 09:35:38,312 node /10.196.79.240 is now part of the cluster  info 09:35:38,312 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-52-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-55-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-54-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-51-data.db')]  info 09:35:38,313 inetaddress /10.196.79.240 is now up  info 09:35:38,314 enqueuing flush of memtable-locationinfo@1850278722(35/43 serialized/live bytes, 1 ops)  info 09:35:38,315 writing memtable-locationinfo@1850278722(35/43 serialized/live bytes, 1 ops)  info 09:35:38,354 completed flushing /raid0/cassandra/data/system/locationinfo-h-56-data.db (89 bytes)  info 09:35:38,355 node /10.99.39.242 has restarted, now up  info 09:35:38,356 inetaddress /10.99.39.242 is now up  info 09:35:38,356 node /10.99.39.242 state jump to normal  info 09:35:38,357 node /10.118.233.198 has restarted, now up  info 09:35:38,358 inetaddress /10.118.233.198 is now up  info 09:35:38,358 node /10.118.233.198 state jump to normal  info 09:35:38,359 node /10.82.210.172 is now part of the cluster  info 09:35:38,359 inetaddress /10.82.210.172 is now up  info 09:35:38,364 enqueuing flush of memtable-locationinfo@786665924(35/43 serialized/live bytes, 1 ops)  info 09:35:38,364 writing memtable-locationinfo@786665924(35/43 serialized/live bytes, 1 ops)  info 09:35:38,439 completed flushing /raid0/cassandra/data/system/locationinfo-h-58-data.db (89 bytes)  info 09:35:38,440 node /10.80.41.192 is now part of the cluster  info 09:35:38,440 inetaddress /10.80.41.192 is now up  info 09:35:38,442 enqueuing flush of memtable-locationinfo@1647844754(35/43 serialized/live bytes, 1 ops)  info 09:35:38,442 writing memtable-locationinfo@1647844754(35/43 serialized/live bytes, 1 ops)  info 09:35:38,451 compacted to [/raid0/cassandra/data/system/locationinfo-h-57-data.db,].  1,669 to 1,515 (~90% of original) bytes for 4 keys at 0.010470mb/s.  time: 138ms.  info 09:35:38,459 completed flushing /raid0/cassandra/data/system/locationinfo-h-60-data.db (89 bytes)  info 09:35:38,459 node /10.76.243.129 is now part of the cluster  info 09:35:38,460 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-56-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-58-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-57-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-60-data.db')]  info 09:35:38,460 inetaddress /10.76.243.129 is now up  info 09:35:38,462 enqueuing flush of memtable-locationinfo@585652261(35/43 serialized/live bytes, 1 ops)  info 09:35:38,462 writing memtable-locationinfo@585652261(35/43 serialized/live bytes, 1 ops)  info 09:35:38,478 completed flushing /raid0/cassandra/data/system/locationinfo-h-61-data.db (89 bytes)  info 09:35:38,486 node /10.34.42.28 state jump to normal  info 09:35:38,487 node /10.34.70.73 state jump to normal  info 09:35:38,488 node /10.99.86.251 state jump to normal  info 09:35:38,489 node /10.118.94.62 state jump to normal  info 09:35:38,489 node /10.80.110.28 state jump to normal  info 09:35:38,490 node /10.80.210.38 state jump to normal  info 09:35:38,491 node /10.40.177.173 state jump to normal  info 09:35:38,493 node /10.101.41.8 state jump to normal  info 09:35:38,493 node /10.113.42.21 state jump to normal  info 09:35:38,494 node /10.85.11.38 state jump to normal  info 09:35:38,495 node /10.34.159.72 state jump to normal  info 09:35:38,496 node /10.34.74.58 state jump to normal  info 09:35:38,497 node /10.84.205.93 state jump to normal  info 09:35:38,497 node /10.118.179.67 state jump to normal  info 09:35:38,498 node /10.34.33.16 state jump to normal  info 09:35:38,499 node /10.196.79.240 state jump to normal  info 09:35:38,500 node /10.118.233.198 state jump to normal  info 09:35:38,501 node /10.80.41.192 state jump to normal  info 09:35:38,502 node /10.76.243.129 state jump to normal  info 09:35:38,508 node /10.118.185.68 state jump to normal  info 09:35:38,524 node /10.118.230.219 state jump to normal  info 09:35:38,536 compacted to [/raid0/cassandra/data/system/locationinfo-h-62-data.db,].  1,782 to 1,620 (~90% of original) bytes for 4 keys at 0.020328mb/s.  time: 76ms.  info 09:35:38,537 node /10.80.110.28 state jump to normal  info 09:35:38,537 node /10.40.177.173 state jump to normal  info 09:35:38,538 node /10.101.41.8 state jump to normal  info 09:35:38,539 node /10.116.241.250 state jump to normal  info 09:35:38,540 node /10.194.29.156 state jump to normal  info 09:35:38,540 node /10.34.74.58 state jump to normal  info 09:35:38,541 node /10.40.22.224 state jump to normal  info 09:35:38,542 node /10.32.79.134 state jump to normal  info 09:35:38,543 node /10.39.107.114 state jump to normal  info 09:35:38,543 node /10.99.39.242 state jump to normal  info 09:35:38,550 node /10.77.63.49 state jump to normal  info 09:35:38,550 node /10.34.42.28 state jump to normal  info 09:35:38,551 node /10.116.134.183 state jump to normal  info 09:35:38,553 node /10.76.243.129 state jump to normal  info 09:35:38,557 node /10.202.67.43 state jump to normal  info 09:35:38,558 node /10.118.94.62 state jump to normal  info 09:35:38,562 node /10.116.215.81 state jump to normal  info 09:35:38,563 node /10.80.210.38 state jump to normal  info 09:35:38,564 node /10.205.23.34 state jump to normal  info 09:35:38,565 node /10.39.107.114 state jump to normal<stacktrace> ubuntu@ip-10-40-190-143:~$ sudo cat /var/log/cassandra/output.log   info 09:24:38,453 jvm vendor/version: java hotspot(tm) 64-bit server vm/1.6.0_26  info 09:24:38,456 heap size: 1936719872/1937768448  info 09:24:38,457 classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar  info 09:24:39,891 jna mlockall successful  info 09:24:39,901 loading settings from file:/etc/cassandra/cassandra.yaml  info 09:24:40,057 diskaccessmode 'auto' determined to be mmap, indexaccessmode is mmap  info 09:24:40,069 global memtable threshold is enabled at 616mb  info 09:24:40,159 ec2snitch using region: us-east, zone: 1d.  info 09:24:40,475 creating new commitlog segment /raid0/cassandra/commitlog/commitlog-1319793880475.log  info 09:24:40,486 couldn't detect any schema definitions in local storage.  info 09:24:40,486 found table data in data directories. consider using the cli to define your schema.  info 09:24:40,497 no commitlog files found; skipping replay  info 09:24:40,501 cassandra version: 1.0.0  info 09:24:40,502 thrift api version: 19.18.0  info 09:24:40,502 loading persisted ring state  info 09:24:40,506 starting up server gossip  info 09:24:40,529 enqueuing flush of memtable-locationinfo@1388314661(190/237 serialized/live bytes, 4 ops)  info 09:24:40,530 writing memtable-locationinfo@1388314661(190/237 serialized/live bytes, 4 ops)  info 09:24:40,600 completed flushing /raid0/cassandra/data/system/locationinfo-h-1-data.db (298 bytes)  info 09:24:40,613 ec2snitch adding applicationstate ec2region=us-east ec2zone=1d  info 09:24:40,621 starting messaging service on /10.40.190.143:7000  info 09:24:40,628 joining: waiting for ring and schema information  info 09:24:43,389 inetaddress /10.194.29.156 is now dead.  info 09:24:43,391 inetaddress /10.85.11.38 is now dead.  info 09:24:43,392 inetaddress /10.34.42.28 is now dead.  info 09:24:43,393 inetaddress /10.77.63.49 is now dead.  info 09:24:43,394 inetaddress /10.194.22.191 is now dead.  info 09:24:43,395 inetaddress /10.34.74.58 is now dead.  info 09:24:43,395 node /10.34.33.16 is now part of the cluster  info 09:24:43,396 inetaddress /10.34.33.16 is now up  info 09:24:43,397 enqueuing flush of memtable-locationinfo@1629818866(20/25 serialized/live bytes, 1 ops)  info 09:24:43,398 writing memtable-locationinfo@1629818866(20/25 serialized/live bytes, 1 ops)  info 09:24:43,417 completed flushing /raid0/cassandra/data/system/locationinfo-h-2-data.db (74 bytes)  info 09:24:43,418 inetaddress /10.202.67.43 is now dead.  info 09:24:43,419 inetaddress /10.116.215.81 is now dead.  info 09:24:43,420 inetaddress /10.99.39.242 is now dead.  info 09:24:43,421 inetaddress /10.80.110.28 is now dead.  info 09:24:43,422 inetaddress /10.118.233.198 is now dead.  info 09:24:43,423 inetaddress /10.40.177.173 is now dead.  info 09:24:43,424 inetaddress /10.205.23.34 is now dead.  info 09:24:43,425 inetaddress /10.101.41.8 is now dead.  info 09:24:43,669 inetaddress /10.118.230.219 is now dead.  info 09:24:43,670 inetaddress /10.80.41.192 is now dead.  info 09:24:43,671 inetaddress /10.40.22.224 is now dead.  info 09:24:43,672 inetaddress /10.39.107.114 is now dead.  info 09:24:46,164 inetaddress /10.118.185.68 is now dead.  info 09:24:46,166 inetaddress /10.84.205.93 is now dead.  info 09:24:46,167 inetaddress /10.116.134.183 is now dead.  info 09:24:46,670 inetaddress /10.118.179.67 is now dead.  info 09:24:46,671 inetaddress /10.116.241.250 is now dead.  info 09:24:48,441 inetaddress /10.118.94.62 is now dead.  info 09:24:48,442 inetaddress /10.99.86.251 is now dead.  info 09:24:50,176 inetaddress /10.113.42.21 is now dead.  info 09:24:50,177 inetaddress /10.34.159.72 is now dead.  info 09:24:50,178 inetaddress /10.32.79.134 is now dead.  info 09:24:50,179 inetaddress /10.80.210.38 is now dead.  info 09:24:50,180 inetaddress /10.34.70.73 is now dead.  info 09:24:50,181 inetaddress /10.196.79.240 is now dead.  info 09:25:01,713 inetaddress /10.82.210.172 is now dead.  info 09:25:06,202 inetaddress /10.80.110.28 is now up  info 09:25:06,908 inetaddress /10.99.39.242 is now up  info 09:25:07,696 inetaddress /10.118.233.198 is now up  info 09:25:07,697 inetaddress /10.205.23.34 is now up  info 09:25:08,704 inetaddress /10.194.22.191 is now up  info 09:25:08,705 inetaddress /10.40.177.173 is now up  info 09:25:08,706 inetaddress /10.101.41.8 is now up  info 09:25:09,489 inetaddress /10.202.67.43 is now up  info 09:25:09,698 inetaddress /10.77.63.49 is now up  info 09:25:10,628 joining: getting bootstrap token  info 09:25:10,631 enqueuing flush of memtable-locationinfo@1733057335(36/45 serialized/live bytes, 1 ops)  info 09:25:10,631 writing memtable-locationinfo@1733057335(36/45 serialized/live bytes, 1 ops)  info 09:25:10,647 completed flushing /raid0/cassandra/data/system/locationinfo-h-3-data.db (87 bytes)  info 09:25:10,649 joining: sleeping 30000 ms for pending range setup  info 09:25:10,689 inetaddress /10.85.11.38 is now up  info 09:25:10,708 inetaddress /10.34.74.58 is now up  info 09:25:10,912 inetaddress /10.194.29.156 is now up  info 09:25:11,261 applying migration bb843dd0-0146-11e1-0000-b877c09da5ff add keyspace: opscenter, rep strategy:simplestrategy{org.apache.cassandra.config.cfmetadata@55e29b99[cfid=1000,ksname=opscenter,cfname=pdps,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=300.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=32,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@105585dc,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@5ec736e4[cfid=1004,ksname=opscenter,cfname=rollups86400,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=2,maxcompactionthreshold=8,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@68e4e358,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@b09dc35[cfid=1003,ksname=opscenter,cfname=rollups7200,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=2,maxcompactionthreshold=8,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@3458213c,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@5ee04fd[cfid=1002,ksname=opscenter,cfname=rollups300,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=16,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@4d898115,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@7e79b177[cfid=1005,ksname=opscenter,cfname=events,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=opscenter raw event storage,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=8,maxcompactionthreshold=12,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@67723c7f,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@540523be[cfid=1006,ksname=opscenter,cfname=events_timeline,cftype=standard,comparator=org.apache.cassandra.db.marshal.longtype,subcolumncomparator=<null>,comment=opscenter event timelines,rowcachesize=0.0,keycachesize=5.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=8,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=0,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@1d6dba0a,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@ed0f59e[cfid=1007,ksname=opscenter,cfname=settings,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=opscenter settings,rowcachesize=0.0,keycachesize=50.0,readrepairchance=1.0,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=8,maxcompactionthreshold=12,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@38ad5fab,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}], org.apache.cassandra.config.cfmetadata@7e63f09e[cfid=1001,ksname=opscenter,cfname=rollups60,cftype=standard,comparator=org.apache.cassandra.db.marshal.bytestype,subcolumncomparator=<null>,comment=,rowcachesize=0.0,keycachesize=50.0,readrepairchance=0.25,replicateonwrite=true,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=32,rowcachesaveperiodinseconds=0,keycachesaveperiodinseconds=43200,rowcachekeystosave=2147483647,rowcacheprovider=org.apache.cassandra.cache.concurrentlinkedhashcacheprovider@534a55e5,mergeshardschance=0.1,keyalias=<null>,column_metadata={},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionoptions={}]}, durable_writes: true  info 09:25:11,273 enqueuing flush of memtable-migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)  info 09:25:11,273 writing memtable-migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)  info 09:25:11,274 enqueuing flush of memtable-schema@1616586953(5820/7275 serialized/live bytes, 3 ops)  info 09:25:11,358 completed flushing /raid0/cassandra/data/system/migrations-h-1-data.db (12989 bytes)  info 09:25:11,358 writing memtable-schema@1616586953(5820/7275 serialized/live bytes, 3 ops)  info 09:25:11,390 completed flushing /raid0/cassandra/data/system/schema-h-1-data.db (5970 bytes)  info 09:25:11,727 inetaddress /10.116.215.81 is now up  info 09:25:11,744 inetaddress /10.34.42.28 is now up  info 09:25:11,750 inetaddress /10.40.22.224 is now up  info 09:25:12,023 inetaddress /10.80.41.192 is now up  info 09:25:12,712 inetaddress /10.39.107.114 is now up  info 09:25:12,717 inetaddress /10.118.185.68 is now up  info 09:25:12,721 inetaddress /10.116.134.183 is now up  info 09:25:13,322 inetaddress /10.118.230.219 is now up  info 09:25:13,632 inetaddress /10.84.205.93 is now up  info 09:25:14,713 inetaddress /10.118.179.67 is now up  info 09:25:14,717 inetaddress /10.116.241.250 is now up  info 09:25:17,468 inetaddress /10.34.159.72 is now up  info 09:25:17,476 inetaddress /10.118.94.62 is now up  info 09:25:17,480 inetaddress /10.80.210.38 is now up  info 09:25:17,716 inetaddress /10.32.79.134 is now up  info 09:25:17,721 inetaddress /10.99.86.251 is now up  info 09:25:18,717 inetaddress /10.196.79.240 is now up  info 09:25:18,727 inetaddress /10.34.70.73 is now up  info 09:25:19,596 inetaddress /10.113.42.21 is now up  info 09:25:25,750 inetaddress /10.82.210.172 is now up  info 09:25:37,743 enqueuing flush of memtable-locationinfo@288976631(35/43 serialized/live bytes, 1 ops)  info 09:25:37,744 writing memtable-locationinfo@288976631(35/43 serialized/live bytes, 1 ops)  info 09:25:37,764 completed flushing /raid0/cassandra/data/system/locationinfo-h-4-data.db (89 bytes)  info 09:25:37,773 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-1-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-3-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-4-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-2-data.db')]  info 09:25:37,776 enqueuing flush of memtable-locationinfo@1950702248(35/43 serialized/live bytes, 1 ops)  info 09:25:37,777 writing memtable-locationinfo@1950702248(35/43 serialized/live bytes, 1 ops)  info 09:25:37,821 completed flushing /raid0/cassandra/data/system/locationinfo-h-5-data.db (89 bytes)  info 09:25:37,869 compacted to [/raid0/cassandra/data/system/locationinfo-h-6-data.db,].  548 to 443 (~80% of original) bytes for 3 keys at 0.006500mb/s.  time: 65ms.  info 09:25:38,740 enqueuing flush of memtable-locationinfo@92917455(35/43 serialized/live bytes, 1 ops)  info 09:25:38,740 writing memtable-locationinfo@92917455(35/43 serialized/live bytes, 1 ops)  info 09:25:38,757 completed flushing /raid0/cassandra/data/system/locationinfo-h-8-data.db (89 bytes)  info 09:25:38,766 enqueuing flush of memtable-locationinfo@1096488363(35/43 serialized/live bytes, 1 ops)  info 09:25:38,767 writing memtable-locationinfo@1096488363(35/43 serialized/live bytes, 1 ops)  info 09:25:38,814 completed flushing /raid0/cassandra/data/system/locationinfo-h-9-data.db (89 bytes)  info 09:25:38,816 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-6-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-9-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-8-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-5-data.db')]  info 09:25:38,823 enqueuing flush of memtable-locationinfo@1734564525(35/43 serialized/live bytes, 1 ops)  info 09:25:38,823 writing memtable-locationinfo@1734564525(35/43 serialized/live bytes, 1 ops)  info 09:25:38,893 completed flushing /raid0/cassandra/data/system/locationinfo-h-10-data.db (89 bytes)  info 09:25:38,916 compacted to [/raid0/cassandra/data/system/locationinfo-h-11-data.db,].  710 to 548 (~77% of original) bytes for 3 keys at 0.005226mb/s.  time: 100ms.  info 09:25:39,538 enqueuing flush of memtable-locationinfo@811507066(35/43 serialized/live bytes, 1 ops)  info 09:25:39,539 writing memtable-locationinfo@811507066(35/43 serialized/live bytes, 1 ops)  info 09:25:39,555 completed flushing /raid0/cassandra/data/system/locationinfo-h-13-data.db (89 bytes)  info 09:25:39,578 enqueuing flush of memtable-locationinfo@1125690366(35/43 serialized/live bytes, 1 ops)  info 09:25:39,578 writing memtable-locationinfo@1125690366(35/43 serialized/live bytes, 1 ops)  info 09:25:39,594 completed flushing /raid0/cassandra/data/system/locationinfo-h-14-data.db (89 bytes)  info 09:25:39,596 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-11-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-10-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-14-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-13-data.db')]  info 09:25:39,613 enqueuing flush of memtable-locationinfo@1870148830(35/43 serialized/live bytes, 1 ops)  info 09:25:39,614 writing memtable-locationinfo@1870148830(35/43 serialized/live bytes, 1 ops)  info 09:25:39,652 completed flushing /raid0/cassandra/data/system/locationinfo-h-16-data.db (89 bytes)  info 09:25:39,692 compacted to [/raid0/cassandra/data/system/locationinfo-h-15-data.db,].  815 to 653 (~80% of original) bytes for 3 keys at 0.006487mb/s.  time: 96ms.  info 09:25:39,731 enqueuing flush of memtable-locationinfo@1279866611(35/43 serialized/live bytes, 1 ops)  info 09:25:39,731 writing memtable-locationinfo@1279866611(35/43 serialized/live bytes, 1 ops)  info 09:25:39,747 completed flushing /raid0/cassandra/data/system/locationinfo-h-18-data.db (89 bytes)  info 09:25:40,649 starting to bootstrap...  info 09:25:40,701 finished streaming session 304272969286 from /10.205.23.34  info 09:25:40,703 enqueuing flush of memtable-locationinfo@1868577756(53/66 serialized/live bytes, 2 ops)  info 09:25:40,703 writing memtable-locationinfo@1868577756(53/66 serialized/live bytes, 2 ops)  info 09:25:40,721 completed flushing /raid0/cassandra/data/system/locationinfo-h-19-data.db (163 bytes)  info 09:25:40,722 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-19-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-15-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-18-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-16-data.db')]  info 09:25:40,726 node /10.40.190.143 state jump to normal  info 09:25:40,734 enqueuing flush of memtable-locationinfo@641287650(35/43 serialized/live bytes, 1 ops)  info 09:25:40,735 writing memtable-locationinfo@641287650(35/43 serialized/live bytes, 1 ops) java.lang.reflect.invocationtargetexception     at sun.reflect.nativemethodaccessorimpl.invoke0(native method)     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)     at java.lang.reflect.method.invoke(method.java:597)     at org.apache.commons.daemon.support.daemonloader.load(daemonloader.java:160) caused by: java.util.concurrentmodificationexception     at java.util.hashmap$hashiterator.nextentry(hashmap.java:793)     at java.util.hashmap$entryiterator.next(hashmap.java:834)     at java.util.hashmap$entryiterator.next(hashmap.java:832)     at com.google.common.collect.abstractbimap$entryset$1.next(abstractbimap.java:301)     at com.google.common.collect.abstractbimap$entryset$1.next(abstractbimap.java:293)     at org.apache.cassandra.service.storageservice.calculatependingranges(storageservice.java:1127)     at org.apache.cassandra.service.storageservice.calculatependingranges(storageservice.java:1084)     at org.apache.cassandra.service.storageservice.handlestatenormal(storageservice.java:920)     at org.apache.cassandra.service.storageservice.onchange(storageservice.java:805)     at org.apache.cassandra.gms.gossiper.donotifications(gossiper.java:880)     at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1027)     at org.apache.cassandra.service.storageservice.settoken(storageservice.java:226)     at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:573)     at org.apache.cassandra.service.storageservice.initserver(storageservice.java:460)     at org.apache.cassandra.service.storageservice.initserver(storageservice.java:381)     at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:215)     at org.apache.cassandra.service.abstractcassandradaemon.init(abstractcassandradaemon.java:238)     ... 5 more cannot load daemon service exit with a return value of 3  info 09:35:35,156 jvm vendor/version: java hotspot(tm) 64-bit server vm/1.6.0_26  info 09:35:35,159 heap size: 1936719872/1937768448  info 09:35:35,160 classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar  info 09:35:36,626 jna mlockall successful  info 09:35:36,636 loading settings from file:/etc/cassandra/cassandra.yaml  info 09:35:36,757 diskaccessmode 'auto' determined to be mmap, indexaccessmode is mmap  info 09:35:36,769 global memtable threshold is enabled at 616mb  info 09:35:36,811 ec2snitch using region: us-east, zone: 1d.  info 09:35:37,030 opening /raid0/cassandra/data/system/schema-h-1 (5970 bytes)  info 09:35:37,067 opening /raid0/cassandra/data/system/migrations-h-1 (12989 bytes)  info 09:35:37,075 opening /raid0/cassandra/data/system/locationinfo-h-19 (163 bytes)  info 09:35:37,075 opening /raid0/cassandra/data/system/locationinfo-h-18 (89 bytes)  info 09:35:37,083 opening /raid0/cassandra/data/system/locationinfo-h-15 (653 bytes)  info 09:35:37,085 opening /raid0/cassandra/data/system/locationinfo-h-16 (89 bytes)  info 09:35:37,131 loading schema version bb843dd0-0146-11e1-0000-b877c09da5ff  info 09:35:37,372 creating new commitlog segment /raid0/cassandra/commitlog/commitlog-1319794537372.log  info 09:35:37,384 replaying /raid0/cassandra/commitlog/commitlog-1319793880475.log  info 09:35:37,416 finished reading /raid0/cassandra/commitlog/commitlog-1319793880475.log  info 09:35:37,422 enqueuing flush of memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)  info 09:35:37,423 writing memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)  info 09:35:37,424 enqueuing flush of memtable-versions@817138449(83/103 serialized/live bytes, 3 ops)  info 09:35:37,472 completed flushing /raid0/cassandra/data/opscenter/events-h-1-data.db (230 bytes)  info 09:35:37,479 writing memtable-versions@817138449(83/103 serialized/live bytes, 3 ops)  info 09:35:37,497 completed flushing /raid0/cassandra/data/system/versions-h-1-data.db (247 bytes)  info 09:35:37,497 log replay complete, 4 replayed mutations  info 09:35:37,509 cassandra version: 1.0.0  info 09:35:37,510 thrift api version: 19.18.0  info 09:35:37,510 loading persisted ring state  info 09:35:37,528 starting up server gossip  info 09:35:37,530 enqueuing flush of memtable-locationinfo@1655441108(29/36 serialized/live bytes, 1 ops)  info 09:35:37,530 writing memtable-locationinfo@1655441108(29/36 serialized/live bytes, 1 ops)  info 09:35:37,554 completed flushing /raid0/cassandra/data/system/locationinfo-h-20-data.db (80 bytes)  info 09:35:37,555 ec2snitch adding applicationstate ec2region=us-east ec2zone=1d  info 09:35:37,562 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-16-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-18-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-19-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-20-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-15-data.db')]  info 09:35:37,566 starting messaging service on /10.40.190.143:7000  info 09:35:37,592 using saved token 19444706681196483626478548996101040654  info 09:35:37,593 enqueuing flush of memtable-locationinfo@995684858(53/66 serialized/live bytes, 2 ops)  info 09:35:37,593 writing memtable-locationinfo@995684858(53/66 serialized/live bytes, 2 ops)  info 09:35:37,616 completed flushing /raid0/cassandra/data/system/locationinfo-h-22-data.db (163 bytes)  info 09:35:37,620 node /10.40.190.143 state jump to normal  info 09:35:37,639 bootstrap/replace/move completed! now serving reads.  info 09:35:37,640 will not load mx4j, mx4j-tools.jar is not in the classpath  info 09:35:37,684 binding thrift service to /0.0.0.0:9160  info 09:35:37,687 compacted to [/raid0/cassandra/data/system/locationinfo-h-21-data.db,].  1,074 to 799 (~74% of original) bytes for 4 keys at 0.007620mb/s.  time: 100ms.  info 09:35:37,688 using tfastframedtransport with a max frame size of 15728640 bytes.  info 09:35:37,692 using synchronous/threadpool thrift server on /0.0.0.0 : 9160  info 09:35:37,695 listening for thrift clients...  info 09:35:37,706 node /10.118.230.219 is now part of the cluster  info 09:35:37,707 inetaddress /10.118.230.219 is now up  info 09:35:37,708 enqueuing flush of memtable-locationinfo@2035487037(35/43 serialized/live bytes, 1 ops)  info 09:35:37,709 writing memtable-locationinfo@2035487037(35/43 serialized/live bytes, 1 ops)  info 09:35:37,725 completed flushing /raid0/cassandra/data/system/locationinfo-h-24-data.db (89 bytes)  info 09:35:37,726 node /10.34.42.28 is now part of the cluster  info 09:35:37,727 inetaddress /10.34.42.28 is now up  info 09:35:37,729 enqueuing flush of memtable-locationinfo@321887181(35/43 serialized/live bytes, 1 ops)  info 09:35:37,729 writing memtable-locationinfo@321887181(35/43 serialized/live bytes, 1 ops)  info 09:35:37,747 completed flushing /raid0/cassandra/data/system/locationinfo-h-25-data.db (89 bytes)  info 09:35:37,748 node /10.77.63.49 has restarted, now up  info 09:35:37,748 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-24-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-22-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-25-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-21-data.db')]  info 09:35:37,748 inetaddress /10.77.63.49 is now up  info 09:35:37,749 node /10.77.63.49 state jump to normal  info 09:35:37,750 node /10.34.70.73 is now part of the cluster  info 09:35:37,750 inetaddress /10.34.70.73 is now up  info 09:35:37,752 enqueuing flush of memtable-locationinfo@1354749546(35/43 serialized/live bytes, 1 ops)  info 09:35:37,752 writing memtable-locationinfo@1354749546(35/43 serialized/live bytes, 1 ops)  info 09:35:37,789 compacted to [/raid0/cassandra/data/system/locationinfo-h-26-data.db,].  1,140 to 877 (~76% of original) bytes for 4 keys at 0.020399mb/s.  time: 41ms.  info 09:35:37,801 completed flushing /raid0/cassandra/data/system/locationinfo-h-27-data.db (89 bytes)  info 09:35:37,801 node /10.99.86.251 is now part of the cluster  info 09:35:37,802 inetaddress /10.99.86.251 is now up  info 09:35:37,803 enqueuing flush of memtable-locationinfo@793374785(35/43 serialized/live bytes, 1 ops)  info 09:35:37,804 writing memtable-locationinfo@793374785(35/43 serialized/live bytes, 1 ops)  info 09:35:37,825 completed flushing /raid0/cassandra/data/system/locationinfo-h-29-data.db (89 bytes)  info 09:35:37,826 node /10.202.67.43 has restarted, now up  info 09:35:37,827 inetaddress /10.202.67.43 is now up  info 09:35:37,827 node /10.202.67.43 state jump to normal  info 09:35:37,828 node /10.116.134.183 is now part of the cluster  info 09:35:37,828 inetaddress /10.116.134.183 is now up  info 09:35:37,829 enqueuing flush of memtable-locationinfo@1728699027(35/43 serialized/live bytes, 1 ops)  info 09:35:37,830 writing memtable-locationinfo@1728699027(35/43 serialized/live bytes, 1 ops)  info 09:35:37,850 completed flushing /raid0/cassandra/data/system/locationinfo-h-30-data.db (89 bytes)  info 09:35:37,852 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-30-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-27-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-26-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-29-data.db')]  info 09:35:37,853 node /10.118.94.62 is now part of the cluster  info 09:35:37,853 inetaddress /10.118.94.62 is now up  info 09:35:37,855 enqueuing flush of memtable-locationinfo@2001229122(35/43 serialized/live bytes, 1 ops)  info 09:35:37,855 writing memtable-locationinfo@2001229122(35/43 serialized/live bytes, 1 ops)  info 09:35:37,885 completed flushing /raid0/cassandra/data/system/locationinfo-h-31-data.db (89 bytes)  info 09:35:37,886 node /10.116.215.81 is now part of the cluster  info 09:35:37,887 inetaddress /10.116.215.81 is now up  info 09:35:37,888 enqueuing flush of memtable-locationinfo@1748800276(35/43 serialized/live bytes, 1 ops)  info 09:35:37,888 writing memtable-locationinfo@1748800276(35/43 serialized/live bytes, 1 ops)  info 09:35:37,909 completed flushing /raid0/cassandra/data/system/locationinfo-h-33-data.db (89 bytes)  info 09:35:37,910 node /10.80.110.28 has restarted, now up  info 09:35:37,911 inetaddress /10.80.110.28 is now up  info 09:35:37,911 node /10.80.110.28 state jump to normal  info 09:35:37,912 node /10.80.210.38 is now part of the cluster  info 09:35:37,912 inetaddress /10.80.210.38 is now up  info 09:35:37,914 enqueuing flush of memtable-locationinfo@1761382005(35/43 serialized/live bytes, 1 ops)  info 09:35:37,914 writing memtable-locationinfo@1761382005(35/43 serialized/live bytes, 1 ops)  info 09:35:37,925 compacted to [/raid0/cassandra/data/system/locationinfo-h-32-data.db,].  1,144 to 982 (~85% of original) bytes for 4 keys at 0.014190mb/s.  time: 66ms.  info 09:35:37,927 completed flushing /raid0/cassandra/data/system/locationinfo-h-35-data.db (89 bytes)  info 09:35:37,928 node /10.40.177.173 has restarted, now up  info 09:35:37,929 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-31-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-32-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-33-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-35-data.db')]  info 09:35:37,929 inetaddress /10.40.177.173 is now up  info 09:35:37,929 node /10.40.177.173 state jump to normal  info 09:35:37,930 node /10.101.41.8 has restarted, now up  info 09:35:37,931 inetaddress /10.101.41.8 is now up  info 09:35:37,931 node /10.101.41.8 state jump to normal  info 09:35:37,931 node /10.205.23.34 has restarted, now up  info 09:35:37,932 inetaddress /10.205.23.34 is now up  info 09:35:37,932 node /10.205.23.34 state jump to normal  info 09:35:37,933 node /10.118.185.68 is now part of the cluster  info 09:35:37,933 inetaddress /10.118.185.68 is now up  info 09:35:37,934 enqueuing flush of memtable-locationinfo@260440278(35/43 serialized/live bytes, 1 ops)  info 09:35:37,935 writing memtable-locationinfo@260440278(35/43 serialized/live bytes, 1 ops)  info 09:35:37,970 completed flushing /raid0/cassandra/data/system/locationinfo-h-36-data.db (89 bytes)  info 09:35:37,971 node /10.116.241.250 is now part of the cluster  info 09:35:37,972 inetaddress /10.116.241.250 is now up  info 09:35:37,973 enqueuing flush of memtable-locationinfo@768673839(35/43 serialized/live bytes, 1 ops)  info 09:35:37,974 writing memtable-locationinfo@768673839(35/43 serialized/live bytes, 1 ops)  info 09:35:38,003 completed flushing /raid0/cassandra/data/system/locationinfo-h-38-data.db (89 bytes)  info 09:35:38,004 node /10.113.42.21 is now part of the cluster  info 09:35:38,005 inetaddress /10.113.42.21 is now up  info 09:35:38,007 enqueuing flush of memtable-locationinfo@1610335061(35/43 serialized/live bytes, 1 ops)  info 09:35:38,008 writing memtable-locationinfo@1610335061(35/43 serialized/live bytes, 1 ops)  info 09:35:38,014 compacted to [/raid0/cassandra/data/system/locationinfo-h-37-data.db,].  1,249 to 1,087 (~87% of original) bytes for 4 keys at 0.012196mb/s.  time: 85ms.  info 09:35:38,024 completed flushing /raid0/cassandra/data/system/locationinfo-h-40-data.db (89 bytes)  info 09:35:38,024 node /10.194.29.156 is now part of the cluster  info 09:35:38,025 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-37-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-40-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-36-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-38-data.db')]  info 09:35:38,025 inetaddress /10.194.29.156 is now up  info 09:35:38,026 enqueuing flush of memtable-locationinfo@1625488363(35/43 serialized/live bytes, 1 ops)  info 09:35:38,027 writing memtable-locationinfo@1625488363(35/43 serialized/live bytes, 1 ops)  info 09:35:38,042 completed flushing /raid0/cassandra/data/system/locationinfo-h-41-data.db (89 bytes)  info 09:35:38,043 node /10.85.11.38 has restarted, now up  info 09:35:38,044 inetaddress /10.85.11.38 is now up  info 09:35:38,044 node /10.85.11.38 state jump to normal  info 09:35:38,045 node /10.34.159.72 is now part of the cluster  info 09:35:38,046 inetaddress /10.34.159.72 is now up  info 09:35:38,047 enqueuing flush of memtable-locationinfo@747881713(35/43 serialized/live bytes, 1 ops)  info 09:35:38,048 writing memtable-locationinfo@747881713(35/43 serialized/live bytes, 1 ops)  info 09:35:38,065 completed flushing /raid0/cassandra/data/system/locationinfo-h-42-data.db (89 bytes)  info 09:35:38,067 node /10.194.22.191 is now part of the cluster  info 09:35:38,067 inetaddress /10.194.22.191 is now up  info 09:35:38,069 enqueuing flush of memtable-locationinfo@709926392(35/43 serialized/live bytes, 1 ops)  info 09:35:38,069 writing memtable-locationinfo@709926392(35/43 serialized/live bytes, 1 ops)  info 09:35:38,092 completed flushing /raid0/cassandra/data/system/locationinfo-h-44-data.db (89 bytes)  info 09:35:38,093 node /10.34.74.58 is now part of the cluster  info 09:35:38,097 inetaddress /10.34.74.58 is now up  info 09:35:38,098 enqueuing flush of memtable-locationinfo@1356841826(35/43 serialized/live bytes, 1 ops)  info 09:35:38,099 writing memtable-locationinfo@1356841826(35/43 serialized/live bytes, 1 ops)  info 09:35:38,105 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-43-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-41-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-44-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-42-data.db')]  info 09:35:38,106 compacted to [/raid0/cassandra/data/system/locationinfo-h-43-data.db,].  1,354 to 1,192 (~88% of original) bytes for 4 keys at 0.014034mb/s.  time: 81ms.  info 09:35:38,144 completed flushing /raid0/cassandra/data/system/locationinfo-h-46-data.db (89 bytes)  info 09:35:38,145 node /10.40.22.224 is now part of the cluster  info 09:35:38,146 inetaddress /10.40.22.224 is now up  info 09:35:38,147 enqueuing flush of memtable-locationinfo@422797318(35/43 serialized/live bytes, 1 ops)  info 09:35:38,148 writing memtable-locationinfo@422797318(35/43 serialized/live bytes, 1 ops)  info 09:35:38,155 compacted to [/raid0/cassandra/data/system/locationinfo-h-47-data.db,].  1,459 to 1,297 (~88% of original) bytes for 4 keys at 0.024738mb/s.  time: 50ms.  info 09:35:38,164 completed flushing /raid0/cassandra/data/system/locationinfo-h-49-data.db (89 bytes)  info 09:35:38,165 node /10.32.79.134 is now part of the cluster  info 09:35:38,166 inetaddress /10.32.79.134 is now up  info 09:35:38,167 enqueuing flush of memtable-locationinfo@1455093129(35/43 serialized/live bytes, 1 ops)  info 09:35:38,168 writing memtable-locationinfo@1455093129(35/43 serialized/live bytes, 1 ops)  info 09:35:38,199 completed flushing /raid0/cassandra/data/system/locationinfo-h-50-data.db (89 bytes)  info 09:35:38,200 node /10.118.179.67 is now part of the cluster  info 09:35:38,200 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-50-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-47-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-49-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-46-data.db')]  info 09:35:38,200 inetaddress /10.118.179.67 is now up  info 09:35:38,202 enqueuing flush of memtable-locationinfo@1105436908(35/43 serialized/live bytes, 1 ops)  info 09:35:38,202 writing memtable-locationinfo@1105436908(35/43 serialized/live bytes, 1 ops)  info 09:35:38,248 completed flushing /raid0/cassandra/data/system/locationinfo-h-51-data.db (89 bytes)  info 09:35:38,249 node /10.84.205.93 is now part of the cluster  info 09:35:38,249 inetaddress /10.84.205.93 is now up  info 09:35:38,251 enqueuing flush of memtable-locationinfo@1306980591(35/43 serialized/live bytes, 1 ops)  info 09:35:38,251 writing memtable-locationinfo@1306980591(35/43 serialized/live bytes, 1 ops)  info 09:35:38,262 compacted to [/raid0/cassandra/data/system/locationinfo-h-52-data.db,].  1,564 to 1,402 (~89% of original) bytes for 4 keys at 0.021919mb/s.  time: 61ms.  info 09:35:38,294 completed flushing /raid0/cassandra/data/system/locationinfo-h-54-data.db (89 bytes)  info 09:35:38,294 node /10.34.33.16 has restarted, now up  info 09:35:38,295 inetaddress /10.34.33.16 is now up  info 09:35:38,296 node /10.34.33.16 state jump to normal  info 09:35:38,296 node /10.39.107.114 is now part of the cluster  info 09:35:38,297 inetaddress /10.39.107.114 is now up  info 09:35:38,298 enqueuing flush of memtable-locationinfo@1038389338(35/43 serialized/live bytes, 1 ops)  info 09:35:38,299 writing memtable-locationinfo@1038389338(35/43 serialized/live bytes, 1 ops)  info 09:35:38,311 completed flushing /raid0/cassandra/data/system/locationinfo-h-55-data.db (89 bytes)  info 09:35:38,312 node /10.196.79.240 is now part of the cluster  info 09:35:38,312 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-52-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-55-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-54-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-51-data.db')]  info 09:35:38,313 inetaddress /10.196.79.240 is now up  info 09:35:38,314 enqueuing flush of memtable-locationinfo@1850278722(35/43 serialized/live bytes, 1 ops)  info 09:35:38,315 writing memtable-locationinfo@1850278722(35/43 serialized/live bytes, 1 ops)  info 09:35:38,354 completed flushing /raid0/cassandra/data/system/locationinfo-h-56-data.db (89 bytes)  info 09:35:38,355 node /10.99.39.242 has restarted, now up  info 09:35:38,356 inetaddress /10.99.39.242 is now up  info 09:35:38,356 node /10.99.39.242 state jump to normal  info 09:35:38,357 node /10.118.233.198 has restarted, now up  info 09:35:38,358 inetaddress /10.118.233.198 is now up  info 09:35:38,358 node /10.118.233.198 state jump to normal  info 09:35:38,359 node /10.82.210.172 is now part of the cluster  info 09:35:38,359 inetaddress /10.82.210.172 is now up  info 09:35:38,364 enqueuing flush of memtable-locationinfo@786665924(35/43 serialized/live bytes, 1 ops)  info 09:35:38,364 writing memtable-locationinfo@786665924(35/43 serialized/live bytes, 1 ops)  info 09:35:38,439 completed flushing /raid0/cassandra/data/system/locationinfo-h-58-data.db (89 bytes)  info 09:35:38,440 node /10.80.41.192 is now part of the cluster  info 09:35:38,440 inetaddress /10.80.41.192 is now up  info 09:35:38,442 enqueuing flush of memtable-locationinfo@1647844754(35/43 serialized/live bytes, 1 ops)  info 09:35:38,442 writing memtable-locationinfo@1647844754(35/43 serialized/live bytes, 1 ops)  info 09:35:38,451 compacted to [/raid0/cassandra/data/system/locationinfo-h-57-data.db,].  1,669 to 1,515 (~90% of original) bytes for 4 keys at 0.010470mb/s.  time: 138ms.  info 09:35:38,459 completed flushing /raid0/cassandra/data/system/locationinfo-h-60-data.db (89 bytes)  info 09:35:38,459 node /10.76.243.129 is now part of the cluster  info 09:35:38,460 compacting [sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-56-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-58-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-57-data.db'), sstablereader(path='/raid0/cassandra/data/system/locationinfo-h-60-data.db')]  info 09:35:38,460 inetaddress /10.76.243.129 is now up  info 09:35:38,462 enqueuing flush of memtable-locationinfo@585652261(35/43 serialized/live bytes, 1 ops)  info 09:35:38,462 writing memtable-locationinfo@585652261(35/43 serialized/live bytes, 1 ops)  info 09:35:38,478 completed flushing /raid0/cassandra/data/system/locationinfo-h-61-data.db (89 bytes)  info 09:35:38,486 node /10.34.42.28 state jump to normal  info 09:35:38,487 node /10.34.70.73 state jump to normal  info 09:35:38,488 node /10.99.86.251 state jump to normal  info 09:35:38,489 node /10.118.94.62 state jump to normal  info 09:35:38,489 node /10.80.110.28 state jump to normal  info 09:35:38,490 node /10.80.210.38 state jump to normal  info 09:35:38,491 node /10.40.177.173 state jump to normal  info 09:35:38,493 node /10.101.41.8 state jump to normal  info 09:35:38,493 node /10.113.42.21 state jump to normal  info 09:35:38,494 node /10.85.11.38 state jump to normal  info 09:35:38,495 node /10.34.159.72 state jump to normal  info 09:35:38,496 node /10.34.74.58 state jump to normal  info 09:35:38,497 node /10.84.205.93 state jump to normal  info 09:35:38,497 node /10.118.179.67 state jump to normal  info 09:35:38,498 node /10.34.33.16 state jump to normal  info 09:35:38,499 node /10.196.79.240 state jump to normal  info 09:35:38,500 node /10.118.233.198 state jump to normal  info 09:35:38,501 node /10.80.41.192 state jump to normal  info 09:35:38,502 node /10.76.243.129 state jump to normal  info 09:35:38,508 node /10.118.185.68 state jump to normal  info 09:35:38,524 node /10.118.230.219 state jump to normal  info 09:35:38,536 compacted to [/raid0/cassandra/data/system/locationinfo-h-62-data.db,].  1,782 to 1,620 (~90% of original) bytes for 4 keys at 0.020328mb/s.  time: 76ms.  info 09:35:38,537 node /10.80.110.28 state jump to normal  info 09:35:38,537 node /10.40.177.173 state jump to normal  info 09:35:38,538 node /10.101.41.8 state jump to normal  info 09:35:38,539 node /10.116.241.250 state jump to normal  info 09:35:38,540 node /10.194.29.156 state jump to normal  info 09:35:38,540 node /10.34.74.58 state jump to normal  info 09:35:38,541 node /10.40.22.224 state jump to normal  info 09:35:38,542 node /10.32.79.134 state jump to normal  info 09:35:38,543 node /10.39.107.114 state jump to normal  info 09:35:38,543 node /10.99.39.242 state jump to normal  info 09:35:38,550 node /10.77.63.49 state jump to normal  info 09:35:38,550 node /10.34.42.28 state jump to normal  info 09:35:38,551 node /10.116.134.183 state jump to normal  info 09:35:38,553 node /10.76.243.129 state jump to normal  info 09:35:38,557 node /10.202.67.43 state jump to normal  info 09:35:38,558 node /10.118.94.62 state jump to normal  info 09:35:38,562 node /10.116.215.81 state jump to normal  info 09:35:38,563 node /10.80.210.38 state jump to normal  info 09:35:38,564 node /10.205.23.34 state jump to normal  info 09:35:38,565 node /10.39.107.114 state jump to normal <code> <text> i was starting up the new datastax ami where the seed starts first and 34 nodes would latch on together. so far things have been working decently for launching, but right now i just got this during startup.",
        "label": 423
    },
    {
        "text": "query failing due to assertionerror <description> i am trying out cassandra for the first time and running it locally for simple session management db. [cassandra-2.0.4, cql3, datastax driver 2.0.0-rc2] the following count query works fine when there is no data in the table: select count(*) from session_data where app_name=? and account=? and last_access > ? but after even a single row is inserted into the table, the query fails with the following error:     java.lang.assertionerror at org.apache.cassandra.db.filter.extendedfilter$withclauses.getextrafilter(extendedfilter.java:258) at org.apache.cassandra.db.columnfamilystore.filter(columnfamilystore.java:1719) at org.apache.cassandra.db.columnfamilystore.getrangeslice(columnfamilystore.java:1674) at org.apache.cassandra.db.pagedrangecommand.executelocally(pagedrangecommand.java:111) at org.apache.cassandra.service.storageproxy$localrangeslicerunnable.runmaythrow(storageproxy.java:1418) at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1931) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:744) here is the schema i am using:     create keyspace session with replication= {'class': 'simplestrategy', 'replication_factor': 1};     create table session_data (     username text,     session_id text,     app_name text,     account text,     last_access timestamp,     created_on timestamp,     primary key (username, session_id, app_name, account)     );     create index sessionindex on session_data (session_id);     create index sessionappname on session_data (app_name);     create index lastaccessindex on session_data (last_access);<stacktrace>     java.lang.assertionerror at org.apache.cassandra.db.filter.extendedfilter$withclauses.getextrafilter(extendedfilter.java:258) at org.apache.cassandra.db.columnfamilystore.filter(columnfamilystore.java:1719) at org.apache.cassandra.db.columnfamilystore.getrangeslice(columnfamilystore.java:1674) at org.apache.cassandra.db.pagedrangecommand.executelocally(pagedrangecommand.java:111) at org.apache.cassandra.service.storageproxy$localrangeslicerunnable.runmaythrow(storageproxy.java:1418) at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1931) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:744) <code> select count(*) from session_data where app_name=? and account=? and last_access > ?     create keyspace session with replication= {'class': 'simplestrategy', 'replication_factor': 1};     create table session_data (     username text,     session_id text,     app_name text,     account text,     last_access timestamp,     created_on timestamp,     primary key (username, session_id, app_name, account)     );     create index sessionindex on session_data (session_id);     create index sessionappname on session_data (app_name);     create index lastaccessindex on session_data (last_access); <text> i am trying out cassandra for the first time and running it locally for simple session management db. [cassandra-2.0.4, cql3, datastax driver 2.0.0-rc2] the following count query works fine when there is no data in the table: but after even a single row is inserted into the table, the query fails with the following error: here is the schema i am using:",
        "label": 520
    },
    {
        "text": "snapshot creation for non system keyspaces snapshots to the flush and data dirs <description> root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot -t testsnapshot requested creating snapshot(s) for [all keyspaces] with snapshot name [testsnapshot] snapshot directory: testsnapshot root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/ foo  system  system_traces root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot requested creating snapshot(s) for [all keyspaces] with snapshot name [1398095603827] snapshot directory: 1398095603827 root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/ foo/           system/        system_traces/  root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo requested creating snapshot(s) for [foo] with snapshot name [1398095800752] snapshot directory: 1398095800752 root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo -cf bar -t testsnapshot requested creating snapshot(s) for [foo] with snapshot name [testsnapshot] error: snapshot testsnapshot already exists. -- stacktrace -- java.io.ioexception: snapshot testsnapshot already exists.  at org.apache.cassandra.service.storageservice.takecolumnfamilysnapshot(storageservice.java:2315)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.trampoline.invoke(methodutil.java:75)  at sun.reflect.generatedmethodaccessor2.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.methodutil.invoke(methodutil.java:279)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)  at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)  at sun.rmi.transport.transport$1.run(transport.java:177)  at sun.rmi.transport.transport$1.run(transport.java:174)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:173)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744) root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ls -r /var/lib/cassandra/data/foo/|grep testsnap root@bw-3:/srv/cassandra-dtest# ls -r /var/lib/cassandra/data/|grep testsnap testsnapshot /var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/testsnapshot: root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo -cf bar -t testsnapshot2 requested creating snapshot(s) for [foo] with snapshot name [testsnapshot2] snapshot directory: testsnapshot2 root@bw-3:/srv/cassandra-dtest# ls -r /var/lib/cassandra/data/|grep testsnap testsnapshot /var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/testsnapshot: root@bw-3:/srv/cassandra-dtest#  as shown above, snapshots for the 'foo' keyspace are never created, but it does seem to think it created them since i can't use the same name twice.<stacktrace> root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot -t testsnapshot requested creating snapshot(s) for [all keyspaces] with snapshot name [testsnapshot] snapshot directory: testsnapshot root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/ foo  system  system_traces root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot requested creating snapshot(s) for [all keyspaces] with snapshot name [1398095603827] snapshot directory: 1398095603827 root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/ foo/           system/        system_traces/  root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo requested creating snapshot(s) for [foo] with snapshot name [1398095800752] snapshot directory: 1398095800752 root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo -cf bar -t testsnapshot requested creating snapshot(s) for [foo] with snapshot name [testsnapshot] error: snapshot testsnapshot already exists. -- stacktrace -- java.io.ioexception: snapshot testsnapshot already exists.  at org.apache.cassandra.service.storageservice.takecolumnfamilysnapshot(storageservice.java:2315)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.trampoline.invoke(methodutil.java:75)  at sun.reflect.generatedmethodaccessor2.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.methodutil.invoke(methodutil.java:279)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)  at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)  at sun.rmi.transport.transport$1.run(transport.java:177)  at sun.rmi.transport.transport$1.run(transport.java:174)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:173)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744) root@bw-3:/srv/cassandra-dtest# ls /var/lib/cassandra/data/foo/ bar-c5e32c10c96b11e39f8d3b546d897db7 root@bw-3:/srv/cassandra-dtest# ls -r /var/lib/cassandra/data/foo/|grep testsnap root@bw-3:/srv/cassandra-dtest# ls -r /var/lib/cassandra/data/|grep testsnap testsnapshot /var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/testsnapshot: root@bw-3:/srv/cassandra-dtest# ../cassandra/bin/nodetool snapshot foo -cf bar -t testsnapshot2 requested creating snapshot(s) for [foo] with snapshot name [testsnapshot2] snapshot directory: testsnapshot2 root@bw-3:/srv/cassandra-dtest# ls -r /var/lib/cassandra/data/|grep testsnap testsnapshot /var/lib/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/snapshots/testsnapshot: root@bw-3:/srv/cassandra-dtest#  <code> <text> as shown above, snapshots for the 'foo' keyspace are never created, but it does seem to think it created them since i can't use the same name twice.",
        "label": 85
    },
    {
        "text": "incorrect jvm metric names <description> some jvm metrics have a double dot in name like:  jvm.memory..total.max , jvm.memory..total.init (etc).  it seems that an extra dot is added at the end of the name in cassandradaemon.java, around line 367 (in 3.0.10):  ...  // enable metrics provided by metrics-jvm.jar  cassandrametricsregistry.metrics.register(\"jvm.buffers.\", new bufferpoolmetricset(managementfactory.getplatformmbeanserver()));  cassandrametricsregistry.metrics.register(\"jvm.gc.\", new garbagecollectormetricset());  cassandrametricsregistry.metrics.register(\"jvm.memory.\", new memoryusagegaugeset()); and also added in append method of metricregistry.  call stack is:  metricregistry>>registerall(string prefix, metricset metrics)  metricregistry>>static string name(string name, string... names)  metricregistry>>static void append(stringbuilder builder, string part) and in append the dot is also added:  ...  if(builder.length() > 0) { builder.append('.'); } builder.append(part);  ... the codahale metricregistry class seems to have no recent modification of name or append methods, so it look like a small bug.  may be the fix could be to simply not to add the final dot in the metric name, ie \"jvm.buffers\" instead of \"jvm.buffers.\"<stacktrace> <code> some jvm metrics have a double dot in name like:  jvm.memory..total.max , jvm.memory..total.init (etc).  it seems that an extra dot is added at the end of the name in cassandradaemon.java, around line 367 (in 3.0.10):  ...  // enable metrics provided by metrics-jvm.jar  cassandrametricsregistry.metrics.register('jvm.buffers.', new bufferpoolmetricset(managementfactory.getplatformmbeanserver()));  cassandrametricsregistry.metrics.register('jvm.gc.', new garbagecollectormetricset());  cassandrametricsregistry.metrics.register('jvm.memory.', new memoryusagegaugeset()); and also added in append method of metricregistry.  call stack is:  metricregistry>>registerall(string prefix, metricset metrics)  metricregistry>>static string name(string name, string... names)  metricregistry>>static void append(stringbuilder builder, string part) and in append the dot is also added:  ...  if(builder.length() > 0) <text> builder.append(part);  ... the codahale metricregistry class seems to have no recent modification of name or append methods, so it look like a small bug.  may be the fix could be to simply not to add the final dot in the metric name, ie 'jvm.buffers' instead of 'jvm.buffers.'",
        "label": 13
    },
    {
        "text": "v4 spec has tons of grammatical mistakes <description> https://github.com/apache/cassandra/blob/cassandra-3.0/doc/native_protocol_v4.spec i notice the following in the first section of the spec and then gave up: \"the list of allowed opcode is defined section 2.3\" => \"the list of allowed opcode*s* is defined in section 2.3\"  \"the details of each corresponding message is described section 4\" => \"the details of each corresponding message are described in section 4\" since the subject is details, not message.  \"requests are those frame sent by\" => \"requests are those frame*s* sent by\" i think someone should go through the whole spec and fix all the mistakes rather than me pointing out the ones i notice piece-meal. i found the grammar errors to be rather distracting.<stacktrace> <code> https://github.com/apache/cassandra/blob/cassandra-3.0/doc/native_protocol_v4.spec 'the list of allowed opcode is defined section 2.3' => 'the list of allowed opcode*s* is defined in section 2.3'  'the details of each corresponding message is described section 4' => 'the details of each corresponding message are described in section 4' since the subject is details, not message.  'requests are those frame sent by' => 'requests are those frame*s* sent by' <text> i notice the following in the first section of the spec and then gave up: i think someone should go through the whole spec and fix all the mistakes rather than me pointing out the ones i notice piece-meal. i found the grammar errors to be rather distracting.",
        "label": 480
    },
    {
        "text": "expose bulk loading progress status over jmx <description> the bulk loading interface should be exposing some progress or status information over jmx. this shouldn't be too difficult and should be exposed in a way that the information is available whether you are using the separate sstableloader utility or calling the bulkload jmx call.<stacktrace> <code> <text> the bulk loading interface should be exposing some progress or status information over jmx. this shouldn't be too difficult and should be exposed in a way that the information is available whether you are using the separate sstableloader utility or calling the bulkload jmx call.",
        "label": 538
    },
    {
        "text": "error on encrypted node communication upgrading from to <description> after updating to cassandra 2.2.0 from 2.1.6 i am having ssl issues. the configuration had not changed from one version to the other, the jvm is still the same however on 2.2.0 it is erroring. i am yet to investigate the source code for it. but for now, this is the information i have to share on it: my jvm is java version \"1.8.0_45\"  java(tm) se runtime environment (build 1.8.0_45-b14)  java hotspot(tm) 64-bit server vm (build 25.45-b02, mixed mode) ubuntu 14.04.2 lts is on all nodes, they are the same. below is the encryption settings from cassandra.yaml of all nodes. i am using the same keystore and trustore as i had used before on 2.1.6 1. enable or disable inter-node encryption 2. default settings are tls v1, rsa 1024-bit keys (it is imperative that 3. users generate their own keys) tls_rsa_with_aes_128_cbc_sha as the cipher 4. suite for authentication, key exchange and encryption of the actual data transfers. 5. use the dhe/ecdhe ciphers if running in fips 140 compliant mode. 6. note: no custom encryption options are enabled at the moment 7. the available internode options are : all, none, dc, rack  # 8. if set to dc cassandra will encrypt the traffic between the dcs 9. if set to rack cassandra will encrypt the traffic between the racks  # 10. the passwords used in these options must match the passwords used when generating 11. the keystore and truststore. for instructions on generating these files, see: 12. http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/jsserefguide.html#createkeystore  #  server_encryption_options:  internode_encryption: all  keystore: /etc/cassandra/certs/node.keystore  keystore_password: mypasswd  truststore: /etc/cassandra/certs/global.truststore  truststore_password: mypasswd 13. more advanced defaults below: 14. protocol: tls 15. algorithm: sunx509 16. store_type: jks  cipher_suites: [tls_rsa_with_aes_128_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_dhe_rsa_with_aes_128_cbc_sha,tls_dhe_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_128_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha]  require_client_auth: false 1. enable or disable client/server encryption. nodes cannot talk to each other as per ssl errors bellow. warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:48,764 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:48,764 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:48,764 outboundtcpconnection.java:316 - error writing to /192.168.1.31  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  warn [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:49,764 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:49,764 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:49,764 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.33  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:49,764 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:50,763 outboundtcpconnection.java:316 - error writing to /192.168.1.31  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:51,766 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:51,767 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:52,764 outboundtcpconnection.java:316 - error writing to /192.168.1.33  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:52,764 outboundtcpconnection.java:316 - error writing to /192.168.1.31  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:53,767 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:53,767 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0] i had also tried to have the unrestricted jce for java 8 in and the error has changed. http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html from: java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:52,764 outboundtcpconnection.java:316 - error writing to /192.168.1.33 to: error [messagingservice-outgoing-/192.168.1.33] 2015-07-23 14:51:01,319 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.33  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]<stacktrace> warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:48,764 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:48,764 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:48,764 outboundtcpconnection.java:316 - error writing to /192.168.1.31  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  warn [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:49,764 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:49,764 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:49,764 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.33  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:49,764 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:50,763 outboundtcpconnection.java:316 - error writing to /192.168.1.31  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:51,766 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:51,767 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:52,764 outboundtcpconnection.java:316 - error writing to /192.168.1.33  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:52,764 outboundtcpconnection.java:316 - error writing to /192.168.1.31  java.lang.nullpointerexception: null  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:323) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:285) [apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:219) [apache-cassandra-2.2.0.jar:2.2.0]  warn [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:53,767 sslfactory.java:163 - filtering out tls_dhe_rsa_with_aes_256_cbc_sha,tls_rsa_with_aes_256_cbc_sha,tls_ecdhe_rsa_with_aes_256_cbc_sha as it isnt supported by the socket  error [messagingservice-outgoing-/192.168.1.31] 2015-07-22 17:29:53,767 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.31  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0] java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]  error [messagingservice-outgoing-/192.168.1.33] 2015-07-22 17:29:52,764 outboundtcpconnection.java:316 - error writing to /192.168.1.33 error [messagingservice-outgoing-/192.168.1.33] 2015-07-23 14:51:01,319 outboundtcpconnection.java:229 - error processing a message intended for /192.168.1.33  java.lang.nullpointerexception: null  at com.google.common.base.preconditions.checknotnull(preconditions.java:213) ~[guava-16.0.jar:na]  at org.apache.cassandra.io.util.buffereddataoutputstreamplus.<init>(buffereddataoutputstreamplus.java:74) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:404) ~[apache-cassandra-2.2.0.jar:2.2.0]  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:218) ~[apache-cassandra-2.2.0.jar:2.2.0]<code> my jvm is java version '1.8.0_45'  java(tm) se runtime environment (build 1.8.0_45-b14)  java hotspot(tm) 64-bit server vm (build 25.45-b02, mixed mode) http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html <text> after updating to cassandra 2.2.0 from 2.1.6 i am having ssl issues. the configuration had not changed from one version to the other, the jvm is still the same however on 2.2.0 it is erroring. i am yet to investigate the source code for it. but for now, this is the information i have to share on it: ubuntu 14.04.2 lts is on all nodes, they are the same. below is the encryption settings from cassandra.yaml of all nodes. i am using the same keystore and trustore as i had used before on 2.1.6 nodes cannot talk to each other as per ssl errors bellow. i had also tried to have the unrestricted jce for java 8 in and the error has changed. from: to: ",
        "label": 577
    },
    {
        "text": "validate dc information on startup <description> when using gossipingpropertyfilesnitch it is possible to change the data center and rack of a live node by changing the cassandra-rackdc.properties file. should this really be possible? in the documentation at http://docs.datastax.com/en/cassandra/2.1/cassandra/initialize/initializemultipleds.html it's stated that you should choose the name carefully; renaming a data center is not possible, but with this functionality it doesn't seem impossible(maybe a bit hard with changing replication etc.). this functionality was introduced by cassandra-5897 so i'm guessing there is some use case for this? personally i would want the dc/rack settings to be as restricted as the cluster name, otherwise if a node could just join another data center without removing it's local information couldn't it mess up the token ranges? and suddenly the old data center/rack would loose 1 replica of all the data that the node contains.<stacktrace> <code> <text> when using gossipingpropertyfilesnitch it is possible to change the data center and rack of a live node by changing the cassandra-rackdc.properties file. should this really be possible? in the documentation at http://docs.datastax.com/en/cassandra/2.1/cassandra/initialize/initializemultipleds.html it's stated that you should choose the name carefully; renaming a data center is not possible, but with this functionality it doesn't seem impossible(maybe a bit hard with changing replication etc.). this functionality was introduced by cassandra-5897 so i'm guessing there is some use case for this? personally i would want the dc/rack settings to be as restricted as the cluster name, otherwise if a node could just join another data center without removing it's local information couldn't it mess up the token ranges? and suddenly the old data center/rack would loose 1 replica of all the data that the node contains.",
        "label": 322
    },
    {
        "text": "bytebuffer bug in expiringcolumn updatedigest  <description> the messagedigest calls in expringcolumn change the position of the bytebuffer.<stacktrace> <code> <text> the messagedigest calls in expringcolumn change the position of the bytebuffer.",
        "label": 521
    },
    {
        "text": "optimize mostrecenttomstone vs maxtimestamp check in collationcontroller collectalldata <description> collationcontroller.collectalldata eliminates a sstable if we've already read a row tombstone more recent that its maxtimestamp. this is however done in 2 passes and can be inefficient (or rather, it's not as efficient as it could). more precisely, say we have 10 sstables s0, ... s9, where s0 is the most recent and s9 the least one (and their maxtimestamp reflect that) and s0 has a row tombstone that is more recent than all of s1-s9 maxtimestamps. now in collectalldata(), we first iterate over sstables in a \"random\" order (because datatracker keeps sstable in a more or less random order). meaning that we may iterate in the order s9, s8, ... s0. in that case, we will end up reading the row header from all the sstable (hitting disk each time). then, and only then, the 2nd pass of collectalldata will eliminate s1 to s9. however, if we were to iterate sstable in maxtimestamps order (as we do in collecttimeordered), we would only need one pass but more importantly we would minimize the number of row header we read to perform that sstable eliminination. in my example, we would only ever read the row tombstone from s0 and eliminate all other sstable directly, simply based on their maxtimestamp.<stacktrace> <code> <text> collationcontroller.collectalldata eliminates a sstable if we've already read a row tombstone more recent that its maxtimestamp. this is however done in 2 passes and can be inefficient (or rather, it's not as efficient as it could). more precisely, say we have 10 sstables s0, ... s9, where s0 is the most recent and s9 the least one (and their maxtimestamp reflect that) and s0 has a row tombstone that is more recent than all of s1-s9 maxtimestamps. now in collectalldata(), we first iterate over sstables in a 'random' order (because datatracker keeps sstable in a more or less random order). meaning that we may iterate in the order s9, s8, ... s0. in that case, we will end up reading the row header from all the sstable (hitting disk each time). then, and only then, the 2nd pass of collectalldata will eliminate s1 to s9. however, if we were to iterate sstable in maxtimestamps order (as we do in collecttimeordered), we would only need one pass but more importantly we would minimize the number of row header we read to perform that sstable eliminination. in my example, we would only ever read the row tombstone from s0 and eliminate all other sstable directly, simply based on their maxtimestamp.",
        "label": 520
    },
    {
        "text": "streaming <description> 2.0 is the good time to redesign streaming api including protocol to make streaming more performant and reliable. design goals that come up in my mind: better performance protocol optimization stream multiple files in parallel (cassandra-4663) persistent connection (cassandra-4660) better control cleaner api for error handling integrate both in/out streams into one session, so the components(bootstrap, move, bulkload, repair...) that use streaming can manage them easily. better reporting better logging/tracing more metrics progress reporting api for external client<stacktrace> <code> <text> 2.0 is the good time to redesign streaming api including protocol to make streaming more performant and reliable. design goals that come up in my mind: better performance better control better reporting",
        "label": 577
    },
    {
        "text": "disabling m a t for fun and profit  and other ant stuff  <description> it should be possible to disable maven-ant-tasks for environments with more rigid dependency control, or where network access isn't available. patches to follow.<stacktrace> <code> <text> it should be possible to disable maven-ant-tasks for environments with more rigid dependency control, or where network access isn't available. patches to follow.",
        "label": 139
    },
    {
        "text": "updates to compact storage tables via cli drop cql information <description> if a compact storage table is altered using the cli all information about the column names reverts to the initial \"key, column1, column2\" namings. additionally, the changes in the columns name will not take effect until the cassandra service is restarted. this means that the clients using cql will continue to work properly until the service is restarted, at which time they will start getting errors about non-existant columns in the table. when attempting to rename the columns back using alter table an error stating the column already exists will be raised. the only way to get it back is to alter table and change the comment or something, which will bring back all the original column names. this seems to be related to cassandra-6676 and cassandra-6370 in cqlsh connected to cluster1 at 127.0.0.3:9160. [cqlsh 3.1.8 | cassandra 1.2.15-snapshot | cql spec 3.0.0 | thrift protocol 19.36.2] use help for help. cqlsh> create keyspace test with replication = { 'class' : 'simplestrategy', 'replication_factor' : 3 }; cqlsh> use test; cqlsh:test> create table foo (bar text, baz text, qux text, primary key(bar, baz) ) with compact storage; cqlsh:test> describe table foo; create table foo (   bar text,   baz text,   qux text,   primary key (bar, baz) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'snappycompressor'}; now in cli:   connected to: \"cluster1\" on 127.0.0.3/9160 welcome to cassandra cli version 1.2.15-snapshot type 'help;' or '?' for help. type 'quit;' or 'exit;' to quit. [default@unknown] use test; authenticated to keyspace: test [default@test] update column family foo with comment='hey this is a comment'; 3bf5fa49-5d03-34f0-b46c-6745f7740925 now back in cqlsh: cqlsh:test> describe table foo; create table foo (   bar text,   column1 text,   value text,   primary key (bar, column1) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='hey this is a comment' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'snappycompressor'}; cqlsh:test> alter table foo with comment='this is a new comment'; cqlsh:test> describe table foo; create table foo (   bar text,   baz text,   qux text,   primary key (bar, baz) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='this is a new comment' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'snappycompressor'};<stacktrace> <code> connected to cluster1 at 127.0.0.3:9160. [cqlsh 3.1.8 | cassandra 1.2.15-snapshot | cql spec 3.0.0 | thrift protocol 19.36.2] use help for help. cqlsh> create keyspace test with replication = { 'class' : 'simplestrategy', 'replication_factor' : 3 }; cqlsh> use test; cqlsh:test> create table foo (bar text, baz text, qux text, primary key(bar, baz) ) with compact storage; cqlsh:test> describe table foo; create table foo (   bar text,   baz text,   qux text,   primary key (bar, baz) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'snappycompressor'};   connected to: 'cluster1' on 127.0.0.3/9160 welcome to cassandra cli version 1.2.15-snapshot type 'help;' or '?' for help. type 'quit;' or 'exit;' to quit. [default@unknown] use test; authenticated to keyspace: test [default@test] update column family foo with comment='hey this is a comment'; 3bf5fa49-5d03-34f0-b46c-6745f7740925 cqlsh:test> describe table foo; create table foo (   bar text,   column1 text,   value text,   primary key (bar, column1) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='hey this is a comment' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'snappycompressor'}; cqlsh:test> alter table foo with comment='this is a new comment'; cqlsh:test> describe table foo; create table foo (   bar text,   baz text,   qux text,   primary key (bar, baz) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='this is a new comment' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'snappycompressor'}; <text> if a compact storage table is altered using the cli all information about the column names reverts to the initial 'key, column1, column2' namings. additionally, the changes in the columns name will not take effect until the cassandra service is restarted. this means that the clients using cql will continue to work properly until the service is restarted, at which time they will start getting errors about non-existant columns in the table. when attempting to rename the columns back using alter table an error stating the column already exists will be raised. the only way to get it back is to alter table and change the comment or something, which will bring back all the original column names. this seems to be related to cassandra-6676 and cassandra-6370 in cqlsh now in cli: now back in cqlsh:",
        "label": 520
    },
    {
        "text": "mark sstables as repaired after full repairs <description> in 2.1 we avoided anticompaction and marking sstables as repaired after old-style full repairs (reasoning was that we wanted users to be able to carry on as before) in 3.0 incremental repairs is on by default and we should always mark and anticompact sstables<stacktrace> <code> <text> in 2.1 we avoided anticompaction and marking sstables as repaired after old-style full repairs (reasoning was that we wanted users to be able to carry on as before) in 3.0 incremental repairs is on by default and we should always mark and anticompact sstables",
        "label": 321
    },
    {
        "text": "add key validation class support to cli <description> also update readme to include utf8type key validator.<stacktrace> <code> <text> also update readme to include utf8type key validator.",
        "label": 412
    },
    {
        "text": "the output of the describe command does not necessarily give you the right ddl to re create the cf <description> if compression is not set for a cf, cqlsh omits the compression attribute. when you replay that very same ddl, you get a cf with snappy compression. this may occur with other parameters. perhaps describe should always show every parameter in full. the absence of a setting is a setting. (think of the arrow in the fedex logo). create a cf with cassandra-stress. cassandra-stress defaults to no compression.  ~/dse/resources/cassandra/tools/bin/cassandra-stress -s 100 -c 1 --num-keys 1 describe it  create table \"standard1\" (  key blob primary key,  \"c0\" blob  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= {'class': 'sizetieredcompactionstrategy'} ; replay it - i changed the cf name to standard2 describe the new cf: create table standard2 (  key blob primary key,  \"c0\" blob  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= {'class': 'sizetieredcompactionstrategy'} and  compression= {'sstable_compression': 'snappycompressor'} ;<stacktrace> <code> ~/dse/resources/cassandra/tools/bin/cassandra-stress -s 100 -c 1 --num-keys 1 describe it  create table 'standard1' (  key blob primary key,  'c0' blob  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= ; create table standard2 (  key blob primary key,  'c0' blob  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= ;<text> if compression is not set for a cf, cqlsh omits the compression attribute. when you replay that very same ddl, you get a cf with snappy compression. this may occur with other parameters. perhaps describe should always show every parameter in full. the absence of a setting is a setting. (think of the arrow in the fedex logo). create a cf with cassandra-stress. cassandra-stress defaults to no compression. replay it - i changed the cf name to standard2 describe the new cf: and  compression= ",
        "label": 18
    },
    {
        "text": "remove assumption that key to token is one to one <description> get_range_slices assumes that tokens do not collide and converts a keyrange to an abstractbounds. for randompartitioner, this assumption isn't safe, and would lead to a very weird heisenberg. converting abstractbounds to use a decoratedkey would solve this, because the byte[] key portion of the decoratedkey can act as a tiebreaker. alternatively, we could make decoratedkey extend token, and then use decoratedkeys in places where collisions are unacceptable.<stacktrace> <code> <text> get_range_slices assumes that tokens do not collide and converts a keyrange to an abstractbounds. for randompartitioner, this assumption isn't safe, and would lead to a very weird heisenberg. converting abstractbounds to use a decoratedkey would solve this, because the byte[] key portion of the decoratedkey can act as a tiebreaker. alternatively, we could make decoratedkey extend token, and then use decoratedkeys in places where collisions are unacceptable.",
        "label": 520
    },
    {
        "text": "fix few minor problems in nodeprobe cfstats <description> nodeprobe cfstats reports that readlatency/writelatency is nan on the keyspace level although it obviously is not. for example:  keyspace: keyspace1  read count: 392  read latency: nan ms.  write count: 262  write latency: nan ms.  pending tasks: 0  column family: mycf  memtable columns count: 143  memtable data size: 123433  memtable switch count: 2  read count: 392  read latency: 0.533 ms.  write count: 262  write latency: 0.000 ms.  pending tasks: 0  column family: standard2  memtable columns count: 0  memtable data size: 0  memtable switch count: 0  read count: 0  read latency: nan ms.  write count: 0  write latency: nan ms.  pending tasks: 0 the problem here is that there is more than one cf, and one of them has read latency/writelatency nan. this causes the keyspace readlatency/writelatency to be nan instead of the average across all cfs. another problem with cfstats is that it does not account for the delays when a read/write times out, so it does not accurately reflect the health of the system under too much stress.<stacktrace> <code> for example:  keyspace: keyspace1  read count: 392  read latency: nan ms.  write count: 262  write latency: nan ms.  pending tasks: 0 column family: mycf  memtable columns count: 143  memtable data size: 123433  memtable switch count: 2  read count: 392  read latency: 0.533 ms.  write count: 262  write latency: 0.000 ms.  pending tasks: 0 <text> nodeprobe cfstats reports that readlatency/writelatency is nan on the keyspace level although it obviously is not. column family: standard2  memtable columns count: 0  memtable data size: 0  memtable switch count: 0  read count: 0  read latency: nan ms.  write count: 0  write latency: nan ms.  pending tasks: 0 the problem here is that there is more than one cf, and one of them has read latency/writelatency nan. this causes the keyspace readlatency/writelatency to be nan instead of the average across all cfs. another problem with cfstats is that it does not account for the delays when a read/write times out, so it does not accurately reflect the health of the system under too much stress.",
        "label": 274
    },
    {
        "text": "keys are now byte  but hashmap cannot have byte  as keys so they need to be fixed <description> thrift client calls use hashmap and needs this fix<stacktrace> <code> <text> thrift client calls use hashmap and needs this fix",
        "label": 555
    },
    {
        "text": "cfmetadata converttothrift makes subcomparator type empty string instead of null <description> as a result of cassandra-891 adding a cfmetadata.converttothrift method, the values such as subcomparator_type are defaulted to empty string instead of null. that makes it so, for example, in columnfamilyrecordreader, in its rowiterator, the check for only null is insufficient. it also needs to check for a blank value. after a discussion about it in irc, jonathan said it was probably easier to just change the creation to give a null value instead of empty string.<stacktrace> <code> <text> as a result of cassandra-891 adding a cfmetadata.converttothrift method, the values such as subcomparator_type are defaulted to empty string instead of null. that makes it so, for example, in columnfamilyrecordreader, in its rowiterator, the check for only null is insufficient. it also needs to check for a blank value. after a discussion about it in irc, jonathan said it was probably easier to just change the creation to give a null value instead of empty string.",
        "label": 270
    },
    {
        "text": "make scrub validate deserialized columns <description> right now, scrub deserialize the columns but don't validate the fields, and such there is a number of errors it could fix (or at least corrupted rows it could skip) but don't.  this ticket proposes to handle those errors.<stacktrace> <code> <text> right now, scrub deserialize the columns but don't validate the fields, and such there is a number of errors it could fix (or at least corrupted rows it could skip) but don't.  this ticket proposes to handle those errors.",
        "label": 520
    },
    {
        "text": "add timeunit days for cassandra stress <description> it's useful for long running test and generating large number of data.<stacktrace> <code> <text> it's useful for long running test and generating large number of data.",
        "label": 234
    },
    {
        "text": "please delete old releases from mirroring system <description> to reduce the load on the asf mirrors, projects are required to delete old releases [1] please can you remove all non-current releases?  thanks!  [note that older releases are always available from the asf archive server] any links to older releases on download pages should first be adjusted to point to the archive server. [1] http://www.apache.org/dev/release.html#when-to-archive<stacktrace> <code> [1] http://www.apache.org/dev/release.html#when-to-archive<text> to reduce the load on the asf mirrors, projects are required to delete old releases [1] please can you remove all non-current releases?  thanks!  [note that older releases are always available from the asf archive server] any links to older releases on download pages should first be adjusted to point to the archive server. ",
        "label": 521
    },
    {
        "text": "provide a per table text blob map for storing extra metadata <description> for some applications that build on cassandra it's important to be able to attach extra metadata to tables, and have it be distributed via regular cassandra schema paths. i propose a new extensions map<text,blob> table param for just that.<stacktrace> <code> <text> for some applications that build on cassandra it's important to be able to attach extra metadata to tables, and have it be distributed via regular cassandra schema paths. i propose a new extensions map<text,blob> table param for just that.",
        "label": 69
    },
    {
        "text": "cassandra stress throws npe if insert section isn't specified in user profile <description> when user profile file is used, and insert section isn't specified, then cassandra-stress is using default values instead. since support for lwts was added, absence of the insert section lead to throwing of nullpointerexception when generating inserts: java.lang.nullpointerexception at org.apache.cassandra.stress.stressprofile.getinsert(stressprofile.java:546) at org.apache.cassandra.stress.stressprofile.printsettings(stressprofile.java:126) at org.apache.cassandra.stress.settings.stresssettings.lambda$printsettings$1(stresssettings.java:311) at java.util.linkedhashmap.foreach(linkedhashmap.java:684) at org.apache.cassandra.stress.settings.stresssettings.printsettings(stresssettings.java:311) at org.apache.cassandra.stress.stress.run(stress.java:108) at org.apache.cassandra.stress.stress.main(stress.java:63) fix is trivial, and will be provided as pr<stacktrace> java.lang.nullpointerexception at org.apache.cassandra.stress.stressprofile.getinsert(stressprofile.java:546) at org.apache.cassandra.stress.stressprofile.printsettings(stressprofile.java:126) at org.apache.cassandra.stress.settings.stresssettings.lambda$printsettings$1(stresssettings.java:311) at java.util.linkedhashmap.foreach(linkedhashmap.java:684) at org.apache.cassandra.stress.settings.stresssettings.printsettings(stresssettings.java:311) at org.apache.cassandra.stress.stress.run(stress.java:108) at org.apache.cassandra.stress.stress.main(stress.java:63) <code> <text> when user profile file is used, and insert section isn't specified, then cassandra-stress is using default values instead. since support for lwts was added, absence of the insert section lead to throwing of nullpointerexception when generating inserts: fix is trivial, and will be provided as pr",
        "label": 24
    },
    {
        "text": "change error message when rr times out <description> when a quorum request detects a checksum mismatch, it then reads the data to repair the mismatch by issuing a request at cl.all to the same endpoints (sp.fetchrows) if this request in turn times out, this delivers a toe to the client with a misleading message that mentions cl.all, possibly causing them to think the request has gone cross-dc when it has not, it was just slow due to timing out.<stacktrace> <code> <text> when a quorum request detects a checksum mismatch, it then reads the data to repair the mismatch by issuing a request at cl.all to the same endpoints (sp.fetchrows) if this request in turn times out, this delivers a toe to the client with a misleading message that mentions cl.all, possibly causing them to think the request has gone cross-dc when it has not, it was just slow due to timing out.",
        "label": 474
    },
    {
        "text": " dtest   trunk  testtopology movement test is flaky  fails assert  values not within  of the max     <description> dtest* testtopology.test_movement* is flaky. all of the testing so far (and thus all of the current known observed failures) have been when running against trunk. when the test fails, it always due to the assert_almost_equal assert. assertionerror: values not within 16.00% of the max: (851.41, 713.26) () the following circleci runs are 2 examples with dtests runs that failed due to this test failing it's assert:  https://circleci.com/gh/mkjellman/cassandra/487  https://circleci.com/gh/mkjellman/cassandra/526 p.s. assert_almost_equal has a comment \"@params error optional margin of error. default 0.16\". i don't see any obvious notes for why the default is this magical 16% number. it looks like it was committed as part of a big bulk commit by sean mccarthy (who i can't find on jira). if anyone has any history on the magic 16% allowed delta please share!<stacktrace> <code> assertionerror: values not within 16.00% of the max: (851.41, 713.26) () the following circleci runs are 2 examples with dtests runs that failed due to this test failing it's assert:  https://circleci.com/gh/mkjellman/cassandra/487  https://circleci.com/gh/mkjellman/cassandra/526 <text> dtest* testtopology.test_movement* is flaky. all of the testing so far (and thus all of the current known observed failures) have been when running against trunk. when the test fails, it always due to the assert_almost_equal assert. p.s. assert_almost_equal has a comment '@params error optional margin of error. default 0.16'. i don't see any obvious notes for why the default is this magical 16% number. it looks like it was committed as part of a big bulk commit by sean mccarthy (who i can't find on jira). if anyone has any history on the magic 16% allowed delta please share!",
        "label": 321
    },
    {
        "text": "gossiper starvation <description> gossiper periodic task will get into starvation in case large sstable files need to be deleted.  indeed the sstabledeletingreference uses the same scheduledtasks pool (from storageservice) as the gossiper and other periodic tasks, but the gossiper tasks should run each second to assure correct cluster status (liveness of nodes). in case of large sstable files to be deleted (several gb) the delete operation can take more than 30 sec, thus making the whole cluster going into a wrong state where nodes are marked as not living while they are!  this will lead to unneeded additional load like hinted hand off, wrong cluster state, increase in latency. one of the possible solution is to use a separate pool for periodic and non periodic tasks.   i've implemented such change and it resolves the problem.   i can provide a patch<stacktrace> <code> <text> gossiper periodic task will get into starvation in case large sstable files need to be deleted.  indeed the sstabledeletingreference uses the same scheduledtasks pool (from storageservice) as the gossiper and other periodic tasks, but the gossiper tasks should run each second to assure correct cluster status (liveness of nodes). in case of large sstable files to be deleted (several gb) the delete operation can take more than 30 sec, thus making the whole cluster going into a wrong state where nodes are marked as not living while they are!  this will lead to unneeded additional load like hinted hand off, wrong cluster state, increase in latency. one of the possible solution is to use a separate pool for periodic and non periodic tasks.   i've implemented such change and it resolves the problem.   i can provide a patch",
        "label": 356
    },
    {
        "text": " sasi pre qa  index mode validation throws exception but do not cleanup index meta data <description> tested from build cassandra-11067 create keyspace music with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; create table music.albums (     id int primary key,     title text ); cqlsh:music> create custom index if not exists on music.albums(title) using 'org.apache.cassandra.index.sasi.sasiindex' with options = { 'mode':'normal'}; servererror: <errormessage code=0000 [server error] message=\"java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception\"> the error message is useless, by looking into the /var/log/cassandra/system.log message, the true error message is:  caused by: java.lang.illegalargumentexception: no enum constant org.apache.cassandra.index.sasi.disk.ondiskindexbuilder.mode.normal  at java.lang.enum.valueof(enum.java:238) ~[na:1.8.0_45]  but worse, even the index creation fails, its metadata is not cleaned up so that further attempt to create an index on the same column using the correct mode also fails: cqlsh:music> create custom index if not exists on music.albums(title) using 'org.apache.cassandra.index.sasi.sasiindex' with options = { 'mode':'prefix'}; servererror: <errormessage code=0000 [server error] message=\"java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception\"> the only word-around is to drop the index. the index is not displayed by describe table albums. it only appears in the system_schema.indexes table...<stacktrace> the error message is useless, by looking into the /var/log/cassandra/system.log message, the true error message is:  caused by: java.lang.illegalargumentexception: no enum constant org.apache.cassandra.index.sasi.disk.ondiskindexbuilder.mode.normal  at java.lang.enum.valueof(enum.java:238) ~[na:1.8.0_45] <code> create keyspace music with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; create table music.albums (     id int primary key,     title text ); cqlsh:music> create custom index if not exists on music.albums(title) using 'org.apache.cassandra.index.sasi.sasiindex' with options = { 'mode':'normal'}; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception'> cqlsh:music> create custom index if not exists on music.albums(title) using 'org.apache.cassandra.index.sasi.sasiindex' with options = { 'mode':'prefix'}; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception'> create keyspace music with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; create table music.albums (     id int primary key,     title text ); cqlsh:music> create custom index if not exists on music.albums(title) using 'org.apache.cassandra.index.sasi.sasiindex' with options = { 'mode':'normal'}; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception'> cqlsh:music> create custom index if not exists on music.albums(title) using 'org.apache.cassandra.index.sasi.sasiindex' with options = { 'mode':'prefix'}; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.runtimeexception: java.lang.reflect.invocationtargetexception'> <text> tested from build cassandra-11067 but worse, even the index creation fails, its metadata is not cleaned up so that further attempt to create an index on the same column using the correct mode also fails: the only word-around is to drop the index. the index is not displayed by describe table albums. it only appears in the system_schema.indexes table...",
        "label": 412
    },
    {
        "text": "include info about sstable on  compacting large row\u201d message <description> on a message like this one, it would be helpful to understand which sstable this large row is going in compacting large row abc/xyz:38956kjhawf (xyz bytes) incrementally<stacktrace> <code> compacting large row abc/xyz:38956kjhawf (xyz bytes) incrementally<text> on a message like this one, it would be helpful to understand which sstable this large row is going in ",
        "label": 482
    },
    {
        "text": "static columns break in clauses <description> if you use static columns, as implemented in cassandra-6561, then very simple select...where...in queries fail with an internal npe. create table foo (x text, y text, s text static, primary key (x,y));  insert into foo (x,y,s) values ('a','b','c');  select * from foo where x='a' and y in ('b','c');  request did not complete within rpc_timeout. error [readstage:190] 2014-02-25 14:19:16,400 cassandradaemon.java (line 196) exception in thread thread[readstage:190,5,main]  java.lang.runtimeexception: java.lang.nullpointerexception  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1900)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:722)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:141)  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:162)  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:162)  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:117)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)  at org.apache.cassandra.db.filter.slicequeryfilter$1.hasnext(slicequeryfilter.java:148)  at org.apache.cassandra.db.filter.queryfilter$2.getnext(queryfilter.java:157)  at org.apache.cassandra.db.filter.queryfilter$2.hasnext(queryfilter.java:140)  at org.apache.cassandra.utils.mergeiterator$onetoone.computenext(mergeiterator.java:200)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:185)  at org.apache.cassandra.db.filter.queryfilter.collatecolumns(queryfilter.java:122)  at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:80)  at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:72)  at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:297)  at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1550)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1379)  at org.apache.cassandra.db.keyspace.getrow(keyspace.java:327)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1341)  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1896)<stacktrace> error [readstage:190] 2014-02-25 14:19:16,400 cassandradaemon.java (line 196) exception in thread thread[readstage:190,5,main]  java.lang.runtimeexception: java.lang.nullpointerexception  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1900)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:722)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:141)  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:162)  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:162)  at org.apache.cassandra.db.filter.columnslice$navigablemapiterator.computenext(columnslice.java:117)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)  at org.apache.cassandra.db.filter.slicequeryfilter$1.hasnext(slicequeryfilter.java:148)  at org.apache.cassandra.db.filter.queryfilter$2.getnext(queryfilter.java:157)  at org.apache.cassandra.db.filter.queryfilter$2.hasnext(queryfilter.java:140)  at org.apache.cassandra.utils.mergeiterator$onetoone.computenext(mergeiterator.java:200)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:185)  at org.apache.cassandra.db.filter.queryfilter.collatecolumns(queryfilter.java:122)  at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:80)  at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:72)  at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:297)  at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1550)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1379)  at org.apache.cassandra.db.keyspace.getrow(keyspace.java:327)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1341)  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1896)<code> <text> if you use static columns, as implemented in cassandra-6561, then very simple select...where...in queries fail with an internal npe. create table foo (x text, y text, s text static, primary key (x,y));  insert into foo (x,y,s) values ('a','b','c');  select * from foo where x='a' and y in ('b','c');  request did not complete within rpc_timeout. ",
        "label": 520
    },
    {
        "text": "versioned documentation on cassandra apache org <description> services like https://readthedocs.org and http://www.javadoc.io/ make it easy to browse the documentation for a particular version or commit of various open source projects. it would be nice to be able to browse the docs for a particular release on http://cassandra.apache.org/doc/. currently it seems only cql has this at http://cassandra.apache.org/doc/old/.<stacktrace> <code> <text> services like https://readthedocs.org and http://www.javadoc.io/ make it easy to browse the documentation for a particular version or commit of various open source projects. it would be nice to be able to browse the docs for a particular release on http://cassandra.apache.org/doc/. currently it seems only cql has this at http://cassandra.apache.org/doc/old/.",
        "label": 347
    },
    {
        "text": "replaying materialized view updates from commitlog after node decommission crashes cassandra <description> this issue is reproducible through a jepsen test, runnable as lein with-profile +trunk test :only cassandra.mv-test/mv-crash-subset-decommission this test crashes/restarts nodes while decommissioning nodes. these actions are not coordinated. in 10164, we introduced a change to re-apply materialized view updates on commitlog replay. some nodes, upon restart, will crash in commitlog replay. they throw the \"trying to get the view natural endpoint on a non-data replica\" runtime exception in getviewnaturalendpoint. i added logging to getviewnaturalendpoint to show the results of replicationstrategy.getnaturalendpoints for the basetoken and viewtoken. it can be seen that these problems occur when the baseendpoints and viewendpoints are identical but do not contain the broadcast address of the local node. for example, a node at 10.0.0.5 crashes on replay of a write whose base token and view token replicas are both [10.0.0.2, 10.0.0.4, 10.0.0.6]. it seems we try to guard against this by considering pendingendpoints for the viewtoken, but this does not appear to be sufficient. i've attached the system.logs for a test run with added logging. in the attached logs, n1 is at 10.0.0.2, n2 is at 10.0.0.3, and so on. 10.0.0.6/n5 is the decommissioned node.<stacktrace> <code> lein with-profile +trunk test :only cassandra.mv-test/mv-crash-subset-decommission <text> this issue is reproducible through a jepsen test, runnable as this test crashes/restarts nodes while decommissioning nodes. these actions are not coordinated. in 10164, we introduced a change to re-apply materialized view updates on commitlog replay. some nodes, upon restart, will crash in commitlog replay. they throw the 'trying to get the view natural endpoint on a non-data replica' runtime exception in getviewnaturalendpoint. i added logging to getviewnaturalendpoint to show the results of replicationstrategy.getnaturalendpoints for the basetoken and viewtoken. it can be seen that these problems occur when the baseendpoints and viewendpoints are identical but do not contain the broadcast address of the local node. for example, a node at 10.0.0.5 crashes on replay of a write whose base token and view token replicas are both [10.0.0.2, 10.0.0.4, 10.0.0.6]. it seems we try to guard against this by considering pendingendpoints for the viewtoken, but this does not appear to be sufficient. i've attached the system.logs for a test run with added logging. in the attached logs, n1 is at 10.0.0.2, n2 is at 10.0.0.3, and so on. 10.0.0.6/n5 is the decommissioned node.",
        "label": 261
    },
    {
        "text": " patch  remove bogus assert   never false <description> code asserts that sstablescanner is closeable  final sstablescanner scanner = sstable.getscanner(filter);  scanner.seekto(startwith); assert scanner instanceof closeable; // otherwise we leak fds always is, unless null, but of course the line before would throw npe. just confusing.<stacktrace> <code> final sstablescanner scanner = sstable.getscanner(filter);  scanner.seekto(startwith); <text> code asserts that sstablescanner is closeable always is, unless null, but of course the line before would throw npe. just confusing.",
        "label": 139
    },
    {
        "text": "reduce computational complexity of processing topology changes <description> this constitutes follow-up work from cassandra-3831 where a partial improvement was committed, but the fundamental issue was not fixed. the maximum \"practical\" cluster size was significantly improved, but further work is expected to be necessary as cluster sizes grow. edit0: appended patch information. patches compare raw diff description 00_snitch_topology 00_snitch_topology.patch adds some functionality to tokenmetadata to track which endpoints and racks exist in a dc. 01_calc_natural_endpoints 01_calc_natural_endpoints.patch rewritten o(logn) implementation of calculatenaturalendpoints using the topology information from the tokenmetadata. note: these are branches managed with topgit. if you are applying the patch output manually, you will either need to filter the topgit metadata files (i.e. wget -o - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1), or remove them afterward (rm .topmsg .topdeps).<stacktrace> <code> <text> this constitutes follow-up work from cassandra-3831 where a partial improvement was committed, but the fundamental issue was not fixed. the maximum 'practical' cluster size was significantly improved, but further work is expected to be necessary as cluster sizes grow. edit0: appended patch information. note: these are branches managed with topgit. if you are applying the patch output manually, you will either need to filter the topgit metadata files (i.e. wget -o - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1), or remove them afterward (rm .topmsg .topdeps).",
        "label": 473
    },
    {
        "text": "create fqltool replay command <description> make it possible to replay the full query logs from cassandra-13983 against one or several clusters. the goal is to be able to compare different runs of production traffic against different versions/configurations of cassandra. it should be possible to take logs from several machines and replay them in \"order\" by the timestamps recorded record the results from each run to be able to compare different runs (against different clusters/versions/etc) if fqltool replay is run against 2 or more clusters, the results should be compared as we go<stacktrace> <code> <text> make it possible to replay the full query logs from cassandra-13983 against one or several clusters. the goal is to be able to compare different runs of production traffic against different versions/configurations of cassandra.",
        "label": 321
    },
    {
        "text": "npe during hh delivery when gossip turned off on target <description> probably not important bug error [optionaltasks:1] 2011-12-27 21:44:25,342 abstractcassandradaemon.java (line 138) fatal exception in thread thread[optionaltasks:1,5,main]  java.lang.nullpointerexception  at org.cliffc.high_scale_lib.nonblockinghashmap.hash(nonblockinghashmap.java:113)  at org.cliffc.high_scale_lib.nonblockinghashmap.putifmatch(nonblockinghashmap.java:553)  at org.cliffc.high_scale_lib.nonblockinghashmap.putifmatch(nonblockinghashmap.java:348)  at org.cliffc.high_scale_lib.nonblockinghashmap.putifabsent(nonblockinghashmap.java:319)  at org.cliffc.high_scale_lib.nonblockinghashset.add(nonblockinghashset.java:32)  at org.apache.cassandra.db.hintedhandoffmanager.schedulehintdelivery(hintedhandoffmanager.java:371)  at org.apache.cassandra.db.hintedhandoffmanager.schedulealldeliveries(hintedhandoffmanager.java:356)  at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:84)  at org.apache.cassandra.db.hintedhandoffmanager$1.run(hintedhandoffmanager.java:119)  at java.util.concurrent.executors$runnableadapter.call(executors.java:471)  at java.util.concurrent.futuretask$sync.innerrunandreset(futuretask.java:351)  at java.util.concurrent.futuretask.runandreset(futuretask.java:178)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:165)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:267)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:679)<stacktrace> error [optionaltasks:1] 2011-12-27 21:44:25,342 abstractcassandradaemon.java (line 138) fatal exception in thread thread[optionaltasks:1,5,main]  java.lang.nullpointerexception  at org.cliffc.high_scale_lib.nonblockinghashmap.hash(nonblockinghashmap.java:113)  at org.cliffc.high_scale_lib.nonblockinghashmap.putifmatch(nonblockinghashmap.java:553)  at org.cliffc.high_scale_lib.nonblockinghashmap.putifmatch(nonblockinghashmap.java:348)  at org.cliffc.high_scale_lib.nonblockinghashmap.putifabsent(nonblockinghashmap.java:319)  at org.cliffc.high_scale_lib.nonblockinghashset.add(nonblockinghashset.java:32)  at org.apache.cassandra.db.hintedhandoffmanager.schedulehintdelivery(hintedhandoffmanager.java:371)  at org.apache.cassandra.db.hintedhandoffmanager.schedulealldeliveries(hintedhandoffmanager.java:356)  at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:84)  at org.apache.cassandra.db.hintedhandoffmanager$1.run(hintedhandoffmanager.java:119)  at java.util.concurrent.executors$runnableadapter.call(executors.java:471)  at java.util.concurrent.futuretask$sync.innerrunandreset(futuretask.java:351)  at java.util.concurrent.futuretask.runandreset(futuretask.java:178)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:165)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:267)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:679)<code> <text> probably not important bug ",
        "label": 85
    },
    {
        "text": "cassandra nodes startup time 20x more after upgarding to x <description> compactionstrategymanage.getcompactionstrategyindex is called on each sstable at startup. and this function calls storageservice.getdiskboundaries. and getdiskboundaries calls abstractreplicationstrategy.getaddressranges.  it appears that last function can be really slow. in our environment we have 1545 tokens and with networktopologystrategy it can make 1545*1545 computations in worst case (maybe i'm wrong, but it really takes lot's of cpu). also this function can affect runtime later, cause it is called not only during startup. i've tried to implement simple cache for getdiskboundaries results and now startup time is about one minute instead of 25m, but i'm not sure if it's a good solution.<stacktrace> <code> <text> compactionstrategymanage.getcompactionstrategyindex is called on each sstable at startup. and this function calls storageservice.getdiskboundaries. and getdiskboundaries calls abstractreplicationstrategy.getaddressranges.  it appears that last function can be really slow. in our environment we have 1545 tokens and with networktopologystrategy it can make 1545*1545 computations in worst case (maybe i'm wrong, but it really takes lot's of cpu). also this function can affect runtime later, cause it is called not only during startup. i've tried to implement simple cache for getdiskboundaries results and now startup time is about one minute instead of 25m, but i'm not sure if it's a good solution.",
        "label": 321
    },
    {
        "text": "flaky test testwriteunknownresult  org apache cassandra distributed test caswritetest <description> failure observed in: https://app.circleci.com/pipelines/github/newkek/cassandra/33/workflows/54007cf7-4424-4ec1-9655-665f6044e6d1/jobs/187/tests testwriteunknownresult - org.apache.cassandra.distributed.test.caswritetest junit.framework.assertionfailederror: expecting cause to be caswriteuncertainexception at org.apache.cassandra.distributed.test.caswritetest.testwriteunknownresult(caswritetest.java:257) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)<stacktrace> testwriteunknownresult - org.apache.cassandra.distributed.test.caswritetest junit.framework.assertionfailederror: expecting cause to be caswriteuncertainexception at org.apache.cassandra.distributed.test.caswritetest.testwriteunknownresult(caswritetest.java:257) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) <code> failure observed in: https://app.circleci.com/pipelines/github/newkek/cassandra/33/workflows/54007cf7-4424-4ec1-9655-665f6044e6d1/jobs/187/tests<text> ",
        "label": 191
    },
    {
        "text": "npe in describe ring <description> not sure how much of the following is relevant besides the stack trace, but here i go: i have a 2 dc, 2 node per dc cluster. dc1 had it's seed replaced but i hadn't restarted. i upgraded to 0.8.4 in the following fashion: -edited seeds  -stopped both dc1 nodes  -upgraded jars  -started both nodes at the same time the non-seed node came up first and showed the following error. then when the seed node came up, the error went away on the non-seed node but started occurring on the seed node: error [pool-2-thread-15] 2011-08-12 22:32:27,438 cassandra.java (line 3668) internal error processing describe_ring  java.lang.nullpointerexception  at org.apache.cassandra.service.storageservice.getrangetorpcaddressmap(storageservice.java:623)  at org.apache.cassandra.thrift.cassandraserver.describe_ring(cassandraserver.java:731)  at org.apache.cassandra.thrift.cassandra$processor$describe_ring.process(cassandra.java:3664)  at org.apache.cassandra.thrift.brisk$processor.process(brisk.java:464)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:187)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)<stacktrace> error [pool-2-thread-15] 2011-08-12 22:32:27,438 cassandra.java (line 3668) internal error processing describe_ring  java.lang.nullpointerexception  at org.apache.cassandra.service.storageservice.getrangetorpcaddressmap(storageservice.java:623)  at org.apache.cassandra.thrift.cassandraserver.describe_ring(cassandraserver.java:731)  at org.apache.cassandra.thrift.cassandra$processor$describe_ring.process(cassandra.java:3664)  at org.apache.cassandra.thrift.brisk$processor.process(brisk.java:464)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:187)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)<code> <text> not sure how much of the following is relevant besides the stack trace, but here i go: i have a 2 dc, 2 node per dc cluster. dc1 had it's seed replaced but i hadn't restarted. i upgraded to 0.8.4 in the following fashion: -edited seeds  -stopped both dc1 nodes  -upgraded jars  -started both nodes at the same time the non-seed node came up first and showed the following error. then when the seed node came up, the error went away on the non-seed node but started occurring on the seed node: ",
        "label": 85
    },
    {
        "text": "error when tracing query with cqlsh <description> cqlsh isn't working for me in 2.1 when executing a query with tracing on; i get the following error myformat_colname() takes exactly 3 arguments (2 given)<stacktrace> <code> myformat_colname() takes exactly 3 arguments (2 given)<text> cqlsh isn't working for me in 2.1 when executing a query with tracing on; i get the following error ",
        "label": 453
    },
    {
        "text": "minimize key cache invalidation by compaction <description> currently, compactions invalidate key cache entries that pointed to the sstables that were compacted. this results in a sudden increase in the number of seeks necessary for reads immediately after compaction. when writing out a new sstable after a compaction, it seems like it should be possible to store a list of keys that are currently cached but would be invalidated along with their new position in the new sstable. matt dennis also seems to think this would be relatively easy to do.<stacktrace> <code> <text> currently, compactions invalidate key cache entries that pointed to the sstables that were compacted. this results in a sudden increase in the number of seeks necessary for reads immediately after compaction. when writing out a new sstable after a compaction, it seems like it should be possible to store a list of keys that are currently cached but would be invalidated along with their new position in the new sstable. matt dennis also seems to think this would be relatively easy to do.",
        "label": 274
    },
    {
        "text": "non disruptive seed node list reload <description> add a mechanism for reloading the gossiper in-memory seed node ip list without requiring a service restart. the gossiper keeps an in-memory copy of the seed node ip list and uses it during a gossip round to determine if the random node that was gossiped to is a seed node and for picking a seed node to gossip to in maybegossiptoseed. currently the gossiper seed node list is only updated when an endpoint is removed, at the start of a shadow round, and on startup. those scenarios don\u2019t handle the case of seed nodes changing ip addresses (eg. dhcp lease changes) or additional seed nodes being added to the cluster. as described in cassandra-3829 the current way to ensure that all nodes in the cluster have the same seed node list when there has been a change is to do a rolling restart of every node in the cluster. in large clusters rolling restarts can be very complicated to manage and can have performance impacts because the caches get flushed.<stacktrace> <code> <text> add a mechanism for reloading the gossiper in-memory seed node ip list without requiring a service restart. the gossiper keeps an in-memory copy of the seed node ip list and uses it during a gossip round to determine if the random node that was gossiped to is a seed node and for picking a seed node to gossip to in maybegossiptoseed. currently the gossiper seed node list is only updated when an endpoint is removed, at the start of a shadow round, and on startup. those scenarios don't handle the case of seed nodes changing ip addresses (eg. dhcp lease changes) or additional seed nodes being added to the cluster. as described in cassandra-3829 the current way to ensure that all nodes in the cluster have the same seed node list when there has been a change is to do a rolling restart of every node in the cluster. in large clusters rolling restarts can be very complicated to manage and can have performance impacts because the caches get flushed.",
        "label": 477
    },
    {
        "text": "topology test dtest fails in <description> $ export max_heap_size=\"1g\"; export heap_newsize=\"256m\"; print_debug=true nosetests --nocapture --nologcapture --verbosity=3 topology_test.py nose.config: info: ignoring files matching ['^\\\\.', '^_', '^setup\\\\.py$'] decomission_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-uhifiq fail move_single_node_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-x1q7pp ok movement_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-t6auxa error: for input string: \"\\-9223372036854775808\" -- stacktrace -- java.io.ioexception: for input string: \"\\-9223372036854775808\"         at org.apache.cassandra.service.storageservice.move(storageservice.java:3044)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.trampoline.invoke(methodutil.java:75)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.methodutil.invoke(methodutil.java:279)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)         at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)         at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)         at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)         at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)         at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)         at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)         at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)         at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)         at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)         at sun.rmi.transport.transport$1.run(transport.java:177)         at sun.rmi.transport.transport$1.run(transport.java:174)         at java.security.accesscontroller.doprivileged(native method)         at sun.rmi.transport.transport.servicecall(transport.java:173)         at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) error: for input string: \"\\-3074457345618258603\" -- stacktrace -- java.io.ioexception: for input string: \"\\-3074457345618258603\"         at org.apache.cassandra.service.storageservice.move(storageservice.java:3044)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.trampoline.invoke(methodutil.java:75)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.methodutil.invoke(methodutil.java:279)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)         at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)         at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)         at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)         at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)         at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)         at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)         at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)         at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)         at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)         at sun.rmi.transport.transport$1.run(transport.java:177)         at sun.rmi.transport.transport$1.run(transport.java:174)         at java.security.accesscontroller.doprivileged(native method)         at sun.rmi.transport.transport.servicecall(transport.java:173)         at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) error: for input string: \"\\3074457345618258602\" -- stacktrace -- java.io.ioexception: for input string: \"\\3074457345618258602\"         at org.apache.cassandra.service.storageservice.move(storageservice.java:3044)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.trampoline.invoke(methodutil.java:75)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.methodutil.invoke(methodutil.java:279)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)         at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)         at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)         at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)         at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)         at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)         at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)         at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)         at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)         at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)         at sun.rmi.transport.transport$1.run(transport.java:177)         at sun.rmi.transport.transport$1.run(transport.java:174)         at java.security.accesscontroller.doprivileged(native method)         at sun.rmi.transport.transport.servicecall(transport.java:173)         at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) fail replace_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-nuysy5 error error saving log: [errno 2] no such file or directory: '/tmp/dtest-nuysy5/test/node4/logs/system.log' error ====================================================================== error: replace_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/tools.py\", line 224, in wrapped     f(obj)   file \"/home/mshuler/git/cassandra-dtest/topology_test.py\", line 139, in replace_test     node4.start(replace_token=tokens[2])   file \"/home/mshuler/git/ccm/ccmlib/node.py\", line 427, in start     raise nodeerror(\"error starting node %s\" % self.name, process) nodeerror: error starting node node4 ====================================================================== error: replace_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/dtest.py\", line 199, in teardown     errors = list(self.__filter_errors([ msg for msg, i in node.grep_log(\"error\")]))   file \"/home/mshuler/git/ccm/ccmlib/node.py\", line 222, in grep_log     with open(self.logfilename()) as f: ioerror: [errno 2] no such file or directory: '/tmp/dtest-nuysy5/test/node4/logs/system.log' ====================================================================== fail: decomission_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/tools.py\", line 224, in wrapped     f(obj)   file \"/home/mshuler/git/cassandra-dtest/topology_test.py\", line 69, in decomission_test     assert_almost_equal(*sizes)   file \"/home/mshuler/git/cassandra-dtest/assertions.py\", line 26, in assert_almost_equal     assert vmin > vmax * (1.0 - error), \"values not within %.2f%% of the max: %s\" % (error * 100, args) assertionerror: values not within 16.00% of the max: (0, 0, 0, 0) ====================================================================== fail: movement_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/tools.py\", line 224, in wrapped     f(obj)   file \"/home/mshuler/git/cassandra-dtest/topology_test.py\", line 47, in movement_test     assert_almost_equal(sizes[0], sizes[1])   file \"/home/mshuler/git/cassandra-dtest/assertions.py\", line 26, in assert_almost_equal     assert vmin > vmax * (1.0 - error), \"values not within %.2f%% of the max: %s\" % (error * 100, args) assertionerror: values not within 16.00% of the max: (0, 0) ---------------------------------------------------------------------- ran 4 tests in 169.513s failed (errors=2, failures=2)<stacktrace> $ export max_heap_size='1g'; export heap_newsize='256m'; print_debug=true nosetests --nocapture --nologcapture --verbosity=3 topology_test.py nose.config: info: ignoring files matching ['^//.', '^_', '^setup//.py$'] decomission_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-uhifiq fail move_single_node_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-x1q7pp ok movement_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-t6auxa error: for input string: '/-9223372036854775808' -- stacktrace -- java.io.ioexception: for input string: '/-9223372036854775808'         at org.apache.cassandra.service.storageservice.move(storageservice.java:3044)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.trampoline.invoke(methodutil.java:75)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.methodutil.invoke(methodutil.java:279)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)         at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)         at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)         at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)         at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)         at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)         at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)         at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)         at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)         at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)         at sun.rmi.transport.transport$1.run(transport.java:177)         at sun.rmi.transport.transport$1.run(transport.java:174)         at java.security.accesscontroller.doprivileged(native method)         at sun.rmi.transport.transport.servicecall(transport.java:173)         at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) error: for input string: '/-3074457345618258603' -- stacktrace -- java.io.ioexception: for input string: '/-3074457345618258603'         at org.apache.cassandra.service.storageservice.move(storageservice.java:3044)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.trampoline.invoke(methodutil.java:75)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.methodutil.invoke(methodutil.java:279)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)         at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)         at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)         at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)         at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)         at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)         at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)         at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)         at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)         at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)         at sun.rmi.transport.transport$1.run(transport.java:177)         at sun.rmi.transport.transport$1.run(transport.java:174)         at java.security.accesscontroller.doprivileged(native method)         at sun.rmi.transport.transport.servicecall(transport.java:173)         at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) error: for input string: '/3074457345618258602' -- stacktrace -- java.io.ioexception: for input string: '/3074457345618258602'         at org.apache.cassandra.service.storageservice.move(storageservice.java:3044)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.trampoline.invoke(methodutil.java:75)         at sun.reflect.generatedmethodaccessor2.invoke(unknown source)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.reflect.misc.methodutil.invoke(methodutil.java:279)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)         at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)         at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)         at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)         at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)         at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)         at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)         at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)         at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)         at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)         at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)         at sun.reflect.nativemethodaccessorimpl.invoke0(native method)         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)         at java.lang.reflect.method.invoke(method.java:606)         at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)         at sun.rmi.transport.transport$1.run(transport.java:177)         at sun.rmi.transport.transport$1.run(transport.java:174)         at java.security.accesscontroller.doprivileged(native method)         at sun.rmi.transport.transport.servicecall(transport.java:173)         at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)         at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) fail replace_test (topology_test.testtopology) ... cluster ccm directory: /tmp/dtest-nuysy5 error error saving log: [errno 2] no such file or directory: '/tmp/dtest-nuysy5/test/node4/logs/system.log' error ====================================================================== error: replace_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/tools.py', line 224, in wrapped     f(obj)   file '/home/mshuler/git/cassandra-dtest/topology_test.py', line 139, in replace_test     node4.start(replace_token=tokens[2])   file '/home/mshuler/git/ccm/ccmlib/node.py', line 427, in start     raise nodeerror('error starting node %s' % self.name, process) nodeerror: error starting node node4 ====================================================================== error: replace_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/dtest.py', line 199, in teardown     errors = list(self.__filter_errors([ msg for msg, i in node.grep_log('error')]))   file '/home/mshuler/git/ccm/ccmlib/node.py', line 222, in grep_log     with open(self.logfilename()) as f: ioerror: [errno 2] no such file or directory: '/tmp/dtest-nuysy5/test/node4/logs/system.log' ====================================================================== fail: decomission_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/tools.py', line 224, in wrapped     f(obj)   file '/home/mshuler/git/cassandra-dtest/topology_test.py', line 69, in decomission_test     assert_almost_equal(*sizes)   file '/home/mshuler/git/cassandra-dtest/assertions.py', line 26, in assert_almost_equal     assert vmin > vmax * (1.0 - error), 'values not within %.2f%% of the max: %s' % (error * 100, args) assertionerror: values not within 16.00% of the max: (0, 0, 0, 0) ====================================================================== fail: movement_test (topology_test.testtopology) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/tools.py', line 224, in wrapped     f(obj)   file '/home/mshuler/git/cassandra-dtest/topology_test.py', line 47, in movement_test     assert_almost_equal(sizes[0], sizes[1])   file '/home/mshuler/git/cassandra-dtest/assertions.py', line 26, in assert_almost_equal     assert vmin > vmax * (1.0 - error), 'values not within %.2f%% of the max: %s' % (error * 100, args) assertionerror: values not within 16.00% of the max: (0, 0) ---------------------------------------------------------------------- ran 4 tests in 169.513s failed (errors=2, failures=2)<code> <text> ",
        "label": 85
    },
    {
        "text": "possible schema corruption with cql <description> hi, i've got some problems while creating schemas with cql 3.0. after that i can't even start cassandra anymore. following steps for reproduction were done on a new installation of cassandra: 1. simply create a keyspace test via \"cqlsh -3\" create keyspace test with strategy_class = 'simplestrategy' and strategy_options:replication_factor = 1; 2. add cf with composite columns via \"cqlsh -3\" create table test1 (  a int,  b int,  c int,  d int,  primary key (a, b, c)  ); 3. drop column family drop columnfamily test1; so until now everything went fine. now i'm trying to insert a slightly modified column family with the same name above. 4. create new cf via \"cqlsh -3\" create table test1 (  a int,  b int,  c int,  primary key (a, b)  ); this creation fails with following exception: java.lang.indexoutofboundsexception: index: 2, size: 2  at java.util.arraylist.rangecheck(arraylist.java:547)  at java.util.arraylist.get(arraylist.java:322)  at org.apache.cassandra.config.cfmetadata.getcolumndefinitioncomparator(cfmetadata.java:1280)  at org.apache.cassandra.config.columndefinition.fromschema(columndefinition.java:256)  at org.apache.cassandra.config.cfmetadata.addcolumndefinitionschema(cfmetadata.java:1293)  at org.apache.cassandra.config.cfmetadata.fromschema(cfmetadata.java:1225)  at org.apache.cassandra.config.ksmetadata.deserializecolumnfamilies(ksmetadata.java:294)  at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:275)  at org.apache.cassandra.db.defstable.loadfromtable(defstable.java:158)  at org.apache.cassandra.config.databasedescriptor.loadschemas(databasedescriptor.java:535)  at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:182)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:353)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106) now at this point you can't do anything anymore via cql or cli. shutting down and starting cassandra again throws same exceptions: error 14:48:41,705 exception encountered during startup  java.lang.indexoutofboundsexception: index: 2, size: 2  at java.util.arraylist.rangecheck(arraylist.java:547)  at java.util.arraylist.get(arraylist.java:322)  at org.apache.cassandra.config.cfmetadata.getcolumndefinitioncomparator(cfmetadata.java:1280)  at org.apache.cassandra.config.columndefinition.fromschema(columndefinition.java:256)  at org.apache.cassandra.config.cfmetadata.addcolumndefinitionschema(cfmetadata.java:1293)  at org.apache.cassandra.config.cfmetadata.fromschema(cfmetadata.java:1225)  at org.apache.cassandra.config.ksmetadata.deserializecolumnfamilies(ksmetadata.java:294)  at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:275)  at org.apache.cassandra.db.defstable.loadfromtable(defstable.java:158)  at org.apache.cassandra.config.databasedescriptor.loadschemas(databasedescriptor.java:535)  at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:182)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:353)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106)  java.lang.indexoutofboundsexception: index: 2, size: 2exception encountered during startup: index: 2, size: 2  at java.util.arraylist.rangecheck(arraylist.java:547)  at java.util.arraylist.get(arraylist.java:322)  at org.apache.cassandra.config.cfmetadata.getcolumndefinitioncomparator(cfmetadata.java:1280)  at org.apache.cassandra.config.columndefinition.fromschema(columndefinition.java:256)  at org.apache.cassandra.config.cfmetadata.addcolumndefinitionschema(cfmetadata.java:1293)  at org.apache.cassandra.config.cfmetadata.fromschema(cfmetadata.java:1225)  at org.apache.cassandra.config.ksmetadata.deserializecolumnfamilies(ksmetadata.java:294)  at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:275)  at org.apache.cassandra.db.defstable.loadfromtable(defstable.java:158)  at org.apache.cassandra.config.databasedescriptor.loadschemas(databasedescriptor.java:535)  at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:182)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:353)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106) actually it's the result of a slightly different problem in combination with composite columns, but i will describe this later. i've got no idea, what the problem is, there might be some corruption in table schemas, even after dropping tables. i have to delete cassandra data in order to get cassandra running again. best regards bert passek<stacktrace> java.lang.indexoutofboundsexception: index: 2, size: 2  at java.util.arraylist.rangecheck(arraylist.java:547)  at java.util.arraylist.get(arraylist.java:322)  at org.apache.cassandra.config.cfmetadata.getcolumndefinitioncomparator(cfmetadata.java:1280)  at org.apache.cassandra.config.columndefinition.fromschema(columndefinition.java:256)  at org.apache.cassandra.config.cfmetadata.addcolumndefinitionschema(cfmetadata.java:1293)  at org.apache.cassandra.config.cfmetadata.fromschema(cfmetadata.java:1225)  at org.apache.cassandra.config.ksmetadata.deserializecolumnfamilies(ksmetadata.java:294)  at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:275)  at org.apache.cassandra.db.defstable.loadfromtable(defstable.java:158)  at org.apache.cassandra.config.databasedescriptor.loadschemas(databasedescriptor.java:535)  at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:182)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:353)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106) error 14:48:41,705 exception encountered during startup  java.lang.indexoutofboundsexception: index: 2, size: 2  at java.util.arraylist.rangecheck(arraylist.java:547)  at java.util.arraylist.get(arraylist.java:322)  at org.apache.cassandra.config.cfmetadata.getcolumndefinitioncomparator(cfmetadata.java:1280)  at org.apache.cassandra.config.columndefinition.fromschema(columndefinition.java:256)  at org.apache.cassandra.config.cfmetadata.addcolumndefinitionschema(cfmetadata.java:1293)  at org.apache.cassandra.config.cfmetadata.fromschema(cfmetadata.java:1225)  at org.apache.cassandra.config.ksmetadata.deserializecolumnfamilies(ksmetadata.java:294)  at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:275)  at org.apache.cassandra.db.defstable.loadfromtable(defstable.java:158)  at org.apache.cassandra.config.databasedescriptor.loadschemas(databasedescriptor.java:535)  at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:182)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:353)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106)  java.lang.indexoutofboundsexception: index: 2, size: 2exception encountered during startup: index: 2, size: 2 at java.util.arraylist.rangecheck(arraylist.java:547)  at java.util.arraylist.get(arraylist.java:322)  at org.apache.cassandra.config.cfmetadata.getcolumndefinitioncomparator(cfmetadata.java:1280)  at org.apache.cassandra.config.columndefinition.fromschema(columndefinition.java:256)  at org.apache.cassandra.config.cfmetadata.addcolumndefinitionschema(cfmetadata.java:1293)  at org.apache.cassandra.config.cfmetadata.fromschema(cfmetadata.java:1225)  at org.apache.cassandra.config.ksmetadata.deserializecolumnfamilies(ksmetadata.java:294)  at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:275)  at org.apache.cassandra.db.defstable.loadfromtable(defstable.java:158)  at org.apache.cassandra.config.databasedescriptor.loadschemas(databasedescriptor.java:535)  at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:182)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:353)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106) <code> create keyspace test with strategy_class = 'simplestrategy' and strategy_options:replication_factor = 1; create table test1 (  a int,  b int,  c int,  d int,  primary key (a, b, c)  ); drop columnfamily test1; create table test1 (  a int,  b int,  c int,  primary key (a, b)  ); <text> hi, i've got some problems while creating schemas with cql 3.0. after that i can't even start cassandra anymore. following steps for reproduction were done on a new installation of cassandra: 1. simply create a keyspace test via 'cqlsh -3' 2. add cf with composite columns via 'cqlsh -3' 3. drop column family so until now everything went fine. now i'm trying to insert a slightly modified column family with the same name above. 4. create new cf via 'cqlsh -3' this creation fails with following exception: now at this point you can't do anything anymore via cql or cli. shutting down and starting cassandra again throws same exceptions: actually it's the result of a slightly different problem in combination with composite columns, but i will describe this later. i've got no idea, what the problem is, there might be some corruption in table schemas, even after dropping tables. i have to delete cassandra data in order to get cassandra running again. best regards bert passek",
        "label": 412
    },
    {
        "text": "adding another datacenter's node results in rows returned on first datacenter <description> on cassandra-1.0.5:  1. create a node in c* with a fresh installation and create a keyspace on that node with one column family - create keyspace test   with placement_strategy = 'simplestrategy'   and strategy_options= {replication_factor:1} ; use test;   create column family cf1; 2. insert values into cf1 - set cf1[ascii('k')][ascii('c')] = ascii('v'); get cf1[ascii('k')];   => (column=63, value=v, timestamp=1325689630397000)   returned 1 results. 3. update the strategy options from simple to networktopology with {cassandra:1, backup:1}  4. read from cf1 to make sure the options change doesn't affect anything - consistencylevel as local_quorum;   get cf1[ascii('k')];   => (column=63, value=v, timestamp=1325689630397000)   returned 1 results. 5. start a second node in the backup datacenter   6. read from cf1 again (on the first node) - consistencylevel as local_quorum;   get cf1[ascii('k')];   returned 0 results. after about 60 seconds, \"get cf1[ascii('k')]\" started to return results again. also, when running at a cl of one on 1.0's head, we were able to see issues as well. but, if more than one node was added to the second datacenter, then replication_strategy is changed, it seems okay.<stacktrace> <code> ; use test;   create column family cf1; set cf1[ascii('k')][ascii('c')] = ascii('v'); <text> on cassandra-1.0.5:  1. create a node in c* with a fresh installation and create a keyspace on that node with one column family - create keyspace test   with placement_strategy = 'simplestrategy'   and strategy_options= 2. insert values into cf1 - get cf1[ascii('k')];   => (column=63, value=v, timestamp=1325689630397000)   returned 1 results. 3. update the strategy options from simple to networktopology with  4. read from cf1 to make sure the options change doesn't affect anything - consistencylevel as local_quorum;   get cf1[ascii('k')];   => (column=63, value=v, timestamp=1325689630397000)   returned 1 results. 5. start a second node in the backup datacenter   6. read from cf1 again (on the first node) - consistencylevel as local_quorum;   get cf1[ascii('k')];   returned 0 results. after about 60 seconds, 'get cf1[ascii('k')]' started to return results again. also, when running at a cl of one on 1.0's head, we were able to see issues as well. but, if more than one node was added to the second datacenter, then replication_strategy is changed, it seems okay.",
        "label": 274
    },
    {
        "text": "cql3 predicate logic is reversed when used on a reversed column <description> example: cqlsh:test> cqlsh:test> create table testrev (         ... key text,         ... rdate timestamp,         ... num double,         ... primary key(key,rdate)         ... ) with compact storage         ...   and clustering order by(rdate desc); cqlsh:test> insert into testrev(key,rdate,num) values ('foo','2012-01-01',10.5); cqlsh:test> select key from testrev where rdate > '2012-01-02' ;  key  -----  foo  cqlsh:test> select key from testrev where rdate < '2012-01-02' ; cqlsh:test><stacktrace> <code> cqlsh:test> cqlsh:test> create table testrev (         ... key text,         ... rdate timestamp,         ... num double,         ... primary key(key,rdate)         ... ) with compact storage         ...   and clustering order by(rdate desc); cqlsh:test> insert into testrev(key,rdate,num) values ('foo','2012-01-01',10.5); cqlsh:test> select key from testrev where rdate > '2012-01-02' ;  key  -----  foo  cqlsh:test> select key from testrev where rdate < '2012-01-02' ; cqlsh:test> <text> example:",
        "label": 520
    },
    {
        "text": "possible regression of cassandra <description> some dtests like consistency_test.testaccuracy.test_network_topology_strategy_each_quorum_counters are failing with the follow auth related assertion exception [node6 error] java.lang.assertionerror: org.apache.cassandra.exceptions.invalidrequestexception: unconfigured table roles at org.apache.cassandra.auth.cassandrarolemanager.prepare(cassandrarolemanager.java:450) at org.apache.cassandra.auth.cassandrarolemanager.setup(cassandrarolemanager.java:144) at org.apache.cassandra.service.storageservice.doauthsetup(storageservice.java:1036) at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:984) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:708) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:579) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:345) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:561) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:689) caused by: org.apache.cassandra.exceptions.invalidrequestexception: unconfigured table roles at org.apache.cassandra.thrift.thriftvalidation.validatecolumnfamily(thriftvalidation.java:114) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:757) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:752) at org.apache.cassandra.auth.cassandrarolemanager.prepare(cassandrarolemanager.java:446) ... 8 more this looks very similar to cassandra-9201.<stacktrace> [node6 error] java.lang.assertionerror: org.apache.cassandra.exceptions.invalidrequestexception: unconfigured table roles at org.apache.cassandra.auth.cassandrarolemanager.prepare(cassandrarolemanager.java:450) at org.apache.cassandra.auth.cassandrarolemanager.setup(cassandrarolemanager.java:144) at org.apache.cassandra.service.storageservice.doauthsetup(storageservice.java:1036) at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:984) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:708) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:579) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:345) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:561) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:689) caused by: org.apache.cassandra.exceptions.invalidrequestexception: unconfigured table roles at org.apache.cassandra.thrift.thriftvalidation.validatecolumnfamily(thriftvalidation.java:114) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:757) at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:752) at org.apache.cassandra.auth.cassandrarolemanager.prepare(cassandrarolemanager.java:446) ... 8 more <code> <text> some dtests like consistency_test.testaccuracy.test_network_topology_strategy_each_quorum_counters are failing with the follow auth related assertion exception this looks very similar to cassandra-9201.",
        "label": 474
    },
    {
        "text": "proactive repair <description> currently cassandra supports \"read repair,\" i.e., lazy repair when a read is done. this is better than nothing but is not sufficient for some cases (e.g. catastrophic node failure where you need to rebuild all of a node's data on a new machine). dynamo uses merkle trees here. this is harder for cassandra given the cf data model but i suppose we could just hash the serialized cf value.<stacktrace> <code> <text> currently cassandra supports 'read repair,' i.e., lazy repair when a read is done. this is better than nothing but is not sufficient for some cases (e.g. catastrophic node failure where you need to rebuild all of a node's data on a new machine). dynamo uses merkle trees here. this is harder for cassandra given the cf data model but i suppose we could just hash the serialized cf value.",
        "label": 515
    },
    {
        "text": "compaction invalidates row cache <description> compactions invalidate row cache after cassandra-3862 https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/compactioniterable.java#l87<stacktrace> <code> https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/compactioniterable.java#l87<text> compactions invalidate row cache after cassandra-3862 ",
        "label": 274
    },
    {
        "text": "testall failure in org apache cassandra db columnfamilystorecqlhelpertest testdynamiccomposite <description> example failure:  http://cassci.datastax.com/job/cassandra-3.0_testall/706/testreport/org.apache.cassandra.db/columnfamilystorecqlhelpertest/testdynamiccomposite/ stacktrace junit.framework.assertionfailederror:  at org.apache.cassandra.db.columnfamilystorecqlhelpertest.testdynamiccomposite(columnfamilystorecqlhelpertest.java:636)<stacktrace> stacktrace junit.framework.assertionfailederror:  at org.apache.cassandra.db.columnfamilystorecqlhelpertest.testdynamiccomposite(columnfamilystorecqlhelpertest.java:636) <code> example failure:  http://cassci.datastax.com/job/cassandra-3.0_testall/706/testreport/org.apache.cassandra.db/columnfamilystorecqlhelpertest/testdynamiccomposite/<text> ",
        "label": 508
    },
    {
        "text": "assertion errors  memory was freed  during streaming <description> we encountered the following assertionerror (twice on the same node) during a repair : on node /172.16.63.41 info  [stream-in-/10.174.216.160] 2016-03-09 02:38:13,900 streamresultfuture.java:180 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] session with /10.174.216.160 is complete                                                                                                         warn  [stream-in-/10.174.216.160] 2016-03-09 02:38:13,900 streamresultfuture.java:207 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] stream failed                            error [stream-out-/10.174.216.160] 2016-03-09 02:38:13,906 streamsession.java:505 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] streaming error occurred                     java.lang.assertionerror: memory was freed                                                                                                                                              at org.apache.cassandra.io.util.safememory.checkbounds(safememory.java:97) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                            at org.apache.cassandra.io.util.memory.getlong(memory.java:249) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                                       at org.apache.cassandra.io.compress.compressionmetadata.gettotalsizeforsections(compressionmetadata.java:247) ~[apache-cassandra-2.1.13.jar:2.1.13]                         at org.apache.cassandra.streaming.messages.filemessageheader.size(filemessageheader.java:112) ~[apache-cassandra-2.1.13.jar:2.1.13]                                         at org.apache.cassandra.streaming.streamsession.filesent(streamsession.java:546) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                      at org.apache.cassandra.streaming.messages.outgoingfilemessage$1.serialize(outgoingfilemessage.java:50) ~[apache-cassandra-2.1.13.jar:2.1.13]                               at org.apache.cassandra.streaming.messages.outgoingfilemessage$1.serialize(outgoingfilemessage.java:41) ~[apache-cassandra-2.1.13.jar:2.1.13]                               at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:45) ~[apache-cassandra-2.1.13.jar:2.1.13]                                             at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) ~[apache-cassandra-2.1.13.jar:2.1.13]                    at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:331) ~[apache-cassandra-2.1.13.jar:2.1.13]                            at java.lang.thread.run(thread.java:745) [na:1.7.0_65]                                                                                                                  on node /10.174.216.160         error [stream-out-/172.16.63.41] 2016-03-09 02:38:14,140 streamsession.java:505 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] streaming error occurred                           java.io.ioexception: connection reset by peer                                                                                                                                       at sun.nio.ch.filedispatcherimpl.write0(native method) ~[na:1.7.0_65]                                                                                                       at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47) ~[na:1.7.0_65]                                                                                               at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:93) ~[na:1.7.0_65]                                                                                                   at sun.nio.ch.ioutil.write(ioutil.java:65) ~[na:1.7.0_65]                                                                                                                   at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:487) ~[na:1.7.0_65]                                                                                            at org.apache.cassandra.io.util.dataoutputstreamandchannel.write(dataoutputstreamandchannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                              at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]                 at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:323) [apache-cassandra-2.1.13.jar:2.1.13]                         at java.lang.thread.run(thread.java:745) [na:1.7.0_65]                                                                                                              info  [stream-in-/172.16.63.41] 2016-03-09 02:38:14,142 streamresultfuture.java:180 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] session with /172.16.63.41 is complete warn  [stream-in-/172.16.63.41] 2016-03-09 02:38:14,142 streamresultfuture.java:207 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] stream failed                          error [stream-out-/172.16.63.41] 2016-03-09 02:38:14,143 streamsession.java:505 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] streaming error occurred                   java.io.ioexception: broken pipe                                                                                                                                                    at sun.nio.ch.filedispatcherimpl.write0(native method) ~[na:1.7.0_65]                                                                                                       at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47) ~[na:1.7.0_65]                                                                                               at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:93) ~[na:1.7.0_65]                                                                                                   at sun.nio.ch.ioutil.write(ioutil.java:65) ~[na:1.7.0_65]                                                                                                                   at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:487) ~[na:1.7.0_65]                                                                                            at org.apache.cassandra.io.util.dataoutputstreamandchannel.write(dataoutputstreamandchannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                              at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]                 at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:331) [apache-cassandra-2.1.13.jar:2.1.13]                         at java.lang.thread.run(thread.java:745) [na:1.7.0_65]     <stacktrace> info  [stream-in-/10.174.216.160] 2016-03-09 02:38:13,900 streamresultfuture.java:180 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] session with /10.174.216.160 is complete                                                                                                         warn  [stream-in-/10.174.216.160] 2016-03-09 02:38:13,900 streamresultfuture.java:207 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] stream failed                            error [stream-out-/10.174.216.160] 2016-03-09 02:38:13,906 streamsession.java:505 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] streaming error occurred                     java.lang.assertionerror: memory was freed                                                                                                                                              at org.apache.cassandra.io.util.safememory.checkbounds(safememory.java:97) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                            at org.apache.cassandra.io.util.memory.getlong(memory.java:249) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                                       at org.apache.cassandra.io.compress.compressionmetadata.gettotalsizeforsections(compressionmetadata.java:247) ~[apache-cassandra-2.1.13.jar:2.1.13]                         at org.apache.cassandra.streaming.messages.filemessageheader.size(filemessageheader.java:112) ~[apache-cassandra-2.1.13.jar:2.1.13]                                         at org.apache.cassandra.streaming.streamsession.filesent(streamsession.java:546) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                      at org.apache.cassandra.streaming.messages.outgoingfilemessage$1.serialize(outgoingfilemessage.java:50) ~[apache-cassandra-2.1.13.jar:2.1.13]                               at org.apache.cassandra.streaming.messages.outgoingfilemessage$1.serialize(outgoingfilemessage.java:41) ~[apache-cassandra-2.1.13.jar:2.1.13]                               at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:45) ~[apache-cassandra-2.1.13.jar:2.1.13]                                             at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) ~[apache-cassandra-2.1.13.jar:2.1.13]                    at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:331) ~[apache-cassandra-2.1.13.jar:2.1.13]                            at java.lang.thread.run(thread.java:745) [na:1.7.0_65]                                                                                                                          error [stream-out-/172.16.63.41] 2016-03-09 02:38:14,140 streamsession.java:505 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] streaming error occurred                           java.io.ioexception: connection reset by peer                                                                                                                                       at sun.nio.ch.filedispatcherimpl.write0(native method) ~[na:1.7.0_65]                                                                                                       at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47) ~[na:1.7.0_65]                                                                                               at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:93) ~[na:1.7.0_65]                                                                                                   at sun.nio.ch.ioutil.write(ioutil.java:65) ~[na:1.7.0_65]                                                                                                                   at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:487) ~[na:1.7.0_65]                                                                                            at org.apache.cassandra.io.util.dataoutputstreamandchannel.write(dataoutputstreamandchannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                              at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]                 at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:323) [apache-cassandra-2.1.13.jar:2.1.13]                         at java.lang.thread.run(thread.java:745) [na:1.7.0_65]                                                                                                              info  [stream-in-/172.16.63.41] 2016-03-09 02:38:14,142 streamresultfuture.java:180 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] session with /172.16.63.41 is complete warn  [stream-in-/172.16.63.41] 2016-03-09 02:38:14,142 streamresultfuture.java:207 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] stream failed                          error [stream-out-/172.16.63.41] 2016-03-09 02:38:14,143 streamsession.java:505 - [stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] streaming error occurred                   java.io.ioexception: broken pipe                                                                                                                                                    at sun.nio.ch.filedispatcherimpl.write0(native method) ~[na:1.7.0_65]                                                                                                       at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47) ~[na:1.7.0_65]                                                                                               at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:93) ~[na:1.7.0_65]                                                                                                   at sun.nio.ch.ioutil.write(ioutil.java:65) ~[na:1.7.0_65]                                                                                                                   at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:487) ~[na:1.7.0_65]                                                                                            at org.apache.cassandra.io.util.dataoutputstreamandchannel.write(dataoutputstreamandchannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                              at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]                 at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:331) [apache-cassandra-2.1.13.jar:2.1.13]                         at java.lang.thread.run(thread.java:745) [na:1.7.0_65]      <code> on node /172.16.63.41 on node /10.174.216.160<text> we encountered the following assertionerror (twice on the same node) during a repair : ",
        "label": 409
    },
    {
        "text": "add warn when there are a lot of tombstones in a query <description> now that we count the number of tombstones hit (so it can go in tracing), can we pick some threshold (or make it configurable with 0 being don't warn), and spit out a warning saying \"just went through 10000 tombstones in partition xyz\". right now if you are having gc problems because some row got a bunch of tombstones, you can turn on server side tracing, and hope the bad query gets in there, or you can keep making heap dumps, dig through them, and hope you catch the query in there. i have seen code problems at multiple places causing this same issue (some code causing way more tombstones than it should, for just one row). and it is a pita+luck to debug it right now.<stacktrace> <code> <text> now that we count the number of tombstones hit (so it can go in tracing), can we pick some threshold (or make it configurable with 0 being don't warn), and spit out a warning saying 'just went through 10000 tombstones in partition xyz'. right now if you are having gc problems because some row got a bunch of tombstones, you can turn on server side tracing, and hope the bad query gets in there, or you can keep making heap dumps, dig through them, and hope you catch the query in there. i have seen code problems at multiple places causing this same issue (some code causing way more tombstones than it should, for just one row). and it is a pita+luck to debug it right now.",
        "label": 466
    },
    {
        "text": "avoid death by tombstone by default <description> we added warnings to 1.2 (cassandra-6042); for 2.0 we should go farther and drop requests (with an error logged) that exceed the threshold. users who want to tread dangerously are free to crank the threshold up, but \"i queried a lot of tombstones and cassandra fell over\" is possibly the number one way of killing cassandra nodes right now.<stacktrace> <code> <text> we added warnings to 1.2 (cassandra-6042); for 2.0 we should go farther and drop requests (with an error logged) that exceed the threshold. users who want to tread dangerously are free to crank the threshold up, but 'i queried a lot of tombstones and cassandra fell over' is possibly the number one way of killing cassandra nodes right now.",
        "label": 274
    },
    {
        "text": " patch  add missing break in nodecmd's command dispatching for setstreamthroughput <description> code falls thru setstreamthroughput into rebuild case.<stacktrace> <code> <text> code falls thru setstreamthroughput into rebuild case.",
        "label": 139
    },
    {
        "text": "improve testing on macos by eliminating sigar logging <description> the changes introduced in cassandra-7838 (resolved; fixed; 2.2.0 beta 1): \"warn user when os settings are poor / integrate sigar\" are not mac friendly. info  [main] 2016-10-18t11:20:10,330 sigarlibrary.java:44 - initializing sigar library debug [main] 2016-10-18t11:20:10,342 sigarlog.java:60 - no libsigar-universal64-macosx.dylib in java.library.path org.hyperic.sigar.sigarexception: no libsigar-universal64-macosx.dylib in java.library.path         at org.hyperic.sigar.sigar.loadlibrary(sigar.java:172) ~[sigar-1.6.4.jar:?]         at org.hyperic.sigar.sigar.<clinit>(sigar.java:100) [sigar-1.6.4.jar:?]         at org.apache.cassandra.utils.sigarlibrary.<init>(sigarlibrary.java:47) [main/:?]         at org.apache.cassandra.utils.sigarlibrary.<clinit>(sigarlibrary.java:28) [main/:?]         at org.apache.cassandra.utils.uuidgen.hash(uuidgen.java:363) [main/:?]         at org.apache.cassandra.utils.uuidgen.makenode(uuidgen.java:342) [main/:?]         at org.apache.cassandra.utils.uuidgen.makeclockseqandnode(uuidgen.java:291) [main/:?]         at org.apache.cassandra.utils.uuidgen.<clinit>(uuidgen.java:42) [main/:?]         at org.apache.cassandra.config.cfmetadata$builder.build(cfmetadata.java:1278) [main/:?]         at org.apache.cassandra.schemaloader.standardcfmd(schemaloader.java:369) [classes/:?]         at org.apache.cassandra.schemaloader.standardcfmd(schemaloader.java:356) [classes/:?]         at org.apache.cassandra.schemaloader.standardcfmd(schemaloader.java:351) [classes/:?]         at org.apache.cassandra.batchlog.batchtest.defineschema(batchtest.java:59) [classes/:?]         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) ~[?:1.8.0_66]         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) ~[?:1.8.0_66]         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) ~[?:1.8.0_66]         at java.lang.reflect.method.invoke(method.java:497) ~[?:1.8.0_66]         at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44) [junit-4.6.jar:?]         at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) [junit-4.6.jar:?]         at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41) [junit-4.6.jar:?]         at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:27) [junit-4.6.jar:?]         at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31) [junit-4.6.jar:?]         at org.junit.runners.parentrunner.run(parentrunner.java:220) [junit-4.6.jar:?]         at junit.framework.junit4testadapter.run(junit4testadapter.java:39) [junit-4.6.jar:?]         at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:535) [ant-junit.jar:?]         at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:1182) [ant-junit.jar:?]         at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:1033) [ant-junit.jar:?] info  [main] 2016-10-18t11:20:10,350 sigarlibrary.java:57 - could not initialize sigar library org.hyperic.sigar.sigar.getfilesystemlistnative()[lorg/hyperic/sigar/filesystem; there are 2 issues addressed by the attached patch: 1. create platform aware (windows, darwin, linux) implementations of clibrary (for instance clibrary today assumes all platforms have support for posix_fadvise but this doesn't exist in the darwin kernel). if methods are defined with the \"native\" jni keyword in java when the class is loaded it will cause our jna check to fail incorrectly making all of clibrary \"disabled\" even though because jnaavailable = false even though on a platform like darwin all of the native methods except posix_fadvise are supported. 2. replace sigar usage to get current pid with calls to clibrary/native equivalent \u2013 and fall back to sigar for platforms like windows who don't have that support with jdk8 (and without a clibrary equivalent)<stacktrace> info  [main] 2016-10-18t11:20:10,330 sigarlibrary.java:44 - initializing sigar library debug [main] 2016-10-18t11:20:10,342 sigarlog.java:60 - no libsigar-universal64-macosx.dylib in java.library.path org.hyperic.sigar.sigarexception: no libsigar-universal64-macosx.dylib in java.library.path         at org.hyperic.sigar.sigar.loadlibrary(sigar.java:172) ~[sigar-1.6.4.jar:?]         at org.hyperic.sigar.sigar.<clinit>(sigar.java:100) [sigar-1.6.4.jar:?]         at org.apache.cassandra.utils.sigarlibrary.<init>(sigarlibrary.java:47) [main/:?]         at org.apache.cassandra.utils.sigarlibrary.<clinit>(sigarlibrary.java:28) [main/:?]         at org.apache.cassandra.utils.uuidgen.hash(uuidgen.java:363) [main/:?]         at org.apache.cassandra.utils.uuidgen.makenode(uuidgen.java:342) [main/:?]         at org.apache.cassandra.utils.uuidgen.makeclockseqandnode(uuidgen.java:291) [main/:?]         at org.apache.cassandra.utils.uuidgen.<clinit>(uuidgen.java:42) [main/:?]         at org.apache.cassandra.config.cfmetadata$builder.build(cfmetadata.java:1278) [main/:?]         at org.apache.cassandra.schemaloader.standardcfmd(schemaloader.java:369) [classes/:?]         at org.apache.cassandra.schemaloader.standardcfmd(schemaloader.java:356) [classes/:?]         at org.apache.cassandra.schemaloader.standardcfmd(schemaloader.java:351) [classes/:?]         at org.apache.cassandra.batchlog.batchtest.defineschema(batchtest.java:59) [classes/:?]         at sun.reflect.nativemethodaccessorimpl.invoke0(native method) ~[?:1.8.0_66]         at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) ~[?:1.8.0_66]         at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) ~[?:1.8.0_66]         at java.lang.reflect.method.invoke(method.java:497) ~[?:1.8.0_66]         at org.junit.runners.model.frameworkmethod$1.runreflectivecall(frameworkmethod.java:44) [junit-4.6.jar:?]         at org.junit.internal.runners.model.reflectivecallable.run(reflectivecallable.java:15) [junit-4.6.jar:?]         at org.junit.runners.model.frameworkmethod.invokeexplosively(frameworkmethod.java:41) [junit-4.6.jar:?]         at org.junit.internal.runners.statements.runbefores.evaluate(runbefores.java:27) [junit-4.6.jar:?]         at org.junit.internal.runners.statements.runafters.evaluate(runafters.java:31) [junit-4.6.jar:?]         at org.junit.runners.parentrunner.run(parentrunner.java:220) [junit-4.6.jar:?]         at junit.framework.junit4testadapter.run(junit4testadapter.java:39) [junit-4.6.jar:?]         at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.run(junittestrunner.java:535) [ant-junit.jar:?]         at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.launch(junittestrunner.java:1182) [ant-junit.jar:?]         at org.apache.tools.ant.taskdefs.optional.junit.junittestrunner.main(junittestrunner.java:1033) [ant-junit.jar:?] info  [main] 2016-10-18t11:20:10,350 sigarlibrary.java:57 - could not initialize sigar library org.hyperic.sigar.sigar.getfilesystemlistnative()[lorg/hyperic/sigar/filesystem; <code> <text> the changes introduced in cassandra-7838 (resolved; fixed; 2.2.0 beta 1): 'warn user when os settings are poor / integrate sigar' are not mac friendly. there are 2 issues addressed by the attached patch:",
        "label": 344
    },
    {
        "text": "race in sstable ref counting during streaming failures <description> we have a seen a machine in prod whose all read threads are blocked(spinning) on trying to acquire the reference lock on stables. there are also some stream sessions which are doing the same.   on looking at the heap dump, we could see that a live sstable which is part of the view has a ref count = 0. this sstable is also not compacting or is part of any failed compaction. on looking through the code, we could see that if ref goes to zero and the stable is part of the view, all reader threads will spin forever. on further looking through the code of streaming, we could see that if streamtransfertask.complete is called after closesession has been called due to error in outgoingmessagehandler, it will double decrement the ref count of an sstable. this race can happen and we see through exception in logs that closesession was triggered by outgoingmessagehandler. the fix for this is very simple i think. in streamtransfertask.abort, we can remove a file from \"files\u201d before decrementing the ref count. this will avoid this race.<stacktrace> <code> <text> we have a seen a machine in prod whose all read threads are blocked(spinning) on trying to acquire the reference lock on stables. there are also some stream sessions which are doing the same.   on looking at the heap dump, we could see that a live sstable which is part of the view has a ref count = 0. this sstable is also not compacting or is part of any failed compaction. on looking through the code, we could see that if ref goes to zero and the stable is part of the view, all reader threads will spin forever. on further looking through the code of streaming, we could see that if streamtransfertask.complete is called after closesession has been called due to error in outgoingmessagehandler, it will double decrement the ref count of an sstable. this race can happen and we see through exception in logs that closesession was triggered by outgoingmessagehandler. the fix for this is very simple i think. in streamtransfertask.abort, we can remove a file from 'files' before decrementing the ref count. this will avoid this race.",
        "label": 67
    },
    {
        "text": "org apache cassandra distributed test readrepairtest movingtokenreadrepairtest passes because the message drop filters are not dropping the messages <description> the test tries to block messages from node 4 to 3 but used verb ordinal rather than id; this causes the verbs to sent to node 3 and allow all replies to happen. if you fix the test to use id (and actually filter), then the test fails patch to show the issue diff --git a/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java b/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java index f0c82b85e1..55d2e8f303 100644 --- a/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java +++ b/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java @@ -30,14 +30,14 @@ import org.apache.cassandra.dht.token; import org.apache.cassandra.distributed.cluster; import org.apache.cassandra.distributed.api.consistencylevel; import org.apache.cassandra.distributed.api.icluster; -import org.apache.cassandra.distributed.shared.networktopology; import org.apache.cassandra.locator.inetaddressandport; import org.apache.cassandra.service.pendingrangecalculatorservice; import org.apache.cassandra.service.storageservice; +import static org.apache.cassandra.distributed.shared.assertutils.assertrows; +import static org.apache.cassandra.distributed.shared.assertutils.row; import static org.apache.cassandra.net.verb.read_repair_req; import static org.apache.cassandra.net.verb.read_req; -import static org.apache.cassandra.distributed.shared.assertutils.*; public class readrepairtest extends testbaseimpl { @@ -115,8 +115,8 @@ public class readrepairtest extends testbaseimpl             // prevent #4 from reading or writing to #3, so our quorum must contain #2 and #4             // since #1 is taking over the range, this means any read-repair must make it to #1 as well -            cluster.filters().verbs(read_req.ordinal()).from(4).to(3).drop(); -            cluster.filters().verbs(read_repair_req.ordinal()).from(4).to(3).drop(); +            cluster.filters().verbs(read_req.id).from(4).to(3).drop(); +            cluster.filters().verbs(read_repair_req.id).from(4).to(3).drop();             assertrows(cluster.coordinator(4).execute(\"select * from \" + keyspace + \".tbl where pk = ?\",                                                       consistencylevel.all, i),                        row(i, 1, 1)); exception org.apache.cassandra.exceptions.readtimeoutexception: operation timed out - received only 2 responses. at org.apache.cassandra.service.reads.readcallback.awaitresults(readcallback.java:120) at org.apache.cassandra.service.reads.abstractreadexecutor.awaitresponses(abstractreadexecutor.java:373) at org.apache.cassandra.service.storageproxy.fetchrows(storageproxy.java:1823) at org.apache.cassandra.service.storageproxy.readregular(storageproxy.java:1713) at org.apache.cassandra.service.storageproxy.read(storageproxy.java:1630) at org.apache.cassandra.db.singlepartitionreadcommand$group.execute(singlepartitionreadcommand.java:1123) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:294) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:246) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:88) at org.apache.cassandra.distributed.impl.coordinator.executeinternal(coordinator.java:100) at org.apache.cassandra.distributed.impl.coordinator.lambda$executewithresult$0(coordinator.java:62) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) at java.lang.thread.run(thread.java:748)<stacktrace> org.apache.cassandra.exceptions.readtimeoutexception: operation timed out - received only 2 responses. at org.apache.cassandra.service.reads.readcallback.awaitresults(readcallback.java:120) at org.apache.cassandra.service.reads.abstractreadexecutor.awaitresponses(abstractreadexecutor.java:373) at org.apache.cassandra.service.storageproxy.fetchrows(storageproxy.java:1823) at org.apache.cassandra.service.storageproxy.readregular(storageproxy.java:1713) at org.apache.cassandra.service.storageproxy.read(storageproxy.java:1630) at org.apache.cassandra.db.singlepartitionreadcommand$group.execute(singlepartitionreadcommand.java:1123) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:294) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:246) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:88) at org.apache.cassandra.distributed.impl.coordinator.executeinternal(coordinator.java:100) at org.apache.cassandra.distributed.impl.coordinator.lambda$executewithresult$0(coordinator.java:62) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) at java.lang.thread.run(thread.java:748) <code> diff --git a/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java b/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java index f0c82b85e1..55d2e8f303 100644 --- a/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java +++ b/test/distributed/org/apache/cassandra/distributed/test/readrepairtest.java @@ -30,14 +30,14 @@ import org.apache.cassandra.dht.token; import org.apache.cassandra.distributed.cluster; import org.apache.cassandra.distributed.api.consistencylevel; import org.apache.cassandra.distributed.api.icluster; -import org.apache.cassandra.distributed.shared.networktopology; import org.apache.cassandra.locator.inetaddressandport; import org.apache.cassandra.service.pendingrangecalculatorservice; import org.apache.cassandra.service.storageservice; +import static org.apache.cassandra.distributed.shared.assertutils.assertrows; +import static org.apache.cassandra.distributed.shared.assertutils.row; import static org.apache.cassandra.net.verb.read_repair_req; import static org.apache.cassandra.net.verb.read_req; -import static org.apache.cassandra.distributed.shared.assertutils.*; public class readrepairtest extends testbaseimpl { @@ -115,8 +115,8 @@ public class readrepairtest extends testbaseimpl             // prevent #4 from reading or writing to #3, so our quorum must contain #2 and #4             // since #1 is taking over the range, this means any read-repair must make it to #1 as well -            cluster.filters().verbs(read_req.ordinal()).from(4).to(3).drop(); -            cluster.filters().verbs(read_repair_req.ordinal()).from(4).to(3).drop(); +            cluster.filters().verbs(read_req.id).from(4).to(3).drop(); +            cluster.filters().verbs(read_repair_req.id).from(4).to(3).drop();             assertrows(cluster.coordinator(4).execute('select * from ' + keyspace + '.tbl where pk = ?',                                                       consistencylevel.all, i),                        row(i, 1, 1)); <text> the test tries to block messages from node 4 to 3 but used verb ordinal rather than id; this causes the verbs to sent to node 3 and allow all replies to happen. if you fix the test to use id (and actually filter), then the test fails patch to show the issue exception",
        "label": 474
    },
    {
        "text": " patch  use cross platform new lines in printf calls <description> code uses printf(\"\\n\") for new lines, should use \"%n\" instead.<stacktrace> <code> <text> code uses printf('/n') for new lines, should use '%n' instead.",
        "label": 139
    },
    {
        "text": "counter decrements require a space around the minus sign but not around the plus sign <description> update validation_cf_counter set test=test+1 where id='test_key' => success  update validation_cf_counter set test=test + 1 where id='test_key' => success  update validation_cf_counter set test=test - 1 where id='test_key' => success  update validation_cf_counter set test=test-1 where id='test_key' => failure (line 1:38 no viable alternative at input 'test')<stacktrace> <code> update validation_cf_counter set test=test+1 where id='test_key' => success  update validation_cf_counter set test=test + 1 where id='test_key' => success  update validation_cf_counter set test=test - 1 where id='test_key' => success  update validation_cf_counter set test=test-1 where id='test_key' => failure (line 1:38 no viable alternative at input 'test')<text> ",
        "label": 412
    },
    {
        "text": "nodetool rebuild creates an additional rebuild session even if there is one already running <description> if a nodetool rebuild session is started and the shell session is finished for whatever reason, a second nodetool rebuild will spawn a second rebuild filestream dc2-s1-100-29:~ # ps aux | grep nodetool  root 10304 0.0 0.0 4532 560 pts/3 s+ 05:23 0:00 grep nodetool  dds-user 20946 0.0 0.0 21180 1880 ? s 04:39 0:00 /bin/sh /usr/share/dse/resources/cassandra/bin/nodetool rebuild group10 <---- there is only one rebuild running dc2-s1-100-29:~ # nodetool netstats | grep -v /var/local/cassandra  mode: normal  rebuild 818307b0-d9ba-11e4-8d4c-7bce93ffad70 <------ does this represent one rebuild?  /10.96.100.22  receiving 63 files, 221542605741 bytes total  /10.96.100.26  receiving 48 files, 47712285610 bytes total  /10.96.100.25  /10.96.100.23  receiving 57 files, 127515362783 bytes total  /10.96.100.27  /10.96.100.24  rebuild 7bf9fcd0-d9bb-11e4-8d4c-7bce93ffad70 <------- does this represent a second rebuild?  /10.96.100.25  /10.96.100.26  receiving 56 files, 47717905924 bytes total  /10.96.100.24  /10.96.100.22  receiving 61 files, 221558642440 bytes total  /10.96.100.23  receiving 62 files, 127528841272 bytes total  /10.96.100.27  read repair statistics:  attempted: 0  mismatch (blocking): 0  mismatch (background): 0  pool name active pending completed  commands n/a 0 2151322  responses n/a 0 3343981<stacktrace> <code> dc2-s1-100-29:~ # ps aux | grep nodetool  root 10304 0.0 0.0 4532 560 pts/3 s+ 05:23 0:00 grep nodetool  dds-user 20946 0.0 0.0 21180 1880 ? s 04:39 0:00 /bin/sh /usr/share/dse/resources/cassandra/bin/nodetool rebuild group10 <---- there is only one rebuild running dc2-s1-100-29:~ # nodetool netstats | grep -v /var/local/cassandra  mode: normal  rebuild 818307b0-d9ba-11e4-8d4c-7bce93ffad70 <------ does this represent one rebuild?  /10.96.100.22  receiving 63 files, 221542605741 bytes total  /10.96.100.26  receiving 48 files, 47712285610 bytes total  /10.96.100.25  /10.96.100.23  receiving 57 files, 127515362783 bytes total  /10.96.100.27  /10.96.100.24  rebuild 7bf9fcd0-d9bb-11e4-8d4c-7bce93ffad70 <------- does this represent a second rebuild?  /10.96.100.25  /10.96.100.26  receiving 56 files, 47717905924 bytes total  /10.96.100.24  /10.96.100.22  receiving 61 files, 221558642440 bytes total  /10.96.100.23  receiving 62 files, 127528841272 bytes total  /10.96.100.27  read repair statistics:  attempted: 0  mismatch (blocking): 0  mismatch (background): 0  pool name active pending completed  commands n/a 0 2151322  responses n/a 0 3343981 <text> if a nodetool rebuild session is started and the shell session is finished for whatever reason, a second nodetool rebuild will spawn a second rebuild filestream",
        "label": 577
    },
    {
        "text": "java agent option missing in cassandra bat file <description> this option must be included in cassandra.bat: -javaagent:%cassandra_home%\\lib\\jamm-0.2.2.jar otherwise you see the following warnings in cassandra log: warn 12:02:32,478 memorymeter uninitialized (jamm not specified as java agent); assuming liveratio of 10.0. usually this means cassandra-env.sh disabled jamm because you are using a buggy jre; upgrade to the sun jre instead<stacktrace> <code> -javaagent:%cassandra_home%/lib/jamm-0.2.2.jar <text> this option must be included in cassandra.bat: otherwise you see the following warnings in cassandra log: warn 12:02:32,478 memorymeter uninitialized (jamm not specified as java agent); assuming liveratio of 10.0. usually this means cassandra-env.sh disabled jamm because you are using a buggy jre; upgrade to the sun jre instead",
        "label": 595
    },
    {
        "text": "keyspace does not show in describe list  if create query times out <description> here is the snapshot of the overall issue : whiterabbit@whiterabbit:~/cassandra/bin$ ccm create 'demo2' -v binary:2.1.7 -n 3 -s -d   current cluster is now: demo2  whiterabbit@whiterabbit:~/cassandra/bin$ ccm list  demo_1node  *demo2  whiterabbit@whiterabbit:~/cassandra/bin$ ccm status  cluster: 'demo2'  ----------------  node1: up  node3: up  node2: up  whiterabbit@whiterabbit:~/cassandra/bin$ ccm node2 nodetool version releaseversion: 2.1.7 whiterabbit@whiterabbit:~/cassandra/bin$ ccm node2 stop  whiterabbit@whiterabbit:~/cassandra/bin$ ccm status  cluster: 'demo2'  ----------------  node1: up  node3: up  node2: down  whiterabbit@whiterabbit:~/cassandra/bin$ ccm node1 cqlsh  connected to demo2 at 127.0.0.1:9042.  [cqlsh 5.0.1 | cassandra 2.1.7 | cql spec 3.2.0 | native protocol v3]  use help for help.  cqlsh> describe keyspaces; system_traces system cqlsh> create keyspace training with replication= {'class':'simplestrategy','replication_factor':1} ;  operationtimedout: errors={}, last_host=127.0.0.1  cqlsh> create keyspace training with replication= {'class':'simplestrategy','replication_factor':1} ;  alreadyexists: keyspace 'training' already exists  cqlsh> describe keyspaces; system_traces system cqlsh><stacktrace> <code> whiterabbit@whiterabbit:~/cassandra/bin$ ccm create 'demo2' -v binary:2.1.7 -n 3 -s -d   current cluster is now: demo2  whiterabbit@whiterabbit:~/cassandra/bin$ ccm list  demo_1node  *demo2  whiterabbit@whiterabbit:~/cassandra/bin$ ccm status  cluster: 'demo2'  ----------------  node1: up  node3: up  node2: up  whiterabbit@whiterabbit:~/cassandra/bin$ ccm node2 nodetool version releaseversion: 2.1.7 whiterabbit@whiterabbit:~/cassandra/bin$ ccm node2 stop  whiterabbit@whiterabbit:~/cassandra/bin$ ccm status  cluster: 'demo2'  ----------------  node1: up  node3: up  node2: down  whiterabbit@whiterabbit:~/cassandra/bin$ ccm node1 cqlsh  connected to demo2 at 127.0.0.1:9042.  [cqlsh 5.0.1 | cassandra 2.1.7 | cql spec 3.2.0 | native protocol v3]  use help for help.  cqlsh> describe keyspaces; ;  operationtimedout: errors={}, last_host=127.0.0.1  cqlsh> create keyspace training with replication= cqlsh><text> here is the snapshot of the overall issue : system_traces system cqlsh> create keyspace training with replication= ;  alreadyexists: keyspace 'training' already exists  cqlsh> describe keyspaces; system_traces system ",
        "label": 409
    },
    {
        "text": "tombstone error warning does not log partition key <description> log partition key if read fails due to the error threshold on read tombstoned cells. right now i can specify a warning and an error threshold for c* when reading from a partition with many tombstones. if the query reads more than the \u201cwarning threshold\u201d then c* writes a warning to the log with the partition key. but if a query reads more than the \u201cerror threshold\u201d then c* aborts the query and writes to the log \u2013 but not the partition key, this time. what i am missing is: could c* also please write the partition key in case of query abort due to tombstones?<stacktrace> <code> <text> log partition key if read fails due to the error threshold on read tombstoned cells. right now i can specify a warning and an error threshold for c* when reading from a partition with many tombstones. if the query reads more than the 'warning threshold' then c* writes a warning to the log with the partition key. but if a query reads more than the 'error threshold' then c* aborts the query and writes to the log - but not the partition key, this time. what i am missing is: could c* also please write the partition key in case of query abort due to tombstones?",
        "label": 88
    },
    {
        "text": "can't start a node with row cache size in mb <description> i consistently get the following error when trying to run 'bin/cassandra': error 12:20:28,144 fatal exception during initialization org.apache.cassandra.config.configurationexception: found system table files, but they couldn't be loaded! at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:279) at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:174) at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:367) at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:107)<stacktrace> error 12:20:28,144 fatal exception during initialization org.apache.cassandra.config.configurationexception: found system table files, but they couldn't be loaded! at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:279) at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:174) at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:367) at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:107) <code> <text> i consistently get the following error when trying to run 'bin/cassandra':",
        "label": 577
    },
    {
        "text": "nodetool hangs  doesn't return prompt  if you specify a table that doesn't exist or a ks that has no cf's <description> invalid cf error 02:18:18,904 fatal exception in thread thread[antientropystage:3,5,main] java.lang.illegalargumentexception: unknown table/cf pair (stresskeyspace.stressstandard) at org.apache.cassandra.db.table.getcolumnfamilystore(table.java:147) at org.apache.cassandra.service.antientropyservice$treerequestverbhandler.doverb(antientropyservice.java:601) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:59) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) empty ks  info 02:19:21,483 waiting for repair requests: []  info 02:19:21,484 waiting for repair requests: []  info 02:19:21,484 waiting for repair requests: []<stacktrace> error 02:18:18,904 fatal exception in thread thread[antientropystage:3,5,main] java.lang.illegalargumentexception: unknown table/cf pair (stresskeyspace.stressstandard) at org.apache.cassandra.db.table.getcolumnfamilystore(table.java:147) at org.apache.cassandra.service.antientropyservice$treerequestverbhandler.doverb(antientropyservice.java:601) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:59) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) <code>  info 02:19:21,483 waiting for repair requests: []  info 02:19:21,484 waiting for repair requests: []  info 02:19:21,484 waiting for repair requests: [] <text> invalid cf empty ks",
        "label": 577
    },
    {
        "text": "cqlsstablewriter addrow map string  object  values  does not work as documented  <description> there are 2 bugs in the method addrow(map<string, object> values) first issue is that the map <b>must</b> contain all the column names as keys in the map otherwise the addrow fails (with invalidrequestexception \"invalid number of arguments, expecting %d values but got %d\"). second issue is that the keys in the map must be in lower-case otherwise they may not be found in the map, which will result in a npe during decompose. suggested solution: fix the addrow method with: public cqlsstablewriter addrow(map<string, object> values)     throws invalidrequestexception, ioexception {     int size = boundnames.size();     map<string, bytebuffer> rawvalues = new hashmap<>(size);     for (int i = 0; i < size; i++) {         columnspecification spec = boundnames.get(i);         string colname = spec.name.tostring();         rawvalues.put(colname, values.get(colname) == null ? null : ((abstracttype)spec.type).decompose(values.get(colname)));     }     return rawaddrow(rawvalues); } when creating the new map for the insert we need to go over all columns and apply null to missing columns. fix the method documentation add this line:      * <p>      * keys in the map <b>must</b> be in lower case, otherwise their value will be null.      *<stacktrace> <code> addrow(map<string, object> values) public cqlsstablewriter addrow(map<string, object> values)     throws invalidrequestexception, ioexception {     int size = boundnames.size();     map<string, bytebuffer> rawvalues = new hashmap<>(size);     for (int i = 0; i < size; i++) {         columnspecification spec = boundnames.get(i);         string colname = spec.name.tostring();         rawvalues.put(colname, values.get(colname) == null ? null : ((abstracttype)spec.type).decompose(values.get(colname)));     }     return rawaddrow(rawvalues); }      * <p>      * keys in the map <b>must</b> be in lower case, otherwise their value will be null.      * <text> there are 2 bugs in the method first issue is that the map <b>must</b> contain all the column names as keys in the map otherwise the addrow fails (with invalidrequestexception 'invalid number of arguments, expecting %d values but got %d'). second issue is that the keys in the map must be in lower-case otherwise they may not be found in the map, which will result in a npe during decompose. fix the addrow method with: when creating the new map for the insert we need to go over all columns and apply null to missing columns. fix the method documentation add this line:",
        "label": 520
    },
    {
        "text": "concurrentmodificationexception during nodetool repair <description> when running a nodetool repair, the following exception can be thrown: error [antientropysessions:12] 2011-10-24 11:17:52,154 abstractcassandradaemon.java (line 139) fatal exception in thread thread[antientropysessions:12,5,rmi runtime]  java.lang.runtimeexception: java.util.concurrentmodificationexception  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.util.concurrentmodificationexception  at java.util.hashmap$hashiterator.nextentry(hashmap.java:793)  at java.util.hashmap$keyiterator.next(hashmap.java:828)  at org.apache.cassandra.service.antientropyservice$repairsession$repairjob.sendtreerequests(antientropyservice.java:784)  at org.apache.cassandra.service.antientropyservice$repairsession.runmaythrow(antientropyservice.java:680)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more<stacktrace> error [antientropysessions:12] 2011-10-24 11:17:52,154 abstractcassandradaemon.java (line 139) fatal exception in thread thread[antientropysessions:12,5,rmi runtime]  java.lang.runtimeexception: java.util.concurrentmodificationexception  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.util.concurrentmodificationexception  at java.util.hashmap$hashiterator.nextentry(hashmap.java:793)  at java.util.hashmap$keyiterator.next(hashmap.java:828)  at org.apache.cassandra.service.antientropyservice$repairsession$repairjob.sendtreerequests(antientropyservice.java:784)  at org.apache.cassandra.service.antientropyservice$repairsession.runmaythrow(antientropyservice.java:680)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more<code> <text> when running a nodetool repair, the following exception can be thrown: ",
        "label": 520
    },
    {
        "text": "add data modeling documentation to docs <description> jeff carpenter and o'reilly have offered to contribute a complete chapter on data modeling from cassandra, the definitive guide as a patch and thus under asfv2.0 license. we've had this stubbed out for some time on our site so this will be a fantastic addition:  http://cassandra.apache.org/doc/latest/data_modeling/index.html this issue will be for converting the text to our site format. for some background, see legal-486. the consensus on a follow up thread on legal-discuss was that jeff c. signing a icla and accepting this as a patch was good enough: https://lists.apache.org/thread.html/86485fd59bdb8d6b7932447c7cd6e1d50d23bb91aaf2680153855597%40%3clegal-discuss.apache.org%3e<stacktrace> <code> <text> jeff carpenter and o'reilly have offered to contribute a complete chapter on data modeling from cassandra, the definitive guide as a patch and thus under asfv2.0 license. we've had this stubbed out for some time on our site so this will be a fantastic addition:  http://cassandra.apache.org/doc/latest/data_modeling/index.html this issue will be for converting the text to our site format. for some background, see legal-486. the consensus on a follow up thread on legal-discuss was that jeff c. signing a icla and accepting this as a patch was good enough: https://lists.apache.org/thread.html/86485fd59bdb8d6b7932447c7cd6e1d50d23bb91aaf2680153855597%40%3clegal-discuss.apache.org%3e",
        "label": 242
    },
    {
        "text": "move source into src java <description> in order to allow code in different languages i suggest we move the current src/* into src/java and test/* into test/java.   perhaps one day we would like to publish easy to use client libs in other languages or put test configuration in test/conf as in cassandra-22, this move would help make such changes easier before we have too many outstanding patches against the current source.<stacktrace> <code> <text> in order to allow code in different languages i suggest we move the current src/* into src/java and test/* into test/java.   perhaps one day we would like to publish easy to use client libs in other languages or put test configuration in test/conf as in cassandra-22, this move would help make such changes easier before we have too many outstanding patches against the current source.",
        "label": 274
    },
    {
        "text": "mutation response writeresponse java  could be smaller and not contain keyspace and key <description> in the mutation response, writeresponse.java object is send back to the co-ordinator. this object has keyspace and key in it which is not required. it is not being used at the co-ordiantor. this wastes io specially in case of wan links between dc. also since response from each node in multi-dc deployments goes back to the co-ordinator in another dc makes it even worse. it also becomes worse if the the keyspace and key are of large size and the data is small. in that case, a node which is not the co-ordinator and purely receiving mutations, the outbound n/w bandwidth could be half of incoming bandwidth.<stacktrace> <code> <text> in the mutation response, writeresponse.java object is send back to the co-ordinator. this object has keyspace and key in it which is not required. it is not being used at the co-ordiantor. this wastes io specially in case of wan links between dc. also since response from each node in multi-dc deployments goes back to the co-ordinator in another dc makes it even worse. it also becomes worse if the the keyspace and key are of large size and the data is small. in that case, a node which is not the co-ordinator and purely receiving mutations, the outbound n/w bandwidth could be half of incoming bandwidth.",
        "label": 274
    },
    {
        "text": "cassandra cli doesn't work with system allowed column family names <description> given the following definitions for columns: <keyspaces> <keyspace name=\"ngram\"> <keyscachedfraction>0.01</keyscachedfraction> <columnfamily comparewith=\"utf8type\" name=\"1gramr\"/> <columnfamily comparewith=\"utf8type\" name=\"1graml\"/> </keyspaces> the appropriate keyspaces are created an persisteted on startup. when executing a query or a set operation in the cassandra-cli, you end up with the following error: ****************************************************** cassandra> get ngram.1gramr['hte'] line 1:10 extraneous input '1' expecting identifier no such column family: gramr ****************************************************** following the syntax of the grammer we can see the following: setstmt  : k_set columnfamilyexpr '=' value -> ^(node_thrift_set columnfamilyexpr value)  ; ... columnfamilyexpr  : table dot columnfamily '[' rowkey ']'  ( '[' a+=columnorsupercolumn ']'  ('[' a+=columnorsupercolumn ']')?  )?  -> ^(node_column_access table columnfamily rowkey ($a+)?)  ;  ... // syntactic elements  identifier  : letter ( alnum | '_' )*  ; there is a mismatch on what is appropriate values for this in the system. so either the restriction needs to be lifted in the cli, or the system must have a way of honoring the names.<stacktrace> <code> <keyspaces> <keyspace name='ngram'> <keyscachedfraction>0.01</keyscachedfraction> <columnfamily comparewith='utf8type' name='1gramr'/> <columnfamily comparewith='utf8type' name='1graml'/> </keyspaces> ****************************************************** cassandra> get ngram.1gramr['hte'] ****************************************************** setstmt  : k_set columnfamilyexpr '=' value -> ^(node_thrift_set columnfamilyexpr value)  ; ... // syntactic elements  identifier  : letter ( alnum | '_' )*  ; <text> given the following definitions for columns: the appropriate keyspaces are created an persisteted on startup. when executing a query or a set operation in the cassandra-cli, you end up with the following error: line 1:10 extraneous input '1' expecting identifier no such column family: gramr following the syntax of the grammer we can see the following: columnfamilyexpr  : table dot columnfamily '[' rowkey ']'  ( '[' a+=columnorsupercolumn ']'  ('[' a+=columnorsupercolumn ']')?  )?  -> ^(node_column_access table columnfamily rowkey ($a+)?)  ;  ... there is a mismatch on what is appropriate values for this in the system. so either the restriction needs to be lifted in the cli, or the system must have a way of honoring the names.",
        "label": 169
    },
    {
        "text": "test eat glass in cqlsh tests fails on windows <description> the cqlsh dtest cqlsh_tests.testcqlsh.test_eat_glass is failing on windows with 2.2-head. it has been failing for a very long time. we've looked into it before, but haven't figured it out. cqlsh does not return anything on the following query: self.assertequals(output.count('\u043c\u043e\u0436\u0430\u043c \u0434\u0430 \u0458\u0430\u0434\u0430\u043c \u0441\u0442\u0430\u043a\u043b\u043e, \u0430 \u043d\u0435 \u043c\u0435 \u0448\u0442\u0435\u0442\u0430.'), 16)<stacktrace> <code> self.assertequals(output.count('   ,    .'), 16) <text> the cqlsh dtest cqlsh_tests.testcqlsh.test_eat_glass is failing on windows with 2.2-head. it has been failing for a very long time. we've looked into it before, but haven't figured it out. cqlsh does not return anything on the following query:",
        "label": 409
    },
    {
        "text": "archive commitlogs tests failing <description> a number of archive commitlog dtests (snapshot_tests.py) are failing on trunk at the point in the tests where the node is asked to restore data from archived commitlogs. it appears that the snapshot functionality works, but the assertion regarding data that should have been restored from archived commitlogs fails. i also tested this manually on trunk and found no success in restoring either, so it appears to not just be a test issue. should note that it seems archiving the commitlogs works (in that they are actually copied) and rather restoring them is the issue. attached is a the commitlog properties file (to show the commands used).<stacktrace> <code> <text> a number of archive commitlog dtests (snapshot_tests.py) are failing on trunk at the point in the tests where the node is asked to restore data from archived commitlogs. it appears that the snapshot functionality works, but the assertion regarding data that should have been restored from archived commitlogs fails. i also tested this manually on trunk and found no success in restoring either, so it appears to not just be a test issue. should note that it seems archiving the commitlogs works (in that they are actually copied) and rather restoring them is the issue. attached is a the commitlog properties file (to show the commands used).",
        "label": 52
    },
    {
        "text": "query validation error   cql in operator over last partitioning clustering column  valid  is rejected if a query fetches collection columns <description> although in operator is allowed over the last clustering or partitioning columns, the cql query validator is rejecting queries when they attempt to fetch collection columns in their result set. it seems a similar bug (cassandra-5376) was raised some time ago, and a fix (rather mask) was provided to give a better error message to such queries in 1.2.4. considering that cassandra & cql has evolved a great deal from that period, it now seems possible to provide an actual fix to this problem, i.e. allowing queries to fetch collection columns even when in operator is used. please read the following mail thread to understand the context : https://lists.apache.org/thread.html/8e1765d14bd9798bf9c0938a793f1dbc9c9349062a8705db2e28d291@%3cuser.cassandra.apache.org%3e<stacktrace> <code> https://lists.apache.org/thread.html/8e1765d14bd9798bf9c0938a793f1dbc9c9349062a8705db2e28d291@%3cuser.cassandra.apache.org%3e<text> although in operator is allowed over the last clustering or partitioning columns, the cql query validator is rejecting queries when they attempt to fetch collection columns in their result set. it seems a similar bug (cassandra-5376) was raised some time ago, and a fix (rather mask) was provided to give a better error message to such queries in 1.2.4. considering that cassandra & cql has evolved a great deal from that period, it now seems possible to provide an actual fix to this problem, i.e. allowing queries to fetch collection columns even when in operator is used. please read the following mail thread to understand the context : ",
        "label": 25
    },
    {
        "text": "add a key value payload for third party usage <description> an useful improvement would be to include a generic key-value payload, so that developers implementing a custom queryhandler could leverage that to move custom data back and forth.<stacktrace> <code> <text> an useful improvement would be to include a generic key-value payload, so that developers implementing a custom queryhandler could leverage that to move custom data back and forth.",
        "label": 453
    },
    {
        "text": "duplicate values in an in restriction on the partition key column can break paging <description> we had an issue (java-515) in the java-driver when the number of parameters in a statement is greater than the supported limit (65k). i added a limit-test to verify that prepared statements with 65535 parameters were accepted by the driver, but ran into an issue on the cassandra side. basically, the test runs forever, because the driver receives an inconsistent answer from cassandra. when we prepare the statement, c* answers that it is correctly prepared, however when we try to execute it, we receive a unprepared answer. here is the code to reproduce the issue.<stacktrace> <code> <text> we had an issue (java-515) in the java-driver when the number of parameters in a statement is greater than the supported limit (65k). i added a limit-test to verify that prepared statements with 65535 parameters were accepted by the driver, but ran into an issue on the cassandra side. basically, the test runs forever, because the driver receives an inconsistent answer from cassandra. when we prepare the statement, c* answers that it is correctly prepared, however when we try to execute it, we receive a unprepared answer. here is the code to reproduce the issue.",
        "label": 69
    },
    {
        "text": "don't announce migrations to pre nodes <description> i have a mixed version cluster consisting of two 1.1.9 nodes and one 1.2.2 node upgraded from 1.1.9. the upgrade works, and i don't see any end user problems, however i see this exception in the logs on the non-upgraded nodes: error [migrationstage:1] 2013-03-11 18:09:09,001 abstractcassandradaemon.java (line 135) exception in thread thread[migrationstage:1,5,main] java.lang.nullpointerexception at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:167) at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:124) at org.apache.cassandra.cql.jdbc.jdbcutf8.getstring(jdbcutf8.java:77) at org.apache.cassandra.cql.jdbc.jdbcutf8.compose(jdbcutf8.java:97) at org.apache.cassandra.db.marshal.utf8type.compose(utf8type.java:35) at org.apache.cassandra.cql3.untypedresultset$row.getstring(untypedresultset.java:87) at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:256) at org.apache.cassandra.db.defstable.mergekeyspaces(defstable.java:397) at org.apache.cassandra.db.defstable.mergeschema(defstable.java:373) at org.apache.cassandra.db.defstable.mergeremoteschema(defstable.java:352) at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:48) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) steps to reproduce: ccm create -v 1.1.9 1.1.9 ccm populate -n 3 ccm start ccm node1 stress ccm node1 stop edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2. edit node1's cassandra.yaml to be 1.2 compliant. ccm node1 start the cluster is now a mixed version, and works for user queries, but with the exception above.<stacktrace> error [migrationstage:1] 2013-03-11 18:09:09,001 abstractcassandradaemon.java (line 135) exception in thread thread[migrationstage:1,5,main] java.lang.nullpointerexception at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:167) at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:124) at org.apache.cassandra.cql.jdbc.jdbcutf8.getstring(jdbcutf8.java:77) at org.apache.cassandra.cql.jdbc.jdbcutf8.compose(jdbcutf8.java:97) at org.apache.cassandra.db.marshal.utf8type.compose(utf8type.java:35) at org.apache.cassandra.cql3.untypedresultset$row.getstring(untypedresultset.java:87) at org.apache.cassandra.config.ksmetadata.fromschema(ksmetadata.java:256) at org.apache.cassandra.db.defstable.mergekeyspaces(defstable.java:397) at org.apache.cassandra.db.defstable.mergeschema(defstable.java:373) at org.apache.cassandra.db.defstable.mergeremoteschema(defstable.java:352) at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:48) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) <code> ccm create -v 1.1.9 1.1.9 ccm populate -n 3 ccm start ccm node1 stress ccm node1 stop ccm node1 start <text> i have a mixed version cluster consisting of two 1.1.9 nodes and one 1.2.2 node upgraded from 1.1.9. the upgrade works, and i don't see any end user problems, however i see this exception in the logs on the non-upgraded nodes: steps to reproduce: edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2. edit node1's cassandra.yaml to be 1.2 compliant. the cluster is now a mixed version, and works for user queries, but with the exception above.",
        "label": 18
    },
    {
        "text": "stop referring to batches as atomic <description> we still refer to logged batches as atomic, we should remove those references.<stacktrace> <code> <text> we still refer to logged batches as atomic, we should remove those references.",
        "label": 520
    },
    {
        "text": "pig does not work on datetype <description> cqlsh:pigdemo> describe columnfamily test1897; create columnfamily test1897 (  key text primary key,  testcol timestamp  ) with  comment='' and  comparator=text and  row_cache_provider='serializingcacheprovider' and  key_cache_size=200000.000000 and  row_cache_size=0.000000 and  read_repair_chance=1.000000 and  gc_grace_seconds=864000 and  default_validation=blob and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  row_cache_save_period_in_seconds=0 and  key_cache_save_period_in_seconds=14400 and  replicate_on_write=true; cqlsh:pigdemo> select * from test1897;  key | testcol  ----+------------------------  akey | 2012-01-21 00:14:12+0000 $ cat test1897.pig  cassandra_data = load 'cassandra://pigdemo/test1897' using cassandrastorage() as (name, columns: bag {t: tuple()} );  dump cassandra_data; there seems problem with the datetype. the above simple pig script fail with the attached err<stacktrace> <code> cqlsh:pigdemo> describe columnfamily test1897; create columnfamily test1897 (  key text primary key,  testcol timestamp  ) with  comment='' and  comparator=text and  row_cache_provider='serializingcacheprovider' and  key_cache_size=200000.000000 and  row_cache_size=0.000000 and  read_repair_chance=1.000000 and  gc_grace_seconds=864000 and  default_validation=blob and  min_compaction_threshold=4 and  max_compaction_threshold=32 and  row_cache_save_period_in_seconds=0 and  key_cache_save_period_in_seconds=14400 and  replicate_on_write=true; cqlsh:pigdemo> select * from test1897;  key | testcol  ----+------------------------  akey | 2012-01-21 00:14:12+0000 $ cat test1897.pig  cassandra_data = load 'cassandra://pigdemo/test1897' using cassandrastorage() as (name, columns: bag );  dump cassandra_data; <text> there seems problem with the datetype. the above simple pig script fail with the attached err",
        "label": 85
    },
    {
        "text": "dtest failure in compaction test testcompaction with datetieredcompactionstrategy bloomfilter size test <description> example failure: http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/19/testreport/compaction_test/testcompaction_with_datetieredcompactionstrategy/bloomfilter_size_test 500352 not less than or equal to 150000<stacktrace> <code> http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/19/testreport/compaction_test/testcompaction_with_datetieredcompactionstrategy/bloomfilter_size_test <text> example failure: 500352 not less than or equal to 150000",
        "label": 321
    },
    {
        "text": "cassandrastorage broken for bigints and ints <description> i am seeing something rather strange in the way cass 1.2 + pig seem to handle integer values. setup: cassandra 1.2.10, osx 10.8, jdk 1.7u40, pig 0.11.1. single node for testing this. first a table: > create table testc (  key text primary key,  ivalue int,  svalue text,  value bigint ) with compact storage; > insert into testc (key,ivalue,svalue,value) values ('foo',10,'bar',65); > select * from testc; key | ivalue | svalue | value -----+--------+--------+------- foo |     10 |    bar |     65 for my pig setup, i then use libraries from different c* versions to actually talk to my database (which stays on 1.2.10 all the time). cassandra 1.0.12 (using cassandra_storage.jar): testc = load 'cassandra://keyspace/testc' using cassandrastorage(); dump testc (foo,(svalue,bar),(ivalue,10),(value,65),{}) cassandra 1.1.10: testc = load 'cassandra://keyspace/testc' using cassandrastorage(); dump testc (foo,(svalue,bar),(ivalue,10),(value,65),{}) cassandra 1.2.10: (testc = load 'cassandra://keyspace/testc' using cassandrastorage(); dump testc foo,{(ivalue, ),(svalue,bar),(value,a)}) to me it appears that ints and bigints are interpreted as ascii values in cass 1.2.10. did something change for cassandrastorage, is there a regression, or am i doing something wrong? quick perusal of the jira didn't reveal anything that i could directly pin on this. note that using compact storage does not seem to affect the issue, though it obviously changes the resulting pig format. in addition, trying to use pygmalion tf = foreach testc generate key, flatten(fromcassandrabag('ivalue,svalue,value',columns)) as (ivalue:int,svalue:chararray,lvalue:long); dump tf (foo, ,bar,a) so no help there. explicitly casting the values to (long) or (int) just results in a classcastexception.<stacktrace> <code> > create table testc (  key text primary key,  ivalue int,  svalue text,  value bigint ) with compact storage; > insert into testc (key,ivalue,svalue,value) values ('foo',10,'bar',65); > select * from testc; key | ivalue | svalue | value -----+--------+--------+------- foo |     10 |    bar |     65 testc = load 'cassandra://keyspace/testc' using cassandrastorage(); dump testc (foo,(svalue,bar),(ivalue,10),(value,65),{}) testc = load 'cassandra://keyspace/testc' using cassandrastorage(); dump testc (foo,(svalue,bar),(ivalue,10),(value,65),{}) (testc = load 'cassandra://keyspace/testc' using cassandrastorage(); dump testc foo,{(ivalue, ),(svalue,bar),(value,a)}) tf = foreach testc generate key, flatten(fromcassandrabag('ivalue,svalue,value',columns)) as (ivalue:int,svalue:chararray,lvalue:long); dump tf (foo, ,bar,a) <text> i am seeing something rather strange in the way cass 1.2 + pig seem to handle integer values. setup: cassandra 1.2.10, osx 10.8, jdk 1.7u40, pig 0.11.1. single node for testing this. first a table: for my pig setup, i then use libraries from different c* versions to actually talk to my database (which stays on 1.2.10 all the time). cassandra 1.0.12 (using cassandra_storage.jar): cassandra 1.1.10: cassandra 1.2.10: to me it appears that ints and bigints are interpreted as ascii values in cass 1.2.10. did something change for cassandrastorage, is there a regression, or am i doing something wrong? quick perusal of the jira didn't reveal anything that i could directly pin on this. note that using compact storage does not seem to affect the issue, though it obviously changes the resulting pig format. in addition, trying to use pygmalion so no help there. explicitly casting the values to (long) or (int) just results in a classcastexception.",
        "label": 22
    },
    {
        "text": "cassandra cli doesn't support jmx authentication  <description> it seems that cassandra-cli doesn't support jmx user authentication. specifically i went about securing our cassandra cluster slightly \u2013 i've added cassandra-level authentication (which cassandra-cli does support), but then i discovered that nodetool is still completely unprotected. so i went ahead and secured jmx (via -dcom.sun.management.jmxremote.password.file and -dcom.sun.management.jmxremote.access.file). nodetool supports jmx authentication via -u and -pw options. however it seems that cassandra-cli doesn't support jmx authentication, e.g.: apache-cassandra-1.1.6\\bin>cassandra-cli -h hostname -u experiment -pw password  starting cassandra client  connected to: \"db\" on hostname/9160  welcome to cassandra cli version 1.1.6 [experiment@unknown] show keyspaces;  warning: could not connect to the jmx on hostname:7199, information won't be shown. keyspace: system:  replication strategy: org.apache.cassandra.locator.localstrategy  durable writes: true  options: [replication_factor:1]  ... (rest of keyspace output snipped) help connect; and cassandra-cli --help do not seem to indicate that there's any way to specify jmx login information.<stacktrace> <code> <text> it seems that cassandra-cli doesn't support jmx user authentication. specifically i went about securing our cassandra cluster slightly - i've added cassandra-level authentication (which cassandra-cli does support), but then i discovered that nodetool is still completely unprotected. so i went ahead and secured jmx (via -dcom.sun.management.jmxremote.password.file and -dcom.sun.management.jmxremote.access.file). nodetool supports jmx authentication via -u and -pw options. however it seems that cassandra-cli doesn't support jmx authentication, e.g.: apache-cassandra-1.1.6/bin>cassandra-cli -h hostname -u experiment -pw password  starting cassandra client  connected to: 'db' on hostname/9160  welcome to cassandra cli version 1.1.6 [experiment@unknown] show keyspaces;  warning: could not connect to the jmx on hostname:7199, information won't be shown. keyspace: system:  replication strategy: org.apache.cassandra.locator.localstrategy  durable writes: true  options: [replication_factor:1]  ... (rest of keyspace output snipped) help connect; and cassandra-cli --help do not seem to indicate that there's any way to specify jmx login information.",
        "label": 352
    },
    {
        "text": "improve anticompaction after incremental repair <description> after an incremental repair we iterate over all sstables and split them in two parts, one containing the repaired data and one the unrepaired. we could in theory double the number of sstables on a node. to avoid this we could make anticompaction also do a compaction, for example, if we are to anticompact 10 sstables, we could anticompact those to 2. note that we need to avoid creating too big sstables though, if we anticompact all sstables on a node it would essentially be a major compaction.<stacktrace> <code> <text> after an incremental repair we iterate over all sstables and split them in two parts, one containing the repaired data and one the unrepaired. we could in theory double the number of sstables on a node. to avoid this we could make anticompaction also do a compaction, for example, if we are to anticompact 10 sstables, we could anticompact those to 2. note that we need to avoid creating too big sstables though, if we anticompact all sstables on a node it would essentially be a major compaction.",
        "label": 466
    },
    {
        "text": "increment counters <description> break out the increment counters out of cassandra-580. classes are shared between the two features but without the plain version vector code the changeset becomes smaller and more manageable.<stacktrace> <code> <text> break out the increment counters out of cassandra-580. classes are shared between the two features but without the plain version vector code the changeset becomes smaller and more manageable.",
        "label": 297
    },
    {
        "text": "countercolumnfamily compaction error  arrayindexoutofboundsexception  <description> on a single node, i'm seeing the following error when trying to compact a countercolumnfamily. this appears to have started with version 1.0.3. nodetool -h localhost compact trprod metricsalltime  error occured during compaction  java.util.concurrent.executionexception: java.lang.arrayindexoutofboundsexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.compaction.compactionmanager.performmaximal(compactionmanager.java:250)  at org.apache.cassandra.db.columnfamilystore.forcemajorcompaction(columnfamilystore.java:1471)  at org.apache.cassandra.service.storageservice.forcetablecompaction(storageservice.java:1523)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:93)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:27)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:208)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:120)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:262)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:836)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:761)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1427)  at javax.management.remote.rmi.rmiconnectionimpl.access$200(rmiconnectionimpl.java:72)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1265)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1360)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:788)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:305)  at sun.rmi.transport.transport$1.run(transport.java:159)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:155)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:535)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:790)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:649)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.lang.arrayindexoutofboundsexception  at org.apache.cassandra.utils.bytebufferutil.arraycopy(bytebufferutil.java:292)  at org.apache.cassandra.db.context.countercontext$contextstate.copyto(countercontext.java:792)  at org.apache.cassandra.db.context.countercontext.removeoldshards(countercontext.java:709)  at org.apache.cassandra.db.countercolumn.removeoldshards(countercolumn.java:260)  at org.apache.cassandra.db.countercolumn.mergeandremoveoldshards(countercolumn.java:306)  at org.apache.cassandra.db.countercolumn.mergeandremoveoldshards(countercolumn.java:271)  at org.apache.cassandra.db.compaction.precompactedrow.removedeletedandoldshards(precompactedrow.java:86)  at org.apache.cassandra.db.compaction.precompactedrow.<init>(precompactedrow.java:102)  at org.apache.cassandra.db.compaction.compactioncontroller.getcompactedrow(compactioncontroller.java:133)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:102)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:87)  at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:116)  at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:99)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at com.google.common.collect.iterators$7.computenext(iterators.java:614)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:172)  at org.apache.cassandra.db.compaction.compactionmanager$4.call(compactionmanager.java:277)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more<stacktrace> nodetool -h localhost compact trprod metricsalltime  error occured during compaction  java.util.concurrent.executionexception: java.lang.arrayindexoutofboundsexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.compaction.compactionmanager.performmaximal(compactionmanager.java:250)  at org.apache.cassandra.db.columnfamilystore.forcemajorcompaction(columnfamilystore.java:1471)  at org.apache.cassandra.service.storageservice.forcetablecompaction(storageservice.java:1523)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:93)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:27)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:208)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:120)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:262)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:836)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:761)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1427)  at javax.management.remote.rmi.rmiconnectionimpl.access$200(rmiconnectionimpl.java:72)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1265)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1360)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:788)  at sun.reflect.generatedmethodaccessor24.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:305)  at sun.rmi.transport.transport$1.run(transport.java:159)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:155)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:535)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:790)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:649)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.lang.arrayindexoutofboundsexception  at org.apache.cassandra.utils.bytebufferutil.arraycopy(bytebufferutil.java:292)  at org.apache.cassandra.db.context.countercontext$contextstate.copyto(countercontext.java:792)  at org.apache.cassandra.db.context.countercontext.removeoldshards(countercontext.java:709)  at org.apache.cassandra.db.countercolumn.removeoldshards(countercolumn.java:260)  at org.apache.cassandra.db.countercolumn.mergeandremoveoldshards(countercolumn.java:306)  at org.apache.cassandra.db.countercolumn.mergeandremoveoldshards(countercolumn.java:271)  at org.apache.cassandra.db.compaction.precompactedrow.removedeletedandoldshards(precompactedrow.java:86)  at org.apache.cassandra.db.compaction.precompactedrow.<init>(precompactedrow.java:102)  at org.apache.cassandra.db.compaction.compactioncontroller.getcompactedrow(compactioncontroller.java:133)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:102)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:87)  at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:116)  at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:99)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at com.google.common.collect.iterators$7.computenext(iterators.java:614)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:172)  at org.apache.cassandra.db.compaction.compactionmanager$4.call(compactionmanager.java:277)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more<code> <text> on a single node, i'm seeing the following error when trying to compact a countercolumnfamily. this appears to have started with version 1.0.3. ",
        "label": 520
    },
    {
        "text": "upgrade from to fails with nullpointerexception <description> i tried to upgrade cassandra from 2.2.1 to 3.0.0, however, i get this error on startup after cassandra 3.0 software was installed: error [main] 2015-11-30 15:44:50,164 cassandradaemon.java:702 - exception encountered during startup java.lang.nullpointerexception: null at org.apache.cassandra.io.util.fileutils.delete(fileutils.java:374) ~[apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.db.systemkeyspace.migratedatadirs(systemkeyspace.java:1341) ~[apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:180) [apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:561) [apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:689) [apache-cassandra-3.0.0.jar:3.0.0]<stacktrace> error [main] 2015-11-30 15:44:50,164 cassandradaemon.java:702 - exception encountered during startup java.lang.nullpointerexception: null at org.apache.cassandra.io.util.fileutils.delete(fileutils.java:374) ~[apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.db.systemkeyspace.migratedatadirs(systemkeyspace.java:1341) ~[apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:180) [apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:561) [apache-cassandra-3.0.0.jar:3.0.0] at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:689) [apache-cassandra-3.0.0.jar:3.0.0] <code> <text> i tried to upgrade cassandra from 2.2.1 to 3.0.0, however, i get this error on startup after cassandra 3.0 software was installed:",
        "label": 508
    },
    {
        "text": "flaky test  testdiskbalance test disk balance after boundary change stcs <description> see https://app.circleci.com/pipelines/github/maedhroz/cassandra/99/workflows/72c69ea8-f347-4b00-aed8-bd465f3549ff/jobs/498 after bootstrapping a second node into the cluster, the sizes of the sstables (per directory) on the first node no longer fall within the 10% margin of error. we don\u2019t have any assertion in the test that they were balanced before bootstrap, however.<stacktrace> <code> see https://app.circleci.com/pipelines/github/maedhroz/cassandra/99/workflows/72c69ea8-f347-4b00-aed8-bd465f3549ff/jobs/498 <text> after bootstrapping a second node into the cluster, the sizes of the sstables (per directory) on the first node no longer fall within the 10% margin of error. we don't have any assertion in the test that they were balanced before bootstrap, however.",
        "label": 8
    },
    {
        "text": "rename columnfamilystorecqlhelper to tablecqlhelper <description> seems like a simple 1:1 rename.<stacktrace> <code> <text> seems like a simple 1:1 rename.",
        "label": 553
    },
    {
        "text": "npe in net outputtcpconnection when tracing is enabled <description> i get multiple nullpointerexception when trying to trace insert statements. to reproduce: $ ccm create -v git:trunk $ ccm populate -n 3 $ ccm start $ ccm node1 cqlsh < 5668_npe_ddl.cql $ ccm node1 cqlsh < 5668_npe_insert.cql and see many exceptions like this in the logs of node1: error [write-/127.0.0.3] 2013-06-19 14:54:35,885 outboundtcpconnection.java (line 197) error writing to /127.0.0.3 java.lang.nullpointerexception         at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:182)         at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:144) this is similar to cassandra-5658 and is the reason that npe_ddl and npe_insert are separate files.<stacktrace> error [write-/127.0.0.3] 2013-06-19 14:54:35,885 outboundtcpconnection.java (line 197) error writing to /127.0.0.3 java.lang.nullpointerexception         at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:182)         at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:144) <code> $ ccm create -v git:trunk $ ccm populate -n 3 $ ccm start $ ccm node1 cqlsh < 5668_npe_ddl.cql $ ccm node1 cqlsh < 5668_npe_insert.cql <text> i get multiple nullpointerexception when trying to trace insert statements. to reproduce: and see many exceptions like this in the logs of node1: this is similar to cassandra-5658 and is the reason that npe_ddl and npe_insert are separate files.",
        "label": 274
    },
    {
        "text": "consistency quorum does not work anymore  hector could not fullfill request on this host  <description> i'm using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25. using consistency level quorum won't work anymore (tested it on read). consisteny level one still works though i have tried this with one dead node in my cluster. if i restart cassandra with an older svn revision (apache-cassandra-2011-01-28_20-06-01.jar), i can access the cluster with consistency level quorum again, while still using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25 in my application. 11/01/31 19:54:38 error connection.cassandrahostretryservice: downed intr1n18(192.168.0.18):9160 host still appears to be down: unable to open transport to intr1n18(192.168.0.18):9160 , java.net.noroutetohostexception: no route to host  11/01/31 19:54:38 info connection.cassandrahostretryservice: downed host retry status false with host: intr1n18(192.168.0.18):9160  11/01/31 19:54:45 error connection.hconnectionmanager: could not fullfill request on this host cassandraclient<intr1n11:9160-483> intr1n11 is marked as up however and i can also access the node through the cassandra cli. 192.168.0.1 up normal 8.02 gb 5.00% 0cc  192.168.0.2 up normal 7.96 gb 5.00% 199  192.168.0.3 up normal 8.24 gb 5.00% 266  192.168.0.4 up normal 4.94 gb 5.00% 333  192.168.0.5 up normal 5.02 gb 5.00% 400  192.168.0.6 up normal 5 gb 5.00% 4cc  192.168.0.7 up normal 5.1 gb 5.00% 599  192.168.0.8 up normal 5.07 gb 5.00% 666  192.168.0.9 up normal 4.78 gb 5.00% 733  192.168.0.10 up normal 4.34 gb 5.00% 7ff  192.168.0.11 up normal 5.01 gb 5.00% 8cc  192.168.0.12 up normal 5.31 gb 5.00% 999  192.168.0.13 up normal 5.56 gb 5.00% a66  192.168.0.14 up normal 5.82 gb 5.00% b33  192.168.0.15 up normal 5.57 gb 5.00% c00  192.168.0.16 up normal 5.03 gb 5.00% ccc  192.168.0.17 up normal 4.77 gb 5.00% d99  192.168.0.18 down normal ? 5.00% e66  192.168.0.19 up normal 4.78 gb 5.00% f33  192.168.0.20 up normal 4.83 gb 5.00% ffffffffffffffff<stacktrace> <code> 11/01/31 19:54:38 error connection.cassandrahostretryservice: downed intr1n18(192.168.0.18):9160 host still appears to be down: unable to open transport to intr1n18(192.168.0.18):9160 , java.net.noroutetohostexception: no route to host  11/01/31 19:54:38 info connection.cassandrahostretryservice: downed host retry status false with host: intr1n18(192.168.0.18):9160  11/01/31 19:54:45 error connection.hconnectionmanager: could not fullfill request on this host cassandraclient<intr1n11:9160-483> 192.168.0.1 up normal 8.02 gb 5.00% 0cc  192.168.0.2 up normal 7.96 gb 5.00% 199  192.168.0.3 up normal 8.24 gb 5.00% 266  192.168.0.4 up normal 4.94 gb 5.00% 333  192.168.0.5 up normal 5.02 gb 5.00% 400  192.168.0.6 up normal 5 gb 5.00% 4cc  192.168.0.7 up normal 5.1 gb 5.00% 599  192.168.0.8 up normal 5.07 gb 5.00% 666  192.168.0.9 up normal 4.78 gb 5.00% 733  192.168.0.10 up normal 4.34 gb 5.00% 7ff  192.168.0.11 up normal 5.01 gb 5.00% 8cc  192.168.0.12 up normal 5.31 gb 5.00% 999  192.168.0.13 up normal 5.56 gb 5.00% a66  192.168.0.14 up normal 5.82 gb 5.00% b33  192.168.0.15 up normal 5.57 gb 5.00% c00  192.168.0.16 up normal 5.03 gb 5.00% ccc  192.168.0.17 up normal 4.77 gb 5.00% d99  192.168.0.18 down normal ? 5.00% e66  192.168.0.19 up normal 4.78 gb 5.00% f33  192.168.0.20 up normal 4.83 gb 5.00% ffffffffffffffff<text> i'm using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25. using consistency level quorum won't work anymore (tested it on read). consisteny level one still works though i have tried this with one dead node in my cluster. if i restart cassandra with an older svn revision (apache-cassandra-2011-01-28_20-06-01.jar), i can access the cluster with consistency level quorum again, while still using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25 in my application. intr1n11 is marked as up however and i can also access the node through the cassandra cli. ",
        "label": 1
    },
    {
        "text": "dtest failure in repair tests repair test testrepair nonexistent table repair test <description> example failure: http://cassci.datastax.com/job/cassandra-3.9_dtest/50/testreport/repair_tests.repair_test/testrepair/nonexistent_table_repair_test stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/repair_tests/repair_test.py\", line 210, in nonexistent_table_repair_test     self.assertfalse(t.isalive(), 'repair thread on inexistent table is still running')   file \"/usr/lib/python2.7/unittest/case.py\", line 416, in assertfalse     raise self.failureexception(msg) \"repair thread on inexistent table is still running<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/repair_tests/repair_test.py', line 210, in nonexistent_table_repair_test     self.assertfalse(t.isalive(), 'repair thread on inexistent table is still running')   file '/usr/lib/python2.7/unittest/case.py', line 416, in assertfalse     raise self.failureexception(msg) 'repair thread on inexistent table is still running http://cassci.datastax.com/job/cassandra-3.9_dtest/50/testreport/repair_tests.repair_test/testrepair/nonexistent_table_repair_test<text> example failure: ",
        "label": 428
    },
    {
        "text": "introduce midres config in circleci <description> from document: https://gist.github.com/newkek/bb79dccbe7d2f5e41b2a3daac3858fde we have identified that the current highres configuration seems to require resources that might not bring the best cost efficiency to the build. we have also identified several \"good compromise\" configurations that allow to get decent performance out of the test suites, without going all out on the big config. it seems it would be useful for a lot of people to have this available as a config.yml.midres configuration in the .circleci folder. this way we do not need to argue on modifying the highres configuration so as to not impact the people already using it, but can still have easy access the \"compromise\" config.<stacktrace> <code> from document: https://gist.github.com/newkek/bb79dccbe7d2f5e41b2a3daac3858fde <text> we have identified that the current highres configuration seems to require resources that might not bring the best cost efficiency to the build. we have also identified several 'good compromise' configurations that allow to get decent performance out of the test suites, without going all out on the big config. it seems it would be useful for a lot of people to have this available as a config.yml.midres configuration in the .circleci folder. this way we do not need to argue on modifying the highres configuration so as to not impact the people already using it, but can still have easy access the 'compromise' config.",
        "label": 165
    },
    {
        "text": "index redistribution breaks sasi index <description> during index redistribution process, a new view is created.  during this creation, old indexes should be released. but, new indexes are \"attached\" to the same sstable as the old indexes. this leads to the deletion of the last sasi index file and breaks the index. the issue is in this function : https://github.com/apache/cassandra/blob/9ee44db49b13d4b4c91c9d6332ce06a6e2abf944/src/java/org/apache/cassandra/index/sasi/conf/view/view.java#l62<stacktrace> <code> the issue is in this function : https://github.com/apache/cassandra/blob/9ee44db49b13d4b4c91c9d6332ce06a6e2abf944/src/java/org/apache/cassandra/index/sasi/conf/view/view.java#l62<text> during index redistribution process, a new view is created.  during this creation, old indexes should be released. but, new indexes are 'attached' to the same sstable as the old indexes. this leads to the deletion of the last sasi index file and breaks the index. ",
        "label": 278
    },
    {
        "text": "invalid response count <description> 2010-03-30_21:59:04.64973 error - error in threadpoolexecutor  2010-03-30_21:59:04.64973 java.lang.assertionerror: invalid response count 4  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.readresponseresolver.<init>(readresponseresolver.java:54)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.doreadrepair(consistencymanager.java:89)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.handledigestresponses(consistencymanager.java:75)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.response(consistencymanager.java:60)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:35)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:40)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  2010-03-30_21:59:04.64973 at java.lang.thread.run(thread.java:636)  2010-03-30_21:59:04.64973 error - fatal exception in thread thread[response-stage:5,5,main]  2010-03-30_21:59:04.64973 java.lang.assertionerror: invalid response count 4  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.readresponseresolver.<init>(readresponseresolver.java:54)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.doreadrepair(consistencymanager.java:89)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.handledigestresponses(consistencymanager.java:75)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.response(consistencymanager.java:60)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:35)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:40)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  2010-03-30_21:59:04.64973 at java.lang.thread.run(thread.java:636)<stacktrace> 2010-03-30_21:59:04.64973 error - error in threadpoolexecutor  2010-03-30_21:59:04.64973 java.lang.assertionerror: invalid response count 4  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.readresponseresolver.<init>(readresponseresolver.java:54)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.doreadrepair(consistencymanager.java:89)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.handledigestresponses(consistencymanager.java:75)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.response(consistencymanager.java:60)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:35)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:40)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  2010-03-30_21:59:04.64973 at java.lang.thread.run(thread.java:636)  2010-03-30_21:59:04.64973 error - fatal exception in thread thread[response-stage:5,5,main]  2010-03-30_21:59:04.64973 java.lang.assertionerror: invalid response count 4  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.readresponseresolver.<init>(readresponseresolver.java:54)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.doreadrepair(consistencymanager.java:89)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.handledigestresponses(consistencymanager.java:75)  2010-03-30_21:59:04.64973 at org.apache.cassandra.service.consistencymanager$digestresponsehandler.response(consistencymanager.java:60)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:35)  2010-03-30_21:59:04.64973 at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:40)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  2010-03-30_21:59:04.64973 at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  2010-03-30_21:59:04.64973 at java.lang.thread.run(thread.java:636)<code> <text> ",
        "label": 274
    },
    {
        "text": "dtest failure in upgrade tests cql tests testcqlnodes2rf1 upgrade current x to indev x select key in test <description> example failure: http://cassci.datastax.com/job/upgrade_tests-all/47/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_3_x_to_indev_3_x/select_key_in_test failed on cassci build upgrade_tests-all #47 attached logs for test failure. error [compactionexecutor:2] 2016-05-21 23:10:35,678 cassandradaemon.java:195 - exception in thread thread[compactionexecutor:2,1,main] java.util.concurrent.rejectedexecutionexception: threadpoolexecutor has shut down at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor$1.rejectedexecution(debuggablethreadpoolexecutor.java:61) ~[apache-cassandra-3.5.jar:3.5] at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:823) ~[na:1.8.0_51] at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1364) ~[na:1.8.0_51] at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute(debuggablethreadpoolexecutor.java:165) ~[apache-cassandra-3.5.jar:3.5] at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:112) ~[na:1.8.0_51] at org.apache.cassandra.db.compaction.compactionmanager.submitbackground(compactionmanager.java:184) ~[apache-cassandra-3.5.jar:3.5] at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactioncandidate.run(compactionmanager.java:270) ~[apache-cassandra-3.5.jar:3.5] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_51] at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_51] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_51] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_51] at java.lang.thread.run(thread.java:745) [na:1.8.0_51]<stacktrace> error [compactionexecutor:2] 2016-05-21 23:10:35,678 cassandradaemon.java:195 - exception in thread thread[compactionexecutor:2,1,main] java.util.concurrent.rejectedexecutionexception: threadpoolexecutor has shut down at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor$1.rejectedexecution(debuggablethreadpoolexecutor.java:61) ~[apache-cassandra-3.5.jar:3.5] at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:823) ~[na:1.8.0_51] at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1364) ~[na:1.8.0_51] at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute(debuggablethreadpoolexecutor.java:165) ~[apache-cassandra-3.5.jar:3.5] at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:112) ~[na:1.8.0_51] at org.apache.cassandra.db.compaction.compactionmanager.submitbackground(compactionmanager.java:184) ~[apache-cassandra-3.5.jar:3.5] at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactioncandidate.run(compactionmanager.java:270) ~[apache-cassandra-3.5.jar:3.5] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_51] at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_51] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_51] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_51] at java.lang.thread.run(thread.java:745) [na:1.8.0_51] <code> http://cassci.datastax.com/job/upgrade_tests-all/47/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_3_x_to_indev_3_x/select_key_in_test <text> example failure: failed on cassci build upgrade_tests-all #47 attached logs for test failure.",
        "label": 321
    },
    {
        "text": "move non generated source <description> as per parent ticket, we should move src/org to src/java/org to allow for multiple source directories. also see cassandra-105<stacktrace> <code> as per parent ticket, we should move src/org to src/java/org to allow for multiple source directories. also see cassandra-105<text> ",
        "label": 264
    },
    {
        "text": "error in threadpoolexecutor <description> on my two-node test setup i get repeatedly following error: the 10.0.18.129 server log:    info 14:10:37,707 node /10.0.18.99 has restarted, now up again  info 14:10:37,708 checking remote schema before delivering hints  info 14:10:37,708 sleeping 45506ms to stagger hint delivery  info 14:10:37,709 node /10.0.18.99 state jump to normal  info 14:11:23,215 started hinted handoff for endpoint /10.0.18.99 error 14:11:23,884 error in threadpoolexecutor java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more error 14:11:23,885 fatal exception in thread thread[hintedhandoff:1,1,main] java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more the 10.0.18.99 server log:    info 14:10:37,691 binding thrift service to /0.0.0.0:9160  info 14:10:37,693 using tfastframedtransport with a max frame size of 15728640 bytes.  info 14:10:37,695 listening for thrift clients...  info 14:10:38,337 gc for parnew: 954 ms, 658827608 reclaimed leaving 966732432 used; max is 4265607168  info 14:11:27,142 started hinted handoff for endpoint /10.0.18.129 error 14:11:27,370 error in threadpoolexecutor java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more error 14:11:27,371 fatal exception in thread thread[hintedhandoff:1,1,main] java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more it happen durring batch_mutate test or after restart, when there are commitlogs to replay. using current 0.7.1 from cassandra-0.7 branch.<stacktrace>    info 14:10:37,707 node /10.0.18.99 has restarted, now up again  info 14:10:37,708 checking remote schema before delivering hints  info 14:10:37,708 sleeping 45506ms to stagger hint delivery  info 14:10:37,709 node /10.0.18.99 state jump to normal  info 14:11:23,215 started hinted handoff for endpoint /10.0.18.99 error 14:11:23,884 error in threadpoolexecutor java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more error 14:11:23,885 fatal exception in thread thread[hintedhandoff:1,1,main] java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more    info 14:10:37,691 binding thrift service to /0.0.0.0:9160  info 14:10:37,693 using tfastframedtransport with a max frame size of 15728640 bytes.  info 14:10:37,695 listening for thrift clients...  info 14:10:38,337 gc for parnew: 954 ms, 658827608 reclaimed leaving 966732432 used; max is 4265607168  info 14:11:27,142 started hinted handoff for endpoint /10.0.18.129 error 14:11:27,370 error in threadpoolexecutor java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more error 14:11:27,371 fatal exception in thread thread[hintedhandoff:1,1,main] java.lang.runtimeexception: java.lang.illegalargumentexception        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)        at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)        at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)        at java.lang.thread.run(thread.java:662) caused by: java.lang.illegalargumentexception        at java.nio.buffer.position(buffer.java:218)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:117)        at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:111)        at org.apache.cassandra.db.hintedhandoffmanager.gettableandcfnames(hintedhandoffmanager.java:237)        at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:306)        at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:88)        at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:385)        at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)        ... 3 more <code> <text> on my two-node test setup i get repeatedly following error: the 10.0.18.129 server log: the 10.0.18.99 server log: it happen durring batch_mutate test or after restart, when there are commitlogs to replay. using current 0.7.1 from cassandra-0.7 branch.",
        "label": 520
    },
    {
        "text": "read repair inserts should not be blocking <description> today, if there\u2019s a digest mismatch in a foreground read repair, the insert to update out of date replicas is blocking. this means, if it fails, the read fails with a timeout. if a node is dropping writes (maybe it is overloaded or the mutation stage is backed up for some other reason), all reads to a replica set could fail. further, replicas dropping writes get more out of sync so will require more read repair. the comment on the code for why the writes are blocking is: // wait for the repair writes to be acknowledged, to minimize impact on any replica that's // behind on writes in case the out-of-sync row is read multiple times in quick succession but the bad side effect is that reads timeout. either the writes should not be blocking or we should return success for the read even if the write times out.<stacktrace> <code> // wait for the repair writes to be acknowledged, to minimize impact on any replica that's // behind on writes in case the out-of-sync row is read multiple times in quick succession <text> today, if there's a digest mismatch in a foreground read repair, the insert to update out of date replicas is blocking. this means, if it fails, the read fails with a timeout. if a node is dropping writes (maybe it is overloaded or the mutation stage is backed up for some other reason), all reads to a replica set could fail. further, replicas dropping writes get more out of sync so will require more read repair. the comment on the code for why the writes are blocking is: but the bad side effect is that reads timeout. either the writes should not be blocking or we should return success for the read even if the write times out.",
        "label": 79
    },
    {
        "text": "file descriptor leak in commitlog <description> there is a file descriptor leak in commitlog.java. on systems with a default ulimit of 1024, this causes cassandra to eventually crash due to too many open files. a descriptor appears to be leaked at each memtable rotation.<stacktrace> <code> <text> there is a file descriptor leak in commitlog.java. on systems with a default ulimit of 1024, this causes cassandra to eventually crash due to too many open files. a descriptor appears to be leaked at each memtable rotation.",
        "label": 85
    },
    {
        "text": "internal error processing get range slices <description> runnig mapreduce task on two or more cassandra nodes gives following error: debug 16:51:48,653 range_slice  debug 16:51:48,653 rangeslicecommand {keyspace='test', column_family='url', super_column=null, predicate=slicepredicate(column_names:[java.nio.heapbytebuffer[pos=57 lim=67 cap=177]]), range=(162950022446285318630909295651345252065,9481098247439719900692337295923514899], max_keys=4096} debug 16:51:48,653 restricted ranges for query (162950022446285318630909295651345252065,9481098247439719900692337295923514899] are [(162950022446285318630909295651345252065,9481098247439719900692337295923514899]]  debug 16:51:48,653 local range slice  error 16:51:48,653 internal error processing get_range_slices  java.lang.assertionerror: (162950022446285318630909295651345252065,9481098247439719900692337295923514899]  at org.apache.cassandra.db.columnfamilystore.getrangeslice(columnfamilystore.java:1264)  at org.apache.cassandra.service.storageproxy.getrangeslice(storageproxy.java:429)  at org.apache.cassandra.thrift.cassandraserver.get_range_slices(cassandraserver.java:514)  at org.apache.cassandra.thrift.cassandra$processor$get_range_slices.process(cassandra.java:2868)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:2555)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:167)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  debug 16:51:48,838 logged out: #<user allow_all groups=[]> you can reproduce this by just running contrib/word_count example. mapreduce last worked with cassandra 0.7-beta2. important is to run more than one node.<stacktrace> debug 16:51:48,653 restricted ranges for query (162950022446285318630909295651345252065,9481098247439719900692337295923514899] are [(162950022446285318630909295651345252065,9481098247439719900692337295923514899]]  debug 16:51:48,653 local range slice  error 16:51:48,653 internal error processing get_range_slices  java.lang.assertionerror: (162950022446285318630909295651345252065,9481098247439719900692337295923514899]  at org.apache.cassandra.db.columnfamilystore.getrangeslice(columnfamilystore.java:1264)  at org.apache.cassandra.service.storageproxy.getrangeslice(storageproxy.java:429)  at org.apache.cassandra.thrift.cassandraserver.get_range_slices(cassandraserver.java:514)  at org.apache.cassandra.thrift.cassandra$processor$get_range_slices.process(cassandra.java:2868)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:2555)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:167)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  debug 16:51:48,838 logged out: #<user allow_all groups=[]> <code> debug 16:51:48,653 range_slice  debug 16:51:48,653 rangeslicecommand <text> runnig mapreduce task on two or more cassandra nodes gives following error: you can reproduce this by just running contrib/word_count example. mapreduce last worked with cassandra 0.7-beta2. important is to run more than one node.",
        "label": 515
    },
    {
        "text": "add ttl support to bulkoutputformat <description> <stacktrace> <code> <text> ",
        "label": 475
    },
    {
        "text": "updating cql created table through cassandra cli transform it into a compact storage table <description> to reproduce : echo \"create table test (aid int, period text, event text, viewer text, primary key (aid, period, event, viewer) );\" | cqlsh -kmykeyspace; echo \"describe table test;\" | cqlsh -kmykeyspace; output >  create table test (  aid int,  period text,  event text,  viewer text,  primary key (aid, period, event, viewer)  ) with  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= {'class': 'sizetieredcompactionstrategy'} and  compression= {'sstable_compression': 'snappycompressor'} ; then do : echo \"update column family test with dclocal_read_repair_chance = 0.1;\" | cassandra-cli -kmykeyspace and finally again : echo \"describe table test;\" | cqlsh -kmykeyspace; output > create table test (  aid int,  column1 text,  column2 text,  column3 text,  column4 text,  value blob,  primary key (aid, column1, column2, column3, column4)  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= {'class': 'sizetieredcompactionstrategy'} and  compression= {'sstable_compression': 'snappycompressor'} ; this is quite annoying in production. if it is happening to you:   update system.schema_columnfamilies set column_aliases = '[\"period\",\"event\",\"viewer\"]' where keyspace_name='mykeyspace' and columnfamily_name='test'; should help restoring the table. (thanks sylvain for this information.)<stacktrace> <code> echo 'create table test (aid int, period text, event text, viewer text, primary key (aid, period, event, viewer) );' | cqlsh -kmykeyspace; echo 'describe table test;' | cqlsh -kmykeyspace; output >  create table test (  aid int,  period text,  event text,  viewer text,  primary key (aid, period, event, viewer)  ) with  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= ; echo 'update column family test with dclocal_read_repair_chance = 0.1;' | cassandra-cli -kmykeyspace and finally again : echo 'describe table test;' | cqlsh -kmykeyspace; create table test (  aid int,  column1 text,  column2 text,  column3 text,  column4 text,  value blob,  primary key (aid, column1, column2, column3, column4)  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='' and  dclocal_read_repair_chance=0.100000 and  gc_grace_seconds=864000 and  read_repair_chance=0.100000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  compaction= ; this is quite annoying in production. if it is happening to you:   update system.schema_columnfamilies set column_aliases = '['period','event','viewer']' where keyspace_name='mykeyspace' and columnfamily_name='test'; should help restoring the table. (thanks sylvain for this information.)<text> to reproduce : and  compression= then do : output > and  compression= ",
        "label": 520
    },
    {
        "text": "npe while freezing a tuple containing a list <description> detected in 2.1.0-rc7 (not released yet): cqlsh:test> create table mytable (k int primary key, v_0 frozen<tuple<list<int>>>); <errormessage code=2000 [syntax error in cql query] message=\"failed parsing statement: [create table mytable (k int primary key, v_0 frozen<tuple<list<int>>>);] reason: nullpointerexception null\"><stacktrace> <code> cqlsh:test> create table mytable (k int primary key, v_0 frozen<tuple<list<int>>>); <errormessage code=2000 [syntax error in cql query] message='failed parsing statement: [create table mytable (k int primary key, v_0 frozen<tuple<list<int>>>);] reason: nullpointerexception null'> <text> detected in 2.1.0-rc7 (not released yet):",
        "label": 394
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh copy tests cqlshcopytest test data validation on read template <description> example failure: http://cassci.datastax.com/job/trunk_offheap_dtest/262/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_data_validation_on_read_template failed on cassci build trunk_offheap_dtest #262 stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 1608, in test_data_validation_on_read_template     self.assertfalse(err)   file \"/usr/lib/python2.7/unittest/case.py\", line 416, in assertfalse     raise self.failureexception(msg) '\\'process importprocess-3:\\\\ntraceback (most recent call last):\\\\n  file \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\\\\n    self.run()\\\\n  file \"/home/automaton/cassandra/bin/../pylib/cqlshlib/copyutil.py\", line 2205, in run\\\\n    self.report_error(exc)\\\\ntypeerror: report_error() takes at least 3 arguments (2 given)\\\\n\\' is not false<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 1608, in test_data_validation_on_read_template     self.assertfalse(err)   file '/usr/lib/python2.7/unittest/case.py', line 416, in assertfalse     raise self.failureexception(msg) '/'process importprocess-3://ntraceback (most recent call last)://n  file '/usr/lib/python2.7/multiprocessing/process.py', line 258, in _bootstrap//n    self.run()//n  file '/home/automaton/cassandra/bin/../pylib/cqlshlib/copyutil.py', line 2205, in run//n    self.report_error(exc)//ntypeerror: report_error() takes at least 3 arguments (2 given)//n/' is not false http://cassci.datastax.com/job/trunk_offheap_dtest/262/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_data_validation_on_read_template <text> example failure: failed on cassci build trunk_offheap_dtest #262",
        "label": 538
    },
    {
        "text": "decide how to deal with conflict between native and user defined functions <description> we have a bunch of native/hardcoded functions (now(), dateof(), ...) and in 3.0, user will be able to define new functions. now, there is a very high change that we will provide more native functions over-time (to be clear, i'm not particularly for adding native functions for allthethings just because we can, but it's clear that we should ultimately provide more than what we have). which begs the question: how do we want to deal with the problem of adding a native function potentially breaking a previously defined user-defined function? a priori i see the following options (maybe there is more?): 1. don't do anything specific, hoping that it won't happen often and consider it a user problem if it does. 2. reserve a big number of names that we're hoping will cover all future need. 3. make native function and user-defined function syntactically distinct so it cannot happen. i'm not a huge fan of solution 1). solution 2) is actually what we did for udt but i think it's somewhat less practical here: there is so much types that it makes sense to provide natively and so it wasn't too hard to come up with a reasonably small list of types name to reserve just in case. this feels a lot harder for functions to me. which leaves solution 3). since we already have the concept of namespaces for functions, a simple idea would be to force user function to have namespace. we could even allow that namespace to be empty as long as we force the namespace separator (so we'd allow bar::foo and ::foo for user functions, but not foo which would be reserved for native function).<stacktrace> <code> <text> we have a bunch of native/hardcoded functions (now(), dateof(), ...) and in 3.0, user will be able to define new functions. now, there is a very high change that we will provide more native functions over-time (to be clear, i'm not particularly for adding native functions for allthethings just because we can, but it's clear that we should ultimately provide more than what we have). which begs the question: how do we want to deal with the problem of adding a native function potentially breaking a previously defined user-defined function? a priori i see the following options (maybe there is more?): i'm not a huge fan of solution 1). solution 2) is actually what we did for udt but i think it's somewhat less practical here: there is so much types that it makes sense to provide natively and so it wasn't too hard to come up with a reasonably small list of types name to reserve just in case. this feels a lot harder for functions to me. which leaves solution 3). since we already have the concept of namespaces for functions, a simple idea would be to force user function to have namespace. we could even allow that namespace to be empty as long as we force the namespace separator (so we'd allow bar::foo and ::foo for user functions, but not foo which would be reserved for native function).",
        "label": 453
    },
    {
        "text": "nullpointer during legacyschemamigrator <description> i'm trying to upgrade my cassandra cluster from 2.2.4 to 3.1.1. i used nodetool upgradesstables  nodetool drain before upgrading. the result is this: info [main] 2015-12-26 22:41:44,114 systemkeyspace.java:1284 - detected version upgrade from 2.2.4 to 3.1.1, snapshotting system keyspace  warn [main] 2015-12-26 22:41:44,318 compressionparams.java:382 - the sstable_compression option has been deprecated. you should use class instead  error [main] 2015-12-26 22:41:44,427 cassandradaemon.java:690 - exception encountered during startup  java.lang.nullpointerexception: null  at org.apache.cassandra.serializers.booleanserializer.deserialize(booleanserializer.java:33) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.serializers.booleanserializer.deserialize(booleanserializer.java:24) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.db.marshal.abstracttype.compose(abstracttype.java:114) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.cql3.untypedresultset$row.getboolean(untypedresultset.java:272) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.decodetablemetadata(legacyschemamigrator.java:264) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.readtablemetadata(legacyschemamigrator.java:250) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.readtable(legacyschemamigrator.java:221) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.lambda$readtables$218(legacyschemamigrator.java:214) ~[apache-cassandra-3.1.1.jar:3.1.1]  at java.util.arraylist.foreach(arraylist.java:1249) ~[na:1.8.0_66]  at org.apache.cassandra.schema.legacyschemamigrator.readtables(legacyschemamigrator.java:214) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.readkeyspace(legacyschemamigrator.java:163) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.lambda$readschema$215(legacyschemamigrator.java:154) ~[apache-cassandra-3.1.1.jar:3.1.1]  at java.util.arraylist.foreach(arraylist.java:1249) ~[na:1.8.0_66]  at org.apache.cassandra.schema.legacyschemamigrator.readschema(legacyschemamigrator.java:154) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.migrate(legacyschemamigrator.java:77) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:223) [apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:549) [apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:677) [apache-cassandra-3.1.1.jar:3.1.1]<stacktrace> info [main] 2015-12-26 22:41:44,114 systemkeyspace.java:1284 - detected version upgrade from 2.2.4 to 3.1.1, snapshotting system keyspace  warn [main] 2015-12-26 22:41:44,318 compressionparams.java:382 - the sstable_compression option has been deprecated. you should use class instead  error [main] 2015-12-26 22:41:44,427 cassandradaemon.java:690 - exception encountered during startup  java.lang.nullpointerexception: null  at org.apache.cassandra.serializers.booleanserializer.deserialize(booleanserializer.java:33) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.serializers.booleanserializer.deserialize(booleanserializer.java:24) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.db.marshal.abstracttype.compose(abstracttype.java:114) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.cql3.untypedresultset$row.getboolean(untypedresultset.java:272) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.decodetablemetadata(legacyschemamigrator.java:264) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.readtablemetadata(legacyschemamigrator.java:250) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.readtable(legacyschemamigrator.java:221) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.lambda$readtables$218(legacyschemamigrator.java:214) ~[apache-cassandra-3.1.1.jar:3.1.1]  at java.util.arraylist.foreach(arraylist.java:1249) ~[na:1.8.0_66]  at org.apache.cassandra.schema.legacyschemamigrator.readtables(legacyschemamigrator.java:214) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.readkeyspace(legacyschemamigrator.java:163) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.lambda$readschema$215(legacyschemamigrator.java:154) ~[apache-cassandra-3.1.1.jar:3.1.1]  at java.util.arraylist.foreach(arraylist.java:1249) ~[na:1.8.0_66]  at org.apache.cassandra.schema.legacyschemamigrator.readschema(legacyschemamigrator.java:154) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.schema.legacyschemamigrator.migrate(legacyschemamigrator.java:77) ~[apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:223) [apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:549) [apache-cassandra-3.1.1.jar:3.1.1]  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:677) [apache-cassandra-3.1.1.jar:3.1.1]<code> <text> i'm trying to upgrade my cassandra cluster from 2.2.4 to 3.1.1. i used nodetool upgradesstables  nodetool drain before upgrading. the result is this: ",
        "label": 18
    },
    {
        "text": "python cql driver does not decode most values <description> most keys, and column name/values are not decoded properly. the attached cql input can be used to demonstrate: note: requires the patch from cassandra-2505 to be applied $ drivers/py/cqlsh localhost 9170 < repro.cql   | '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01','\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01' | '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02','\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02' e\ufffd#j\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd | 'e\\xe2#\\x01j\\xa2\\x11\\xe0\\x00\\x00\\xfe\\x8e\\xbe\\xea\\xd9\\xff','e\\xe2#\\x02j\\xa2\\x11\\xe0\\x00\\x00\\xfe\\x8e\\xbe\\xea\\xd9\\xff' for all practical purposes, this renders the driver useless for everything but strings.<stacktrace> <code> $ drivers/py/cqlsh localhost 9170 < repro.cql   | '/x00/x00/x00/x00/x00/x00/x00/x01','/x00/x00/x00/x00/x00/x00/x00/x01' | '/x00/x00/x00/x00/x00/x00/x00/x02','/x00/x00/x00/x00/x00/x00/x00/x02' e#j | 'e/xe2#/x01j/xa2/x11/xe0/x00/x00/xfe/x8e/xbe/xea/xd9/xff','e/xe2#/x02j/xa2/x11/xe0/x00/x00/xfe/x8e/xbe/xea/xd9/xff' <text> most keys, and column name/values are not decoded properly. the attached cql input can be used to demonstrate: note: requires the patch from cassandra-2505 to be applied for all practical purposes, this renders the driver useless for everything but strings.",
        "label": 538
    },
    {
        "text": "cqlsh should support collections in copy from <description> concrete operation is as follows.  ----------------------------------------------------------  (1)map type's export/import  <export>  [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace maptestks with replication = { 'class' : 'simplestrategy', 'replication_factor' : '1' } ;  cqlsh> use maptestks;  cqlsh:maptestks> create table maptestcf (rowkey varchar primary key, targetmap map<varchar,varchar>);  cqlsh:maptestks> insert into maptestcf (rowkey, targetmap) values ('rowkey', {'mapkey':'mapvalue'} );  cqlsh:maptestks> select * from maptestcf;  rowkey | targetmap  -------+-------------------  rowkey | {mapkey: mapvalue} cqlsh:maptestks> copy maptestcf to 'maptestcf-20130619.txt';  1 rows exported in 0.008 seconds.  cqlsh:maptestks> exit; [root@castor bin]# cat maptestcf-20130619.txt  rowkey, {mapkey: mapvalue} <------------------------(a)  <import>  [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace mapimptestks with replication = { 'class' : 'simplestrategy', 'replication_factor' : '1' } ;  cqlsh> use mapimptestks;  cqlsh:mapimptestks> create table mapimptestcf (rowkey varchar primary key, targetmap map<varchar,varchar>); cqlsh:mapimptestks> copy mapimptestcf from ' maptestcf-20130619.txt ';  bad request: line 1:83 no viable alternative at input '}'  aborting import at record #0 (line 1). previously-inserted values still present.  0 rows imported in 0.025 seconds.  ----------------------------------------------------------  (2)list type's export/import  <export>  [root@castor bin]#./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace listtestks with replication = { 'class' : 'simplestrategy', 'replication_factor' : '1' } ;  cqlsh> use listtestks;  cqlsh:listtestks> create table listtestcf (rowkey varchar primary key, value list<varchar>);  cqlsh:listtestks> insert into listtestcf (rowkey,value) values ('rowkey',['value1','value2']);  cqlsh:listtestks> select * from listtestcf;  rowkey | value  -------+-----------------  rowkey | [value1, value2] cqlsh:listtestks> copy listtestcf to 'listtestcf-20130619.txt';  1 rows exported in 0.014 seconds.  cqlsh:listtestks> exit; [root@castor bin]# cat listtestcf-20130619.txt  rowkey,\"[value1, value2]\"  <------------------------(b)  <export>  [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace listimptestks with replication = { 'class' : 'simplestrategy', 'replication_factor' : '1' } ;  cqlsh> use listimptestks;  cqlsh:listimptestks> create table listimptestcf (rowkey varchar primary key, value list<varchar>);  cqlsh:listimptestks> copy listimptestcf from ' listtestcf-20130619.txt ';  bad request: line 1:79 no viable alternative at input ']'  aborting import at record #0 (line 1). previously-inserted values still present.  0 rows imported in 0.030 seconds.  ----------------------------------------------------------  reference: (correct, or error, in another dimension) manually, i have rewritten the export file.  [root@castor bin]# cat nlisttestcf-20130619.txt  rowkey,\"['value1',' value2']\" ....  cqlsh:listimptestks> copy listimptestcf from 'nlisttestcf-20130619.txt';  1 rows imported in 0.035 seconds. cqlsh:listimptestks> select * from implisttestcf;  rowkey | value  -------+-----------------  rowkey | [value1, value2]  cqlsh:implisttestks> exit; [root@castor bin]# cat nmaptestcf-20130619.txt  rowkey,\u201d {'mapkey': 'mapvalue'} \u201d [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> use mapimptestks;  cqlsh:mapimptestks> copy mapimptestcf from 'nmaptestcf-20130619.txt';  1 rows imported in 0.023 seconds.  cqlsh:mapimptestks> select * from mapimptestcf;  rowkey | targetmap  -------+-------------------  rowkey | {mapkey: mapvalue} (it appears to be as normal processing.)  ----------------------------------------------------------  please confirm from the operation described above.  comparing the above (a) and (b), in the data format of export file,   only the presence or absence of (\"),   it suggests a lack of consistency of the treatment.<stacktrace> <code> concrete operation is as follows.  ----------------------------------------------------------  (1)map type's export/import  <export>  [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace maptestks with replication = );  cqlsh:maptestks> select * from maptestcf; rowkey | targetmap  -------+-------------------  rowkey | cqlsh:maptestks> copy maptestcf to 'maptestcf-20130619.txt';  1 rows exported in 0.008 seconds.  cqlsh:maptestks> exit; <------------------------(a)  <import>  [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace mapimptestks with replication = ;  cqlsh> use mapimptestks;  cqlsh:mapimptestks> create table mapimptestcf (rowkey varchar primary key, targetmap map<varchar,varchar>); ;  cqlsh> use listtestks;  cqlsh:listtestks> create table listtestcf (rowkey varchar primary key, value list<varchar>);  cqlsh:listtestks> insert into listtestcf (rowkey,value) values ('rowkey',['value1','value2']);  cqlsh:listtestks> select * from listtestcf; rowkey | value  -------+-----------------  rowkey | [value1, value2] cqlsh:listtestks> copy listtestcf to 'listtestcf-20130619.txt';  1 rows exported in 0.014 seconds.  cqlsh:listtestks> exit; [root@castor bin]# cat listtestcf-20130619.txt  rowkey,'[value1, value2]'  <------------------------(b)  <export>  [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace listimptestks with replication = ;  cqlsh> use listimptestks;  cqlsh:listimptestks> create table listimptestcf (rowkey varchar primary key, value list<varchar>);  cqlsh:listimptestks> copy listimptestcf from ' listtestcf-20130619.txt ';  bad request: line 1:79 no viable alternative at input ']'  aborting import at record #0 (line 1). previously-inserted values still present.  0 rows imported in 0.030 seconds.  ----------------------------------------------------------  reference: (correct, or error, in another dimension) manually, i have rewritten the export file.  [root@castor bin]# cat nlisttestcf-20130619.txt  rowkey,'['value1',' value2']' cqlsh:listimptestks> select * from implisttestcf;  rowkey | value  -------+-----------------  rowkey | [value1, value2]  cqlsh:implisttestks> exit; [root@castor bin]# cat nmaptestcf-20130619.txt  rowkey,' ' [root@castor bin]# ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> use mapimptestks;  cqlsh:mapimptestks> copy mapimptestcf from 'nmaptestcf-20130619.txt';  1 rows imported in 0.023 seconds.  cqlsh:mapimptestks> select * from mapimptestcf; rowkey | targetmap  -------+-------------------  rowkey | <text> ;  cqlsh> use maptestks;  cqlsh:maptestks> create table maptestcf (rowkey varchar primary key, targetmap map<varchar,varchar>);  cqlsh:maptestks> insert into maptestcf (rowkey, targetmap) values ('rowkey', [root@castor bin]# cat maptestcf-20130619.txt  rowkey, cqlsh:mapimptestks> copy mapimptestcf from ' maptestcf-20130619.txt ';  bad request: line 1:83 no viable alternative at input '}'  aborting import at record #0 (line 1). previously-inserted values still present.  0 rows imported in 0.025 seconds.  ----------------------------------------------------------  (2)list type's export/import  <export>  [root@castor bin]#./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 3.0.2 | cassandra 1.2.5 | cql spec 3.0.0 | thrift protocol 19.36.0]  use help for help.  cqlsh> create keyspace listtestks with replication = ....  cqlsh:listimptestks> copy listimptestcf from 'nlisttestcf-20130619.txt';  1 rows imported in 0.035 seconds. (it appears to be as normal processing.)  ----------------------------------------------------------  please confirm from the operation described above.  comparing the above (a) and (b), in the data format of export file,   only the presence or absence of ('),   it suggests a lack of consistency of the treatment.",
        "label": 18
    },
    {
        "text": "new cqlsh formatter for cassandra driver type supporting nested  frozen collections <description> supporting nested frozen collections required a new type returned for map objects in cassandra-driver. the attached patch adds a formatter for this type. references:  driver issue  changeset<stacktrace> <code> <text> supporting nested frozen collections required a new type returned for map objects in cassandra-driver. the attached patch adds a formatter for this type. references:  driver issue  changeset",
        "label": 8
    },
    {
        "text": " dreplace token leaves old node  ip  in the gossip with the token  <description> https://issues.apache.org/jira/browse/cassandra-957 introduce a -dreplace_token, however, the replaced ip keeps on showing up in the gossiper when starting the replacement node:  info [thread-2] 2012-01-12 23:59:35,162 cassandradaemon.java (line 213) listening for thrift clients...  info [gossipstage:1] 2012-01-12 23:59:35,173 gossiper.java (line 836) node /50.56.59.68 has restarted, now up  info [gossipstage:1] 2012-01-12 23:59:35,174 gossiper.java (line 804) inetaddress /50.56.59.68 is now up  info [gossipstage:1] 2012-01-12 23:59:35,175 storageservice.java (line 988) node /50.56.59.68 state jump to normal  info [gossipstage:1] 2012-01-12 23:59:35,176 gossiper.java (line 836) node /50.56.58.55 has restarted, now up  info [gossipstage:1] 2012-01-12 23:59:35,176 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-12 23:59:35,177 storageservice.java (line 1016) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  ignoring /50.56.58.55  info [gossiptasks:1] 2012-01-12 23:59:45,048 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossiptasks:1] 2012-01-13 00:00:06,062 gossiper.java (line 632) fatclient /50.56.58.55 has been silent for 30000ms, removing from gossip  info [gossipstage:1] 2012-01-13 00:01:06,320 gossiper.java (line 838) node /50.56.58.55 is now part of the cluster  info [gossipstage:1] 2012-01-13 00:01:06,320 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-13 00:01:06,321 storageservice.java (line 1016) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  ignoring /50.56.58.55  info [gossiptasks:1] 2012-01-13 00:01:16,106 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossiptasks:1] 2012-01-13 00:01:37,121 gossiper.java (line 632) fatclient /50.56.58.55 has been silent for 30000ms, removing from gossip  info [gossipstage:1] 2012-01-13 00:02:37,352 gossiper.java (line 838) node /50.56.58.55 is now part of the cluster  info [gossipstage:1] 2012-01-13 00:02:37,353 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-13 00:02:37,353 storageservice.java (line 1016) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  ignoring /50.56.58.55  info [gossiptasks:1] 2012-01-13 00:02:47,158 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossipstage:1] 2012-01-13 00:02:50,162 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossipstage:1] 2012-01-13 00:02:50,163 storageservice.java (line 1156) removing token 122029383590318827259508597176866581733 for /50.56.58.55 in the above, /50.56.58.55 was the replaced ip. tried adding the \"gossiper.instance.removeendpoint(endpoint);\" in the storageservice.java where the message 'nodes %s and %s have the same token %s. ignoring %s\",' seems only have fixed this temporary. here is a ring output: riptano@action-quick:~/work/cassandra$ ./bin/nodetool -h localhost ring address         dc          rack        status state   load            owns    token                                                                                                                       85070591730234615865843651857942052864       50.56.59.68     datacenter1 rack1       up     normal  6.67 kb         85.56%  60502102442797279294142560823234402248       50.56.31.186    datacenter1 rack1       up     normal  11.12 kb        14.44%  85070591730234615865843651857942052864  gossipinfo: $ ./bin/nodetool -h localhost gossipinfo /50.56.58.55   load:6835.0   schema:00000000-0000-1000-0000-000000000000   rpc_address:50.56.58.55   status:normal,85070591730234615865843651857942052864   release_version:1.0.7-snapshot /50.56.59.68   load:6835.0   schema:00000000-0000-1000-0000-000000000000   rpc_address:50.56.59.68   status:normal,60502102442797279294142560823234402248   release_version:1.0.7-snapshot action-quick2/50.56.31.186   load:11387.0   schema:00000000-0000-1000-0000-000000000000   rpc_address:50.56.31.186   status:normal,85070591730234615865843651857942052864   release_version:1.0.7-snapshot note that at 1 point earlier it seems to have been removed: $ ./bin/nodetool -h localhost gossipinfo  /50.56.59.68  load:13815.0  schema:00000000-0000-1000-0000-000000000000  rpc_address:50.56.59.68  status:normal,60502102442797279294142560823234402248  release_version:1.0.7-snapshot  action-quick2/50.56.31.186  load:13725.0  schema:00000000-0000-1000-0000-000000000000  rpc_address:50.56.31.186  status:normal,85070591730234615865843651857942052864  release_version:1.0.7-snapshot riptano@action-quick2:~/work/cassandra$ info [gossipstage:1] 2012-01-13 01:03:30,073 gossiper.java (line 838) node /50.56.58.55 is now part of the cluster  info [gossipstage:1] 2012-01-13 01:03:30,073 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-13 01:03:30,074 storageservice.java (line 1017) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864. ignoring /50.56.58.55<stacktrace> <code>  info [thread-2] 2012-01-12 23:59:35,162 cassandradaemon.java (line 213) listening for thrift clients...  info [gossipstage:1] 2012-01-12 23:59:35,173 gossiper.java (line 836) node /50.56.59.68 has restarted, now up  info [gossipstage:1] 2012-01-12 23:59:35,174 gossiper.java (line 804) inetaddress /50.56.59.68 is now up  info [gossipstage:1] 2012-01-12 23:59:35,175 storageservice.java (line 988) node /50.56.59.68 state jump to normal  info [gossipstage:1] 2012-01-12 23:59:35,176 gossiper.java (line 836) node /50.56.58.55 has restarted, now up  info [gossipstage:1] 2012-01-12 23:59:35,176 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-12 23:59:35,177 storageservice.java (line 1016) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  ignoring /50.56.58.55  info [gossiptasks:1] 2012-01-12 23:59:45,048 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossiptasks:1] 2012-01-13 00:00:06,062 gossiper.java (line 632) fatclient /50.56.58.55 has been silent for 30000ms, removing from gossip  info [gossipstage:1] 2012-01-13 00:01:06,320 gossiper.java (line 838) node /50.56.58.55 is now part of the cluster  info [gossipstage:1] 2012-01-13 00:01:06,320 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-13 00:01:06,321 storageservice.java (line 1016) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  ignoring /50.56.58.55  info [gossiptasks:1] 2012-01-13 00:01:16,106 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossiptasks:1] 2012-01-13 00:01:37,121 gossiper.java (line 632) fatclient /50.56.58.55 has been silent for 30000ms, removing from gossip  info [gossipstage:1] 2012-01-13 00:02:37,352 gossiper.java (line 838) node /50.56.58.55 is now part of the cluster  info [gossipstage:1] 2012-01-13 00:02:37,353 gossiper.java (line 804) inetaddress /50.56.58.55 is now up  info [gossipstage:1] 2012-01-13 00:02:37,353 storageservice.java (line 1016) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  ignoring /50.56.58.55  info [gossiptasks:1] 2012-01-13 00:02:47,158 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossipstage:1] 2012-01-13 00:02:50,162 gossiper.java (line 818) inetaddress /50.56.58.55 is now dead.  info [gossipstage:1] 2012-01-13 00:02:50,163 storageservice.java (line 1156) removing token 122029383590318827259508597176866581733 for /50.56.58.55 riptano@action-quick:~/work/cassandra$ ./bin/nodetool -h localhost ring address         dc          rack        status state   load            owns    token                                                                                                                       85070591730234615865843651857942052864       50.56.59.68     datacenter1 rack1       up     normal  6.67 kb         85.56%  60502102442797279294142560823234402248       50.56.31.186    datacenter1 rack1       up     normal  11.12 kb        14.44%  85070591730234615865843651857942052864  $ ./bin/nodetool -h localhost gossipinfo /50.56.58.55   load:6835.0   schema:00000000-0000-1000-0000-000000000000   rpc_address:50.56.58.55   status:normal,85070591730234615865843651857942052864   release_version:1.0.7-snapshot /50.56.59.68   load:6835.0   schema:00000000-0000-1000-0000-000000000000   rpc_address:50.56.59.68   status:normal,60502102442797279294142560823234402248   release_version:1.0.7-snapshot action-quick2/50.56.31.186   load:11387.0   schema:00000000-0000-1000-0000-000000000000   rpc_address:50.56.31.186   status:normal,85070591730234615865843651857942052864   release_version:1.0.7-snapshot https://issues.apache.org/jira/browse/cassandra-957 introduce a -dreplace_token, $ ./bin/nodetool -h localhost gossipinfo  /50.56.59.68  load:13815.0  schema:00000000-0000-1000-0000-000000000000  rpc_address:50.56.59.68  status:normal,60502102442797279294142560823234402248  release_version:1.0.7-snapshot  action-quick2/50.56.31.186  load:13725.0  schema:00000000-0000-1000-0000-000000000000  rpc_address:50.56.31.186  status:normal,85070591730234615865843651857942052864  release_version:1.0.7-snapshot riptano@action-quick2:~/work/cassandra$ info [gossipstage:1] 2012-01-13 01:03:30,073 gossiper.java (line 838) node /50.56.58.55 is now part of the cluster info [gossipstage:1] 2012-01-13 01:03:30,073 gossiper.java (line 804) inetaddress /50.56.58.55 is now up info [gossipstage:1] 2012-01-13 01:03:30,074 storageservice.java (line 1017) nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864. ignoring /50.56.58.55<text> however, the replaced ip keeps on showing up in the gossiper when starting the replacement node: in the above, /50.56.58.55 was the replaced ip. tried adding the 'gossiper.instance.removeendpoint(endpoint);' in the storageservice.java where the message 'nodes %s and %s have the same token %s. ignoring %s',' seems only have fixed this temporary. here is a ring output: gossipinfo: note that at 1 point earlier it seems to have been removed: ",
        "label": 555
    },
    {
        "text": "datatracker view markcompacting adds all sstables and marks them as compacting <description> at some point if the list isn't cleaned up with this symptom compactions will stop until the server is restarted.<stacktrace> <code> <text> at some point if the list isn't cleaned up with this symptom compactions will stop until the server is restarted.",
        "label": 68
    },
    {
        "text": "under some races commit log may incorrectly think it has unflushed data <description> this can mainfest itself as a \"failed to force-recycle all segments; at least one segment is still in use with dirty cfs.\" message after cassandra-11828.<stacktrace> <code> <text> this can mainfest itself as a 'failed to force-recycle all segments; at least one segment is still in use with dirty cfs.' message after cassandra-11828.",
        "label": 86
    },
    {
        "text": "require specifying rows per partition to cache <description> we should require specifying rows_to_cache_per_partition for new tables or newly altered when row caching is enabled. pre-upgrade should be grandfathered in as all to match existing semantics.<stacktrace> <code> <text> we should require specifying rows_to_cache_per_partition for new tables or newly altered when row caching is enabled. pre-upgrade should be grandfathered in as all to match existing semantics.",
        "label": 321
    },
    {
        "text": "concurrentmodificationexception in nodetool upgradesstables <description> when upgrading from 2.2.8 to cassandra 3.11 we were able to upgrade all other sstables except 1 file on 3 nodes (out of 4). those are related to 2 different tables. upgrading sstables fails with concurrentmodificationexception. $ nodetool upgradesstables error: null -- stacktrace -- java.util.concurrentmodificationexception at java.util.treemap$privateentryiterator.nextentry(treemap.java:1211) at java.util.treemap$keyiterator.next(treemap.java:1265) at org.apache.cassandra.utils.streaminghistogram.flushhistogram(streaminghistogram.java:168) at org.apache.cassandra.utils.streaminghistogram.update(streaminghistogram.java:124) at org.apache.cassandra.utils.streaminghistogram.update(streaminghistogram.java:96) at org.apache.cassandra.io.sstable.metadata.metadatacollector.updatelocaldeletiontime(metadatacollector.java:209) at org.apache.cassandra.io.sstable.metadata.metadatacollector.update(metadatacollector.java:182) at org.apache.cassandra.db.rows.cells.collectstats(cells.java:44) at org.apache.cassandra.db.rows.rows.lambda$collectstats$0(rows.java:102) at org.apache.cassandra.utils.btree.btree.applyforwards(btree.java:1242) at org.apache.cassandra.utils.btree.btree.apply(btree.java:1197) at org.apache.cassandra.db.rows.btreerow.apply(btreerow.java:172) at org.apache.cassandra.db.rows.rows.collectstats(rows.java:97) at org.apache.cassandra.io.sstable.format.big.bigtablewriter$statscollector.applytorow(bigtablewriter.java:237) at org.apache.cassandra.db.transform.baserows.hasnext(baserows.java:141) at org.apache.cassandra.db.columnindex.buildrowindex(columnindex.java:110) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.append(bigtablewriter.java:173) at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:135) at org.apache.cassandra.db.compaction.writers.defaultcompactionwriter.realappend(defaultcompactionwriter.java:65) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.append(compactionawarewriter.java:141) at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:201) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:85) at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:61) at org.apache.cassandra.db.compaction.compactionmanager$5.execute(compactionmanager.java:428) at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:315) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:81) at java.lang.thread.run(thread.java:745)<stacktrace> $ nodetool upgradesstables error: null -- stacktrace -- java.util.concurrentmodificationexception at java.util.treemap$privateentryiterator.nextentry(treemap.java:1211) at java.util.treemap$keyiterator.next(treemap.java:1265) at org.apache.cassandra.utils.streaminghistogram.flushhistogram(streaminghistogram.java:168) at org.apache.cassandra.utils.streaminghistogram.update(streaminghistogram.java:124) at org.apache.cassandra.utils.streaminghistogram.update(streaminghistogram.java:96) at org.apache.cassandra.io.sstable.metadata.metadatacollector.updatelocaldeletiontime(metadatacollector.java:209) at org.apache.cassandra.io.sstable.metadata.metadatacollector.update(metadatacollector.java:182) at org.apache.cassandra.db.rows.cells.collectstats(cells.java:44) at org.apache.cassandra.db.rows.rows.lambda$collectstats$0(rows.java:102) at org.apache.cassandra.utils.btree.btree.applyforwards(btree.java:1242) at org.apache.cassandra.utils.btree.btree.apply(btree.java:1197) at org.apache.cassandra.db.rows.btreerow.apply(btreerow.java:172) at org.apache.cassandra.db.rows.rows.collectstats(rows.java:97) at org.apache.cassandra.io.sstable.format.big.bigtablewriter$statscollector.applytorow(bigtablewriter.java:237) at org.apache.cassandra.db.transform.baserows.hasnext(baserows.java:141) at org.apache.cassandra.db.columnindex.buildrowindex(columnindex.java:110) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.append(bigtablewriter.java:173) at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:135) at org.apache.cassandra.db.compaction.writers.defaultcompactionwriter.realappend(defaultcompactionwriter.java:65) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.append(compactionawarewriter.java:141) at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:201) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:85) at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:61) at org.apache.cassandra.db.compaction.compactionmanager$5.execute(compactionmanager.java:428) at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:315) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:81) at java.lang.thread.run(thread.java:745) <code> <text> when upgrading from 2.2.8 to cassandra 3.11 we were able to upgrade all other sstables except 1 file on 3 nodes (out of 4). those are related to 2 different tables. upgrading sstables fails with concurrentmodificationexception.",
        "label": 241
    },
    {
        "text": "adjust stop server ps1 to behave similarly to stop behavior in cassandra init script <description> i have a test that repeatedly rolls nodes in a c* cluster. occasionally i run into a case where a node is not restarted because stop-server.ps1 returns before the c* process had terminated. this is because stop-server.ps1 currently will wait for up to 2 seconds for cassandra to exit from a ctrl+c and then returns. in practice it can take cassandra longer than that to exit and the script doesn't give you any indication that cassandra is still running. proposing that stop-server.ps1 behaves the same way as the 'cassandra' init script provided by dsc21 does: start-stop-daemon -k -p \"$pidfile\" -r term/30/kill/5 >/dev/null the init script currently sends a sigterm (ctrl+c) to the process and if it hasn't terminated after 30 seconds sends a sigkill to it.<stacktrace> <code> start-stop-daemon -k -p '$pidfile' -r term/30/kill/5 >/dev/null <text> i have a test that repeatedly rolls nodes in a c* cluster. occasionally i run into a case where a node is not restarted because stop-server.ps1 returns before the c* process had terminated. this is because stop-server.ps1 currently will wait for up to 2 seconds for cassandra to exit from a ctrl+c and then returns. in practice it can take cassandra longer than that to exit and the script doesn't give you any indication that cassandra is still running. proposing that stop-server.ps1 behaves the same way as the 'cassandra' init script provided by dsc21 does: the init script currently sends a sigterm (ctrl+c) to the process and if it hasn't terminated after 30 seconds sends a sigkill to it.",
        "label": 44
    },
    {
        "text": "restarting node crashes with npe when  while replaying the commitlog  the cfmetadata is requested <description> removing the commitlog directory completely fixes this. i can reliably reproduce it by 1) starting and configuring a schema with one keyspace, one super cf with longtype supercolumns; 2) inserting data; 3) shutting down and restarting the node. here's my schema expressed in cassidy.pl, should be obvious what the parameters are:  ./cassidy.pl -server x -port y -keyspace system 'kdefine test org.apache.cassandra.locator.rackunawarestrategy 2 org.apache.cassandra.locator.endpointsnitch'  ./cassidy.pl -server x -port y -keyspace test 'fdefine status super longtype bytestype comment=statuschanges,row_cache_size=0,key_cache_size=20000' the problem seems to be related to cassandra-44 as it happens when the cf metadata is requested but i don't know what's causing it. 10/04/16 15:25:11 info commitlog.commitlog: replaying /home/cassandra/commitlog/commitlog-1271449410100.log, /home/cassandra/commitlog/commitlog-1271449378151.log, /home/cassandra/commitlog/commitlog-1271449415800.log  java.lang.reflect.invocationtargetexception  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.commons.daemon.support.daemonloader.load(daemonloader.java:160)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.table.<init>(table.java:261)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:233)  at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:172)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:104)  at org.apache.cassandra.thrift.cassandradaemon.init(cassandradaemon.java:151)  ... 5 more<stacktrace> 10/04/16 15:25:11 info commitlog.commitlog: replaying /home/cassandra/commitlog/commitlog-1271449410100.log, /home/cassandra/commitlog/commitlog-1271449378151.log, /home/cassandra/commitlog/commitlog-1271449415800.log  java.lang.reflect.invocationtargetexception  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.commons.daemon.support.daemonloader.load(daemonloader.java:160)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.table.<init>(table.java:261)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:233)  at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:172)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:104)  at org.apache.cassandra.thrift.cassandradaemon.init(cassandradaemon.java:151)  ... 5 more<code> here's my schema expressed in cassidy.pl, should be obvious what the parameters are:  ./cassidy.pl -server x -port y -keyspace system 'kdefine test org.apache.cassandra.locator.rackunawarestrategy 2 org.apache.cassandra.locator.endpointsnitch'  ./cassidy.pl -server x -port y -keyspace test 'fdefine status super longtype bytestype comment=statuschanges,row_cache_size=0,key_cache_size=20000' <text> removing the commitlog directory completely fixes this. i can reliably reproduce it by 1) starting and configuring a schema with one keyspace, one super cf with longtype supercolumns; 2) inserting data; 3) shutting down and restarting the node. the problem seems to be related to cassandra-44 as it happens when the cf metadata is requested but i don't know what's causing it. ",
        "label": 186
    },
    {
        "text": "a hadoop output format that targets cassandra <description> currently, there exists a hadoop-specific input format (viz., columnfamilyinputformat) that allows one to iterate over the rows in a given cassandra column family and treat it as the input to a hadoop map task. by the same token, one may need to feed the output of a hadoop reduce task into a cassandra column family, for which no mechanism exists today. this calls for the definition of a hadoop-specific output format which accepts a pair of key and columns, and writes it out to a given column family. here, we describe an output format known as columnfamilyoutputformat, which allows reduce tasks to persist keys and their associated columns as cassandra rows in a given column family. by default, it prevents overwriting existing rows in the column family, by ensuring at initialization time that it contains no rows in the given slice predicate. for the sake of speed, it employs a lazy write-back caching mechanism, where its record writer batches mutations created based on the reduce's inputs (in a task-specific map) but stops short of actually mutating the rows. the latter responsibility falls on its output committer, which makes the changes official by sending a batch mutate request to cassandra. the record writer, which is called columnfamilyrecordwriter, maps the input <key, value> pairs to a cassandra column family. in particular, it creates mutations for each column in the value, which it then associates with the key, and in turn the responsible endpoint. note that, given that round trips to the server are fairly expensive, it merely batches the mutations in-memory, and leaves it on the output committer to send the batched mutations to the server. furthermore, the writer groups the mutations by the endpoint responsible for the rows being affected. this allows the output committer to execute the mutations in parallel, on an endpoint-by-endpoint basis. the output committer, which is called columnfamilyoutputcommitter, traverses the mutations collected by the record writer, and sends them to the endpoints responsible for them. since the total set of mutations is partitioned by their endpoints, each of which can be performed in parallel, it allows us to commit the mutations using multiple threads, one per endpoint. as a result, it reduces the time it takes to propagate the mutations to the server considering that (a) the client eliminates one network hop that the server would otherwise have had to make and (b) each endpoint node has to deal with but a sub-set of the total set of mutations. for convenience, we also define a default reduce task, called columnfamilyoutputreducer, which collects the columns in the input value and maps them to a data structure expected by cassandra. by default, it assumes the input value to be in the form of a columnwritable, which denotes a name value pair corresponding to a certain column. this reduce task is in turn used by the attached test case, which maps every <key, value> pair in a sample input sequence file to a <key, column> pair, and then reduces them by aggregating columns corresponding to the same key. eventually, the batched <key, columns> pairs are written to the column family associated with the output format.<stacktrace> <code> <text> currently, there exists a hadoop-specific input format (viz., columnfamilyinputformat) that allows one to iterate over the rows in a given cassandra column family and treat it as the input to a hadoop map task. by the same token, one may need to feed the output of a hadoop reduce task into a cassandra column family, for which no mechanism exists today. this calls for the definition of a hadoop-specific output format which accepts a pair of key and columns, and writes it out to a given column family. here, we describe an output format known as columnfamilyoutputformat, which allows reduce tasks to persist keys and their associated columns as cassandra rows in a given column family. by default, it prevents overwriting existing rows in the column family, by ensuring at initialization time that it contains no rows in the given slice predicate. for the sake of speed, it employs a lazy write-back caching mechanism, where its record writer batches mutations created based on the reduce's inputs (in a task-specific map) but stops short of actually mutating the rows. the latter responsibility falls on its output committer, which makes the changes official by sending a batch mutate request to cassandra. the record writer, which is called columnfamilyrecordwriter, maps the input <key, value> pairs to a cassandra column family. in particular, it creates mutations for each column in the value, which it then associates with the key, and in turn the responsible endpoint. note that, given that round trips to the server are fairly expensive, it merely batches the mutations in-memory, and leaves it on the output committer to send the batched mutations to the server. furthermore, the writer groups the mutations by the endpoint responsible for the rows being affected. this allows the output committer to execute the mutations in parallel, on an endpoint-by-endpoint basis. the output committer, which is called columnfamilyoutputcommitter, traverses the mutations collected by the record writer, and sends them to the endpoints responsible for them. since the total set of mutations is partitioned by their endpoints, each of which can be performed in parallel, it allows us to commit the mutations using multiple threads, one per endpoint. as a result, it reduces the time it takes to propagate the mutations to the server considering that (a) the client eliminates one network hop that the server would otherwise have had to make and (b) each endpoint node has to deal with but a sub-set of the total set of mutations. for convenience, we also define a default reduce task, called columnfamilyoutputreducer, which collects the columns in the input value and maps them to a data structure expected by cassandra. by default, it assumes the input value to be in the form of a columnwritable, which denotes a name value pair corresponding to a certain column. this reduce task is in turn used by the attached test case, which maps every <key, value> pair in a sample input sequence file to a <key, column> pair, and then reduces them by aggregating columns corresponding to the same key. eventually, the batched <key, columns> pairs are written to the column family associated with the output format.",
        "label": 294
    },
    {
        "text": "jdbc driver does not build <description> need a way to build (and run tests for) the java driver. also: still some vestigal references to drivers/ in trunk build.xml. should we remove drivers/ from the 0.8 branch as well?<stacktrace> <code> <text> need a way to build (and run tests for) the java driver. also: still some vestigal references to drivers/ in trunk build.xml. should we remove drivers/ from the 0.8 branch as well?",
        "label": 449
    },
    {
        "text": "change value to binary from string <description> in the thrift interface the value is of type string but internally in cassandra it is handled as a byte array. exposing the value as thrift type binary would avoid conversions and charset issues.<stacktrace> <code> <text> in the thrift interface the value is of type string but internally in cassandra it is handled as a byte array. exposing the value as thrift type binary would avoid conversions and charset issues.",
        "label": 274
    },
    {
        "text": "make cache saving less contentious <description> the current default for saving key caches is every hour. additionally the default timeout for flushing memtables is every hour. i've seen situations where both of these occuring at the same time every hour causes enough pressure on the node to have it drop messages and other nodes mark it dead. this happens across the cluster and results in flapping. we should do something to spread this out. perhaps staggering cache saves/flushes that occur due to timeouts.<stacktrace> <code> <text> the current default for saving key caches is every hour. additionally the default timeout for flushing memtables is every hour. i've seen situations where both of these occuring at the same time every hour causes enough pressure on the node to have it drop messages and other nodes mark it dead. this happens across the cluster and results in flapping. we should do something to spread this out. perhaps staggering cache saves/flushes that occur due to timeouts.",
        "label": 274
    },
    {
        "text": "broken get paged slice <description> get_paged_slice doesn't reset the start column filter for the second returned row sometimes. so instead of getting a slice: row 0: <start_column>...<last_column_in_row>  row 1: <first column in a row>...<last_column_in_row>  row 2: <first column in a row>... you sometimes get: row 0: <start_column>...<last_column_in_row>  row 1: <start_column>...<last_column_in_row>  row 2: <first column in a row>...<stacktrace> <code> <text> get_paged_slice doesn't reset the start column filter for the second returned row sometimes. so instead of getting a slice: row 0: <start_column>...<last_column_in_row>  row 1: <first column in a row>...<last_column_in_row>  row 2: <first column in a row>... you sometimes get: row 0: <start_column>...<last_column_in_row>  row 1: <start_column>...<last_column_in_row>  row 2: <first column in a row>...",
        "label": 432
    },
    {
        "text": "add configuration setting to cap the number of thrift connections <description> at least until cassandra-1405 is done, it's useful to have a connection cap to prevent misbehaving clients from dosing the server.<stacktrace> <code> <text> at least until cassandra-1405 is done, it's useful to have a connection cap to prevent misbehaving clients from dosing the server.",
        "label": 521
    },
    {
        "text": "pluggable compaction <description> in cassandra-1608, i proposed some changes on how compaction works. i think it also makes sense to allow the ability to have pluggable compaction per cf. there could be many types of workloads where this makes sense. one example we had at digg was to completely throw away certain sstables after n days. this ticket addresses making compaction pluggable only.<stacktrace> <code> <text> in cassandra-1608, i proposed some changes on how compaction works. i think it also makes sense to allow the ability to have pluggable compaction per cf. there could be many types of workloads where this makes sense. one example we had at digg was to completely throw away certain sstables after n days. this ticket addresses making compaction pluggable only.",
        "label": 15
    },
    {
        "text": "fat clients are never removed <description> after a failed bootstrap, these lines repeat infinitely:  info [timer-0] 2010-11-11 01:58:32,708 gossiper.java (line 406) fatclient /10.104.73.164 has been silent for 3600000ms, removing from gossip  info [gmfd:1] 2010-11-11 01:59:03,685 gossiper.java (line 591) node /10.104.73.164 is now part of the cluster changing the ip on the node but using the same token causes a conflict, requiring either a full cluster restart or changing the token. this is especially easy to run into in practice in a virtual environment such as ec2.<stacktrace> <code> info [timer-0] 2010-11-11 01:58:32,708 gossiper.java (line 406) fatclient /10.104.73.164 has been silent for 3600000ms, removing from gossip  info [gmfd:1] 2010-11-11 01:59:03,685 gossiper.java (line 591) node /10.104.73.164 is now part of the cluster <text> after a failed bootstrap, these lines repeat infinitely: changing the ip on the node but using the same token causes a conflict, requiring either a full cluster restart or changing the token. this is especially easy to run into in practice in a virtual environment such as ec2.",
        "label": 85
    },
    {
        "text": "request to change the accessibility of  rowcacheserializer  needed for capi flash enablement  <description> hi all, rei odaira , and i are working on creating a plugin for vendor specific (capi-flash ) implementation. as for our capi flash enablement code, we are now working on the plugin approach .and for creating the same we would need a change in the accessibility of `rowcacheserializer` . please refer to jira ticket/dev mails for reference :   1) https://issues.apache.org/jira/browse/cassandra-13486  2) http://www.mail-archive.com/dev@cassandra.apache.org/msg11102.html [~spodxx@gmail.com] jeff jirsa : fyi . adding a patch to change the accessibility. let me know once the change will be merged in the mainline. pr raised : https://github.com/apache/cassandra/pull/117 thanks !!!<stacktrace> <code> pr raised : https://github.com/apache/cassandra/pull/117 <text> hi all, rei odaira , and i are working on creating a plugin for vendor specific (capi-flash ) implementation. as for our capi flash enablement code, we are now working on the plugin approach .and for creating the same we would need a change in the accessibility of `rowcacheserializer` . please refer to jira ticket/dev mails for reference :   1) https://issues.apache.org/jira/browse/cassandra-13486  2) http://www.mail-archive.com/dev@cassandra.apache.org/msg11102.html [~spodxx@gmail.com] jeff jirsa : fyi . adding a patch to change the accessibility. let me know once the change will be merged in the mainline. thanks !!!",
        "label": 34
    },
    {
        "text": "broke cqlsh for ipv6 <description> cqlsh in 2.1 switched to the cassandra-driver python library, which only recently added ipv6 support. the version bundled with 2.1.0 does not include a sufficiently recent version, so cqlsh is unusable for those of us running ipv6 (us? me...?) the fix is to simply upgrade the bundled version of the python cassandra-driver to at least version 2.1.1<stacktrace> <code> <text> cqlsh in 2.1 switched to the cassandra-driver python library, which only recently added ipv6 support. the version bundled with 2.1.0 does not include a sufficiently recent version, so cqlsh is unusable for those of us running ipv6 (us? me...?) the fix is to simply upgrade the bundled version of the python cassandra-driver to at least version 2.1.1",
        "label": 538
    },
    {
        "text": "enable disable hh via jmx <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "deletable rows are sometimes not removed during compaction <description> our use case is write heavy and read seldom. to optimize the space used, we've set the bloom_filter_fp_ratio=1.0 that along with the fact that each row is only written to one time and that there are more than 20 sstables keeps the rows from ever being compacted. here is the code:  https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/compactioncontroller.java#l162  we hit this conner case and because of this c* keeps consuming more and more space on disk while it should not.<stacktrace> <code> <text> our use case is write heavy and read seldom. to optimize the space used, we've set the bloom_filter_fp_ratio=1.0 that along with the fact that each row is only written to one time and that there are more than 20 sstables keeps the rows from ever being compacted. here is the code:  https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/compactioncontroller.java#l162  we hit this conner case and because of this c* keeps consuming more and more space on disk while it should not.",
        "label": 577
    },
    {
        "text": "cql3 alter table command causes npe <description> to reproduce the problem: ./cqlsh --cql3  connected to test cluster at localhost:9160.  [cqlsh 2.2.0 | cassandra 1.1.0-rc1-snapshot | cql spec 3.0.0 | thrift protocol 19.30.0]  use help for help. cqlsh> create keyspace test34 with strategy_class = 'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor='1'; cqlsh> use test34; cqlsh:test34> create table users (  ... password varchar,  ... gender varchar,  ... session_token varchar,  ... state varchar,  ... birth_year bigint,  ... pk varchar,  ... primary key (pk)  ... ); cqlsh:test34> alter table users add coupon_code varchar;  tsocket read 0 bytes<stacktrace> <code> cqlsh> create keyspace test34 with strategy_class = 'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor='1'; cqlsh> use test34; cqlsh:test34> alter table users add coupon_code varchar;  tsocket read 0 bytes<text> to reproduce the problem: ./cqlsh --cql3  connected to test cluster at localhost:9160.  [cqlsh 2.2.0 | cassandra 1.1.0-rc1-snapshot | cql spec 3.0.0 | thrift protocol 19.30.0]  use help for help. cqlsh:test34> create table users (  ... password varchar,  ... gender varchar,  ... session_token varchar,  ... state varchar,  ... birth_year bigint,  ... pk varchar,  ... primary key (pk)  ... ); ",
        "label": 593
    },
    {
        "text": "cassandra x breakes apt on debian openvz <description> during upgrade from 1.0.6 setting up cassandra (1.0.6) ... *error: permission denied on key 'vm.max_map_count'* dpkg: error processing cassandra (--configure):  subprocess installed post-installation script returned error exit status 255 errors were encountered while processing:  cassandra<stacktrace> <code> setting up cassandra (1.0.6) ... *error: permission denied on key 'vm.max_map_count'* dpkg: error processing cassandra (--configure):  subprocess installed post-installation script returned error exit status 255 errors were encountered while processing:  cassandra <text> during upgrade from 1.0.6",
        "label": 593
    },
    {
        "text": "create a system table to expose prepared statements <description> because drivers abstract from users the handling of up/down nodes, they have to deal with the fact that when a node is restarted (or join), it won't know any prepared statement. drivers could somewhat ignore that problem and wait for a query to return an error (that the statement is unknown by the node) to re-prepare the query on that node, but it's relatively inefficient because every time a node comes back up, you'll get bad latency spikes due to some queries first failing, then being re-prepared and then only being executed. so instead, drivers (at least the java driver but i believe others do as well) pro-actively re-prepare statements when a node comes up. it solves the latency problem, but currently every driver instance blindly re-prepare all statements, meaning that in a large cluster with many clients there is a lot of duplication of work (it would be enough for a single client to prepare the statements) and a bigger than necessary load on the node that started. an idea to solve this it to have a (cheap) way for clients to check if some statements are prepared on the node. there is different options to provide that but what i'd suggest is to add a system table to expose the (cached) prepared statements because: 1. it's reasonably straightforward to implement: we just add a line to the table when a statement is prepared and remove it when it's evicted (we already have eviction listeners). we'd also truncate the table on startup but that's easy enough). we can even switch it to a \"virtual table\" if/when cassandra-7622 lands but it's trivial to do with a normal table in the meantime. 2. it doesn't require a change to the protocol or something like that. it could even be done in 2.1 if we wish to. 3. exposing prepared statements feels like a genuinely useful information to have (outside of the problem exposed here that is), if only for debugging/educational purposes. the exposed table could look something like: create table system.prepared_statements (    keyspace_name text,    table_name text,    prepared_id blob,    query_string text,    primary key (keyspace_name, table_name, prepared_id) )<stacktrace> <code> create table system.prepared_statements (    keyspace_name text,    table_name text,    prepared_id blob,    query_string text,    primary key (keyspace_name, table_name, prepared_id) ) <text> because drivers abstract from users the handling of up/down nodes, they have to deal with the fact that when a node is restarted (or join), it won't know any prepared statement. drivers could somewhat ignore that problem and wait for a query to return an error (that the statement is unknown by the node) to re-prepare the query on that node, but it's relatively inefficient because every time a node comes back up, you'll get bad latency spikes due to some queries first failing, then being re-prepared and then only being executed. so instead, drivers (at least the java driver but i believe others do as well) pro-actively re-prepare statements when a node comes up. it solves the latency problem, but currently every driver instance blindly re-prepare all statements, meaning that in a large cluster with many clients there is a lot of duplication of work (it would be enough for a single client to prepare the statements) and a bigger than necessary load on the node that started. an idea to solve this it to have a (cheap) way for clients to check if some statements are prepared on the node. there is different options to provide that but what i'd suggest is to add a system table to expose the (cached) prepared statements because: the exposed table could look something like:",
        "label": 453
    },
    {
        "text": "isrunning flag set prematurely in org apache cassandra transport server <description> in org.apache.cassandra.transport.server, the start() method sets the isrunning flag before calling the run() method. in the event of an initialization error like a port conflict an exception will be thrown at line 136 which is,  channel channel = bootstrap.bind(socket); it seems like it might make more sense to set the isrunning flag after binding to the socket. i have a tool that deploys a node and then verifies it is ready to receive cql requests. i do this via jmx. unless i use a delay before making that check, the jmx call will return true even though there is a port conflict.<stacktrace> <code> channel channel = bootstrap.bind(socket); <text> in org.apache.cassandra.transport.server, the start() method sets the isrunning flag before calling the run() method. in the event of an initialization error like a port conflict an exception will be thrown at line 136 which is, it seems like it might make more sense to set the isrunning flag after binding to the socket. i have a tool that deploys a node and then verifies it is ready to receive cql requests. i do this via jmx. unless i use a delay before making that check, the jmx call will return true even though there is a port conflict.",
        "label": 520
    },
    {
        "text": "cliclient contains an unused  unavailable import <description> the following library is included in src/java/org/apache/cassandra/cli/cliclient.java yet is not used or included via the pom: import org.hamcrest.core.issame<stacktrace> <code> <text> the following library is included in src/java/org/apache/cassandra/cli/cliclient.java yet is not used or included via the pom: import org.hamcrest.core.issame",
        "label": 373
    },
    {
        "text": "copy from with null '' fails when inserting empty row in primary key <description> using this table: create table testtab (  a_id text,  b_id text,  c_id text,  d_id text,  order_id uuid,  acc_id bigint,  bucket bigint,  r_id text,  ts bigint,  primary key ((a_id, b_id, c_id, d_id), order_id)); insert one row: insert into testtab (a_id, b_id , c_id , d_id , order_id, r_id ) values ( '', '', '', 'a1', 645e7d3c-aef7-4e3c-b834-24b792cf2e55, 'r1'); use copy to dump the row to temp.csv: copy testtab to 'temp.csv'; which creates this file: $ cat temp.csv  ,,,a1,645e7d3c-aef7-4e3c-b834-24b792cf2e55,,,r1, truncate the testtab table and then use copy from with null='' to insert the row: cqlsh:sbkeyspace> copy testtab from 'temp.csv' with null=''; using 1 child processes starting copy of sbkeyspace.testtab with columns ['a_id', 'b_id', 'c_id', 'd_id', 'order_id', 'acc_id', 'bucket', 'r_id', 'ts']. failed to import 1 rows: parseerror - cannot insert null value for primary key column 'a_id'. if you want to insert empty strings, consider using the with null=<marker> option for copy.,  given up without retries failed to process 1 rows; failed rows written to import_sbkeyspace_testtab.err processed: 1 rows; rate:       2 rows/s; avg. rate:       3 rows/s 1 rows imported from 1 files in 0.398 seconds (0 skipped). it shows 1 rows inserted, but the table is empty: select * from testtab ;  a_id | b_id | c_id | d_id | order_id | acc_id | bucket | r_id | ts ------+------+------+------+----------+--------+--------+------+---- (0 rows) the same error is returned even without the with null=''. is it actually possible for copy from to insert an empty row into the primary key? the insert command shown above inserts the empty row for the primary key without any problems. is this related to https://issues.apache.org/jira/browse/cassandra-7792?<stacktrace> <code> create table testtab (  a_id text,  b_id text,  c_id text,  d_id text,  order_id uuid,  acc_id bigint,  bucket bigint,  r_id text,  ts bigint,  primary key ((a_id, b_id, c_id, d_id), order_id)); insert into testtab (a_id, b_id , c_id , d_id , order_id, r_id ) values ( '', '', '', 'a1', 645e7d3c-aef7-4e3c-b834-24b792cf2e55, 'r1'); copy testtab to 'temp.csv'; $ cat temp.csv  ,,,a1,645e7d3c-aef7-4e3c-b834-24b792cf2e55,,,r1, cqlsh:sbkeyspace> copy testtab from 'temp.csv' with null=''; using 1 child processes starting copy of sbkeyspace.testtab with columns ['a_id', 'b_id', 'c_id', 'd_id', 'order_id', 'acc_id', 'bucket', 'r_id', 'ts']. failed to import 1 rows: parseerror - cannot insert null value for primary key column 'a_id'. if you want to insert empty strings, consider using the with null=<marker> option for copy.,  given up without retries failed to process 1 rows; failed rows written to import_sbkeyspace_testtab.err processed: 1 rows; rate:       2 rows/s; avg. rate:       3 rows/s 1 rows imported from 1 files in 0.398 seconds (0 skipped). select * from testtab ;  a_id | b_id | c_id | d_id | order_id | acc_id | bucket | r_id | ts ------+------+------+------+----------+--------+--------+------+---- (0 rows) <text> using this table: insert one row: use copy to dump the row to temp.csv: which creates this file: truncate the testtab table and then use copy from with null='' to insert the row: it shows 1 rows inserted, but the table is empty: the same error is returned even without the with null=''. is it actually possible for copy from to insert an empty row into the primary key? the insert command shown above inserts the empty row for the primary key without any problems. is this related to https://issues.apache.org/jira/browse/cassandra-7792?",
        "label": 508
    },
    {
        "text": "reads with each quorum on keyspace with simpletopologystrategy throw a classcastexception <description> i think this may be a regression introduced w/ cassandra-9602. starting with c* 3.0.0-rc2 an error is returned when querying a keyspace with simpletopologystrategy using each_quorum cl: cqlsh> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': 1}; cqlsh> create table test.test (k int primary key, i int); cqlsh> consistency each_quorum; consistency level set to each_quorum. cqlsh> select * from test.test; servererror: <errormessage code=0000 [server error] message=\"java.lang.classcastexception: org.apache.cassandra.locator.simplestrategy cannot be cast to org.apache.cassandra.locator.networktopologystrategy\"> the exception yielded in the system logs: error [sharedpool-worker-1] 2015-10-23 13:02:15,405 errormessage.java:336 - unexpected exception during request java.lang.classcastexception: org.apache.cassandra.locator.simplestrategy cannot be cast to org.apache.cassandra.locator.networktopologystrategy         at org.apache.cassandra.db.consistencylevel.filterforeachquorum(consistencylevel.java:227) ~[main/:na]         at org.apache.cassandra.db.consistencylevel.filterforquery(consistencylevel.java:188) ~[main/:na]         at org.apache.cassandra.db.consistencylevel.filterforquery(consistencylevel.java:180) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangeiterator.computenext(storageproxy.java:1795) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangeiterator.computenext(storageproxy.java:1762) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at com.google.common.collect.iterators$peekingimpl.hasnext(iterators.java:1149) ~[guava-18.0.jar:na]         at org.apache.cassandra.service.storageproxy$rangemerger.computenext(storageproxy.java:1814) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangemerger.computenext(storageproxy.java:1799) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangecommanditerator.computenext(storageproxy.java:1925) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangecommanditerator.computenext(storageproxy.java:1892) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at org.apache.cassandra.db.partitions.wrappingpartitioniterator.hasnext(wrappingpartitioniterator.java:33) ~[main/:na]         at org.apache.cassandra.db.partitions.countingpartitioniterator.hasnext(countingpartitioniterator.java:49) ~[main/:na]         at org.apache.cassandra.db.partitions.wrappingpartitioniterator.hasnext(wrappingpartitioniterator.java:33) ~[main/:na]         at org.apache.cassandra.db.partitions.countingpartitioniterator.hasnext(countingpartitioniterator.java:49) ~[main/:na]         at org.apache.cassandra.service.pager.abstractquerypager$pageriterator.hasnext(abstractquerypager.java:99) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:610) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:371) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:327) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:213) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:76) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:205) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:236) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:221) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_60]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_60] on 3.0.0-rc1 we get an invalid query error back instead (as expected with prior to cassandra-9602): invalidrequest: code=2200 [invalid query] message=\"each_quorum consistencylevel is only supported for writes\"<stacktrace> error [sharedpool-worker-1] 2015-10-23 13:02:15,405 errormessage.java:336 - unexpected exception during request java.lang.classcastexception: org.apache.cassandra.locator.simplestrategy cannot be cast to org.apache.cassandra.locator.networktopologystrategy         at org.apache.cassandra.db.consistencylevel.filterforeachquorum(consistencylevel.java:227) ~[main/:na]         at org.apache.cassandra.db.consistencylevel.filterforquery(consistencylevel.java:188) ~[main/:na]         at org.apache.cassandra.db.consistencylevel.filterforquery(consistencylevel.java:180) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangeiterator.computenext(storageproxy.java:1795) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangeiterator.computenext(storageproxy.java:1762) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at com.google.common.collect.iterators$peekingimpl.hasnext(iterators.java:1149) ~[guava-18.0.jar:na]         at org.apache.cassandra.service.storageproxy$rangemerger.computenext(storageproxy.java:1814) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangemerger.computenext(storageproxy.java:1799) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangecommanditerator.computenext(storageproxy.java:1925) ~[main/:na]         at org.apache.cassandra.service.storageproxy$rangecommanditerator.computenext(storageproxy.java:1892) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at org.apache.cassandra.db.partitions.wrappingpartitioniterator.hasnext(wrappingpartitioniterator.java:33) ~[main/:na]         at org.apache.cassandra.db.partitions.countingpartitioniterator.hasnext(countingpartitioniterator.java:49) ~[main/:na]         at org.apache.cassandra.db.partitions.wrappingpartitioniterator.hasnext(wrappingpartitioniterator.java:33) ~[main/:na]         at org.apache.cassandra.db.partitions.countingpartitioniterator.hasnext(countingpartitioniterator.java:49) ~[main/:na]         at org.apache.cassandra.service.pager.abstractquerypager$pageriterator.hasnext(abstractquerypager.java:99) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:610) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:371) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:327) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:213) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:76) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:205) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:236) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:221) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_60]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_60] <code> cqlsh> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': 1}; cqlsh> create table test.test (k int primary key, i int); cqlsh> consistency each_quorum; consistency level set to each_quorum. cqlsh> select * from test.test; servererror: <errormessage code=0000 [server error] message='java.lang.classcastexception: org.apache.cassandra.locator.simplestrategy cannot be cast to org.apache.cassandra.locator.networktopologystrategy'> <text> invalidrequest: code=2200 [invalid query] message='each_quorum consistencylevel is only supported for writes' i think this may be a regression introduced w/ cassandra-9602. starting with c* 3.0.0-rc2 an error is returned when querying a keyspace with simpletopologystrategy using each_quorum cl: the exception yielded in the system logs: on 3.0.0-rc1 we get an invalid query error back instead (as expected with prior to cassandra-9602):",
        "label": 98
    },
    {
        "text": "support for preparedstatement with like <description> using the java driver for example: preparedstatement pst = session.prepare(\"select * from test.users where first_name like ?\"); boundstatement bs = pst.bind(\"jon%\"); the first line fails with syntaxerror: line 1:47 mismatched input '?' expecting string_literal (which makes sense since it's how it's declared in the grammar). other operators declare the right-hand side value as a term.raw, which can also be a bind marker. i think users will expect to be able to bind the argument this way.<stacktrace> <code> preparedstatement pst = session.prepare('select * from test.users where first_name like ?'); boundstatement bs = pst.bind('jon%'); <text> using the java driver for example: the first line fails with syntaxerror: line 1:47 mismatched input '?' expecting string_literal (which makes sense since it's how it's declared in the grammar). other operators declare the right-hand side value as a term.raw, which can also be a bind marker. i think users will expect to be able to bind the argument this way.",
        "label": 474
    },
    {
        "text": "anti compaction briefly corrupts sstable state for reads <description> since we use multiple sstable rewriters in anticompaction, the first call to preparetocommit will remove the original sstables from the tracker view before the other rewriters add their sstables. this creates a brief window where reads can miss data.<stacktrace> <code> <text> since we use multiple sstable rewriters in anticompaction, the first call to preparetocommit will remove the original sstables from the tracker view before the other rewriters add their sstables. this creates a brief window where reads can miss data.",
        "label": 67
    },
    {
        "text": "cassandra cli bat fails to start cli client <description> when one fetch latest and greates from svn - the cassandra.bat works. (like - the server starts and get ready to rock) but the cassandra-cli.bat fails.  java claims noclassdeffounderrror: jline/completor after joining all four braincells i noticed that %cassandra_home%\\build\\lib\\jars* is not part of the classpath that is setup inside the bat file. adding this solved the issue.<stacktrace> <code> <text> when one fetch latest and greates from svn - the cassandra.bat works. (like - the server starts and get ready to rock) but the cassandra-cli.bat fails.  java claims noclassdeffounderrror: jline/completor",
        "label": 368
    },
    {
        "text": "pig output not working with branch <description> for some reason running a simple column family copy with pig is not writing out, though pig reports that it is successful.  steps to reproduce on a local node:  1. create the schema:  http://aep.appspot.com/display/vgbvdtp6qexc3oty3hbry9ncc3k/  2. run the following pig script (i did it with pig 0.8.0 from cdh3) using contrib/pig/bin/pig_cassandra -x local:  http://aep.appspot.com/display/pawjkcqrgbp7crgjt7qoyx9izn8/<stacktrace> <code> <text> for some reason running a simple column family copy with pig is not writing out, though pig reports that it is successful.  steps to reproduce on a local node:  1. create the schema:  http://aep.appspot.com/display/vgbvdtp6qexc3oty3hbry9ncc3k/  2. run the following pig script (i did it with pig 0.8.0 from cdh3) using contrib/pig/bin/pig_cassandra -x local:  http://aep.appspot.com/display/pawjkcqrgbp7crgjt7qoyx9izn8/",
        "label": 85
    },
    {
        "text": "pitr commitlog replay only actually replays mutation every other time <description> version: cassandra 2.1.4.374 | dse 4.7.0 the main issue here is that the restore-cycle only replays the mutations  every other try. on the first try, it will restore the snapshot as expected  and the cassandra system load will show that it's reading the mutations, but  they do not actually get replayed, and at the end you're left with only the  snapshot data (2k records). if you re-run the restore-cycle again, the commitlogs are replayed as expected,  and the data expected is present in the table (4k records, with a spot check of   record 4500, as it's in the commitlog but not the snapshot). then if you run the cycle again, it will fail. then again, and it will work. the work/  not work pattern continues. even re-running the commitlog replay a 2nd time, without  reloading the snapshot doesn't work the load process is: modify commitlog segment to 1mb archive to directory create keyspace/table insert base data initial snapshot write more data capture timestamp write more data final snapshot copy commitlogs to 2nd location modify cassandra-env to replay only specified keyspace modify commitlog properties to restore from 2nd location, with noted timestamp the restore cycle is: truncate table sstableload snapshot flush output data status restart to replay commitlogs output data status ====  see attached .py for a mostly automated reproduction scenario. it expects dse (and i found it with dse 4.7.0-1), rather than \"actual\" cassandra, but it's not using any dse specific features. the script looks for the configs in the dse locations, but they're set at the top, and there's only 2 places where dse is restarted.<stacktrace> <code> version: cassandra 2.1.4.374 | dse 4.7.0 <text> the main issue here is that the restore-cycle only replays the mutations  every other try. on the first try, it will restore the snapshot as expected  and the cassandra system load will show that it's reading the mutations, but  they do not actually get replayed, and at the end you're left with only the  snapshot data (2k records). if you re-run the restore-cycle again, the commitlogs are replayed as expected,  and the data expected is present in the table (4k records, with a spot check of   record 4500, as it's in the commitlog but not the snapshot). then if you run the cycle again, it will fail. then again, and it will work. the work/  not work pattern continues. even re-running the commitlog replay a 2nd time, without  reloading the snapshot doesn't work the load process is: the restore cycle is: ====  see attached .py for a mostly automated reproduction scenario. it expects dse (and i found it with dse 4.7.0-1), rather than 'actual' cassandra, but it's not using any dse specific features. the script looks for the configs in the dse locations, but they're set at the top, and there's only 2 places where dse is restarted.",
        "label": 86
    },
    {
        "text": "cqlsh support for native protocol v4 features <description> cqlsh/python-driver need to add support for all the new 2.2 cql features: date and time types - cassandra-7523 - not in cqlsh yet smallint and tinyint types - cassandra-8951 - not in the driver yet client warnings - cassandra-8930 tracing improvements - cassandra-7807<stacktrace> <code> <text> cqlsh/python-driver need to add support for all the new 2.2 cql features:",
        "label": 508
    },
    {
        "text": "clean up and optimize message <description> the message class has grown largely by accretion and it shows. there are several problems: outbound and inbound messages aren't really the same thing and should not be conflated we pre-serialize message bodies to byte[], then copy those bytes onto the socket buffer, instead of just keeping a reference to the object being serialized and then writing it out directly to the socket messagingservice versioning is poorly encapsulating, scattering version variables and references to things like cachingmessageproducer across the codebase<stacktrace> <code> <text> the message class has grown largely by accretion and it shows. there are several problems:",
        "label": 577
    },
    {
        "text": "errors with super columns  mixup of and <description> on our test cluster, we tried a upgrade of cassandra from 1.2.11 to 2.0.6. during the time we were running with 2 different versions of cassandra, there was errors in the logs: error [write-/10.10.0.41] 2014-03-19 11:23:27,523 outboundtcpconnection.java (line 234) error writing to /10.10.0.41  java.lang.runtimeexception: cannot convert filter to old super column format. update all nodes to cassandra 2.0 first.  at org.apache.cassandra.db.supercolumns.slicefiltertosc(supercolumns.java:357)  at org.apache.cassandra.db.supercolumns.filtertosc(supercolumns.java:258)  at org.apache.cassandra.db.readcommandserializer.serializedsize(readcommand.java:192)  at org.apache.cassandra.db.readcommandserializer.serializedsize(readcommand.java:134)  at org.apache.cassandra.net.messageout.serialize(messageout.java:116)  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:251)  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:203)  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:151) i confirm we do have old style super columns which were designed when cassandra was 1.0.x. since in our test cluster the replication factor is 1, i can see errors on the client side, since 1 node among 2 was down. so i don't know for sure if this error in cassandra affected the client, the time frame is too short to be sure from the logs.<stacktrace> error [write-/10.10.0.41] 2014-03-19 11:23:27,523 outboundtcpconnection.java (line 234) error writing to /10.10.0.41  java.lang.runtimeexception: cannot convert filter to old super column format. update all nodes to cassandra 2.0 first.  at org.apache.cassandra.db.supercolumns.slicefiltertosc(supercolumns.java:357)  at org.apache.cassandra.db.supercolumns.filtertosc(supercolumns.java:258)  at org.apache.cassandra.db.readcommandserializer.serializedsize(readcommand.java:192)  at org.apache.cassandra.db.readcommandserializer.serializedsize(readcommand.java:134)  at org.apache.cassandra.net.messageout.serialize(messageout.java:116)  at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:251)  at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:203)  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:151) <code> <text> on our test cluster, we tried a upgrade of cassandra from 1.2.11 to 2.0.6. during the time we were running with 2 different versions of cassandra, there was errors in the logs: i confirm we do have old style super columns which were designed when cassandra was 1.0.x. since in our test cluster the replication factor is 1, i can see errors on the client side, since 1 node among 2 was down. so i don't know for sure if this error in cassandra affected the client, the time frame is too short to be sure from the logs.",
        "label": 520
    },
    {
        "text": "timestamp data type does iso formats with 'z' as time zone  <description> the timestamp data type does not support format where time zone is specified with 'z' (as in zulu aka. utc+0 aka +0000 time zone). example: create table foo(ts timestamp primary key);  insert into foo(ts) values('2014-04-01t20:17:35+0000'); \u2013 this works  cqlsh:test> insert into foo(ts) values('2014-04-01t20:17:35z');  bad request: unable to coerce '2014-04-01t20:17:35z' to a formatted date (long) the example date was copied directly from iso 8601 wikipedia page. the standard says that \"if the time is in utc, add a z directly after the time without a space. z is the zone designator for the zero utc offset.\" tested with cqlsh with 2.0.6 version.<stacktrace> <code> create table foo(ts timestamp primary key);  insert into foo(ts) values('2014-04-01t20:17:35+0000'); - this works  cqlsh:test> insert into foo(ts) values('2014-04-01t20:17:35z');  bad request: unable to coerce '2014-04-01t20:17:35z' to a formatted date (long) <text> the timestamp data type does not support format where time zone is specified with 'z' (as in zulu aka. utc+0 aka +0000 time zone). example: the example date was copied directly from iso 8601 wikipedia page. the standard says that 'if the time is in utc, add a z directly after the time without a space. z is the zone designator for the zero utc offset.' tested with cqlsh with 2.0.6 version.",
        "label": 100
    },
    {
        "text": "nodetool doesn't work when cassandra run with the property java net preferipv6addresses true <description> nodetool doesn't work when cassandra run with the property java.net.preferipv6addresses=true. $ sudo netstat -tlpn | grep $(cat /var/run/cassandra/cassandra.pid) | grep 7199 tcp6       0      0 ::1:7199                :::*                    listen      27560/java $ nodetool -h ::1 status nodetool: failed to connect to '::1:7199' - connectexception: 'connection refused'. hardcoded value of the property java.rmi.server.hostname (https://github.com/apache/cassandra/blob/cassandra-2.1.4/src/java/org/apache/cassandra/service/cassandradaemon.java#l91) makes rmi returns the address 127.0.0.1 instead of ::1 that jmxserver listens to. 21:52:26.300192 ip6 (hlim 64, next-header tcp (6) payload length: 259) ::1.7199 > ::1.58706: flags [p.], cksum 0x010b (incorrect -> 0x6a57), seq 23:250, ack 88, win 2048, options [nop,nop,ts val 1833457 ecr 1833456], length 227 `......@...................................r.4...../........... ........q....w..sz.y...l.[....sr..javax.management.remote.rmi.rmiserverimpl_stub...........pxr..java.rmi.server.remotestub......e....pxr..javsz.y...l.[.....xoteobject.a...a3....pxpw4..unicastref2.. 127.0.0.1...(.l....d 21:52:26.336400 ip6 (hlim 64, next-header tcp (6) payload length: 32) ::1.58706 > ::1.7199: flags [.], cksum 0x0028 (incorrect -> 0xfe1c), seq 88, ack 250, win 2048, options [nop,nop,ts val 1833467 ecr 1833457], length 0 `.... .@.................................r...../.4.......(..... ........ jmxserver listens to the an ip address that was resolved from localhost and it depends on value of the property java.net.preferipv6addresses or lack of it (  https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/rmiserversocketfactoryimpl.java#l13).  this is a simple patch that works correctly with java.net.preferipv6addresses=(true|false) and java.net.preferipv4stack=(true|false): diff --git a/src/java/org/apache/cassandra/service/cassandradaemon.java b/src/java/org/apache/cassandra/service/cassandradaemon.java index 3e398bf..66e9cca 100644 --- a/src/java/org/apache/cassandra/service/cassandradaemon.java +++ b/src/java/org/apache/cassandra/service/cassandradaemon.java @@ -88,7 +88,7 @@ public class cassandradaemon              }              else              { -                system.setproperty(\"java.rmi.server.hostname\",\"127.0.0.1\"); +                system.setproperty(\"java.rmi.server.hostname\", inetaddress.getloopbackaddress().gethostaddress());                  try                  {<stacktrace> <code> diff --git a/src/java/org/apache/cassandra/service/cassandradaemon.java b/src/java/org/apache/cassandra/service/cassandradaemon.java index 3e398bf..66e9cca 100644 --- a/src/java/org/apache/cassandra/service/cassandradaemon.java +++ b/src/java/org/apache/cassandra/service/cassandradaemon.java @@ -88,7 +88,7 @@ public class cassandradaemon              }              else              { -                system.setproperty('java.rmi.server.hostname','127.0.0.1'); +                system.setproperty('java.rmi.server.hostname', inetaddress.getloopbackaddress().gethostaddress());                  try                  { $ sudo netstat -tlpn | grep $(cat /var/run/cassandra/cassandra.pid) | grep 7199 tcp6       0      0 ::1:7199                :::*                    listen      27560/java hardcoded value of the property java.rmi.server.hostname (https://github.com/apache/cassandra/blob/cassandra-2.1.4/src/java/org/apache/cassandra/service/cassandradaemon.java#l91) makes rmi returns the address 127.0.0.1 instead of ::1 that jmxserver listens to. jmxserver listens to the an ip address that was resolved from localhost and it depends on value of the property java.net.preferipv6addresses or lack of it (  https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/rmiserversocketfactoryimpl.java#l13).  this is a simple patch that works correctly with java.net.preferipv6addresses=(true|false) and java.net.preferipv4stack=(true|false):<text> $ nodetool -h ::1 status nodetool: failed to connect to '::1:7199' - connectexception: 'connection refused'. 21:52:26.300192 ip6 (hlim 64, next-header tcp (6) payload length: 259) ::1.7199 > ::1.58706: flags [p.], cksum 0x010b (incorrect -> 0x6a57), seq 23:250, ack 88, win 2048, options [nop,nop,ts val 1833457 ecr 1833456], length 227 `......@...................................r.4...../........... ........q....w..sz.y...l.[....sr..javax.management.remote.rmi.rmiserverimpl_stub...........pxr..java.rmi.server.remotestub......e....pxr..javsz.y...l.[.....xoteobject.a...a3....pxpw4..unicastref2.. 127.0.0.1...(.l....d 21:52:26.336400 ip6 (hlim 64, next-header tcp (6) payload length: 32) ::1.58706 > ::1.7199: flags [.], cksum 0x0028 (incorrect -> 0xfe1c), seq 88, ack 250, win 2048, options [nop,nop,ts val 1833467 ecr 1833457], length 0 `.... .@.................................r...../.4.......(..... ........ nodetool doesn't work when cassandra run with the property java.net.preferipv6addresses=true. ",
        "label": 41
    },
    {
        "text": "testall failure in org apache cassandra dht tokenallocator randomreplicationawaretokenallocatortest testexistingcluster <description> example failure:  http://cassci.datastax.com/job/trunk_testall/1239/testreport/org.apache.cassandra.dht.tokenallocator/randomreplicationawaretokenallocatortest/testexistingcluster/ error message expected max unit size below 1.2500, was 1.2564 stacktrace junit.framework.assertionfailederror: expected max unit size below 1.2500, was 1.2564 at org.apache.cassandra.dht.tokenallocator.abstractreplicationawaretokenallocatortest.grow(abstractreplicationawaretokenallocatortest.java:657) at org.apache.cassandra.dht.tokenallocator.randomreplicationawaretokenallocatortest.grow(randomreplicationawaretokenallocatortest.java:26) at org.apache.cassandra.dht.tokenallocator.abstractreplicationawaretokenallocatortest.testexistingcluster(abstractreplicationawaretokenallocatortest.java:545) at org.apache.cassandra.dht.tokenallocator.abstractreplicationawaretokenallocatortest.testexistingcluster(abstractreplicationawaretokenallocatortest.java:518) at org.apache.cassandra.dht.tokenallocator.randomreplicationawaretokenallocatortest.testexistingcluster(randomreplicationawaretokenallocatortest.java:38)<stacktrace> stacktrace junit.framework.assertionfailederror: expected max unit size below 1.2500, was 1.2564 at org.apache.cassandra.dht.tokenallocator.abstractreplicationawaretokenallocatortest.grow(abstractreplicationawaretokenallocatortest.java:657) at org.apache.cassandra.dht.tokenallocator.randomreplicationawaretokenallocatortest.grow(randomreplicationawaretokenallocatortest.java:26) at org.apache.cassandra.dht.tokenallocator.abstractreplicationawaretokenallocatortest.testexistingcluster(abstractreplicationawaretokenallocatortest.java:545) at org.apache.cassandra.dht.tokenallocator.abstractreplicationawaretokenallocatortest.testexistingcluster(abstractreplicationawaretokenallocatortest.java:518) at org.apache.cassandra.dht.tokenallocator.randomreplicationawaretokenallocatortest.testexistingcluster(randomreplicationawaretokenallocatortest.java:38) <code> error message expected max unit size below 1.2500, was 1.2564 example failure:  http://cassci.datastax.com/job/trunk_testall/1239/testreport/org.apache.cassandra.dht.tokenallocator/randomreplicationawaretokenallocatortest/testexistingcluster/<text> ",
        "label": 508
    },
    {
        "text": "make repair  pr work within a datacenter <description> as was noticed in cassandra-7317, using '-pr' alongside '-local' for repair doesn't really work properly, and disabling the combination was definitively the right short time fix. however, the main goal of '-pr' is to make it easy to repair a full cluster without doing any duplication of work. doing the same only within a data-center is obviously desirable. i think a reasonably simple solution would be modify the behavior of '-pr' when it's limited to only one dc. if applied to nodex in dcy, instead of repairing only the \"primary\" range of nodex for the whole ring, we'll repair that range but also all ranges that are \"primary\" for a node not in dcy and for which nodex is the first node of dcy found in ring order. basically we'll ensure that running 'repair -local -pr' on every nodes of a given dc will repair all ranges for the nodes of that dc without repairing the same range twice.<stacktrace> <code> <text> as was noticed in cassandra-7317, using '-pr' alongside '-local' for repair doesn't really work properly, and disabling the combination was definitively the right short time fix. however, the main goal of '-pr' is to make it easy to repair a full cluster without doing any duplication of work. doing the same only within a data-center is obviously desirable. i think a reasonably simple solution would be modify the behavior of '-pr' when it's limited to only one dc. if applied to nodex in dcy, instead of repairing only the 'primary' range of nodex for the whole ring, we'll repair that range but also all ranges that are 'primary' for a node not in dcy and for which nodex is the first node of dcy found in ring order. basically we'll ensure that running 'repair -local -pr' on every nodes of a given dc will repair all ranges for the nodes of that dc without repairing the same range twice.",
        "label": 409
    },
    {
        "text": "disable rpc ready gossip flag when shutting down client servers <description> example failure: http://cassci.datastax.com/job/cassandra-3.x_dtest/4/testreport/pushed_notifications_test/testpushednotifications/restart_node_test error message [{'change_type': u'down', 'address': ('127.0.0.2', 9042)}, {'change_type': u'up', 'address': ('127.0.0.2', 9042)}, {'change_type': u'down', 'address': ('127.0.0.2', 9042)}] stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/pushed_notifications_test.py\", line 181, in restart_node_test     self.assertequals(expected_notifications, len(notifications), notifications)   file \"/usr/lib/python2.7/unittest/case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 506, in _baseassertequal     raise self.failureexception(msg)<stacktrace> <code> error message [{'change_type': u'down', 'address': ('127.0.0.2', 9042)}, {'change_type': u'up', 'address': ('127.0.0.2', 9042)}, {'change_type': u'down', 'address': ('127.0.0.2', 9042)}] stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/pushed_notifications_test.py', line 181, in restart_node_test     self.assertequals(expected_notifications, len(notifications), notifications)   file '/usr/lib/python2.7/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file '/usr/lib/python2.7/unittest/case.py', line 506, in _baseassertequal     raise self.failureexception(msg) http://cassci.datastax.com/job/cassandra-3.x_dtest/4/testreport/pushed_notifications_test/testpushednotifications/restart_node_test<text> example failure: ",
        "label": 508
    },
    {
        "text": "force provided columns in clustering key order in 'clustering order by' <description> using this table:  create table video_event (  videoid_username varchar,  event varchar,  event_timestamp timestamp,  video_timestamp timestamp,  primary key (videoid_username, event, event_timestamp)  )with clustering order by (event_timestamp desc); inserting these records: insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:05:00','2012-09-02 18:05:00');  insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:05:30','2012-09-02 18:05:30');  insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:35:00','2012-09-02 18:35:00');  insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:37:30','2012-09-02 18:37:30'); running this select: select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1; i get this:  videoid_username | event | event_timestamp | video_timestamp  ------------------------------------------------------------------------+-------------------------  99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start | 2012-09-02 18:05:00+0000 | 2012-09-02 18:05:00+0000 i would expect to see this:  videoid_username | event | event_timestamp | video_timestamp  ------------------------------------------------------------------------+-------------------------  99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | stop | 2012-09-02 18:37:30+0000 | 2012-09-02 18:37:30+0000 where the first record pulled was the sorted record by event_timestamp in reverse order.<stacktrace> <code> using this table:  create table video_event (  videoid_username varchar,  event varchar,  event_timestamp timestamp,  video_timestamp timestamp,  primary key (videoid_username, event, event_timestamp)  )with clustering order by (event_timestamp desc); insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:05:00','2012-09-02 18:05:00');  insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:05:30','2012-09-02 18:05:30');  insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:35:00','2012-09-02 18:35:00');  insert into video_event (videoid_username, event, event_timestamp, video_timestamp)   values ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:37:30','2012-09-02 18:37:30'); select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1; videoid_username | event | event_timestamp | video_timestamp  ------------------------------------------------------------------------+-------------------------  99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start | 2012-09-02 18:05:00+0000 | 2012-09-02 18:05:00+0000 videoid_username | event | event_timestamp | video_timestamp  ------------------------------------------------------------------------+-------------------------  99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | stop | 2012-09-02 18:37:30+0000 | 2012-09-02 18:37:30+0000 <text> inserting these records: running this select: i get this: i would expect to see this: where the first record pulled was the sorted record by event_timestamp in reverse order.",
        "label": 520
    },
    {
        "text": "support paging in cqlsh <description> once we've switch cqlsh to use the python driver 2.x (cassandra-7506), we should also make it use paging. currently cqlsh adds an implicit limit which is kind of ugly. instead we should use some reasonably small page size (100 is probably fine) and display one page at a time, adding some \"next\" command to query/display following pages.<stacktrace> <code> <text> once we've switch cqlsh to use the python driver 2.x (cassandra-7506), we should also make it use paging. currently cqlsh adds an implicit limit which is kind of ugly. instead we should use some reasonably small page size (100 is probably fine) and display one page at a time, adding some 'next' command to query/display following pages.",
        "label": 362
    },
    {
        "text": "range query support <description> scan for keys by range (between x and y) or prefix (starting with p).<stacktrace> <code> <text> scan for keys by range (between x and y) or prefix (starting with p).",
        "label": 274
    },
    {
        "text": "add progress column in percents for system views sstable tasks <description> it would be very handy to have a percentage column in system_views.sstable_tasks which would say how far a respective task is. indeed, there are currently \"progress\" and \"total\" columns but honestly, for every day usage, it is rather strange to expect that humans will divide these numbers in head if they want to roughly know what the overall progress is. one just does not have a rough estimation of the task progress when he is presented with two quite big numbers and to estimate the progress from the.  in the following output, the field \"progress_in_percents\" is introduced.   pr is here https://github.com/apache/cassandra/pull/566   admin@cqlsh> select * from system_views.sstable_tasks ; @ row 1 -----------------------+--------------------------------------  keyspace_name         | mykeyspace  table_name            | mytable  task_id               | 0db5d9b1-8726-11ea-8a6c-b92f3be367bb  kind                  | secondary index build  progress              | 19456965  progress_in_percents  | 8.17  total                 | 238208674  unit                  | bytes @ row 2 ----------------------+--------------------------------------  keyspace_name        | mykeyspace  table_name           | mytable.mytable_surname_idx  task_id              | 1817ee71-8726-11ea-8a6c-b92f3be367bb  kind                 | compaction  progress             | 284396233  progress_in_percents | 75.92  total                | 374598446  unit                 | bytes<stacktrace> <code> admin@cqlsh> select * from system_views.sstable_tasks ; @ row 1 -----------------------+--------------------------------------  keyspace_name         | mykeyspace  table_name            | mytable  task_id               | 0db5d9b1-8726-11ea-8a6c-b92f3be367bb  kind                  | secondary index build  progress              | 19456965  progress_in_percents  | 8.17  total                 | 238208674  unit                  | bytes @ row 2 ----------------------+--------------------------------------  keyspace_name        | mykeyspace  table_name           | mytable.mytable_surname_idx  task_id              | 1817ee71-8726-11ea-8a6c-b92f3be367bb  kind                 | compaction  progress             | 284396233  progress_in_percents | 75.92  total                | 374598446  unit                 | bytes <text> it would be very handy to have a percentage column in system_views.sstable_tasks which would say how far a respective task is. indeed, there are currently 'progress' and 'total' columns but honestly, for every day usage, it is rather strange to expect that humans will divide these numbers in head if they want to roughly know what the overall progress is. one just does not have a rough estimation of the task progress when he is presented with two quite big numbers and to estimate the progress from the.  in the following output, the field 'progress_in_percents' is introduced.   pr is here https://github.com/apache/cassandra/pull/566  ",
        "label": 506
    },
    {
        "text": "optimize batchlog manager to avoid full scans <description> now that we use time-uuids for batchlog ids, and given that w/ local strategy the partitions are ordered in time-order here, we can optimize the scanning by limiting the range to replay taking the last replayed batch's id as the beginning of the range, and uuid(now+timeout) as its end.<stacktrace> <code> <text> now that we use time-uuids for batchlog ids, and given that w/ local strategy the partitions are ordered in time-order here, we can optimize the scanning by limiting the range to replay taking the last replayed batch's id as the beginning of the range, and uuid(now+timeout) as its end.",
        "label": 86
    },
    {
        "text": "files added with missing license headers <description> src/java/org/apache/cassandra/utils/bloomfilterserializer.java  src/java/org/apache/cassandra/utils/legacybloomfilterserializer.java  src/java/org/apache/cassandra/service/repaircallback.java  src/java/org/apache/cassandra/io/util/columnsortedmap.java are all missing license headers... aslv2 i assume<stacktrace> <code> src/java/org/apache/cassandra/utils/bloomfilterserializer.java  src/java/org/apache/cassandra/utils/legacybloomfilterserializer.java  src/java/org/apache/cassandra/service/repaircallback.java  src/java/org/apache/cassandra/io/util/columnsortedmap.java <text> are all missing license headers... aslv2 i assume",
        "label": 510
    },
    {
        "text": "message coalescing regression <description> the default in 2.2+ was to enable timehorizon message coalescing. after reports of performance regressions after upgrading from 2.1 to 2.2/3.0 we have discovered the issue to be this default. we need to re-test our assumptions on this feature but in the meantime we should default back to disabled. here is a performance run with and without message coalescing<stacktrace> <code> <text> the default in 2.2+ was to enable timehorizon message coalescing. after reports of performance regressions after upgrading from 2.1 to 2.2/3.0 we have discovered the issue to be this default. we need to re-test our assumptions on this feature but in the meantime we should default back to disabled. here is a performance run with and without message coalescing",
        "label": 241
    },
    {
        "text": "java util nosuchelementexception when returning a node to the cluster <description> i'm running the v0.7-beta1 in a 4 nodes cluster and just doing some simple testing. one of the nodes had been down (machine off, unclean shutdown) for an hour or so not sure how many writes were going on, when i bought it back up this message appears in the other 3 nodes... info [gossip_stage:1] 2010-08-25 19:29:51,199 gossiper.java (line 584) node /192.168.34.27 has restarted, now up again  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,200 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.27  info [gossip_stage:1] 2010-08-25 19:29:51,201 storageservice.java (line 636) node /192.168.34.27 state jump to normal  info [gossip_stage:1] 2010-08-25 19:29:51,201 storageservice.java (line 643) will not change my token ownership to /192.168.34.27  error [hinted-handoff-pool:1] 2010-08-25 19:29:51,640 cassandradaemon.java (line 82) uncaught exception in thread thread[hinted-handoff-pool:1,5,main]  java.util.concurrent.executionexception: java.lang.runtimeexception: java.util.nosuchelementexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.afterexecute(debuggablethreadpoolexecutor.java:87)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:888)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.lang.runtimeexception: java.util.nosuchelementexception  at orgapache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  ... 2 more  caused by: java.util.nosuchelementexception  at java.util.concurrent.concurrentskiplistmap.lastkey(concurrentskiplistmap.java:1981)  at java.util.concurrent.concurrentskiplistmap$keyset.last(concurrentskiplistmap.java:2331)  at org.apache.cassandra.db.hintedhandoffmanager.sendmessage(hintedhandoffmanager.java:121)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:218)  at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:78)  at org.apache.cassandra.db.hintedhandoffmanager$1.runmaythrow(hintedhandoffmanager.java:296) not sure how many writes were going on  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more on the machine that was off (34.27) there are no errors in the logs, and here are the entries for around the same time...  info [main] 2010-08-25 19:29:50,679 commitlog.java (line 340) recovery complete  info [main] 2010-08-25 19:29:50,769 commitlog.java (line 180) log replay complete  info [main] 2010-08-25 19:29:50,797 storageservice.java (line 342) cassandra version: 0.7.0-beta1-snapshot  info [main] 2010-08-25 19:29:50,797 storageservice.java (line 343) thrift api version: 10.0.0  info [main] 2010-08-25 19:29:50,813 systemtable.java (line 240) saved token found: 85070591730234615865843651857942052864  info [main] 2010-08-25 19:29:50,813 systemtable.java (line 257) saved clustername found: foo  info [main] 2010-08-25 19:29:50,813 systemtable.java (line 272) saved partitioner not found. using org.apache.cassandra.dht.randompartitioner  info [main] 2010-08-25 19:29:50,814 columnfamilystore.java (line 422) switching in a fresh memtable for locationinfo at commitlogcontext(file='/local1/junkbox/cassandra/commitlog/commitlog-12827213897  70.log', position=41336)  info [main] 2010-08-25 19:29:50,814 columnfamilystore.java (line 706) enqueuing flush of memtable-locationinfo@916236367(95 bytes, 2 operations)  info [flush-writer-pool:1] 2010-08-25 19:29:50,815 memtable.java (line 150) writing memtable-locationinfo@916236367(95 bytes, 2 operations)  info [flush-writer-pool:1] 2010-08-25 19:29:50,873 memtable.java (line 157) completed flushing /local1/junkbox/cassandra/data/system/locationinfo-e-6-data.db  info [main] 2010-08-25 19:29:50,917 storageservice.java (line 374) starting up server gossip  info [main] 2010-08-25 19:29:51,093 columnfamilystore.java (line 1239) loaded 0 rows into the super2 cache  info [main] 2010-08-25 19:29:51,170 cassandradaemon.java (line 153) binding thrift service to /0.0.0.0:9160  info [main] 2010-08-25 19:29:51,174 cassandradaemon.java (line 167) using tframedtransport with a max frame size of 15728640 bytes.  info [gossip_stage:1] 2010-08-25 19:29:51,198 gossiper.java (line 578) node /192.168.34.28 is now part of the cluster  info [gossip_stage:1] 2010-08-25 19:29:51,199 gossiper.java (line 578) node /192.168.34.29 is now part of the cluster  info [gossip_stage:1] 2010-08-25 19:29:51,199 gossiper.java (line 578) node /192.168.34.26 is now part of the cluster  info [main] 2010-08-25 19:29:51,204 cassandradaemon.java (line 208) listening for thrift clients...  info [main] 2010-08-25 19:29:51,210 mx4jtool.java (line 73) will not load mx4j, mx4j-tools.jar is not in the classpath  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,417 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.28  info [gossip_stage:1] 2010-08-25 19:29:51,417 gossiper.java (line 570) inetaddress /192.168.34.28 is now up  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,418 hintedhandoffmanager.java (line 247) finished hinted handoff of 0 rows to endpoint /192.168.34.28  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,855 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.29  info [gossip_stage:1] 2010-08-25 19:29:51,855 gossiper.java (line 570) inetaddress /192.168.34.29 is now up  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,860 hintedhandoffmanager.java (line 247) finished hinted handoff of 0 rows to endpoint /192.168.34.29  info [hinted-handoff-pool:1] 2010-08-25 19:29:52,930 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.26  info [gossip_stage:1] 2010-08-25 19:29:52,930 gossiper.java (line 570) inetaddress /192.168.34.26 is now up  info [hinted-handoff-pool:1] 2010-08-25 19:29:52,930 hintedhandoffmanager.java (line 247) finished hinted handoff of 0 rows to endpoint /192.168.34.26 i ran a repair on all the nodes and this was all that they each logged   info [manual-repair-fe7c5abb-bb0a-4415-aa75-0d72ba4e7f1b] 2010-08-25 19:49:24,194 antientropyservice.java (line 803) waiting for repair requests to: [] the cluster seemed ok and kept on working.<stacktrace> info [gossip_stage:1] 2010-08-25 19:29:51,199 gossiper.java (line 584) node /192.168.34.27 has restarted, now up again  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,200 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.27  info [gossip_stage:1] 2010-08-25 19:29:51,201 storageservice.java (line 636) node /192.168.34.27 state jump to normal  info [gossip_stage:1] 2010-08-25 19:29:51,201 storageservice.java (line 643) will not change my token ownership to /192.168.34.27  error [hinted-handoff-pool:1] 2010-08-25 19:29:51,640 cassandradaemon.java (line 82) uncaught exception in thread thread[hinted-handoff-pool:1,5,main]  java.util.concurrent.executionexception: java.lang.runtimeexception: java.util.nosuchelementexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.afterexecute(debuggablethreadpoolexecutor.java:87)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:888)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.lang.runtimeexception: java.util.nosuchelementexception  at orgapache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  ... 2 more  caused by: java.util.nosuchelementexception  at java.util.concurrent.concurrentskiplistmap.lastkey(concurrentskiplistmap.java:1981)  at java.util.concurrent.concurrentskiplistmap$keyset.last(concurrentskiplistmap.java:2331)  at org.apache.cassandra.db.hintedhandoffmanager.sendmessage(hintedhandoffmanager.java:121)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:218)  at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:78)  at org.apache.cassandra.db.hintedhandoffmanager$1.runmaythrow(hintedhandoffmanager.java:296) not sure how many writes were going on  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more <code> info [main] 2010-08-25 19:29:50,679 commitlog.java (line 340) recovery complete  info [main] 2010-08-25 19:29:50,769 commitlog.java (line 180) log replay complete  info [main] 2010-08-25 19:29:50,797 storageservice.java (line 342) cassandra version: 0.7.0-beta1-snapshot  info [main] 2010-08-25 19:29:50,797 storageservice.java (line 343) thrift api version: 10.0.0  info [main] 2010-08-25 19:29:50,813 systemtable.java (line 240) saved token found: 85070591730234615865843651857942052864  info [main] 2010-08-25 19:29:50,813 systemtable.java (line 257) saved clustername found: foo  info [main] 2010-08-25 19:29:50,813 systemtable.java (line 272) saved partitioner not found. using org.apache.cassandra.dht.randompartitioner  info [main] 2010-08-25 19:29:50,814 columnfamilystore.java (line 422) switching in a fresh memtable for locationinfo at commitlogcontext(file='/local1/junkbox/cassandra/commitlog/commitlog-12827213897  70.log', position=41336)  info [main] 2010-08-25 19:29:50,814 columnfamilystore.java (line 706) enqueuing flush of memtable-locationinfo@916236367(95 bytes, 2 operations)  info [flush-writer-pool:1] 2010-08-25 19:29:50,815 memtable.java (line 150) writing memtable-locationinfo@916236367(95 bytes, 2 operations)  info [flush-writer-pool:1] 2010-08-25 19:29:50,873 memtable.java (line 157) completed flushing /local1/junkbox/cassandra/data/system/locationinfo-e-6-data.db  info [main] 2010-08-25 19:29:50,917 storageservice.java (line 374) starting up server gossip  info [main] 2010-08-25 19:29:51,093 columnfamilystore.java (line 1239) loaded 0 rows into the super2 cache  info [main] 2010-08-25 19:29:51,170 cassandradaemon.java (line 153) binding thrift service to /0.0.0.0:9160  info [main] 2010-08-25 19:29:51,174 cassandradaemon.java (line 167) using tframedtransport with a max frame size of 15728640 bytes.  info [gossip_stage:1] 2010-08-25 19:29:51,198 gossiper.java (line 578) node /192.168.34.28 is now part of the cluster  info [gossip_stage:1] 2010-08-25 19:29:51,199 gossiper.java (line 578) node /192.168.34.29 is now part of the cluster  info [gossip_stage:1] 2010-08-25 19:29:51,199 gossiper.java (line 578) node /192.168.34.26 is now part of the cluster  info [main] 2010-08-25 19:29:51,204 cassandradaemon.java (line 208) listening for thrift clients...  info [main] 2010-08-25 19:29:51,210 mx4jtool.java (line 73) will not load mx4j, mx4j-tools.jar is not in the classpath  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,417 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.28  info [gossip_stage:1] 2010-08-25 19:29:51,417 gossiper.java (line 570) inetaddress /192.168.34.28 is now up  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,418 hintedhandoffmanager.java (line 247) finished hinted handoff of 0 rows to endpoint /192.168.34.28  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,855 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.29  info [gossip_stage:1] 2010-08-25 19:29:51,855 gossiper.java (line 570) inetaddress /192.168.34.29 is now up  info [hinted-handoff-pool:1] 2010-08-25 19:29:51,860 hintedhandoffmanager.java (line 247) finished hinted handoff of 0 rows to endpoint /192.168.34.29  info [hinted-handoff-pool:1] 2010-08-25 19:29:52,930 hintedhandoffmanager.java (line 191) started hinted handoff for endpoint /192.168.34.26  info [gossip_stage:1] 2010-08-25 19:29:52,930 gossiper.java (line 570) inetaddress /192.168.34.26 is now up  info [hinted-handoff-pool:1] 2010-08-25 19:29:52,930 hintedhandoffmanager.java (line 247) finished hinted handoff of 0 rows to endpoint /192.168.34.26 i ran a repair on all the nodes and this was all that they each logged   info [manual-repair-fe7c5abb-bb0a-4415-aa75-0d72ba4e7f1b] 2010-08-25 19:49:24,194 antientropyservice.java (line 803) waiting for repair requests to: [] <text> i'm running the v0.7-beta1 in a 4 nodes cluster and just doing some simple testing. one of the nodes had been down (machine off, unclean shutdown) for an hour or so not sure how many writes were going on, when i bought it back up this message appears in the other 3 nodes... on the machine that was off (34.27) there are no errors in the logs, and here are the entries for around the same time... the cluster seemed ok and kept on working.",
        "label": 274
    },
    {
        "text": "npe in mvs on update <description> i've stumbled upon an npe in mvs on update. this script will reproduce 100% on trunk from date: sat oct 10 09:23:15 2015 +0100 error [sharedpool-worker-3] 2015-10-10 21:35:01,867 keyspace.java:487 - unknown exception caught while attempting to update materializedview! test.test_with_cluster java.lang.nullpointerexception: null         at org.apache.cassandra.db.view.temporalrow.clusteringvalue(temporalrow.java:381) ~[main/:na]         at org.apache.cassandra.db.view.view.createupdatesforinserts(view.java:355) ~[main/:na]         at org.apache.cassandra.db.view.view.createmutations(view.java:664) ~[main/:na]         at org.apache.cassandra.db.view.viewmanager.pushviewreplicaupdates(viewmanager.java:130) ~[main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:482) [main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:387) [main/:na]         at org.apache.cassandra.db.mutation.apply(mutation.java:205) [main/:na]         at org.apache.cassandra.service.storageproxy$$lambda$149/1333013217.run(unknown source) [main/:na]         at org.apache.cassandra.service.storageproxy$7.runmaythrow(storageproxy.java:1247) [main/:na]         at org.apache.cassandra.service.storageproxy$localmutationrunnable.run(storageproxy.java:2399) [main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] and the script to trigger: ccm remove test;  ccm create test --install-dir=/users/jeff/desktop/dev/cassandra/ -s -n 1 ;  echo \"create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': 1}; use test; create table test ( id text primary key, last text, first text, high int, low int); insert into test(id,last,first,high,low) values ('a', 'a', 'a', 1, 1); insert into test(id,last,first,high,low) values ('a', 'b', 'b', 2, 2); insert into test(id,last,first,high,low) values ('a', 'c', 'c', 3, 3); insert into test(id,last,first,high,low) values ('a', 'e', 'e', 5, 5); insert into test(id,last,first,high,low) values ('a', 'd', 'd', 4, 4); select * from test where id='a';\" | ccm node1 cqlsh echo \"creating mv test_by_high on test\" echo \"use test; create materialized view test_by_high as select id, high from test where high is not null primary key(high, id);\" | ccm node1 cqlsh echo \"insert high score 6, this will succeed\" echo \"use test; insert into test(id,last,first,high,low) values ('a', 'f', 'f', 6, 6); \" | ccm node1 cqlsh  sleep 1 echo \"select from mv where score = 6, this will succeed\" echo \"use test; select * from test_by_high where high=6; \" | ccm node1 cqlsh echo \"create a larger table with clustering key\" echo \"use test; create table test_with_cluster(part text, clus text, last text, first text, high int, low int, primary key (part, clus));\" | ccm node1 cqlsh echo \"use test; create materialized view high_view as select part, clus, high from test_with_cluster where part is not null and clus is not null and high is not null primary key(high, part, clus);\" | ccm node1 cqlsh echo \"populate test_with_cluster, this will break\" echo \"use test; insert into test_with_cluster(part, clus,last,first,high,low) values ('a', 'a', 'a', 'a', 1, 1); \" | ccm node1 cqlsh logs from my previous tests (which i've deleted, unfortunately) suggest that the npe is due to using the wrong columnidentifier - it's using id (from test.test?) which causes the npe in clusteringvalue(), since it's in the wrong base table.<stacktrace> error [sharedpool-worker-3] 2015-10-10 21:35:01,867 keyspace.java:487 - unknown exception caught while attempting to update materializedview! test.test_with_cluster java.lang.nullpointerexception: null         at org.apache.cassandra.db.view.temporalrow.clusteringvalue(temporalrow.java:381) ~[main/:na]         at org.apache.cassandra.db.view.view.createupdatesforinserts(view.java:355) ~[main/:na]         at org.apache.cassandra.db.view.view.createmutations(view.java:664) ~[main/:na]         at org.apache.cassandra.db.view.viewmanager.pushviewreplicaupdates(viewmanager.java:130) ~[main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:482) [main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:387) [main/:na]         at org.apache.cassandra.db.mutation.apply(mutation.java:205) [main/:na]         at org.apache.cassandra.service.storageproxy$$lambda$149/1333013217.run(unknown source) [main/:na]         at org.apache.cassandra.service.storageproxy$7.runmaythrow(storageproxy.java:1247) [main/:na]         at org.apache.cassandra.service.storageproxy$localmutationrunnable.run(storageproxy.java:2399) [main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] <code> ccm remove test;  ccm create test --install-dir=/users/jeff/desktop/dev/cassandra/ -s -n 1 ;  echo 'create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': 1}; use test; create table test ( id text primary key, last text, first text, high int, low int); insert into test(id,last,first,high,low) values ('a', 'a', 'a', 1, 1); insert into test(id,last,first,high,low) values ('a', 'b', 'b', 2, 2); insert into test(id,last,first,high,low) values ('a', 'c', 'c', 3, 3); insert into test(id,last,first,high,low) values ('a', 'e', 'e', 5, 5); insert into test(id,last,first,high,low) values ('a', 'd', 'd', 4, 4); select * from test where id='a';' | ccm node1 cqlsh echo 'creating mv test_by_high on test' echo 'use test; create materialized view test_by_high as select id, high from test where high is not null primary key(high, id);' | ccm node1 cqlsh echo 'insert high score 6, this will succeed' echo 'use test; insert into test(id,last,first,high,low) values ('a', 'f', 'f', 6, 6); ' | ccm node1 cqlsh  sleep 1 echo 'select from mv where score = 6, this will succeed' echo 'use test; select * from test_by_high where high=6; ' | ccm node1 cqlsh echo 'create a larger table with clustering key' echo 'use test; create table test_with_cluster(part text, clus text, last text, first text, high int, low int, primary key (part, clus));' | ccm node1 cqlsh echo 'use test; create materialized view high_view as select part, clus, high from test_with_cluster where part is not null and clus is not null and high is not null primary key(high, part, clus);' | ccm node1 cqlsh echo 'populate test_with_cluster, this will break' echo 'use test; insert into test_with_cluster(part, clus,last,first,high,low) values ('a', 'a', 'a', 'a', 1, 1); ' | ccm node1 cqlsh <text> i've stumbled upon an npe in mvs on update. this script will reproduce 100% on trunk from date: sat oct 10 09:23:15 2015 +0100 and the script to trigger: logs from my previous tests (which i've deleted, unfortunately) suggest that the npe is due to using the wrong columnidentifier - it's using id (from test.test?) which causes the npe in clusteringvalue(), since it's in the wrong base table.",
        "label": 241
    },
    {
        "text": "provide option for cassandra stress to dump all settings <description> cassandra-stress has quite a lot of default settings and settings that are derived as side effects of explicit options. for people learning the tool and saving a clear record of what was run, i think it would be useful if there was an option to have the tool print all its settings at the start of a run.<stacktrace> <code> <text> cassandra-stress has quite a lot of default settings and settings that are derived as side effects of explicit options. for people learning the tool and saving a clear record of what was run, i think it would be useful if there was an option to have the tool print all its settings at the start of a run.",
        "label": 66
    },
    {
        "text": "maxpurgeabletimestamp needs to check memtables too <description> overlapiterator/maxpurgeabletimestamp don't include the memtables, so a very-out-of-order write could be ignored<stacktrace> <code> <text> overlapiterator/maxpurgeabletimestamp don't include the memtables, so a very-out-of-order write could be ignored",
        "label": 508
    },
    {
        "text": "make concurrent reads  concurrent writes configurable at runtime via jmx <description> <stacktrace> <code> <text> ",
        "label": 455
    },
    {
        "text": "better performance on long row read <description> currently if a row contains > 1000 columns, the run time becomes considerably slow (my test of   a row with 30 00 columns (standard, regular) each with 8 bytes in name, and 40 bytes in value, is about 16ms.  this is all running in memory, no disk read is involved. through debugging we can find  most of this time is spent on   [wall time] org.apache.cassandra.db.table.getrow(queryfilter)  [wall time] org.apache.cassandra.db.columnfamilystore.getcolumnfamily(queryfilter, columnfamily)  [wall time] org.apache.cassandra.db.columnfamilystore.getcolumnfamily(queryfilter, int, columnfamily)  [wall time] org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(queryfilter, int, columnfamily)  [wall time] org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(columnfamily, iterator, int)  [wall time] org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(icolumncontainer, iterator, int)  [wall time] org.apache.cassandra.db.columnfamily.addcolumn(icolumn) columnfamily.addcolumn() is slow because it inserts into an internal concurrentskiplistmap() that maps column names to values.  this structure is slow for two reasons: it needs to do synchronization; it needs to maintain a more complex structure of map. but if we look at the whole read path, thrift already defines the read output to be list<columnorsupercolumn> so it does not make sense to use a luxury map data structure in the interium and finally convert it to a list. on the synchronization side, since the return cf is never going to be shared/modified by other threads, we know the access is always single thread, so no synchronization is needed. but these 2 features are indeed needed for columnfamily in other cases, particularly write. so we can provide a different columnfamily to cfs.gettoplevelcolumnfamily(), so gettoplevelcolumnfamily no longer always creates the standard columnfamily, but take a provided returncf, whose cost is much cheaper. the provided patch is for demonstration now, will work further once we agree on the general direction.   cfs, columnfamily, and table are changed; a new fastcolumnfamily is provided. the main work is to let the fastcolumnfamily use an array for internal storage. at first i used binary search to insert new columns in addcolumn(), but later i found that even this is not necessary, since all calling scenarios of columnfamily.addcolumn() has an invariant that the inserted columns come in sorted order (i still have an issue to resolve descending or ascending now, but ascending works). so the current logic is simply to compare the new column against the end column in the array, if names not equal, append, if equal, reconcile. slight temporary hacks are made on gettoplevelcolumnfamily so we have 2 flavors of the method, one accepting a returncf. but we could definitely think about what is the better way to provide this returncf. this patch compiles fine, no tests are provided yet. but i tested it in my application, and the performance improvement is dramatic: it offers about 50% reduction in read time in the 3000-column case. thanks  yang<stacktrace> <code> <text> currently if a row contains > 1000 columns, the run time becomes considerably slow (my test of   a row with 30 00 columns (standard, regular) each with 8 bytes in name, and 40 bytes in value, is about 16ms.  this is all running in memory, no disk read is involved. through debugging we can find  most of this time is spent on   [wall time] org.apache.cassandra.db.table.getrow(queryfilter)  [wall time] org.apache.cassandra.db.columnfamilystore.getcolumnfamily(queryfilter, columnfamily)  [wall time] org.apache.cassandra.db.columnfamilystore.getcolumnfamily(queryfilter, int, columnfamily)  [wall time] org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(queryfilter, int, columnfamily)  [wall time] org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(columnfamily, iterator, int)  [wall time] org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(icolumncontainer, iterator, int)  [wall time] org.apache.cassandra.db.columnfamily.addcolumn(icolumn) columnfamily.addcolumn() is slow because it inserts into an internal concurrentskiplistmap() that maps column names to values.  this structure is slow for two reasons: it needs to do synchronization; it needs to maintain a more complex structure of map. but if we look at the whole read path, thrift already defines the read output to be list<columnorsupercolumn> so it does not make sense to use a luxury map data structure in the interium and finally convert it to a list. on the synchronization side, since the return cf is never going to be shared/modified by other threads, we know the access is always single thread, so no synchronization is needed. but these 2 features are indeed needed for columnfamily in other cases, particularly write. so we can provide a different columnfamily to cfs.gettoplevelcolumnfamily(), so gettoplevelcolumnfamily no longer always creates the standard columnfamily, but take a provided returncf, whose cost is much cheaper. the provided patch is for demonstration now, will work further once we agree on the general direction.   cfs, columnfamily, and table are changed; a new fastcolumnfamily is provided. the main work is to let the fastcolumnfamily use an array for internal storage. at first i used binary search to insert new columns in addcolumn(), but later i found that even this is not necessary, since all calling scenarios of columnfamily.addcolumn() has an invariant that the inserted columns come in sorted order (i still have an issue to resolve descending or ascending now, but ascending works). so the current logic is simply to compare the new column against the end column in the array, if names not equal, append, if equal, reconcile. slight temporary hacks are made on gettoplevelcolumnfamily so we have 2 flavors of the method, one accepting a returncf. but we could definitely think about what is the better way to provide this returncf. this patch compiles fine, no tests are provided yet. but i tested it in my application, and the performance improvement is dramatic: it offers about 50% reduction in read time in the 3000-column case. thanks  yang",
        "label": 520
    },
    {
        "text": "cqlsh  add tests for insert and update completion <description> i've added tests for cqlsh completion for insert and update statements. in the process, i also refactored trycompletions and removed old datatypes from the keyspace initialization script. the changes are on github here: https://github.com/mambocab/cassandra/tree/cqlsh-completion-tests the diff for review is here: https://github.com/apache/cassandra/compare/trunk...mambocab:cqlsh-completion-tests happy to prepare a patch; let me know what to do.<stacktrace> <code> https://github.com/mambocab/cassandra/tree/cqlsh-completion-tests https://github.com/apache/cassandra/compare/trunk...mambocab:cqlsh-completion-tests <text> i've added tests for cqlsh completion for insert and update statements. in the process, i also refactored trycompletions and removed old datatypes from the keyspace initialization script. the changes are on github here: the diff for review is here: happy to prepare a patch; let me know what to do.",
        "label": 252
    },
    {
        "text": "optimize locking in paxosstate <description> in paxosstate, we want to lock on same rows and have created 1024 size array with java objects in them to be used for locking.   we should replace these objects with some lock so that we can know whether there is contention trying to acquire a lock for different rows.  we can achieve that by also storing the hash of the row which acquired the lock. this will tell us if this needs to be improved. here is an improvement which i was thinking about.   say two rows a and b map to the same 1024 bucket which we have. a get the lock and b has to wait. here b can check if his hash is different and create a new object and chain it to the other one.   this looks close to a hashmap with chaining for same key.   the hard part will be removing the entries no longer being used.<stacktrace> <code> <text> in paxosstate, we want to lock on same rows and have created 1024 size array with java objects in them to be used for locking.   we should replace these objects with some lock so that we can know whether there is contention trying to acquire a lock for different rows.  we can achieve that by also storing the hash of the row which acquired the lock. this will tell us if this needs to be improved. here is an improvement which i was thinking about.   say two rows a and b map to the same 1024 bucket which we have. a get the lock and b has to wait. here b can check if his hash is different and create a new object and chain it to the other one.   this looks close to a hashmap with chaining for same key.   the hard part will be removing the entries no longer being used.",
        "label": 18
    },
    {
        "text": "index validation fails for non indexed column <description> on 2.1.3, updates are failing with an invalidrequestexception when an unindexed column is greater than the maximum allowed for indexed entries. responseerror: can't index column value of size 1483409 for index null on local_group_default_t_parsoid_html.data in this case, the update does include a 1483409 byte column value, but it is for a column that is not indexed, (the single indexed column is < 32 bytes), presumably this is why cfm.getcolumndefinition(cell.name()).getindexname() returns null.<stacktrace> <code> <text> responseerror: can't index column value of size 1483409 for index null on local_group_default_t_parsoid_html.data on 2.1.3, updates are failing with an invalidrequestexception when an unindexed column is greater than the maximum allowed for indexed entries. in this case, the update does include a 1483409 byte column value, but it is for a column that is not indexed, (the single indexed column is < 32 bytes), presumably this is why cfm.getcolumndefinition(cell.name()).getindexname() returns null.",
        "label": 98
    },
    {
        "text": "bundle jna <description> jna 4.0 is reported to be dual-licensed lgpl/apl.<stacktrace> <code> <text> jna 4.0 is reported to be dual-licensed lgpl/apl.",
        "label": 315
    },
    {
        "text": "compaction cleanupifnecessary costly when many files in data dir <description> from what i can tell sstablewriter.cleanupifnecessary seems increasingly costly as the number of files in the data dir increases.  it calls sstable.componentsfor(descriptor, descriptor.tempstate.temp) which lists all files in the data dir to find matching components. am i roughly correct that (cleanupcost = sstable count * data dir size)? we had been doing write load testing with default compaction throttling (16mb/s) and leveledcompaction.  unfortunately we haven't been keeping tabs on sstable counts and it grew out of control. on a system with 300,000 sstables here is an example of our compaction rate. note that as you're probably aware cleanupifnecessary is included in the timing:  info [compactionexecutor:48] 2011-11-25 22:25:30,353 compactiontask.java (line 213) compacted to [/data1/cassandra/data/ma_ddr/indexes_03-hc-5369-data.db,]. 5,821,590 to 5,306,354 (~91% of original) bytes for 123 keys at 0.163755mb/s. time: 30,903ms. here's a slightly larger one:  info [compactionexecutor:43] 2011-11-25 22:23:28,956 compactiontask.java (line 213) compacted to [/data1/cassandra/data/ma_ddr/indexes_03-hc-5336-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5337-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5338-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5339-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5340-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5341-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5342-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5343-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5344-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5345-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5346-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5347-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5348-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5349-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5350-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5351-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5352-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5353-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5354-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5355-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5356-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5357-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5358-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5359-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5360-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5361-data.db,]. 140,706,512 to 137,990,868 (~98% of original) bytes for 2,181 keys at 0.338627mb/s. time: 388,623ms. this is with compaction throttling set to 0 (off). so i believe because of this it's going to take a very long time to recover from having so many small sstables.   it might be notable that we're using solaris 10, possibly listfiles() is faster on other platforms? is it feasible to keep track of the temp files and just delete them rather than searching for them for each sstable using sstable.componentsfor()? here's the stack trace for the compactionexecutor:14 thread that appears to be occupying the majority of the cpu time on this node: name: compactionexecutor:14  state: runnable  total blocked: 3 total waited: 1,610,714 stack trace:   java.io.unixfilesystem.getbooleanattributes0(native method)  java.io.unixfilesystem.getbooleanattributes(unknown source)  java.io.file.isdirectory(unknown source)  org.apache.cassandra.io.sstable.sstable$3.accept(sstable.java:204)  java.io.file.listfiles(unknown source)  org.apache.cassandra.io.sstable.sstable.componentsfor(sstable.java:200)  org.apache.cassandra.io.sstable.sstablewriter.cleanupifnecessary(sstablewriter.java:289)  org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:189)  org.apache.cassandra.db.compaction.leveledcompactiontask.execute(leveledcompactiontask.java:57)  org.apache.cassandra.db.compaction.compactionmanager$1.call(compactionmanager.java:134)  org.apache.cassandra.db.compaction.compactionmanager$1.call(compactionmanager.java:114)  java.util.concurrent.futuretask$sync.innerrun(unknown source)  java.util.concurrent.futuretask.run(unknown source)  java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  java.lang.thread.run(unknown source) no matter where i click in the busy compaction thread timeline in yourkit it's in running state and showing this above trace, except for short periods of time where it's actually compacting thanks,  eric<stacktrace> <code> info [compactionexecutor:48] 2011-11-25 22:25:30,353 compactiontask.java (line 213) compacted to [/data1/cassandra/data/ma_ddr/indexes_03-hc-5369-data.db,]. 5,821,590 to 5,306,354 (~91% of original) bytes for 123 keys at 0.163755mb/s. time: 30,903ms. here's a slightly larger one:  info [compactionexecutor:43] 2011-11-25 22:23:28,956 compactiontask.java (line 213) compacted to [/data1/cassandra/data/ma_ddr/indexes_03-hc-5336-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5337-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5338-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5339-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5340-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5341-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5342-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5343-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5344-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5345-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5346-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5347-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5348-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5349-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5350-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5351-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5352-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5353-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5354-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5355-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5356-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5357-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5358-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5359-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5360-data.db,/data1/cassandra/data/ma_ddr/indexes_03-hc-5361-data.db,]. 140,706,512 to 137,990,868 (~98% of original) bytes for 2,181 keys at 0.338627mb/s. time: 388,623ms. stack trace:   java.io.unixfilesystem.getbooleanattributes0(native method)  java.io.unixfilesystem.getbooleanattributes(unknown source)  java.io.file.isdirectory(unknown source)  org.apache.cassandra.io.sstable.sstable$3.accept(sstable.java:204)  java.io.file.listfiles(unknown source)  org.apache.cassandra.io.sstable.sstable.componentsfor(sstable.java:200)  org.apache.cassandra.io.sstable.sstablewriter.cleanupifnecessary(sstablewriter.java:289)  org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:189)  org.apache.cassandra.db.compaction.leveledcompactiontask.execute(leveledcompactiontask.java:57)  org.apache.cassandra.db.compaction.compactionmanager$1.call(compactionmanager.java:134)  org.apache.cassandra.db.compaction.compactionmanager$1.call(compactionmanager.java:114)  java.util.concurrent.futuretask$sync.innerrun(unknown source)  java.util.concurrent.futuretask.run(unknown source)  java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  java.lang.thread.run(unknown source) <text> from what i can tell sstablewriter.cleanupifnecessary seems increasingly costly as the number of files in the data dir increases.  it calls sstable.componentsfor(descriptor, descriptor.tempstate.temp) which lists all files in the data dir to find matching components. am i roughly correct that (cleanupcost = sstable count * data dir size)? we had been doing write load testing with default compaction throttling (16mb/s) and leveledcompaction.  unfortunately we haven't been keeping tabs on sstable counts and it grew out of control. on a system with 300,000 sstables here is an example of our compaction rate. note that as you're probably aware cleanupifnecessary is included in the timing: this is with compaction throttling set to 0 (off). so i believe because of this it's going to take a very long time to recover from having so many small sstables.   it might be notable that we're using solaris 10, possibly listfiles() is faster on other platforms? is it feasible to keep track of the temp files and just delete them rather than searching for them for each sstable using sstable.componentsfor()? here's the stack trace for the compactionexecutor:14 thread that appears to be occupying the majority of the cpu time on this node: name: compactionexecutor:14  state: runnable  total blocked: 3 total waited: 1,610,714 no matter where i click in the busy compaction thread timeline in yourkit it's in running state and showing this above trace, except for short periods of time where it's actually compacting thanks,  eric",
        "label": 172
    },
    {
        "text": "the inspectjvmoptions startup check can trigger some exception on some jre versions <description> andres de la pe\u00f1a pointed out that the startup check added by cassandra-13006 can cause some exception if cassandra is run on a non ga version.  after investigation it seems that it can also be the case for major versions or some jre 9 versions.<stacktrace> <code> <text> andres de la pea pointed out that the startup check added by cassandra-13006 can cause some exception if cassandra is run on a non ga version.  after investigation it seems that it can also be the case for major versions or some jre 9 versions.",
        "label": 69
    },
    {
        "text": "make it safe to concurrently access absc after its construction <description> this is a physical four node cluster. configuration is attached. create a keyspace and table from the first node: create keyspace \"keyspace1\" with replication = {   'class': 'simplestrategy',   'replication_factor': '1' }; use \"keyspace1\"; create table \"counter1\" (   key blob,   column1 ascii,   value counter,   primary key (key, column1) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   index_interval=128 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   default_time_to_live=0 and   speculative_retry='none' and   memtable_flush_period_in_ms=0 and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={}; and the find the following in the logs: info  [thrift:1] 2014-02-19 14:04:35,828 migrationmanager.java:210 - create new columnfamily: org.apache.cassandra.config.cfmetadata@d824292[cfid=d1bc0c30-99b1-11e3-a5f9-c187ff8103e2,ksname=keyspace1,cfname=counter1,cftype=standard,comparator=org.apache.cassandra.db.marshal.asciitype,comment=,readrepairchance=0.1,dclocalreadrepairchance=0.0,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.countercolumntype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=3 cap=3]=columndefinition{name=key, type=org.apache.cassandra.db.marshal.bytestype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=5 cap=5]=columndefinition{name=value, type=org.apache.cassandra.db.marshal.countercolumntype, kind=compact_value, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=7 cap=7]=columndefinition{name=column1, type=org.apache.cassandra.db.marshal.asciitype, kind=clustering_column, componentindex=null, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={},bloomfilterfpchance=0.01,memtableflushperiod=0,caching=keys_only,defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=none,populateiocacheonflush=false,droppedcolumns={},triggers={},rowsperpartitiontocache=100] error [write-/172.16.1.211] 2014-02-19 14:04:35,838 outboundtcpconnection.java:256 - error writing to /172.16.1.211 java.lang.arrayindexoutofboundsexception: -1         at org.apache.cassandra.db.arraybackedsortedcolumns.internalappendorreconcile(arraybackedsortedcolumns.java:231) ~[main/:na]         at org.apache.cassandra.db.arraybackedsortedcolumns.sortcells(arraybackedsortedcolumns.java:143) ~[main/:na]         at org.apache.cassandra.db.arraybackedsortedcolumns.maybesortcells(arraybackedsortedcolumns.java:103) ~[main/:na]         at org.apache.cassandra.db.arraybackedsortedcolumns.getcolumncount(arraybackedsortedcolumns.java:313) ~[main/:na]         at org.apache.cassandra.db.columnfamilyserializer.contentserializedsize(columnfamilyserializer.java:117) ~[main/:na]         at org.apache.cassandra.db.columnfamilyserializer.serializedsize(columnfamilyserializer.java:132) ~[main/:na]         at org.apache.cassandra.db.mutation$mutationserializer.serializedsize(mutation.java:337) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$migrationsserializer.serializedsize(migrationmanager.java:397) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$migrationsserializer.serializedsize(migrationmanager.java:371) ~[main/:na]         at org.apache.cassandra.net.messageout.serialize(messageout.java:116) ~[main/:na]         at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:273) [main/:na]         at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:225) [main/:na]         at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:163) [main/:na] among a few other similar errors. see the attached log. there were no errors in the log of the node it was trying to contact.<stacktrace> info  [thrift:1] 2014-02-19 14:04:35,828 migrationmanager.java:210 - create new columnfamily: org.apache.cassandra.config.cfmetadata@d824292[cfid=d1bc0c30-99b1-11e3-a5f9-c187ff8103e2,ksname=keyspace1,cfname=counter1,cftype=standard,comparator=org.apache.cassandra.db.marshal.asciitype,comment=,readrepairchance=0.1,dclocalreadrepairchance=0.0,gcgraceseconds=864000,defaultvalidator=org.apache.cassandra.db.marshal.countercolumntype,keyvalidator=org.apache.cassandra.db.marshal.bytestype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=3 cap=3]=columndefinition{name=key, type=org.apache.cassandra.db.marshal.bytestype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=5 cap=5]=columndefinition{name=value, type=org.apache.cassandra.db.marshal.countercolumntype, kind=compact_value, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=7 cap=7]=columndefinition{name=column1, type=org.apache.cassandra.db.marshal.asciitype, kind=clustering_column, componentindex=null, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={},bloomfilterfpchance=0.01,memtableflushperiod=0,caching=keys_only,defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=none,populateiocacheonflush=false,droppedcolumns={},triggers={},rowsperpartitiontocache=100] error [write-/172.16.1.211] 2014-02-19 14:04:35,838 outboundtcpconnection.java:256 - error writing to /172.16.1.211 java.lang.arrayindexoutofboundsexception: -1         at org.apache.cassandra.db.arraybackedsortedcolumns.internalappendorreconcile(arraybackedsortedcolumns.java:231) ~[main/:na]         at org.apache.cassandra.db.arraybackedsortedcolumns.sortcells(arraybackedsortedcolumns.java:143) ~[main/:na]         at org.apache.cassandra.db.arraybackedsortedcolumns.maybesortcells(arraybackedsortedcolumns.java:103) ~[main/:na]         at org.apache.cassandra.db.arraybackedsortedcolumns.getcolumncount(arraybackedsortedcolumns.java:313) ~[main/:na]         at org.apache.cassandra.db.columnfamilyserializer.contentserializedsize(columnfamilyserializer.java:117) ~[main/:na]         at org.apache.cassandra.db.columnfamilyserializer.serializedsize(columnfamilyserializer.java:132) ~[main/:na]         at org.apache.cassandra.db.mutation$mutationserializer.serializedsize(mutation.java:337) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$migrationsserializer.serializedsize(migrationmanager.java:397) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$migrationsserializer.serializedsize(migrationmanager.java:371) ~[main/:na]         at org.apache.cassandra.net.messageout.serialize(messageout.java:116) ~[main/:na]         at org.apache.cassandra.net.outboundtcpconnection.writeinternal(outboundtcpconnection.java:273) [main/:na]         at org.apache.cassandra.net.outboundtcpconnection.writeconnected(outboundtcpconnection.java:225) [main/:na]         at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:163) [main/:na] <code> create keyspace 'keyspace1' with replication = {   'class': 'simplestrategy',   'replication_factor': '1' }; use 'keyspace1'; create table 'counter1' (   key blob,   column1 ascii,   value counter,   primary key (key, column1) ) with compact storage and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   index_interval=128 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   default_time_to_live=0 and   speculative_retry='none' and   memtable_flush_period_in_ms=0 and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={}; <text> this is a physical four node cluster. configuration is attached. create a keyspace and table from the first node: and the find the following in the logs: among a few other similar errors. see the attached log. there were no errors in the log of the node it was trying to contact.",
        "label": 18
    },
    {
        "text": "remove leftover code from refactor <description> code seems to have been left over from refactor from 2.2 to 3.0. removed.<stacktrace> <code> <text> code seems to have been left over from refactor from 2.2 to 3.0. removed.",
        "label": 139
    },
    {
        "text": "dead code  net headertypes java <description> class o.a.c.net.headertypes is not used anywhere, and, even though public, is of dubious value, because it only contains a couple of constants and no code. googling resulted in no hits other than from this source file. searching commit history didn't uncover any reasons for its continued existence, so that whole file can be safely removed.<stacktrace> <code> <text> class o.a.c.net.headertypes is not used anywhere, and, even though public, is of dubious value, because it only contains a couple of constants and no code. googling resulted in no hits other than from this source file. searching commit history didn't uncover any reasons for its continued existence, so that whole file can be safely removed.",
        "label": 134
    },
    {
        "text": "clustering key with bytebuffer size   64k throws assertion error <description> setup: create keyspace blues with replication = { 'class' : 'simplestrategy', 'replication_factor' : 2}; create table test (a text, b text, primary key ((a), b)) there currently doesn't seem to be an existing check for selecting clustering keys that are larger than 64k. so if we proceed to do the following select: consistency all; select * from blues.test where a = 'foo' and b = 'something larger than 64k'; an assertionerror is thrown in `bytebufferutil` with just a number and an error message detailing 'coordinator node timed out waiting for replica nodes responses' . additionally, because an error extends throwable (it's not a subclass of exception), it's not caught so the connection between the coordinator node and the other nodes which have the replicas seem to be 'stuck' until it's restarted. any other subsequent queries, even if it's just select where a = 'foo' and b = 'bar', will always return the coordinator timing out waiting for replica nodes responses'.<stacktrace> <code> create keyspace blues with replication = { 'class' : 'simplestrategy', 'replication_factor' : 2}; create table test (a text, b text, primary key ((a), b)) consistency all; select * from blues.test where a = 'foo' and b = 'something larger than 64k'; <text> setup: there currently doesn't seem to be an existing check for selecting clustering keys that are larger than 64k. so if we proceed to do the following select: an assertionerror is thrown in `bytebufferutil` with just a number and an error message detailing 'coordinator node timed out waiting for replica nodes responses' . additionally, because an error extends throwable (it's not a subclass of exception), it's not caught so the connection between the coordinator node and the other nodes which have the replicas seem to be 'stuck' until it's restarted. any other subsequent queries, even if it's just select where a = 'foo' and b = 'bar', will always return the coordinator timing out waiting for replica nodes responses'.",
        "label": 309
    },
    {
        "text": "dtest failure in replace address test testreplaceaddress replace first boot test <description> this looks like a timeout kind of flap. it's flapped once. example failure: http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/344/testreport/replace_address_test/testreplaceaddress/replace_first_boot_test failed on cassci build cassandra-2.2_offheap_dtest #344 - 2.2.6-tentative error message 15 apr 2016 16:23:41 [node3] missing: ['127.0.0.4.* now up']: info  [main] 2016-04-15 16:21:32,345 config.java:4..... see system.log for remainder -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /mnt/tmp/dtest-4i5qke dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'memtable_allocation_type': 'offheap_objects',     'num_tokens': '32',     'phi_convict_threshold': 5,     'start_rpc': 'true'} dtest: debug: starting cluster with 3 nodes. dtest: debug: 32 dtest: debug: inserting data... dtest: debug: stopping node 3. dtest: debug: testing node stoppage (query should fail). dtest: debug: retrying read after timeout. attempt #0 dtest: debug: retrying read after timeout. attempt #1 dtest: debug: retrying request after ue. attempt #2 dtest: debug: retrying request after ue. attempt #3 dtest: debug: retrying request after ue. attempt #4 dtest: debug: starting node 4 to replace node 3 dtest: debug: verifying querying works again. dtest: debug: verifying tokens migrated sucessfully dtest: debug: ('warn  [main] 2016-04-15 16:21:21,068 tokenmetadata.java:196 - token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4\\n', <_sre.sre_match object at 0x7fd21c0e2370>) dtest: debug: try to restart node 3 (should fail) dtest: debug: [('warn  [gossipstage:1] 2016-04-15 16:21:22,942 storageservice.java:1962 - host id collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner\\n', <_sre.sre_match object at 0x7fd1f83555e0>)] --------------------- >> end captured logging << --------------------- stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/replace_address_test.py\", line 212, in replace_first_boot_test     node4.start(wait_for_binary_proto=true)   file \"/home/automaton/ccm/ccmlib/node.py\", line 610, in start     node.watch_log_for_alive(self, from_mark=mark)   file \"/home/automaton/ccm/ccmlib/node.py\", line 457, in watch_log_for_alive     self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)   file \"/home/automaton/ccm/ccmlib/node.py\", line 425, in watch_log_for     raise timeouterror(time.strftime(\"%d %b %y %h:%m:%s\", time.gmtime()) + \" [\" + self.name + \"] missing: \" + str([e.pattern for e in tofind]) + \":\\n\" + reads[:50] + \".....\\nsee {} for remainder\".format(filename)) \"15 apr 2016 16:23:41 [node3] missing: ['127.0.0.4.* now up']:\\ninfo  [main] 2016-04-15 16:21:32,345 config.java:4.....\\nsee system.log for remainder\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: /mnt/tmp/dtest-4i5qke\\ndtest: debug: custom init_config not found. setting defaults.\\ndtest: debug: done setting configuration options:\\n{   'initial_token': none,\\n    'memtable_allocation_type': 'offheap_objects',\\n    'num_tokens': '32',\\n    'phi_convict_threshold': 5,\\n    'start_rpc': 'true'}\\ndtest: debug: starting cluster with 3 nodes.\\ndtest: debug: 32\\ndtest: debug: inserting data...\\ndtest: debug: stopping node 3.\\ndtest: debug: testing node stoppage (query should fail).\\ndtest: debug: retrying read after timeout. attempt #0\\ndtest: debug: retrying read after timeout. attempt #1\\ndtest: debug: retrying request after ue. attempt #2\\ndtest: debug: retrying request after ue. attempt #3\\ndtest: debug: retrying request after ue. attempt #4\\ndtest: debug: starting node 4 to replace node 3\\ndtest: debug: verifying querying works again.\\ndtest: debug: verifying tokens migrated sucessfully\\ndtest: debug: ('warn  [main] 2016-04-15 16:21:21,068 tokenmetadata.java:196 - token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4\\\\n', <_sre.sre_match object at 0x7fd21c0e2370>)\\ndtest: debug: try to restart node 3 (should fail)\\ndtest: debug: [('warn  [gossipstage:1] 2016-04-15 16:21:22,942 storageservice.java:1962 - host id collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner\\\\n', <_sre.sre_match object at 0x7fd1f83555e0>)]\\n--------------------- >> end captured logging << ---------------------\"<stacktrace> <code> error message 15 apr 2016 16:23:41 [node3] missing: ['127.0.0.4.* now up']: info  [main] 2016-04-15 16:21:32,345 config.java:4..... see system.log for remainder -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /mnt/tmp/dtest-4i5qke dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'memtable_allocation_type': 'offheap_objects',     'num_tokens': '32',     'phi_convict_threshold': 5,     'start_rpc': 'true'} dtest: debug: starting cluster with 3 nodes. dtest: debug: 32 dtest: debug: inserting data... dtest: debug: stopping node 3. dtest: debug: testing node stoppage (query should fail). dtest: debug: retrying read after timeout. attempt #0 dtest: debug: retrying read after timeout. attempt #1 dtest: debug: retrying request after ue. attempt #2 dtest: debug: retrying request after ue. attempt #3 dtest: debug: retrying request after ue. attempt #4 dtest: debug: starting node 4 to replace node 3 dtest: debug: verifying querying works again. dtest: debug: verifying tokens migrated sucessfully dtest: debug: ('warn  [main] 2016-04-15 16:21:21,068 tokenmetadata.java:196 - token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4/n', <_sre.sre_match object at 0x7fd21c0e2370>) dtest: debug: try to restart node 3 (should fail) dtest: debug: [('warn  [gossipstage:1] 2016-04-15 16:21:22,942 storageservice.java:1962 - host id collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner/n', <_sre.sre_match object at 0x7fd1f83555e0>)] --------------------- >> end captured logging << --------------------- stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/replace_address_test.py', line 212, in replace_first_boot_test     node4.start(wait_for_binary_proto=true)   file '/home/automaton/ccm/ccmlib/node.py', line 610, in start     node.watch_log_for_alive(self, from_mark=mark)   file '/home/automaton/ccm/ccmlib/node.py', line 457, in watch_log_for_alive     self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)   file '/home/automaton/ccm/ccmlib/node.py', line 425, in watch_log_for     raise timeouterror(time.strftime('%d %b %y %h:%m:%s', time.gmtime()) + ' [' + self.name + '] missing: ' + str([e.pattern for e in tofind]) + ':/n' + reads[:50] + '...../nsee {} for remainder'.format(filename)) '15 apr 2016 16:23:41 [node3] missing: ['127.0.0.4.* now up']:/ninfo  [main] 2016-04-15 16:21:32,345 config.java:4...../nsee system.log for remainder/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: /mnt/tmp/dtest-4i5qke/ndtest: debug: custom init_config not found. setting defaults./ndtest: debug: done setting configuration options:/n{   'initial_token': none,/n    'memtable_allocation_type': 'offheap_objects',/n    'num_tokens': '32',/n    'phi_convict_threshold': 5,/n    'start_rpc': 'true'}/ndtest: debug: starting cluster with 3 nodes./ndtest: debug: 32/ndtest: debug: inserting data.../ndtest: debug: stopping node 3./ndtest: debug: testing node stoppage (query should fail)./ndtest: debug: retrying read after timeout. attempt #0/ndtest: debug: retrying read after timeout. attempt #1/ndtest: debug: retrying request after ue. attempt #2/ndtest: debug: retrying request after ue. attempt #3/ndtest: debug: retrying request after ue. attempt #4/ndtest: debug: starting node 4 to replace node 3/ndtest: debug: verifying querying works again./ndtest: debug: verifying tokens migrated sucessfully/ndtest: debug: ('warn  [main] 2016-04-15 16:21:21,068 tokenmetadata.java:196 - token -3855903180169109916 changing ownership from /127.0.0.3 to /127.0.0.4//n', <_sre.sre_match object at 0x7fd21c0e2370>)/ndtest: debug: try to restart node 3 (should fail)/ndtest: debug: [('warn  [gossipstage:1] 2016-04-15 16:21:22,942 storageservice.java:1962 - host id collision for 75916cc0-86ec-4136-b336-862a49953616 between /127.0.0.3 and /127.0.0.4; /127.0.0.4 is the new owner//n', <_sre.sre_match object at 0x7fd1f83555e0>)]/n--------------------- >> end captured logging << ---------------------' http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/344/testreport/replace_address_test/testreplaceaddress/replace_first_boot_test failed on cassci build cassandra-2.2_offheap_dtest #344 - 2.2.6-tentative<text> this looks like a timeout kind of flap. it's flapped once. example failure: ",
        "label": 428
    },
    {
        "text": "clean up mmap support <description> awareness of mmap is currently embedded into the sstablereader implementation and indexsummary. a good number of bugs experienced recently have been due to this lack of separation, so it is ripe for abstraction. additionally, the current implementation does not provide a good method for iterating over the segments of a file, which is useful for range queries, and lays more stable groundwork for #998.<stacktrace> <code> <text> awareness of mmap is currently embedded into the sstablereader implementation and indexsummary. a good number of bugs experienced recently have been due to this lack of separation, so it is ripe for abstraction. additionally, the current implementation does not provide a good method for iterating over the segments of a file, which is useful for range queries, and lays more stable groundwork for #998.",
        "label": 515
    },
    {
        "text": "indexsummary   2g causes an assertion error <description> error [compactionexecutor:1546280] 2016-06-01 13:21:00,444  cassandradaemon.java:229 - exception in thread thread[compactionexecutor:1546280,1,main] java.lang.assertionerror: null     at org.apache.cassandra.io.sstable.indexsummarybuilder.maybeaddentry(indexsummarybuilder.java:171) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablewriter$indexwriter.append(sstablewriter.java:634) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablewriter.afterappend(sstablewriter.java:179) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:205) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:126) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:197) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:73) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactioncandidate.run(compactionmanager.java:263) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_51]     at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_51]     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_51]     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_51]     at java.lang.thread.run(thread.java:744) [na:1.7.0_51] i believe this can be fixed by raising the min_index_interval, but we should have a better method of coping with this than throwing the ae.<stacktrace> error [compactionexecutor:1546280] 2016-06-01 13:21:00,444  cassandradaemon.java:229 - exception in thread thread[compactionexecutor:1546280,1,main] java.lang.assertionerror: null     at org.apache.cassandra.io.sstable.indexsummarybuilder.maybeaddentry(indexsummarybuilder.java:171) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablewriter$indexwriter.append(sstablewriter.java:634) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablewriter.afterappend(sstablewriter.java:179) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:205) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:126) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:197) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:73) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactioncandidate.run(compactionmanager.java:263) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]     at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_51]     at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_51]     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_51]     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_51]     at java.lang.thread.run(thread.java:744) [na:1.7.0_51] <code> <text> i believe this can be fixed by raising the min_index_interval, but we should have a better method of coping with this than throwing the ae.",
        "label": 508
    },
    {
        "text": "dtest failure  sstablesplit test testsstablesplit test single file split <description> https://builds.apache.org/view/a-d/view/cassandra/job/cassandra-trunk-dtest/489/testreport/sstablesplit_test/testsstablesplit/test_single_file_split/         for (stdout, stderr, rc) in result:             logger.debug(stderr) >           failure = stderr.find(\"java.lang.assertionerror: data component is missing\") e           typeerror: a bytes-like object is required, not 'str'<stacktrace> <code>         for (stdout, stderr, rc) in result:             logger.debug(stderr) >           failure = stderr.find('java.lang.assertionerror: data component is missing') e           typeerror: a bytes-like object is required, not 'str' https://builds.apache.org/view/a-d/view/cassandra/job/cassandra-trunk-dtest/489/testreport/sstablesplit_test/testsstablesplit/test_single_file_split/<text> ",
        "label": 399
    },
    {
        "text": "select on tuple relations are broken for mixed asc desc clustering order <description> as noted on cassandra-6875, the tuple notation is broken when the clustering order mixes asc and desc directives because the range of data they describe don't correspond to a single continuous slice internally. to copy the example from cassandra-6875: cqlsh:ks> create table foo (a int, b int, c int, primary key (a, b, c)) with clustering order by (b desc, c asc); cqlsh:ks> insert into foo (a, b, c) values (0, 2, 0); cqlsh:ks> insert into foo (a, b, c) values (0, 1, 0); cqlsh:ks> insert into foo (a, b, c) values (0, 1, 1); cqlsh:ks> insert into foo (a, b, c) values (0, 0, 0); cqlsh:ks> select * from foo where a=0;  a | b | c ---+---+---  0 | 2 | 0  0 | 1 | 0  0 | 1 | 1  0 | 0 | 0 (4 rows) cqlsh:ks> select * from foo where a=0 and (b, c) > (1, 0);  a | b | c ---+---+---  0 | 2 | 0 (1 rows) the last query should really return (0, 2, 0) and (0, 1, 1). for that specific example we should generate 2 internal slices, but i believe that with more clustering columns we may have more slices.<stacktrace> <code> cqlsh:ks> create table foo (a int, b int, c int, primary key (a, b, c)) with clustering order by (b desc, c asc); cqlsh:ks> insert into foo (a, b, c) values (0, 2, 0); cqlsh:ks> insert into foo (a, b, c) values (0, 1, 0); cqlsh:ks> insert into foo (a, b, c) values (0, 1, 1); cqlsh:ks> insert into foo (a, b, c) values (0, 0, 0); cqlsh:ks> select * from foo where a=0;  a | b | c ---+---+---  0 | 2 | 0  0 | 1 | 0  0 | 1 | 1  0 | 0 | 0 (4 rows) cqlsh:ks> select * from foo where a=0 and (b, c) > (1, 0);  a | b | c ---+---+---  0 | 2 | 0 (1 rows) <text> as noted on cassandra-6875, the tuple notation is broken when the clustering order mixes asc and desc directives because the range of data they describe don't correspond to a single continuous slice internally. to copy the example from cassandra-6875: the last query should really return (0, 2, 0) and (0, 1, 1). for that specific example we should generate 2 internal slices, but i believe that with more clustering columns we may have more slices.",
        "label": 319
    },
    {
        "text": "reduce bloom filter garbage allocation <description> just spotted that we allocate potentially large amounts of garbage on bloom filter lookups, since we allocate a new long[] for each hash() and to store the bucket indexes we visit, in a manner that guarantees they are allocated on heap. with a lot of sstables and many requests, this could easily be hundreds of megabytes of young gen churn per second.<stacktrace> <code> <text> just spotted that we allocate potentially large amounts of garbage on bloom filter lookups, since we allocate a new long[] for each hash() and to store the bucket indexes we visit, in a manner that guarantees they are allocated on heap. with a lot of sstables and many requests, this could easily be hundreds of megabytes of young gen churn per second.",
        "label": 67
    },
    {
        "text": "npe in sstablereader invalidatecachekey <description> error [nonperiodictasks:1] 2015-09-03 05:24:51,056 cassandradaemon.java:223 - exception in thread thread[nonperiodictasks:1,5,main] java.lang.nullpointerexception: null         at org.apache.cassandra.io.sstable.sstablereader.invalidatecachekey(sstablereader.java:1445) ~[apache-cassandra-2.1.7.jar:2.1.7]         at org.apache.cassandra.io.sstable.sstablerewriter$1.run(sstablerewriter.java:323) ~[apache-cassandra-2.1.7.jar:2.1.7]         at org.apache.cassandra.io.sstable.sstablereader$5.run(sstablereader.java:1024) ~[apache-cassandra-2.1.7.jar:2.1.7]         at org.apache.cassandra.io.sstable.sstablereader$instancetidier$1.run(sstablereader.java:2146) ~[apache-cassandra-2.1.7.jar:2.1.7]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_51]         at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_51]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:180) ~[na:1.8.0_51]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:293) ~[na:1.8.0_51]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_51]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_51]         at java.lang.thread.run(thread.java:745) [na:1.8.0_51] seen in 2.1.7 in a node running cleanup. may be fixed already, logging to be safe.<stacktrace> error [nonperiodictasks:1] 2015-09-03 05:24:51,056 cassandradaemon.java:223 - exception in thread thread[nonperiodictasks:1,5,main] java.lang.nullpointerexception: null         at org.apache.cassandra.io.sstable.sstablereader.invalidatecachekey(sstablereader.java:1445) ~[apache-cassandra-2.1.7.jar:2.1.7]         at org.apache.cassandra.io.sstable.sstablerewriter$1.run(sstablerewriter.java:323) ~[apache-cassandra-2.1.7.jar:2.1.7]         at org.apache.cassandra.io.sstable.sstablereader$5.run(sstablereader.java:1024) ~[apache-cassandra-2.1.7.jar:2.1.7]         at org.apache.cassandra.io.sstable.sstablereader$instancetidier$1.run(sstablereader.java:2146) ~[apache-cassandra-2.1.7.jar:2.1.7]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_51]         at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_51]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:180) ~[na:1.8.0_51]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:293) ~[na:1.8.0_51]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_51]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_51]         at java.lang.thread.run(thread.java:745) [na:1.8.0_51] <code> <text> seen in 2.1.7 in a node running cleanup. may be fixed already, logging to be safe.",
        "label": 409
    },
    {
        "text": "setting log4j logger org apache cassandra debug causes keyspace username password to show up in system log <description> if using a third party log aggregator (which many cloud users use), this causes db credentials to be reproduced offsite, which has potential to be security issue. i would prefer the ability to disable the logging of this information while still setting log4j.logger.org.apache.cassandra=debug example system.log entry: debug [native-transport-requests:373] 2015-06-21 07:52:44,595 message.java (line 326) responding: authenticate org.apache.cassandra.auth.passwordauthenticator, v=1 debug [native-transport-requests:384] 2015-06-21 07:52:44,597 message.java (line 319) received: credentials {username=redacted, password=redacted}, v=1<stacktrace> <code> debug [native-transport-requests:373] 2015-06-21 07:52:44,595 message.java (line 326) responding: authenticate org.apache.cassandra.auth.passwordauthenticator, v=1 debug [native-transport-requests:384] 2015-06-21 07:52:44,597 message.java (line 319) received: credentials {username=redacted, password=redacted}, v=1 <text> if using a third party log aggregator (which many cloud users use), this causes db credentials to be reproduced offsite, which has potential to be security issue. i would prefer the ability to disable the logging of this information while still setting log4j.logger.org.apache.cassandra=debug example system.log entry:",
        "label": 474
    },
    {
        "text": "make memtable flush thresholds per cf instead of global <description> this is particularly useful in the scenario where you have a few cfs with a high volume of overwrite operations; increasing the memtable size/op count means that you can do the overwrite in memory before it ever hits disk. once on disk compaction is much more work for the system. but, you don't want to give all your cfs that high of a threshold because the memory is better used elsewhere, and because it makes commitlog replay unnecessarily painful.<stacktrace> <code> <text> this is particularly useful in the scenario where you have a few cfs with a high volume of overwrite operations; increasing the memtable size/op count means that you can do the overwrite in memory before it ever hits disk. once on disk compaction is much more work for the system. but, you don't want to give all your cfs that high of a threshold because the memory is better used elsewhere, and because it makes commitlog replay unnecessarily painful.",
        "label": 270
    },
    {
        "text": "create junit xml files <description> the continuous integration engine hudson that is used by apache doesn't seem to read testng reports. testng can output junit reports though, we should change the build.xml to do so.<stacktrace> <code> <text> the continuous integration engine hudson that is used by apache doesn't seem to read testng reports. testng can output junit reports though, we should change the build.xml to do so.",
        "label": 264
    },
    {
        "text": "wrong check of partitioner for secondary indexes <description> cassandra-3407 doesn't handle the fact that secondary indexes have a specific partitioner (localpartitioner). this result in the following error when starting nodes in 1.0.4: java.lang.runtimeexception: cannot open /var/lib/cassandra/data/index/attractionlocationcategorydateidx.attractionlocationcategorydateidx_09partition_idx-h-1 because partitioner does not match org.apache.cassandra.dht.localpartitioner<stacktrace> <code> java.lang.runtimeexception: cannot open /var/lib/cassandra/data/index/attractionlocationcategorydateidx.attractionlocationcategorydateidx_09partition_idx-h-1 because partitioner does not match org.apache.cassandra.dht.localpartitioner <text> cassandra-3407 doesn't handle the fact that secondary indexes have a specific partitioner (localpartitioner). this result in the following error when starting nodes in 1.0.4:",
        "label": 577
    },
    {
        "text": "once a message has been dropped  cassandra logs total messages dropped and tpstats every 5s forever <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "corruptsstableexception should print the ss table name <description> we should print the ss table name that's being reported as corrupt to help with quick recovery. info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-21214 (23832772 bytes)  info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-18398 (149675 bytes)  info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-23707 (18270 bytes)  info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-13656 (814588 bytes)  error 16:32:15 exiting forcefully due to file system exception on startup, disk failure policy \"stop\"  org.apache.cassandra.io.sstable.corruptsstableexception: java.io.eofexception  at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:131) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at org.apache.cassandra.io.compress.compressionmetadata.create(compressionmetadata.java:85) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at org.apache.cassandra.io.util.compressedsegmentedfile$builder.metadata(compressedsegmentedfile.java:79) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:72) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at<stacktrace> info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-21214 (23832772 bytes)  info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-18398 (149675 bytes)  info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-23707 (18270 bytes)  info 16:32:15 opening /mnt/cassandra/data/exchangecf/udsuserhourlysnapshot-d1260590711511e587125dc4955cc492/exchangecf-udsuserhourlysnapshot-ka-13656 (814588 bytes)  error 16:32:15 exiting forcefully due to file system exception on startup, disk failure policy 'stop'  org.apache.cassandra.io.sstable.corruptsstableexception: java.io.eofexception  at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:131) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at org.apache.cassandra.io.compress.compressionmetadata.create(compressionmetadata.java:85) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at org.apache.cassandra.io.util.compressedsegmentedfile$builder.metadata(compressedsegmentedfile.java:79) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:72) ~[apache-cassandra-2.1.9-snapshot.jar:2.1.9-snapshot]  at<code> <text> we should print the ss table name that's being reported as corrupt to help with quick recovery. ",
        "label": 244
    },
    {
        "text": "typo in snitch rst <description> a patch to fix the typo: https://github.com/dongqixue/cassandra/commit/c70496884967e166859abce597e737e4f77b1ddc<stacktrace> <code> a patch to fix the typo: https://github.com/dongqixue/cassandra/commit/c70496884967e166859abce597e737e4f77b1ddc<text> ",
        "label": 158
    },
    {
        "text": "dtest failure in bootstrap test testbootstrap resumable bootstrap test <description> stress is failing to read back all data. we can see this output from the stress read java.io.ioexception: operation x0 on key(s) [314c384f304f4c325030]: data returned was not validated at org.apache.cassandra.stress.operation.error(operation.java:138) at org.apache.cassandra.stress.operation.timewithretry(operation.java:116) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:101) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:109) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:261) at org.apache.cassandra.stress.stressaction$consumer.run(stressaction.java:327) java.io.ioexception: operation x0 on key(s) [33383438363931353131]: data returned was not validated at org.apache.cassandra.stress.operation.error(operation.java:138) at org.apache.cassandra.stress.operation.timewithretry(operation.java:116) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:101) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:109) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:261) at org.apache.cassandra.stress.stressaction$consumer.run(stressaction.java:327) failure started happening with build 1075. does not appear flaky on ci. example failure: http://cassci.datastax.com/job/trunk_dtest/1076/testreport/bootstrap_test/testbootstrap/resumable_bootstrap_test failed on cassci build trunk_dtest #1076<stacktrace> java.io.ioexception: operation x0 on key(s) [314c384f304f4c325030]: data returned was not validated at org.apache.cassandra.stress.operation.error(operation.java:138) at org.apache.cassandra.stress.operation.timewithretry(operation.java:116) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:101) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:109) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:261) at org.apache.cassandra.stress.stressaction$consumer.run(stressaction.java:327) java.io.ioexception: operation x0 on key(s) [33383438363931353131]: data returned was not validated at org.apache.cassandra.stress.operation.error(operation.java:138) at org.apache.cassandra.stress.operation.timewithretry(operation.java:116) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:101) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:109) at org.apache.cassandra.stress.operations.predefined.cqloperation.run(cqloperation.java:261) at org.apache.cassandra.stress.stressaction$consumer.run(stressaction.java:327) failure <code> http://cassci.datastax.com/job/trunk_dtest/1076/testreport/bootstrap_test/testbootstrap/resumable_bootstrap_test <text> stress is failing to read back all data. we can see this output from the stress read started happening with build 1075. does not appear flaky on ci. example failure: failed on cassci build trunk_dtest #1076",
        "label": 409
    },
    {
        "text": "move the decompose  methods from o a c db marshal type to o a c cql jdbc jdbc  <description> the preparedstatement code for 1.1 has been committed to trunk with bytebuffer as the argument for bound variables. the client side code using the cassandra-clientutil jar file needs access to decompose() methods for the various client supported java types to decompose the java bind variables to bytebuffer.<stacktrace> <code> <text> the preparedstatement code for 1.1 has been committed to trunk with bytebuffer as the argument for bound variables. the client side code using the cassandra-clientutil jar file needs access to decompose() methods for the various client supported java types to decompose the java bind variables to bytebuffer.",
        "label": 449
    },
    {
        "text": "streaming needs to synchronise access to lifecycletransaction <description> when lifecycletransaction is used in a multi-threaded context, we encounter this exception - java.util.concurrentmodificationexception: null  at java.util.linkedhashmap$linkedhashiterator.nextnode(linkedhashmap.java:719)  at java.util.linkedhashmap$linkedkeyiterator.next(linkedhashmap.java:742)  at java.lang.iterable.foreach(iterable.java:74)  at org.apache.cassandra.db.lifecycle.logreplicaset.maybecreatereplica(logreplicaset.java:78)  at org.apache.cassandra.db.lifecycle.logfile.makerecord(logfile.java:320)  at org.apache.cassandra.db.lifecycle.logfile.add(logfile.java:285)  at org.apache.cassandra.db.lifecycle.logtransaction.tracknew(logtransaction.java:136)  at org.apache.cassandra.db.lifecycle.lifecycletransaction.tracknew(lifecycletransaction.java:529) during streaming we create a reference to a lifecycletransaction and share it between threads - https://github.com/apache/cassandra/blob/5cc68a87359dd02412bdb70a52dfcd718d44a5ba/src/java/org/apache/cassandra/db/streaming/cassandrastreamreader.java#l156 this is used in a multi-threaded context inside cassandraincomingfile which is an incomingstreammessage. this is being deserialized in parallel. lifecycletransaction is not meant to be used in a multi-threaded context and this leads to streaming failures due to object sharing. on trunk, this object is shared across all threads that transfer sstables in parallel for the given tableid in a streamsession. there are two options to solve this - make lifecycletransaction and the associated objects thread safe, scope the transaction to a single cassandraincomingfile. the consequences of the latter option is that if we experience streaming failure we may have redundant sstables on disk. this is ok as compaction should clean this up. a third option is we synchronize access in the streaming infrastructure.<stacktrace> java.util.concurrentmodificationexception: null  at java.util.linkedhashmap$linkedhashiterator.nextnode(linkedhashmap.java:719)  at java.util.linkedhashmap$linkedkeyiterator.next(linkedhashmap.java:742)  at java.lang.iterable.foreach(iterable.java:74)  at org.apache.cassandra.db.lifecycle.logreplicaset.maybecreatereplica(logreplicaset.java:78)  at org.apache.cassandra.db.lifecycle.logfile.makerecord(logfile.java:320)  at org.apache.cassandra.db.lifecycle.logfile.add(logfile.java:285)  at org.apache.cassandra.db.lifecycle.logtransaction.tracknew(logtransaction.java:136)  at org.apache.cassandra.db.lifecycle.lifecycletransaction.tracknew(lifecycletransaction.java:529) <code> https://github.com/apache/cassandra/blob/5cc68a87359dd02412bdb70a52dfcd718d44a5ba/src/java/org/apache/cassandra/db/streaming/cassandrastreamreader.java#l156 <text> when lifecycletransaction is used in a multi-threaded context, we encounter this exception - during streaming we create a reference to a lifecycletransaction and share it between threads - this is used in a multi-threaded context inside cassandraincomingfile which is an incomingstreammessage. this is being deserialized in parallel. lifecycletransaction is not meant to be used in a multi-threaded context and this leads to streaming failures due to object sharing. on trunk, this object is shared across all threads that transfer sstables in parallel for the given tableid in a streamsession. there are two options to solve this - make lifecycletransaction and the associated objects thread safe, scope the transaction to a single cassandraincomingfile. the consequences of the latter option is that if we experience streaming failure we may have redundant sstables on disk. this is ok as compaction should clean this up. a third option is we synchronize access in the streaming infrastructure.",
        "label": 508
    },
    {
        "text": "report compression ratio via nodetool cfstats <description> cassandra-3393 adds a getcompressionratio jmx call, and was originally supposed to also expose this value per cf via nodetool cfstats. however, the nodetool cfstats part was not done in cassandra-3393. this ticket serves as a request to expose this valuable data about compression via nodetool cfstats. (cc: vijay, who did the cassandra-3393 work)<stacktrace> <code> (cc: vijay, who did the cassandra-3393 work)<text> cassandra-3393 adds a getcompressionratio jmx call, and was originally supposed to also expose this value per cf via nodetool cfstats. however, the nodetool cfstats part was not done in cassandra-3393. this ticket serves as a request to expose this valuable data about compression via nodetool cfstats. ",
        "label": 555
    },
    {
        "text": "loss of secondary index entries if nodetool cleanup called before compaction <description> from time to time we had the feeling of not getting all results that should have been returned using secondary indexes. now we tracked down some situations and found out, it happened: 1) to primary keys that were already deleted and have been re-created later on 2) after our nightly maintenance scripts were running we can reproduce now the following szenario: create a row entry with an indexed column included query it and use the secondary index criteria -> success delete it, query again -> entry gone as expected re-create it with the same key, query it -> success again now use in exactly that sequence nodetool cleanup  nodetool flush  nodetool compact when issuing the query now, we don't get the result using the index. the entry is indeed available in it's table when i just ask for the key. below is the exact copy-paste output from cql when i reproduced the problem with an example entry on on of our tables. mwerrch@mstc01401:/opt/cassandra$ current/bin/cqlsh connected to 14-15-cluster at localhost:9160.  [cqlsh 4.1.0 | cassandra 2.0.3 | cql spec 3.1.1 | thrift protocol 19.38.0] use help for help.  cqlsh> use mwerrch;  cqlsh:mwerrch> desc tables; b4container_demo cqlsh:mwerrch> desc table \"b4container_demo\"; create table \"b4container_demo\" (  key uuid,  archived boolean,  bytes int,  computer int,  deleted boolean,  description text,  doarchive boolean,  filename text,  first boolean,  frames int,  ifversion int,  imported boolean,  jobid int,  keepuntil bigint,  nextchunk text,  node int,  recordingkey blob,  recstart bigint,  recstop bigint,  simulationid bigint,  systemstart bigint,  systemstop bigint,  tapelabel bigint,  version blob,  primary key (key)  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='demo' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=604800 and  index_interval=128 and  read_repair_chance=1.000000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  default_time_to_live=0 and  speculative_retry='none' and  memtable_flush_period_in_ms=0 and  compaction= {'class': 'sizetieredcompactionstrategy'} and  compression= {'sstable_compression': 'lz4compressor'} ; create index mwerrch_demo_computer on \"b4container_demo\" (computer); create index mwerrch_demo_node on \"b4container_demo\" (node); create index mwerrch_demo_recordingkey on \"b4container_demo\" (recordingkey); cqlsh:mwerrch> insert into \"b4container_demo\" (key,computer,node) values (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from \"b4container_demo\" where computer=50;  key | node | computer  --------------------------------------------------  78c70562-1f98-3971-9c28-2c3d8e09c10f | 50 | 50 (1 rows) cqlsh:mwerrch> delete from \"b4container_demo\" where key=78c70562-1f98-3971-9c28-2c3d8e09c10f;  cqlsh:mwerrch> select key,node,computer from \"b4container_demo\" where computer=50; (0 rows) cqlsh:mwerrch> insert into \"b4container_demo\" (key,computer,node) values (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from \"b4container_demo\" where computer=50;  key | node | computer  --------------------------------------------------  78c70562-1f98-3971-9c28-2c3d8e09c10f | 50 | 50 (1 rows) **********************************  now we execute (maybe from a different shell so we don't have to close this session) from /opt/cassandra/current/bin directory:  ./nodetool cleanup  ./nodetool flush  ./nodetool compact going back to our cql session the result will no longer be available if queried via the index:  ********************************* cqlsh:mwerrch> select key,node,computer from \"b4container_demo\" where computer=50; (0 rows)<stacktrace> <code> mwerrch@mstc01401:/opt/cassandra$ current/bin/cqlsh connected to 14-15-cluster at localhost:9160.  [cqlsh 4.1.0 | cassandra 2.0.3 | cql spec 3.1.1 | thrift protocol 19.38.0] use help for help.  cqlsh> use mwerrch;  cqlsh:mwerrch> desc tables; b4container_demo cqlsh:mwerrch> desc table 'b4container_demo'; create table 'b4container_demo' (  key uuid,  archived boolean,  bytes int,  computer int,  deleted boolean,  description text,  doarchive boolean,  filename text,  first boolean,  frames int,  ifversion int,  imported boolean,  jobid int,  keepuntil bigint,  nextchunk text,  node int,  recordingkey blob,  recstart bigint,  recstop bigint,  simulationid bigint,  systemstart bigint,  systemstop bigint,  tapelabel bigint,  version blob,  primary key (key)  ) with compact storage and  bloom_filter_fp_chance=0.010000 and  caching='keys_only' and  comment='demo' and  dclocal_read_repair_chance=0.000000 and  gc_grace_seconds=604800 and  index_interval=128 and  read_repair_chance=1.000000 and  replicate_on_write='true' and  populate_io_cache_on_flush='false' and  default_time_to_live=0 and  speculative_retry='none' and  memtable_flush_period_in_ms=0 and  compaction= ; create index mwerrch_demo_computer on 'b4container_demo' (computer); create index mwerrch_demo_node on 'b4container_demo' (node); create index mwerrch_demo_recordingkey on 'b4container_demo' (recordingkey); cqlsh:mwerrch> insert into 'b4container_demo' (key,computer,node) values (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from 'b4container_demo' where computer=50; key | node | computer  --------------------------------------------------  78c70562-1f98-3971-9c28-2c3d8e09c10f | 50 | 50 (1 rows) cqlsh:mwerrch> delete from 'b4container_demo' where key=78c70562-1f98-3971-9c28-2c3d8e09c10f;  cqlsh:mwerrch> select key,node,computer from 'b4container_demo' where computer=50; (0 rows) cqlsh:mwerrch> insert into 'b4container_demo' (key,computer,node) values (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from 'b4container_demo' where computer=50; key | node | computer  --------------------------------------------------  78c70562-1f98-3971-9c28-2c3d8e09c10f | 50 | 50 (1 rows) **********************************  now we execute (maybe from a different shell so we don't have to close this session) from /opt/cassandra/current/bin directory:  ./nodetool cleanup  ./nodetool flush  ./nodetool compact going back to our cql session the result will no longer be available if queried via the index:  ********************************* cqlsh:mwerrch> select key,node,computer from 'b4container_demo' where computer=50; (0 rows)<text> from time to time we had the feeling of not getting all results that should have been returned using secondary indexes. now we tracked down some situations and found out, it happened: 1) to primary keys that were already deleted and have been re-created later on 2) after our nightly maintenance scripts were running we can reproduce now the following szenario: now use in exactly that sequence nodetool cleanup  nodetool flush  nodetool compact when issuing the query now, we don't get the result using the index. the entry is indeed available in it's table when i just ask for the key. below is the exact copy-paste output from cql when i reproduced the problem with an example entry on on of our tables. and  compression= ",
        "label": 474
    },
    {
        "text": "old package name in cql g <description> antlr file cql.g contains old (com.facebook) package name<stacktrace> <code> <text> antlr file cql.g contains old (com.facebook) package name",
        "label": 418
    },
    {
        "text": "remove test distributed <description> now that we've shifted focus to the new ccm-based distributed tests (https://github.com/riptano/cassandra-dtest) i think it's time to remove the now long-neglected distributed test cruft from our tree.<stacktrace> <code> <text> now that we've shifted focus to the new ccm-based distributed tests (https://github.com/riptano/cassandra-dtest) i think it's time to remove the now long-neglected distributed test cruft from our tree.",
        "label": 85
    },
    {
        "text": "columnfamilyrecordreader fails for a given split because a host is down  even if records could reasonably be read from other replica  <description> columnfamilyrecordreader only tries the first location for a given split. we should try multiple locations for a given split.<stacktrace> <code> <text> columnfamilyrecordreader only tries the first location for a given split. we should try multiple locations for a given split.",
        "label": 409
    },
    {
        "text": "streaming occasionally makes gossip back up <description> streaming occasionally makes gossip back up, causing nodes to mark each other as down even though the network is ok. this appears to happen just after streaming has finished. i noticed this in the course of working on cassandra-2072, so decommission is one way to reproduce. it seems to happen maybe one of fifteen or twenty tries, so it's fairly rare.<stacktrace> <code> <text> streaming occasionally makes gossip back up, causing nodes to mark each other as down even though the network is ok. this appears to happen just after streaming has finished. i noticed this in the course of working on cassandra-2072, so decommission is one way to reproduce. it seems to happen maybe one of fifteen or twenty tries, so it's fairly rare.",
        "label": 274
    },
    {
        "text": "failed bootstrap wiped node can join test is failing <description> bootstrap_test.testbootstrap.failed_bootstap_wiped_node_can_join_test is failing on 2.1-head. the second node fails to join the cluster. i see a lot of exceptions in node1's log, such as error [stream-out-/127.0.0.2] 2015-12-11 12:06:13,778 streamsession.java:505 - [stream #7b5ec5a0-a029-11e5-bad9-ffd0922f40e6] streaming error occurred java.io.ioexception: broken pipe         at sun.nio.ch.filedispatcherimpl.write0(native method) ~[na:1.8.0_51]         at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47) ~[na:1.8.0_51]         at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:93) ~[na:1.8.0_51]         at sun.nio.ch.ioutil.write(ioutil.java:65) ~[na:1.8.0_51]         at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:471) ~[na:1.8.0_51]         at org.apache.cassandra.io.util.dataoutputstreamandchannel.write(dataoutputstreamandchannel.java:48) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:44) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:331) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_51] which seem consistent with node2 being killed, so the bootstrap fails. but then when restarting node2, it does not join. it looks like it fails to rejoin because of a false positive in checking the 2 minute rule. error [main] 2015-12-11 12:06:17,954 cassandradaemon.java:579 - except ion encountered during startup java.lang.unsupportedoperationexception: other bootstrapping/leaving/m oving nodes detected, cannot bootstrap while cassandra.consistent.rang emovement is true         at org.apache.cassandra.service.storageservice.checkforendpoin tcollision(storageservice.java:559) ~[main/:na]         at org.apache.cassandra.service.storageservice.preparetojoin(s torageservice.java:789) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(stor ageservice.java:721) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(stor ageservice.java:612) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:387) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:562) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:651) [main/:na] this fails consistently locally and on cassci. logs attached.<stacktrace> error [stream-out-/127.0.0.2] 2015-12-11 12:06:13,778 streamsession.java:505 - [stream #7b5ec5a0-a029-11e5-bad9-ffd0922f40e6] streaming error occurred java.io.ioexception: broken pipe         at sun.nio.ch.filedispatcherimpl.write0(native method) ~[na:1.8.0_51]         at sun.nio.ch.socketdispatcher.write(socketdispatcher.java:47) ~[na:1.8.0_51]         at sun.nio.ch.ioutil.writefromnativebuffer(ioutil.java:93) ~[na:1.8.0_51]         at sun.nio.ch.ioutil.write(ioutil.java:65) ~[na:1.8.0_51]         at sun.nio.ch.socketchannelimpl.write(socketchannelimpl.java:471) ~[na:1.8.0_51]         at org.apache.cassandra.io.util.dataoutputstreamandchannel.write(dataoutputstreamandchannel.java:48) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.serialize(streammessage.java:44) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.sendmessage(connectionhandler.java:351) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:331) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_51] error [main] 2015-12-11 12:06:17,954 cassandradaemon.java:579 - except ion encountered during startup java.lang.unsupportedoperationexception: other bootstrapping/leaving/m oving nodes detected, cannot bootstrap while cassandra.consistent.rang emovement is true         at org.apache.cassandra.service.storageservice.checkforendpoin tcollision(storageservice.java:559) ~[main/:na]         at org.apache.cassandra.service.storageservice.preparetojoin(s torageservice.java:789) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(stor ageservice.java:721) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(stor ageservice.java:612) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:387) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:562) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:651) [main/:na] <code> <text> bootstrap_test.testbootstrap.failed_bootstap_wiped_node_can_join_test is failing on 2.1-head. the second node fails to join the cluster. i see a lot of exceptions in node1's log, such as which seem consistent with node2 being killed, so the bootstrap fails. but then when restarting node2, it does not join. it looks like it fails to rejoin because of a false positive in checking the 2 minute rule. this fails consistently locally and on cassci. logs attached.",
        "label": 261
    },
    {
        "text": "super columns are broken after upgrading to on thrift <description> super columns are broken after upgrading to cassandra-3.0 head. the below script shows this. 2.1 cli output for get: [default@test] get sites[utf8('bob')][utf8('attr')]['name'] as utf8; => (name=name, value=bob, timestamp=1469724504357000) cqlsh: [default@test]  key          | blobastext(column1) --------------+---------------------  0x53696d6f6e |                attr      0x426f62 |                attr 3.0 cli: [default@unknown] use test; unconfigured table schema_columnfamilies [default@test] get sites[utf8('bob')][utf8('attr')]['name'] as utf8; null [default@test] cqlsh:  key          | system.blobastext(column1) --------------+----------------------------------  0x53696d6f6e | \\x00\\x04attr\\x00\\x00\\x04name\\x00      0x426f62 | \\x00\\x04attr\\x00\\x00\\x04name\\x00 run this from a directory with cassandra-3.0 checked out and compiled ccm create -n 2 -v 2.1.14 testsuper echo \"####################### starting 2.1 #######################\" ccm start myfile=`mktemp` echo \"create keyspace test with placement_strategy = 'org.apache.cassandra.locator.simplestrategy' and strategy_options = {replication_factor:2}; use test; create column family sites with column_type = 'super' and comparator = 'bytestype' and subcomparator='utf8type'; set sites[utf8('simon')][utf8('attr')]['name'] = utf8('simon'); set sites[utf8('bob')][utf8('attr')]['name'] = utf8('bob'); get sites[utf8('bob')][utf8('attr')]['name'] as utf8;\" > $myfile ~/.ccm/repository/2.1.14/bin/cassandra-cli < $myfile rm $myfile ~/.ccm/repository/2.1.14/bin/nodetool -p 7100 flush ~/.ccm/repository/2.1.14/bin/nodetool -p 7200 flush ccm stop # run from cassandra-3.0 checked out and compiled ccm setdir echo \"####################### starting current directory #######################\" ccm start ./bin/nodetool -p 7100 upgradesstables ./bin/nodetool -p 7200 upgradesstables ./bin/nodetool -p 7100 enablethrift ./bin/nodetool -p 7200 enablethrift myfile=`mktemp` echo \"use test; get sites[utf8('bob')][utf8('attr')]['name'] as utf8;\" > $myfile ~/.ccm/repository/2.1.14/bin/cassandra-cli < $myfile rm $myfile<stacktrace> <code> [default@test] get sites[utf8('bob')][utf8('attr')]['name'] as utf8; => (name=name, value=bob, timestamp=1469724504357000) [default@test]  key          | blobastext(column1) --------------+---------------------  0x53696d6f6e |                attr      0x426f62 |                attr [default@unknown] use test; unconfigured table schema_columnfamilies [default@test] get sites[utf8('bob')][utf8('attr')]['name'] as utf8; null [default@test]  key          | system.blobastext(column1) --------------+----------------------------------  0x53696d6f6e | /x00/x04attr/x00/x00/x04name/x00      0x426f62 | /x00/x04attr/x00/x00/x04name/x00 ccm create -n 2 -v 2.1.14 testsuper echo '####################### starting 2.1 #######################' ccm start myfile=`mktemp` echo 'create keyspace test with placement_strategy = 'org.apache.cassandra.locator.simplestrategy' and strategy_options = {replication_factor:2}; use test; create column family sites with column_type = 'super' and comparator = 'bytestype' and subcomparator='utf8type'; set sites[utf8('simon')][utf8('attr')]['name'] = utf8('simon'); set sites[utf8('bob')][utf8('attr')]['name'] = utf8('bob'); get sites[utf8('bob')][utf8('attr')]['name'] as utf8;' > $myfile ~/.ccm/repository/2.1.14/bin/cassandra-cli < $myfile rm $myfile ~/.ccm/repository/2.1.14/bin/nodetool -p 7100 flush ~/.ccm/repository/2.1.14/bin/nodetool -p 7200 flush ccm stop # run from cassandra-3.0 checked out and compiled ccm setdir echo '####################### starting current directory #######################' ccm start ./bin/nodetool -p 7100 upgradesstables ./bin/nodetool -p 7200 upgradesstables ./bin/nodetool -p 7100 enablethrift ./bin/nodetool -p 7200 enablethrift myfile=`mktemp` echo 'use test; get sites[utf8('bob')][utf8('attr')]['name'] as utf8;' > $myfile ~/.ccm/repository/2.1.14/bin/cassandra-cli < $myfile rm $myfile <text> super columns are broken after upgrading to cassandra-3.0 head. the below script shows this. 2.1 cli output for get: cqlsh: 3.0 cli: cqlsh: run this from a directory with cassandra-3.0 checked out and compiled",
        "label": 520
    },
    {
        "text": "multi get slice failing nullpointer exception <description> noticed this in trunk error [pool-1-thread-40] 2010-01-11 22:13:55,333 cassandra.java (line 960) internal error processing multiget_slice  java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.nullpointerexception  at org.apache.cassandra.service.storageproxy.weakreadlocal(storageproxy.java:510)  at org.apache.cassandra.service.storageproxy.readprotocol(storageproxy.java:375)  at org.apache.cassandra.service.cassandraserver.readcolumnfamily(cassandraserver.java:81)  at org.apache.cassandra.service.cassandraserver.getslice(cassandraserver.java:164)  at org.apache.cassandra.service.cassandraserver.multigetsliceinternal(cassandraserver.java:237)  at org.apache.cassandra.service.cassandraserver.multiget_slice(cassandraserver.java:209)  at org.apache.cassandra.service.cassandra$processor$multiget_slice.process(cassandra.java:952)  at org.apache.cassandra.service.cassandra$processor.process(cassandra.java:842)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.util.concurrent.executionexception: java.lang.nullpointerexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.service.storageproxy.weakreadlocal(storageproxy.java:506)  ... 11 more  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.filter.slicequeryfilter.filtersupercolumn(slicequeryfilter.java:70)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:809)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:750)  at org.apache.cassandra.db.table.getrow(table.java:398)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:59)  at org.apache.cassandra.service.storageproxy$weakreadlocalcallable.call(storageproxy.java:691)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more<stacktrace> error [pool-1-thread-40] 2010-01-11 22:13:55,333 cassandra.java (line 960) internal error processing multiget_slice  java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.nullpointerexception  at org.apache.cassandra.service.storageproxy.weakreadlocal(storageproxy.java:510)  at org.apache.cassandra.service.storageproxy.readprotocol(storageproxy.java:375)  at org.apache.cassandra.service.cassandraserver.readcolumnfamily(cassandraserver.java:81)  at org.apache.cassandra.service.cassandraserver.getslice(cassandraserver.java:164)  at org.apache.cassandra.service.cassandraserver.multigetsliceinternal(cassandraserver.java:237)  at org.apache.cassandra.service.cassandraserver.multiget_slice(cassandraserver.java:209)  at org.apache.cassandra.service.cassandra$processor$multiget_slice.process(cassandra.java:952)  at org.apache.cassandra.service.cassandra$processor.process(cassandra.java:842)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.util.concurrent.executionexception: java.lang.nullpointerexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.service.storageproxy.weakreadlocal(storageproxy.java:506)  ... 11 more  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.filter.slicequeryfilter.filtersupercolumn(slicequeryfilter.java:70)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:809)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:750)  at org.apache.cassandra.db.table.getrow(table.java:398)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:59)  at org.apache.cassandra.service.storageproxy$weakreadlocalcallable.call(storageproxy.java:691)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more<code> <text> noticed this in trunk ",
        "label": 274
    },
    {
        "text": "providing multiple keys to sstable2json results in invalid json <description> if you pass multiple keys via the -k parameter to sstable for json the 2nd row will be appended to the end of the first without a comma. it would look like so: sstable2json foo-data.db -k key1 -k key2 -k key3 -k key4 { key1 : [[]...]key2: [[]...], key3 : [[]...], key4 : [[]...] }<stacktrace> <code> sstable2json foo-data.db -k key1 -k key2 -k key3 -k key4 { key1 : [[]...]key2: [[]...], key3 : [[]...], key4 : [[]...] } <text> if you pass multiple keys via the -k parameter to sstable for json the 2nd row will be appended to the end of the first without a comma. it would look like so:",
        "label": 106
    },
    {
        "text": "enable describe on indices <description> describe index should be supported, right now, the only way is to export the schema and find what it really is before updating/dropping the index. verified in   [cqlsh 3.1.8 | cassandra 1.2.18.1 | cql spec 3.0.0 | thrift protocol 19.36.2]<stacktrace> <code> verified in   [cqlsh 3.1.8 | cassandra 1.2.18.1 | cql spec 3.0.0 | thrift protocol 19.36.2]<text> describe index should be supported, right now, the only way is to export the schema and find what it really is before updating/dropping the index. ",
        "label": 508
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh copy tests cqlshcopytest test explicit column order reading <description> example failure: http://cassci.datastax.com/job/trunk_novnode_dtest/405/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_explicit_column_order_reading failed on cassci build trunk_novnode_dtest #405 stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 1453, in test_explicit_column_order_reading     self.assertcsvresultequal(reference_file.name, results, 'testorder')   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 318, in assertcsvresultequal     raise e \"element counts were not equal:\\nfirst has 1, second has 0:  ['1', 'ham', '20']\\nfirst has 1, second has 0:  ['2', 'eggs', '40']\\nfirst has 1, second has 0:  ['3', 'beans', '60']\\nfirst has 1, second has 0:  ['4', 'toast', '80']<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 1453, in test_explicit_column_order_reading     self.assertcsvresultequal(reference_file.name, results, 'testorder')   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 318, in assertcsvresultequal     raise e 'element counts were not equal:/nfirst has 1, second has 0:  ['1', 'ham', '20']/nfirst has 1, second has 0:  ['2', 'eggs', '40']/nfirst has 1, second has 0:  ['3', 'beans', '60']/nfirst has 1, second has 0:  ['4', 'toast', '80'] http://cassci.datastax.com/job/trunk_novnode_dtest/405/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_explicit_column_order_reading <text> example failure: failed on cassci build trunk_novnode_dtest #405",
        "label": 261
    },
    {
        "text": "remove clustertool <description> clustertool is a \"run an operation against the whole cluster\" tool, as opposed to nodetool which is single-node. clustertool never achieved feature parity with nodetool and never will for long, since it has to be manually updated for each operation type. since it's trivial to just use standard tools to run nodetool against an entire cluster, let's drop clustertool in favor of encouraging that. some examples. the first two assume a file \"clustermembers\" with your cluster host names; dsh will want that in /etc/dsh/groups instead of cwd. bash: for host in `cat clustermembers`; do nodetool -h $host command;done xargs: # one op at a time cat clustermembers | xargs -n 1 -i{} nodetool -h {} command # parallelize cat clustermembers | xargs -p `wc -l clustermembers` -n 1 -i{} nodetool -h {} command dsh: # one at a time dsh -g clustermembers -- nodetool -h localhost command # parallelize dsh -g -c clustermembers -- nodetool -h localhost command<stacktrace> <code> # one op at a time cat clustermembers | xargs -n 1 -i{} nodetool -h {} command # parallelize cat clustermembers | xargs -p `wc -l clustermembers` -n 1 -i{} nodetool -h {} command <text> for host in `cat clustermembers`; do nodetool -h $host command;done # one at a time dsh -g clustermembers -- nodetool -h localhost command # parallelize dsh -g -c clustermembers -- nodetool -h localhost command clustertool is a 'run an operation against the whole cluster' tool, as opposed to nodetool which is single-node. clustertool never achieved feature parity with nodetool and never will for long, since it has to be manually updated for each operation type. since it's trivial to just use standard tools to run nodetool against an entire cluster, let's drop clustertool in favor of encouraging that. some examples. the first two assume a file 'clustermembers' with your cluster host names; dsh will want that in /etc/dsh/groups instead of cwd. bash: xargs: dsh:",
        "label": 274
    },
    {
        "text": "debian packaging doesn't do auto reloading of log4j properties file <description> cassandra isn't starting the log4j auto-reload thread because it requires -dlog4j.defaultinitoverride=true on initialization. is there a reason to not do this when installed from the debian package?<stacktrace> <code> <text> cassandra isn't starting the log4j auto-reload thread because it requires -dlog4j.defaultinitoverride=true on initialization. is there a reason to not do this when installed from the debian package?",
        "label": 85
    },
    {
        "text": "creation and maintenance of roles should not require superuser status <description> currently, only roles with superuser status are permitted to create/drop/grant/revoke roles, which violates the principal of least privilege. in addition, in order to run alter role statements a user must log in directly as that role or else be a superuser. this requirement increases the (ab)use of superuser privileges, especially where roles are created without login privileges to model groups of permissions granted to individual db users. in this scenario, a superuser is always required if such roles are to be granted and modified. we should add more granular permissions to allow administration of roles without requiring superuser status.<stacktrace> <code> <text> currently, only roles with superuser status are permitted to create/drop/grant/revoke roles, which violates the principal of least privilege. in addition, in order to run alter role statements a user must log in directly as that role or else be a superuser. this requirement increases the (ab)use of superuser privileges, especially where roles are created without login privileges to model groups of permissions granted to individual db users. in this scenario, a superuser is always required if such roles are to be granted and modified. we should add more granular permissions to allow administration of roles without requiring superuser status.",
        "label": 474
    },
    {
        "text": "dealing with hints after a topology change <description> hints are stored and delivered by destination node id. this allows them to survive ip changes in the target, while making \"scan all the hints for a given destination\" an efficient operation. however, we do not detect and handle new node assuming responsibility for the hinted row via bootstrap before it can be delivered. i think we have to take a performance hit in this case \u2013 we need to deliver such a hint to all replicas, since we don't know which is the \"new\" one. this happens infrequently enough, however \u2013 requiring first the target node to be down to create the hint, then the hint owner to be down long enough for the target to both recover and stream to a new node \u2013 that this should be okay.<stacktrace> <code> <text> hints are stored and delivered by destination node id. this allows them to survive ip changes in the target, while making 'scan all the hints for a given destination' an efficient operation. however, we do not detect and handle new node assuming responsibility for the hinted row via bootstrap before it can be delivered. i think we have to take a performance hit in this case - we need to deliver such a hint to all replicas, since we don't know which is the 'new' one. this happens infrequently enough, however - requiring first the target node to be down to create the hint, then the hint owner to be down long enough for the target to both recover and stream to a new node - that this should be okay.",
        "label": 86
    },
    {
        "text": "improve trace messages <description> currently, tracing only records lines like  enqueuing response to / processing response from or  sending message to / message received from. it would help if these messages also contain some information about the verb and (if easily accessible) about kind of content.<stacktrace> <code> <text> currently, tracing only records lines like  enqueuing response to / processing response from or  sending message to / message received from. it would help if these messages also contain some information about the verb and (if easily accessible) about kind of content.",
        "label": 453
    },
    {
        "text": "replacing a dead node using replace address fails <description> failed assertion error [main] 2014-07-17 10:24:21,171 cassandradaemon.java:474 - exception encountered during startup java.lang.assertionerror: expected 1 endpoint but found 0         at org.apache.cassandra.dht.rangestreamer.getallrangeswithstrictsourcesfor(rangestreamer.java:222) ~[main/:na]         at org.apache.cassandra.dht.rangestreamer.addranges(rangestreamer.java:131) ~[main/:na]         at org.apache.cassandra.dht.bootstrapper.bootstrap(bootstrapper.java:72) ~[main/:na]         at org.apache.cassandra.service.storageservice.bootstrap(storageservice.java:1049) ~[main/:na]         at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:811) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:626) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:511) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:338) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:457) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:546) [main/:na] to replicate the bug run the \"replace_address_test.replace_stopped_node_test\" dtest<stacktrace> error [main] 2014-07-17 10:24:21,171 cassandradaemon.java:474 - exception encountered during startup java.lang.assertionerror: expected 1 endpoint but found 0         at org.apache.cassandra.dht.rangestreamer.getallrangeswithstrictsourcesfor(rangestreamer.java:222) ~[main/:na]         at org.apache.cassandra.dht.rangestreamer.addranges(rangestreamer.java:131) ~[main/:na]         at org.apache.cassandra.dht.bootstrapper.bootstrap(bootstrapper.java:72) ~[main/:na]         at org.apache.cassandra.service.storageservice.bootstrap(storageservice.java:1049) ~[main/:na]         at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:811) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:626) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:511) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:338) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:457) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:546) [main/:na] <code> <text> failed assertion to replicate the bug run the 'replace_address_test.replace_stopped_node_test' dtest",
        "label": 85
    },
    {
        "text": "fix skipping logic on upgrade tests in dtest <description> this will be a general ticket for upgrade dtests that fail because of bad logic surrounding skipping tests. we need a better system in place for skipping tests that are not intended to work on certain versions of cassandra; at present, we run the upgrade tests with skip=false because, again, the built-in skipping logic is bad. one such test is test_v2_protocol_in_with_tuples: http://cassci.datastax.com/job/storage_engine_upgrade_dtest-22_tarball-311/lastcompletedbuild/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3/test_v2_protocol_in_with_tuples/ this shouldn't be run on clusters that include nodes running 3.0.<stacktrace> <code> http://cassci.datastax.com/job/storage_engine_upgrade_dtest-22_tarball-311/lastcompletedbuild/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3/test_v2_protocol_in_with_tuples/ <text> this will be a general ticket for upgrade dtests that fail because of bad logic surrounding skipping tests. we need a better system in place for skipping tests that are not intended to work on certain versions of cassandra; at present, we run the upgrade tests with skip=false because, again, the built-in skipping logic is bad. one such test is test_v2_protocol_in_with_tuples: this shouldn't be run on clusters that include nodes running 3.0.",
        "label": 124
    },
    {
        "text": "npe in storageproxy java <description> got this this morning under heavy load: error [readstage:128] 2014-05-21 07:59:03,274 cassandradaemon.java (line 198) exception in thread thread[readstage:128,5,main]  java.lang.runtimeexception: java.lang.nullpointerexception  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1920)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.io.util.randomaccessreader.gettotalbuffersize(randomaccessreader.java:157)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.gettotalbuffersize(compressedrandomaccessreader.java:159)  at org.apache.cassandra.service.filecacheservice.get(filecacheservice.java:96)  at org.apache.cassandra.io.util.poolingsegmentedfile.getsegment(poolingsegmentedfile.java:36)  at org.apache.cassandra.io.sstable.sstablereader.getfiledatainput(sstablereader.java:1195)  at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:57)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:65)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:42)  at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:167)  at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:62)  at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:250)  at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1540)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1369)  at org.apache.cassandra.db.keyspace.getrow(keyspace.java:327)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1352)  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1916)  ... 3 more there had just been a 20 second gc pause, and the system was dropping messages like mad, see attached log snippet.<stacktrace> error [readstage:128] 2014-05-21 07:59:03,274 cassandradaemon.java (line 198) exception in thread thread[readstage:128,5,main]  java.lang.runtimeexception: java.lang.nullpointerexception  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1920)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.io.util.randomaccessreader.gettotalbuffersize(randomaccessreader.java:157)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.gettotalbuffersize(compressedrandomaccessreader.java:159)  at org.apache.cassandra.service.filecacheservice.get(filecacheservice.java:96)  at org.apache.cassandra.io.util.poolingsegmentedfile.getsegment(poolingsegmentedfile.java:36)  at org.apache.cassandra.io.sstable.sstablereader.getfiledatainput(sstablereader.java:1195)  at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:57)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:65)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:42)  at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:167)  at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:62)  at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:250)  at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1540)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1369)  at org.apache.cassandra.db.keyspace.getrow(keyspace.java:327)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1352)  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1916)  ... 3 more <code> <text> got this this morning under heavy load: there had just been a 20 second gc pause, and the system was dropping messages like mad, see attached log snippet.",
        "label": 482
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh tests cqllogintest test login rejects bad pass <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1300/testreport/cqlsh_tests.cqlsh_tests/cqllogintest/test_login_rejects_bad_pass failed on cassci build trunk_dtest #1300 standard output (ee)  <stdin>:2:('unable to connect to any servers', {'127.0.0.1': authenticationfailed(u'failed to authenticate to 127.0.0.1: code=0100 [bad credentials] message=\"provided username user1 and/or password are incorrect\"',)})(ee)  related failure:  http://cassci.datastax.com/job/trunk_dtest/1300/testreport/cqlsh_tests.cqlsh_tests/cqllogintest/test_login_allows_bad_pass_and_continued_use/<stacktrace> <code> standard output (ee)  <stdin>:2:('unable to connect to any servers', {'127.0.0.1': authenticationfailed(u'failed to authenticate to 127.0.0.1: code=0100 [bad credentials] message='provided username user1 and/or password are incorrect'',)})(ee)  http://cassci.datastax.com/job/trunk_dtest/1300/testreport/cqlsh_tests.cqlsh_tests/cqllogintest/test_login_rejects_bad_pass related failure:  http://cassci.datastax.com/job/trunk_dtest/1300/testreport/cqlsh_tests.cqlsh_tests/cqllogintest/test_login_allows_bad_pass_and_continued_use/<text> example failure: failed on cassci build trunk_dtest #1300 ",
        "label": 261
    },
    {
        "text": "dtest failure in materialized views test testmaterializedviews base replica repair test <description> example failure: http://cassci.datastax.com/job/cassandra-3.x_dtest/61/testreport/materialized_views_test/testmaterializedviews/base_replica_repair_test_2/ standard output unexpected error in node1 log, error:  error [main] 2016-12-15 23:40:43,219 cassandradaemon.java:752 - exception encountered during startup java.lang.runtimeexception: cannot replace_address /127.0.0.1 because it doesn't exist in gossip at org.apache.cassandra.service.storageservice.prepareforreplacement(storageservice.java:514) ~[main/:na] at org.apache.cassandra.service.storageservice.preparetojoin(storageservice.java:781) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:668) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:614) ~[main/:na] at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:394) [main/:na] at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:601) [main/:na] at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:735) [main/:na] stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 358, in run     self.teardown()   file \"/home/automaton/cassandra-dtest/dtest.py\", line 583, in teardown     raise assertionerror('unexpected error in log, see stdout')<stacktrace> standard output unexpected error in node1 log, error:  error [main] 2016-12-15 23:40:43,219 cassandradaemon.java:752 - exception encountered during startup java.lang.runtimeexception: cannot replace_address /127.0.0.1 because it doesn't exist in gossip at org.apache.cassandra.service.storageservice.prepareforreplacement(storageservice.java:514) ~[main/:na] at org.apache.cassandra.service.storageservice.preparetojoin(storageservice.java:781) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:668) ~[main/:na] at org.apache.cassandra.service.storageservice.initserver(storageservice.java:614) ~[main/:na] at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:394) [main/:na] at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:601) [main/:na] at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:735) [main/:na] <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 358, in run     self.teardown()   file '/home/automaton/cassandra-dtest/dtest.py', line 583, in teardown     raise assertionerror('unexpected error in log, see stdout') http://cassci.datastax.com/job/cassandra-3.x_dtest/61/testreport/materialized_views_test/testmaterializedviews/base_replica_repair_test_2/<text> example failure: ",
        "label": 409
    },
    {
        "text": "batch mutate failing on trunk <description> batch_mutate is failing on trunk with the following error:     java.lang.assertionerror: current = columndefinition{name=b@706172656e745f70617468, type=org.apache.cassandra.db.marshal.bytestype, kind=static, componentindex=null, indexname=cfs_parent_path, indextype=keys}, new = columndefinition{name=b@70617468, type=org.apache.cassandra.db.marshal.bytestype, kind=static, componentindex=null, indexname=cfs_path, indextype=keys}         at org.apache.cassandra.db.rows.arraybackedrow$sortedbuilder.setcolumn(arraybackedrow.java:617)         at org.apache.cassandra.db.rows.arraybackedrow$sortedbuilder.addcell(arraybackedrow.java:630)         at org.apache.cassandra.db.legacylayout$cellgrouper.addcell(legacylayout.java:891)         at org.apache.cassandra.db.legacylayout$cellgrouper.addatom(legacylayout.java:843)         at org.apache.cassandra.db.legacylayout.getnextrow(legacylayout.java:390)         at org.apache.cassandra.db.legacylayout.tounfilteredrowiterator(legacylayout.java:326)         at org.apache.cassandra.db.legacylayout.tounfilteredrowiterator(legacylayout.java:288)         at org.apache.cassandra.thrift.cassandraserver.createmutationlist(cassandraserver.java:1110)         at org.apache.cassandra.thrift.cassandraserver.batch_mutate(cassandraserver.java:1249) the following mutations was passed to batch_mutate to get this error     mutationmap = {java.nio.heapbytebuffer[pos=0 lim=32 cap=32]= {inode=[ mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 04 70 61 74 68 00, value:2f, timestamp:1438165021749))),  mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 0b 70 61 72 65 6e 74 5f 70 61 74 68 00, value:6e 75 6c 6c, timestamp:1438165021749))),  mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 08 73 65 6e 74 69 6e 65 6c 00, value:78, timestamp:1438165021749))),  mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 04 64 61 74 61 00, value:00 00 00 08 64 61 74 61 73 74 61 78 00 00 00 05 75 73 65 72 73 01 ff 00 00 00 00 00 04 00 00 00 01, timestamp:1438165021749))) ]}}<stacktrace>     java.lang.assertionerror: current = columndefinition{name=b@706172656e745f70617468, type=org.apache.cassandra.db.marshal.bytestype, kind=static, componentindex=null, indexname=cfs_parent_path, indextype=keys}, new = columndefinition{name=b@70617468, type=org.apache.cassandra.db.marshal.bytestype, kind=static, componentindex=null, indexname=cfs_path, indextype=keys}         at org.apache.cassandra.db.rows.arraybackedrow$sortedbuilder.setcolumn(arraybackedrow.java:617)         at org.apache.cassandra.db.rows.arraybackedrow$sortedbuilder.addcell(arraybackedrow.java:630)         at org.apache.cassandra.db.legacylayout$cellgrouper.addcell(legacylayout.java:891)         at org.apache.cassandra.db.legacylayout$cellgrouper.addatom(legacylayout.java:843)         at org.apache.cassandra.db.legacylayout.getnextrow(legacylayout.java:390)         at org.apache.cassandra.db.legacylayout.tounfilteredrowiterator(legacylayout.java:326)         at org.apache.cassandra.db.legacylayout.tounfilteredrowiterator(legacylayout.java:288)         at org.apache.cassandra.thrift.cassandraserver.createmutationlist(cassandraserver.java:1110)         at org.apache.cassandra.thrift.cassandraserver.batch_mutate(cassandraserver.java:1249) <code>     mutationmap = {java.nio.heapbytebuffer[pos=0 lim=32 cap=32]= {inode=[ mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 04 70 61 74 68 00, value:2f, timestamp:1438165021749))),  mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 0b 70 61 72 65 6e 74 5f 70 61 74 68 00, value:6e 75 6c 6c, timestamp:1438165021749))),  mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 08 73 65 6e 74 69 6e 65 6c 00, value:78, timestamp:1438165021749))),  mutation(column_or_supercolumn:columnorsupercolumn(column:column(name:80 62 00 04 64 61 74 61 00, value:00 00 00 08 64 61 74 61 73 74 61 78 00 00 00 05 75 73 65 72 73 01 ff 00 00 00 00 00 04 00 00 00 01, timestamp:1438165021749))) ]}} <text> batch_mutate is failing on trunk with the following error: the following mutations was passed to batch_mutate to get this error",
        "label": 79
    },
    {
        "text": "missing flexibility to have file server etc  vs  file  when loading config file cassandra yaml <description> the parameter in the vm options -dcassandra.config= needs file:///  allow the user to have optional \"file:///\" when loading the config file from the filesystem<stacktrace> <code> the parameter in the vm options -dcassandra.config= needs file:///  allow the user to have optional 'file:///' when loading the config file from the filesystem<text> ",
        "label": 280
    },
    {
        "text": "exception count not incremented on outofmemoryerror  hsha  <description> one of our nodes decided to stop listening on 9160 (netstat -l was showing nothing and telnet was reporting connection refused). nodetool status showed no hosts down and on the offending node nodetool info gave the following: nodetool info token            : (invoke with -t/--tokens to see all 256 tokens) id               : (removed) gossip active    : true thrift active    : true native transport active: false load             : 2.05 tb generation no    : 1382536528 uptime (seconds) : 432970 heap memory (mb) : 8098.05 / 14131.25 data center      : dc1 rack             : rac2 exceptions       : 0 key cache        : size 536854996 (bytes), capacity 536870912 (bytes), 41383646 hits, 1710831591 requests, 0.024 recent hit rate, 0 save period in seconds row cache        : size 0 (bytes), capacity 0 (bytes), 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds after looking at the cassandra log, i saw a bunch of the following: error [selector-thread-16] 2013-10-27 17:36:00,370 customthshaserver.java (line 187) uncaught exception:  java.lang.outofmemoryerror: unable to create new native thread         at java.lang.thread.start0(native method)         at java.lang.thread.start(thread.java:691)         at java.util.concurrent.threadpoolexecutor.addworker(threadpoolexecutor.java:949)         at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1371)         at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute(debuggablethreadpoolexecutor.java:145)         at org.apache.cassandra.thrift.customthshaserver.requestinvoke(customthshaserver.java:337)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.handleread(customthshaserver.java:281)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.select(customthshaserver.java:224)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.run(customthshaserver.java:182) error [selector-thread-7] 2013-10-27 17:36:00,370 customthshaserver.java (line 187) uncaught exception:  java.lang.outofmemoryerror: unable to create new native thread         at java.lang.thread.start0(native method)         at java.lang.thread.start(thread.java:691)         at java.util.concurrent.threadpoolexecutor.addworker(threadpoolexecutor.java:949)         at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1371)         at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute(debuggablethreadpoolexecutor.java:145)         at org.apache.cassandra.thrift.customthshaserver.requestinvoke(customthshaserver.java:337)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.handleread(customthshaserver.java:281)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.select(customthshaserver.java:224)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.run(customthshaserver.java:182) there wasn't anything else overtly suspicious in the logs except for the occasional error [selector-thread-0] 2013-10-27 17:35:58,662 tnonblockingserver.java (line 468) read an invalid frame size of 0. are you using tframedtransport on the client side? but that periodically comes up - i have looked into it before but it has never seemed to have any serious impact. this ticket is not about why an outofmemoryerror occurred - which is bad but i don't think i have enough information to reproduce or speculate on a cause. this ticket is about the fact that an outofmemoryerror occurred and nodetool info was reporting thrift active : true and exceptions : 0. our monitoring systems and investigation processes are both starting to rely on on the exception count. the fact that it was not accurate here is disconcerting.<stacktrace> error [selector-thread-16] 2013-10-27 17:36:00,370 customthshaserver.java (line 187) uncaught exception:  java.lang.outofmemoryerror: unable to create new native thread         at java.lang.thread.start0(native method)         at java.lang.thread.start(thread.java:691)         at java.util.concurrent.threadpoolexecutor.addworker(threadpoolexecutor.java:949)         at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1371)         at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute(debuggablethreadpoolexecutor.java:145)         at org.apache.cassandra.thrift.customthshaserver.requestinvoke(customthshaserver.java:337)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.handleread(customthshaserver.java:281)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.select(customthshaserver.java:224)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.run(customthshaserver.java:182) error [selector-thread-7] 2013-10-27 17:36:00,370 customthshaserver.java (line 187) uncaught exception:  java.lang.outofmemoryerror: unable to create new native thread         at java.lang.thread.start0(native method)         at java.lang.thread.start(thread.java:691)         at java.util.concurrent.threadpoolexecutor.addworker(threadpoolexecutor.java:949)         at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:1371)         at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.execute(debuggablethreadpoolexecutor.java:145)         at org.apache.cassandra.thrift.customthshaserver.requestinvoke(customthshaserver.java:337)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.handleread(customthshaserver.java:281)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.select(customthshaserver.java:224)         at org.apache.cassandra.thrift.customthshaserver$selectorthread.run(customthshaserver.java:182) <code> nodetool info token            : (invoke with -t/--tokens to see all 256 tokens) id               : (removed) gossip active    : true thrift active    : true native transport active: false load             : 2.05 tb generation no    : 1382536528 uptime (seconds) : 432970 heap memory (mb) : 8098.05 / 14131.25 data center      : dc1 rack             : rac2 exceptions       : 0 key cache        : size 536854996 (bytes), capacity 536870912 (bytes), 41383646 hits, 1710831591 requests, 0.024 recent hit rate, 0 save period in seconds row cache        : size 0 (bytes), capacity 0 (bytes), 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds <text> error [selector-thread-0] 2013-10-27 17:35:58,662 tnonblockingserver.java (line 468) read an invalid frame size of 0. are you using tframedtransport on the client side? one of our nodes decided to stop listening on 9160 (netstat -l was showing nothing and telnet was reporting connection refused). nodetool status showed no hosts down and on the offending node nodetool info gave the following: after looking at the cassandra log, i saw a bunch of the following: there wasn't anything else overtly suspicious in the logs except for the occasional but that periodically comes up - i have looked into it before but it has never seemed to have any serious impact. this ticket is not about why an outofmemoryerror occurred - which is bad but i don't think i have enough information to reproduce or speculate on a cause. this ticket is about the fact that an outofmemoryerror occurred and nodetool info was reporting thrift active : true and exceptions : 0. our monitoring systems and investigation processes are both starting to rely on on the exception count. the fact that it was not accurate here is disconcerting.",
        "label": 362
    },
    {
        "text": "filenotfoundexception during compaction <description> i can't finish any compaction because my nodes always throw a \"filenotfoundexception\". i've already tried the following but nothing helped: 1. nodetool flush  2. nodetool repair (ends with runtimeexception; see attachment)  3. node restart (via dse cassandra-stop) whenever i restart the nodes, another type of exception is logged (see attachment) somewhere near the end of startup process. this particular exception doesn't seem to be critical because the nodes still manage to finish the startup and become online. i don't have specific steps to reproduce the problem that i'm experiencing with compaction and repair. i'm in the middle of migrating 4.8 billion rows from mysql via sstableloader. some things that may or may not be relevant:  1. i didn't drop and recreate the keyspace (so probably not related to cassandra-4857)  2. i do the bulk-loading in batches of 1 to 20 millions rows. when a batch reaches 100% total progress (i.e. starts to build secondary index), i kill the sstableloader process and cancel the index build  3. i restart the nodes occasionally. it's possible that there is an on-going compaction during one of those restarts. related stackoverflow question (mine): http://stackoverflow.com/questions/23435847/filenotfoundexception-during-compaction<stacktrace> <code> related stackoverflow question (mine): http://stackoverflow.com/questions/23435847/filenotfoundexception-during-compaction<text> i can't finish any compaction because my nodes always throw a 'filenotfoundexception'. i've already tried the following but nothing helped: 1. nodetool flush  2. nodetool repair (ends with runtimeexception; see attachment)  3. node restart (via dse cassandra-stop) whenever i restart the nodes, another type of exception is logged (see attachment) somewhere near the end of startup process. this particular exception doesn't seem to be critical because the nodes still manage to finish the startup and become online. i don't have specific steps to reproduce the problem that i'm experiencing with compaction and repair. i'm in the middle of migrating 4.8 billion rows from mysql via sstableloader. some things that may or may not be relevant:  1. i didn't drop and recreate the keyspace (so probably not related to cassandra-4857)  2. i do the bulk-loading in batches of 1 to 20 millions rows. when a batch reaches 100% total progress (i.e. starts to build secondary index), i kill the sstableloader process and cancel the index build  3. i restart the nodes occasionally. it's possible that there is an on-going compaction during one of those restarts. ",
        "label": 321
    },
    {
        "text": "initial view build can be parallel <description> on a node with lots of data (~3tb) building a materialized view takes several weeks, which is not ideal. it's doing this in a single thread. there are several potential ways this can be optimized : do vnodes in parallel, instead of going through the entire range in one thread just iterate through sstables, not worrying about duplicates, and include the timestamp of the original write in the mv mutation. since this doesn't exclude duplicates it does increase the amount of work and could temporarily surface ghost rows (yikes) but i guess that's why they call it eventual consistency. doing it this way can avoid holding references to all tables on disk, allows parallelization, and removes the need to check other sstables for existing data. this is essentially the 'do a full repair' path<stacktrace> <code> <text> on a node with lots of data (~3tb) building a materialized view takes several weeks, which is not ideal. it's doing this in a single thread. there are several potential ways this can be optimized :",
        "label": 37
    },
    {
        "text": "memtable flush causes bad  reversed  get slice <description> if columns are inserted into a row before and after a memtable flush, a get_slice() after the flush with reversed=true will return incorrect results. see attached patch to reproduce.<stacktrace> <code> <text> if columns are inserted into a row before and after a memtable flush, a get_slice() after the flush with reversed=true will return incorrect results. see attached patch to reproduce.",
        "label": 274
    },
    {
        "text": "missing timeout option propagation in cqlsh  cqlsh py  <description> on a slow cluster (here used for testing purpose), cqlsh fails with a timeout error, whatever --connect-timeout option you can pass. here is a sample call: cqlsh 192.168.xxx.yyy connection error: ('unable to connect to any servers', {'192.168.xxx.yyy': operationtimedout('errors=none, last_host=none',)}) cqlsh --connect-timeout=30 192.168.xxx.yyy connection error: ('unable to connect to any servers', {'192.168.xxx.yyy': operationtimedout('errors=none, last_host=none',)}) debugging shows that the timeout is not properly propagated on the underlying responsewaiter.deliver() method in /usr/share/cassandra/lib/cassandra-driver-internal-only-3.0.0-6af642d.zip/cassandra-driver-3.0.0-6af642d/cassandra/connection.py workaround is to propagate, in cqlsh.py, the --connect-timeout option when initialize the cluster connection object (i.e. add kwarg \"control_connection_timeout\" in addition to the existing kwarg \"connect_timeout\") cluster(     <other args>,     control_connection_timeout=float(connect_timeout),     connect_timeout=connect_timeout)<stacktrace> <code> cqlsh 192.168.xxx.yyy connection error: ('unable to connect to any servers', {'192.168.xxx.yyy': operationtimedout('errors=none, last_host=none',)}) cqlsh --connect-timeout=30 192.168.xxx.yyy connection error: ('unable to connect to any servers', {'192.168.xxx.yyy': operationtimedout('errors=none, last_host=none',)}) cluster(     <other args>,     control_connection_timeout=float(connect_timeout),     connect_timeout=connect_timeout) debugging shows that the timeout is not properly propagated on the underlying responsewaiter.deliver() method in /usr/share/cassandra/lib/cassandra-driver-internal-only-3.0.0-6af642d.zip/cassandra-driver-3.0.0-6af642d/cassandra/connection.py workaround is to propagate, in cqlsh.py, the --connect-timeout option when initialize the cluster connection object (i.e. add kwarg 'control_connection_timeout' in addition to the existing kwarg 'connect_timeout')<text> on a slow cluster (here used for testing purpose), cqlsh fails with a timeout error, whatever --connect-timeout option you can pass. here is a sample call: ",
        "label": 284
    },
    {
        "text": "desc alias for describe keyword in cql shell <description> allow desc to be used instead of full describe keyword. mysql and oracle users are used to desc.<stacktrace> <code> <text> allow desc to be used instead of full describe keyword. mysql and oracle users are used to desc.",
        "label": 593
    },
    {
        "text": "undeletable   duplicate rows after upgrading from to <description> we upgraded our cluster today and now have a some rows that refuse to delete. here are some example traces. https://gist.github.com/vishnevskiy/36aa18c468344ea22d14f9fb9b99171d even weirder. updating the row and querying it back results in 2 rows even though the id is the clustering key. user_id            | id                 | since                    | type -------------------+--------------------+--------------------------+------ 116138050710536192 | 153047019424972800 |                     null |    0 116138050710536192 | 153047019424972800 | 2016-05-30 14:53:08+0000 |    2 and then deleting it again only removes the new one. cqlsh:discord_relationships> delete from relationships where user_id = 116138050710536192 and id = 153047019424972800; cqlsh:discord_relationships> select * from relationships where user_id = 116138050710536192 and id = 153047019424972800;  user_id            | id                 | since                    | type --------------------+--------------------+--------------------------+------  116138050710536192 | 153047019424972800 | 2016-05-30 14:53:08+0000 |    2 we tried repairing, compacting, scrubbing. no luck. not sure what to do. is anyone aware of this?<stacktrace> <code> user_id            | id                 | since                    | type -------------------+--------------------+--------------------------+------ 116138050710536192 | 153047019424972800 |                     null |    0 116138050710536192 | 153047019424972800 | 2016-05-30 14:53:08+0000 |    2 cqlsh:discord_relationships> delete from relationships where user_id = 116138050710536192 and id = 153047019424972800; cqlsh:discord_relationships> select * from relationships where user_id = 116138050710536192 and id = 153047019424972800;  user_id            | id                 | since                    | type --------------------+--------------------+--------------------------+------  116138050710536192 | 153047019424972800 | 2016-05-30 14:53:08+0000 |    2 https://gist.github.com/vishnevskiy/36aa18c468344ea22d14f9fb9b99171d <text> we upgraded our cluster today and now have a some rows that refuse to delete. here are some example traces. even weirder. updating the row and querying it back results in 2 rows even though the id is the clustering key. and then deleting it again only removes the new one. we tried repairing, compacting, scrubbing. no luck. not sure what to do. is anyone aware of this?",
        "label": 25
    },
    {
        "text": "nosuchelementexception when executing empty batch  <description> after upgrade to c* 3.0, it fails when executes empty batch: java.util.nosuchelementexception: null         at java.util.arraylist$itr.next(arraylist.java:854) ~[na:1.8.0_60]         at org.apache.cassandra.service.storageproxy.mutatewithtriggers(storageproxy.java:737) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.statements.batchstatement.executewithoutconditions(batchstatement.java:356) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:337) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:323) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.queryprocessor.processbatch(queryprocessor.java:490) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.queryprocessor.processbatch(queryprocessor.java:480) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.transport.messages.batchmessage.execute(batchmessage.java:217) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [apache-cassandra-3.0.0.jar:3.0.0]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_60]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [apache-cassandra-3.0.0.jar:3.0.0]         at java.lang.thread.run(thread.java:745) [na:1.8.0_60]<stacktrace> java.util.nosuchelementexception: null         at java.util.arraylist$itr.next(arraylist.java:854) ~[na:1.8.0_60]         at org.apache.cassandra.service.storageproxy.mutatewithtriggers(storageproxy.java:737) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.statements.batchstatement.executewithoutconditions(batchstatement.java:356) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:337) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:323) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.queryprocessor.processbatch(queryprocessor.java:490) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.cql3.queryprocessor.processbatch(queryprocessor.java:480) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.transport.messages.batchmessage.execute(batchmessage.java:217) ~[apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [apache-cassandra-3.0.0.jar:3.0.0]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_60]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [apache-cassandra-3.0.0.jar:3.0.0]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [apache-cassandra-3.0.0.jar:3.0.0]         at java.lang.thread.run(thread.java:745) [na:1.8.0_60] <code> <text> after upgrade to c* 3.0, it fails when executes empty batch:",
        "label": 580
    },
    {
        "text": "messageserializationpropertytest fails with bytes should not be empty for type org apache cassandra db marshal bytestype <description> cause was :- java.lang.assertionerror: bytes should not be empty for type org.apache.cassandra.db.marshal.bytestype at org.apache.cassandra.db.marshal.abstracttype.writtenlength(abstracttype.java:423) at org.apache.cassandra.db.singlepartitionreadcommand.selectionserializedsize(singlepartitionreadcommand.java:1043) at org.apache.cassandra.db.readcommand$serializer.serializedsize(readcommand.java:1038) at org.apache.cassandra.db.readcommand$serializer.serializedsize(readcommand.java:909) at org.apache.cassandra.net.message$serializer.payloadsize(message.java:1289) at org.apache.cassandra.net.message.payloadsize(message.java:1333) at org.apache.cassandra.net.message$serializer.serializepre40(message.java:917) at org.apache.cassandra.net.message$serializer.serialize(message.java:620) at org.apache.cassandra.net.messageserializationpropertytest.lambda$serializesizeproperty$0(messageserializationpropertytest.java:73) seed was 35361441975355 this is caused by the fact the generators allow empty types.<stacktrace> cause was :- java.lang.assertionerror: bytes should not be empty for type org.apache.cassandra.db.marshal.bytestype at org.apache.cassandra.db.marshal.abstracttype.writtenlength(abstracttype.java:423) at org.apache.cassandra.db.singlepartitionreadcommand.selectionserializedsize(singlepartitionreadcommand.java:1043) at org.apache.cassandra.db.readcommand$serializer.serializedsize(readcommand.java:1038) at org.apache.cassandra.db.readcommand$serializer.serializedsize(readcommand.java:909) at org.apache.cassandra.net.message$serializer.payloadsize(message.java:1289) at org.apache.cassandra.net.message.payloadsize(message.java:1333) at org.apache.cassandra.net.message$serializer.serializepre40(message.java:917) at org.apache.cassandra.net.message$serializer.serialize(message.java:620) at org.apache.cassandra.net.messageserializationpropertytest.lambda$serializesizeproperty$0(messageserializationpropertytest.java:73) seed was 35361441975355 <code> <text> this is caused by the fact the generators allow empty types.",
        "label": 82
    },
    {
        "text": "sstablesplit fails in <description> sstablesplit dtest began failing in 2.1 at http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/95/ triggered by http://cassci.datastax.com/job/cassandra-2.1/186/  repro: (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./bin/cassandra >/dev/null  (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./tools/bin/cassandra-stress write n=1000000 created keyspaces. sleeping 1s for propagation. warming up write with 50000 iterations... connected to cluster: test cluster datatacenter: datacenter1; host: localhost/127.0.0.1; rack: rack1 sleeping 2s... running write with 50 threads  for 1000000 iterations ops       ,    op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr 26836     ,   26830,   26830,     2.0,     1.1,     4.0,    20.8,   131.4,   207.4,    1.0,  0.00000 64002     ,   36236,   36236,     1.4,     0.8,     4.2,    13.8,    41.3,   234.8,    2.0,  0.00000 105604    ,   38188,   38188,     1.3,     0.8,     3.2,    10.6,    78.4,    93.7,    3.1,  0.10546 156179    ,   36750,   36750,     1.4,     0.9,     2.9,     8.8,   117.0,   139.8,    4.5,  0.08482 202092    ,   40487,   40487,     1.2,     0.9,     2.9,     7.3,    45.6,   122.5,    5.6,  0.07231 246947    ,   40583,   40583,     1.2,     0.8,     3.0,     7.6,    98.2,   152.1,    6.7,  0.07056 290186    ,   39867,   39867,     1.3,     0.8,     2.6,     8.9,   113.3,   126.4,    7.8,  0.06391 331609    ,   40155,   40155,     1.2,     0.8,     3.1,     8.7,    99.1,   124.9,    8.8,  0.05731 371813    ,   38742,   38742,     1.3,     0.8,     3.1,     9.2,   117.2,   123.9,    9.9,  0.05153 416853    ,   40024,   40024,     1.2,     0.8,     3.2,     8.1,    70.4,   119.8,   11.0,  0.04634 458389    ,   39045,   39045,     1.3,     0.8,     3.2,     9.1,   106.4,   135.9,   12.1,  0.04236 511323    ,   36513,   36513,     1.4,     0.8,     3.3,     9.2,   120.2,   161.0,   13.5,  0.03883 549872    ,   34296,   34296,     1.5,     0.9,     3.4,    11.5,   106.7,   132.7,   14.6,  0.03678 589405    ,   34535,   34535,     1.4,     0.9,     2.9,    10.6,   106.2,   147.9,   15.8,  0.03607 633225    ,   39472,   39472,     1.3,     0.8,     3.0,     7.6,   106.3,   125.1,   16.9,  0.03374 672751    ,   38251,   38251,     1.3,     0.8,     3.0,     8.0,    94.7,   157.5,   17.9,  0.03193 714762    ,   38047,   38047,     1.3,     0.8,     3.0,     9.3,   102.6,   167.8,   19.0,  0.03001 756629    ,   38080,   38080,     1.3,     0.8,     3.2,     8.8,   101.7,   117.4,   20.1,  0.02847 802981    ,   38955,   38955,     1.3,     0.8,     3.0,     9.1,   105.2,   164.6,   21.3,  0.02708 847262    ,   38817,   38817,     1.3,     0.7,     3.2,     9.8,   112.1,   137.4,   22.5,  0.02581 887639    ,   38403,   38403,     1.3,     0.8,     2.9,    10.0,    99.1,   147.8,   23.5,  0.02470 929362    ,   35056,   35056,     1.4,     0.8,     3.3,    11.5,   111.8,   149.3,   24.7,  0.02360 980996    ,   38247,   38247,     1.3,     0.8,     3.5,     8.3,    78.8,   129.0,   26.1,  0.02338 1000000   ,   39379,   39379,     1.2,     0.9,     3.1,     9.0,    29.4,    83.8,   26.5,  0.02238 results: real op rate              : 37673 adjusted op rate stderr   : 0 key rate                  : 37673 latency mean              : 1.3 latency median            : 0.8 latency 95th percentile   : 3.2 latency 99th percentile   : 10.4 latency 99.9th percentile : 92.1 latency max               : 234.8 total operation time      : 00:00:26 end (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./bin/nodetool compact keyspace1 (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./bin/sstablesplit /var/lib/cassandra/data/keyspace1/standard1-*/keyspace1-standard1-ka-2-data.db exception in thread \"main\" java.lang.assertionerror         at org.apache.cassandra.db.keyspace.openwithoutsstables(keyspace.java:104)         at org.apache.cassandra.tools.standalonesplitter.main(standalonesplitter.java:108) there are no errors in system.log.<stacktrace> (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./bin/cassandra >/dev/null  (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./tools/bin/cassandra-stress write n=1000000 created keyspaces. sleeping 1s for propagation. warming up write with 50000 iterations... connected to cluster: test cluster datatacenter: datacenter1; host: localhost/127.0.0.1; rack: rack1 sleeping 2s... running write with 50 threads  for 1000000 iterations ops       ,    op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr 26836     ,   26830,   26830,     2.0,     1.1,     4.0,    20.8,   131.4,   207.4,    1.0,  0.00000 64002     ,   36236,   36236,     1.4,     0.8,     4.2,    13.8,    41.3,   234.8,    2.0,  0.00000 105604    ,   38188,   38188,     1.3,     0.8,     3.2,    10.6,    78.4,    93.7,    3.1,  0.10546 156179    ,   36750,   36750,     1.4,     0.9,     2.9,     8.8,   117.0,   139.8,    4.5,  0.08482 202092    ,   40487,   40487,     1.2,     0.9,     2.9,     7.3,    45.6,   122.5,    5.6,  0.07231 246947    ,   40583,   40583,     1.2,     0.8,     3.0,     7.6,    98.2,   152.1,    6.7,  0.07056 290186    ,   39867,   39867,     1.3,     0.8,     2.6,     8.9,   113.3,   126.4,    7.8,  0.06391 331609    ,   40155,   40155,     1.2,     0.8,     3.1,     8.7,    99.1,   124.9,    8.8,  0.05731 371813    ,   38742,   38742,     1.3,     0.8,     3.1,     9.2,   117.2,   123.9,    9.9,  0.05153 416853    ,   40024,   40024,     1.2,     0.8,     3.2,     8.1,    70.4,   119.8,   11.0,  0.04634 458389    ,   39045,   39045,     1.3,     0.8,     3.2,     9.1,   106.4,   135.9,   12.1,  0.04236 511323    ,   36513,   36513,     1.4,     0.8,     3.3,     9.2,   120.2,   161.0,   13.5,  0.03883 549872    ,   34296,   34296,     1.5,     0.9,     3.4,    11.5,   106.7,   132.7,   14.6,  0.03678 589405    ,   34535,   34535,     1.4,     0.9,     2.9,    10.6,   106.2,   147.9,   15.8,  0.03607 633225    ,   39472,   39472,     1.3,     0.8,     3.0,     7.6,   106.3,   125.1,   16.9,  0.03374 672751    ,   38251,   38251,     1.3,     0.8,     3.0,     8.0,    94.7,   157.5,   17.9,  0.03193 714762    ,   38047,   38047,     1.3,     0.8,     3.0,     9.3,   102.6,   167.8,   19.0,  0.03001 756629    ,   38080,   38080,     1.3,     0.8,     3.2,     8.8,   101.7,   117.4,   20.1,  0.02847 802981    ,   38955,   38955,     1.3,     0.8,     3.0,     9.1,   105.2,   164.6,   21.3,  0.02708 847262    ,   38817,   38817,     1.3,     0.7,     3.2,     9.8,   112.1,   137.4,   22.5,  0.02581 887639    ,   38403,   38403,     1.3,     0.8,     2.9,    10.0,    99.1,   147.8,   23.5,  0.02470 929362    ,   35056,   35056,     1.4,     0.8,     3.3,    11.5,   111.8,   149.3,   24.7,  0.02360 980996    ,   38247,   38247,     1.3,     0.8,     3.5,     8.3,    78.8,   129.0,   26.1,  0.02338 1000000   ,   39379,   39379,     1.2,     0.9,     3.1,     9.0,    29.4,    83.8,   26.5,  0.02238 results: real op rate              : 37673 adjusted op rate stderr   : 0 key rate                  : 37673 latency mean              : 1.3 latency median            : 0.8 latency 95th percentile   : 3.2 latency 99th percentile   : 10.4 latency 99.9th percentile : 92.1 latency max               : 234.8 total operation time      : 00:00:26 end (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./bin/nodetool compact keyspace1 (cassandra-2.1)mshuler@hana:~/git/cassandra$ ./bin/sstablesplit /var/lib/cassandra/data/keyspace1/standard1-*/keyspace1-standard1-ka-2-data.db exception in thread 'main' java.lang.assertionerror         at org.apache.cassandra.db.keyspace.openwithoutsstables(keyspace.java:104)         at org.apache.cassandra.tools.standalonesplitter.main(standalonesplitter.java:108) <code> <text> sstablesplit dtest began failing in 2.1 at http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/95/ triggered by http://cassci.datastax.com/job/cassandra-2.1/186/  repro: there are no errors in system.log.",
        "label": 67
    },
    {
        "text": "stop paranoid disk failure policy is ignored on corruptsstableexception after node is up <description> there is a bug when disk_failure_policy is set to stop_paranoid and corruptsstableexception is thrown after server is up. the problem is that this setting is ignored. normally, it should stop gossip and transport but it just continues to serve requests and an exception is just logged.   this patch unifies the exception handling in jvmstabilityinspector and code is reworked in such way that this inspector acts as a central place where such exceptions are inspected.    the core reason for ignoring that exception is that thrown exception in abstractlocalawareexecturorservice is not corruptsstableexception but it is runtimeexception and that exception is as its cause. hence it is better if we handle this in jvmstabilityinspector which can recursively examine it, hence act accordingly. behaviour before: stop_paranoid of disk_failure_policy is ignored when corruptsstableexception is thrown, e.g. on a regular select statement behaviour after: gossip and transport (cql) is turned off, jvm is still up for further investigation e.g. by jmx.<stacktrace> <code> <text> there is a bug when disk_failure_policy is set to stop_paranoid and corruptsstableexception is thrown after server is up. the problem is that this setting is ignored. normally, it should stop gossip and transport but it just continues to serve requests and an exception is just logged.   this patch unifies the exception handling in jvmstabilityinspector and code is reworked in such way that this inspector acts as a central place where such exceptions are inspected.    the core reason for ignoring that exception is that thrown exception in abstractlocalawareexecturorservice is not corruptsstableexception but it is runtimeexception and that exception is as its cause. hence it is better if we handle this in jvmstabilityinspector which can recursively examine it, hence act accordingly. behaviour before: stop_paranoid of disk_failure_policy is ignored when corruptsstableexception is thrown, e.g. on a regular select statement behaviour after: gossip and transport (cql) is turned off, jvm is still up for further investigation e.g. by jmx.",
        "label": 506
    },
    {
        "text": "json2sstable should support streaming <description> json2sstable loads the entire json file into memory. this is so it can sort the file before creating an sstable. if the file was created using sstable2json and the partitioner isn't changing, this isn't necessary. for very large files this means json2sstable requires a huge amount of memory. there should be an option to stream the file. a simple check for out of order keys will prevent writing bad sstables. this should be possible with the sax style parser available in our current json library.<stacktrace> <code> <text> json2sstable loads the entire json file into memory. this is so it can sort the file before creating an sstable. if the file was created using sstable2json and the partitioner isn't changing, this isn't necessary. for very large files this means json2sstable requires a huge amount of memory. there should be an option to stream the file. a simple check for out of order keys will prevent writing bad sstables. this should be possible with the sax style parser available in our current json library.",
        "label": 412
    },
    {
        "text": "mixed case usernames do not work <description> when you create a user with a mixed case username it is stored as all lower case. when you try and login with the mixed case username it will fail, but logging in as the lower case name works. this is a change from the 2.1.x versions that are released where mixed case usernames worked. example:  create user stbarts with password 'island';   the above statement changes the username to \"stbarts\". this would not be so bad except during login case does matter and has to match what is stored in the system.   recommended fix: allow mixed case usernames to be stored in system, or convert mixed case username entered to lower case during login.<stacktrace> <code> <text> when you create a user with a mixed case username it is stored as all lower case. when you try and login with the mixed case username it will fail, but logging in as the lower case name works. this is a change from the 2.1.x versions that are released where mixed case usernames worked. example:  create user stbarts with password 'island';   the above statement changes the username to 'stbarts'. this would not be so bad except during login case does matter and has to match what is stored in the system.   recommended fix: allow mixed case usernames to be stored in system, or convert mixed case username entered to lower case during login.",
        "label": 474
    },
    {
        "text": "internal messaging should be backwards compatible <description> currently, incompatible changes in the node-to-node communication prevent rolling restarts of clusters. in order to fix this we should: 1) use a framework that makes doing compatible changes easy  2) have a policy of only making compatible changes between versions n and n+1* running multiple versions should only be supported for small periods of time. running clusters of mixed version is not needed here.<stacktrace> <code> <text> currently, incompatible changes in the node-to-node communication prevent rolling restarts of clusters. in order to fix this we should: 1) use a framework that makes doing compatible changes easy  2) have a policy of only making compatible changes between versions n and n+1*",
        "label": 186
    },
    {
        "text": "jna native check can throw a nosuchmethoderror <description> looks like older versions of jna have a different native.register() method from irc:  hi, i'm having trouble starting cassandra up...the error is very bizarre: java.lang.nosuchmethoderror: com.sun.jna.native.register(ljava/lang/string;)v<stacktrace> <code> from irc:  hi, i'm having trouble starting cassandra up...the error is very bizarre: java.lang.nosuchmethoderror: com.sun.jna.native.register(ljava/lang/string;)v<text> looks like older versions of jna have a different native.register() method ",
        "label": 521
    },
    {
        "text": "legacysstabletest fails after cassandra <description> legacysstabletest fails reading pre-3.0 sstables (versions jb, ka, la) with clustering keys and counters. first failing 3.0 testall build /cc sylvain lebresne<stacktrace> <code> <text> legacysstabletest fails reading pre-3.0 sstables (versions jb, ka, la) with clustering keys and counters. first failing 3.0 testall build /cc sylvain lebresne",
        "label": 520
    },
    {
        "text": "give read access to system schema usertypes to all authenticated users <description> when i try to login to cqlsh as a non-superuser i get the following error: errormessage code=2100 [unauthorized] message=\"user mike has no select permission on <table system.schema_usertypes> or any of its parents\"<stacktrace> <code> <text> errormessage code=2100 [unauthorized] message='user mike has no select permission on <table system.schema_usertypes> or any of its parents' when i try to login to cqlsh as a non-superuser i get the following error:",
        "label": 357
    },
    {
        "text": "selectstatement parameters fields should be inspectable by custom indexes and query handlers <description> selectstatement.parameters fields should be inspectable by custom indexes and query handlers<stacktrace> <code> <text> selectstatement.parameters fields should be inspectable by custom indexes and query handlers",
        "label": 244
    },
    {
        "text": "add lz4compressor in cql document <description> lz4compressor should also be documented in sstable_compression option.  http://cassandra.apache.org/doc/cql3/cql.html<stacktrace> <code> <text> lz4compressor should also be documented in sstable_compression option.  http://cassandra.apache.org/doc/cql3/cql.html",
        "label": 362
    },
    {
        "text": "allow specification of tls protocol to use for cqlsh <description> currently when using cqlsh with --ssl it tries to use tls 1.0 to connect. i have my server only serving tls 1.2 which means that i cannot connect. it would be nice if cqlsh allowed the tls protocol it uses to connect to be configured.<stacktrace> <code> <text> currently when using cqlsh with --ssl it tries to use tls 1.0 to connect. i have my server only serving tls 1.2 which means that i cannot connect. it would be nice if cqlsh allowed the tls protocol it uses to connect to be configured.",
        "label": 248
    },
    {
        "text": "optimise sequential overlap visitation for checking tombstone retention in compaction <description> the intervaltree only maps partition keys. since a majority of users deploy a hashed partitioner the work is mostly wasted, since they will be evenly distributed across the full token range owned by the node - and in some cases it is a significant amount of work. we can perform a corroboration against the file bounds if we get a bf match as a sanity check if we like, but performing an intervaltree search is significantly more expensive (esp. once murmur hash calculation memoization goes mainstream). in lcs, the keys are bounded, to it might appear that it would help, but in this scenario we only compact against like bounds, so again it is not helpful. with a byteorderedpartitioner it could potentially be of use, but this is sufficiently rare to not optimise for imo.<stacktrace> <code> <text> the intervaltree only maps partition keys. since a majority of users deploy a hashed partitioner the work is mostly wasted, since they will be evenly distributed across the full token range owned by the node - and in some cases it is a significant amount of work. we can perform a corroboration against the file bounds if we get a bf match as a sanity check if we like, but performing an intervaltree search is significantly more expensive (esp. once murmur hash calculation memoization goes mainstream). in lcs, the keys are bounded, to it might appear that it would help, but in this scenario we only compact against like bounds, so again it is not helpful. with a byteorderedpartitioner it could potentially be of use, but this is sufficiently rare to not optimise for imo.",
        "label": 67
    },
    {
        "text": "fix cqlsstablewriter compatibility changes from cassandra <description> cassandra-11844 changed the way the cqlsstablewriter works out of the box, which we should avoid until 4.0 output directory now includes subdirectories for keyspace/table (by default this shouldn't happen) writing to multiple sstablewriters requires passing the offline cfs object. this should be changed to work as it used to.<stacktrace> <code> <text> cassandra-11844 changed the way the cqlsstablewriter works out of the box, which we should avoid until 4.0",
        "label": 244
    },
    {
        "text": "hadoop on cf with columncounter columns fails <description> accessing countercolumn from hadoop fails with an exception:   java.lang.runtimeexception: java.lang.nullpointerexception         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.maybeinit(columnfamilyrecordreader.java:456)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.computenext(columnfamilyrecordreader.java:462)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.computenext(columnfamilyrecordreader.java:409)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.hadoop.columnfamilyrecordreader.nextkeyvalue(columnfamilyrecordreader.java:184)         at org.apache.hadoop.mapred.maptask$newtrackingrecordreader.nextkeyvalue(maptask.java:532)         at org.apache.hadoop.mapreduce.mapcontext.nextkeyvalue(mapcontext.java:67)         at org.apache.hadoop.mapreduce.mapper.run(mapper.java:143)         at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:764)         at org.apache.hadoop.mapred.maptask.run(maptask.java:370)         at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:212) caused by: java.lang.nullpointerexception         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator$widecolumniterator.computenext(columnfamilyrecordreader.java:500)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator$widecolumniterator.computenext(columnfamilyrecordreader.java:472)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at com.google.common.collect.iterators$peekingimpl.hasnext(iterators.java:1080)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.maybeinit(columnfamilyrecordreader.java:449)         ... 11 more<stacktrace>   java.lang.runtimeexception: java.lang.nullpointerexception         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.maybeinit(columnfamilyrecordreader.java:456)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.computenext(columnfamilyrecordreader.java:462)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.computenext(columnfamilyrecordreader.java:409)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.hadoop.columnfamilyrecordreader.nextkeyvalue(columnfamilyrecordreader.java:184)         at org.apache.hadoop.mapred.maptask$newtrackingrecordreader.nextkeyvalue(maptask.java:532)         at org.apache.hadoop.mapreduce.mapcontext.nextkeyvalue(mapcontext.java:67)         at org.apache.hadoop.mapreduce.mapper.run(mapper.java:143)         at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:764)         at org.apache.hadoop.mapred.maptask.run(maptask.java:370)         at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:212) caused by: java.lang.nullpointerexception         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator$widecolumniterator.computenext(columnfamilyrecordreader.java:500)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator$widecolumniterator.computenext(columnfamilyrecordreader.java:472)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at com.google.common.collect.iterators$peekingimpl.hasnext(iterators.java:1080)         at org.apache.cassandra.hadoop.columnfamilyrecordreader$widerowiterator.maybeinit(columnfamilyrecordreader.java:449)         ... 11 more <code> <text> accessing countercolumn from hadoop fails with an exception:",
        "label": 320
    },
    {
        "text": "add schema to snapshot manifest <description> followup from cassandra-6326<stacktrace> <code> <text> followup from cassandra-6326",
        "label": 25
    },
    {
        "text": "avoid quadratic startup time in leveledmanifest <description> checking that each sstable is in the manifest on startup is o(n**2) in the number of sstables: .       // ensure all sstables are in the manifest         for (sstablereader sstablereader : cfs.getsstables())         {             if (manifest.levelof(sstablereader) < 0)                 manifest.add(sstablereader);         } private int levelof(sstablereader sstable) {     for (int level = 0; level < generations.length; level++)     {         if (generations[level].contains(sstable))             return level;     }     return -1; } note that the contains call is a linear list.contains. we need to switch to a sorted list and bsearch, or a tree, to support tb-levels of data in leveledcompactionstrategy.<stacktrace> <code> .       // ensure all sstables are in the manifest         for (sstablereader sstablereader : cfs.getsstables())         {             if (manifest.levelof(sstablereader) < 0)                 manifest.add(sstablereader);         } private int levelof(sstablereader sstable) {     for (int level = 0; level < generations.length; level++)     {         if (generations[level].contains(sstable))             return level;     }     return -1; } <text> checking that each sstable is in the manifest on startup is o(n**2) in the number of sstables: note that the contains call is a linear list.contains. we need to switch to a sorted list and bsearch, or a tree, to support tb-levels of data in leveledcompactionstrategy.",
        "label": 139
    },
    {
        "text": "using config converter  some fields are not converted <description> when running the config-converter, the value of endpointsnitch isn't transfered to the yaml version.  the dynamic_snitch value is added, though.  i imagine the value of org.apache.cassandra.locator.endpointsnitch should be converted to org.apache.cassandra.locator.rackinferringsnitch rather than to simplesnitch. also, <thriftframedtransport>false</thriftframedtransport> is not converted to the yaml version.  that is, the configuration uses the new default for 0.7 which is framed transport. perhaps, for the benefit of the users, the config-converter should produce informative messages on both issues after the operation is complete to make the user aware that the defaults have changed, so that the user will look into it.<stacktrace> <code> <text> when running the config-converter, the value of endpointsnitch isn't transfered to the yaml version.  the dynamic_snitch value is added, though.  i imagine the value of org.apache.cassandra.locator.endpointsnitch should be converted to org.apache.cassandra.locator.rackinferringsnitch rather than to simplesnitch. also, <thriftframedtransport>false</thriftframedtransport> is not converted to the yaml version.  that is, the configuration uses the new default for 0.7 which is framed transport. perhaps, for the benefit of the users, the config-converter should produce informative messages on both issues after the operation is complete to make the user aware that the defaults have changed, so that the user will look into it.",
        "label": 270
    },
    {
        "text": "testall failure in org apache cassandra db keyspacetest testlimitsstables compression <description> example failure: http://cassci.datastax.com/job/cassandra-3.x_testall/38/testreport/org.apache.cassandra.db/keyspacetest/testlimitsstables_compression/ error message expected:<5.0> but was:<6.0> stacktrace junit.framework.assertionfailederror: expected:<5.0> but was:<6.0> at org.apache.cassandra.db.keyspacetest.testlimitsstables(keyspacetest.java:421) standard output error [main] 2016-10-20 05:56:18,156 ?:? - slf4j: stderr info  [main] 2016-10-20 05:56:18,516 ?:? - configuration location: file:/home/automaton/cassandra/test/conf/cassandra.yaml debug [main] 2016-10-20 05:56:18,532 ?:? - loading settings from file:/home/automaton/cassandra/test/conf/cassandra.yaml info  [main] 2016-10-20 05:56:19,632 ?:? - node configuration:[allocate_tokens_for_keyspace=null; authenticator=null; authorizer=null; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=f ...[truncated 453203 chars]... ablereader(path='/home/automaton/cassandra/build/test/cassandra/data:108/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/mc-26-big-data.db')] (1 sstables, 6.278kib), biggest 6.278kib, smallest 6.278kib debug [memtableflushwriter:2] 2016-10-20 05:56:34,725 ?:? - flushed to [bigtablereader(path='/home/automaton/cassandra/build/test/cassandra/data:108/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/mc-22-big-data.db')] (1 sstables, 5.559kib), biggest 5.559kib, smallest 5.559kib<stacktrace> stacktrace junit.framework.assertionfailederror: expected:<5.0> but was:<6.0> at org.apache.cassandra.db.keyspacetest.testlimitsstables(keyspacetest.java:421) <code> error message expected:<5.0> but was:<6.0> standard output error [main] 2016-10-20 05:56:18,156 ?:? - slf4j: stderr info  [main] 2016-10-20 05:56:18,516 ?:? - configuration location: file:/home/automaton/cassandra/test/conf/cassandra.yaml debug [main] 2016-10-20 05:56:18,532 ?:? - loading settings from file:/home/automaton/cassandra/test/conf/cassandra.yaml info  [main] 2016-10-20 05:56:19,632 ?:? - node configuration:[allocate_tokens_for_keyspace=null; authenticator=null; authorizer=null; auto_bootstrap=true; auto_snapshot=true; back_pressure_enabled=f ...[truncated 453203 chars]... ablereader(path='/home/automaton/cassandra/build/test/cassandra/data:108/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/mc-26-big-data.db')] (1 sstables, 6.278kib), biggest 6.278kib, smallest 6.278kib debug [memtableflushwriter:2] 2016-10-20 05:56:34,725 ?:? - flushed to [bigtablereader(path='/home/automaton/cassandra/build/test/cassandra/data:108/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/mc-22-big-data.db')] (1 sstables, 5.559kib), biggest 5.559kib, smallest 5.559kib http://cassci.datastax.com/job/cassandra-3.x_testall/38/testreport/org.apache.cassandra.db/keyspacetest/testlimitsstables_compression/<text> example failure: ",
        "label": 86
    },
    {
        "text": "nodetool listsnapshots output is missing a newline  if there are no snapshots <description> when there are no snapshots, the nodetool listsnaphots command output is missing a newline, which gives a somewhat bad user experience: root@cassandra2:~# nodetool listsnapshots snapshot details:  there are no snapshotsroot@cassandra2:~#  i<stacktrace> <code> root@cassandra2:~# nodetool listsnapshots snapshot details:  there are no snapshotsroot@cassandra2:~#  <text> when there are no snapshots, the nodetool listsnaphots command output is missing a newline, which gives a somewhat bad user experience: i",
        "label": 55
    },
    {
        "text": "migrate cql dtests to unit tests <description> we have cql tests in 2 places: dtests and unit tests. the unit tests are actually somewhat better in the sense that they have the ability to test both prepared and unprepared statements at the flip of a switch. it's also better to have all those tests in the same place so we can improve the test framework in only one place (cassandra-7959, cassandra-9159, etc...). so we should move the cql dtests to the unit tests (which will be a good occasion to organize them better).<stacktrace> <code> <text> we have cql tests in 2 places: dtests and unit tests. the unit tests are actually somewhat better in the sense that they have the ability to test both prepared and unprepared statements at the flip of a switch. it's also better to have all those tests in the same place so we can improve the test framework in only one place (cassandra-7959, cassandra-9159, etc...). so we should move the cql dtests to the unit tests (which will be a good occasion to organize them better).",
        "label": 508
    },
    {
        "text": "add index metadata to cfmetadata <description> <stacktrace> <code> <text> ",
        "label": 334
    },
    {
        "text": "cassandrastorage doesn't decode name in widerow mode <description> cassandrastorage doesn't decode name in widerow mode. this causes functions such as filter to fail with a classcastexception, since the key is a bytearray instead of a chararray. test.pig define cassandrastorage org.apache.cassandra.hadoop.pig.cassandrastorage; a  = load 'cassandra://metrics/evententries?widerows=true' using cassandrastorage(); -- describe a --> a: {key: chararray,columns: {(name: (),value: chararray)}} b = filter a by key matches '^user.hit'; -- throws cce: org.apache.pig.data.databytearray cannot be cast to java.lang.string<stacktrace> <code> define cassandrastorage org.apache.cassandra.hadoop.pig.cassandrastorage; a  = load 'cassandra://metrics/evententries?widerows=true' using cassandrastorage(); -- describe a --> a: {key: chararray,columns: {(name: (),value: chararray)}} b = filter a by key matches '^user.hit'; -- throws cce: org.apache.pig.data.databytearray cannot be cast to java.lang.string <text> cassandrastorage doesn't decode name in widerow mode. this causes functions such as filter to fail with a classcastexception, since the key is a bytearray instead of a chararray.",
        "label": 85
    },
    {
        "text": "nullpointerexception in keycacheserializer <description> hi, i have this stack trace in the logs of cassandra server (v2.1) error [compactionexecutor:14] 2014-10-06 23:32:02,098 cassandradaemon.java:166 - exception in thread thread[compactionexecutor:14,1,main] java.lang.nullpointerexception: null         at org.apache.cassandra.service.cacheservice$keycacheserializer.serialize(cacheservice.java:475) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.service.cacheservice$keycacheserializer.serialize(cacheservice.java:463) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.cache.autosavingcache$writer.savecache(autosavingcache.java:225) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.db.compaction.compactionmanager$11.run(compactionmanager.java:1061) ~[apache-cassandra-2.1.0.jar:2.1.0]         at java.util.concurrent.executors$runnableadapter.call(unknown source) ~[na:1.7.0]         at java.util.concurrent.futuretask$sync.innerrun(unknown source) ~[na:1.7.0]         at java.util.concurrent.futuretask.run(unknown source) ~[na:1.7.0]         at java.util.concurrent.threadpoolexecutor.runworker(unknown source) [na:1.7.0]         at java.util.concurrent.threadpoolexecutor$worker.run(unknown source) [na:1.7.0]         at java.lang.thread.run(unknown source) [na:1.7.0] it may not be critical because this error occured in the autosavingcache. however the line 475 is about the cfmetadata so it may hide bigger issue...  474             cfmetadata cfm = schema.instance.getcfmetadata(key.desc.ksname, key.desc.cfname);  475             cfm.comparator.rowindexentryserializer().serialize(entry, out); regards,  eric<stacktrace> error [compactionexecutor:14] 2014-10-06 23:32:02,098 cassandradaemon.java:166 - exception in thread thread[compactionexecutor:14,1,main] java.lang.nullpointerexception: null         at org.apache.cassandra.service.cacheservice$keycacheserializer.serialize(cacheservice.java:475) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.service.cacheservice$keycacheserializer.serialize(cacheservice.java:463) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.cache.autosavingcache$writer.savecache(autosavingcache.java:225) ~[apache-cassandra-2.1.0.jar:2.1.0]         at org.apache.cassandra.db.compaction.compactionmanager$11.run(compactionmanager.java:1061) ~[apache-cassandra-2.1.0.jar:2.1.0]         at java.util.concurrent.executors$runnableadapter.call(unknown source) ~[na:1.7.0]         at java.util.concurrent.futuretask$sync.innerrun(unknown source) ~[na:1.7.0]         at java.util.concurrent.futuretask.run(unknown source) ~[na:1.7.0]         at java.util.concurrent.threadpoolexecutor.runworker(unknown source) [na:1.7.0]         at java.util.concurrent.threadpoolexecutor$worker.run(unknown source) [na:1.7.0]         at java.lang.thread.run(unknown source) [na:1.7.0] <code>  474             cfmetadata cfm = schema.instance.getcfmetadata(key.desc.ksname, key.desc.cfname);  475             cfm.comparator.rowindexentryserializer().serialize(entry, out); <text> hi, i have this stack trace in the logs of cassandra server (v2.1) it may not be critical because this error occured in the autosavingcache. however the line 475 is about the cfmetadata so it may hide bigger issue... regards,  eric",
        "label": 18
    },
    {
        "text": "dtest failure in disk balance test testdiskbalance disk balance bootstrap test <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1011/testreport/disk_balance_test/testdiskbalance/disk_balance_bootstrap_test failed on cassci build trunk_dtest #1011 this looks likely to be a test issue, perhaps we need to relax the assertion here a bit: values not within 20.00% of the max: (474650, 382235, 513385) (node1) this is flaky with several flaps in the last few weeks.<stacktrace> <code> values not within 20.00% of the max: (474650, 382235, 513385) (node1) http://cassci.datastax.com/job/trunk_dtest/1011/testreport/disk_balance_test/testdiskbalance/disk_balance_bootstrap_test <text> example failure: failed on cassci build trunk_dtest #1011 this looks likely to be a test issue, perhaps we need to relax the assertion here a bit: this is flaky with several flaps in the last few weeks.",
        "label": 321
    },
    {
        "text": "fix flaky org apache cassandra schema schematest testtransksmigration cdc <description> failing for the past 28 builds java.lang.nullpointerexception  at org.apache.cassandra.db.commitlog.commitlogsegmentmanagercdc$cdcsizetracker.shutdown(commitlogsegmentmanagercdc.java:312)  at org.apache.cassandra.db.commitlog.commitlogsegmentmanagercdc.shutdown(commitlogsegmentmanagercdc.java:89)  at org.apache.cassandra.db.commitlog.abstractcommitlogsegmentmanager.stopunsafe(abstractcommitlogsegmentmanager.java:413)  at org.apache.cassandra.db.commitlog.commitlog.stopunsafe(commitlog.java:467)  at org.apache.cassandra.schemaloader.cleanupandleavedirs(schemaloader.java:785)  at org.apache.cassandra.schema.schematest.testtransksmigration(schematest.java:46)<stacktrace> java.lang.nullpointerexception  at org.apache.cassandra.db.commitlog.commitlogsegmentmanagercdc$cdcsizetracker.shutdown(commitlogsegmentmanagercdc.java:312)  at org.apache.cassandra.db.commitlog.commitlogsegmentmanagercdc.shutdown(commitlogsegmentmanagercdc.java:89)  at org.apache.cassandra.db.commitlog.abstractcommitlogsegmentmanager.stopunsafe(abstractcommitlogsegmentmanager.java:413)  at org.apache.cassandra.db.commitlog.commitlog.stopunsafe(commitlog.java:467)  at org.apache.cassandra.schemaloader.cleanupandleavedirs(schemaloader.java:785)  at org.apache.cassandra.schema.schematest.testtransksmigration(schematest.java:46)<code> <text> failing for the past 28 builds ",
        "label": 73
    },
    {
        "text": "sstables from stalled repair sessions become live after a reboot and can resurrect deleted data <description> the sstables streamed in during a repair session don't become active until the session finishes. if something causes the repair session to hang for some reason, those sstables will hang around until the next reboot, and become active then. if you don't reboot for 3 months, this can cause data to resurrect, as gc grace has expired, so tombstones for the data in those sstables may have already been collected.<stacktrace> <code> <text> the sstables streamed in during a repair session don't become active until the session finishes. if something causes the repair session to hang for some reason, those sstables will hang around until the next reboot, and become active then. if you don't reboot for 3 months, this can cause data to resurrect, as gc grace has expired, so tombstones for the data in those sstables may have already been collected.",
        "label": 232
    },
    {
        "text": "rows that cross index block boundaries can cause incomplete reverse reads in some cases  <description> when we're reading 2.1 sstables in reverse, we skip the first row of an index block if it's split across index boundaries. the entire row will be read at the end of the next block. in some cases though, the only thing in this index block is the partial row, so we return an empty iterator. the empty iterator is then interpreted as the end of the row further down the call stack, so we return early without reading the rest of the data. this only affects 3.x during upgrades from 2.1<stacktrace> <code> <text> when we're reading 2.1 sstables in reverse, we skip the first row of an index block if it's split across index boundaries. the entire row will be read at the end of the next block. in some cases though, the only thing in this index block is the partial row, so we return an empty iterator. the empty iterator is then interpreted as the end of the row further down the call stack, so we return early without reading the rest of the data. this only affects 3.x during upgrades from 2.1",
        "label": 79
    },
    {
        "text": "eclipse warnings <description> cassandra-2.2 artifact builds are failing from eclipse-warnings. # 11/11/19 2:58:41 pm utc # eclipse compiler for java(tm) v20150120-1634, 3.10.2, copyright ibm corp 2000, 2013. all rights reserved. incorrect classpath: /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/build/cobertura/classes ---------- 1. error in /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/compactionmanager.java (at line 880) isstablescanner scanner = cleanupstrategy.getscanner(sstable, getratelimiter());                 ^^^^^^^ resource 'scanner' should be managed by try-with-resource ---------- ---------- 2. error in /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/leveledcompactionstrategy.java (at line 257) scanners.add(new leveledscanner(intersecting, range));              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ potential resource leak: '<unassigned closeable value>' may not be closed ---------- ---------- 3. error in /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/src/java/org/apache/cassandra/tools/sstableexport.java (at line 315) isstablescanner scanner = reader.getscanner();                 ^^^^^^^ resource 'scanner' should be managed by try-with-resource ---------- 3 problems (3 errors) ref: https://builds.apache.org/job/cassandra-2.2-artifacts/180/artifact/build/ecj/eclipse_compiler_checks.txt<stacktrace> <code> # 11/11/19 2:58:41 pm utc # eclipse compiler for java(tm) v20150120-1634, 3.10.2, copyright ibm corp 2000, 2013. all rights reserved. incorrect classpath: /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/build/cobertura/classes ---------- 1. error in /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/compactionmanager.java (at line 880) isstablescanner scanner = cleanupstrategy.getscanner(sstable, getratelimiter());                 ^^^^^^^ resource 'scanner' should be managed by try-with-resource ---------- ---------- 2. error in /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/leveledcompactionstrategy.java (at line 257) scanners.add(new leveledscanner(intersecting, range));              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ potential resource leak: '<unassigned closeable value>' may not be closed ---------- ---------- 3. error in /home/jenkins/jenkins-slave/workspace/cassandra-2.2-artifacts/src/java/org/apache/cassandra/tools/sstableexport.java (at line 315) isstablescanner scanner = reader.getscanner();                 ^^^^^^^ resource 'scanner' should be managed by try-with-resource ---------- 3 problems (3 errors) ref: https://builds.apache.org/job/cassandra-2.2-artifacts/180/artifact/build/ecj/eclipse_compiler_checks.txt<text> cassandra-2.2 artifact builds are failing from eclipse-warnings. ",
        "label": 347
    },
    {
        "text": "nodetool info doesn't show the correct dc and rack <description> when running nodetool info cassandra returns unknown_dc and unknown_rack: # nodetool info id                     : b94f9ca0-f886-4111-a471-02f295573f37 gossip active          : true thrift active          : true native transport active: true load                   : 44.97 mb generation no          : 1442913138 uptime (seconds)       : 5386 heap memory (mb)       : 429.07 / 3972.00 off heap memory (mb)   : 0.08 data center            : unknown_dc rack                   : unknown_rack exceptions             : 1 key cache              : entries 642, size 58.16 kb, capacity 100 mb, 5580 hits, 8320 requests, 0.671 recent hit rate, 14400 save period in seconds row cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds counter cache          : entries 0, size 0 bytes, capacity 50 mb, 0 hits, 0 requests, nan recent hit rate, 7200 save period in seconds token                  : (invoke with -t/--tokens to see all 256 tokens) correct dcs and racks are returned by nodetool status and nodetool gossipinfo commands: # nodetool gossipinfo|grep -e 'rack|dc'   dc:poz   rack:rack30   dc:poz   rack:rack30   dc:sjc   rack:rack68   dc:poz   rack:rack30   dc:sjc   rack:rack62   dc:sjc   rack:rack62 # nodetool status|grep datacenter datacenter: sjc datacenter: poz<stacktrace> <code> # nodetool info id                     : b94f9ca0-f886-4111-a471-02f295573f37 gossip active          : true thrift active          : true native transport active: true load                   : 44.97 mb generation no          : 1442913138 uptime (seconds)       : 5386 heap memory (mb)       : 429.07 / 3972.00 off heap memory (mb)   : 0.08 data center            : unknown_dc rack                   : unknown_rack exceptions             : 1 key cache              : entries 642, size 58.16 kb, capacity 100 mb, 5580 hits, 8320 requests, 0.671 recent hit rate, 14400 save period in seconds row cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds counter cache          : entries 0, size 0 bytes, capacity 50 mb, 0 hits, 0 requests, nan recent hit rate, 7200 save period in seconds token                  : (invoke with -t/--tokens to see all 256 tokens) # nodetool gossipinfo|grep -e 'rack|dc'   dc:poz   rack:rack30   dc:poz   rack:rack30   dc:sjc   rack:rack68   dc:poz   rack:rack30   dc:sjc   rack:rack62   dc:sjc   rack:rack62 # nodetool status|grep datacenter datacenter: sjc datacenter: poz <text> when running nodetool info cassandra returns unknown_dc and unknown_rack: correct dcs and racks are returned by nodetool status and nodetool gossipinfo commands:",
        "label": 98
    },
    {
        "text": "replication test testsnitchconfigurationupdate test rf expand gossiping property file snitch multi dc failure <description> replication_test.testsnitchconfigurationupdate.test_rf_expand_gossiping_property_file_snitch_multi_dc has been failing for a while. also fails locally<stacktrace> <code> replication_test.testsnitchconfigurationupdate.test_rf_expand_gossiping_property_file_snitch_multi_dc has been failing for a while. also fails locally<text> ",
        "label": 73
    },
    {
        "text": "parallelise jenkins dtests <description> currently dtests in jenkins take ~10 hours. using the jenkins matrix plugin these jobs can be split into smaller lists of dtests and run in parallel. this is the approach circleci takes. this approach was trialed with the dtest-upgrade jobs (which are not yet part of the branch pipelines, and haven't previously worked at all due to their duration). in addition to the matrix plugin, the priority-sorter and matrix reloaded plugins also needed to be added. the splits will occupy all executors, and multiple builds will lead to a long build queue. more important builds (artifacts and unit tests) need a way to be prioritised in such saturated situations. splits can fail for silly reasons (false-positive), like full /tmp disks, or connectivity issues between the donated agent servers. the matrix reloaded plugin makes it easy to rebuilt just those failed splits.<stacktrace> <code> <text> currently dtests in jenkins take ~10 hours. using the jenkins matrix plugin these jobs can be split into smaller lists of dtests and run in parallel. this is the approach circleci takes. this approach was trialed with the dtest-upgrade jobs (which are not yet part of the branch pipelines, and haven't previously worked at all due to their duration). in addition to the matrix plugin, the priority-sorter and matrix reloaded plugins also needed to be added. the splits will occupy all executors, and multiple builds will lead to a long build queue. more important builds (artifacts and unit tests) need a way to be prioritised in such saturated situations. splits can fail for silly reasons (false-positive), like full /tmp disks, or connectivity issues between the donated agent servers. the matrix reloaded plugin makes it easy to rebuilt just those failed splits.",
        "label": 347
    },
    {
        "text": "rpm package has too many executable files <description> when installing using the rpm files:  in /etc/cassandra/conf, the files should not be execuable, as they are either properties-like files readme-like files, or files to be sourced by shell scripts i'm adding a patch (cassandra-permissions-fix.patch) to the cassandra.spec file which fixes this.<stacktrace> <code> <text> when installing using the rpm files:  in /etc/cassandra/conf, the files should not be execuable, as they are either i'm adding a patch (cassandra-permissions-fix.patch) to the cassandra.spec file which fixes this.",
        "label": 544
    },
    {
        "text": "inserting static column fails with secondary index on clustering key <description> creating a secondary index on a clustering key fails with an exception in case a static column is involved. create table test (k int, t int, s text static, v text, primary key (k, t)); create index ix on test (t); insert into test(k, t, s, v) values (0, 1, 'abc', 'def'); error [sharedpool-worker-2] 2016-01-15 11:42:27,484 storageproxy.java:1336 - failed to apply mutation locally : {} java.lang.arrayindexoutofboundsexception: 0         at org.apache.cassandra.db.abstractclusteringprefix.get(abstractclusteringprefix.java:59) ~[main/:na]         at org.apache.cassandra.index.internal.composites.clusteringcolumnindex.getindexedvalue(clusteringcolumnindex.java:58) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex.getindexedvalue(cassandraindex.java:598) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex.insert(cassandraindex.java:490) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex.access$100(cassandraindex.java:53) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex$1.indexprimarykey(cassandraindex.java:437) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex$1.insertrow(cassandraindex.java:347) ~[main/:na]         at org.apache.cassandra.index.secondaryindexmanager$writetimetransaction.oninserted(secondaryindexmanager.java:795) ~[main/:na]         at org.apache.cassandra.db.partitions.atomicbtreepartition$rowupdater.apply(atomicbtreepartition.java:275) ~[main/:na]         at org.apache.cassandra.db.partitions.atomicbtreepartition.addallwithsizedelta(atomicbtreepartition.java:154) ~[main/:na]         at org.apache.cassandra.db.memtable.put(memtable.java:240) ~[main/:na]         at org.apache.cassandra.db.columnfamilystore.apply(columnfamilystore.java:1145) ~[main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:494) ~[main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:384) ~[main/:na]         at org.apache.cassandra.db.mutation.apply(mutation.java:205) ~[main/:na]         at org.apache.cassandra.service.storageproxy$$lambda$166/492512700.run(unknown source) ~[na:na]         at org.apache.cassandra.service.storageproxy$8.runmaythrow(storageproxy.java:1330) ~[main/:na]         at org.apache.cassandra.service.storageproxy$localmutationrunnable.run(storageproxy.java:2480) [main/:n<stacktrace> error [sharedpool-worker-2] 2016-01-15 11:42:27,484 storageproxy.java:1336 - failed to apply mutation locally : {} java.lang.arrayindexoutofboundsexception: 0         at org.apache.cassandra.db.abstractclusteringprefix.get(abstractclusteringprefix.java:59) ~[main/:na]         at org.apache.cassandra.index.internal.composites.clusteringcolumnindex.getindexedvalue(clusteringcolumnindex.java:58) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex.getindexedvalue(cassandraindex.java:598) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex.insert(cassandraindex.java:490) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex.access$100(cassandraindex.java:53) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex$1.indexprimarykey(cassandraindex.java:437) ~[main/:na]         at org.apache.cassandra.index.internal.cassandraindex$1.insertrow(cassandraindex.java:347) ~[main/:na]         at org.apache.cassandra.index.secondaryindexmanager$writetimetransaction.oninserted(secondaryindexmanager.java:795) ~[main/:na]         at org.apache.cassandra.db.partitions.atomicbtreepartition$rowupdater.apply(atomicbtreepartition.java:275) ~[main/:na]         at org.apache.cassandra.db.partitions.atomicbtreepartition.addallwithsizedelta(atomicbtreepartition.java:154) ~[main/:na]         at org.apache.cassandra.db.memtable.put(memtable.java:240) ~[main/:na]         at org.apache.cassandra.db.columnfamilystore.apply(columnfamilystore.java:1145) ~[main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:494) ~[main/:na]         at org.apache.cassandra.db.keyspace.apply(keyspace.java:384) ~[main/:na]         at org.apache.cassandra.db.mutation.apply(mutation.java:205) ~[main/:na]         at org.apache.cassandra.service.storageproxy$$lambda$166/492512700.run(unknown source) ~[na:na]         at org.apache.cassandra.service.storageproxy$8.runmaythrow(storageproxy.java:1330) ~[main/:na]         at org.apache.cassandra.service.storageproxy$localmutationrunnable.run(storageproxy.java:2480) [main/:n <code> create table test (k int, t int, s text static, v text, primary key (k, t)); create index ix on test (t); insert into test(k, t, s, v) values (0, 1, 'abc', 'def'); <text> creating a secondary index on a clustering key fails with an exception in case a static column is involved.",
        "label": 507
    },
    {
        "text": "hintscolumnfamily compactions hang when using multithreaded compaction <description> running into an issue on a 6 node ring running 1.0.11 where hintscolumnfamily compactions often hang indefinitely when using multithreaded compaction. nothing of note in the logs. in some cases, the compaction hangs before a tmp sstable is even created. i've wiped out every hints sstable and restarted several times. the issue always comes back rather quickly and predictably. the compactions sometimes complete if the hint sstables are very small. disabling multithreaded compaction stops this issue from occurring. compactions of all other cfs seem to work just fine. this ring was upgraded from 1.0.7. i didn't keep any hints from the upgrade. i should note that the ring gets a huge amount of writes, and as a result the hintedhandoff rows get be quite wide. i didn't see any large-row compaction notices when the compaction was hanging (perhaps the bug was triggered by incremental compaction?). after disabling multithreaded compaction, several of the rows that were successfully compacted were over 1gb. here is the output i see from compactionstats where a compaction has hung. the 'bytes compacted' column never changes. pending tasks: 1           compaction type        keyspace   column family bytes compacted     bytes total  progress                compaction          systemhintscolumnfamily          268082       464784758     0.06% the hung thread stack is as follows: (full jstack attached, as well) \"compactionexecutor:37\" daemon prio=10 tid=0x00000000063df800 nid=0x49d9 waiting on condition [0x00007eb8c6ffa000]    java.lang.thread.state: waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x000000050f2e0e58> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)         at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1987)         at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:399)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$deserializer.computenext(parallelcompactioniterable.java:329)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$deserializer.computenext(parallelcompactioniterable.java:281)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.utils.mergeiterator$candidate.advance(mergeiterator.java:147)         at org.apache.cassandra.utils.mergeiterator$manytoone.advance(mergeiterator.java:126)         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:100)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$unwrapper.computenext(parallelcompactioniterable.java:101)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$unwrapper.computenext(parallelcompactioniterable.java:88)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at com.google.common.collect.iterators$7.computenext(iterators.java:614)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:141)         at org.apache.cassandra.db.compaction.compactionmanager$7.call(compactionmanager.java:395)         at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)         at java.util.concurrent.futuretask.run(futuretask.java:138)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662)<stacktrace> 'compactionexecutor:37' daemon prio=10 tid=0x00000000063df800 nid=0x49d9 waiting on condition [0x00007eb8c6ffa000]    java.lang.thread.state: waiting (parking)         at sun.misc.unsafe.park(native method)         - parking to wait for  <0x000000050f2e0e58> (a java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject)         at java.util.concurrent.locks.locksupport.park(locksupport.java:158)         at java.util.concurrent.locks.abstractqueuedsynchronizer$conditionobject.await(abstractqueuedsynchronizer.java:1987)         at java.util.concurrent.linkedblockingqueue.take(linkedblockingqueue.java:399)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$deserializer.computenext(parallelcompactioniterable.java:329)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$deserializer.computenext(parallelcompactioniterable.java:281)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.utils.mergeiterator$candidate.advance(mergeiterator.java:147)         at org.apache.cassandra.utils.mergeiterator$manytoone.advance(mergeiterator.java:126)         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:100)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$unwrapper.computenext(parallelcompactioniterable.java:101)         at org.apache.cassandra.db.compaction.parallelcompactioniterable$unwrapper.computenext(parallelcompactioniterable.java:88)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at com.google.common.collect.iterators$7.computenext(iterators.java:614)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)         at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:141)         at org.apache.cassandra.db.compaction.compactionmanager$7.call(compactionmanager.java:395)         at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)         at java.util.concurrent.futuretask.run(futuretask.java:138)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) <code> pending tasks: 1           compaction type        keyspace   column family bytes compacted     bytes total  progress                compaction          systemhintscolumnfamily          268082       464784758     0.06% <text> running into an issue on a 6 node ring running 1.0.11 where hintscolumnfamily compactions often hang indefinitely when using multithreaded compaction. nothing of note in the logs. in some cases, the compaction hangs before a tmp sstable is even created. i've wiped out every hints sstable and restarted several times. the issue always comes back rather quickly and predictably. the compactions sometimes complete if the hint sstables are very small. disabling multithreaded compaction stops this issue from occurring. compactions of all other cfs seem to work just fine. this ring was upgraded from 1.0.7. i didn't keep any hints from the upgrade. i should note that the ring gets a huge amount of writes, and as a result the hintedhandoff rows get be quite wide. i didn't see any large-row compaction notices when the compaction was hanging (perhaps the bug was triggered by incremental compaction?). after disabling multithreaded compaction, several of the rows that were successfully compacted were over 1gb. here is the output i see from compactionstats where a compaction has hung. the 'bytes compacted' column never changes. the hung thread stack is as follows: (full jstack attached, as well)",
        "label": 98
    },
    {
        "text": "populate io cache on flush option should be configurable for each column family independently <description> i suggest to configure populate_io_cache_on_flush option for each column family. it should be configurable from cassandra-cli and should be stored in system keyspace. that could be useful if you have a few column families inside single keyspace and you need to fit in memory only one of them. patch has been attached. i've been testing it on pseudo-cluster using ccm. so i don't have fully confidence about lack of bugs. please carefully review that code.<stacktrace> <code> <text> i suggest to configure populate_io_cache_on_flush option for each column family. it should be configurable from cassandra-cli and should be stored in system keyspace. that could be useful if you have a few column families inside single keyspace and you need to fit in memory only one of them. patch has been attached. i've been testing it on pseudo-cluster using ccm. so i don't have fully confidence about lack of bugs. please carefully review that code.",
        "label": 28
    },
    {
        "text": "cassandra x validates thrift columns incorrectly and causes invalidrequestexception <description> i just upgrade my local dev machine to cassandra 2.0, which causes one of my automated tests to fail now. with the latest 1.2.x it was working fine. the exception i get on my client (using hector) is: me.prettyprint.hector.api.exceptions.hinvalidrequestexception: invalidrequestexception(why:(expected 8 or 0 byte long (21)) [mds_0][masterdataindex][key2] failed validation) at me.prettyprint.cassandra.service.exceptionstranslatorimpl.translate(exceptionstranslatorimpl.java:52) at me.prettyprint.cassandra.connection.hconnectionmanager.operatewithfailover(hconnectionmanager.java:265) at me.prettyprint.cassandra.model.executingkeyspace.doexecuteoperation(executingkeyspace.java:113) at me.prettyprint.cassandra.model.mutatorimpl.execute(mutatorimpl.java:243) at me.prettyprint.cassandra.service.template.abstractcolumnfamilytemplate.executebatch(abstractcolumnfamilytemplate.java:115) at me.prettyprint.cassandra.service.template.abstractcolumnfamilytemplate.executeifnotbatched(abstractcolumnfamilytemplate.java:163) at me.prettyprint.cassandra.service.template.columnfamilytemplate.update(columnfamilytemplate.java:69) at com.mycompany.spring3utils.dataaccess.cassandra.abstractcassandradao.doupdate(abstractcassandradao.java:482) .... caused by: invalidrequestexception(why:(expected 8 or 0 byte long (21)) [mds_0][masterdataindex][key2] failed validation) at org.apache.cassandra.thrift.cassandra$batch_mutate_result.read(cassandra.java:20833) at org.apache.thrift.tserviceclient.receivebase(tserviceclient.java:78) at org.apache.cassandra.thrift.cassandra$client.recv_batch_mutate(cassandra.java:964) at org.apache.cassandra.thrift.cassandra$client.batch_mutate(cassandra.java:950) at me.prettyprint.cassandra.model.mutatorimpl$3.execute(mutatorimpl.java:246) at me.prettyprint.cassandra.model.mutatorimpl$3.execute(mutatorimpl.java:1) at me.prettyprint.cassandra.service.operation.executeandsetresult(operation.java:104) at me.prettyprint.cassandra.connection.hconnectionmanager.operatewithfailover(hconnectionmanager.java:258) ... 46 more the schema of my column family is: create column family masterdataindex with     compression_options = {sstable_compression:snappycompressor, chunk_length_kb:64} and     comparator = utf8type and     key_validation_class = 'compositetype(utf8type,longtype)' and     default_validation_class = bytestype; from the error message it looks like cassandra is trying to validate the value with the key-validator! (my value in this case it 21 bytes long) i studied the cassandra 2.0 code and found something wrong. it seems in cfmetadata.adddefaultkeyaliases it passes the keyvalidator into columndefinition.partitionkeydef. inside columndefinition the validator is expected to be the value validator! in cfmetadata:     private list<columndefinition> adddefaultkeyaliases(list<columndefinition> pkcols)     {         for (int i = 0; i < pkcols.size(); i++)         {             if (pkcols.get(i) == null)             {                 integer idx = null;                 abstracttype<?> type = keyvalidator;                 if (keyvalidator instanceof compositetype)                 {                     idx = i;                     type = ((compositetype)keyvalidator).types.get(i);                 }                 // for compatibility sake, we call the first alias 'key' rather than 'key1'. this                 // is inconsistent with column alias, but it's probably not worth risking breaking compatibility now.                 bytebuffer name = bytebufferutil.bytes(i == 0 ? default_key_alias : default_key_alias + (i + 1));                 columndefinition newdef = columndefinition.partitionkeydef(name, type, idx); // type is longtype in my case, as it uses keyvalidator !!!                 column_metadata.put(newdef.name, newdef);                 pkcols.set(i, newdef);             }         }         return pkcols;     } ...     public abstracttype<?> getvalidator() // in thriftvalidation this is expected to be the value validator!     {         return validator;     }<stacktrace> me.prettyprint.hector.api.exceptions.hinvalidrequestexception: invalidrequestexception(why:(expected 8 or 0 byte long (21)) [mds_0][masterdataindex][key2] failed validation) at me.prettyprint.cassandra.service.exceptionstranslatorimpl.translate(exceptionstranslatorimpl.java:52) at me.prettyprint.cassandra.connection.hconnectionmanager.operatewithfailover(hconnectionmanager.java:265) at me.prettyprint.cassandra.model.executingkeyspace.doexecuteoperation(executingkeyspace.java:113) at me.prettyprint.cassandra.model.mutatorimpl.execute(mutatorimpl.java:243) at me.prettyprint.cassandra.service.template.abstractcolumnfamilytemplate.executebatch(abstractcolumnfamilytemplate.java:115) at me.prettyprint.cassandra.service.template.abstractcolumnfamilytemplate.executeifnotbatched(abstractcolumnfamilytemplate.java:163) at me.prettyprint.cassandra.service.template.columnfamilytemplate.update(columnfamilytemplate.java:69) at com.mycompany.spring3utils.dataaccess.cassandra.abstractcassandradao.doupdate(abstractcassandradao.java:482) .... caused by: invalidrequestexception(why:(expected 8 or 0 byte long (21)) [mds_0][masterdataindex][key2] failed validation) at org.apache.cassandra.thrift.cassandra$batch_mutate_result.read(cassandra.java:20833) at org.apache.thrift.tserviceclient.receivebase(tserviceclient.java:78) at org.apache.cassandra.thrift.cassandra$client.recv_batch_mutate(cassandra.java:964) at org.apache.cassandra.thrift.cassandra$client.batch_mutate(cassandra.java:950) at me.prettyprint.cassandra.model.mutatorimpl$3.execute(mutatorimpl.java:246) at me.prettyprint.cassandra.model.mutatorimpl$3.execute(mutatorimpl.java:1) at me.prettyprint.cassandra.service.operation.executeandsetresult(operation.java:104) at me.prettyprint.cassandra.connection.hconnectionmanager.operatewithfailover(hconnectionmanager.java:258) ... 46 more <code> create column family masterdataindex with     compression_options = {sstable_compression:snappycompressor, chunk_length_kb:64} and     comparator = utf8type and     key_validation_class = 'compositetype(utf8type,longtype)' and     default_validation_class = bytestype;     private list<columndefinition> adddefaultkeyaliases(list<columndefinition> pkcols)     {         for (int i = 0; i < pkcols.size(); i++)         {             if (pkcols.get(i) == null)             {                 integer idx = null;                 abstracttype<?> type = keyvalidator;                 if (keyvalidator instanceof compositetype)                 {                     idx = i;                     type = ((compositetype)keyvalidator).types.get(i);                 }                 // for compatibility sake, we call the first alias 'key' rather than 'key1'. this                 // is inconsistent with column alias, but it's probably not worth risking breaking compatibility now.                 bytebuffer name = bytebufferutil.bytes(i == 0 ? default_key_alias : default_key_alias + (i + 1));                 columndefinition newdef = columndefinition.partitionkeydef(name, type, idx); // type is longtype in my case, as it uses keyvalidator !!!                 column_metadata.put(newdef.name, newdef);                 pkcols.set(i, newdef);             }         }         return pkcols;     } ...     public abstracttype<?> getvalidator() // in thriftvalidation this is expected to be the value validator!     {         return validator;     } <text> i just upgrade my local dev machine to cassandra 2.0, which causes one of my automated tests to fail now. with the latest 1.2.x it was working fine. the exception i get on my client (using hector) is: the schema of my column family is: from the error message it looks like cassandra is trying to validate the value with the key-validator! (my value in this case it 21 bytes long) i studied the cassandra 2.0 code and found something wrong. it seems in cfmetadata.adddefaultkeyaliases it passes the keyvalidator into columndefinition.partitionkeydef. inside columndefinition the validator is expected to be the value validator! in cfmetadata:",
        "label": 520
    },
    {
        "text": "cassandra causes embeddedcassandraservice to fail <description> starting with 1.2.11 calling statement.prepare() when running embeddedcassandraservice causes an exception due to missing java agent. prior to 1.2.11 this never caused an exception, though there were warnings in the logs about the missing agent. 2013-11-01 11:14:06,508 error native-transport-requests:109 org.apache.cassandra.transport.messages.errormessage - unexpected exception during request  java.lang.illegalstateexception: instrumentation is not set; jamm must be set as -javaagent  at org.github.jamm.memorymeter.measure(memorymeter.java:70)  at org.github.jamm.memorymeter.measuredeep(memorymeter.java:102)  at org.apache.cassandra.cql3.queryprocessor.storepreparedstatement(queryprocessor.java:214)  at org.apache.cassandra.cql3.queryprocessor.prepare(queryprocessor.java:202)  at org.apache.cassandra.transport.messages.preparemessage.execute(preparemessage.java:77)  at org.apache.cassandra.transport.message$dispatcher.messagereceived(message.java:287)  at org.jboss.netty.channel.simplechannelupstreamhandler.handleupstream(simplechannelupstreamhandler.java:70)  at org.jboss.netty.channel.defaultchannelpipeline.sendupstream(defaultchannelpipeline.java:564)  at org.jboss.netty.channel.defaultchannelpipeline$defaultchannelhandlercontext.sendupstream(defaultchannelpipeline.java:791)  at org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun(channelupstreameventrunnable.java:43)  at org.jboss.netty.handler.execution.channeleventrunnable.run(channeleventrunnable.java:67)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:722)<stacktrace> 2013-11-01 11:14:06,508 error native-transport-requests:109 org.apache.cassandra.transport.messages.errormessage - unexpected exception during request  java.lang.illegalstateexception: instrumentation is not set; jamm must be set as -javaagent  at org.github.jamm.memorymeter.measure(memorymeter.java:70)  at org.github.jamm.memorymeter.measuredeep(memorymeter.java:102)  at org.apache.cassandra.cql3.queryprocessor.storepreparedstatement(queryprocessor.java:214)  at org.apache.cassandra.cql3.queryprocessor.prepare(queryprocessor.java:202)  at org.apache.cassandra.transport.messages.preparemessage.execute(preparemessage.java:77)  at org.apache.cassandra.transport.message$dispatcher.messagereceived(message.java:287)  at org.jboss.netty.channel.simplechannelupstreamhandler.handleupstream(simplechannelupstreamhandler.java:70)  at org.jboss.netty.channel.defaultchannelpipeline.sendupstream(defaultchannelpipeline.java:564)  at org.jboss.netty.channel.defaultchannelpipeline$defaultchannelhandlercontext.sendupstream(defaultchannelpipeline.java:791)  at org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun(channelupstreameventrunnable.java:43)  at org.jboss.netty.handler.execution.channeleventrunnable.run(channeleventrunnable.java:67)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:722)<code> <text> starting with 1.2.11 calling statement.prepare() when running embeddedcassandraservice causes an exception due to missing java agent. prior to 1.2.11 this never caused an exception, though there were warnings in the logs about the missing agent. ",
        "label": 362
    },
    {
        "text": "parallelize streaming of different keyspaces for bootstrap and rebuild <description> this is not fast enough when someone is using ssd and may be 10g link. we should try to create multiple connections and send multiple files in parallel. current approach under utilize the link(even 1g). this change will improve the bootstrapping time of a node.<stacktrace> <code> <text> this is not fast enough when someone is using ssd and may be 10g link. we should try to create multiple connections and send multiple files in parallel. current approach under utilize the link(even 1g). this change will improve the bootstrapping time of a node.",
        "label": 120
    },
    {
        "text": "transient replication  add support for correct reads when transient replication is in use <description> digest reads should never be sent to transient replicas. mismatches with results from transient replicas shouldn't trigger read repair. read repair should never attempt to repair a transient replica. reads should always include at least one full replica. they should also prefer transient replicas where possible. range scans must ensure the entire scanned range performs replica selection that satisfies the requirement that every range scanned includes one full replica.<stacktrace> <code> <text> digest reads should never be sent to transient replicas. mismatches with results from transient replicas shouldn't trigger read repair. read repair should never attempt to repair a transient replica. reads should always include at least one full replica. they should also prefer transient replicas where possible. range scans must ensure the entire scanned range performs replica selection that satisfies the requirement that every range scanned includes one full replica.",
        "label": 79
    },
    {
        "text": "assertion error while running compaction <description> while running compaction i run into an error sometimes : nodetool compact error: null -- stacktrace -- java.lang.assertionerror         at org.apache.cassandra.io.compress.compressionmetadata$chunk.<init>(compressionmetadata.java:463)         at org.apache.cassandra.io.compress.compressionmetadata.chunkfor(compressionmetadata.java:228)         at org.apache.cassandra.io.util.compressedsegmentedfile.createmappedsegments(compressedsegmentedfile.java:80)         at org.apache.cassandra.io.util.compressedpoolingsegmentedfile.<init>(compressedpoolingsegmentedfile.java:38)         at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:101)         at org.apache.cassandra.io.util.segmentedfile$builder.complete(segmentedfile.java:198)         at org.apache.cassandra.io.sstable.format.big.bigtablewriter.openearly(bigtablewriter.java:315)         at org.apache.cassandra.io.sstable.sstablerewriter.maybereopenearly(sstablerewriter.java:171)         at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:116)         at org.apache.cassandra.db.compaction.writers.defaultcompactionwriter.append(defaultcompactionwriter.java:64)         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:184)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:74)         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59)         at org.apache.cassandra.db.compaction.compactionmanager$8.runmaythrow(compactionmanager.java:599)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at java.util.concurrent.executors$runnableadapter.call(executors.java:511)         at java.util.concurrent.futuretask.run(futuretask.java:266)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)         at java.lang.thread.run(thread.java:745) why is that happening?  is there anyway to provide more details (e.g. which sstable cannot be compacted)? we are using cassandra 2.2.7<stacktrace> nodetool compact error: null -- stacktrace -- java.lang.assertionerror         at org.apache.cassandra.io.compress.compressionmetadata$chunk.<init>(compressionmetadata.java:463)         at org.apache.cassandra.io.compress.compressionmetadata.chunkfor(compressionmetadata.java:228)         at org.apache.cassandra.io.util.compressedsegmentedfile.createmappedsegments(compressedsegmentedfile.java:80)         at org.apache.cassandra.io.util.compressedpoolingsegmentedfile.<init>(compressedpoolingsegmentedfile.java:38)         at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:101)         at org.apache.cassandra.io.util.segmentedfile$builder.complete(segmentedfile.java:198)         at org.apache.cassandra.io.sstable.format.big.bigtablewriter.openearly(bigtablewriter.java:315)         at org.apache.cassandra.io.sstable.sstablerewriter.maybereopenearly(sstablerewriter.java:171)         at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:116)         at org.apache.cassandra.db.compaction.writers.defaultcompactionwriter.append(defaultcompactionwriter.java:64)         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:184)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:74)         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59)         at org.apache.cassandra.db.compaction.compactionmanager$8.runmaythrow(compactionmanager.java:599)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at java.util.concurrent.executors$runnableadapter.call(executors.java:511)         at java.util.concurrent.futuretask.run(futuretask.java:266)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)         at java.lang.thread.run(thread.java:745) <code> <text> while running compaction i run into an error sometimes : why is that happening?  is there anyway to provide more details (e.g. which sstable cannot be compacted)? we are using cassandra 2.2.7",
        "label": 234
    },
    {
        "text": "dtest failure in repair tests repair test testrepair test dead sync participant <description> example failure: http://cassci.datastax.com/job/trunk_offheap_dtest/427/testreport/repair_tests.repair_test/testrepair/test_dead_sync_participant error message 08 feb 2017 04:31:07 [node1] missing: ['endpoint .* died']: info  [main] 2017-02-08 04:28:51,776 yamlconfigura..... see system.log for remainder stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/tools/decorators.py\", line 48, in wrapped     f(obj)   file \"/home/automaton/cassandra-dtest/repair_tests/repair_test.py\", line 1049, in test_dead_sync_participant     self._test_failure_during_repair(phase='sync', initiator=false,)   file \"/home/automaton/cassandra-dtest/repair_tests/repair_test.py\", line 1139, in _test_failure_during_repair     node1.watch_log_for('endpoint .* died', timeout=60)   file \"/usr/local/lib/python2.7/dist-packages/ccmlib/node.py\", line 471, in watch_log_for     raise timeouterror(time.strftime(\"%d %b %y %h:%m:%s\", time.gmtime()) + \" [\" + self.name + \"] missing: \" + str([e.pattern for e in tofind]) + \":\\n\" + reads[:50] + \".....\\nsee {} for remainder\".format(filename))<stacktrace> <code> error message 08 feb 2017 04:31:07 [node1] missing: ['endpoint .* died']: info  [main] 2017-02-08 04:28:51,776 yamlconfigura..... see system.log for remainder stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/tools/decorators.py', line 48, in wrapped     f(obj)   file '/home/automaton/cassandra-dtest/repair_tests/repair_test.py', line 1049, in test_dead_sync_participant     self._test_failure_during_repair(phase='sync', initiator=false,)   file '/home/automaton/cassandra-dtest/repair_tests/repair_test.py', line 1139, in _test_failure_during_repair     node1.watch_log_for('endpoint .* died', timeout=60)   file '/usr/local/lib/python2.7/dist-packages/ccmlib/node.py', line 471, in watch_log_for     raise timeouterror(time.strftime('%d %b %y %h:%m:%s', time.gmtime()) + ' [' + self.name + '] missing: ' + str([e.pattern for e in tofind]) + ':/n' + reads[:50] + '...../nsee {} for remainder'.format(filename)) http://cassci.datastax.com/job/trunk_offheap_dtest/427/testreport/repair_tests.repair_test/testrepair/test_dead_sync_participant<text> example failure: ",
        "label": 79
    },
    {
        "text": "'alter table' when it includes collections makes cqlsh hang <description> having just installed 1.2.0-beta3 issue the following cql into cqlsh: drop keyspace test; create keyspace test with replication = {           'class': 'simplestrategy',           'replication_factor': '1'         }; use test; create table users (             user_id text primary key,             first_name text,             last_name text,             email_addresses set<text>         ); alter table users add mailing_address_lines list<text>; as soon as you issue the alter table statement cqlsh hangs, and the java process hosting cassandra consumes 100% of a single core's cpu. if the alter table doesn't include a collection, it runs fine.<stacktrace> <code> drop keyspace test; create keyspace test with replication = {           'class': 'simplestrategy',           'replication_factor': '1'         }; use test; create table users (             user_id text primary key,             first_name text,             last_name text,             email_addresses set<text>         ); alter table users add mailing_address_lines list<text>; <text> having just installed 1.2.0-beta3 issue the following cql into cqlsh: as soon as you issue the alter table statement cqlsh hangs, and the java process hosting cassandra consumes 100% of a single core's cpu. if the alter table doesn't include a collection, it runs fine.",
        "label": 520
    },
    {
        "text": "add tracing of cell name index effectiveness <description> exposing row index sizes would give us the information to tune column_index_size correctly (see cassandra-2297).<stacktrace> <code> <text> exposing row index sizes would give us the information to tune column_index_size correctly (see cassandra-2297).",
        "label": 274
    },
    {
        "text": "paxos update corrupted empty row exception <description> cqlsh> create table test.test (test_id text, last_updated timestamp, message_id text, primary key(test_id)); update test.test set last_updated = 1474494363669 where test_id = 'test1' if message_id = null; then nodetool flush on the all 3 nodes. cqlsh> update test.test set last_updated = 1474494363669 where test_id = 'test1' if message_id = null; servererror: <errormessage code=0000 [server error] message=\"java.io.ioerror: java.io.ioexception: corrupt empty row found in unfiltered partition\"> from cassandra log error [sharedpool-worker-1] 2016-09-23 12:09:13,179 message.java:611 - unexpected exception during request; channel = [id: 0x7a22599e, l:/127.0.0.1:9042 - r:/127.0.0.1:58297] java.io.ioerror: java.io.ioexception: corrupt empty row found in unfiltered partition         at org.apache.cassandra.db.rows.unfilteredrowiteratorserializer$1.computenext(unfilteredrowiteratorserializer.java:224) ~[main/:na]         at org.apache.cassandra.db.rows.unfilteredrowiteratorserializer$1.computenext(unfilteredrowiteratorserializer.java:212) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at org.apache.cassandra.db.rows.unfilteredrowiterators.digest(unfilteredrowiterators.java:125) ~[main/:na]         at org.apache.cassandra.db.partitions.unfilteredpartitioniterators.digest(unfilteredpartitioniterators.java:249) ~[main/:na]         at org.apache.cassandra.db.readresponse.makedigest(readresponse.java:87) ~[main/:na]         at org.apache.cassandra.db.readresponse$dataresponse.digest(readresponse.java:192) ~[main/:na]         at org.apache.cassandra.service.digestresolver.resolve(digestresolver.java:80) ~[main/:na]         at org.apache.cassandra.service.readcallback.get(readcallback.java:139) ~[main/:na]         at org.apache.cassandra.service.abstractreadexecutor.get(abstractreadexecutor.java:145) ~[main/:na]         at org.apache.cassandra.service.storageproxy$singlepartitionreadlifecycle.awaitresultsandretryondigestmismatch(storageproxy.java:1714) ~[main/:na]         at org.apache.cassandra.service.storageproxy.fetchrows(storageproxy.java:1663) ~[main/:na]         at org.apache.cassandra.service.storageproxy.readregular(storageproxy.java:1604) ~[main/:na]         at org.apache.cassandra.service.storageproxy.read(storageproxy.java:1523) ~[main/:na]         at org.apache.cassandra.service.storageproxy.readone(storageproxy.java:1497) ~[main/:na]         at org.apache.cassandra.service.storageproxy.readone(storageproxy.java:1491) ~[main/:na]         at org.apache.cassandra.service.storageproxy.cas(storageproxy.java:249) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement.executewithcondition(modificationstatement.java:441) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement.execute(modificationstatement.java:416) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:208) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:239) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:224) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na]<stacktrace> error [sharedpool-worker-1] 2016-09-23 12:09:13,179 message.java:611 - unexpected exception during request; channel = [id: 0x7a22599e, l:/127.0.0.1:9042 - r:/127.0.0.1:58297] java.io.ioerror: java.io.ioexception: corrupt empty row found in unfiltered partition         at org.apache.cassandra.db.rows.unfilteredrowiteratorserializer$1.computenext(unfilteredrowiteratorserializer.java:224) ~[main/:na]         at org.apache.cassandra.db.rows.unfilteredrowiteratorserializer$1.computenext(unfilteredrowiteratorserializer.java:212) ~[main/:na]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[main/:na]         at org.apache.cassandra.db.rows.unfilteredrowiterators.digest(unfilteredrowiterators.java:125) ~[main/:na]         at org.apache.cassandra.db.partitions.unfilteredpartitioniterators.digest(unfilteredpartitioniterators.java:249) ~[main/:na]         at org.apache.cassandra.db.readresponse.makedigest(readresponse.java:87) ~[main/:na]         at org.apache.cassandra.db.readresponse$dataresponse.digest(readresponse.java:192) ~[main/:na]         at org.apache.cassandra.service.digestresolver.resolve(digestresolver.java:80) ~[main/:na]         at org.apache.cassandra.service.readcallback.get(readcallback.java:139) ~[main/:na]         at org.apache.cassandra.service.abstractreadexecutor.get(abstractreadexecutor.java:145) ~[main/:na]         at org.apache.cassandra.service.storageproxy$singlepartitionreadlifecycle.awaitresultsandretryondigestmismatch(storageproxy.java:1714) ~[main/:na]         at org.apache.cassandra.service.storageproxy.fetchrows(storageproxy.java:1663) ~[main/:na]         at org.apache.cassandra.service.storageproxy.readregular(storageproxy.java:1604) ~[main/:na]         at org.apache.cassandra.service.storageproxy.read(storageproxy.java:1523) ~[main/:na]         at org.apache.cassandra.service.storageproxy.readone(storageproxy.java:1497) ~[main/:na]         at org.apache.cassandra.service.storageproxy.readone(storageproxy.java:1491) ~[main/:na]         at org.apache.cassandra.service.storageproxy.cas(storageproxy.java:249) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement.executewithcondition(modificationstatement.java:441) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement.execute(modificationstatement.java:416) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:208) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:239) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:224) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na] <code> cqlsh> create table test.test (test_id text, last_updated timestamp, message_id text, primary key(test_id)); update test.test set last_updated = 1474494363669 where test_id = 'test1' if message_id = null; cqlsh> update test.test set last_updated = 1474494363669 where test_id = 'test1' if message_id = null; servererror: <errormessage code=0000 [server error] message='java.io.ioerror: java.io.ioexception: corrupt empty row found in unfiltered partition'> <text> then nodetool flush on the all 3 nodes. from cassandra log",
        "label": 25
    },
    {
        "text": "o a c db recoverymanagertest testnothingtorecover unit test flaps in <description> example:  http://cassci.datastax.com/job/cassandra-2.0_utest/326/ (this test appears to pass consistently in 1.2 and 2.1 with a quick glance - will test out the other branches more thoroughly and bisect) regression:  org.apache.cassandra.db.recoverymanagertest.testnothingtorecover error message: java.io.filenotfoundexception: /var/lib/jenkins/jobs/cassandra-2.0_test/workspace/build/test/cassandra/commitlog/commitlog-3-1398354429966.log (no such file or directory) stack trace: java.lang.runtimeexception: java.io.filenotfoundexception: /var/lib/jenkins/jobs/cassandra-2.0_test/workspace/build/test/cassandra/commitlog/commitlog-3-1398354429966.log (no such file or directory) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:102) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:90) at org.apache.cassandra.db.commitlog.commitlogreplayer.recover(commitlogreplayer.java:186) at org.apache.cassandra.db.commitlog.commitlogreplayer.recover(commitlogreplayer.java:95) at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:151) at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:131) at org.apache.cassandra.db.recoverymanagertest.testnothingtorecover(recoverymanagertest.java:42) caused by: java.io.filenotfoundexception: /var/lib/jenkins/jobs/cassandra-2.0_test/workspace/build/test/cassandra/commitlog/commitlog-3-1398354429966.log (no such file or directory) at java.io.randomaccessfile.open(native method) at java.io.randomaccessfile.<init>(randomaccessfile.java:241) at org.apache.cassandra.io.util.randomaccessreader.<init>(randomaccessreader.java:58) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:98)<stacktrace> regression:  org.apache.cassandra.db.recoverymanagertest.testnothingtorecover error message: java.io.filenotfoundexception: /var/lib/jenkins/jobs/cassandra-2.0_test/workspace/build/test/cassandra/commitlog/commitlog-3-1398354429966.log (no such file or directory) stack trace: java.lang.runtimeexception: java.io.filenotfoundexception: /var/lib/jenkins/jobs/cassandra-2.0_test/workspace/build/test/cassandra/commitlog/commitlog-3-1398354429966.log (no such file or directory) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:102) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:90) at org.apache.cassandra.db.commitlog.commitlogreplayer.recover(commitlogreplayer.java:186) at org.apache.cassandra.db.commitlog.commitlogreplayer.recover(commitlogreplayer.java:95) at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:151) at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:131) at org.apache.cassandra.db.recoverymanagertest.testnothingtorecover(recoverymanagertest.java:42) caused by: java.io.filenotfoundexception: /var/lib/jenkins/jobs/cassandra-2.0_test/workspace/build/test/cassandra/commitlog/commitlog-3-1398354429966.log (no such file or directory) at java.io.randomaccessfile.open(native method) at java.io.randomaccessfile.<init>(randomaccessfile.java:241) at org.apache.cassandra.io.util.randomaccessreader.<init>(randomaccessreader.java:58) at org.apache.cassandra.io.util.randomaccessreader.open(randomaccessreader.java:98) <code> example:  http://cassci.datastax.com/job/cassandra-2.0_utest/326/ <text> (this test appears to pass consistently in 1.2 and 2.1 with a quick glance - will test out the other branches more thoroughly and bisect)",
        "label": 67
    },
    {
        "text": "assertionerror on compaction after upgrade     <description> after upgrading a cassandra cluster from 2.1.9 to 3.7, one column family (using sizetieredcompaction) repeatedly and continuously failed compaction (and thus also repair) across the cluster, with all nodes producing the following errors in the logs: 016-07-14t09:29:47.96855 |srv=cassandra|error: exception in thread thread[compactionexecutor:3,1,main] 2016-07-14t09:29:47.96858 |srv=cassandra|java.lang.assertionerror: null 2016-07-14t09:29:47.96859 |srv=cassandra|   at org.apache.cassandra.db.unfiltereddeserializer$oldformatdeserializer$tombstonetracker.opennew(unfiltereddeserializer.java:650) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96860 |srv=cassandra|   at org.apache.cassandra.db.unfiltereddeserializer$oldformatdeserializer$unfilterediterator.hasnext(unfiltereddeserializer.java:423) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96860 |srv=cassandra|   at org.apache.cassandra.db.unfiltereddeserializer$oldformatdeserializer.hasnext(unfiltereddeserializer.java:298) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96860 |srv=cassandra|   at org.apache.cassandra.io.sstable.sstablesimpleiterator$oldformatiterator.readstaticrow(sstablesimpleiterator.java:133) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96861 |srv=cassandra|   at org.apache.cassandra.io.sstable.sstableidentityiterator.<init>(sstableidentityiterator.java:57) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96861 |srv=cassandra|   at org.apache.cassandra.io.sstable.format.big.bigtablescanner$keyscanningiterator$1.initializeiterator(bigtablescanner.java:334) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96862 |srv=cassandra|   at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.maybeinit(lazilyinitializedunfilteredrowiterator.java:48) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96862 |srv=cassandra|   at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.isreverseorder(lazilyinitializedunfilteredrowiterator.java:70) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96863 |srv=cassandra|   at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$1.reduce(unfilteredpartitioniterators.java:109) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96863 |srv=cassandra|   at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$1.reduce(unfilteredpartitioniterators.java:100) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96864 |srv=cassandra|   at org.apache.cassandra.utils.mergeiterator$candidate.consume(mergeiterator.java:408) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96864 |srv=cassandra|   at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:203) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96865 |srv=cassandra|   at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:156) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96865 |srv=cassandra|   at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96866 |srv=cassandra|   at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$2.hasnext(unfilteredpartitioniterators.java:150) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96866 |srv=cassandra|   at org.apache.cassandra.db.transform.basepartitions.hasnext(basepartitions.java:72) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96867 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactioniterator.hasnext(compactioniterator.java:226) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96867 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:182) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96867 |srv=cassandra|   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96868 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:82) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96868 |srv=cassandra|   at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:60) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96869 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactioncandidate.run(compactionmanager.java:264) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96870 |srv=cassandra|   at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_91] 2016-07-14t09:29:47.96870 |srv=cassandra|   at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_91] 2016-07-14t09:29:47.96870 |srv=cassandra|   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_91] 2016-07-14t09:29:47.96871 |srv=cassandra|   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_91] 2016-07-14t09:29:47.96871 |srv=cassandra| at java.lang.thread.run(thread.java:745) [na:1.8.0_91] only 1 out of 14 tables was affected by this problem after the upgrade. the schema of the affected table looks like this:         create columnfamily if not exists cassandra12203             ( field1   uuid             , field2  ascii             , field3    int             , field4   text             , primary key (field1, field2, field3)             );<stacktrace> 016-07-14t09:29:47.96855 |srv=cassandra|error: exception in thread thread[compactionexecutor:3,1,main] 2016-07-14t09:29:47.96858 |srv=cassandra|java.lang.assertionerror: null 2016-07-14t09:29:47.96859 |srv=cassandra|   at org.apache.cassandra.db.unfiltereddeserializer$oldformatdeserializer$tombstonetracker.opennew(unfiltereddeserializer.java:650) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96860 |srv=cassandra|   at org.apache.cassandra.db.unfiltereddeserializer$oldformatdeserializer$unfilterediterator.hasnext(unfiltereddeserializer.java:423) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96860 |srv=cassandra|   at org.apache.cassandra.db.unfiltereddeserializer$oldformatdeserializer.hasnext(unfiltereddeserializer.java:298) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96860 |srv=cassandra|   at org.apache.cassandra.io.sstable.sstablesimpleiterator$oldformatiterator.readstaticrow(sstablesimpleiterator.java:133) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96861 |srv=cassandra|   at org.apache.cassandra.io.sstable.sstableidentityiterator.<init>(sstableidentityiterator.java:57) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96861 |srv=cassandra|   at org.apache.cassandra.io.sstable.format.big.bigtablescanner$keyscanningiterator$1.initializeiterator(bigtablescanner.java:334) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96862 |srv=cassandra|   at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.maybeinit(lazilyinitializedunfilteredrowiterator.java:48) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96862 |srv=cassandra|   at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.isreverseorder(lazilyinitializedunfilteredrowiterator.java:70) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96863 |srv=cassandra|   at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$1.reduce(unfilteredpartitioniterators.java:109) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96863 |srv=cassandra|   at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$1.reduce(unfilteredpartitioniterators.java:100) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96864 |srv=cassandra|   at org.apache.cassandra.utils.mergeiterator$candidate.consume(mergeiterator.java:408) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96864 |srv=cassandra|   at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:203) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96865 |srv=cassandra|   at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:156) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96865 |srv=cassandra|   at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96866 |srv=cassandra|   at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$2.hasnext(unfilteredpartitioniterators.java:150) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96866 |srv=cassandra|   at org.apache.cassandra.db.transform.basepartitions.hasnext(basepartitions.java:72) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96867 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactioniterator.hasnext(compactioniterator.java:226) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96867 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:182) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96867 |srv=cassandra|   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96868 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:82) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96868 |srv=cassandra|   at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:60) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96869 |srv=cassandra|   at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactioncandidate.run(compactionmanager.java:264) ~[apache-cassandra-3.7.jar:3.7] 2016-07-14t09:29:47.96870 |srv=cassandra|   at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_91] 2016-07-14t09:29:47.96870 |srv=cassandra|   at java.util.concurrent.futuretask.run(futuretask.java:266) ~[na:1.8.0_91] 2016-07-14t09:29:47.96870 |srv=cassandra|   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_91] 2016-07-14t09:29:47.96871 |srv=cassandra|   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_91] 2016-07-14t09:29:47.96871 |srv=cassandra| at java.lang.thread.run(thread.java:745) [na:1.8.0_91] <code>         create columnfamily if not exists cassandra12203             ( field1   uuid             , field2  ascii             , field3    int             , field4   text             , primary key (field1, field2, field3)             ); <text> after upgrading a cassandra cluster from 2.1.9 to 3.7, one column family (using sizetieredcompaction) repeatedly and continuously failed compaction (and thus also repair) across the cluster, with all nodes producing the following errors in the logs: only 1 out of 14 tables was affected by this problem after the upgrade. the schema of the affected table looks like this:",
        "label": 577
    },
    {
        "text": "authentication provider in cassandra itself <description> i've been working on an implementation for both iauthority2 and iauthenticator that uses cassandra itself to store the necessary credentials. i'm planning on open sourcing this shortly. is there any interest in this? it tries to provide reasonable security, for example using pbkdf2 to store passwords with a configurable configuration cycle and managing all the rights available in iauthority2. my main use goal isn't security / confidentiality of the data, but more that i don't want multiple consumers of the cluster to accidentally screw stuff up. only certain users can write data, others can read it out again and further process it. i'm planning on releasing this soon under an open source license (probably the same as cassandra itself). would there be interest in incorporating it as a new reference implementation instead of the properties file implementation perhaps? or can i better maintain it separately? i would love if people from the community would want to review it, since i have been dabbling in the cassandra source code only for a short while now. during the development of this i've encountered a few bumps and i wonder whether they could be addressed or not. = moment when validateconfiguration() runs = is there a deliberate reason that validateconfiguration() is executed before all information about keyspaces, column families etc. is available? in the current form i therefore can't validate whether column families etc. are available for authentication since they aren't loaded yet. i've wanted to use this to make relatively easy bootstrapping possible. my approach here would be to only enable authentication if the needed keyspace is available. this allows for configuring the cluster, then import the necessary authentication data for an admin user to bootstrap further and then restart every node in the cluster. basically the questions here are, can the moment when validateconfiguration() runs for an authentication provider be changed? is this approach to bootstrapping reasonable or do people have better ideas? = abstractreplicationstrategy has package visible constructor = i've added a strategy that basically says that data should be available on all nodes. the amount of data use for authentication is very limited. replicating it to every node is there for not very problematic and allows for every node to have all data locally available for verifying requests. i wanted to put this strategy into it's own package inside the authentication module, but since the constructor of abstractreplicationstrategy has no visibility explicitly marked, it's only available inside the same package. i'm not sure whether implementing a strategy to replicate data to all nodes is a sane idea and whether my implementation of this strategy is correct. what do you people think of this? would people want to review the implementation?<stacktrace> <code> = moment when validateconfiguration() runs = <text> i've been working on an implementation for both iauthority2 and iauthenticator that uses cassandra itself to store the necessary credentials. i'm planning on open sourcing this shortly. is there any interest in this? it tries to provide reasonable security, for example using pbkdf2 to store passwords with a configurable configuration cycle and managing all the rights available in iauthority2. my main use goal isn't security / confidentiality of the data, but more that i don't want multiple consumers of the cluster to accidentally screw stuff up. only certain users can write data, others can read it out again and further process it. i'm planning on releasing this soon under an open source license (probably the same as cassandra itself). would there be interest in incorporating it as a new reference implementation instead of the properties file implementation perhaps? or can i better maintain it separately? i would love if people from the community would want to review it, since i have been dabbling in the cassandra source code only for a short while now. during the development of this i've encountered a few bumps and i wonder whether they could be addressed or not. is there a deliberate reason that validateconfiguration() is executed before all information about keyspaces, column families etc. is available? in the current form i therefore can't validate whether column families etc. are available for authentication since they aren't loaded yet. i've wanted to use this to make relatively easy bootstrapping possible. my approach here would be to only enable authentication if the needed keyspace is available. this allows for configuring the cluster, then import the necessary authentication data for an admin user to bootstrap further and then restart every node in the cluster. basically the questions here are, can the moment when validateconfiguration() runs for an authentication provider be changed? is this approach to bootstrapping reasonable or do people have better ideas? = abstractreplicationstrategy has package visible constructor = i've added a strategy that basically says that data should be available on all nodes. the amount of data use for authentication is very limited. replicating it to every node is there for not very problematic and allows for every node to have all data locally available for verifying requests. i wanted to put this strategy into it's own package inside the authentication module, but since the constructor of abstractreplicationstrategy has no visibility explicitly marked, it's only available inside the same package. i'm not sure whether implementing a strategy to replicate data to all nodes is a sane idea and whether my implementation of this strategy is correct. what do you people think of this? would people want to review the implementation?",
        "label": 18
    },
    {
        "text": "add   shortcut syntax <description> for collections and counters, the current syntax to add/remove elements is: update foo set mycollection = mycollection + ...; which is fine, though it's already tad annoying to have to repeat mycollection. but moving forward, with tickets cassandra-7826, we'll start being able to add to nested collections and we'll end up with queries like: update foo set mycollection['someelement']['otherelemnt'] = mycollection['someelement']['otherelemnt'] + ...; where the repetition is starting to be really annoying and it makes the query less readable. it's trivial however to add a +=/-= shortcut syntax which would read instead: update foo set mycollection['someelement']['otherelemnt'] += ...; as this would just be syntactic sugar, it only requires a few minor addition to the grammar and this would be completely optional: if some users prefer the verbose syntax, that's fine. also note that while this will be even more useful after things like cassandra-7826, it's already a nice to have today so it's not dependent on that latter ticket in any way.<stacktrace> <code> update foo set mycollection = mycollection + ...; update foo set mycollection['someelement']['otherelemnt'] = mycollection['someelement']['otherelemnt'] + ...; update foo set mycollection['someelement']['otherelemnt'] += ...; <text> for collections and counters, the current syntax to add/remove elements is: which is fine, though it's already tad annoying to have to repeat mycollection. but moving forward, with tickets cassandra-7826, we'll start being able to add to nested collections and we'll end up with queries like: where the repetition is starting to be really annoying and it makes the query less readable. it's trivial however to add a +=/-= shortcut syntax which would read instead: as this would just be syntactic sugar, it only requires a few minor addition to the grammar and this would be completely optional: if some users prefer the verbose syntax, that's fine. also note that while this will be even more useful after things like cassandra-7826, it's already a nice to have today so it's not dependent on that latter ticket in any way.",
        "label": 25
    },
    {
        "text": "custom cql protocol transport <description> a custom wire protocol would give us the flexibility to optimize for our specific use-cases, and eliminate a troublesome dependency (i'm referring to thrift, but none of the others would be significantly better). additionally, rpc is bad fit here, and we'd do better to move in the direction of something that natively supports streaming. i don't think this is as daunting as it might seem initially. utilizing an existing server framework like netty, combined with some copy-and-paste of bits from other floss projects would probably get us 80% of the way there.<stacktrace> <code> <text> a custom wire protocol would give us the flexibility to optimize for our specific use-cases, and eliminate a troublesome dependency (i'm referring to thrift, but none of the others would be significantly better). additionally, rpc is bad fit here, and we'd do better to move in the direction of something that natively supports streaming. i don't think this is as daunting as it might seem initially. utilizing an existing server framework like netty, combined with some copy-and-paste of bits from other floss projects would probably get us 80% of the way there.",
        "label": 520
    },
    {
        "text": "mv add write survey node after mv test is failing on trunk <description> http://cassci.datastax.com/job/trunk_dtest/893/testreport/materialized_views_test/testmaterializedviews/add_write_survey_node_after_mv_test/ ====================================================================== error: add_write_survey_node_after_mv_test (materialized_views_test.testmaterializedviews) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/aboudreault/git/cstar/cassandra-dtest/dtest.py\", line 558, in teardown     raise assertionerror('unexpected error in %s node log: %s' % (node.name, errors)) assertionerror: unexpected error in node4 node log: ['error [main] 2016-01-06 17:03:41,614 migrationmanager.java:164 - migration task failed to complete\\nerror [main] 2016-01-06 17:03:42,614 migrationmanager.java:164 - migration task failed to complete'] -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-14gpww dtest: debug: removing ccm cluster test at: /tmp/dtest-14gpww dtest: debug: clearing ssl stores from [/tmp/dtest-14gpww] directory --------------------- >> end captured logging << --------------------- ---------------------------------------------------------------------- ran 1 test in 85.369s failed (errors=1)<stacktrace> <code> ====================================================================== error: add_write_survey_node_after_mv_test (materialized_views_test.testmaterializedviews) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/aboudreault/git/cstar/cassandra-dtest/dtest.py', line 558, in teardown     raise assertionerror('unexpected error in %s node log: %s' % (node.name, errors)) assertionerror: unexpected error in node4 node log: ['error [main] 2016-01-06 17:03:41,614 migrationmanager.java:164 - migration task failed to complete/nerror [main] 2016-01-06 17:03:42,614 migrationmanager.java:164 - migration task failed to complete'] -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-14gpww dtest: debug: removing ccm cluster test at: /tmp/dtest-14gpww dtest: debug: clearing ssl stores from [/tmp/dtest-14gpww] directory --------------------- >> end captured logging << --------------------- ---------------------------------------------------------------------- ran 1 test in 85.369s failed (errors=1) http://cassci.datastax.com/job/trunk_dtest/893/testreport/materialized_views_test/testmaterializedviews/add_write_survey_node_after_mv_test/<text> ",
        "label": 261
    },
    {
        "text": "rename gmfd to gossip stage <description> attached is a simple (one line) patch to rename \"gmfd\" to \"gossip_stage\", because that's what it is and \"gmfd\" is cryptic. clicking \"patch submitted\" this time, too. =rob<stacktrace> <code> =rob<text> attached is a simple (one line) patch to rename 'gmfd' to 'gossip_stage', because that's what it is and 'gmfd' is cryptic. clicking 'patch submitted' this time, too. ",
        "label": 451
    },
    {
        "text": "create inprocesscassandraserver for unit tests <description> i've been personally using an in-process cassandra server and found it useful so i was ask to make it available publicly, here goes.  when unit-testing with cassandra i create an in process cassandra instance. that's nice since it lets you isolate tests, and you don't have to worry about a server being available for your unit tests.  the code goes more or less like this (i'll attach a patch when the work is done after cleanup etc) /** an in-memory cassandra storage service that listens to the thrift interface. useful for unit testing,  * @author ran tavory (rantav@gmail.com)  *  */  public class inprocesscassandraserver implements runnable { private static final logger log = loggerfactory.getlogger(inprocesscassandraserver.class);  cassandradaemon cassandradaemon;  public void init() {  try { prepare(); } catch (ioexception e) { log.error(\"cannot prepare cassandra.\", e); } try { cassandradaemon = new cassandradaemon(); cassandradaemon.init(null); } catch (ttransportexception e) { log.error(\"ttransportexception\", e); } catch (ioexception e) { log.error(\"ioexception\", e); } }  @override  public void run() { cassandradaemon.start(); } public void stop() { cassandradaemon.stop(); rmdir(\"tmp\"); } /** creates all files and directories needed @throws ioexception  */  private void prepare() throws ioexception {  // delete tmp dir first  rmdir(\"tmp\");  // make a tmp dir and copy storag-conf.xml and log4j.properties to it  copy(\"/cassandra/storage-conf.xml\", \"tmp\");  copy(\"/cassandra/log4j.properties\", \"tmp\");  system.setproperty(\"storage-config\", \"tmp\"); // make cassandra directories.  for (string s: databasedescriptor.getalldatafilelocations()) { mkdir(s); } mkdir(databasedescriptor.getbootstrapfilelocation());  mkdir(databasedescriptor.getlogfilelocation());  }  /** copies a resource from within the jar to a directory.  * @param resourcename @param directory @throws ioexception  */  private void copy(string resource, string directory) throws ioexception unknown macro: { mkdir(directory); inputstream is = getclass().getresourceasstream(resource); string filename = resource.substring(resource.lastindexof(\"/\") + 1); file file = new file(directory + system.getproperty(\"file.separator\") + filename); outputstream out = new fileoutputstream(file); byte buf[] = new byte[1024]; int len; while ((len = is.read(buf)) > 0) { out.write(buf, 0, len); } out.close(); is.close(); }  /** creates a directory @param dir @throws ioexception  */  private void mkdir(string dir) throws ioexception { fileutils.createdirectory(dir); } /** removes a directory from file system @param dir  */  private void rmdir(string dir) { fileutils.deletedir(new file(dir)); } } and test code using this class looks like this: public class xxxtest {  private static inprocesscassandraserver cassandra;  @beforeclass  public static void setup() throws ttransportexception, ioexception, interruptedexception { cassandra = new inprocesscassandraserver(); cassandra.init(); thread t = new thread(cassandra); t.setdaemon(true); t.start(); } @afterclass  public static void shutdown() { cassandra.stop(); } public void testx() { // connect to cassandra at localhost:9160 } } note: i've set fix version to 6.0, hope it's correct<stacktrace> <code> i've been personally using an in-process cassandra server and found it useful so i was ask to make it available publicly, here goes.  when unit-testing with cassandra i create an in process cassandra instance. that's nice since it lets you isolate tests, and you don't have to worry about a server being available for your unit tests.  the code goes more or less like this (i'll attach a patch when the work is done after cleanup etc) /** private static final logger log = loggerfactory.getlogger(inprocesscassandraserver.class); cassandradaemon cassandradaemon; public void init() {  try catch (ioexception e) catch (ttransportexception e) catch (ioexception e) } @override  public void run() public void stop() /** // make cassandra directories.  for (string s: databasedescriptor.getalldatafilelocations()) mkdir(databasedescriptor.getbootstrapfilelocation());  mkdir(databasedescriptor.getlogfilelocation());  } /** /** /** } public class xxxtest { private static inprocesscassandraserver cassandra; @afterclass  public static void shutdown() public void testx() } <text> try and test code using this class looks like this: @beforeclass  public static void setup() throws ttransportexception, ioexception, interruptedexception note: i've set fix version to 6.0, hope it's correct",
        "label": 440
    },
    {
        "text": "time to add first class thrift methods for the string meta queries  <description> i'd like to add these with a c_ (or system_? or meta_?) prefix, to make clear that clients who just want to insert and update stuff don't need to care about it. currently get_string_property provides  \"cluster name\"  \"config file\" [i think we should get rid of this one]  \"token map\"  \"version\" get_string_list_property provides  \"keyspaces\"<stacktrace> <code> <text> i'd like to add these with a c_ (or system_? or meta_?) prefix, to make clear that clients who just want to insert and update stuff don't need to care about it. currently get_string_property provides  'cluster name'  'config file' [i think we should get rid of this one]  'token map'  'version' get_string_list_property provides  'keyspaces'",
        "label": 274
    },
    {
        "text": "improve javadoc for keysindex <description> i found the wording of the javadoc a bit hard to read. my attempt at rewording it and added a link to inverted index. hopefully that makes it easier to read and gives potential contributors a better explanation. https://github.com/woolfel/apache-cassandra/commit/53902032f2f7a0a3e015af19a5849c8bf36664e2<stacktrace> <code> https://github.com/woolfel/apache-cassandra/commit/53902032f2f7a0a3e015af19a5849c8bf36664e2<text> i found the wording of the javadoc a bit hard to read. my attempt at rewording it and added a link to inverted index. hopefully that makes it easier to read and gives potential contributors a better explanation. ",
        "label": 422
    },
    {
        "text": "deleted row still can be selected out <description> first create  keyspace space1 with replication = {'class': 'simplestrategy', 'replication_factor': 3}; create  table space1.table3(a int, b int, c text,primary key(a,b)); create  keyspace space2 with replication = {'class': 'simplestrategy', 'replication_factor': 3}; second create  table space2.table1(a int, b int, c int, primary key(a,b)); create  table space2.table2(a int, b int, c int, primary key(a,b)); insert into space1.table3(a,b,c) values(1,1,'1'); drop table space2.table1; delete from space1.table3 where a=1 and b=1; drop table space2.table2; select * from space1.table3 where a=1 and b=1; you will find that the row (a=1 and b=1) in space1.table3 is not deleted.<stacktrace> <code> create  keyspace space1 with replication = {'class': 'simplestrategy', 'replication_factor': 3}; create  table space1.table3(a int, b int, c text,primary key(a,b)); create  keyspace space2 with replication = {'class': 'simplestrategy', 'replication_factor': 3}; create  table space2.table1(a int, b int, c int, primary key(a,b)); create  table space2.table2(a int, b int, c int, primary key(a,b)); insert into space1.table3(a,b,c) values(1,1,'1'); drop table space2.table1; delete from space1.table3 where a=1 and b=1; drop table space2.table2; select * from space1.table3 where a=1 and b=1; <text> first second you will find that the row (a=1 and b=1) in space1.table3 is not deleted.",
        "label": 520
    },
    {
        "text": "stackoverflowerror in leveledcompactionstrategy leveledscanner computenext <description> while running nodetool repair, the following was logged in system.log: error [validationexecutor:2] 2012-08-30 10:58:19,490 abstractcassandradaemon.java (line 134) exception in thread thread[validationexecutor:2,1,main]  java.lang.stackoverflowerror  at sun.nio.cs.utf_8.updatepositions(utf_8.java:76)  at sun.nio.cs.utf_8$encoder.encodearrayloop(utf_8.java:411)  at sun.nio.cs.utf_8$encoder.encodeloop(utf_8.java:466)  at java.nio.charset.charsetencoder.encode(charsetencoder.java:561)  at java.lang.stringcoding$stringencoder.encode(stringcoding.java:258)  at java.lang.stringcoding.encode(stringcoding.java:290)  at java.lang.string.getbytes(string.java:954)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:233)  at org.apache.cassandra.io.util.randomaccessreader.<init>(randomaccessreader.java:67)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.<init>(compressedrandomaccessreader.java:64)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.open(compressedrandomaccessreader.java:46)  at org.apache.cassandra.io.sstable.sstablereader.opendatareader(sstablereader.java:1007)  at org.apache.cassandra.io.sstable.sstablescanner.<init>(sstablescanner.java:56)  at org.apache.cassandra.io.sstable.sstableboundedscanner.<init>(sstableboundedscanner.java:41)  at org.apache.cassandra.io.sstable.sstablereader.getdirectscanner(sstablereader.java:869)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:247)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  . (about 900 lines deleted)  .  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:202)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at org.apache.cassandra.utils.mergeiterator$candidate.advance(mergeiterator.java:147)  at org.apache.cassandra.utils.mergeiterator$manytoone.<init>(mergeiterator.java:90)  at org.apache.cassandra.utils.mergeiterator.get(mergeiterator.java:47)  at org.apache.cassandra.db.compaction.compactioniterable.iterator(compactioniterable.java:60)  at org.apache.cassandra.db.compaction.compactionmanager.dovalidationcompaction(compactionmanager.java:703)  at org.apache.cassandra.db.compaction.compactionmanager.access$600(compactionmanager.java:69)  at org.apache.cassandra.db.compaction.compactionmanager$8.call(compactionmanager.java:442)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334)  at java.util.concurrent.futuretask.run(futuretask.java:166)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:636)<stacktrace> error [validationexecutor:2] 2012-08-30 10:58:19,490 abstractcassandradaemon.java (line 134) exception in thread thread[validationexecutor:2,1,main]  java.lang.stackoverflowerror  at sun.nio.cs.utf_8.updatepositions(utf_8.java:76)  at sun.nio.cs.utf_8$encoder.encodearrayloop(utf_8.java:411)  at sun.nio.cs.utf_8$encoder.encodeloop(utf_8.java:466)  at java.nio.charset.charsetencoder.encode(charsetencoder.java:561)  at java.lang.stringcoding$stringencoder.encode(stringcoding.java:258)  at java.lang.stringcoding.encode(stringcoding.java:290)  at java.lang.string.getbytes(string.java:954)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:233)  at org.apache.cassandra.io.util.randomaccessreader.<init>(randomaccessreader.java:67)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.<init>(compressedrandomaccessreader.java:64)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.open(compressedrandomaccessreader.java:46)  at org.apache.cassandra.io.sstable.sstablereader.opendatareader(sstablereader.java:1007)  at org.apache.cassandra.io.sstable.sstablescanner.<init>(sstablescanner.java:56)  at org.apache.cassandra.io.sstable.sstableboundedscanner.<init>(sstableboundedscanner.java:41)  at org.apache.cassandra.io.sstable.sstablereader.getdirectscanner(sstablereader.java:869)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:247)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  . at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:240)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:248)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy$leveledscanner.computenext(leveledcompactionstrategy.java:202)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at org.apache.cassandra.utils.mergeiterator$candidate.advance(mergeiterator.java:147)  at org.apache.cassandra.utils.mergeiterator$manytoone.<init>(mergeiterator.java:90)  at org.apache.cassandra.utils.mergeiterator.get(mergeiterator.java:47)  at org.apache.cassandra.db.compaction.compactioniterable.iterator(compactioniterable.java:60)  at org.apache.cassandra.db.compaction.compactionmanager.dovalidationcompaction(compactionmanager.java:703)  at org.apache.cassandra.db.compaction.compactionmanager.access$600(compactionmanager.java:69)  at org.apache.cassandra.db.compaction.compactionmanager$8.call(compactionmanager.java:442)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334)  at java.util.concurrent.futuretask.run(futuretask.java:166)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:636)<code> <text> while running nodetool repair, the following was logged in system.log: (about 900 lines deleted)  . ",
        "label": 274
    },
    {
        "text": "remove jdk derived code from project <description> some classes appear to have been copied from the jdk source and altered to improve them for use in cassandra. unfortunately neither the classic jdk nor the openjdk license is compatible with the apache license that cassandra is operating under. see https://issues.apache.org/jira/browse/legal-46 modified jdk classes include: bitset [jdk: bitset] fastbytearrayinputstream [bytearrayinputstream] fastbytearrayoutputstream [bytearrayoutputstream]<stacktrace> <code> <text> some classes appear to have been copied from the jdk source and altered to improve them for use in cassandra. unfortunately neither the classic jdk nor the openjdk license is compatible with the apache license that cassandra is operating under. see https://issues.apache.org/jira/browse/legal-46 modified jdk classes include:",
        "label": 274
    },
    {
        "text": "consider removing udf as class functionality <description> we've introduce 2 ways to provide (scalar) udfs: either by providing a class+method name (and assuming c* can find such class and method in the classpath), or by providing the body of the function directly in the create function statement (with such body being able to be in either java or some variety of scripting languages). i submit that we remove the first option: the declaration of functions from a class+method name. i was the first to insist on adding it, but in hindsight i think it adds more complexity/confusion than anything else. more specifically, i think the udf-as-class option is always inferior to cassandra-7562 because: 1. it's more error prone. you have to manually deploy the class containing the function to every c* node (and make sure it is in the classpath). it's way too easy to end up with some node not having the function due to simple operator error. 2. it's not faster. in fact, post-cassandra-7924, the udf-as-java-source is probably faster since it doesn't involve reflection while the udf-as-class option does. overall, i think removing the udf-as-class will simplify things without really losing anything (of course, we can re-evaluate this in the future if new arguments arises, but it's easier to add than to remove).<stacktrace> <code> <text> we've introduce 2 ways to provide (scalar) udfs: either by providing a class+method name (and assuming c* can find such class and method in the classpath), or by providing the body of the function directly in the create function statement (with such body being able to be in either java or some variety of scripting languages). i submit that we remove the first option: the declaration of functions from a class+method name. i was the first to insist on adding it, but in hindsight i think it adds more complexity/confusion than anything else. more specifically, i think the udf-as-class option is always inferior to cassandra-7562 because: overall, i think removing the udf-as-class will simplify things without really losing anything (of course, we can re-evaluate this in the future if new arguments arises, but it's easier to add than to remove).",
        "label": 453
    },
    {
        "text": "uda fails without input rows <description> when creating the following user defined aggregation and function: init.cql create function state_group_and_total(state map<uuid, int>, type uuid)     returns null on null input     returns map<uuid, int>     language java as '         integer count = (integer) state.get(type);         count = (count == null ? 1 : count + 1);         state.put(type, count);         return state;     '; create or replace aggregate group_and_total(uuid)     sfunc state_group_and_total     stype map<uuid, int>     initcond {}; and creating a statement like: select group_and_total(\"id\") from mytable; when mytable is empty, it throws the following null assertion error [native-transport-requests-1] 2017-04-03 07:25:09,787 message.java:623 - unexpected exception during request; channel = [id: 0xd7d9159b, l:/172.19.0.2:9042 - r:/172.19.0.3:43444] java.lang.assertionerror: null         at org.apache.cassandra.cql3.functions.udaggregate$2.compute(udaggregate.java:189) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.aggregatefunctionselector.getoutput(aggregatefunctionselector.java:53) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.selection$selectionwithprocessing$1.getoutputrow(selection.java:592) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.selection$resultsetbuilder.getoutputrow(selection.java:430) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.selection$resultsetbuilder.build(selection.java:424) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:763) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:400) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:378) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:251) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:79) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:217) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:248) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:233) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:116) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:517) [apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:410) [apache-cassandra-3.10.jar:3.10]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:366) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.access$600(abstractchannelhandlercontext.java:35) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext$7.run(abstractchannelhandlercontext.java:357) [netty-all-4.0.39.final.jar:4.0.39.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_121]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:162) [apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:109) [apache-cassandra-3.10.jar:3.10]         at java.lang.thread.run(thread.java:745) [na:1.8.0_121] even if my function only returns state, it creates that assertion null. thank you in advance.<stacktrace> error [native-transport-requests-1] 2017-04-03 07:25:09,787 message.java:623 - unexpected exception during request; channel = [id: 0xd7d9159b, l:/172.19.0.2:9042 - r:/172.19.0.3:43444] java.lang.assertionerror: null         at org.apache.cassandra.cql3.functions.udaggregate$2.compute(udaggregate.java:189) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.aggregatefunctionselector.getoutput(aggregatefunctionselector.java:53) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.selection$selectionwithprocessing$1.getoutputrow(selection.java:592) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.selection$resultsetbuilder.getoutputrow(selection.java:430) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.selection.selection$resultsetbuilder.build(selection.java:424) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:763) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:400) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:378) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:251) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:79) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:217) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:248) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:233) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:116) ~[apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:517) [apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:410) [apache-cassandra-3.10.jar:3.10]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:366) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.access$600(abstractchannelhandlercontext.java:35) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext$7.run(abstractchannelhandlercontext.java:357) [netty-all-4.0.39.final.jar:4.0.39.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_121]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:162) [apache-cassandra-3.10.jar:3.10]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:109) [apache-cassandra-3.10.jar:3.10]         at java.lang.thread.run(thread.java:745) [na:1.8.0_121] <code> create function state_group_and_total(state map<uuid, int>, type uuid)     returns null on null input     returns map<uuid, int>     language java as '         integer count = (integer) state.get(type);         count = (count == null ? 1 : count + 1);         state.put(type, count);         return state;     '; create or replace aggregate group_and_total(uuid)     sfunc state_group_and_total     stype map<uuid, int>     initcond {}; select group_and_total('id') from mytable; <text> when creating the following user defined aggregation and function: and creating a statement like: when mytable is empty, it throws the following null assertion even if my function only returns state, it creates that assertion null. thank you in advance.",
        "label": 453
    },
    {
        "text": "cassandra eclipse warnings <description> ref = origin/cassandra-3.3   commit = 1a31958bfa2adb04ec965e6e2776862eee30ecf4 # 1/27/16 9:03:40 pm utc # eclipse compiler for java(tm) v20150120-1634, 3.10.2, copyright ibm corp 2000, 2013. all rights reserved. ---------- 1. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/db/compaction/writers/maxsstablesizewriter.java (at line 86) sstablewriter writer = sstablewriter.create(descriptor.fromfilename(cfs.getsstablepath(getdirectories().getlocationfordisk(sstabledirectory))),               ^^^^^^ potential resource leak: 'writer' may not be closed ---------- ---------- 2. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/db/compaction/writers/splittingsizetieredcompactionwriter.java (at line 108) sstablewriter writer = sstablewriter.create(descriptor.fromfilename(cfs.getsstablepath(getdirectories().getlocationfordisk(location))),               ^^^^^^ potential resource leak: 'writer' may not be closed ---------- ---------- 3. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/db/columnfamilystore.java (at line 1073) sstablemultiwriter writer = writeriterator.next();                    ^^^^^^ potential resource leak: 'writer' may not be closed ---------- ---------- 4. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/net/outboundtcpconnectionpool.java (at line 141) return channel.socket(); ^^^^^^^^^^^^^^^^^^^^^^^^ potential resource leak: 'channel' may not be closed at this location ---------- ---------- 5. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/metrics/tablemetrics.java (at line 335) return computecompressionratio(iterables.concat(iterables.transform(keyspace.all(),        ^^^^^^^^^^^^^^^^^^^^^^^ the method computecompressionratio(iterable<sstablereader>) in the type tablemetrics is not applicable for the arguments (iterable<object>) ---------- ---------- 6. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/hints/compressedchecksummeddatainput.java (at line 156) return builder.build(); ^^^^^^^^^^^^^^^^^^^^^^^ potential resource leak: '<unassigned closeable value from line 153>' may not be closed at this location ---------- 6 problems (6 errors) check latest build on http://cassci.datastax.com/job/cassandra-3.3_eclipse-warnings/ for the most current warnings<stacktrace> <code> # 1/27/16 9:03:40 pm utc # eclipse compiler for java(tm) v20150120-1634, 3.10.2, copyright ibm corp 2000, 2013. all rights reserved. ---------- 1. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/db/compaction/writers/maxsstablesizewriter.java (at line 86) sstablewriter writer = sstablewriter.create(descriptor.fromfilename(cfs.getsstablepath(getdirectories().getlocationfordisk(sstabledirectory))),               ^^^^^^ potential resource leak: 'writer' may not be closed ---------- ---------- 2. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/db/compaction/writers/splittingsizetieredcompactionwriter.java (at line 108) sstablewriter writer = sstablewriter.create(descriptor.fromfilename(cfs.getsstablepath(getdirectories().getlocationfordisk(location))),               ^^^^^^ potential resource leak: 'writer' may not be closed ---------- ---------- 3. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/db/columnfamilystore.java (at line 1073) sstablemultiwriter writer = writeriterator.next();                    ^^^^^^ potential resource leak: 'writer' may not be closed ---------- ---------- 4. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/net/outboundtcpconnectionpool.java (at line 141) return channel.socket(); ^^^^^^^^^^^^^^^^^^^^^^^^ potential resource leak: 'channel' may not be closed at this location ---------- ---------- 5. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/metrics/tablemetrics.java (at line 335) return computecompressionratio(iterables.concat(iterables.transform(keyspace.all(),        ^^^^^^^^^^^^^^^^^^^^^^^ the method computecompressionratio(iterable<sstablereader>) in the type tablemetrics is not applicable for the arguments (iterable<object>) ---------- ---------- 6. error in /var/lib/jenkins/workspace/cassandra-3.3_eclipse-warnings/src/java/org/apache/cassandra/hints/compressedchecksummeddatainput.java (at line 156) return builder.build(); ^^^^^^^^^^^^^^^^^^^^^^^ potential resource leak: '<unassigned closeable value from line 153>' may not be closed at this location ---------- 6 problems (6 errors) ref = origin/cassandra-3.3   commit = 1a31958bfa2adb04ec965e6e2776862eee30ecf4 check latest build on http://cassci.datastax.com/job/cassandra-3.3_eclipse-warnings/ for the most current warnings<text> ",
        "label": 69
    },
    {
        "text": "serialize the header only once per message <description> one last improvement i'd like to do on the serialization side is that we currently serialize the serializationheader for each partition. that header contains the serialized columns in particular and for range queries, serializing that for every partition is wasted (note that it's only a problem for the messaging protocol as for sstable we only write the header once per sstable).<stacktrace> <code> <text> one last improvement i'd like to do on the serialization side is that we currently serialize the serializationheader for each partition. that header contains the serialized columns in particular and for range queries, serializing that for every partition is wasted (note that it's only a problem for the messaging protocol as for sstable we only write the header once per sstable).",
        "label": 67
    },
    {
        "text": "dtest failure in materialized views test testmaterializedviews view tombstone test <description> example failure: http://cassci.datastax.com/job/cassandra-3.0_dtest/564/testreport/materialized_views_test/testmaterializedviews/view_tombstone_test failed on cassci build cassandra-3.0_dtest #564 intermittent failure, error: expected [[1, 1, 'b', 3.0]] from select * from t_by_v where v = 1, but got []<stacktrace> <code> expected [[1, 1, 'b', 3.0]] from select * from t_by_v where v = 1, but got [] http://cassci.datastax.com/job/cassandra-3.0_dtest/564/testreport/materialized_views_test/testmaterializedviews/view_tombstone_test failed on cassci build cassandra-3.0_dtest #564 <text> example failure: intermittent failure, error:",
        "label": 428
    },
    {
        "text": "percentile computation should use ceil not floor in estimatedhistogram <description> when computing the pcount cassandra uses floor and the comparison with elements is >= so given a simple example of there being a total of five elements // data [1, 1, 1, 1, 1] // offsets [1, 2, 3, 4, 5] cassandra would report the 50th percentile as 2. while 3 is the more expected value. as a comparison using numpy import numpy as np np.percentile(np.array([1, 2, 3, 4, 5]), 50) ==> 3.0 the percentiles was added in cassandra-4022 but is now used a lot in metrics cassandra reports. i think it should error on the side on overestimating instead of underestimating.<stacktrace> <code> // data [1, 1, 1, 1, 1] // offsets [1, 2, 3, 4, 5] import numpy as np np.percentile(np.array([1, 2, 3, 4, 5]), 50) ==> 3.0 <text> when computing the pcount cassandra uses floor and the comparison with elements is >= so given a simple example of there being a total of five elements cassandra would report the 50th percentile as 2. while 3 is the more expected value. as a comparison using numpy the percentiles was added in cassandra-4022 but is now used a lot in metrics cassandra reports. i think it should error on the side on overestimating instead of underestimating.",
        "label": 98
    },
    {
        "text": "gcinspector uses com sun management   exception under ibm jdk <description> com.sun.management.* classes are not available on ibm jdk's a relatively quick patch would be to just log a message and let storageservice go on even if the gcinspector does not start. with this, at least the released versions of cassandra compiled on sun jdk's will work on ibm jdk. is this enough of a work around? (cassandra won't compile on ibm jdk unless gcinspector is re-written for both environments using reflection and platform specific classes) index: src/java/org/apache/cassandra/service/storageservice.java  ===================================================================  \u2014 src/java/org/apache/cassandra/service/storageservice.java (revision 941276)  +++ src/java/org/apache/cassandra/service/storageservice.java (working copy)  @@ -341,7 +341,14 @@  }  databasedescriptor.createalldirectories(); gcinspector.instance.start();  + try  + { + gcinspector.instance.start(); + } + catch (throwable t)  + { + logger_.info(\"gcinspector is disabled\"); + } logger_.info(\"starting up server gossip\");  messagingservice.instance.listen(fbutilities.getlocaladdress());<stacktrace> <code> com.sun.management.* classes are not available on ibm jdk's a relatively quick patch would be to just log a message and let storageservice go on even if the gcinspector does not start. with this, at least the released versions of cassandra compiled on sun jdk's will work on ibm jdk. is this enough of a work around? (cassandra won't compile on ibm jdk unless gcinspector is re-written for both environments using reflection and platform specific classes) index: src/java/org/apache/cassandra/service/storageservice.java  ===================================================================  - src/java/org/apache/cassandra/service/storageservice.java (revision 941276)  +++ src/java/org/apache/cassandra/service/storageservice.java (working copy)  @@ -341,7 +341,14 @@  } databasedescriptor.createalldirectories(); logger_.info('starting up server gossip'); messagingservice.instance.listen(fbutilities.getlocaladdress());<text> + catch (throwable t)  + ",
        "label": 186
    },
    {
        "text": "make node tool command take a password file <description> we are sending the jmx password in the clear to the node tool command in production. this is a security risk. any one doing a 'ps' can see the clear password. can we change the node tool command to also take a password file argument. this file will list the jmx user and passwords. example below: cat /cassandra/run/10003004.jmxpasswd  monitorrole abc  controlrole def based on the user name provided, node tool can pick up the right password.<stacktrace> <code> <text> we are sending the jmx password in the clear to the node tool command in production. this is a security risk. any one doing a 'ps' can see the clear password. can we change the node tool command to also take a password file argument. this file will list the jmx user and passwords. example below: cat /cassandra/run/10003004.jmxpasswd  monitorrole abc  controlrole def based on the user name provided, node tool can pick up the right password.",
        "label": 117
    },
    {
        "text": "reading cardinality from statistics db failed <description> there is some issue with sstable metadata which is visible in system.log, the messages says: warn  [thread-6] 2018-07-25 07:12:47,928 sstablereader.java:249 - reading cardinality from statistics.db failed for /opt/data/disk5/data/keyspace/table/mc-big-data.db. although there is no such file. the message has appeared after i've changed the compaction strategy from sizetiered to leveled. compaction strategy has been changed region by region (total 3 regions) and it has coincided with the double client write traffic increase.  i have tried to run nodetool scrub to rebuilt the sstable, but that does not fix the issue. so very hard to define the steps to reproduce, probably it will be: 1. run stress tool with write traffic 2. under load change compaction strategy from siretiered to leveled for the bunch of hosts 3. add more write traffic reading the code it is said that if this metadata is broken, then \"estimating the keys will be done using index summary\".   https://github.com/apache/cassandra/blob/cassandra-3.0.17/src/java/org/apache/cassandra/io/sstable/format/sstablereader.java#l247   <stacktrace> <code> warn  [thread-6] 2018-07-25 07:12:47,928 sstablereader.java:249 - reading cardinality from statistics.db failed for /opt/data/disk5/data/keyspace/table/mc-big-data.db. reading the code it is said that if this metadata is broken, then 'estimating the keys will be done using index summary'.   https://github.com/apache/cassandra/blob/cassandra-3.0.17/src/java/org/apache/cassandra/io/sstable/format/sstablereader.java#l247   <text> there is some issue with sstable metadata which is visible in system.log, the messages says: although there is no such file. the message has appeared after i've changed the compaction strategy from sizetiered to leveled. compaction strategy has been changed region by region (total 3 regions) and it has coincided with the double client write traffic increase.  i have tried to run nodetool scrub to rebuilt the sstable, but that does not fix the issue. so very hard to define the steps to reproduce, probably it will be: ",
        "label": 553
    },
    {
        "text": "npe when running sstablesplit on valid sstable <description> #create cluster  ccm create --cassandra-version git:cassandra-1.2 test  ccm populate -n 1  ccm start #run stress  ccm node1 stress -n 10000000 -o insert  ccm node1 compact cd ~/.ccm/test/node1/data  ../bin/sstablesplit -n 100 ./keyspace1/standard1/keyspace1-standard1-ic-16-data.db #expected  single large sstable should be split into multiple sstables with max size 100 mb #actual  error 10:14:06,992 error in threadpoolexecutor  java.lang.nullpointerexception  at org.apache.cassandra.io.sstable.sstabledeletingtask.run(sstabledeletingtask.java:70)  at java.util.concurrent.executors$runnableadapter.call(executors.java:471)  at java.util.concurrent.futuretask.run(futuretask.java:262)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:178)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:292)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:724) notes: it seems like the split occurs and can be recompacted.  last known commit where split was working on 1.2 branch: 47b2cd6620894bf0c4c4584036eab49a2e14a50e  have not bisected further. sstablesplit is also broken on 2.0 branch; however, it fails differently. filing separate bug on that.<stacktrace> #actual  error 10:14:06,992 error in threadpoolexecutor  java.lang.nullpointerexception  at org.apache.cassandra.io.sstable.sstabledeletingtask.run(sstabledeletingtask.java:70)  at java.util.concurrent.executors$runnableadapter.call(executors.java:471)  at java.util.concurrent.futuretask.run(futuretask.java:262)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$201(scheduledthreadpoolexecutor.java:178)  at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:292)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:724) <code> #create cluster  ccm create --cassandra-version git:cassandra-1.2 test  ccm populate -n 1  ccm start #run stress  ccm node1 stress -n 10000000 -o insert  ccm node1 compact cd ~/.ccm/test/node1/data  ../bin/sstablesplit -n 100 ./keyspace1/standard1/keyspace1-standard1-ic-16-data.db <text> #expected  single large sstable should be split into multiple sstables with max size 100 mb notes: it seems like the split occurs and can be recompacted.  last known commit where split was working on 1.2 branch: 47b2cd6620894bf0c4c4584036eab49a2e14a50e  have not bisected further. sstablesplit is also broken on 2.0 branch; however, it fails differently. filing separate bug on that.",
        "label": 520
    },
    {
        "text": "update hadoop version in wordcount exampe <description> example/hadoop_word_count uses hadoop 0.20.2, should use most recent stable version of hadoop, 1.0.3<stacktrace> <code> <text> example/hadoop_word_count uses hadoop 0.20.2, should use most recent stable version of hadoop, 1.0.3",
        "label": 427
    },
    {
        "text": "batch that updates two or more table can produce unreadable sstable  was  auto bootstraping a new node fails  <description> i've been trying to add a new node in my 3.0 cluster and it seems to fail. all my nodes are using apache/cassandra-3.0.0 branch. at the beginning, i can see the following error: info  18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a id#0] prepare completed. receiving 42 files(1910066622 bytes), sending 0 files(0 bytes) warn  18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] retrying for following error java.lang.runtimeexception: unknown column added_time during deserialization         at org.apache.cassandra.db.serializationheader$component.toheader(serializationheader.java:331) ~[main/:na]         at org.apache.cassandra.streaming.streamreader.createwriter(streamreader.java:136) ~[main/:na]         at org.apache.cassandra.streaming.compress.compressedstreamreader.read(compressedstreamreader.java:77) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:50) [main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:39) [main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:59) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error 18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] streaming error occurred java.lang.illegalargumentexception: unknown type 0         at org.apache.cassandra.streaming.messages.streammessage$type.get(streammessage.java:97) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:58) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] info  18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] session with /54.210.187.114 is complete info  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a id#0] prepare completed. receiving 38 files(2323537628 bytes), sending 0 files(0 bytes) warn  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] retrying for following error java.lang.runtimeexception: unknown column added_time during deserialization         at org.apache.cassandra.db.serializationheader$component.toheader(serializationheader.java:331) ~[main/:na]         at org.apache.cassandra.streaming.streamreader.createwriter(streamreader.java:136) ~[main/:na]         at org.apache.cassandra.streaming.compress.compressedstreamreader.read(compressedstreamreader.java:77) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:50) [main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:39) [main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:59) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error 18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] streaming error occurred java.lang.illegalargumentexception: unknown type 0         at org.apache.cassandra.streaming.messages.streammessage$type.get(streammessage.java:97) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:58) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] info  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] session with /54.210.184.198 is complete info  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a id#0] prepare completed. receiving 35 files(2069893759 bytes), sending 0 files(0 bytes) warn  18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] retrying for following error java.lang.assertionerror: null         at org.apache.cassandra.db.partitioncolumns$builder.add(partitioncolumns.java:168) ~[main/:na]         at org.apache.cassandra.db.serializationheader$component.toheader(serializationheader.java:333) ~[main/:na]         at org.apache.cassandra.streaming.streamreader.createwriter(streamreader.java:136) ~[main/:na]         at org.apache.cassandra.streaming.compress.compressedstreamreader.read(compressedstreamreader.java:77) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:50) [main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:39) [main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:59) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error 18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] streaming error occurred java.lang.illegalargumentexception: unknown type 0         at org.apache.cassandra.streaming.messages.streammessage$type.get(streammessage.java:97) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:58) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] info  18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] session with /54.210.157.2 is complete warn  18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] stream failed error 18:45:57 error while waiting on bootstrap to complete. bootstrap will have to be restarted. java.util.concurrent.executionexception: org.apache.cassandra.streaming.streamexception: stream failed         at com.google.common.util.concurrent.abstractfuture$sync.getvalue(abstractfuture.java:299) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.abstractfuture$sync.get(abstractfuture.java:286) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.abstractfuture.get(abstractfuture.java:116) ~[guava-18.0.jar:na]         at org.apache.cassandra.service.storageservice.bootstrap(storageservice.java:1245) [main/:na]         at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:935) [main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:710) [main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:581) [main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:345) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:561) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:689) [main/:na] caused by: org.apache.cassandra.streaming.streamexception: stream failed         at org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85) ~[main/:na]         at com.google.common.util.concurrent.futures$6.run(futures.java:1310) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202) ~[guava-18.0.jar:na]         at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:210) ~[main/:na]         at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:186) ~[main/:na]         at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:430) ~[main/:na]         at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:525) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:279) ~[main/:na]         at java.lang.thread.run(thread.java:745) ~[na:1.8.0_45] oct 19, 2015 6:45:57 pm com.google.common.util.concurrent.executionlist executelistener severe: runtimeexception while executing runnable com.google.common.util.concurrent.futures$6@59444306 with executor instance java.lang.nullpointerexception         at org.apache.cassandra.service.storageservice$2.onfailure(storageservice.java:1240)         at com.google.common.util.concurrent.futures$6.run(futures.java:1310)         at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457)         at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156)         at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145)         at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202)         at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:210)         at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:186)         at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:430)         at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:525)         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:279)         at java.lang.thread.run(thread.java:745) the \"added_time\" is a bigint column. i do see that the node is marked as uj in nodetool status and the bootstraping continue to run. but at a certain point, it hangs forever and restarting it doesn't help. i'm not 100% sure the hang issue is related to the above error though. but i'd like to fix this one before trying again. any idea what's going on with that bigint type?<stacktrace> info  18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a id#0] prepare completed. receiving 42 files(1910066622 bytes), sending 0 files(0 bytes) warn  18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] retrying for following error java.lang.runtimeexception: unknown column added_time during deserialization         at org.apache.cassandra.db.serializationheader$component.toheader(serializationheader.java:331) ~[main/:na]         at org.apache.cassandra.streaming.streamreader.createwriter(streamreader.java:136) ~[main/:na]         at org.apache.cassandra.streaming.compress.compressedstreamreader.read(compressedstreamreader.java:77) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:50) [main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:39) [main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:59) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error 18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] streaming error occurred java.lang.illegalargumentexception: unknown type 0         at org.apache.cassandra.streaming.messages.streammessage$type.get(streammessage.java:97) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:58) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] info  18:45:55 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] session with /54.210.187.114 is complete info  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a id#0] prepare completed. receiving 38 files(2323537628 bytes), sending 0 files(0 bytes) warn  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] retrying for following error java.lang.runtimeexception: unknown column added_time during deserialization         at org.apache.cassandra.db.serializationheader$component.toheader(serializationheader.java:331) ~[main/:na]         at org.apache.cassandra.streaming.streamreader.createwriter(streamreader.java:136) ~[main/:na]         at org.apache.cassandra.streaming.compress.compressedstreamreader.read(compressedstreamreader.java:77) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:50) [main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:39) [main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:59) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error 18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] streaming error occurred java.lang.illegalargumentexception: unknown type 0         at org.apache.cassandra.streaming.messages.streammessage$type.get(streammessage.java:97) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:58) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] info  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] session with /54.210.184.198 is complete info  18:45:56 [stream #9f95fa90-7691-11e5-931f-5b735851f84a id#0] prepare completed. receiving 35 files(2069893759 bytes), sending 0 files(0 bytes) warn  18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] retrying for following error java.lang.assertionerror: null         at org.apache.cassandra.db.partitioncolumns$builder.add(partitioncolumns.java:168) ~[main/:na]         at org.apache.cassandra.db.serializationheader$component.toheader(serializationheader.java:333) ~[main/:na]         at org.apache.cassandra.streaming.streamreader.createwriter(streamreader.java:136) ~[main/:na]         at org.apache.cassandra.streaming.compress.compressedstreamreader.read(compressedstreamreader.java:77) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:50) [main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:39) [main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:59) [main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error 18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] streaming error occurred java.lang.illegalargumentexception: unknown type 0         at org.apache.cassandra.streaming.messages.streammessage$type.get(streammessage.java:97) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:58) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:261) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] info  18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] session with /54.210.157.2 is complete warn  18:45:57 [stream #9f95fa90-7691-11e5-931f-5b735851f84a] stream failed error 18:45:57 error while waiting on bootstrap to complete. bootstrap will have to be restarted. java.util.concurrent.executionexception: org.apache.cassandra.streaming.streamexception: stream failed         at com.google.common.util.concurrent.abstractfuture$sync.getvalue(abstractfuture.java:299) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.abstractfuture$sync.get(abstractfuture.java:286) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.abstractfuture.get(abstractfuture.java:116) ~[guava-18.0.jar:na]         at org.apache.cassandra.service.storageservice.bootstrap(storageservice.java:1245) [main/:na]         at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:935) [main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:710) [main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:581) [main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:345) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:561) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:689) [main/:na] caused by: org.apache.cassandra.streaming.streamexception: stream failed         at org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85) ~[main/:na]         at com.google.common.util.concurrent.futures$6.run(futures.java:1310) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145) ~[guava-18.0.jar:na]         at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202) ~[guava-18.0.jar:na]         at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:210) ~[main/:na]         at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:186) ~[main/:na]         at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:430) ~[main/:na]         at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:525) ~[main/:na]         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:279) ~[main/:na]         at java.lang.thread.run(thread.java:745) ~[na:1.8.0_45] oct 19, 2015 6:45:57 pm com.google.common.util.concurrent.executionlist executelistener severe: runtimeexception while executing runnable com.google.common.util.concurrent.futures$6@59444306 with executor instance java.lang.nullpointerexception         at org.apache.cassandra.service.storageservice$2.onfailure(storageservice.java:1240)         at com.google.common.util.concurrent.futures$6.run(futures.java:1310)         at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457)         at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156)         at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145)         at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202)         at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:210)         at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:186)         at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:430)         at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:525)         at org.apache.cassandra.streaming.connectionhandler$incomingmessagehandler.run(connectionhandler.java:279)         at java.lang.thread.run(thread.java:745) <code> <text> i've been trying to add a new node in my 3.0 cluster and it seems to fail. all my nodes are using apache/cassandra-3.0.0 branch. at the beginning, i can see the following error: the 'added_time' is a bigint column. i do see that the node is marked as uj in nodetool status and the bootstraping continue to run. but at a certain point, it hangs forever and restarting it doesn't help. i'm not 100% sure the hang issue is related to the above error though. but i'd like to fix this one before trying again. any idea what's going on with that bigint type?",
        "label": 520
    },
    {
        "text": "review handling crypto rules and update eccn page if needed <description> it is suggested in legal-358 that cassandra is containing/using cryptographic functions and does not have an entry on the eccn page ( http://www.apache.org/licenses/exports/ ). see http://www.apache.org/dev/crypto.html to review and confirm whether you should add something to the eccn page, and if needed, please do so. the text in legal-358 was:    nate mccall added a comment - 26/dec/17 14:58  ok, i think i have this sorted. our entry on that page will need to look like this: product name versions eccn controlled source apache cassandra development 5d002 asf 0.8 and later 5d002 asf we first added ssl support in 0.8 via cassandra-1567 we rely solely on the jdk functionality for all encryption.<stacktrace> <code> <text> product name versions eccn controlled source apache cassandra development 5d002 asf 0.8 and later 5d002 asf it is suggested in legal-358 that cassandra is containing/using cryptographic functions and does not have an entry on the eccn page ( http://www.apache.org/licenses/exports/ ). see http://www.apache.org/dev/crypto.html to review and confirm whether you should add something to the eccn page, and if needed, please do so. the text in legal-358 was:    nate mccall added a comment - 26/dec/17 14:58  ok, i think i have this sorted. our entry on that page will need to look like this: we first added ssl support in 0.8 via cassandra-1567 we rely solely on the jdk functionality for all encryption.",
        "label": 373
    },
    {
        "text": "thrift max message length in mb makes long lived connections error out <description> when running mapreduce jobs that read directly from cassandra, the job will sometimes fail with an exception like this: java.lang.runtimeexception: com.rockmelt.org.apache.thrift.texception: message length exceeded: 40  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.maybeinit(columnfamilyrecordreader.java:400)  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.computenext(columnfamilyrecordreader.java:406)  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.computenext(columnfamilyrecordreader.java:329)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)  at org.apache.cassandra.hadoop.columnfamilyrecordreader.getprogress(columnfamilyrecordreader.java:109)  at org.apache.hadoop.mapred.maptask$newtrackingrecordreader.getprogress(maptask.java:522)  at org.apache.hadoop.mapred.maptask$newtrackingrecordreader.nextkeyvalue(maptask.java:547)  at org.apache.hadoop.mapreduce.mapcontext.nextkeyvalue(mapcontext.java:67)  at org.apache.hadoop.mapreduce.mapper.run(mapper.java:143)  at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:771)  at org.apache.hadoop.mapred.maptask.run(maptask.java:375)  at org.apache.hadoop.mapred.child$4.run(child.java:255)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1132)  at org.apache.hadoop.mapred.child.main(child.java:249)  caused by: com.rockmelt.org.apache.thrift.texception: message length exceeded: 40  at com.rockmelt.org.apache.thrift.protocol.tbinaryprotocol.checkreadlength(tbinaryprotocol.java:393)  at com.rockmelt.org.apache.thrift.protocol.tbinaryprotocol.readbinary(tbinaryprotocol.java:363)  at org.apache.cassandra.thrift.column.read(column.java:528)  at org.apache.cassandra.thrift.columnorsupercolumn.read(columnorsupercolumn.java:507)  at org.apache.cassandra.thrift.keyslice.read(keyslice.java:408)  at org.apache.cassandra.thrift.cassandra$get_range_slices_result.read(cassandra.java:12422)  at com.rockmelt.org.apache.thrift.tserviceclient.receivebase(tserviceclient.java:78)  at org.apache.cassandra.thrift.cassandra$client.recv_get_range_slices(cassandra.java:696)  at org.apache.cassandra.thrift.cassandra$client.get_range_slices(cassandra.java:680)  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.maybeinit(columnfamilyrecordreader.java:362)  ... 16 more in columnfamilyrecordreader#initialize, a tbinaryprotocol is created as follows: ttransport transport = confighelper.getinputtransportfactory(conf).opentransport(socket, conf);  tbinaryprotocol binaryprotocol = new tbinaryprotocol(transport, confighelper.getthriftmaxmessagelength(conf));  client = new cassandra.client(binaryprotocol); but each time a call to cassandra is made, checkreadlength(int length) is called in tbinaryprotocol, which includes this: readlength_ -= length;  if (readlength_ < 0) {  throw new texception(\"message length exceeded: \" + length);  } the result is that readlength_ is decreased each time, until it goes negative and exception is thrown. this will only happen if you're reading a lot of data and your split size is large (which is maybe why people haven't noticed it earlier). this happens regardless of whether you use wide row support. i'm not sure what the right fix is. it seems like you could either reset the length of tbinaryprotocol after each call or just use a new tbinaryprotocol each time.<stacktrace> java.lang.runtimeexception: com.rockmelt.org.apache.thrift.texception: message length exceeded: 40  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.maybeinit(columnfamilyrecordreader.java:400)  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.computenext(columnfamilyrecordreader.java:406)  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.computenext(columnfamilyrecordreader.java:329)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)  at org.apache.cassandra.hadoop.columnfamilyrecordreader.getprogress(columnfamilyrecordreader.java:109)  at org.apache.hadoop.mapred.maptask$newtrackingrecordreader.getprogress(maptask.java:522)  at org.apache.hadoop.mapred.maptask$newtrackingrecordreader.nextkeyvalue(maptask.java:547)  at org.apache.hadoop.mapreduce.mapcontext.nextkeyvalue(mapcontext.java:67)  at org.apache.hadoop.mapreduce.mapper.run(mapper.java:143)  at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:771)  at org.apache.hadoop.mapred.maptask.run(maptask.java:375)  at org.apache.hadoop.mapred.child$4.run(child.java:255)  at java.security.accesscontroller.doprivileged(native method)  at javax.security.auth.subject.doas(subject.java:396)  at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1132)  at org.apache.hadoop.mapred.child.main(child.java:249)  caused by: com.rockmelt.org.apache.thrift.texception: message length exceeded: 40  at com.rockmelt.org.apache.thrift.protocol.tbinaryprotocol.checkreadlength(tbinaryprotocol.java:393)  at com.rockmelt.org.apache.thrift.protocol.tbinaryprotocol.readbinary(tbinaryprotocol.java:363)  at org.apache.cassandra.thrift.column.read(column.java:528)  at org.apache.cassandra.thrift.columnorsupercolumn.read(columnorsupercolumn.java:507)  at org.apache.cassandra.thrift.keyslice.read(keyslice.java:408)  at org.apache.cassandra.thrift.cassandra$get_range_slices_result.read(cassandra.java:12422)  at com.rockmelt.org.apache.thrift.tserviceclient.receivebase(tserviceclient.java:78)  at org.apache.cassandra.thrift.cassandra$client.recv_get_range_slices(cassandra.java:696)  at org.apache.cassandra.thrift.cassandra$client.get_range_slices(cassandra.java:680)  at org.apache.cassandra.hadoop.columnfamilyrecordreader$staticrowiterator.maybeinit(columnfamilyrecordreader.java:362)  ... 16 more <code> ttransport transport = confighelper.getinputtransportfactory(conf).opentransport(socket, conf);  tbinaryprotocol binaryprotocol = new tbinaryprotocol(transport, confighelper.getthriftmaxmessagelength(conf));  client = new cassandra.client(binaryprotocol); readlength_ -= length;  if (readlength_ < 0) {  throw new texception('message length exceeded: ' + length);  } <text> when running mapreduce jobs that read directly from cassandra, the job will sometimes fail with an exception like this: in columnfamilyrecordreader#initialize, a tbinaryprotocol is created as follows: but each time a call to cassandra is made, checkreadlength(int length) is called in tbinaryprotocol, which includes this: the result is that readlength_ is decreased each time, until it goes negative and exception is thrown. this will only happen if you're reading a lot of data and your split size is large (which is maybe why people haven't noticed it earlier). this happens regardless of whether you use wide row support. i'm not sure what the right fix is. it seems like you could either reset the length of tbinaryprotocol after each call or just use a new tbinaryprotocol each time.",
        "label": 274
    },
    {
        "text": "move single node localhost test is failing <description> pushed_notifications_test.testpushednotifications.move_single_node_localhost_test is failing across all tested versions. example failure is here. we need to debug this failure, as it is entirely likely it is a test issue and not a bug.<stacktrace> <code> <text> pushed_notifications_test.testpushednotifications.move_single_node_localhost_test is failing across all tested versions. example failure is here. we need to debug this failure, as it is entirely likely it is a test issue and not a bug.",
        "label": 348
    },
    {
        "text": "dtest failure in pushed notifications test testpushednotifications restart node test <description> example failure: http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/275/testreport/pushed_notifications_test/testpushednotifications/restart_node_test error message 'up' != u'new_node' stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/pushed_notifications_test.py\", line 185, in restart_node_test     self.assertequals(\"up\", notifications[1][\"change_type\"])   file \"/usr/lib/python2.7/unittest/case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 506, in _baseassertequal     raise self.failureexception(msg)<stacktrace> <code> error message 'up' != u'new_node' stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/pushed_notifications_test.py', line 185, in restart_node_test     self.assertequals('up', notifications[1]['change_type'])   file '/usr/lib/python2.7/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file '/usr/lib/python2.7/unittest/case.py', line 506, in _baseassertequal     raise self.failureexception(msg) http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/275/testreport/pushed_notifications_test/testpushednotifications/restart_node_test<text> example failure: ",
        "label": 474
    },
    {
        "text": "improve commit log chain marker updating <description> cassandra-13987 addressed the commit log behavior change that was introduced with cassandra-3578. after that patch was committed, ariel weisberg did his own review and found a bug as well as having some concerns about the configuration. he and i discussed offline, and agreed on some improvements. instead of requiring users to configure a deep, dark implementation detail like the commit log chained markers (via commitlog_marker_period_in_ms in the yaml), we decided it is best to eliminate thew configuration and always update the chained markers (when in periodic mode). the bug ariel weisberg found was when the chained marker update is not a value that evenly divides into the periodic sync mode value, we would not sync in an expected manner. for example if the marker interval is 9 seconds, and the sync interval is 10 seconds, we would update the markers at time9, but we would then sleep for another 9 seconds, and when we wake up at time18, it is then that we flush - 8 seconds later than we should have.<stacktrace> <code> <text> cassandra-13987 addressed the commit log behavior change that was introduced with cassandra-3578. after that patch was committed, ariel weisberg did his own review and found a bug as well as having some concerns about the configuration. he and i discussed offline, and agreed on some improvements. instead of requiring users to configure a deep, dark implementation detail like the commit log chained markers (via commitlog_marker_period_in_ms in the yaml), we decided it is best to eliminate thew configuration and always update the chained markers (when in periodic mode). the bug ariel weisberg found was when the chained marker update is not a value that evenly divides into the periodic sync mode value, we would not sync in an expected manner. for example if the marker interval is 9 seconds, and the sync interval is 10 seconds, we would update the markers at time9, but we would then sleep for another 9 seconds, and when we wake up at time18, it is then that we flush - 8 seconds later than we should have.",
        "label": 232
    },
    {
        "text": "pig memory issues with default limit and large rows  <description> rows with a lot of columns, especially super-colums with a lot of values can cause outofmemory errors in cassandra when queried with pig.<stacktrace> <code> <text> rows with a lot of columns, especially super-colums with a lot of values can cause outofmemory errors in cassandra when queried with pig.",
        "label": 332
    },
    {
        "text": "pig  composite column support for row key <description> currently pig only understands composite columns not composite row keys. support for querying column family that has composite type for its key_validation_class will be nice.<stacktrace> <code> <text> currently pig only understands composite columns not composite row keys. support for querying column family that has composite type for its key_validation_class will be nice.",
        "label": 152
    },
    {
        "text": "fsreaderror and leak detected after upgrading <description> after upgrading one of 15 nodes from 2.1.7 to 2.2.0-rc1 i get fsreaderror and leak detected on start. deleting the listed files, the failure goes away. system.log error [sstablebatchopen:1] 2015-06-29 14:38:34,554 debuggablethreadpoolexecutor.java:242 - error in threadpoolexecutor org.apache.cassandra.io.fsreaderror: java.io.ioexception: compressed file with 0 chunks encountered: java.io.datainputstream@1c42271 at org.apache.cassandra.io.compress.compressionmetadata.readchunkoffsets(compressionmetadata.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:117) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.compress.compressionmetadata.create(compressionmetadata.java:86) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.util.compressedsegmentedfile$builder.metadata(compressedsegmentedfile.java:142) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:101) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.util.segmentedfile$builder.complete(segmentedfile.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.load(sstablereader.java:681) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.load(sstablereader.java:644) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.open(sstablereader.java:443) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.open(sstablereader.java:350) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader$4.run(sstablereader.java:480) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at java.util.concurrent.executors$runnableadapter.call(unknown source) ~[na:1.7.0_55] at java.util.concurrent.futuretask.run(unknown source) ~[na:1.7.0_55] at java.util.concurrent.threadpoolexecutor.runworker(unknown source) [na:1.7.0_55] at java.util.concurrent.threadpoolexecutor$worker.run(unknown source) [na:1.7.0_55] at java.lang.thread.run(unknown source) [na:1.7.0_55] caused by: java.io.ioexception: compressed file with 0 chunks encountered: java.io.datainputstream@1c42271 at org.apache.cassandra.io.compress.compressionmetadata.readchunkoffsets(compressionmetadata.java:174) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] ... 15 common frames omitted error [reference-reaper:1] 2015-06-29 14:38:34,734 ref.java:189 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@3e547f) to class org.apache.cassandra.io.sstable.format.sstablereader$instancetidier@1926439:d:\\programme\\cassandra\\data\\data\\system\\compactions_in_progress\\system-compactions_in_progress-ka-6866 was not released before the reference was garbage collected<stacktrace> error [sstablebatchopen:1] 2015-06-29 14:38:34,554 debuggablethreadpoolexecutor.java:242 - error in threadpoolexecutor org.apache.cassandra.io.fsreaderror: java.io.ioexception: compressed file with 0 chunks encountered: java.io.datainputstream@1c42271 at org.apache.cassandra.io.compress.compressionmetadata.readchunkoffsets(compressionmetadata.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.compress.compressionmetadata.<init>(compressionmetadata.java:117) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.compress.compressionmetadata.create(compressionmetadata.java:86) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.util.compressedsegmentedfile$builder.metadata(compressedsegmentedfile.java:142) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.util.compressedpoolingsegmentedfile$builder.complete(compressedpoolingsegmentedfile.java:101) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.util.segmentedfile$builder.complete(segmentedfile.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.load(sstablereader.java:681) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.load(sstablereader.java:644) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.open(sstablereader.java:443) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader.open(sstablereader.java:350) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at org.apache.cassandra.io.sstable.format.sstablereader$4.run(sstablereader.java:480) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] at java.util.concurrent.executors$runnableadapter.call(unknown source) ~[na:1.7.0_55] at java.util.concurrent.futuretask.run(unknown source) ~[na:1.7.0_55] at java.util.concurrent.threadpoolexecutor.runworker(unknown source) [na:1.7.0_55] at java.util.concurrent.threadpoolexecutor$worker.run(unknown source) [na:1.7.0_55] at java.lang.thread.run(unknown source) [na:1.7.0_55] caused by: java.io.ioexception: compressed file with 0 chunks encountered: java.io.datainputstream@1c42271 at org.apache.cassandra.io.compress.compressionmetadata.readchunkoffsets(compressionmetadata.java:174) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1] ... 15 common frames omitted error [reference-reaper:1] 2015-06-29 14:38:34,734 ref.java:189 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@3e547f) to class org.apache.cassandra.io.sstable.format.sstablereader$instancetidier@1926439:d:/programme/cassandra/data/data/system/compactions_in_progress/system-compactions_in_progress-ka-6866 was not released before the reference was garbage collected <code> <text> after upgrading one of 15 nodes from 2.1.7 to 2.2.0-rc1 i get fsreaderror and leak detected on start. deleting the listed files, the failure goes away.",
        "label": 508
    },
    {
        "text": "better support of null for udf <description> currently, every function needs to deal with it's argument potentially being null. there is very many case where that's just annoying, users should be able to define a function like: create function addtwo(val int) returns int language java as 'return val + 2;' without having this crashing as soon as a column it's applied to doesn't a value for some rows (i'll note that this definition apparently cannot be compiled currently, which should be looked into). in fact, i think that by default methods shouldn't have to care about null values: if the value is null, we should not call the method at all and return null. there is still methods that may explicitely want to handle null (to return a default value for instance), so maybe we can add an allow nulls to the creation syntax.<stacktrace> <code> create function addtwo(val int) returns int language java as 'return val + 2;' <text> currently, every function needs to deal with it's argument potentially being null. there is very many case where that's just annoying, users should be able to define a function like: without having this crashing as soon as a column it's applied to doesn't a value for some rows (i'll note that this definition apparently cannot be compiled currently, which should be looked into). in fact, i think that by default methods shouldn't have to care about null values: if the value is null, we should not call the method at all and return null. there is still methods that may explicitely want to handle null (to return a default value for instance), so maybe we can add an allow nulls to the creation syntax.",
        "label": 453
    },
    {
        "text": "cql in query wrong on rows with values bigger than 64kb <description> we are investigating a weird issue where one of our clients doesn't get data on his dashboard. it seems cassandra is not returning data for a particular key (\"brokenkey\" from now on). some background:  we have a row where we store a \"metadata\" column and data in columns \"bucket/0\", \"bucket/1\", \"bucket/2\", etc. depending on the date selection of the ui, we know that we only need to retrieve bucket/0, bucket/0 and bucket/1 etc. (we always need to retrieve \"metadata\"). a typical query may look like this (using select column1 to just show what is returned, normally we would of course do select value): cqlsh:appbrain> select blobastext(column1) from \"groupedseries\" where key=textasblob('install/workingkey');  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) cqlsh:appbrain> select blobastext(column1) from \"groupedseries\" where key=textasblob('install/brokenkey');  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) these two queries work as expected, and return the information that we actually stored.  however, when we \"filter\" for certain columns, the brokenkey starts behaving very weird: cqlsh:appbrain> select blobastext(column1) from \"groupedseries\" where key=textasblob('install/workingkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'));  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) cqlsh:appbrain> select blobastext(column1) from \"groupedseries\" where key=textasblob('install/workingkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'),textasblob('asdfasdfasdf'));  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) ***  as expected, querying for more information doesn't really matter for the working key *** cqlsh:appbrain> select blobastext(column1) from \"groupedseries\" where key=textasblob('install/brokenkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'));  blobastext(column1) ---------------------             bucket/0 (1 rows) *** cassandra stops giving us the metadata column when asking for a few more columns! *** cqlsh:appbrain> select blobastext(column1) from \"groupedseries\" where key=textasblob('install/brokenkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'),textasblob('asdfasdfasdf'));  key | column1 | value -----+---------+------- (0 rows) *** adding the bogus column name even makes it return nothing from this row anymore! *** there are at least two rows that malfunction like this in our table (which is quite old already and has gone through a bunch of cassandra upgrades). i've upgraded our whole cluster to 2.1.5 (we were on 2.1.2 when i discovered this problem) and compacted, repaired and scrubbed this column family, which hasn't helped. our table structure is: cqlsh:appbrain> describe table \"groupedseries\"; create table \"appbrain\".\"groupedseries\" (     key blob,     column1 blob,     value blob,     primary key (key, column1) ) with compact storage     and clustering order by (column1 asc)     and caching = '{\"keys\":\"all\", \"rows_per_partition\":\"none\"}'     and comment = ''     and compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy', 'max_threshold': '32'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 1.0     and speculative_retry = 'none'; let me know if i can give more information that may be helpful.<stacktrace> <code> cqlsh:appbrain> select blobastext(column1) from 'groupedseries' where key=textasblob('install/workingkey');  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) cqlsh:appbrain> select blobastext(column1) from 'groupedseries' where key=textasblob('install/brokenkey');  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) cqlsh:appbrain> select blobastext(column1) from 'groupedseries' where key=textasblob('install/workingkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'));  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) cqlsh:appbrain> select blobastext(column1) from 'groupedseries' where key=textasblob('install/workingkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'),textasblob('asdfasdfasdf'));  blobastext(column1) ---------------------             bucket/0             metadata (2 rows) ***  as expected, querying for more information doesn't really matter for the working key *** cqlsh:appbrain> select blobastext(column1) from 'groupedseries' where key=textasblob('install/brokenkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'));  blobastext(column1) ---------------------             bucket/0 (1 rows) *** cassandra stops giving us the metadata column when asking for a few more columns! *** cqlsh:appbrain> select blobastext(column1) from 'groupedseries' where key=textasblob('install/brokenkey') and column1 in (textasblob('metadata'),textasblob('bucket/0'),textasblob('bucket/1'),textasblob('bucket/2'),textasblob('asdfasdfasdf'));  key | column1 | value -----+---------+------- (0 rows) *** adding the bogus column name even makes it return nothing from this row anymore! *** cqlsh:appbrain> describe table 'groupedseries'; create table 'appbrain'.'groupedseries' (     key blob,     column1 blob,     value blob,     primary key (key, column1) ) with compact storage     and clustering order by (column1 asc)     and caching = '{'keys':'all', 'rows_per_partition':'none'}'     and comment = ''     and compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy', 'max_threshold': '32'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 1.0     and speculative_retry = 'none'; <text> we are investigating a weird issue where one of our clients doesn't get data on his dashboard. it seems cassandra is not returning data for a particular key ('brokenkey' from now on). some background:  we have a row where we store a 'metadata' column and data in columns 'bucket/0', 'bucket/1', 'bucket/2', etc. depending on the date selection of the ui, we know that we only need to retrieve bucket/0, bucket/0 and bucket/1 etc. (we always need to retrieve 'metadata'). a typical query may look like this (using select column1 to just show what is returned, normally we would of course do select value): these two queries work as expected, and return the information that we actually stored.  however, when we 'filter' for certain columns, the brokenkey starts behaving very weird: there are at least two rows that malfunction like this in our table (which is quite old already and has gone through a bunch of cassandra upgrades). i've upgraded our whole cluster to 2.1.5 (we were on 2.1.2 when i discovered this problem) and compacted, repaired and scrubbed this column family, which hasn't helped. our table structure is: let me know if i can give more information that may be helpful.",
        "label": 98
    },
    {
        "text": "sstablemetadata can't load org apache cassandra tools sstablemetadataviewer <description> the sstablemetadata tool only works when running from the source tree. the classpath doesn't get set correctly when running on a deployed environment. this bug looks to exist in 2.1 as well.<stacktrace> <code> <text> the sstablemetadata tool only works when running from the source tree. the classpath doesn't get set correctly when running on a deployed environment. this bug looks to exist in 2.1 as well.",
        "label": 253
    },
    {
        "text": "npe when you mistakenly set listen address to <description> it's clearly stated that setting listen_address to 0.0.0.0 is always wrong. but if you mistakenly do it anyway you end up with an npe on 1.2.8 while it's not the case on 2.0.0-rc1. see bellow:  info 16:34:43,598 joining: waiting for ring information  info 16:34:44,505 handshaking version with /127.0.0.1  info 16:34:44,533 handshaking version with /0.0.0.0  info 16:35:13,626 joining: schema complete, ready to bootstrap  info 16:35:13,631 joining: getting bootstrap token error 16:35:13,633 exception encountered during startup java.lang.runtimeexception: no other nodes seen!  unable to bootstrap.if you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  usually, this can be solved by giving all nodes the same seed list. at org.apache.cassandra.dht.bootstrapper.getbootstrapsource(bootstrapper.java:154) at org.apache.cassandra.dht.bootstrapper.getbalancedtoken(bootstrapper.java:135) at org.apache.cassandra.dht.bootstrapper.getbootstraptokens(bootstrapper.java:115) at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:666) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:554) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:451) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:348) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:447) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:490) java.lang.runtimeexception: no other nodes seen!  unable to bootstrap.if you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  usually, this can be solved by giving all nodes the same seed list. at org.apache.cassandra.dht.bootstrapper.getbootstrapsource(bootstrapper.java:154) at org.apache.cassandra.dht.bootstrapper.getbalancedtoken(bootstrapper.java:135) at org.apache.cassandra.dht.bootstrapper.getbootstraptokens(bootstrapper.java:115) at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:666) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:554) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:451) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:348) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:447) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:490) exception encountered during startup: no other nodes seen!  unable to bootstrap.if you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  usually, this can be solved by giving all nodes the same seed list. error 16:35:13,668 exception in thread thread[storageserviceshutdownhook,5,main] java.lang.nullpointerexception at org.apache.cassandra.service.storageservice.stoprpcserver(storageservice.java:321) at org.apache.cassandra.service.storageservice.shutdownclientservers(storageservice.java:370) at org.apache.cassandra.service.storageservice.access$000(storageservice.java:88) at org.apache.cassandra.service.storageservice$1.runmaythrow(storageservice.java:519) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.lang.thread.run(thread.java:724)<stacktrace>  info 16:34:43,598 joining: waiting for ring information  info 16:34:44,505 handshaking version with /127.0.0.1  info 16:34:44,533 handshaking version with /0.0.0.0  info 16:35:13,626 joining: schema complete, ready to bootstrap  info 16:35:13,631 joining: getting bootstrap token error 16:35:13,633 exception encountered during startup java.lang.runtimeexception: no other nodes seen!  unable to bootstrap.if you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  usually, this can be solved by giving all nodes the same seed list. at org.apache.cassandra.dht.bootstrapper.getbootstrapsource(bootstrapper.java:154) at org.apache.cassandra.dht.bootstrapper.getbalancedtoken(bootstrapper.java:135) at org.apache.cassandra.dht.bootstrapper.getbootstraptokens(bootstrapper.java:115) at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:666) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:554) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:451) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:348) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:447) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:490) java.lang.runtimeexception: no other nodes seen!  unable to bootstrap.if you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  usually, this can be solved by giving all nodes the same seed list. at org.apache.cassandra.dht.bootstrapper.getbootstrapsource(bootstrapper.java:154) at org.apache.cassandra.dht.bootstrapper.getbalancedtoken(bootstrapper.java:135) at org.apache.cassandra.dht.bootstrapper.getbootstraptokens(bootstrapper.java:115) at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:666) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:554) at org.apache.cassandra.service.storageservice.initserver(storageservice.java:451) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:348) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:447) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:490) exception encountered during startup: no other nodes seen!  unable to bootstrap.if you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  usually, this can be solved by giving all nodes the same seed list. error 16:35:13,668 exception in thread thread[storageserviceshutdownhook,5,main] java.lang.nullpointerexception at org.apache.cassandra.service.storageservice.stoprpcserver(storageservice.java:321) at org.apache.cassandra.service.storageservice.shutdownclientservers(storageservice.java:370) at org.apache.cassandra.service.storageservice.access$000(storageservice.java:88) at org.apache.cassandra.service.storageservice$1.runmaythrow(storageservice.java:519) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.lang.thread.run(thread.java:724) <code> <text> it's clearly stated that setting listen_address to 0.0.0.0 is always wrong. but if you mistakenly do it anyway you end up with an npe on 1.2.8 while it's not the case on 2.0.0-rc1. see bellow:",
        "label": 85
    },
    {
        "text": "nodetool status reports negative load with vnodes disabled <description> when i run stress on a large cluster without vnodes (num_token =1 initial token set) the loads reported by nodetool status are negative, or become negative after stress is run. un  10.97.155.31    -447426217 bytes  1       0.2%    8d40568c-044c-4753-be26-4ab62710beba  rack1                                                                                                               un  10.9.132.53     -447342449 bytes  1       0.2%    58e7f255-803d-493b-a19e-58137466fb78  rack1                                                                                                               un  10.37.151.202   -447298672 bytes  1       0.2%    ba29b1f1-186f-45d0-9e59-6a528db8df5d  rack1          <stacktrace> <code> un  10.97.155.31    -447426217 bytes  1       0.2%    8d40568c-044c-4753-be26-4ab62710beba  rack1                                                                                                               un  10.9.132.53     -447342449 bytes  1       0.2%    58e7f255-803d-493b-a19e-58137466fb78  rack1                                                                                                               un  10.37.151.202   -447298672 bytes  1       0.2%    ba29b1f1-186f-45d0-9e59-6a528db8df5d  rack1           <text> when i run stress on a large cluster without vnodes (num_token =1 initial token set) the loads reported by nodetool status are negative, or become negative after stress is run.",
        "label": 321
    },
    {
        "text": "leak detected while bringing nodes up and down when under stress  <description> so after cassandra-10688 has been fixed i'm able to reproduce some leaks consistently with my stress test suite that is doing repairs/stress/bringing nodes up and down.  one such example: error [strong-reference-leak-detector:1] 2016-02-03 23:32:38,827  nospamlogger.java:97 - strong self-ref loop detected [/var/lib/cassandra/data/keyspace1/standard1-7cddd4c1cacc11e5aa69f375b464842d/ma-14-big, private java.util.concurrent.scheduledfuture org.apache.cassandra.io.sstable.format.sstablereader$globaltidy.readmetersyncfuture-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, final java.util.concurrent.scheduledthreadpoolexecutor java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.this$0-java.util.concurrent.scheduledthreadpoolexecutor, private final java.util.hashset java.util.concurrent.threadpoolexecutor.workers-java.util.hashset, private transient java.util.hashmap java.util.hashset.map-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, final java.lang.object java.util.hashmap$node.key-java.util.concurrent.threadpoolexecutor$worker, final java.lang.thread java.util.concurrent.threadpoolexecutor$worker.thread-java.lang.thread, private java.lang.threadgroup java.lang.thread.group-java.lang.threadgroup, private final java.lang.threadgroup java.lang.threadgroup.parent-java.lang.threadgroup, java.lang.thread[] java.lang.threadgroup.threads-[ljava.lang.thread;, java.lang.thread[] java.lang.threadgroup.threads-java.lang.thread, private java.lang.runnable java.lang.thread.target-java.util.concurrent.threadpoolexecutor$worker, final java.util.concurrent.threadpoolexecutor java.util.concurrent.threadpoolexecutor$worker.this$0-java.util.concurrent.scheduledthreadpoolexecutor, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, private java.util.concurrent.callable java.util.concurrent.futuretask.callable-java.util.concurrent.executors$runnableadapter, final java.lang.runnable java.util.concurrent.executors$runnableadapter.task-sun.rmi.transport.dgcimpl$1, final sun.rmi.transport.dgcimpl sun.rmi.transport.dgcimpl$1.this$0-sun.rmi.transport.dgcimpl, private java.util.map sun.rmi.transport.dgcimpl.leasetable-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, java.lang.object java.util.hashmap$node.value-sun.rmi.transport.dgcimpl$leaseinfo, java.util.set sun.rmi.transport.dgcimpl$leaseinfo.notifyset-java.util.hashset, private transient java.util.hashmap java.util.hashset.map-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, final java.lang.object java.util.hashmap$node.key-sun.rmi.transport.target, private final sun.rmi.transport.weakref sun.rmi.transport.target.weakimpl-sun.rmi.transport.weakref, private java.lang.object sun.rmi.transport.weakref.strongref-javax.management.remote.rmi.rmijrmpserverimpl, private final java.util.list javax.management.remote.rmi.rmiserverimpl.clientlist-java.util.arraylist, transient java.lang.object[] java.util.arraylist.elementdata-[ljava.lang.object;, transient java.lang.object[] java.util.arraylist.elementdata-java.lang.ref.weakreference, private java.lang.object java.lang.ref.reference.referent-javax.management.remote.rmi.rmiconnectionimpl, private final javax.management.mbeanserver javax.management.remote.rmi.rmiconnectionimpl.mbeanserver-com.sun.jmx.mbeanserver.jmxmbeanserver, private volatile javax.management.mbeanserver com.sun.jmx.mbeanserver.jmxmbeanserver.mbsinterceptor-com.sun.jmx.interceptor.defaultmbeanserverinterceptor, private final transient com.sun.jmx.mbeanserver.repository com.sun.jmx.interceptor.defaultmbeanserverinterceptor.repository-com.sun.jmx.mbeanserver.repository, private final java.util.map com.sun.jmx.mbeanserver.repository.domaintb-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, java.lang.object java.util.hashmap$node.value-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, java.lang.object java.util.hashmap$node.value-com.sun.jmx.mbeanserver.namedobject, private final javax.management.dynamicmbean com.sun.jmx.mbeanserver.namedobject.object-com.sun.jmx.mbeanserver.standardmbeansupport, private final java.lang.object com.sun.jmx.mbeanserver.mbeansupport.resource-org.apache.cassandra.db.columnfamilystore, public final org.apache.cassandra.db.keyspace org.apache.cassandra.db.columnfamilystore.keyspace-org.apache.cassandra.db.keyspace, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-java.util.concurrent.concurrenthashmap, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-org.apache.cassandra.db.columnfamilystore, private final java.util.concurrent.scheduledfuture org.apache.cassandra.db.columnfamilystore.latencycalculator-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, final java.util.concurrent.scheduledthreadpoolexecutor java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.this$0-org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, private java.util.concurrent.callable java.util.concurrent.futuretask.callable-java.util.concurrent.executors$runnableadapter, final java.lang.runnable java.util.concurrent.executors$runnableadapter.task-org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable, private final java.lang.runnable org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.runnable-org.apache.cassandra.db.columnfamilystore$3, final org.apache.cassandra.db.columnfamilystore org.apache.cassandra.db.columnfamilystore$3.this$0-org.apache.cassandra.db.columnfamilystore, public final org.apache.cassandra.db.keyspace org.apache.cassandra.db.columnfamilystore.keyspace-org.apache.cassandra.db.keyspace, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-java.util.concurrent.concurrenthashmap, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-org.apache.cassandra.db.columnfamilystore, private final org.apache.cassandra.db.lifecycle.tracker org.apache.cassandra.db.columnfamilystore.data-org.apache.cassandra.db.lifecycle.tracker, public final java.util.collection org.apache.cassandra.db.lifecycle.tracker.subscribers-java.util.concurrent.copyonwritearraylist, public final java.util.collection org.apache.cassandra.db.lifecycle.tracker.subscribers-org.apache.cassandra.db.compaction.compactionstrategymanager, private volatile org.apache.cassandra.db.compaction.abstractcompactionstrategy org.apache.cassandra.db.compaction.compactionstrategymanager.repaired-org.apache.cassandra.db.compaction.sizetieredcompactionstrategy, private final java.util.set org.apache.cassandra.db.compaction.sizetieredcompactionstrategy.sstables-java.util.hashset, private transient java.util.hashmap java.util.hashset.map-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, final java.lang.object java.util.hashmap$node.key-org.apache.cassandra.io.sstable.format.big.bigtablereader, private final org.apache.cassandra.io.sstable.format.sstablereader$instancetidier org.apache.cassandra.io.sstable.format.sstablereader.tidy-org.apache.cassandra.io.sstable.format.sstablereader$instancetidier, private org.apache.cassandra.utils.concurrent.ref org.apache.cassandra.io.sstable.format.sstablereader$instancetidier.globalref-org.apache.cassandra.utils.concurrent.ref] cc benedict elliott smith<stacktrace> <code> error [strong-reference-leak-detector:1] 2016-02-03 23:32:38,827  nospamlogger.java:97 - strong self-ref loop detected [/var/lib/cassandra/data/keyspace1/standard1-7cddd4c1cacc11e5aa69f375b464842d/ma-14-big, private java.util.concurrent.scheduledfuture org.apache.cassandra.io.sstable.format.sstablereader$globaltidy.readmetersyncfuture-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, final java.util.concurrent.scheduledthreadpoolexecutor java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.this$0-java.util.concurrent.scheduledthreadpoolexecutor, private final java.util.hashset java.util.concurrent.threadpoolexecutor.workers-java.util.hashset, private transient java.util.hashmap java.util.hashset.map-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, final java.lang.object java.util.hashmap$node.key-java.util.concurrent.threadpoolexecutor$worker, final java.lang.thread java.util.concurrent.threadpoolexecutor$worker.thread-java.lang.thread, private java.lang.threadgroup java.lang.thread.group-java.lang.threadgroup, private final java.lang.threadgroup java.lang.threadgroup.parent-java.lang.threadgroup, java.lang.thread[] java.lang.threadgroup.threads-[ljava.lang.thread;, java.lang.thread[] java.lang.threadgroup.threads-java.lang.thread, private java.lang.runnable java.lang.thread.target-java.util.concurrent.threadpoolexecutor$worker, final java.util.concurrent.threadpoolexecutor java.util.concurrent.threadpoolexecutor$worker.this$0-java.util.concurrent.scheduledthreadpoolexecutor, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, private java.util.concurrent.callable java.util.concurrent.futuretask.callable-java.util.concurrent.executors$runnableadapter, final java.lang.runnable java.util.concurrent.executors$runnableadapter.task-sun.rmi.transport.dgcimpl$1, final sun.rmi.transport.dgcimpl sun.rmi.transport.dgcimpl$1.this$0-sun.rmi.transport.dgcimpl, private java.util.map sun.rmi.transport.dgcimpl.leasetable-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, java.lang.object java.util.hashmap$node.value-sun.rmi.transport.dgcimpl$leaseinfo, java.util.set sun.rmi.transport.dgcimpl$leaseinfo.notifyset-java.util.hashset, private transient java.util.hashmap java.util.hashset.map-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, final java.lang.object java.util.hashmap$node.key-sun.rmi.transport.target, private final sun.rmi.transport.weakref sun.rmi.transport.target.weakimpl-sun.rmi.transport.weakref, private java.lang.object sun.rmi.transport.weakref.strongref-javax.management.remote.rmi.rmijrmpserverimpl, private final java.util.list javax.management.remote.rmi.rmiserverimpl.clientlist-java.util.arraylist, transient java.lang.object[] java.util.arraylist.elementdata-[ljava.lang.object;, transient java.lang.object[] java.util.arraylist.elementdata-java.lang.ref.weakreference, private java.lang.object java.lang.ref.reference.referent-javax.management.remote.rmi.rmiconnectionimpl, private final javax.management.mbeanserver javax.management.remote.rmi.rmiconnectionimpl.mbeanserver-com.sun.jmx.mbeanserver.jmxmbeanserver, private volatile javax.management.mbeanserver com.sun.jmx.mbeanserver.jmxmbeanserver.mbsinterceptor-com.sun.jmx.interceptor.defaultmbeanserverinterceptor, private final transient com.sun.jmx.mbeanserver.repository com.sun.jmx.interceptor.defaultmbeanserverinterceptor.repository-com.sun.jmx.mbeanserver.repository, private final java.util.map com.sun.jmx.mbeanserver.repository.domaintb-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, java.lang.object java.util.hashmap$node.value-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, java.lang.object java.util.hashmap$node.value-com.sun.jmx.mbeanserver.namedobject, private final javax.management.dynamicmbean com.sun.jmx.mbeanserver.namedobject.object-com.sun.jmx.mbeanserver.standardmbeansupport, private final java.lang.object com.sun.jmx.mbeanserver.mbeansupport.resource-org.apache.cassandra.db.columnfamilystore, public final org.apache.cassandra.db.keyspace org.apache.cassandra.db.columnfamilystore.keyspace-org.apache.cassandra.db.keyspace, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-java.util.concurrent.concurrenthashmap, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-org.apache.cassandra.db.columnfamilystore, private final java.util.concurrent.scheduledfuture org.apache.cassandra.db.columnfamilystore.latencycalculator-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, final java.util.concurrent.scheduledthreadpoolexecutor java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.this$0-org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$delayedworkqueue, private final java.util.concurrent.blockingqueue java.util.concurrent.threadpoolexecutor.workqueue-java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask, private java.util.concurrent.callable java.util.concurrent.futuretask.callable-java.util.concurrent.executors$runnableadapter, final java.lang.runnable java.util.concurrent.executors$runnableadapter.task-org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable, private final java.lang.runnable org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.runnable-org.apache.cassandra.db.columnfamilystore$3, final org.apache.cassandra.db.columnfamilystore org.apache.cassandra.db.columnfamilystore$3.this$0-org.apache.cassandra.db.columnfamilystore, public final org.apache.cassandra.db.keyspace org.apache.cassandra.db.columnfamilystore.keyspace-org.apache.cassandra.db.keyspace, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-java.util.concurrent.concurrenthashmap, private final java.util.concurrent.concurrentmap org.apache.cassandra.db.keyspace.columnfamilystores-org.apache.cassandra.db.columnfamilystore, private final org.apache.cassandra.db.lifecycle.tracker org.apache.cassandra.db.columnfamilystore.data-org.apache.cassandra.db.lifecycle.tracker, public final java.util.collection org.apache.cassandra.db.lifecycle.tracker.subscribers-java.util.concurrent.copyonwritearraylist, public final java.util.collection org.apache.cassandra.db.lifecycle.tracker.subscribers-org.apache.cassandra.db.compaction.compactionstrategymanager, private volatile org.apache.cassandra.db.compaction.abstractcompactionstrategy org.apache.cassandra.db.compaction.compactionstrategymanager.repaired-org.apache.cassandra.db.compaction.sizetieredcompactionstrategy, private final java.util.set org.apache.cassandra.db.compaction.sizetieredcompactionstrategy.sstables-java.util.hashset, private transient java.util.hashmap java.util.hashset.map-java.util.hashmap, transient java.util.hashmap$node[] java.util.hashmap.table-[ljava.util.hashmap$node;, transient java.util.hashmap$node[] java.util.hashmap.table-java.util.hashmap$node, final java.lang.object java.util.hashmap$node.key-org.apache.cassandra.io.sstable.format.big.bigtablereader, private final org.apache.cassandra.io.sstable.format.sstablereader$instancetidier org.apache.cassandra.io.sstable.format.sstablereader.tidy-org.apache.cassandra.io.sstable.format.sstablereader$instancetidier, private org.apache.cassandra.utils.concurrent.ref org.apache.cassandra.io.sstable.format.sstablereader$instancetidier.globalref-org.apache.cassandra.utils.concurrent.ref] <text> so after cassandra-10688 has been fixed i'm able to reproduce some leaks consistently with my stress test suite that is doing repairs/stress/bringing nodes up and down.  one such example: cc benedict elliott smith",
        "label": 52
    },
    {
        "text": "add forceuserdefinedcleanup to allow more flexible cleanup for operators <description> nodetool cleanup currently executes in parallel on all sstables in a table. no source sstables are gcd until the parallel operation completes. in certain scenarios, this is nonideal (it has both memory and disk usage implications for operators who try to run the operation on larger tables). adding forceuserdefinedcleanup puts cleanup operations closer to parity with compaction forceuserdefinedcompaction for the rare cases where operators need to do something slightly different than the traditional cleanup.<stacktrace> <code> <text> nodetool cleanup currently executes in parallel on all sstables in a table. no source sstables are gcd until the parallel operation completes. in certain scenarios, this is nonideal (it has both memory and disk usage implications for operators who try to run the operation on larger tables). adding forceuserdefinedcleanup puts cleanup operations closer to parity with compaction forceuserdefinedcompaction for the rare cases where operators need to do something slightly different than the traditional cleanup.",
        "label": 241
    },
    {
        "text": "mark test populate mv after insert wide rows as flaky <description> see cassandra-15845. this test can still fail in a flaky way so we better mark it as such to avoid confusion and dup investigation efforts on failing tests<stacktrace> <code> <text> see cassandra-15845. this test can still fail in a flaky way so we better mark it as such to avoid confusion and dup investigation efforts on failing tests",
        "label": 73
    },
    {
        "text": "cqlsh describe keyspace returns  'nonetype' object has no attribute 'get usertypes names'  <description> start a fresh cluster on trunk and try to describe any keyspace : ccm create -v git:trunk test ccm populate -n 1 ccm start ccm node1 cqlsh cqlsh> describe keyspace system; create keyspace system with replication = {   'class': 'localstrategy' }; 'nonetype' object has no attribute 'get_usertypes_names'<stacktrace> <code> ccm create -v git:trunk test ccm populate -n 1 ccm start ccm node1 cqlsh cqlsh> describe keyspace system; create keyspace system with replication = {   'class': 'localstrategy' }; 'nonetype' object has no attribute 'get_usertypes_names' <text> start a fresh cluster on trunk and try to describe any keyspace :",
        "label": 362
    },
    {
        "text": "allow sstableloader to handle a larger number of files <description> with the default heap size, sstableloader will oom when there are roughly 25k files in the directory to load. it's easy to reach this number of files in a single lcs column family. by avoiding creating all sstablereaders up front in sstableloader, we should be able to increase the number of files that sstableloader can handle considerably.<stacktrace> <code> <text> with the default heap size, sstableloader will oom when there are roughly 25k files in the directory to load. it's easy to reach this number of files in a single lcs column family. by avoiding creating all sstablereaders up front in sstableloader, we should be able to increase the number of files that sstableloader can handle considerably.",
        "label": 538
    },
    {
        "text": "inputstreams not closed <description> inputstreams are not closed properly in few places.<stacktrace> <code> <text> inputstreams are not closed properly in few places.",
        "label": 360
    },
    {
        "text": "checking if an unlogged batch is local is inefficient <description> based on cassandra-11363 report i noticed that on cassandra-9303 we introduced the following check to avoid printing a warn in case an unlogged batch statement is local:              for (imutation im : mutations)              {                  keyset.add(im.key());                  for (columnfamily cf : im.getcolumnfamilies())                      kscfpairs.add(string.format(\"%s.%s\", cf.metadata().ksname, cf.metadata().cfname)); + +                if (localmutationsonly) +                    localmutationsonly &= ismutationlocal(localtokensbyks, im);              }   +            // cassandra-9303: if we only have local mutations we do not warn +            if (localmutationsonly) +                return; +              nospamlogger.log(logger, nospamlogger.level.warn, 1, timeunit.minutes, unloggedbatchwarning,                               keyset.size(), keyset.size() == 1 ? \"\" : \"s\",                               kscfpairs.size() == 1 ? \"\" : \"s\", kscfpairs); the ismutationlocal check uses storageservice.instance.getlocalranges(mutation.getkeyspacename()), which underneaths uses abstractreplication.getaddressranges to calculate local ranges. recalculating this at every unlogged batch can be pretty inefficient, so we should at the very least cache it every time the ring changes.<stacktrace> <code>              for (imutation im : mutations)              {                  keyset.add(im.key());                  for (columnfamily cf : im.getcolumnfamilies())                      kscfpairs.add(string.format('%s.%s', cf.metadata().ksname, cf.metadata().cfname)); + +                if (localmutationsonly) +                    localmutationsonly &= ismutationlocal(localtokensbyks, im);              }   +            // cassandra-9303: if we only have local mutations we do not warn +            if (localmutationsonly) +                return; +              nospamlogger.log(logger, nospamlogger.level.warn, 1, timeunit.minutes, unloggedbatchwarning,                               keyset.size(), keyset.size() == 1 ? '' : 's',                               kscfpairs.size() == 1 ? '' : 's', kscfpairs); <text> based on cassandra-11363 report i noticed that on cassandra-9303 we introduced the following check to avoid printing a warn in case an unlogged batch statement is local: the ismutationlocal check uses storageservice.instance.getlocalranges(mutation.getkeyspacename()), which underneaths uses abstractreplication.getaddressranges to calculate local ranges. recalculating this at every unlogged batch can be pretty inefficient, so we should at the very least cache it every time the ring changes.",
        "label": 508
    },
    {
        "text": "indexoutofboundsexception inserting into tupletype <description> trying to run this query on the 2.2 branch resulted in indexoutofboundsexception: insert into datatypes.alltypes (pk,c_tuple) values (1,'01:02:03.456789012') the column c_tuple is of type tuple<int>. the cause seems to be that tupletype.fromstring is splitting the source string on : (meant for udt only?) but does not consider that the number of those fields may exceed the number of types defined in the tuple. a simple fix would be to bound the for loop with types.size() instead of fieldstrings.size() but still some error should be raised if fieldstrings.size() > types.size(). java.lang.indexoutofboundsexception: index: 1, size: 1         at java.util.arraylist.rangecheck(arraylist.java:635) ~[na:1.7.0_51]         at java.util.arraylist.get(arraylist.java:411) ~[na:1.7.0_51]         at org.apache.cassandra.db.marshal.tupletype.type(tupletype.java:60) ~[main/:na]         at org.apache.cassandra.db.marshal.tupletype.fromstring(tupletype.java:224) ~[main/:na]         at org.apache.cassandra.cql3.constants$literal.parsedvalue(constants.java:152) ~[main/:na]         at org.apache.cassandra.cql3.constants$literal.prepare(constants.java:138) ~[main/:na]         at org.apache.cassandra.cql3.constants$literal.prepare(constants.java:91) ~[main/:na]         at org.apache.cassandra.cql3.operation$setvalue.prepare(operation.java:167) ~[main/:na]         at org.apache.cassandra.cql3.statements.updatestatement$parsedinsert.prepareinternal(updatestatement.java:213) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement$parsed.prepare(modificationstatement.java:710) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement$parsed.prepare(modificationstatement.java:698) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.getstatement(queryprocessor.java:493) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:247) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:241) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:119) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_51]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:744) [na:1.7.0_51]<stacktrace> java.lang.indexoutofboundsexception: index: 1, size: 1         at java.util.arraylist.rangecheck(arraylist.java:635) ~[na:1.7.0_51]         at java.util.arraylist.get(arraylist.java:411) ~[na:1.7.0_51]         at org.apache.cassandra.db.marshal.tupletype.type(tupletype.java:60) ~[main/:na]         at org.apache.cassandra.db.marshal.tupletype.fromstring(tupletype.java:224) ~[main/:na]         at org.apache.cassandra.cql3.constants$literal.parsedvalue(constants.java:152) ~[main/:na]         at org.apache.cassandra.cql3.constants$literal.prepare(constants.java:138) ~[main/:na]         at org.apache.cassandra.cql3.constants$literal.prepare(constants.java:91) ~[main/:na]         at org.apache.cassandra.cql3.operation$setvalue.prepare(operation.java:167) ~[main/:na]         at org.apache.cassandra.cql3.statements.updatestatement$parsedinsert.prepareinternal(updatestatement.java:213) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement$parsed.prepare(modificationstatement.java:710) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement$parsed.prepare(modificationstatement.java:698) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.getstatement(queryprocessor.java:493) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:247) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:241) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:119) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_51]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:744) [na:1.7.0_51] <code> insert into datatypes.alltypes (pk,c_tuple) values (1,'01:02:03.456789012') <text> trying to run this query on the 2.2 branch resulted in indexoutofboundsexception: the column c_tuple is of type tuple<int>. the cause seems to be that tupletype.fromstring is splitting the source string on : (meant for udt only?) but does not consider that the number of those fields may exceed the number of types defined in the tuple. a simple fix would be to bound the for loop with types.size() instead of fieldstrings.size() but still some error should be raised if fieldstrings.size() > types.size().",
        "label": 69
    },
    {
        "text": "bloom filter fp chance needs to be validated up front <description> hi, i was doing some bench-marking on bloom_filter_fp_chance values. everything worked fine for values .01(default for stcs), .001, .0001. but when i set bloom_filter_fp_chance = .00001 i observed following behaviour: 1). reads and writes looked normal from cqlsh.  2). ssttables are never created.  3). it just creates two files (*-data.db and *-index.db) of size 0kb.  4). nodetool flush does not work and produce following exception: java.lang.unsupportedoperationexception: unable to satisfy 1.0e-5 with 20 buckets per element  at org.apache.cassandra.utils.bloomcalculations.computebloomspec(bloomcalculations.java:150) ..... i checked bloomcalculations class and following lines are responsible for this exception: if (maxfalseposprob < probs[maxbucketsperelement][maxk]) { throw new unsupportedoperationexception(string.format(\"unable to satisfy %s with %s buckets per element\", maxfalseposprob, maxbucketsperelement)); } from the code it looks like a hard coaded validation (unless we can change the nuber of buckets).  so, if this validation is hard coaded then why it is even allowed to set such value of bloom_fileter_fp_chance, that can prevent sstable generation? please correct this issue.<stacktrace> <code> if (maxfalseposprob < probs[maxbucketsperelement][maxk]) <text> hi, i was doing some bench-marking on bloom_filter_fp_chance values. everything worked fine for values .01(default for stcs), .001, .0001. but when i set bloom_filter_fp_chance = .00001 i observed following behaviour: 1). reads and writes looked normal from cqlsh.  2). ssttables are never created.  3). it just creates two files (*-data.db and *-index.db) of size 0kb.  4). nodetool flush does not work and produce following exception: java.lang.unsupportedoperationexception: unable to satisfy 1.0e-5 with 20 buckets per element  at org.apache.cassandra.utils.bloomcalculations.computebloomspec(bloomcalculations.java:150) ..... i checked bloomcalculations class and following lines are responsible for this exception: from the code it looks like a hard coaded validation (unless we can change the nuber of buckets).  so, if this validation is hard coaded then why it is even allowed to set such value of bloom_fileter_fp_chance, that can prevent sstable generation? please correct this issue.",
        "label": 54
    },
    {
        "text": "expose private listen address in system local <description> we had some hopes cassandra-9436 would add it, yet it added rpc_address instead of both rpc_address and listen_address. we really need listen_address here, because we need to get information on the private ip c* binds to. knowing this we could better match spark nodes to c* nodes and process data locally in environments where rpc_address != listen_address like ec2. see, spark does not know rpc addresses nor it has a concept of broadcast address. it only knows the hostname / ip its workers bind to. in case of cloud environments, these are private ips. now if we give spark a set of c* nodes identified by rpc_addresses, spark doesn't recognize them as belonging to the same cluster. it treats them as \"remote\" nodes and has no idea where to send tasks optimally. current situation (example):  spark worker nodes: [10.0.0.1, 10.0.0.2, 10.0.0.3]  c* nodes: [10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com]  what the application knows about the cluster: [node1.blah.ec2.com, node2.blah.ec2.com, node3.blah.ec2.com]  what the application sends to spark for execution:  task1 - please execute on node1.blah.ec2.com  task2 - please execute on node2.blah.ec2.com  task3 - please execute on node3.blah.ec2.com  how spark understands it: \"i have no idea what node1.blah.ec2.com is, let's assign task1 it to a random node\" expected:  spark worker nodes: [10.0.0.1, 10.0.0.2, 10.0.0.3]  c* nodes: [10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com]  what the application knows about the cluster: [10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com]  what the application sends to spark for execution:  task1 - please execute on node1.blah.ec2.com or 10.0.0.1  task2 - please execute on node2.blah.ec2.com or 10.0.0.2  task3 - please execute on node3.blah.ec2.com or 10.0.0.3  how spark understands it: \"10.0.0.1? - i have a worker on that node, lets put task 1 there\"<stacktrace> <code> <text> we had some hopes cassandra-9436 would add it, yet it added rpc_address instead of both rpc_address and listen_address. we really need listen_address here, because we need to get information on the private ip c* binds to. knowing this we could better match spark nodes to c* nodes and process data locally in environments where rpc_address != listen_address like ec2. see, spark does not know rpc addresses nor it has a concept of broadcast address. it only knows the hostname / ip its workers bind to. in case of cloud environments, these are private ips. now if we give spark a set of c* nodes identified by rpc_addresses, spark doesn't recognize them as belonging to the same cluster. it treats them as 'remote' nodes and has no idea where to send tasks optimally. current situation (example):  spark worker nodes: [10.0.0.1, 10.0.0.2, 10.0.0.3]  c* nodes: [10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com]  what the application knows about the cluster: [node1.blah.ec2.com, node2.blah.ec2.com, node3.blah.ec2.com]  what the application sends to spark for execution:  task1 - please execute on node1.blah.ec2.com  task2 - please execute on node2.blah.ec2.com  task3 - please execute on node3.blah.ec2.com  how spark understands it: 'i have no idea what node1.blah.ec2.com is, let's assign task1 it to a random node' expected:  spark worker nodes: [10.0.0.1, 10.0.0.2, 10.0.0.3]  c* nodes: [10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com]  what the application knows about the cluster: [10.0.0.1 / node1.blah.ec2.com, 10.0.0.2 / node2.blah.ec2.com, 10.0.0.3 / node3.blah.ec2.com]  what the application sends to spark for execution:  task1 - please execute on node1.blah.ec2.com or 10.0.0.1  task2 - please execute on node2.blah.ec2.com or 10.0.0.2  task3 - please execute on node3.blah.ec2.com or 10.0.0.3  how spark understands it: '10.0.0.1? - i have a worker on that node, lets put task 1 there'",
        "label": 98
    },
    {
        "text": "remove unused variables <description> there are a number of variables sprinkled around in the code that are never read from. we should remove these.<stacktrace> <code> <text> there are a number of variables sprinkled around in the code that are never read from. we should remove these.",
        "label": 264
    },
    {
        "text": "typo in sstablerepairedset <description> <stacktrace> <code> <text> ",
        "label": 142
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh copy tests cqlshcopytest test round trip with different number precision <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1284/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_round_trip_with_different_number_precision failed on cassci build trunk_dtest #1284 stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/tools.py\", line 288, in wrapped     f(obj)   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 2046, in test_round_trip_with_different_number_precision     do_test(none, none)   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 2044, in do_test     self.assertitemsequal(sorted(list(csv_rows(tempfile1.name))), sorted(list(csv_rows(tempfile2.name))))   file \"/usr/lib/python2.7/unittest/case.py\", line 901, in assertitemsequal     self.fail(msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 410, in fail     raise self.failureexception(msg) \"element counts were not equal:\\nfirst has 1, second has 0:  ['1', '1.1235', '1.12345678912'] logs are attached.<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/tools.py', line 288, in wrapped     f(obj)   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 2046, in test_round_trip_with_different_number_precision     do_test(none, none)   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 2044, in do_test     self.assertitemsequal(sorted(list(csv_rows(tempfile1.name))), sorted(list(csv_rows(tempfile2.name))))   file '/usr/lib/python2.7/unittest/case.py', line 901, in assertitemsequal     self.fail(msg)   file '/usr/lib/python2.7/unittest/case.py', line 410, in fail     raise self.failureexception(msg) 'element counts were not equal:/nfirst has 1, second has 0:  ['1', '1.1235', '1.12345678912'] http://cassci.datastax.com/job/trunk_dtest/1284/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_round_trip_with_different_number_precision <text> example failure: failed on cassci build trunk_dtest #1284 logs are attached.",
        "label": 261
    },
    {
        "text": "test pep8 compliance   cqlsh tests cqlsh tests testcqlsh  pep8 has been renamed to pycodestyle  github issue   <description> test_pep8_compliance - cqlsh_tests.cqlsh_tests.testcqlsh always fails due to us catching a informative warning from the pip8 tool.. looks like we just need to swap out the usage /home/cassandra/env/local/lib/python2.7/site-packages/pep8.py:2124: userwarning: pep8 has been renamed to pycodestyle (github issue #466)  use of the pep8 tool will be removed in a future release.  please install and use `pycodestyle` instead. $ pip install pycodestyle  $ pycodestyle ...  '\\n\\n'<stacktrace> <code> /home/cassandra/env/local/lib/python2.7/site-packages/pep8.py:2124: userwarning: '/n/n'<text> test_pep8_compliance - cqlsh_tests.cqlsh_tests.testcqlsh always fails due to us catching a informative warning from the pip8 tool.. looks like we just need to swap out the usage pep8 has been renamed to pycodestyle (github issue #466)  use of the pep8 tool will be removed in a future release.  please install and use `pycodestyle` instead. $ pip install pycodestyle  $ pycodestyle ... ",
        "label": 344
    },
    {
        "text": "cqlsh should handle 'null' as session duration <description> mysteriously, tracing doesn't fail all the time. if i run the query multiple times at different consistency levels, tracing sometimes starts working. cqlsh:some_keyspace> tracing on; cqlsh:some_keyspace> select * from appservers;  key     | status ---------+--------  server1 |      1  server2 |      1  server3 |      1 unsupported operand type(s) for /: 'nonetype' and 'float'<stacktrace> <code> cqlsh:some_keyspace> tracing on; cqlsh:some_keyspace> select * from appservers;  key     | status ---------+--------  server1 |      1  server2 |      1  server3 |      1 unsupported operand type(s) for /: 'nonetype' and 'float' <text> mysteriously, tracing doesn't fail all the time. if i run the query multiple times at different consistency levels, tracing sometimes starts working.",
        "label": 362
    },
    {
        "text": "use atomic fieldupdater to save memory <description> followup to cassandra-6278, use atomic*fieldupdater in;  atomicsortedcolumns  readcallback  writeresponsehandler<stacktrace> <code> <text> followup to cassandra-6278, use atomic*fieldupdater in;  atomicsortedcolumns  readcallback  writeresponsehandler",
        "label": 321
    },
    {
        "text": "rename 'table'   'keyspace' in public apis <description> thrift.cfdef uses the name 'table' rather than 'keyspace'. we need to make sure that all of our public apis use consistent naming, despite the fact that our private apis won't change until 0.7 is branched.<stacktrace> <code> <text> thrift.cfdef uses the name 'table' rather than 'keyspace'. we need to make sure that all of our public apis use consistent naming, despite the fact that our private apis won't change until 0.7 is branched.",
        "label": 515
    },
    {
        "text": "assertionerror for rows with zero columns <description> after upgrading from 1.2.5 to 1.2.9 and then to 2.0.2 we've got those exceptions: error [flushwriter:1] 2013-11-18 16:14:36,305 cassandradaemon.java (line 187) exception in thread thread[flushwriter:1,5,main] java.lang.assertionerror         at org.apache.cassandra.io.sstable.sstablewriter.rawappend(sstablewriter.java:198)         at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:186)         at org.apache.cassandra.db.memtable$flushrunnable.writesortedcontents(memtable.java:360)         at org.apache.cassandra.db.memtable$flushrunnable.runwith(memtable.java:315)         at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskawarerunnable.java:48)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)         at java.lang.thread.run(thread.java:722) also found similar issue in this thread:  http://www.mail-archive.com/user@cassandra.apache.org/msg32875.html  there aaron morton said that its caused by leaving rows with zero columns - that's exactly what we do in some cfs (using thrift & astyanax).<stacktrace> error [flushwriter:1] 2013-11-18 16:14:36,305 cassandradaemon.java (line 187) exception in thread thread[flushwriter:1,5,main] java.lang.assertionerror         at org.apache.cassandra.io.sstable.sstablewriter.rawappend(sstablewriter.java:198)         at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:186)         at org.apache.cassandra.db.memtable$flushrunnable.writesortedcontents(memtable.java:360)         at org.apache.cassandra.db.memtable$flushrunnable.runwith(memtable.java:315)         at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskawarerunnable.java:48)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)         at java.lang.thread.run(thread.java:722) <code> <text> after upgrading from 1.2.5 to 1.2.9 and then to 2.0.2 we've got those exceptions: also found similar issue in this thread:  http://www.mail-archive.com/user@cassandra.apache.org/msg32875.html  there aaron morton said that its caused by leaving rows with zero columns - that's exactly what we do in some cfs (using thrift & astyanax).",
        "label": 274
    },
    {
        "text": "cqlsh tests test simple insert dtest fails on <description> $ print_debug=true nosetests --nocapture --nologcapture --verbosity=3 cqlsh_tests.py:testcqlsh.test_simple_insert nose.config: info: ignoring files matching ['^\\\\.', '^_', '^setup\\\\.py$'] test_simple_insert (cqlsh_tests.testcqlsh) ... cluster ccm directory: /tmp/dtest-acgjrr (ee) connection error: ('unable to connect to any servers', {'127.0.0.1': error(111, 'econnrefused')}) error ====================================================================== error: test_simple_insert (cqlsh_tests.testcqlsh) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/cqlsh_tests.py\", line 34, in test_simple_insert     cursor.execute(\"select id, value from simple.simple\");   file \"/home/mshuler/git/cassandra-dbapi2/cql/cursor.py\", line 80, in execute     response = self.get_response(prepared_q, cl)   file \"/home/mshuler/git/cassandra-dbapi2/cql/thrifteries.py\", line 78, in get_response     return self.handle_cql_execution_errors(doquery, compressed_q, compress, cl)   file \"/home/mshuler/git/cassandra-dbapi2/cql/thrifteries.py\", line 100, in handle_cql_execution_errors     raise cql.programmingerror(\"bad request: %s\" % ire.why) programmingerror: bad request: keyspace simple does not exist ---------------------------------------------------------------------- ran 1 test in 4.278s failed (errors=1)<stacktrace> <code> $ print_debug=true nosetests --nocapture --nologcapture --verbosity=3 cqlsh_tests.py:testcqlsh.test_simple_insert nose.config: info: ignoring files matching ['^//.', '^_', '^setup//.py$'] test_simple_insert (cqlsh_tests.testcqlsh) ... cluster ccm directory: /tmp/dtest-acgjrr (ee) connection error: ('unable to connect to any servers', {'127.0.0.1': error(111, 'econnrefused')}) error ====================================================================== error: test_simple_insert (cqlsh_tests.testcqlsh) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/cqlsh_tests.py', line 34, in test_simple_insert     cursor.execute('select id, value from simple.simple');   file '/home/mshuler/git/cassandra-dbapi2/cql/cursor.py', line 80, in execute     response = self.get_response(prepared_q, cl)   file '/home/mshuler/git/cassandra-dbapi2/cql/thrifteries.py', line 78, in get_response     return self.handle_cql_execution_errors(doquery, compressed_q, compress, cl)   file '/home/mshuler/git/cassandra-dbapi2/cql/thrifteries.py', line 100, in handle_cql_execution_errors     raise cql.programmingerror('bad request: %s' % ire.why) programmingerror: bad request: keyspace simple does not exist ---------------------------------------------------------------------- ran 1 test in 4.278s failed (errors=1)<text> ",
        "label": 470
    },
    {
        "text": "memtables do not need to be flushed on the table apply  path anymore after <description> 2449 removes auto-flush from table.apply(), but the data structure is still there, no harm, but better remove it: in  https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/table.java line 399 and 470<stacktrace> <code> in  https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/table.java <text> 2449 removes auto-flush from table.apply(), but the data structure is still there, no harm, but better remove it: line 399 and 470",
        "label": 571
    },
    {
        "text": "cassandra changed the behavior of  clustering order  on  create table  <description> cassandra-13426 changed the behavior of \"clustering order\" on \"create table\", it now complains if you don't specify all the columns. it was nice that previously you could just specify to make the first clustering desc and leave the rest asc without needing to specify them.  also it would be nice i think to avoid breaking changes to the create table syntax. we should either update news.txt to call out the breaking change there, or update the new code to be able to default the columns which were not mentioned.<stacktrace> <code> <text> cassandra-13426 changed the behavior of 'clustering order' on 'create table', it now complains if you don't specify all the columns. it was nice that previously you could just specify to make the first clustering desc and leave the rest asc without needing to specify them.  also it would be nice i think to avoid breaking changes to the create table syntax. we should either update news.txt to call out the breaking change there, or update the new code to be able to default the columns which were not mentioned.",
        "label": 165
    },
    {
        "text": "throw an error when auto bootstrap  true and bootstrapping node is listed in seeds <description> obviously when this condition exists the node will not bootstrap. but it is not obvious from the logs why it is not bootstrapping. throwing an error would make it obvious and therefore faster to correct.<stacktrace> <code> <text> obviously when this condition exists the node will not bootstrap. but it is not obvious from the logs why it is not bootstrapping. throwing an error would make it obvious and therefore faster to correct.",
        "label": 85
    },
    {
        "text": "race condition leads to filenotfoundexception on startup <description> on startup locationinfo file is deleted then attempted to be read from. steps to reproduce: kill then quickly restart switching to parallelgc to avoid cms/compressedoops incompatibility  info 17:05:08,680 diskaccessmode isstandard, indexaccessmode is mmap  info 17:05:08,786 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,797 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,807 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,833 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,834 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,839 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,862 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,864 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,876 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,885 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,892 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,893 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,897 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,901 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,906 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,909 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,918 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,922 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,928 creating new commitlog segment /var/lib/cassandra/commitlog/commitlog-1281571508928.log  info 17:05:08,933 deleted /var/lib/cassandra/data/system/locationinfo-e-16-data.db  info 17:05:08,936 deleted /var/lib/cassandra/data/system/locationinfo-e-15-data.db  info 17:05:08,936 sampling index for /var/lib/cassandra/data/system/locationinfo-e-16-<>  error 17:05:08,937 corrupt file /var/lib/cassandra/data/system/locationinfo-e-16-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-16-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,947 deleted /var/lib/cassandra/data/system/locationinfo-e-14-data.db  info 17:05:08,947 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,948 sampling index for /var/lib/cassandra/data/system/locationinfo-e-15-<>  error 17:05:08,948 corrupt file /var/lib/cassandra/data/system/locationinfo-e-15-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-15-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,950 sampling index for /var/lib/cassandra/data/system/locationinfo-e-16-<>  error 17:05:08,951 corrupt file /var/lib/cassandra/data/system/locationinfo-e-16-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-16-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,970 deleted /var/lib/cassandra/data/system/locationinfo-e-13-data.db  info 17:05:08,971 sampling index for /var/lib/cassandra/data/system/locationinfo-e-14-<>  error 17:05:08,971 corrupt file /var/lib/cassandra/data/system/locationinfo-e-14-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-14-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,972 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,973 sampling index for /var/lib/cassandra/data/system/locationinfo-e-15-<>  error 17:05:08,973 corrupt file /var/lib/cassandra/data/system/locationinfo-e-15-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-15-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,974 sampling index for /var/lib/cassandra/data/system/locationinfo-e-16-<>  error 17:05:08,974 corrupt file /var/lib/cassandra/data/system/locationinfo-e-16-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-16-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,996 loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc  warn 17:05:09,158 schema definitions were defined both locally and in cassandra.yaml. definitions in cassandra.yaml were ignored.  info 17:05:09,164 replaying /var/lib/cassandra/commitlog/commitlog-1281571453475.log, /var/lib/cassandra/commitlog/commitlog-1281571508928.log  info 17:05:09,172 finished reading /var/lib/cassandra/commitlog/commitlog-1281571453475.log  info 17:05:09,172 finished reading /var/lib/cassandra/commitlog/commitlog-1281571508928.log  info 17:05:09,173 switching in a fresh memtable for locationinfo at commitlogcontext(file='/var/lib/cassandra/commitlog/commitlog-1281571508928.log', position=592)  info 17:05:09,183 enqueuing flush of memtable-locationinfo@137493297(17 bytes, 1 operations)  info 17:05:09,183 writing memtable-locationinfo@137493297(17 bytes, 1 operations)  info 17:05:09,184 switching in a fresh memtable for statistics at commitlogcontext(file='/var/lib/cassandra/commitlog/commitlog-1281571508928.log', position=592)  info 17:05:09,184 enqueuing flush of memtable-statistics@86823325(0 bytes, 0 operations)  info 17:05:09,265 completed flushing /var/lib/cassandra/data/system/locationinfo-e-18-data.db  info 17:05:09,273 writing memtable-statistics@86823325(0 bytes, 0 operations)  info 17:05:09,352 completed flushing /var/lib/cassandra/data/system/statistics-e-1-data.db  info 17:05:09,353 recovery complete<stacktrace> switching to parallelgc to avoid cms/compressedoops incompatibility  info 17:05:08,680 diskaccessmode isstandard, indexaccessmode is mmap  info 17:05:08,786 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,797 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,807 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,833 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,834 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,839 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,862 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,864 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,876 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,885 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,892 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,893 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,897 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,901 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,906 sampling index for /var/lib/cassandra/data/system/schema-e-1-<>  info 17:05:08,909 sampling index for /var/lib/cassandra/data/system/schema-e-2-<>  info 17:05:08,918 sampling index for /var/lib/cassandra/data/system/migrations-e-1-<>  info 17:05:08,922 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,928 creating new commitlog segment /var/lib/cassandra/commitlog/commitlog-1281571508928.log  info 17:05:08,933 deleted /var/lib/cassandra/data/system/locationinfo-e-16-data.db  info 17:05:08,936 deleted /var/lib/cassandra/data/system/locationinfo-e-15-data.db  info 17:05:08,936 sampling index for /var/lib/cassandra/data/system/locationinfo-e-16-<>  error 17:05:08,937 corrupt file /var/lib/cassandra/data/system/locationinfo-e-16-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-16-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,947 deleted /var/lib/cassandra/data/system/locationinfo-e-14-data.db  info 17:05:08,947 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,948 sampling index for /var/lib/cassandra/data/system/locationinfo-e-15-<>  error 17:05:08,948 corrupt file /var/lib/cassandra/data/system/locationinfo-e-15-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-15-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,950 sampling index for /var/lib/cassandra/data/system/locationinfo-e-16-<>  error 17:05:08,951 corrupt file /var/lib/cassandra/data/system/locationinfo-e-16-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-16-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.rowmutation.apply(rowmutation.java:196)  at org.apache.cassandra.db.statisticstable.deletesstablestatistics(statisticstable.java:81)  at org.apache.cassandra.io.sstable.sstable.deleteifcompacted(sstable.java:136)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:202)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,970 deleted /var/lib/cassandra/data/system/locationinfo-e-13-data.db  info 17:05:08,971 sampling index for /var/lib/cassandra/data/system/locationinfo-e-14-<>  error 17:05:08,971 corrupt file /var/lib/cassandra/data/system/locationinfo-e-14-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-14-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,972 sampling index for /var/lib/cassandra/data/system/locationinfo-e-17-<>  info 17:05:08,973 sampling index for /var/lib/cassandra/data/system/locationinfo-e-15-<>  error 17:05:08,973 corrupt file /var/lib/cassandra/data/system/locationinfo-e-15-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-15-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,974 sampling index for /var/lib/cassandra/data/system/locationinfo-e-16-<>  error 17:05:08,974 corrupt file /var/lib/cassandra/data/system/locationinfo-e-16-data.db; skipped  java.io.filenotfoundexception: /var/lib/cassandra/data/system/locationinfo-e-16-index.db (no such file or directory)  at java.io.randomaccessfile.open(native method)  at java.io.randomaccessfile.<init>(randomaccessfile.java:212)  at java.io.randomaccessfile.<init>(randomaccessfile.java:98)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:142)  at org.apache.cassandra.io.util.bufferedrandomaccessfile.<init>(bufferedrandomaccessfile.java:137)  at org.apache.cassandra.io.sstable.sstablereader.load(sstablereader.java:289)  at org.apache.cassandra.io.sstable.sstablereader.internalopen(sstablereader.java:197)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:176)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:208)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:342)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:308)  at org.apache.cassandra.db.table.<init>(table.java:245)  at org.apache.cassandra.db.table.open(table.java:102)  at org.apache.cassandra.db.systemtable.checkhealth(systemtable.java:121)  at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:93)  at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:90)  at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  info 17:05:08,996 loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc  warn 17:05:09,158 schema definitions were defined both locally and in cassandra.yaml. definitions in cassandra.yaml were ignored.  info 17:05:09,164 replaying /var/lib/cassandra/commitlog/commitlog-1281571453475.log, /var/lib/cassandra/commitlog/commitlog-1281571508928.log  info 17:05:09,172 finished reading /var/lib/cassandra/commitlog/commitlog-1281571453475.log  info 17:05:09,172 finished reading /var/lib/cassandra/commitlog/commitlog-1281571508928.log  info 17:05:09,173 switching in a fresh memtable for locationinfo at commitlogcontext(file='/var/lib/cassandra/commitlog/commitlog-1281571508928.log', position=592)  info 17:05:09,183 enqueuing flush of memtable-locationinfo@137493297(17 bytes, 1 operations)  info 17:05:09,183 writing memtable-locationinfo@137493297(17 bytes, 1 operations)  info 17:05:09,184 switching in a fresh memtable for statistics at commitlogcontext(file='/var/lib/cassandra/commitlog/commitlog-1281571508928.log', position=592)  info 17:05:09,184 enqueuing flush of memtable-statistics@86823325(0 bytes, 0 operations)  info 17:05:09,265 completed flushing /var/lib/cassandra/data/system/locationinfo-e-18-data.db  info 17:05:09,273 writing memtable-statistics@86823325(0 bytes, 0 operations)  info 17:05:09,352 completed flushing /var/lib/cassandra/data/system/statistics-e-1-data.db  info 17:05:09,353 recovery complete<code> <text> on startup locationinfo file is deleted then attempted to be read from. steps to reproduce: kill then quickly restart ",
        "label": 186
    },
    {
        "text": " dtest  test sstableofflinerelevel   offline tools test testofflinetools <description> consistently failing dtest on 3.0 (no other branches). output from pytest:         output, _, rc = node1.run_sstableofflinerelevel(\"keyspace1\", \"standard1\") >       assert re.search(\"l0=1\", output) e       assertionerror: assert none e        +  where none = <function search at 0x7f99afffbe18>('l0=1', 'new leveling: \\nl0=0\\nl1 10\\n') e        +    where <function search at 0x7f99afffbe18> = re.search offline_tools_test.py:160: assertionerror<stacktrace> <code>         output, _, rc = node1.run_sstableofflinerelevel('keyspace1', 'standard1') >       assert re.search('l0=1', output) e       assertionerror: assert none e        +  where none = <function search at 0x7f99afffbe18>('l0=1', 'new leveling: /nl0=0/nl1 10/n') e        +    where <function search at 0x7f99afffbe18> = re.search offline_tools_test.py:160: assertionerror <text> consistently failing dtest on 3.0 (no other branches). output from pytest:",
        "label": 321
    },
    {
        "text": "jenkins pipeline can copy wrong test report artefacts from stage builds <description> spotted in https://ci-cassandra.apache.org/view/patches/job/cassandra-devbranch/196/console looks like copyartifact will need to be specific to a build.<stacktrace> <code> spotted in https://ci-cassandra.apache.org/view/patches/job/cassandra-devbranch/196/console <text> looks like copyartifact will need to be specific to a build.",
        "label": 347
    },
    {
        "text": "fix upgrade paging dtest failures on path <description> edit: this list of failures is no longer current; see comments for current failures. the following upgrade tests for paging features fail or flap on the upgrade path from 2.2 to 3.0: upgrade_tests/paging_test.py:testpagingdata.static_columns_paging_test upgrade_tests/paging_test.py:testpagingsize.test_undefined_page_size_default upgrade_tests/paging_test.py:testpagingsize.test_with_more_results_than_page_size upgrade_tests/paging_test.py:testpagingwithdeletions.test_failure_threshold_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_multiple_cell_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_single_cell_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_single_row_deletions upgrade_tests/paging_test.py:testpagingdatasetchanges.test_cell_ttl_expiry_during_paging/ i've grouped them all together because i don't know how to tell if they're related; once someone triages them, it may be appropriate to break this out into multiple tickets. the failures can be found here: http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingdata/static_columns_paging_test/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingsize/test_undefined_page_size_default/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/42/testreport/upgrade_tests.paging_test/testpagingsize/test_with_more_results_than_page_size/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_failure_threshold_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_multiple_cell_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_single_cell_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_single_row_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingdatasetchanges/test_cell_ttl_expiry_during_paging/ once this dtest pr is merged, these tests should also run with this upgrade path on normal 3.0 jobs. until then, you can run them with the following command: skip=false cassandra_version=binary:2.2.0 upgrade_to=git:cassandra-3.0 nosetests upgrade_tests/paging_test.py:testpagingdata.static_columns_paging_test upgrade_tests/paging_test.py:testpagingsize.test_undefined_page_size_default upgrade_tests/paging_test.py:testpagingsize.test_with_more_results_than_page_size upgrade_tests/paging_test.py:testpagingwithdeletions.test_failure_threshold_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_multiple_cell_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_single_cell_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_single_row_deletions upgrade_tests/paging_test.py:testpagingdatasetchanges.test_cell_ttl_expiry_during_paging<stacktrace> <code> skip=false cassandra_version=binary:2.2.0 upgrade_to=git:cassandra-3.0 nosetests upgrade_tests/paging_test.py:testpagingdata.static_columns_paging_test upgrade_tests/paging_test.py:testpagingsize.test_undefined_page_size_default upgrade_tests/paging_test.py:testpagingsize.test_with_more_results_than_page_size upgrade_tests/paging_test.py:testpagingwithdeletions.test_failure_threshold_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_multiple_cell_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_single_cell_deletions upgrade_tests/paging_test.py:testpagingwithdeletions.test_single_row_deletions upgrade_tests/paging_test.py:testpagingdatasetchanges.test_cell_ttl_expiry_during_paging http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingdata/static_columns_paging_test/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingsize/test_undefined_page_size_default/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/42/testreport/upgrade_tests.paging_test/testpagingsize/test_with_more_results_than_page_size/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_failure_threshold_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_multiple_cell_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_single_cell_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingwithdeletions/test_single_row_deletions/history/  http://cassci.datastax.com/view/upgrades/job/storage_engine_upgrade_dtest-22_tarball-30_head/44/testreport/upgrade_tests.paging_test/testpagingdatasetchanges/test_cell_ttl_expiry_during_paging/ <text> edit: this list of failures is no longer current; see comments for current failures. the following upgrade tests for paging features fail or flap on the upgrade path from 2.2 to 3.0: i've grouped them all together because i don't know how to tell if they're related; once someone triages them, it may be appropriate to break this out into multiple tickets. the failures can be found here: once this dtest pr is merged, these tests should also run with this upgrade path on normal 3.0 jobs. until then, you can run them with the following command:",
        "label": 69
    },
    {
        "text": "dtest failure in repair tests incremental repair test testincrepair sstable repairedset test <description> sstable_repairedset_test is failing on 2.1 and 2.2, but not on trunk. in the final assertion, after running sstablemetadata on both nodes, we see unrepaired sstables, when we expect all sstables to be repaired. example failure: http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/220/testreport/repair_tests.incremental_repair_test/testincrepair/sstable_repairedset_test failed on cassci build cassandra-2.1_novnode_dtest #220<stacktrace> <code> http://cassci.datastax.com/job/cassandra-2.1_novnode_dtest/220/testreport/repair_tests.incremental_repair_test/testincrepair/sstable_repairedset_test failed on cassci build cassandra-2.1_novnode_dtest #220<text> sstable_repairedset_test is failing on 2.1 and 2.2, but not on trunk. in the final assertion, after running sstablemetadata on both nodes, we see unrepaired sstables, when we expect all sstables to be repaired. example failure: ",
        "label": 321
    },
    {
        "text": "extend descriptor to include a format value and refactor reader writer apis <description> as part of a wider effort to improve the performance of our storage engine we will need to support basic pluggability of the sstable reader/writer. we primarily need this to support the current sstable format and new sstable format in the same version. this will also let us encapsulate the changes in a single layer vs forcing the whole engine to change at once. we previously discussed how to accomplish this in cassandra-3067<stacktrace> <code> <text> as part of a wider effort to improve the performance of our storage engine we will need to support basic pluggability of the sstable reader/writer. we primarily need this to support the current sstable format and new sstable format in the same version. this will also let us encapsulate the changes in a single layer vs forcing the whole engine to change at once. we previously discussed how to accomplish this in cassandra-3067",
        "label": 521
    },
    {
        "text": "anti compaction proceeds if any part of the repair failed <description> incremental repair will proceed to anti-compact it's stables even if any part of the repair process fails.<stacktrace> <code> <text> incremental repair will proceed to anti-compact it's stables even if any part of the repair process fails.",
        "label": 232
    },
    {
        "text": "wrong tostring  of cql3type collection <description> collection is an inner class of cql3type that represents column types that can contain several values (lists, sets, maps). its tostring() is very helpful: it generates a string representation of type that can be used for generating of create table statement. unfortunately this method works incorrectly for maps. instead of returning something like map<text, int> it returns set<text, int>. here is the appropriate code fragment: cql3type$collection.java        public string tostring()         {             switch (type.kind)             {                 case list:                     return \"list<\" + ((listtype)type).elements.ascql3type() + \">\";                 case set:                     return \"set<\" + ((settype)type).elements.ascql3type() + \">\";                 case map:                     maptype mt = (maptype)type;                     return \"set<\" + mt.keys.ascql3type() + \", \" + mt.values.ascql3type() + \">\";             }             throw new assertionerror();         } the obvious bug is here:                 case map:                     maptype mt = (maptype)type;                     return \"set<\" + mt.keys.ascql3type() + \", \" +  it should be \"map<\" + ... instead of \"set<\" + ...<stacktrace> <code>        public string tostring()         {             switch (type.kind)             {                 case list:                     return 'list<' + ((listtype)type).elements.ascql3type() + '>';                 case set:                     return 'set<' + ((settype)type).elements.ascql3type() + '>';                 case map:                     maptype mt = (maptype)type;                     return 'set<' + mt.keys.ascql3type() + ', ' + mt.values.ascql3type() + '>';             }             throw new assertionerror();         }                 case map:                     maptype mt = (maptype)type;                     return 'set<' + mt.keys.ascql3type() + ', ' +  <text> collection is an inner class of cql3type that represents column types that can contain several values (lists, sets, maps). its tostring() is very helpful: it generates a string representation of type that can be used for generating of create table statement. unfortunately this method works incorrectly for maps. instead of returning something like map<text, int> it returns set<text, int>. here is the appropriate code fragment: the obvious bug is here: it should be 'map<' + ... instead of 'set<' + ...",
        "label": 520
    },
    {
        "text": "nodetool repair does not obey the column family parameter when  st and  et are provided  subrange repair  <description> command 1: repairs all the cfs in adl_global keyspace and ignores the parameter assetmodifytimes_data used to restrict the cfs  executing: /aladdin/local/apps/apache-cassandra-2.1.8a/bin/nodetool -h localhost -p 7199 -u user-pw ****** repair adl_global assetmodifytimes_data -st 205279477618143669 -et 230991685737746901 -par  [2016-05-20 17:31:39,116] starting repair command #9, repairing 1 ranges for keyspace adl_global (parallelism=parallel, full=true)  [2016-05-20 17:32:21,568] repair session 3cae2530-1ed2-11e6-b490-d9df6932c7cf for range (205279477618143669,230991685737746901] finished command 2: repairs all the cfs in adl_global keyspace and ignores the parameter assetmodifytimes_data used to restrict the cfs  executing: /aladdin/local/apps/apache-cassandra-2.1.8a/bin/nodetool -h localhost -p 7199 -u controlrole -pw ****** repair -st 205279477618143669 -et 230991685737746901 -par \u2013 adl_global assetmodifytimes_data  [2016-05-20 17:36:34,473] starting repair command #10, repairing 1 ranges for keyspace adl_global (parallelism=parallel, full=true)  [2016-05-20 17:37:15,365] repair session ecb996d0-1ed2-11e6-b490-d9df6932c7cf for range (205279477618143669,230991685737746901] finished  [2016-05-20 17:37:15,365] repair command #10 finished command 3: repairs only the cf adl3test1_data in keyspace adl_global  executing: /aladdin/local/apps/apache-cassandra-2.1.8a/bin/nodetool -h localhost -p 7199 -u controlrole -pw ****** repair \u2013 adl_global adl3test1_data  [2016-05-20 17:38:35,781] starting repair command #11, repairing 1043 ranges for keyspace adl_global (parallelism=sequential, full=true) [2016-05-20 17:42:32,682] repair session 3c8af050-1ed3-11e6-b490-d9df6932c7cf for range (6241639152751626129,6241693909092643958] finished  [2016-05-20 17:42:32,683] repair session 3caf1a20-1ed3-11e6-b490-d9df6932c7cf for range (-7096993048358106082,-7095000706885780850] finished  [2016-05-20 17:42:32,683] repair session 3ccfc180-1ed3-11e6-b490-d9df6932c7cf for range (-7218939248114487080,-7218289345961492809] finished  [2016-05-20 17:42:32,683] repair session 3cf21690-1ed3-11e6-b490-d9df6932c7cf for range (-5244794756638190874,-5190307341355030282] finished  [2016-05-20 17:42:32,683] repair session 3d126fd0-1ed3-11e6-b490-d9df6932c7cf for range (3551629701277971766,3555521736534916502] finished  [2016-05-20 17:42:32,683] repair session 3d32f020-1ed3-11e6-b490-d9df6932c7cf for range (-8139355591560661944,-8127928369093576603] finished  [2016-05-20 17:42:32,683] repair session 3d537070-1ed3-11e6-b490-d9df6932c7cf for range (7098010153980465751,7100863011896759020] finished  [2016-05-20 17:42:32,683] repair session 3d73f0c0-1ed3-11e6-b490-d9df6932c7cf for range (1004538726866173536,1008586133746764703] finished  [2016-05-20 17:42:32,683] repair session 3d947110-1ed3-11e6-b490-d9df6932c7cf for range (5770817093573726645,5771418910784831587] finished  .  .  .  [2016-05-20 17:42:32,732] repair command #11 finished<stacktrace> <code> command 1: repairs all the cfs in adl_global keyspace and ignores the parameter assetmodifytimes_data used to restrict the cfs  executing: /aladdin/local/apps/apache-cassandra-2.1.8a/bin/nodetool -h localhost -p 7199 -u user-pw ****** repair adl_global assetmodifytimes_data -st 205279477618143669 -et 230991685737746901 -par  [2016-05-20 17:31:39,116] starting repair command #9, repairing 1 ranges for keyspace adl_global (parallelism=parallel, full=true)  [2016-05-20 17:32:21,568] repair session 3cae2530-1ed2-11e6-b490-d9df6932c7cf for range (205279477618143669,230991685737746901] finished command 2: repairs all the cfs in adl_global keyspace and ignores the parameter assetmodifytimes_data used to restrict the cfs  executing: /aladdin/local/apps/apache-cassandra-2.1.8a/bin/nodetool -h localhost -p 7199 -u controlrole -pw ****** repair -st 205279477618143669 -et 230991685737746901 -par - adl_global assetmodifytimes_data  [2016-05-20 17:36:34,473] starting repair command #10, repairing 1 ranges for keyspace adl_global (parallelism=parallel, full=true)  [2016-05-20 17:37:15,365] repair session ecb996d0-1ed2-11e6-b490-d9df6932c7cf for range (205279477618143669,230991685737746901] finished  [2016-05-20 17:37:15,365] repair command #10 finished command 3: repairs only the cf adl3test1_data in keyspace adl_global  executing: /aladdin/local/apps/apache-cassandra-2.1.8a/bin/nodetool -h localhost -p 7199 -u controlrole -pw ****** repair - adl_global adl3test1_data  [2016-05-20 17:38:35,781] starting repair command #11, repairing 1043 ranges for keyspace adl_global (parallelism=sequential, full=true) [2016-05-20 17:42:32,682] repair session 3c8af050-1ed3-11e6-b490-d9df6932c7cf for range (6241639152751626129,6241693909092643958] finished  [2016-05-20 17:42:32,683] repair session 3caf1a20-1ed3-11e6-b490-d9df6932c7cf for range (-7096993048358106082,-7095000706885780850] finished  [2016-05-20 17:42:32,683] repair session 3ccfc180-1ed3-11e6-b490-d9df6932c7cf for range (-7218939248114487080,-7218289345961492809] finished  [2016-05-20 17:42:32,683] repair session 3cf21690-1ed3-11e6-b490-d9df6932c7cf for range (-5244794756638190874,-5190307341355030282] finished  [2016-05-20 17:42:32,683] repair session 3d126fd0-1ed3-11e6-b490-d9df6932c7cf for range (3551629701277971766,3555521736534916502] finished  [2016-05-20 17:42:32,683] repair session 3d32f020-1ed3-11e6-b490-d9df6932c7cf for range (-8139355591560661944,-8127928369093576603] finished  [2016-05-20 17:42:32,683] repair session 3d537070-1ed3-11e6-b490-d9df6932c7cf for range (7098010153980465751,7100863011896759020] finished  [2016-05-20 17:42:32,683] repair session 3d73f0c0-1ed3-11e6-b490-d9df6932c7cf for range (1004538726866173536,1008586133746764703] finished  [2016-05-20 17:42:32,683] repair session 3d947110-1ed3-11e6-b490-d9df6932c7cf for range (5770817093573726645,5771418910784831587] finished  .  .  .  [2016-05-20 17:42:32,732] repair command #11 finished<text> ",
        "label": 558
    },
    {
        "text": "log when sstables are deleted <description> this was removed in 3.0 and is super helpful for debugging issues in prod<stacktrace> <code> <text> this was removed in 3.0 and is super helpful for debugging issues in prod",
        "label": 79
    },
    {
        "text": "binary protocol  when an invalid event type is watched via a register message  the response message does not have an associated stream id <description> i tried sending a register message with an eventlist including the string \"status_foo\", in order to test error handling in the python driver for that eventuality. but the response from the server (a \"server error\" with a message of \"java.lang.illegalargumentexception: no enum const class org.apache.cassandra.transport.event$type.status_foo\") had a stream_id of 0, so the driver was not able to associate it with the request.<stacktrace> <code> <text> i tried sending a register message with an eventlist including the string 'status_foo', in order to test error handling in the python driver for that eventuality. but the response from the server (a 'server error' with a message of 'java.lang.illegalargumentexception: no enum const class org.apache.cassandra.transport.event$type.status_foo') had a stream_id of 0, so the driver was not able to associate it with the request.",
        "label": 520
    },
    {
        "text": "batch with multiple conditional updates for the same partition causes assertionerror <description> reproduced in 3.0.10 and 3.10. used to work in 3.0.9 and earlier. bug was introduced in cassandra-12060. the following causes an assertionerror: create keyspace test with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }; create table test.test (id int primary key, val text); begin batch insert into test.test (id, val) values (999, 'aaa') if not exists; insert into test.test (id, val) values (999, 'ccc') if not exists; apply batch ; stack trace is as follows: error [native-transport-requests-2] 2016-10-31 04:16:44,231 message.java:622 - unexpected exception during request; channel = [id: 0x176e1c04, l:/127.0.0.1:9042 - r:/127.0.0.1:59743] java.lang.assertionerror: null         at org.apache.cassandra.cql3.statements.cql3casrequest.setconditionsforrow(cql3casrequest.java:138) ~[main/:na]         at org.apache.cassandra.cql3.statements.cql3casrequest.addexistscondition(cql3casrequest.java:104) ~[main/:na]         at org.apache.cassandra.cql3.statements.cql3casrequest.addnotexist(cql3casrequest.java:84) ~[main/:na]         at org.apache.cassandra.cql3.ifnotexistscondition.addconditionsto(ifnotexistscondition.java:28) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement.addconditions(modificationstatement.java:482) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.makecasrequest(batchstatement.java:434) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.executewithconditions(batchstatement.java:379) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:358) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:346) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:341) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:218) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:249) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:234) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:516) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:409) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:366) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.access$600(abstractchannelhandlercontext.java:35) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext$7.run(abstractchannelhandlercontext.java:357) [netty-all-4.0.39.final.jar:4.0.39.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_102]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:162) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:109) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_102] the problem is that previous will receive a value after the first statement in the batch is evaluated in batchstatement.makecasrequest. i can't see any reason why we have this assertion, it seems to me that it's unnecessary.  removing it fixes the problem (obviously) but i'm not sure if it breaks something else, or if this is an intended failure case (in which case it should be caught earlier on). relevant code is as follows: cql3casrequest.java     private void setconditionsforrow(clustering clustering, rowcondition condition)     {         if (clustering == clustering.static_clustering)         {             assert staticconditions == null;             staticconditions = condition;         }         else         {             rowcondition previous = conditions.put(clustering, condition);             assert previous == null;         }     } i've attached a patch that fixes the issue by removing the assert<stacktrace> error [native-transport-requests-2] 2016-10-31 04:16:44,231 message.java:622 - unexpected exception during request; channel = [id: 0x176e1c04, l:/127.0.0.1:9042 - r:/127.0.0.1:59743] java.lang.assertionerror: null         at org.apache.cassandra.cql3.statements.cql3casrequest.setconditionsforrow(cql3casrequest.java:138) ~[main/:na]         at org.apache.cassandra.cql3.statements.cql3casrequest.addexistscondition(cql3casrequest.java:104) ~[main/:na]         at org.apache.cassandra.cql3.statements.cql3casrequest.addnotexist(cql3casrequest.java:84) ~[main/:na]         at org.apache.cassandra.cql3.ifnotexistscondition.addconditionsto(ifnotexistscondition.java:28) ~[main/:na]         at org.apache.cassandra.cql3.statements.modificationstatement.addconditions(modificationstatement.java:482) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.makecasrequest(batchstatement.java:434) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.executewithconditions(batchstatement.java:379) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:358) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:346) ~[main/:na]         at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:341) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:218) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:249) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:234) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:115) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:516) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:409) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:366) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext.access$600(abstractchannelhandlercontext.java:35) [netty-all-4.0.39.final.jar:4.0.39.final]         at io.netty.channel.abstractchannelhandlercontext$7.run(abstractchannelhandlercontext.java:357) [netty-all-4.0.39.final.jar:4.0.39.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_102]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:162) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:109) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_102] <code> create keyspace test with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }; create table test.test (id int primary key, val text); begin batch insert into test.test (id, val) values (999, 'aaa') if not exists; insert into test.test (id, val) values (999, 'ccc') if not exists; apply batch ;     private void setconditionsforrow(clustering clustering, rowcondition condition)     {         if (clustering == clustering.static_clustering)         {             assert staticconditions == null;             staticconditions = condition;         }         else         {             rowcondition previous = conditions.put(clustering, condition);             assert previous == null;         }     } <text> reproduced in 3.0.10 and 3.10. used to work in 3.0.9 and earlier. bug was introduced in cassandra-12060. the following causes an assertionerror: stack trace is as follows: the problem is that previous will receive a value after the first statement in the batch is evaluated in batchstatement.makecasrequest. i can't see any reason why we have this assertion, it seems to me that it's unnecessary.  removing it fixes the problem (obviously) but i'm not sure if it breaks something else, or if this is an intended failure case (in which case it should be caught earlier on). relevant code is as follows: i've attached a patch that fixes the issue by removing the assert",
        "label": 520
    },
    {
        "text": "cqlsh  describe table shows wrong data type for compositetype <description> describe for compositetype produces wrong output. currently: create table compo.comp (     id int primary key,     comp 'org.apache.cassandra.db.marshal.compositetype'<int, text> )... correct: create table compo.comp (     id int primary key,     comp 'org.apache.cassandra.db.marshal.compositetype(int32type,utf8type)' )... means:  1. use normal brackets instead of <>  1. use c* type names instead of cql3 names  1. move types inside quoted<stacktrace> <code> <text> create table compo.comp (     id int primary key,     comp 'org.apache.cassandra.db.marshal.compositetype'<int, text> )... create table compo.comp (     id int primary key,     comp 'org.apache.cassandra.db.marshal.compositetype(int32type,utf8type)' )... describe for compositetype produces wrong output. currently: correct: means:  1. use normal brackets instead of <>  1. use c* type names instead of cql3 names  1. move types inside quoted",
        "label": 362
    },
    {
        "text": "make binarymemtable work <description> <stacktrace> <code> <text> ",
        "label": 104
    },
    {
        "text": "get rid of thriftglue <description> thriftglue is not necessary with thrift's chained methods.<stacktrace> <code> <text> thriftglue is not necessary with thrift's chained methods.",
        "label": 524
    },
    {
        "text": "nodetool tablestats cfstats output has inconsistent formatting for latency <description> latencies are reported at keyspace level with `ms.` and at table level with `ms`. there should be no trailing `.` as it is not a sentence and `.` is not part of the abbreviation. this is also present in 2.x with `nodetool cfstats`.<stacktrace> <code> <text> latencies are reported at keyspace level with `ms.` and at table level with `ms`. there should be no trailing `.` as it is not a sentence and `.` is not part of the abbreviation. this is also present in 2.x with `nodetool cfstats`.",
        "label": 479
    },
    {
        "text": "the class o a c cql jdbc typedcolumn needs to be declared public <description> the implementation of resultset in the jdbc package provides a method: unwrap( class<t> interfacename) in order to allow some of the methods in the resultset implementation class to be exposed. the implementation currently restricts the access to only one acceptable interface: cassandraresultset. two of the getters in that interface return typedcolumn which cleverly contains the \"cassandra\" details of the desired column such as raw column details, and the abstracttype of the validator and the comparator among others. (nice!) unfortunately the typedcolumn class is not public so is it is not accessible to the callers code.<stacktrace> <code> <text> the implementation of resultset in the jdbc package provides a method: unwrap( class<t> interfacename) in order to allow some of the methods in the resultset implementation class to be exposed. the implementation currently restricts the access to only one acceptable interface: cassandraresultset. two of the getters in that interface return typedcolumn which cleverly contains the 'cassandra' details of the desired column such as raw column details, and the abstracttype of the validator and the comparator among others. (nice!) unfortunately the typedcolumn class is not public so is it is not accessible to the callers code.",
        "label": 449
    },
    {
        "text": "change cassandra wait for tracing events timeout secs default to  so c  doesn't wait on trace events to be written before responding to request by default <description> cassandra-11465 introduces a new system property cassandra.wait_for_tracing_events_timeout_secs that controls whether or not c* waits for events to be written before responding to client. the current default behavior is to wait up to 1 second and then respond and timeout. if using probabilistic tracing this can cause queries to be randomly delayed up to 1 second. changing the default to -1 (disabled and enabling it explicitly in cql_tracing_test.testcqltracing.tracing_unknown_impl_test. ideally it would be nice to be able to control this behavior on a per request basis (which would require native protocol changes).<stacktrace> <code> <text> cassandra-11465 introduces a new system property cassandra.wait_for_tracing_events_timeout_secs that controls whether or not c* waits for events to be written before responding to client. the current default behavior is to wait up to 1 second and then respond and timeout. if using probabilistic tracing this can cause queries to be randomly delayed up to 1 second. changing the default to -1 (disabled and enabling it explicitly in cql_tracing_test.testcqltracing.tracing_unknown_impl_test. ideally it would be nice to be able to control this behavior on a per request basis (which would require native protocol changes).",
        "label": 508
    },
    {
        "text": "optimize disk seek using min max column name meta data when the limit clause is used <description> i was working on an example of sensor data table (timeseries) and face a use case where c* does not optimize read on disk. cqlsh:test> create table test(id int, col int, val text, primary key(id,col)) with clustering order by (col desc); cqlsh:test> insert into test(id, col , val ) values ( 1, 10, '10'); ... >nodetool flush test test ... cqlsh:test> insert into test(id, col , val ) values ( 1, 20, '20'); ... >nodetool flush test test ... cqlsh:test> insert into test(id, col , val ) values ( 1, 30, '30'); ... >nodetool flush test test after that, i activate request tracing: cqlsh:test> select * from test where id=1 limit 1;  activity                                                                  | timestamp    | source    | source_elapsed ---------------------------------------------------------------------------+--------------+-----------+----------------                                                         execute_cql3_query | 23:48:46,498 | 127.0.0.1 |              0                             parsing select * from test where id=1 limit 1; | 23:48:46,498 | 127.0.0.1 |             74                                                        preparing statement | 23:48:46,499 | 127.0.0.1 |            253                                   executing single-partition query on test | 23:48:46,499 | 127.0.0.1 |            930                                               acquiring sstable references | 23:48:46,499 | 127.0.0.1 |            943                                                merging memtable tombstones | 23:48:46,499 | 127.0.0.1 |           1032                                                key cache hit for sstable 3 | 23:48:46,500 | 127.0.0.1 |           1160                                seeking to partition beginning in data file | 23:48:46,500 | 127.0.0.1 |           1173                                                key cache hit for sstable 2 | 23:48:46,500 | 127.0.0.1 |           1889                                seeking to partition beginning in data file | 23:48:46,500 | 127.0.0.1 |           1901                                                key cache hit for sstable 1 | 23:48:46,501 | 127.0.0.1 |           2373                                seeking to partition beginning in data file | 23:48:46,501 | 127.0.0.1 |           2384  skipped 0/3 non-slice-intersecting sstables, included 0 due to tombstones | 23:48:46,501 | 127.0.0.1 |           2768                                 merging data from memtables and 3 sstables | 23:48:46,501 | 127.0.0.1 |           2784                                         read 2 live and 0 tombstoned cells | 23:48:46,501 | 127.0.0.1 |           2976                                                           request complete | 23:48:46,501 | 127.0.0.1 |           3551 we can clearly see that c* hits 3 sstables on disk instead of just one, although it has the min/max column meta data to decide which sstable contains the most recent data. funny enough, if we add a clause on the clustering column to the select, this time c* optimizes the read path: cqlsh:test> select * from test where id=1 and col > 25 limit 1;  activity                                                                  | timestamp    | source    | source_elapsed ---------------------------------------------------------------------------+--------------+-----------+----------------                                                         execute_cql3_query | 23:52:31,888 | 127.0.0.1 |              0                parsing select * from test where id=1 and col > 25 limit 1; | 23:52:31,888 | 127.0.0.1 |             60                                                        preparing statement | 23:52:31,888 | 127.0.0.1 |            277                                   executing single-partition query on test | 23:52:31,889 | 127.0.0.1 |            961                                               acquiring sstable references | 23:52:31,889 | 127.0.0.1 |            971                                                merging memtable tombstones | 23:52:31,889 | 127.0.0.1 |           1020                                                key cache hit for sstable 3 | 23:52:31,889 | 127.0.0.1 |           1108                                seeking to partition beginning in data file | 23:52:31,889 | 127.0.0.1 |           1117  skipped 2/3 non-slice-intersecting sstables, included 0 due to tombstones | 23:52:31,889 | 127.0.0.1 |           1611                                 merging data from memtables and 1 sstables | 23:52:31,890 | 127.0.0.1 |           1624                                         read 1 live and 0 tombstoned cells | 23:52:31,890 | 127.0.0.1 |           1700                                                           request complete | 23:52:31,890 | 127.0.0.1 |           2140<stacktrace> <code> cqlsh:test> create table test(id int, col int, val text, primary key(id,col)) with clustering order by (col desc); cqlsh:test> insert into test(id, col , val ) values ( 1, 10, '10'); ... >nodetool flush test test ... cqlsh:test> insert into test(id, col , val ) values ( 1, 20, '20'); ... >nodetool flush test test ... cqlsh:test> insert into test(id, col , val ) values ( 1, 30, '30'); ... >nodetool flush test test cqlsh:test> select * from test where id=1 limit 1;  activity                                                                  | timestamp    | source    | source_elapsed ---------------------------------------------------------------------------+--------------+-----------+----------------                                                         execute_cql3_query | 23:48:46,498 | 127.0.0.1 |              0                             parsing select * from test where id=1 limit 1; | 23:48:46,498 | 127.0.0.1 |             74                                                        preparing statement | 23:48:46,499 | 127.0.0.1 |            253                                   executing single-partition query on test | 23:48:46,499 | 127.0.0.1 |            930                                               acquiring sstable references | 23:48:46,499 | 127.0.0.1 |            943                                                merging memtable tombstones | 23:48:46,499 | 127.0.0.1 |           1032                                                key cache hit for sstable 3 | 23:48:46,500 | 127.0.0.1 |           1160                                seeking to partition beginning in data file | 23:48:46,500 | 127.0.0.1 |           1173                                                key cache hit for sstable 2 | 23:48:46,500 | 127.0.0.1 |           1889                                seeking to partition beginning in data file | 23:48:46,500 | 127.0.0.1 |           1901                                                key cache hit for sstable 1 | 23:48:46,501 | 127.0.0.1 |           2373                                seeking to partition beginning in data file | 23:48:46,501 | 127.0.0.1 |           2384  skipped 0/3 non-slice-intersecting sstables, included 0 due to tombstones | 23:48:46,501 | 127.0.0.1 |           2768                                 merging data from memtables and 3 sstables | 23:48:46,501 | 127.0.0.1 |           2784                                         read 2 live and 0 tombstoned cells | 23:48:46,501 | 127.0.0.1 |           2976                                                           request complete | 23:48:46,501 | 127.0.0.1 |           3551 cqlsh:test> select * from test where id=1 and col > 25 limit 1;  activity                                                                  | timestamp    | source    | source_elapsed ---------------------------------------------------------------------------+--------------+-----------+----------------                                                         execute_cql3_query | 23:52:31,888 | 127.0.0.1 |              0                parsing select * from test where id=1 and col > 25 limit 1; | 23:52:31,888 | 127.0.0.1 |             60                                                        preparing statement | 23:52:31,888 | 127.0.0.1 |            277                                   executing single-partition query on test | 23:52:31,889 | 127.0.0.1 |            961                                               acquiring sstable references | 23:52:31,889 | 127.0.0.1 |            971                                                merging memtable tombstones | 23:52:31,889 | 127.0.0.1 |           1020                                                key cache hit for sstable 3 | 23:52:31,889 | 127.0.0.1 |           1108                                seeking to partition beginning in data file | 23:52:31,889 | 127.0.0.1 |           1117  skipped 2/3 non-slice-intersecting sstables, included 0 due to tombstones | 23:52:31,889 | 127.0.0.1 |           1611                                 merging data from memtables and 1 sstables | 23:52:31,890 | 127.0.0.1 |           1624                                         read 1 live and 0 tombstoned cells | 23:52:31,890 | 127.0.0.1 |           1700                                                           request complete | 23:52:31,890 | 127.0.0.1 |           2140 <text> i was working on an example of sensor data table (timeseries) and face a use case where c* does not optimize read on disk. after that, i activate request tracing: we can clearly see that c* hits 3 sstables on disk instead of just one, although it has the min/max column meta data to decide which sstable contains the most recent data. funny enough, if we add a clause on the clustering column to the select, this time c* optimizes the read path:",
        "label": 508
    },
    {
        "text": "possible livelock during commit log playback <description> in commitlog.recover, there seems to be the possibility of concurrent inserts to tablesrecovered (a hashset) in the runnables instantiated a bit below (line 323 in 1.0.7). this apparently happened during a commit log playback during startup of a node that had not shut down cleanly (the cluster was under heavy load previously and there were several gigabytes of commit logs), resulting in two threads running in perpetuity (2 cores were at 100% from running these threads), preventing the node from coming up. the relevant portion of the stack trace is: info   | jvm 1    | 2012/01/16 16:54:42 | \"mutationstage:25\" prio=10 tid=0x00002aaad01e0800 nid=0x6f62 runnable [0x0000000044d54000] info   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.thread.state: runnable info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashmap.put(hashmap.java:374) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashset.add(hashset.java:200) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.db.commitlog.commitlog$2.runmaythrow(commitlog.java:338) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.executors$runnableadapter.call(executors.java:441) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask.run(futuretask.java:138) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.lang.thread.run(thread.java:662) info   | jvm 1    | 2012/01/16 16:54:42 |  info   | jvm 1    | 2012/01/16 16:54:42 | \"mutationstage:21\" prio=10 tid=0x00002aaad00a2800 nid=0x6f5e runnable [0x0000000044950000] info   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.thread.state: runnable info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashmap.put(hashmap.java:374) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashset.add(hashset.java:200) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.db.commitlog.commitlog$2.runmaythrow(commitlog.java:338) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.executors$runnableadapter.call(executors.java:441) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask.run(futuretask.java:138) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.lang.thread.run(thread.java:662) the most recently modified file in the commit log directory was this entry: -rw-r----- 1 <redacted> <redacted>    0 jan 16 16:03 commitlog-1326758622599.log though i'm not sure if this was related or not.<stacktrace> info   | jvm 1    | 2012/01/16 16:54:42 | 'mutationstage:25' prio=10 tid=0x00002aaad01e0800 nid=0x6f62 runnable [0x0000000044d54000] info   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.thread.state: runnable info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashmap.put(hashmap.java:374) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashset.add(hashset.java:200) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.db.commitlog.commitlog$2.runmaythrow(commitlog.java:338) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.executors$runnableadapter.call(executors.java:441) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask.run(futuretask.java:138) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.lang.thread.run(thread.java:662) info   | jvm 1    | 2012/01/16 16:54:42 |  info   | jvm 1    | 2012/01/16 16:54:42 | 'mutationstage:21' prio=10 tid=0x00002aaad00a2800 nid=0x6f5e runnable [0x0000000044950000] info   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.thread.state: runnable info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashmap.put(hashmap.java:374) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.hashset.add(hashset.java:200) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.db.commitlog.commitlog$2.runmaythrow(commitlog.java:338) info   | jvm 1    | 2012/01/16 16:54:42 |  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.executors$runnableadapter.call(executors.java:441) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.futuretask.run(futuretask.java:138) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) info   | jvm 1    | 2012/01/16 16:54:42 |  at java.lang.thread.run(thread.java:662) <code> -rw-r----- 1 <redacted> <redacted>    0 jan 16 16:03 commitlog-1326758622599.log <text> in commitlog.recover, there seems to be the possibility of concurrent inserts to tablesrecovered (a hashset) in the runnables instantiated a bit below (line 323 in 1.0.7). this apparently happened during a commit log playback during startup of a node that had not shut down cleanly (the cluster was under heavy load previously and there were several gigabytes of commit logs), resulting in two threads running in perpetuity (2 cores were at 100% from running these threads), preventing the node from coming up. the relevant portion of the stack trace is: the most recently modified file in the commit log directory was this entry: though i'm not sure if this was related or not.",
        "label": 274
    },
    {
        "text": "missing columns  errors when requesting specific columns from wide rows <description> with cassandra 1.2.1 (and probably 1.2.0), i'm seeing some problems with thrift queries that request a set of specific column names when the row is very wide. to reproduce, i'm inserting 10 million columns into a single row and then randomly requesting three columns by name in a loop. it's common for only one or two of the three columns to be returned. i'm also seeing stack traces like the following in the cassandra log: error 13:12:01,017 exception in thread thread[readstage:76,5,main] java.lang.runtimeexception: org.apache.cassandra.io.sstable.corruptsstableexception: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0 (/var/lib/cassandra/data/keyspace1/cf1/keyspace1-cf1-ib-5-data.db, 14035168 bytes remaining) at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1576) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: org.apache.cassandra.io.sstable.corruptsstableexception: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0 (/var/lib/cassandra/data/keyspace1/cf1/keyspace1-cf1-ib-5-data.db, 14035168 bytes remaining) at org.apache.cassandra.db.columniterator.sstablenamesiterator.<init>(sstablenamesiterator.java:69) at org.apache.cassandra.db.filter.namesqueryfilter.getsstablecolumniterator(namesqueryfilter.java:81) at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:68) at org.apache.cassandra.db.collationcontroller.collecttimeordereddata(collationcontroller.java:133) at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:65) at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1358) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1215) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1127) at org.apache.cassandra.db.table.getrow(table.java:355) at org.apache.cassandra.db.slicebynamesreadcommand.getrow(slicebynamesreadcommand.java:64) at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1052) at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1572) ... 3 more this doesn't seem to happen when the row is smaller, so it might have something to do with incremental large row compaction.<stacktrace> error 13:12:01,017 exception in thread thread[readstage:76,5,main] java.lang.runtimeexception: org.apache.cassandra.io.sstable.corruptsstableexception: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0 (/var/lib/cassandra/data/keyspace1/cf1/keyspace1-cf1-ib-5-data.db, 14035168 bytes remaining) at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1576) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: org.apache.cassandra.io.sstable.corruptsstableexception: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0 (/var/lib/cassandra/data/keyspace1/cf1/keyspace1-cf1-ib-5-data.db, 14035168 bytes remaining) at org.apache.cassandra.db.columniterator.sstablenamesiterator.<init>(sstablenamesiterator.java:69) at org.apache.cassandra.db.filter.namesqueryfilter.getsstablecolumniterator(namesqueryfilter.java:81) at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:68) at org.apache.cassandra.db.collationcontroller.collecttimeordereddata(collationcontroller.java:133) at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:65) at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1358) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1215) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1127) at org.apache.cassandra.db.table.getrow(table.java:355) at org.apache.cassandra.db.slicebynamesreadcommand.getrow(slicebynamesreadcommand.java:64) at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1052) at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1572) ... 3 more <code> <text> with cassandra 1.2.1 (and probably 1.2.0), i'm seeing some problems with thrift queries that request a set of specific column names when the row is very wide. to reproduce, i'm inserting 10 million columns into a single row and then randomly requesting three columns by name in a loop. it's common for only one or two of the three columns to be returned. i'm also seeing stack traces like the following in the cassandra log: this doesn't seem to happen when the row is smaller, so it might have something to do with incremental large row compaction.",
        "label": 520
    },
    {
        "text": "add a notice on cqlsh startup about cql2 switches <description> several developers i've talked with seem not to have noticed the -3 switch immediately to run in cql3 mode. if missing, cqlsh can obviously appear buggy in its way to handle cql3.  i guess it would be worth to add a notice at startup about this important detail.<stacktrace> <code> <text> several developers i've talked with seem not to have noticed the -3 switch immediately to run in cql3 mode. if missing, cqlsh can obviously appear buggy in its way to handle cql3.  i guess it would be worth to add a notice at startup about this important detail.",
        "label": 593
    },
    {
        "text": "make buffered read size configurable <description> on read workloads, cassandra 2.1 reads drastically more data than it emits over the network. this causes problems throughput the system by wasting disk io and causing unnecessary gc. i have reproduce the issue on clusters and locally with a single instance. the only requirement to reproduce the issue is enough data to blow through the page cache. the default schema and data size with cassandra-stress is sufficient for exposing the issue. with stock 2.1.9 i regularly observed anywhere from 300:1 to 500 disk:network ratio. that is to say, for 1mb/s of network io, cassandra was doing 300-500mb/s of disk reads, saturating the drive. after applying this patch for standard io mode https://gist.github.com/tobert/10c307cf3709a585a7cf the ratio fell to around 100:1 on my local test rig. latency improved considerably and gc became a lot less frequent. i tested with 512 byte reads as well, but got the same performance, which makes sense since all hdd and ssd made in the last few years have a 4k block size (many of them lie and say 512). i'm re-running the numbers now and will post them tomorrow.<stacktrace> <code> <text> on read workloads, cassandra 2.1 reads drastically more data than it emits over the network. this causes problems throughput the system by wasting disk io and causing unnecessary gc. i have reproduce the issue on clusters and locally with a single instance. the only requirement to reproduce the issue is enough data to blow through the page cache. the default schema and data size with cassandra-stress is sufficient for exposing the issue. with stock 2.1.9 i regularly observed anywhere from 300:1 to 500 disk:network ratio. that is to say, for 1mb/s of network io, cassandra was doing 300-500mb/s of disk reads, saturating the drive. after applying this patch for standard io mode https://gist.github.com/tobert/10c307cf3709a585a7cf the ratio fell to around 100:1 on my local test rig. latency improved considerably and gc became a lot less frequent. i tested with 512 byte reads as well, but got the same performance, which makes sense since all hdd and ssd made in the last few years have a 4k block size (many of them lie and say 512). i'm re-running the numbers now and will post them tomorrow.",
        "label": 16
    },
    {
        "text": "cql does not work in cqlsh with uppercase select <description> uppercase select prevents usage of cql 3.0 features like order by example: select * from test order by number; # works  select * from test order by number; # fails<stacktrace> <code> select * from test order by number; # works  select * from test order by number; # fails<text> uppercase select prevents usage of cql 3.0 features like order by example: ",
        "label": 273
    },
    {
        "text": "modernize schema tables <description> there is a few problems/improvements that can be done with the way we store schema: 1. cassandra-4988: as explained on the ticket, storing the comparator is now redundant (or almost, we'd need to store whether the table is compact or not too, which we don't currently is easy and probably a good idea anyway), it can be entirely reconstructed from the infos in schema_columns (the same is true of key_validator and subcomparator, and replacing default_validator by a compact_value column in all case is relatively simple). and storing the comparator as an opaque string broke concurrent updates of sub-part of said comparator (concurrent collection addition or altering 2 separate clustering columns typically) so it's really worth removing it. 2. cassandra-4603: it's time to get rid of those ugly json maps. i'll note that schema_keyspaces is a problem due to its use of compact storage, but i think we should fix it once and for-all nonetheless (see below). 3. for cassandra-6382 and to allow indexing both map keys and values at the same time, we'd need to be able to have more than one index definition for a given column. 4. there is a few mismatches in table options between the one stored in the schema and the one used when declaring/altering a table which would be nice to fix. the compaction, compression and replication maps are one already mentioned from cassandra-4603, but also for some reason 'dclocal_read_repair_chance' in cql is called just 'local_read_repair_chance' in the schema table, and 'min/max_compaction_threshold' are column families option in the schema but just compaction options for cql (which makes more sense). none of those issues are major, and we could probably deal with them independently but it might be simpler to just fix them all in one shot so i wanted to sum them all up here. in particular, the fact that 'schema_keyspaces' uses compact storage is annoying (for the replication map, but it may limit future stuff too) which suggest we should migrate it to a new, non compact table. and while that's arguably a detail, it wouldn't hurt to rename schema_columnfamilies to schema_tables for the years to come since that's the prefered vernacular for cql. overall, what i would suggest is to move all schema tables to a new keyspace, named 'schema' for instance (or 'system_schema' but i prefer the shorter version), and fix all the issues above at once. since we currently don't exchange schema between nodes of different versions, all we'd need to do that is a one shot startup migration, and overall, i think it could be simpler for clients to deal with one clear migration than to have to handle minor individual changes all over the place. i also think it's somewhat cleaner conceptually to have schema tables in their own keyspace since they are replicated through a different mechanism than other system tables. if we do that, we could, for instance, migrate to the following schema tables (details up for discussion of course): create type user_type (   name text,   column_names list<text>,   column_types list<text> ) create table keyspaces (   name text primary key,   durable_writes boolean,   replication map<string, string>,   user_types map<string, user_type> ) create type trigger_definition (   name text,   options map<tex, text> ) create table tables (   keyspace text,   name text,   id uuid,   table_type text, // compact, cql or super   dropped_columns map<text, bigint>,   triggers map<text, trigger_definition>,   // options   comment text,   compaction map<text, text>,   compression map<text, text>,   read_repair_chance double,   dclocal_read_repair_chance double,   gc_grace_seconds int,   caching text,   rows_per_partition_to_cache text,   default_time_to_live int,   min_index_interval int,   max_index_interval int,   speculative_retry text,   populate_io_cache_on_flush boolean,   bloom_filter_fp_chance double   memtable_flush_period_in_ms int,   primary key (keyspace, name) ) create type index_definition (   name text,   index_type text,   options map<text, text> ) create table columns (   keyspace text,   table text,   name text,   kind text, // partition_key, clustering_column, regular or compact_value   component_index int;   type text,   indexes map<text, index_definition>,   primary key (keyspace, table, name) ) nit: wouldn't hurt to create a simple enum that is reuse by both cfmetadata and cfpropdefs for table options names while we're at it once they are the same instead of repeating string constants which is fragile.<stacktrace> <code> create type user_type (   name text,   column_names list<text>,   column_types list<text> ) create table keyspaces (   name text primary key,   durable_writes boolean,   replication map<string, string>,   user_types map<string, user_type> ) create type trigger_definition (   name text,   options map<tex, text> ) create table tables (   keyspace text,   name text,   id uuid,   table_type text, // compact, cql or super   dropped_columns map<text, bigint>,   triggers map<text, trigger_definition>,   // options   comment text,   compaction map<text, text>,   compression map<text, text>,   read_repair_chance double,   dclocal_read_repair_chance double,   gc_grace_seconds int,   caching text,   rows_per_partition_to_cache text,   default_time_to_live int,   min_index_interval int,   max_index_interval int,   speculative_retry text,   populate_io_cache_on_flush boolean,   bloom_filter_fp_chance double   memtable_flush_period_in_ms int,   primary key (keyspace, name) ) create type index_definition (   name text,   index_type text,   options map<text, text> ) create table columns (   keyspace text,   table text,   name text,   kind text, // partition_key, clustering_column, regular or compact_value   component_index int;   type text,   indexes map<text, index_definition>,   primary key (keyspace, table, name) ) <text> there is a few problems/improvements that can be done with the way we store schema: none of those issues are major, and we could probably deal with them independently but it might be simpler to just fix them all in one shot so i wanted to sum them all up here. in particular, the fact that 'schema_keyspaces' uses compact storage is annoying (for the replication map, but it may limit future stuff too) which suggest we should migrate it to a new, non compact table. and while that's arguably a detail, it wouldn't hurt to rename schema_columnfamilies to schema_tables for the years to come since that's the prefered vernacular for cql. overall, what i would suggest is to move all schema tables to a new keyspace, named 'schema' for instance (or 'system_schema' but i prefer the shorter version), and fix all the issues above at once. since we currently don't exchange schema between nodes of different versions, all we'd need to do that is a one shot startup migration, and overall, i think it could be simpler for clients to deal with one clear migration than to have to handle minor individual changes all over the place. i also think it's somewhat cleaner conceptually to have schema tables in their own keyspace since they are replicated through a different mechanism than other system tables. if we do that, we could, for instance, migrate to the following schema tables (details up for discussion of course): nit: wouldn't hurt to create a simple enum that is reuse by both cfmetadata and cfpropdefs for table options names while we're at it once they are the same instead of repeating string constants which is fragile.",
        "label": 18
    },
    {
        "text": "remove thrift dependency in stress schema creation <description> with cassandra-9319 the thrift server is turned off by default, which makes stress no longer work out of the box. even though stress uses native cql3 by default, there is still some remaining piece that uses thrift for schema creation. this is what you get by default now: $ java_home=~/fab/java ~/fab/stress/default/tools/bin/cassandra-stress write n=19000000 -rate threads=300 -node blade-11-4a,blade-11-3a,blade-11-2a exception in thread \"main\" java.lang.runtimeexception: org.apache.thrift.transport.ttransportexception: java.net.connectexception: connection refused         at org.apache.cassandra.stress.settings.stresssettings.getrawthriftclient(stresssettings.java:144)         at org.apache.cassandra.stress.settings.stresssettings.getrawthriftclient(stresssettings.java:110)         at org.apache.cassandra.stress.settings.settingsschema.createkeyspacesthrift(settingsschema.java:111)         at org.apache.cassandra.stress.settings.settingsschema.createkeyspaces(settingsschema.java:59)         at org.apache.cassandra.stress.settings.stresssettings.maybecreatekeyspaces(stresssettings.java:205)         at org.apache.cassandra.stress.stressaction.run(stressaction.java:55)         at org.apache.cassandra.stress.stress.main(stress.java:109) caused by: org.apache.thrift.transport.ttransportexception: java.net.connectexception: connection refused         at org.apache.thrift.transport.tsocket.open(tsocket.java:187)         at org.apache.thrift.transport.tframedtransport.open(tframedtransport.java:81)         at org.apache.cassandra.thrift.tframedtransportfactory.opentransport(tframedtransportfactory.java:41)         at org.apache.cassandra.stress.settings.stresssettings.getrawthriftclient(stresssettings.java:124)         ... 6 more caused by: java.net.connectexception: connection refused         at java.net.plainsocketimpl.socketconnect(native method)         at java.net.abstractplainsocketimpl.doconnect(abstractplainsocketimpl.java:339)         at java.net.abstractplainsocketimpl.connecttoaddress(abstractplainsocketimpl.java:200)         at java.net.abstractplainsocketimpl.connect(abstractplainsocketimpl.java:182)         at java.net.sockssocketimpl.connect(sockssocketimpl.java:392)         at java.net.socket.connect(socket.java:579)         at org.apache.thrift.transport.tsocket.open(tsocket.java:182)         ... 9 more<stacktrace> $ java_home=~/fab/java ~/fab/stress/default/tools/bin/cassandra-stress write n=19000000 -rate threads=300 -node blade-11-4a,blade-11-3a,blade-11-2a exception in thread 'main' java.lang.runtimeexception: org.apache.thrift.transport.ttransportexception: java.net.connectexception: connection refused         at org.apache.cassandra.stress.settings.stresssettings.getrawthriftclient(stresssettings.java:144)         at org.apache.cassandra.stress.settings.stresssettings.getrawthriftclient(stresssettings.java:110)         at org.apache.cassandra.stress.settings.settingsschema.createkeyspacesthrift(settingsschema.java:111)         at org.apache.cassandra.stress.settings.settingsschema.createkeyspaces(settingsschema.java:59)         at org.apache.cassandra.stress.settings.stresssettings.maybecreatekeyspaces(stresssettings.java:205)         at org.apache.cassandra.stress.stressaction.run(stressaction.java:55)         at org.apache.cassandra.stress.stress.main(stress.java:109) caused by: org.apache.thrift.transport.ttransportexception: java.net.connectexception: connection refused         at org.apache.thrift.transport.tsocket.open(tsocket.java:187)         at org.apache.thrift.transport.tframedtransport.open(tframedtransport.java:81)         at org.apache.cassandra.thrift.tframedtransportfactory.opentransport(tframedtransportfactory.java:41)         at org.apache.cassandra.stress.settings.stresssettings.getrawthriftclient(stresssettings.java:124)         ... 6 more caused by: java.net.connectexception: connection refused         at java.net.plainsocketimpl.socketconnect(native method)         at java.net.abstractplainsocketimpl.doconnect(abstractplainsocketimpl.java:339)         at java.net.abstractplainsocketimpl.connecttoaddress(abstractplainsocketimpl.java:200)         at java.net.abstractplainsocketimpl.connect(abstractplainsocketimpl.java:182)         at java.net.sockssocketimpl.connect(sockssocketimpl.java:392)         at java.net.socket.connect(socket.java:579)         at org.apache.thrift.transport.tsocket.open(tsocket.java:182)         ... 9 more <code> <text> with cassandra-9319 the thrift server is turned off by default, which makes stress no longer work out of the box. even though stress uses native cql3 by default, there is still some remaining piece that uses thrift for schema creation. this is what you get by default now:",
        "label": 521
    },
    {
        "text": "counters in columns don't preserve correct values after cluster restart <description> similar to #3821. but affecting normal columns. set up a 2-node cluster with rf=2.  1. create a counter column family and increment a 100 keys in loop 5000 times.   2. then make a rolling restart to cluster.   3. again increment another 5000 times.  4. make a rolling restart to cluster.  5. again increment another 5000 times.  6. make a rolling restart to cluster. after step 6 we were able to reproduce bug with bad counter values.   expected values were 15 000. values returned from cluster are higher then 15000 + some random number.  rolling restarts are done with nodetool drain. always waiting until second node discover its down then kill java process.<stacktrace> <code> <text> similar to #3821. but affecting normal columns. set up a 2-node cluster with rf=2.  1. create a counter column family and increment a 100 keys in loop 5000 times.   2. then make a rolling restart to cluster.   3. again increment another 5000 times.  4. make a rolling restart to cluster.  5. again increment another 5000 times.  6. make a rolling restart to cluster. after step 6 we were able to reproduce bug with bad counter values.   expected values were 15 000. values returned from cluster are higher then 15000 + some random number.  rolling restarts are done with nodetool drain. always waiting until second node discover its down then kill java process.",
        "label": 520
    },
    {
        "text": "less crappy failure mode when swamped with inserts than  run out of memory and gc storm to death  <description> suggestion was made that http://java.sun.com/j2se/1.5.0/docs/api/java/lang/management/memorypoolmxbean.html#setcollectionusagethreshold(long) is relevant. correlation eludes me, but i am not a java expert.<stacktrace> suggestion was made that http://java.sun.com/j2se/1.5.0/docs/api/java/lang/management/memorypoolmxbean.html#setcollectionusagethreshold(long) is relevant. correlation eludes me, but i am not a java expert.<code> <text> ",
        "label": 274
    },
    {
        "text": "reject counter writes in cqlsstablewriter <description> we use cqlsstablewriter to produce testing datasets. here are the steps to reproduce this issue : 1) definition of a table with counter create table my_counter (   my_id text,   my_counter counter,   primary key (my_id) ) 2) with cqlsstablewriter initialize this table (about 2millions entries) with this insert order (one insert / key only)  update myks.my_counter set my_counter = my_counter + ? where my_id = ? 3) load the files written by cqlsstablewriter with sstableloader in your cassandra cluster (tested on a single node and a 3 nodes cluster) 4) start a process that updates the counters (we used 3millions entries distributed on the key my_id) 5) after a while try to query a key in the my_counter table  cqlsh:myks> select * from my_counter where my_id='0000001';  request did not complete within rpc_timeout. in the logs of cassandra (2.0.12) : error [compactionexecutor:3] 2015-05-28 15:53:39,491 cassandradaemon.java (line 258) exception in thread thread[compactionexecutor:3,1,main] java.lang.assertionerror: wrong class type.         at org.apache.cassandra.db.counterupdatecolumn.reconcile(counterupdatecolumn.java:70)         at org.apache.cassandra.db.arraybackedsortedcolumns.resolveagainst(arraybackedsortedcolumns.java:147)         at org.apache.cassandra.db.arraybackedsortedcolumns.addcolumn(arraybackedsortedcolumns.java:126)         at org.apache.cassandra.db.columnfamily.addcolumn(columnfamily.java:121)         at org.apache.cassandra.db.compaction.precompactedrow$1.reduce(precompactedrow.java:120)         at org.apache.cassandra.db.compaction.precompactedrow$1.reduce(precompactedrow.java:115)         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:112)         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:98)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:191)         at org.apache.cassandra.db.compaction.precompactedrow.merge(precompactedrow.java:144)         at org.apache.cassandra.db.compaction.precompactedrow.merge(precompactedrow.java:103)         at org.apache.cassandra.db.compaction.precompactedrow.<init>(precompactedrow.java:85)         at org.apache.cassandra.db.compaction.compactioncontroller.getcompactedrow(compactioncontroller.java:196)         at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:74)         at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:55)         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:115)         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:98)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:164)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:60)         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59)         at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:198)         at java.util.concurrent.executors$runnableadapter.call(executors.java:471)         at java.util.concurrent.futuretask.run(futuretask.java:262)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:745) in the logs of cassandra v2.1.5 : warn  [sharedpool-worker-38] 2015-06-11 16:39:06,008  abstracttracingawareexecutorservice.java:169 - uncaught exception on thread thread[sharedpool-worker-38,5, main]: {} java.lang.assertionerror: wrong class type: class org.apache.cassandra.db.buffercounterupdatecell         at org.apache.cassandra.db.abstractcell.reconcilecounter(abstractcell.java:211) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.buffercountercell.reconcile(buffercountercell.java:118) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter$1.reduce(queryfilter.java:122) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter$1.reduce(queryfilter.java:116) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:114) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:100) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143) ~[guava-16.0.1.jar:na]         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138) ~[guava-16.0.1.jar:na]         at org.apache.cassandra.db.filter.namesqueryfilter.collectreducedcolumns(namesqueryfilter.java:100) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter.collatecolumns(queryfilter.java:108) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:82) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:69) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:314) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:62) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1900) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1758) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.keyspace.getrow(keyspace.java:346) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.slicebynamesreadcommand.getrow(slicebynamesreadcommand.java:53) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.getcurrentvaluesfromcfs(countermutation.java:262) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.getcurrentvalues(countermutation.java:229) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.processmodifications(countermutation.java:197) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.apply(countermutation.java:124) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.service.storageproxy$8.runmaythrow(storageproxy.java:1155) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:2191) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_45]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [cassandra-all-2.1.5.469.jar:2.1.5.469]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] same exception as issue https://issues.apache.org/jira/browse/cassandra-7188<stacktrace> error [compactionexecutor:3] 2015-05-28 15:53:39,491 cassandradaemon.java (line 258) exception in thread thread[compactionexecutor:3,1,main] java.lang.assertionerror: wrong class type.         at org.apache.cassandra.db.counterupdatecolumn.reconcile(counterupdatecolumn.java:70)         at org.apache.cassandra.db.arraybackedsortedcolumns.resolveagainst(arraybackedsortedcolumns.java:147)         at org.apache.cassandra.db.arraybackedsortedcolumns.addcolumn(arraybackedsortedcolumns.java:126)         at org.apache.cassandra.db.columnfamily.addcolumn(columnfamily.java:121)         at org.apache.cassandra.db.compaction.precompactedrow$1.reduce(precompactedrow.java:120)         at org.apache.cassandra.db.compaction.precompactedrow$1.reduce(precompactedrow.java:115)         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:112)         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:98)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:191)         at org.apache.cassandra.db.compaction.precompactedrow.merge(precompactedrow.java:144)         at org.apache.cassandra.db.compaction.precompactedrow.merge(precompactedrow.java:103)         at org.apache.cassandra.db.compaction.precompactedrow.<init>(precompactedrow.java:85)         at org.apache.cassandra.db.compaction.compactioncontroller.getcompactedrow(compactioncontroller.java:196)         at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:74)         at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:55)         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:115)         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:98)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:164)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:60)         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59)         at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:198)         at java.util.concurrent.executors$runnableadapter.call(executors.java:471)         at java.util.concurrent.futuretask.run(futuretask.java:262)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:745) warn  [sharedpool-worker-38] 2015-06-11 16:39:06,008  abstracttracingawareexecutorservice.java:169 - uncaught exception on thread thread[sharedpool-worker-38,5, main]: {} java.lang.assertionerror: wrong class type: class org.apache.cassandra.db.buffercounterupdatecell         at org.apache.cassandra.db.abstractcell.reconcilecounter(abstractcell.java:211) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.buffercountercell.reconcile(buffercountercell.java:118) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter$1.reduce(queryfilter.java:122) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter$1.reduce(queryfilter.java:116) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:114) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:100) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143) ~[guava-16.0.1.jar:na]         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138) ~[guava-16.0.1.jar:na]         at org.apache.cassandra.db.filter.namesqueryfilter.collectreducedcolumns(namesqueryfilter.java:100) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter.collatecolumns(queryfilter.java:108) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:82) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.filter.queryfilter.collateondiskatom(queryfilter.java:69) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:314) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:62) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1900) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1758) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.keyspace.getrow(keyspace.java:346) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.slicebynamesreadcommand.getrow(slicebynamesreadcommand.java:53) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.getcurrentvaluesfromcfs(countermutation.java:262) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.getcurrentvalues(countermutation.java:229) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.processmodifications(countermutation.java:197) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.db.countermutation.apply(countermutation.java:124) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.service.storageproxy$8.runmaythrow(storageproxy.java:1155) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:2191) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_45]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) ~[cassandra-all-2.1.5.469.jar:2.1.5.469]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [cassandra-all-2.1.5.469.jar:2.1.5.469]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] <code> create table my_counter (   my_id text,   my_counter counter,   primary key (my_id) ) 3) load the files written by cqlsstablewriter with sstableloader in your cassandra cluster (tested on a single node and a 3 nodes cluster) same exception as issue https://issues.apache.org/jira/browse/cassandra-7188<text> we use cqlsstablewriter to produce testing datasets. here are the steps to reproduce this issue : 1) definition of a table with counter 2) with cqlsstablewriter initialize this table (about 2millions entries) with this insert order (one insert / key only)  update myks.my_counter set my_counter = my_counter + ? where my_id = ? 4) start a process that updates the counters (we used 3millions entries distributed on the key my_id) 5) after a while try to query a key in the my_counter table  cqlsh:myks> select * from my_counter where my_id='0000001';  request did not complete within rpc_timeout. in the logs of cassandra (2.0.12) : in the logs of cassandra v2.1.5 : ",
        "label": 409
    },
    {
        "text": "cqlsh does not connect if passwordauthenticator is enabled <description> the python driver was upgraded to v2.1 as part of cassandra-7509. this has changed it's auth_provider implementation meaning that cqlsh no longer connects if passwordauthenticator is configured and instead returns: typeerror: auth_provider must implement the cassandra.auth.authprovider interface when protocol_version >= 2<stacktrace> <code> <text> typeerror: auth_provider must implement the cassandra.auth.authprovider interface when protocol_version >= 2 the python driver was upgraded to v2.1 as part of cassandra-7509. this has changed it's auth_provider implementation meaning that cqlsh no longer connects if passwordauthenticator is configured and instead returns:",
        "label": 357
    },
    {
        "text": "stress   spams console with java util nosuchelementexception when run against nodes recently created <description> i don't get any stack trace on the console, but i get two java.util.nosuchelementexception for each operation stress is doing. this seems to occur when stress is being run against a recently created node (such as one from ccm). to reproduce: create a ccm cluster, and run stress against it within a few minutes . run a simple stress command like cassandra-stress write n=10 .<stacktrace> <code> <text> i don't get any stack trace on the console, but i get two java.util.nosuchelementexception for each operation stress is doing. this seems to occur when stress is being run against a recently created node (such as one from ccm). to reproduce: create a ccm cluster, and run stress against it within a few minutes . run a simple stress command like cassandra-stress write n=10 .",
        "label": 67
    },
    {
        "text": "dtest failure in replace address test testreplaceaddress insert data during replace same address test <description> example failure: http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testreport/replace_address_test/testreplaceaddress/insert_data_during_replace_same_address_test error message 13 sep 2016 05:42:44 [replacement] missing: ['writes will not be forwarded to this node during replacement']: info  [main] 2016-09-13 05:41:07,618 yamlconfigura..... see system.log for remainder stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/tools/decorators.py\", line 48, in wrapped     f(obj)   file \"/home/automaton/cassandra-dtest/replace_address_test.py\", line 422, in insert_data_during_replace_same_address_test     self._test_insert_data_during_replace(same_address=true)   file \"/home/automaton/cassandra-dtest/replace_address_test.py\", line 217, in _test_insert_data_during_replace     self._do_replace(same_address=same_address, extra_jvm_args=[\"-dcassandra.write_survey=true\"])   file \"/home/automaton/cassandra-dtest/replace_address_test.py\", line 112, in _do_replace     timeout=60)   file \"/home/automaton/src/ccm/ccmlib/node.py\", line 450, in watch_log_for     raise timeouterror(time.strftime(\"%d %b %y %h:%m:%s\", time.gmtime()) + \" [\" + self.name + \"] missing: \" + str([e.pattern for e in tofind]) + \":\\n\" + reads[:50] + \".....\\nsee {} for remainder\".format(filename))<stacktrace> <code> error message 13 sep 2016 05:42:44 [replacement] missing: ['writes will not be forwarded to this node during replacement']: info  [main] 2016-09-13 05:41:07,618 yamlconfigura..... see system.log for remainder stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/tools/decorators.py', line 48, in wrapped     f(obj)   file '/home/automaton/cassandra-dtest/replace_address_test.py', line 422, in insert_data_during_replace_same_address_test     self._test_insert_data_during_replace(same_address=true)   file '/home/automaton/cassandra-dtest/replace_address_test.py', line 217, in _test_insert_data_during_replace     self._do_replace(same_address=same_address, extra_jvm_args=['-dcassandra.write_survey=true'])   file '/home/automaton/cassandra-dtest/replace_address_test.py', line 112, in _do_replace     timeout=60)   file '/home/automaton/src/ccm/ccmlib/node.py', line 450, in watch_log_for     raise timeouterror(time.strftime('%d %b %y %h:%m:%s', time.gmtime()) + ' [' + self.name + '] missing: ' + str([e.pattern for e in tofind]) + ':/n' + reads[:50] + '...../nsee {} for remainder'.format(filename)) http://cassci.datastax.com/job/cassandra-3.9_novnode_dtest/48/testreport/replace_address_test/testreplaceaddress/insert_data_during_replace_same_address_test<text> example failure: ",
        "label": 428
    },
    {
        "text": "cassandra failed to run on linux aarch64 <description> steps to reproduce:  1. download cassandra latest source  2. build it with \"ant\"  3. run with \"./bin/cassandra\". daemon is crashed with following error message: info 05:30:21 initializing system.schema_functions  info 05:30:21 initializing system.schema_aggregates  error 05:30:22 exception in thread thread[memtableflushwriter:1,5,main]  java.lang.noclassdeffounderror: could not initialize class com.sun.jna.native  at org.apache.cassandra.utils.memory.memoryutil.allocate(memoryutil.java:97) ~[main/:na]  at org.apache.cassandra.io.util.memory.<init>(memory.java:74) ~[main/:na]  at org.apache.cassandra.io.util.safememory.<init>(safememory.java:32) ~[main/:na]  at org.apache.cassandra.io.compress.compressionmetadata$writer.<init>(compressionmetadata.java:316) ~[main/:na]  at org.apache.cassandra.io.compress.compressionmetadata$writer.open(compressionmetadata.java:330) ~[main/:na]  at org.apache.cassandra.io.compress.compressedsequentialwriter.<init>(compressedsequentialwriter.java:76) ~[main/:na]  at org.apache.cassandra.io.util.sequentialwriter.open(sequentialwriter.java:163) ~[main/:na]  at org.apache.cassandra.io.sstable.format.big.bigtablewriter.<init>(bigtablewriter.java:73) ~[main/:na]  at org.apache.cassandra.io.sstable.format.big.bigformat$writerfactory.open(bigformat.java:93) ~[main/:na]  at org.apache.cassandra.io.sstable.format.sstablewriter.create(sstablewriter.java:96) ~[main/:na]  at org.apache.cassandra.io.sstable.simplesstablemultiwriter.create(simplesstablemultiwriter.java:114) ~[main/:na]  at org.apache.cassandra.db.compaction.abstractcompactionstrategy.createsstablemultiwriter(abstractcompactionstrategy.java:519) ~[main/:na]  at org.apache.cassandra.db.compaction.compactionstrategymanager.createsstablemultiwriter(compactionstrategymanager.java:497) ~[main/:na]  at org.apache.cassandra.db.columnfamilystore.createsstablemultiwriter(columnfamilystore.java:480) ~[main/:na]  at org.apache.cassandra.db.memtable.createflushwriter(memtable.java:439) ~[main/:na]  at org.apache.cassandra.db.memtable.writesortedcontents(memtable.java:371) ~[main/:na]  at org.apache.cassandra.db.memtable.flush(memtable.java:332) ~[main/:na]  at org.apache.cassandra.db.columnfamilystore$flush.run(columnfamilystore.java:1054) ~[main/:na]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_111]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) ~[na:1.8.0_111]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_111] analyze:  this issue is caused by bundled jna-4.0.0.jar which doesn't come with aarch64 native support. replace lib/jna-4.0.0.jar with jna-4.2.0.jar from http://central.maven.org/maven2/net/java/dev/jna/jna/4.2.0/ can fix this problem. attached is the binary compatibility report of jna.jar between 4.0 and 4.2. the result is good (97.4%). so is there possibility to upgrade jna to 4.2.0 in upstream? should there be any kind of tests to execute, please kindly point me. thanks a lot.<stacktrace> info 05:30:21 initializing system.schema_functions  info 05:30:21 initializing system.schema_aggregates  error 05:30:22 exception in thread thread[memtableflushwriter:1,5,main]  java.lang.noclassdeffounderror: could not initialize class com.sun.jna.native  at org.apache.cassandra.utils.memory.memoryutil.allocate(memoryutil.java:97) ~[main/:na]  at org.apache.cassandra.io.util.memory.<init>(memory.java:74) ~[main/:na]  at org.apache.cassandra.io.util.safememory.<init>(safememory.java:32) ~[main/:na]  at org.apache.cassandra.io.compress.compressionmetadata$writer.<init>(compressionmetadata.java:316) ~[main/:na]  at org.apache.cassandra.io.compress.compressionmetadata$writer.open(compressionmetadata.java:330) ~[main/:na]  at org.apache.cassandra.io.compress.compressedsequentialwriter.<init>(compressedsequentialwriter.java:76) ~[main/:na]  at org.apache.cassandra.io.util.sequentialwriter.open(sequentialwriter.java:163) ~[main/:na]  at org.apache.cassandra.io.sstable.format.big.bigtablewriter.<init>(bigtablewriter.java:73) ~[main/:na]  at org.apache.cassandra.io.sstable.format.big.bigformat$writerfactory.open(bigformat.java:93) ~[main/:na]  at org.apache.cassandra.io.sstable.format.sstablewriter.create(sstablewriter.java:96) ~[main/:na]  at org.apache.cassandra.io.sstable.simplesstablemultiwriter.create(simplesstablemultiwriter.java:114) ~[main/:na]  at org.apache.cassandra.db.compaction.abstractcompactionstrategy.createsstablemultiwriter(abstractcompactionstrategy.java:519) ~[main/:na]  at org.apache.cassandra.db.compaction.compactionstrategymanager.createsstablemultiwriter(compactionstrategymanager.java:497) ~[main/:na]  at org.apache.cassandra.db.columnfamilystore.createsstablemultiwriter(columnfamilystore.java:480) ~[main/:na]  at org.apache.cassandra.db.memtable.createflushwriter(memtable.java:439) ~[main/:na]  at org.apache.cassandra.db.memtable.writesortedcontents(memtable.java:371) ~[main/:na]  at org.apache.cassandra.db.memtable.flush(memtable.java:332) ~[main/:na]  at org.apache.cassandra.db.columnfamilystore$flush.run(columnfamilystore.java:1054) ~[main/:na]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_111]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) ~[na:1.8.0_111]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_111] <code> analyze:  this issue is caused by bundled jna-4.0.0.jar which doesn't come with aarch64 native support. replace lib/jna-4.0.0.jar with jna-4.2.0.jar from http://central.maven.org/maven2/net/java/dev/jna/jna/4.2.0/ can fix this problem. <text> steps to reproduce:  1. download cassandra latest source  2. build it with 'ant'  3. run with './bin/cassandra'. daemon is crashed with following error message: attached is the binary compatibility report of jna.jar between 4.0 and 4.2. the result is good (97.4%). so is there possibility to upgrade jna to 4.2.0 in upstream? should there be any kind of tests to execute, please kindly point me. thanks a lot.",
        "label": 69
    },
    {
        "text": "structure for tpstats output  json  yaml  <description> in cassandra-5977, some extra output formats such as json and yaml were added for nodetool tablestats.   similarly, i would like to add the output formats in nodetool tpstats. also, i tried to refactor the tablestats's code about the output formats to integrate the existing code with my code. please review the attached patch.<stacktrace> <code> <text> in cassandra-5977, some extra output formats such as json and yaml were added for nodetool tablestats.   similarly, i would like to add the output formats in nodetool tpstats. also, i tried to refactor the tablestats's code about the output formats to integrate the existing code with my code. please review the attached patch.",
        "label": 207
    },
    {
        "text": "fix asynconeresponse <description> current implementation of asynconeresponse suffers from two problems:  1. spurious wakeup will lead to timeoutexception being thrown because awaiting for condition is not done in loop;  2. condition.signal() is used where .signalall() should be used - this leads to only one thread blocked on .get() to be unblocked. other threads will stay blocked forever.<stacktrace> <code> <text> current implementation of asynconeresponse suffers from two problems:  1. spurious wakeup will lead to timeoutexception being thrown because awaiting for condition is not done in loop;  2. condition.signal() is used where .signalall() should be used - this leads to only one thread blocked on .get() to be unblocked. other threads will stay blocked forever.",
        "label": 360
    },
    {
        "text": "nodes are not reported as alive after being marked down <description> to reproduce: start two nodes in foreground mode, then suspend (^z) one of them. once it is marked down, resume the process (fg) and it will not be marked up again.<stacktrace> <code> <text> to reproduce: start two nodes in foreground mode, then suspend (^z) one of them. once it is marked down, resume the process (fg) and it will not be marked up again.",
        "label": 85
    },
    {
        "text": "turn on log4j output during tests <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "data from truncated cf reappears after server restart <description> configure 3 node cluster ensure the java stress tool creates keyspace1 with rf=3 // run stress tool to generate 10 keys, 1 column stress --operation=insert -t 2 --num-keys=50 --columns=20 --consistency-level=quorum --average-size-values --replication-factor=3 --create-index=keys --nodes=cathy1,cathy2 // verify 50 keys in cli use keyspace1;  list standard1;  // truncate cf in cli use keyspace1; truncate counter1; list counter1; // run stress tool and verify creation of 1 key with 10 columns stress --operation=insert -t 2 --num-keys=1 --columns=10 --consistency-level=quorum --average-size-values --replication-factor=3 --create-index=keys --nodes=cathy1,cathy2 // verify 1 key in cli use keyspace1;  list standard1;  // restart all three nodes // you will see 51 keys in cli use keyspace1;  list standard1; <stacktrace> <code> // run stress tool to generate 10 keys, 1 column stress --operation=insert -t 2 --num-keys=50 --columns=20 --consistency-level=quorum --average-size-values --replication-factor=3 --create-index=keys --nodes=cathy1,cathy2 // verify 50 keys in cli use keyspace1;  list standard1;  // truncate cf in cli use keyspace1; truncate counter1; list counter1; // run stress tool and verify creation of 1 key with 10 columns stress --operation=insert -t 2 --num-keys=1 --columns=10 --consistency-level=quorum --average-size-values --replication-factor=3 --create-index=keys --nodes=cathy1,cathy2 // verify 1 key in cli use keyspace1;  list standard1;  // restart all three nodes // you will see 51 keys in cli use keyspace1;  list standard1; <text> ",
        "label": 274
    },
    {
        "text": "many sstablescanners opened during repair <description> since cassandra-5220 we open one sstablescanner per range per sstable. if compaction gets behind and you are running vnodes with 256 tokens and rf3, this could become a problem (ie, 768 * number of sstables scanners) we could probably refactor this similar to the way we handle scanners with lcs - only open the scanner once we need it<stacktrace> <code> since cassandra-5220 we open one sstablescanner per range per sstable. if compaction gets behind and you are running vnodes with 256 tokens and rf3, this could become a problem (ie, 768 * number of sstables scanners) <text> we could probably refactor this similar to the way we handle scanners with lcs - only open the scanner once we need it",
        "label": 321
    },
    {
        "text": "cassandrastorage does not function properly when used multiple times in a single pig script due to udfcontext sharing issues <description> cassandrastorage appears to have threading issues along the lines of those described at http://pig.markmail.org/message/oz7oz2x2dwp66eoz due to the sharing of the udfcontext. i believe the fix lies in implementing public void setstorefuncudfcontextsignature(string signature)     {     } and then using that signature when getting the udfcontext. from the pig manual: setstorefunc!udfcontextsignature(): this method will be called by pig both in the front end and back end to pass a unique signature to the storer. the signature can be used to store into the udfcontext any information which the storer needs to store between various method invocations in the front end and back end. the default implementation in storefunc has an empty body. this method will be called before other methods.<stacktrace> <code> public void setstorefuncudfcontextsignature(string signature)     {     } <text> cassandrastorage appears to have threading issues along the lines of those described at http://pig.markmail.org/message/oz7oz2x2dwp66eoz due to the sharing of the udfcontext. i believe the fix lies in implementing and then using that signature when getting the udfcontext. from the pig manual: setstorefunc!udfcontextsignature(): this method will be called by pig both in the front end and back end to pass a unique signature to the storer. the signature can be used to store into the udfcontext any information which the storer needs to store between various method invocations in the front end and back end. the default implementation in storefunc has an empty body. this method will be called before other methods.",
        "label": 246
    },
    {
        "text": "speed up start up sequence by avoiding un needed flushes <description> similar to cassandra-12969, do a conditional update for all functions  with a forced blocking flush to avoid slowed-down boot sequences. the  small performance hit of doing a read is always smaller than the one  associated with a fsync().<stacktrace> <code> <text> similar to cassandra-12969, do a conditional update for all functions  with a forced blocking flush to avoid slowed-down boot sequences. the  small performance hit of doing a read is always smaller than the one  associated with a fsync().",
        "label": 120
    },
    {
        "text": "hashmap illegalstateexception in anticompaction <description> error [antientropystage:4] 2014-09-05 19:55:06,439 cassandradaemon.java:166 - exception in thread thread[antientropystage:4,5,main] java.lang.runtimeexception: java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.illegalstateexception         at org.apache.cassandra.repair.repairmessageverbhandler.doverb(repairmessageverbhandler.java:117) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:62) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_60]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) ~[na:1.7.0_60]         at java.lang.thread.run(thread.java:745) ~[na:1.7.0_60] caused by: java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.illegalstateexception         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.utils.fbutilities.waitonfutures(fbutilities.java:400) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.repair.repairmessageverbhandler.doverb(repairmessageverbhandler.java:113) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         ... 4 common frames omitted caused by: java.util.concurrent.executionexception: java.lang.illegalstateexception         at java.util.concurrent.futuretask.report(futuretask.java:122) ~[na:1.7.0_60]         at java.util.concurrent.futuretask.get(futuretask.java:188) ~[na:1.7.0_60]         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         ... 6 common frames omitted caused by: java.lang.illegalstateexception: null         at java.util.hashmap$hashiterator.remove(hashmap.java:938) ~[na:1.7.0_60]         at org.apache.cassandra.db.compaction.compactionmanager.performanticompaction(compactionmanager.java:425) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.db.compaction.compactionmanager$6.runmaythrow(compactionmanager.java:381) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_60]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_60]         ... 3 common frames omitted and on a different host: error [compactionexecutor:43] 2014-09-05 19:55:06,376 cassandradaemon.java:166 - exception in thread thread[compactionexecutor:4 3,1,main] java.lang.illegalstateexception: null         at java.util.hashmap$hashiterator.remove(hashmap.java:938) ~[na:1.7.0_60]         at org.apache.cassandra.db.compaction.compactionmanager.performanticompaction(compactionmanager.java:425) ~[apache-cassa ndra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.db.compaction.compactionmanager$6.runmaythrow(compactionmanager.java:381) ~[apache-cassandra-2.1 .0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_60]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_60]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_60]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_60]         at java.lang.thread.run(thread.java:745) [na:1.7.0_60] the exceptions happened in our test environment after i added a bunch of new-nodes at the same time that was in a different (previously non-existing) datacenter. the datacenter is determined using gossipingpropertyfilesnitch and started repairs on a bunch of nodes at the same time using nodetool -h <node> repair -inc -par the nodes in the old datacenter had previously been repaired using the -inc option.<stacktrace> error [antientropystage:4] 2014-09-05 19:55:06,439 cassandradaemon.java:166 - exception in thread thread[antientropystage:4,5,main] java.lang.runtimeexception: java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.illegalstateexception         at org.apache.cassandra.repair.repairmessageverbhandler.doverb(repairmessageverbhandler.java:117) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:62) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_60]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) ~[na:1.7.0_60]         at java.lang.thread.run(thread.java:745) ~[na:1.7.0_60] caused by: java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.illegalstateexception         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.utils.fbutilities.waitonfutures(fbutilities.java:400) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.repair.repairmessageverbhandler.doverb(repairmessageverbhandler.java:113) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         ... 4 common frames omitted caused by: java.util.concurrent.executionexception: java.lang.illegalstateexception         at java.util.concurrent.futuretask.report(futuretask.java:122) ~[na:1.7.0_60]         at java.util.concurrent.futuretask.get(futuretask.java:188) ~[na:1.7.0_60]         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         ... 6 common frames omitted caused by: java.lang.illegalstateexception: null         at java.util.hashmap$hashiterator.remove(hashmap.java:938) ~[na:1.7.0_60]         at org.apache.cassandra.db.compaction.compactionmanager.performanticompaction(compactionmanager.java:425) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.db.compaction.compactionmanager$6.runmaythrow(compactionmanager.java:381) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_60]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_60]         ... 3 common frames omitted error [compactionexecutor:43] 2014-09-05 19:55:06,376 cassandradaemon.java:166 - exception in thread thread[compactionexecutor:4 3,1,main] java.lang.illegalstateexception: null         at java.util.hashmap$hashiterator.remove(hashmap.java:938) ~[na:1.7.0_60]         at org.apache.cassandra.db.compaction.compactionmanager.performanticompaction(compactionmanager.java:425) ~[apache-cassa ndra-2.1.0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.db.compaction.compactionmanager$6.runmaythrow(compactionmanager.java:381) ~[apache-cassandra-2.1 .0-rc5.jar:2.1.0-rc5]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[apache-cassandra-2.1.0-rc5.jar:2.1.0-rc5]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_60]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_60]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_60]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_60]         at java.lang.thread.run(thread.java:745) [na:1.7.0_60] <code> <text> nodetool -h <node> repair -inc -par and on a different host: the exceptions happened in our test environment after i added a bunch of new-nodes at the same time that was in a different (previously non-existing) datacenter. the datacenter is determined using gossipingpropertyfilesnitch and started repairs on a bunch of nodes at the same time using the nodes in the old datacenter had previously been repaired using the -inc option.",
        "label": 362
    },
    {
        "text": "don't skip corrupt sstables on startup <description> if we get an ioexception when opening an sstable on startup, we just skip it and continue starting we should use the diskfailurepolicy and never explicitly catch an ioexception here<stacktrace> <code> <text> if we get an ioexception when opening an sstable on startup, we just skip it and continue starting we should use the diskfailurepolicy and never explicitly catch an ioexception here",
        "label": 321
    },
    {
        "text": "node refuses to start with exception in columnfamilystore removeunfinishedcompactionleftovers when find that some to be removed files are already removed <description> node refuses to start with caused by: java.lang.illegalstateexception: unfinished compactions reference missing sstables. this should never happen since compactions are marked finished before we start removing the old sstables.       at org.apache.cassandra.db.columnfamilystore.removeunfinishedcompactionleftovers(columnfamilystore.java:544)       at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:262) imo, there is no reason to refuse to start discivering files that must be removed are already removed. it looks like pure bug diagnostic code and mean nothing to operator (nor he can do anything about this). replaced throw of excepion with dump of diagnostic warning and continue startup.<stacktrace> caused by: java.lang.illegalstateexception: unfinished compactions reference missing sstables. this should never happen since compactions are marked finished before we start removing the old sstables.       at org.apache.cassandra.db.columnfamilystore.removeunfinishedcompactionleftovers(columnfamilystore.java:544)       at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:262) <code> <text> node refuses to start with imo, there is no reason to refuse to start discivering files that must be removed are already removed. it looks like pure bug diagnostic code and mean nothing to operator (nor he can do anything about this). replaced throw of excepion with dump of diagnostic warning and continue startup.",
        "label": 538
    },
    {
        "text": " bat files fails when cassandra home contains a white space  <description> issues 2952 and 2237 fixed the issue for cassandra.bat. but the following bat files need the same fix:  json2sstable.bat  nodetool.bat  sstable2json.bat  sstablekeys.bat<stacktrace> <code> <text> issues 2952 and 2237 fixed the issue for cassandra.bat. but the following bat files need the same fix:  json2sstable.bat  nodetool.bat  sstable2json.bat  sstablekeys.bat",
        "label": 532
    },
    {
        "text": "closing cqlrecordreader doesn't release all resources <description> closing cqlrecordreader doesn't release all resources - only session object is closed, cluster remains open. it may result in \"too many open files\" error, caused by still open cassandra connections (i had this error while working on my own inputformat for cassandra - https://bitbucket.org/mrk/cassandra_hadoop_utils/). attached patch fixes this issue for me.<stacktrace> <code> <text> closing cqlrecordreader doesn't release all resources - only session object is closed, cluster remains open. it may result in 'too many open files' error, caused by still open cassandra connections (i had this error while working on my own inputformat for cassandra - https://bitbucket.org/mrk/cassandra_hadoop_utils/). attached patch fixes this issue for me.",
        "label": 324
    },
    {
        "text": "specify keyspace in error message when streaming fails due to missing replicas <description> when there aren't sufficient live replicas for streaming (during bootstrap, etc), you'll get an error message like \"unable to find sufficient sources for streaming range\". it would be helpful to include the keyspace that this failed for, since each keyspace can have different replication settings.<stacktrace> <code> <text> when there aren't sufficient live replicas for streaming (during bootstrap, etc), you'll get an error message like 'unable to find sufficient sources for streaming range'. it would be helpful to include the keyspace that this failed for, since each keyspace can have different replication settings.",
        "label": 439
    },
    {
        "text": "assertionerror with static columns <description> depending on how you insert static column values, regular values (and implicitly static column row markers and regular column rows markers), you can cause an assertionerror on select (collision between some of the empty composite name fragments) if you have an empty partition key example: cqlsh:test> create table select_error(pkey text, ckey text, value text, static_value text static, primary key(pkey, ckey)); cqlsh:test> insert into select_error(pkey, static_value) values('partition1', 'static value'); cqlsh:test> insert into select_error(pkey, ckey, value) values('partition1', '', 'value'); cqlsh:test> select * from select_error; tsocket read 0 bytes causes java.lang.assertionerror at org.apache.cassandra.cql3.statements.columngroupmap.add(columngroupmap.java:64) at org.apache.cassandra.cql3.statements.columngroupmap.access$200(columngroupmap.java:32) at org.apache.cassandra.cql3.statements.columngroupmap$builder.add(columngroupmap.java:151) at org.apache.cassandra.cql3.statements.selectstatement.processcolumnfamily(selectstatement.java:1202) at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1078) at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:280) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:257) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:222) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:60) at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:158) at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:175) at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1958) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4486) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4470) at org.apache.thrift.processfunction.process(processfunction.java:39) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:201) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:744)<stacktrace> java.lang.assertionerror at org.apache.cassandra.cql3.statements.columngroupmap.add(columngroupmap.java:64) at org.apache.cassandra.cql3.statements.columngroupmap.access$200(columngroupmap.java:32) at org.apache.cassandra.cql3.statements.columngroupmap$builder.add(columngroupmap.java:151) at org.apache.cassandra.cql3.statements.selectstatement.processcolumnfamily(selectstatement.java:1202) at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1078) at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:280) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:257) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:222) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:60) at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:158) at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:175) at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1958) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4486) at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4470) at org.apache.thrift.processfunction.process(processfunction.java:39) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:201) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:744) <code> cqlsh:test> create table select_error(pkey text, ckey text, value text, static_value text static, primary key(pkey, ckey)); cqlsh:test> insert into select_error(pkey, static_value) values('partition1', 'static value'); cqlsh:test> insert into select_error(pkey, ckey, value) values('partition1', '', 'value'); cqlsh:test> select * from select_error; tsocket read 0 bytes <text> depending on how you insert static column values, regular values (and implicitly static column row markers and regular column rows markers), you can cause an assertionerror on select (collision between some of the empty composite name fragments) if you have an empty partition key example: causes",
        "label": 520
    },
    {
        "text": "ae in bounds when running word count <description> just run the word count in examples: error 20:01:24,693 exception in thread thread[thrift:16,5,main] java.lang.assertionerror: [decoratedkey(663380155395974698, 6b6579333137),max(-8100212023555812159)]         at org.apache.cassandra.dht.bounds.<init>(bounds.java:41)         at org.apache.cassandra.dht.bounds.<init>(bounds.java:34)         at org.apache.cassandra.thrift.cassandraserver.get_paged_slice(cassandraserver.java:1041)         at org.apache.cassandra.thrift.cassandra$processor$get_paged_slice.getresult(cassandra.java:3478)         at org.apache.cassandra.thrift.cassandra$processor$get_paged_slice.getresult(cassandra.java:3466)         at org.apache.thrift.processfunction.process(processfunction.java:32)         at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34)         at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:199)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662)<stacktrace> error 20:01:24,693 exception in thread thread[thrift:16,5,main] java.lang.assertionerror: [decoratedkey(663380155395974698, 6b6579333137),max(-8100212023555812159)]         at org.apache.cassandra.dht.bounds.<init>(bounds.java:41)         at org.apache.cassandra.dht.bounds.<init>(bounds.java:34)         at org.apache.cassandra.thrift.cassandraserver.get_paged_slice(cassandraserver.java:1041)         at org.apache.cassandra.thrift.cassandra$processor$get_paged_slice.getresult(cassandra.java:3478)         at org.apache.cassandra.thrift.cassandra$processor$get_paged_slice.getresult(cassandra.java:3466)         at org.apache.thrift.processfunction.process(processfunction.java:32)         at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34)         at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:199)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) <code> <text> just run the word count in examples:",
        "label": 85
    },
    {
        "text": "support streaming of older version sstables in <description> in 2.0 we introduced support for streaming older versioned sstables (cassandra-5772). in 3.0, because of the rewrite of the storage layer, this became no longer supported. so currently, while 3.0 can read sstables in the 2.1/2.2 format, it cannot stream the older versioned sstables. we should do some work to make this still possible to be consistent with what cassandra-5772 provided.<stacktrace> <code> <text> in 2.0 we introduced support for streaming older versioned sstables (cassandra-5772). in 3.0, because of the rewrite of the storage layer, this became no longer supported. so currently, while 3.0 can read sstables in the 2.1/2.2 format, it cannot stream the older versioned sstables. we should do some work to make this still possible to be consistent with what cassandra-5772 provided.",
        "label": 409
    },
    {
        "text": "implement virtual tables <description> there are a variety of reasons to want virtual tables, which would be any table that would be backed by an api, rather than data explicitly managed and stored as sstables. one possible use case would be to expose jmx data through cql as a resurrection of cassandra-3527. another is a more general framework to implement the ability to expose yaml configuration information. so it would be an alternate approach to cassandra-7370. a possible implementation would be in terms of cassandra-7443, but i am not presupposing.<stacktrace> <code> <text> there are a variety of reasons to want virtual tables, which would be any table that would be backed by an api, rather than data explicitly managed and stored as sstables. one possible use case would be to expose jmx data through cql as a resurrection of cassandra-3527. another is a more general framework to implement the ability to expose yaml configuration information. so it would be an alternate approach to cassandra-7370. a possible implementation would be in terms of cassandra-7443, but i am not presupposing.",
        "label": 106
    },
    {
        "text": "dtest failure in compaction test testcompaction with leveledcompactionstrategy compaction throughput test <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1479/testreport/compaction_test/testcompaction_with_leveledcompactionstrategy/compaction_throughput_test error message 5.5 not greater than or equal to 6.175 stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/compaction_test.py\", line 280, in compaction_throughput_test     self.assertgreaterequal(float(threshold) + 0.5, float(avgthroughput))   file \"/usr/lib/python2.7/unittest/case.py\", line 948, in assertgreaterequal     self.fail(self._formatmessage(msg, standardmsg))   file \"/usr/lib/python2.7/unittest/case.py\", line 410, in fail     raise self.failureexception(msg) related failure:  http://cassci.datastax.com/job/trunk_dtest/1479/testreport/compaction_test/testcompaction_with_sizetieredcompactionstrategy/compaction_throughput_test/  http://cassci.datastax.com/job/trunk_dtest/1481/testreport/compaction_test/testcompaction_with_datetieredcompactionstrategy/compaction_throughput_test/ note: this test is failing for both leveled compaction, size tiered compaction, and date tiered compaction.<stacktrace> <code> error message 5.5 not greater than or equal to 6.175 stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/compaction_test.py', line 280, in compaction_throughput_test     self.assertgreaterequal(float(threshold) + 0.5, float(avgthroughput))   file '/usr/lib/python2.7/unittest/case.py', line 948, in assertgreaterequal     self.fail(self._formatmessage(msg, standardmsg))   file '/usr/lib/python2.7/unittest/case.py', line 410, in fail     raise self.failureexception(msg) http://cassci.datastax.com/job/trunk_dtest/1479/testreport/compaction_test/testcompaction_with_leveledcompactionstrategy/compaction_throughput_test related failure:  http://cassci.datastax.com/job/trunk_dtest/1479/testreport/compaction_test/testcompaction_with_sizetieredcompactionstrategy/compaction_throughput_test/  http://cassci.datastax.com/job/trunk_dtest/1481/testreport/compaction_test/testcompaction_with_datetieredcompactionstrategy/compaction_throughput_test/ <text> example failure: note: this test is failing for both leveled compaction, size tiered compaction, and date tiered compaction.",
        "label": 17
    },
    {
        "text": "change news txt in to make it clear that a repair is required post upgrade <description> in trunk 1.2.x, news.txt says the following : - the hints schema was changed from 1.1 to 1.2. cassandra automatically snapshots and then truncates the hints column family as part of starting up 1.2 for the first time.  additionally, upgraded nodes will not store new hints destined for older (pre-1.2) nodes. it is therefore recommended that you perform a cluster upgrade when all nodes are up. but this means that : 1) as you do a rolling restart upgrade, your new nodes don't store hints for old nodes as they restart for the upgrade.  2) the last old node being upgraded will be not have any hints stored for it while it is upgrading. these two facts mean that you need to run a cluster-wide repair after upgrading in order to be as consistent as you would expect if hinted handoff were working. i propose modifying news.txt to read : - the hints schema was changed from 1.1 to 1.2. cassandra automatically snapshots and then truncates the hints column family as part of starting up 1.2 for the first time.  additionally, upgraded nodes will not store new hints destined for older (pre-1.2) nodes. it is therefore recommended that you perform a cluster upgrade when all nodes are up. because hints *will* be lost, a cluster-wide repair (with -pr) is required after upgrade of all nodes.<stacktrace> <code> <text> - the hints schema was changed from 1.1 to 1.2. cassandra automatically snapshots and then truncates the hints column family as part of starting up 1.2 for the first time.  additionally, upgraded nodes will not store new hints destined for older (pre-1.2) nodes. it is therefore recommended that you perform a cluster upgrade when all nodes are up. - the hints schema was changed from 1.1 to 1.2. cassandra automatically snapshots and then truncates the hints column family as part of starting up 1.2 for the first time.  additionally, upgraded nodes will not store new hints destined for older (pre-1.2) nodes. it is therefore recommended that you perform a cluster upgrade when all nodes are up. because hints *will* be lost, a cluster-wide repair (with -pr) is required after upgrade of all nodes. in trunk 1.2.x, news.txt says the following : but this means that : 1) as you do a rolling restart upgrade, your new nodes don't store hints for old nodes as they restart for the upgrade.  2) the last old node being upgraded will be not have any hints stored for it while it is upgrading. these two facts mean that you need to run a cluster-wide repair after upgrading in order to be as consistent as you would expect if hinted handoff were working. i propose modifying news.txt to read :",
        "label": 451
    },
    {
        "text": "select json does not return the correct value for empty blobs <description> in an attempt to fix the side effect of a problem cassandra-13868 was committed in 2.2.11, 3.0.15, 3.11.1 and trunk.  this patch introduced an issue on how empty values were rendered by select json queries.  instead of returning the correct value for the type a null value was now returned.  a user detected that problem for text column and opened cassandra-14245 to request a fix for that problem. unfortunately, i misunderstood the problem and the fix did not solve the real problem. it only made the code return 'an empty string instead of null values. the proper fix is to rollback the changes made for cassandra-13868 and cassandra-14245.  some unit tests also need to be added to test the behavior.<stacktrace> <code> <text> in an attempt to fix the side effect of a problem cassandra-13868 was committed in 2.2.11, 3.0.15, 3.11.1 and trunk.  this patch introduced an issue on how empty values were rendered by select json queries.  instead of returning the correct value for the type a null value was now returned.  a user detected that problem for text column and opened cassandra-14245 to request a fix for that problem. unfortunately, i misunderstood the problem and the fix did not solve the real problem. it only made the code return 'an empty string instead of null values. the proper fix is to rollback the changes made for cassandra-13868 and cassandra-14245.  some unit tests also need to be added to test the behavior.",
        "label": 165
    },
    {
        "text": "reduce contention in compositetype instance interning <description> while running some workload tests on cassandra 2.2.1 and profiling with flight recorder in a test environment, we have noticed significant contention on the static synchronized org.apache.cassandra.db.marshal.compositetype.getinstance(list) method. we are seeing threads blocked for 22.828 seconds from a 60 second snapshot while under a mix of reads and writes from a thrift based client. i would propose to reduce contention in org.apache.cassandra.db.marshal.compositetype.getinstance(list) by using a concurrenthashmap for the instances cache. contention back trace org.apache.cassandra.db.marshal.compositetype.getinstance(list)   org.apache.cassandra.db.composites.abstractcompoundcellnametype.asabstracttype()     org.apache.cassandra.db.supercolumns.getcomparatorfor(cfmetadata, boolean)       org.apache.cassandra.db.supercolumns.getcomparatorfor(cfmetadata, bytebuffer)         org.apache.cassandra.thrift.thriftvalidation.validatecolumnnames(cfmetadata, bytebuffer, iterable)           org.apache.cassandra.thrift.thriftvalidation.validatecolumnpath(cfmetadata, columnpath)             org.apache.cassandra.thrift.thriftvalidation.validatecolumnorsupercolumn(cfmetadata, bytebuffer, columnorsupercolumn)               org.apache.cassandra.thrift.thriftvalidation.validatemutation(cfmetadata, bytebuffer, mutation)                 org.apache.cassandra.thrift.cassandraserver.createmutationlist(consistencylevel, map, boolean)                   org.apache.cassandra.thrift.cassandraserver.batch_mutate(map, consistencylevel)                     org.apache.cassandra.thrift.cassandra$processor$batch_mutate.getresult(cassandra$iface, cassandra$batch_mutate_args)         org.apache.cassandra.thrift.thriftvalidation.validaterange(cfmetadata, columnparent, slicerange)           org.apache.cassandra.thrift.thriftvalidation.validatepredicate(cfmetadata, columnparent, slicepredicate)             org.apache.cassandra.thrift.cassandraserver.get_range_slices(columnparent, slicepredicate, keyrange, consistencylevel)               org.apache.cassandra.thrift.cassandra$processor$get_range_slices.getresult(cassandra$iface, cassandra$get_range_slices_args)                 org.apache.cassandra.thrift.cassandra$processor$get_range_slices.getresult(object, tbase)                   org.apache.thrift.processfunction.process(int, tprotocol, tprotocol, object)                     org.apache.thrift.tbaseprocessor.process(tprotocol, tprotocol)                       org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run()                         java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor$worker)                           java.util.concurrent.threadpoolexecutor$worker.run()             org.apache.cassandra.thrift.cassandraserver.multigetsliceinternal(string, list, columnparent, long, slicepredicate, consistencylevel, clientstate)               org.apache.cassandra.thrift.cassandraserver.multiget_slice(list, columnparent, slicepredicate, consistencylevel)                 org.apache.cassandra.thrift.cassandra$processor$multiget_slice.getresult(cassandra$iface, cassandra$multiget_slice_args)                   org.apache.cassandra.thrift.cassandra$processor$multiget_slice.getresult(object, tbase)<stacktrace> <code> contention back trace org.apache.cassandra.db.marshal.compositetype.getinstance(list)   org.apache.cassandra.db.composites.abstractcompoundcellnametype.asabstracttype()     org.apache.cassandra.db.supercolumns.getcomparatorfor(cfmetadata, boolean)       org.apache.cassandra.db.supercolumns.getcomparatorfor(cfmetadata, bytebuffer)         org.apache.cassandra.thrift.thriftvalidation.validatecolumnnames(cfmetadata, bytebuffer, iterable)           org.apache.cassandra.thrift.thriftvalidation.validatecolumnpath(cfmetadata, columnpath)             org.apache.cassandra.thrift.thriftvalidation.validatecolumnorsupercolumn(cfmetadata, bytebuffer, columnorsupercolumn)               org.apache.cassandra.thrift.thriftvalidation.validatemutation(cfmetadata, bytebuffer, mutation)                 org.apache.cassandra.thrift.cassandraserver.createmutationlist(consistencylevel, map, boolean)                   org.apache.cassandra.thrift.cassandraserver.batch_mutate(map, consistencylevel)                     org.apache.cassandra.thrift.cassandra$processor$batch_mutate.getresult(cassandra$iface, cassandra$batch_mutate_args)         org.apache.cassandra.thrift.thriftvalidation.validaterange(cfmetadata, columnparent, slicerange)           org.apache.cassandra.thrift.thriftvalidation.validatepredicate(cfmetadata, columnparent, slicepredicate)             org.apache.cassandra.thrift.cassandraserver.get_range_slices(columnparent, slicepredicate, keyrange, consistencylevel)               org.apache.cassandra.thrift.cassandra$processor$get_range_slices.getresult(cassandra$iface, cassandra$get_range_slices_args)                 org.apache.cassandra.thrift.cassandra$processor$get_range_slices.getresult(object, tbase)                   org.apache.thrift.processfunction.process(int, tprotocol, tprotocol, object)                     org.apache.thrift.tbaseprocessor.process(tprotocol, tprotocol)                       org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run()                         java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor$worker)                           java.util.concurrent.threadpoolexecutor$worker.run()             org.apache.cassandra.thrift.cassandraserver.multigetsliceinternal(string, list, columnparent, long, slicepredicate, consistencylevel, clientstate)               org.apache.cassandra.thrift.cassandraserver.multiget_slice(list, columnparent, slicepredicate, consistencylevel)                 org.apache.cassandra.thrift.cassandra$processor$multiget_slice.getresult(cassandra$iface, cassandra$multiget_slice_args)                   org.apache.cassandra.thrift.cassandra$processor$multiget_slice.getresult(object, tbase) <text> while running some workload tests on cassandra 2.2.1 and profiling with flight recorder in a test environment, we have noticed significant contention on the static synchronized org.apache.cassandra.db.marshal.compositetype.getinstance(list) method. we are seeing threads blocked for 22.828 seconds from a 60 second snapshot while under a mix of reads and writes from a thrift based client. i would propose to reduce contention in org.apache.cassandra.db.marshal.compositetype.getinstance(list) by using a concurrenthashmap for the instances cache.",
        "label": 143
    },
    {
        "text": "cql help in trunk doc cql3 cql textile outdated <description> https://github.com/apache/cassandra/blob/trunk/doc/cql3/cql.textile doesn't include the new create keyspace syntax or the collections. last time, i updated the cql.textile for paul cannon to review. want me to do it again? bnr-like formatting needs to be replaced, right?, because the brackets now have literal meaning. i test-applied this custom formatting to commands and it seems ok: uppercase means literal (lowercase nonliteral), italics mean optional, the | symbol means or, ... means repeatable. the ... in italics doesn't strictly explain things like nested [...] does, but it's easier on the eyes and loosely understandable. any doubt could be erased by examples, i think.<stacktrace> <code> <text> https://github.com/apache/cassandra/blob/trunk/doc/cql3/cql.textile doesn't include the new create keyspace syntax or the collections. last time, i updated the cql.textile for paul cannon to review. want me to do it again? bnr-like formatting needs to be replaced, right?, because the brackets now have literal meaning. i test-applied this custom formatting to commands and it seems ok: uppercase means literal (lowercase nonliteral), italics mean optional, the | symbol means or, ... means repeatable. the ... in italics doesn't strictly explain things like nested [...] does, but it's easier on the eyes and loosely understandable. any doubt could be erased by examples, i think.",
        "label": 169
    },
    {
        "text": "sstable generation loading test sstableloader compression  dtests fail in   and <description> this looks like something fundamentally wrong with the sstable_generation_loading_test.py dtest - 1.2 and 2.0 look like: $ export max_heap_size=\"1g\"; export heap_newsize=\"256m\"; print_debug=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py nose.config: info: ignoring files matching ['^\\\\.', '^_', '^setup\\\\.py$'] incompressible_data_in_compressed_table_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-sjdykb ok remove_index_file_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-grtcgb created keyspaces. sleeping 1s for propagation. total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time 10000,1000,1000,11.5,80.3,280.6,5 end ok sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-xneocv testing sstableloader with pre_compression=deflate and post_compression=deflate creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-_ij1td testing sstableloader with pre_compression=deflate and post_compression=none creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-fzaci9 testing sstableloader with pre_compression=deflate and post_compression=snappy creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-asoepn testing sstableloader with pre_compression=none and post_compression=deflate creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-cfiwh8 testing sstableloader with pre_compression=none and post_compression=none creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-ttuqqg testing sstableloader with pre_compression=none and post_compression=snappy creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-la0rxi testing sstableloader with pre_compression=snappy and post_compression=deflate creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-jj5iub testing sstableloader with pre_compression=snappy and post_compression=none creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-jtmbkg testing sstableloader with pre_compression=snappy and post_compression=snappy creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail ====================================================================== fail: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 108, in sstableloader_compression_deflate_to_deflate_test     self.load_sstable_with_configuration('deflate', 'deflate')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 102, in sstableloader_compression_deflate_to_none_test     self.load_sstable_with_configuration('deflate', none)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 105, in sstableloader_compression_deflate_to_snappy_test     self.load_sstable_with_configuration('deflate', 'snappy')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 90, in sstableloader_compression_none_to_deflate_test     self.load_sstable_with_configuration(none, 'deflate')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 84, in sstableloader_compression_none_to_none_test     self.load_sstable_with_configuration(none, none)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 87, in sstableloader_compression_none_to_snappy_test     self.load_sstable_with_configuration(none, 'snappy')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 99, in sstableloader_compression_snappy_to_deflate_test     self.load_sstable_with_configuration('snappy', 'deflate')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 93, in sstableloader_compression_snappy_to_none_test     self.load_sstable_with_configuration('snappy', none)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 96, in sstableloader_compression_snappy_to_snappy_test     self.load_sstable_with_configuration('snappy', 'snappy')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ---------------------------------------------------------------------- ran 11 tests in 336.704s failed (failures=9) 2.1 looks like: $ export max_heap_size=\"1g\"; export heap_newsize=\"256m\"; print_debug=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py nose.config: info: ignoring files matching ['^\\\\.', '^_', '^setup\\\\.py$'] incompressible_data_in_compressed_table_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-gpkstl [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) error remove_index_file_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-vx3bsj [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-ap0vwz testing sstableloader with pre_compression=deflate and post_compression=deflate [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-nsy44b testing sstableloader with pre_compression=deflate and post_compression=none [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-77_xih testing sstableloader with pre_compression=deflate and post_compression=snappy [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-x3esb0 testing sstableloader with pre_compression=none and post_compression=deflate [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-hgjni9 testing sstableloader with pre_compression=none and post_compression=none [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-qkuef5 testing sstableloader with pre_compression=none and post_compression=snappy [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-mgv9zg testing sstableloader with pre_compression=snappy and post_compression=deflate [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-e0mevq testing sstableloader with pre_compression=snappy and post_compression=none [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-x3d6bu testing sstableloader with pre_compression=snappy and post_compression=snappy [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error ====================================================================== error: incompressible_data_in_compressed_table_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 25, in incompressible_data_in_compressed_table_test     cluster.populate(1).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: remove_index_file_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 55, in remove_index_file_test     cluster.populate(1).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 108, in sstableloader_compression_deflate_to_deflate_test     self.load_sstable_with_configuration('deflate', 'deflate')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 102, in sstableloader_compression_deflate_to_none_test     self.load_sstable_with_configuration('deflate', none)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 105, in sstableloader_compression_deflate_to_snappy_test     self.load_sstable_with_configuration('deflate', 'snappy')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 90, in sstableloader_compression_none_to_deflate_test     self.load_sstable_with_configuration(none, 'deflate')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 84, in sstableloader_compression_none_to_none_test     self.load_sstable_with_configuration(none, none)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 87, in sstableloader_compression_none_to_snappy_test     self.load_sstable_with_configuration(none, 'snappy')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 99, in sstableloader_compression_snappy_to_deflate_test     self.load_sstable_with_configuration('snappy', 'deflate')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 93, in sstableloader_compression_snappy_to_none_test     self.load_sstable_with_configuration('snappy', none)   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 96, in sstableloader_compression_snappy_to_snappy_test     self.load_sstable_with_configuration('snappy', 'snappy')   file \"/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py\", line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file \"/home/mshuler/git/ccm/ccmlib/cluster.py\", line 230, in start     raise nodeerror(\"error starting {0}.\".format(node.name), p) nodeerror: error starting node1. ---------------------------------------------------------------------- ran 11 tests in 56.599s failed (errors=11) 2.1 last ccm node1 log (node2 is the same): info  [main] 2014-04-08 19:20:33,339 cassandradaemon.java:102 - hostname: hana.12.am info  [main] 2014-04-08 19:20:33,396 yamlconfigurationloader.java:80 - loading settings from file:/tmp/dtest-x3d6bu/test/node1/conf/cassandra.yaml info  [main] 2014-04-08 19:20:33,514 yamlconfigurationloader.java:123 - node configuration:[authenticator=allowallauthenticator; authorizer=allowallauthorizer; auto_bootstrap=false; auto_snapshot=true; batchlog_replay_throttle_in_kb=1024; cas_contention_timeout_in_ms=1000; client_encryption_options=<redacted>; cluster_name=test; column_index_size_in_kb=64; commitlog_directory=/tmp/dtest-x3d6bu/test/node1/commitlogs; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_period_in_ms=10000; compaction_preheat_key_cache=true; compaction_throughput_mb_per_sec=16; concurrent_counter_writes=32; concurrent_reads=32; concurrent_writes=32; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=[/tmp/dtest-x3d6bu/test/node1/data]; disk_failure_policy=stop; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; endpoint_snitch=simplesnitch; flush_directory=/tmp/dtest-x3d6bu/test/node1/flush; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; in_memory_compaction_limit_in_mb=64; incremental_backups=false; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=-9223372036854775808; inter_dc_tcp_nodelay=false; internode_compression=all; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=127.0.0.1; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=0.4; native_transport_port=9042; partitioner=org.apache.cassandra.dht.murmur3partitioner; permissions_validity_in_ms=2000; phi_convict_threshold=5; preheat_kernel_page_cache=false; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=10000; request_scheduler=org.apache.cassandra.scheduler.noscheduler; request_timeout_in_ms=10000; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=127.0.0.1; rpc_keepalive=true; rpc_port=9160; rpc_server_type=sync; saved_caches_directory=/tmp/dtest-x3d6bu/test/node1/saved_caches; seed_provider=[{class_name=org.apache.cassandra.locator.simpleseedprovider, parameters=[{seeds=127.0.0.1}]}]; server_encryption_options=<redacted>; snapshot_before_compaction=false; ssl_storage_port=7001; start_native_transport=true; start_rpc=true; storage_port=7000; thrift_framed_transport_size_in_mb=15; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=10000; write_request_timeout_in_ms=10000] info  [main] 2014-04-08 19:20:33,740 databasedescriptor.java:197 - diskaccessmode 'auto' determined to be mmap, indexaccessmode is mmap info  [main] 2014-04-08 19:20:33,746 databasedescriptor.java:285 - global memtable on-heap threshold is enabled at 249mb info  [main] 2014-04-08 19:20:33,747 databasedescriptor.java:289 - global memtable off-heap threshold is enabled at 249mb info  [main] 2014-04-08 19:20:34,141 cassandradaemon.java:113 - jvm vendor/version: java hotspot(tm) 64-bit server vm/1.7.0_51 info  [main] 2014-04-08 19:20:34,141 cassandradaemon.java:141 - heap size: 1046937600/1046937600 info  [main] 2014-04-08 19:20:34,141 cassandradaemon.java:143 - code cache non-heap memory: init = 2555904(2496k) used = 675712(659k) committed = 2555904(2496k) max = 50331648(49152k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - par eden space heap memory: init = 214827008(209792k) used = 81668768(79754k) committed = 214827008(209792k) max = 214827008(209792k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - par survivor space heap memory: init = 26804224(26176k) used = 0(0k) committed = 26804224(26176k) max = 26804224(26176k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - cms old gen heap memory: init = 805306368(786432k) used = 0(0k) committed = 805306368(786432k) max = 805306368(786432k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - cms perm gen non-heap memory: init = 21757952(21248k) used = 16716776(16324k) committed = 21757952(21248k) max = 85983232(83968k) info  [main] 2014-04-08 19:20:34,143 cassandradaemon.java:144 - classpath: /home/mshuler/git/cassandra/build/cobertura/classes:/tmp/dtest-x3d6bu/test/node1/conf:/home/mshuler/git/cassandra/build/classes/main:/home/mshuler/git/cassandra/build/classes/thrift:/home/mshuler/git/cassandra/lib/airline-0.6.jar:/home/mshuler/git/cassandra/lib/antlr-3.2.jar:/home/mshuler/git/cassandra/lib/commons-cli-1.1.jar:/home/mshuler/git/cassandra/lib/commons-codec-1.2.jar:/home/mshuler/git/cassandra/lib/commons-lang3-3.1.jar:/home/mshuler/git/cassandra/lib/commons-math3-3.2.jar:/home/mshuler/git/cassandra/lib/compress-lzf-0.8.4.jar:/home/mshuler/git/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/home/mshuler/git/cassandra/lib/disruptor-3.0.1.jar:/home/mshuler/git/cassandra/lib/guava-16.0.jar:/home/mshuler/git/cassandra/lib/high-scale-lib-1.1.2.jar:/home/mshuler/git/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar:/home/mshuler/git/cassandra/lib/javax.inject.jar:/home/mshuler/git/cassandra/lib/jbcrypt-0.3m.jar:/home/mshuler/git/cassandra/lib/jline-1.0.jar:/home/mshuler/git/cassandra/lib/jna-4.0.0.jar:/home/mshuler/git/cassandra/lib/json-simple-1.1.jar:/home/mshuler/git/cassandra/lib/libthrift-0.9.1.jar:/home/mshuler/git/cassandra/lib/logback-classic-1.1.2.jar:/home/mshuler/git/cassandra/lib/logback-core-1.1.12.jar:/home/mshuler/git/cassandra/lib/lz4-1.2.0.jar:/home/mshuler/git/cassandra/lib/metrics-core-2.2.0.jar:/home/mshuler/git/cassandra/lib/netty-all-4.0.17.final.jar:/home/mshuler/git/cassandra/lib/reporter-config-2.1.0.jar:/home/mshuler/git/cassandra/lib/slf4j-api-1.7.2.jar:/home/mshuler/git/cassandra/lib/snakeyaml-1.11.jar:/home/mshuler/git/cassandra/lib/snappy-java-1.0.5.jar:/home/mshuler/git/cassandra/lib/stream-2.5.2.jar:/home/mshuler/git/cassandra/lib/super-csv-2.1.0.jar:/home/mshuler/git/cassandra/lib/thrift-server-0.3.3.jar:/home/mshuler/.m2/repository/net/sourceforge/cobertura/cobertura/1.9.4.1/cobertura-1.9.4.1.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar warn  [main] 2014-04-08 19:20:34,204 clibrary.java:130 - unable to lock jvm memory (enomem). this can result in part of the jvm being swapped out, especially with mmapped i/o enabled. increase rlimit_memlock or run cassandra as root. info  [main] 2014-04-08 19:20:34,227 cacheservice.java:111 - initializing key cache with capacity of 49 mbs. info  [main] 2014-04-08 19:20:34,237 cacheservice.java:133 - initializing row cache with capacity of 0 mbs info  [main] 2014-04-08 19:20:34,244 cacheservice.java:150 - initializing counter cache with capacity of 24 mbs info  [main] 2014-04-08 19:20:34,246 cacheservice.java:161 - scheduling counter cache save to every 7200 seconds (going to save all keys). info  [main] 2014-04-08 19:20:34,360 columnfamilystore.java:283 - initializing system.schema_triggers info  [main] 2014-04-08 19:20:35,608 columnfamilystore.java:283 - initializing system.compaction_history info  [main] 2014-04-08 19:20:35,618 columnfamilystore.java:283 - initializing system.batchlog info  [main] 2014-04-08 19:20:35,624 columnfamilystore.java:283 - initializing system.sstable_activity info  [main] 2014-04-08 19:20:35,630 columnfamilystore.java:283 - initializing system.peer_events info  [main] 2014-04-08 19:20:35,643 columnfamilystore.java:283 - initializing system.compactions_in_progress info  [main] 2014-04-08 19:20:35,652 columnfamilystore.java:283 - initializing system.hints info  [main] 2014-04-08 19:20:35,656 columnfamilystore.java:283 - initializing system.schema_keyspaces info  [main] 2014-04-08 19:20:35,660 columnfamilystore.java:283 - initializing system.range_xfers info  [main] 2014-04-08 19:20:35,664 columnfamilystore.java:283 - initializing system.schema_columnfamilies info  [main] 2014-04-08 19:20:35,669 columnfamilystore.java:283 - initializing system.nodeidinfo info  [main] 2014-04-08 19:20:35,677 columnfamilystore.java:283 - initializing system.paxos info  [main] 2014-04-08 19:20:35,682 columnfamilystore.java:283 - initializing system.schema_usertypes info  [main] 2014-04-08 19:20:35,686 columnfamilystore.java:283 - initializing system.schema_columns info  [main] 2014-04-08 19:20:35,691 columnfamilystore.java:283 - initializing system.indexinfo info  [main] 2014-04-08 19:20:35,695 columnfamilystore.java:283 - initializing system.peers info  [main] 2014-04-08 19:20:35,700 columnfamilystore.java:283 - initializing system.local info  [main] 2014-04-08 19:20:35,820 databasedescriptor.java:587 - couldn't detect any schema definitions in local storage. info  [main] 2014-04-08 19:20:35,821 databasedescriptor.java:592 - to create keyspaces and column families, see 'help create' in cqlsh. info  [main] 2014-04-08 19:20:35,903 columnfamilystore.java:853 - enqueuing flush of local: 1109 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:1] 2014-04-08 19:20:35,921 memtable.java:344 - writing memtable-local@1878619947(220 serialized bytes, 5 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:1] 2014-04-08 19:20:36,021 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-1-data.db (171 bytes) for commitlog position replayposition(segmentid=1397002835560, position=403) info  [main] 2014-04-08 19:20:36,038 commitlog.java:108 - no commitlog files found; skipping replay info  [main] 2014-04-08 19:20:36,535 storageservice.java:510 - cassandra version: 2.1.0-beta1-snapshot info  [main] 2014-04-08 19:20:36,535 storageservice.java:511 - thrift api version: 19.39.0 info  [main] 2014-04-08 19:20:36,559 storageservice.java:512 - cql supported versions: 2.0.0,3.1.5 (default: 3.1.5) info  [main] 2014-04-08 19:20:36,595 indexsummarymanager.java:99 - initializing index summary manager with a memory pool size of 49 mb and a resize interval of 60 minutes info  [main] 2014-04-08 19:20:36,607 storageservice.java:537 - loading persisted ring state info  [main] 2014-04-08 19:20:36,679 storageservice.java:858 - saved tokens not found. using configuration value: [-9223372036854775808] info  [main] 2014-04-08 19:20:36,701 migrationmanager.java:206 - create new keyspace: ksmetadata{name=system_traces, strategyclass=simplestrategy, strategyoptions={replication_factor=2}, cfmetadata={sessions=org.apache.cassandra.config.cfmetadata@7892cf41[cfid=c5e99f16-8677-3914-b17e-960613512345,ksname=system_traces,cfname=sessions,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.columntocollectiontype(706172616d6574657273:org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type))),comment=traced sessions,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=duration, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=parameters, type=org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type), kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=started_at, type=org.apache.cassandra.db.marshal.timestamptype, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=7 cap=7]=columndefinition{name=request, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=11 cap=11]=columndefinition{name=coordinator, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={\"keys\":\"all\", \"rows_per_partition\":\"none\"},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}], events=org.apache.cassandra.config.cfmetadata@3a437a40[cfid=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksname=system_traces,cfname=events,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.timeuuidtype,org.apache.cassandra.db.marshal.utf8type),comment=,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=source, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=thread, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=activity, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=14 cap=14]=columndefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=event_id, type=org.apache.cassandra.db.marshal.timeuuidtype, kind=clustering_column, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={\"keys\":\"all\", \"rows_per_partition\":\"none\"},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}]}, durablewrites=true, usertypes=org.apache.cassandra.config.utmetadata@240f1da2} info  [migrationstage:1] 2014-04-08 19:20:36,826 columnfamilystore.java:853 - enqueuing flush of schema_keyspaces: 990 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:2] 2014-04-08 19:20:36,834 memtable.java:344 - writing memtable-schema_keyspaces@2107075397(251 serialized bytes, 7 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:2] 2014-04-08 19:20:36,915 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-1-data.db (216 bytes) for commitlog position replayposition(segmentid=1397002835560, position=99535) info  [migrationstage:1] 2014-04-08 19:20:36,920 columnfamilystore.java:853 - enqueuing flush of schema_columnfamilies: 164066 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:1] 2014-04-08 19:20:36,921 memtable.java:344 - writing memtable-schema_columnfamilies@1732384250(30202 serialized bytes, 514 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:1] 2014-04-08 19:20:37,019 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-data.db (6720 bytes) for commitlog position replayposition(segmentid=1397002835560, position=99535) info  [migrationstage:1] 2014-04-08 19:20:37,033 columnfamilystore.java:853 - enqueuing flush of schema_columns: 288881 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:2] 2014-04-08 19:20:37,034 memtable.java:344 - writing memtable-schema_columns@985819426(46627 serialized bytes, 904 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:2] 2014-04-08 19:20:37,140 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-1-data.db (10835 bytes) for commitlog position replayposition(segmentid=1397002835560, position=99535) info  [migrationstage:1] 2014-04-08 19:20:37,351 defstables.java:388 - loading org.apache.cassandra.config.cfmetadata@40e9da6[cfid=c5e99f16-8677-3914-b17e-960613512345,ksname=system_traces,cfname=sessions,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.columntocollectiontype(706172616d6574657273:org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type))),comment=traced sessions,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=duration, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=parameters, type=org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type), kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=started_at, type=org.apache.cassandra.db.marshal.timestamptype, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=7 cap=7]=columndefinition{name=request, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=11 cap=11]=columndefinition{name=coordinator, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={\"keys\":\"all\", \"rows_per_partition\":\"none\"},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}] info  [migrationstage:1] 2014-04-08 19:20:37,357 columnfamilystore.java:283 - initializing system_traces.sessions info  [migrationstage:1] 2014-04-08 19:20:37,358 defstables.java:388 - loading org.apache.cassandra.config.cfmetadata@5b8fff5e[cfid=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksname=system_traces,cfname=events,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.timeuuidtype,org.apache.cassandra.db.marshal.utf8type),comment=,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=source, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=thread, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=activity, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=14 cap=14]=columndefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=event_id, type=org.apache.cassandra.db.marshal.timeuuidtype, kind=clustering_column, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={\"keys\":\"all\", \"rows_per_partition\":\"none\"},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}] info  [migrationstage:1] 2014-04-08 19:20:37,363 columnfamilystore.java:283 - initializing system_traces.events error [migrationstage:1] 2014-04-08 19:20:37,426 cassandradaemon.java:166 - exception in thread thread[migrationstage:1,5,main] java.lang.assertionerror: null         at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) ~[main/:na]         at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) ~[main/:na]         at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_51]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_51]         at java.lang.thread.run(thread.java:744) [na:1.7.0_51] error [main] 2014-04-08 19:20:37,427 cassandradaemon.java:471 - exception encountered during startup java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) ~[main/:na]         at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [main/:na] caused by: java.util.concurrent.executionexception: java.lang.assertionerror         at java.util.concurrent.futuretask.report(futuretask.java:122) ~[na:1.7.0_51]         at java.util.concurrent.futuretask.get(futuretask.java:188) ~[na:1.7.0_51]         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) ~[main/:na]         ... 8 common frames omitted caused by: java.lang.assertionerror: null         at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) ~[main/:na]         at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) ~[main/:na]         at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_51]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) ~[na:1.7.0_51]         at java.lang.thread.run(thread.java:744) ~[na:1.7.0_51] error [storageserviceshutdownhook] 2014-04-08 19:20:37,437 cassandradaemon.java:166 - exception in thread thread[storageserviceshutdownhook,5,main] java.lang.nullpointerexception: null         at org.apache.cassandra.gms.gossiper.stop(gossiper.java:1270) ~[main/:na]         at org.apache.cassandra.service.storageservice$1.runmaythrow(storageservice.java:581) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at java.lang.thread.run(thread.java:744) ~[na:1.7.0_51]<stacktrace> $ export max_heap_size='1g'; export heap_newsize='256m'; print_debug=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py nose.config: info: ignoring files matching ['^//.', '^_', '^setup//.py$'] incompressible_data_in_compressed_table_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-gpkstl [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) error remove_index_file_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-vx3bsj [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-ap0vwz testing sstableloader with pre_compression=deflate and post_compression=deflate [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-nsy44b testing sstableloader with pre_compression=deflate and post_compression=none [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-77_xih testing sstableloader with pre_compression=deflate and post_compression=snappy [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-x3esb0 testing sstableloader with pre_compression=none and post_compression=deflate [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-hgjni9 testing sstableloader with pre_compression=none and post_compression=none [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-qkuef5 testing sstableloader with pre_compression=none and post_compression=snappy [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-mgv9zg testing sstableloader with pre_compression=snappy and post_compression=deflate [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-e0mevq testing sstableloader with pre_compression=snappy and post_compression=none [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-x3d6bu testing sstableloader with pre_compression=snappy and post_compression=snappy [node1 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node1 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node1 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node1 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node1 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node1 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node1 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node1 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node1 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node1 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node1 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node1 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node1 error]   ... 8 more [node1 error] caused by: java.lang.assertionerror [node1 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node1 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node1 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node1 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node1 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node1 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node1 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node1 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node1 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node1 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node1 error]   at java.lang.thread.run(thread.java:744) [node2 error] java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) [node2 error]   at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) [node2 error]   at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) [node2 error]   at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) [node2 error]   at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) [node2 error]   at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [node2 error]   at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [node2 error]   at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [node2 error] caused by: java.util.concurrent.executionexception: java.lang.assertionerror [node2 error]   at java.util.concurrent.futuretask.report(futuretask.java:122) [node2 error]   at java.util.concurrent.futuretask.get(futuretask.java:188) [node2 error]   at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) [node2 error]   ... 8 more [node2 error] caused by: java.lang.assertionerror [node2 error]   at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) [node2 error]   at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) [node2 error]   at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) [node2 error]   at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) [node2 error]   at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) [node2 error]   at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) [node2 error]   at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [node2 error]   at java.util.concurrent.futuretask.run(futuretask.java:262) [node2 error]   at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [node2 error]   at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [node2 error]   at java.lang.thread.run(thread.java:744) error ====================================================================== error: incompressible_data_in_compressed_table_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 25, in incompressible_data_in_compressed_table_test     cluster.populate(1).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: remove_index_file_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 55, in remove_index_file_test     cluster.populate(1).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 108, in sstableloader_compression_deflate_to_deflate_test     self.load_sstable_with_configuration('deflate', 'deflate')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 102, in sstableloader_compression_deflate_to_none_test     self.load_sstable_with_configuration('deflate', none)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 105, in sstableloader_compression_deflate_to_snappy_test     self.load_sstable_with_configuration('deflate', 'snappy')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 90, in sstableloader_compression_none_to_deflate_test     self.load_sstable_with_configuration(none, 'deflate')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 84, in sstableloader_compression_none_to_none_test     self.load_sstable_with_configuration(none, none)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 87, in sstableloader_compression_none_to_snappy_test     self.load_sstable_with_configuration(none, 'snappy')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 99, in sstableloader_compression_snappy_to_deflate_test     self.load_sstable_with_configuration('snappy', 'deflate')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 93, in sstableloader_compression_snappy_to_none_test     self.load_sstable_with_configuration('snappy', none)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ====================================================================== error: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 96, in sstableloader_compression_snappy_to_snappy_test     self.load_sstable_with_configuration('snappy', 'snappy')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 127, in load_sstable_with_configuration     cluster.populate(2).start()   file '/home/mshuler/git/ccm/ccmlib/cluster.py', line 230, in start     raise nodeerror('error starting {0}.'.format(node.name), p) nodeerror: error starting node1. ---------------------------------------------------------------------- ran 11 tests in 56.599s failed (errors=11) info  [main] 2014-04-08 19:20:33,339 cassandradaemon.java:102 - hostname: hana.12.am info  [main] 2014-04-08 19:20:33,396 yamlconfigurationloader.java:80 - loading settings from file:/tmp/dtest-x3d6bu/test/node1/conf/cassandra.yaml info  [main] 2014-04-08 19:20:33,514 yamlconfigurationloader.java:123 - node configuration:[authenticator=allowallauthenticator; authorizer=allowallauthorizer; auto_bootstrap=false; auto_snapshot=true; batchlog_replay_throttle_in_kb=1024; cas_contention_timeout_in_ms=1000; client_encryption_options=<redacted>; cluster_name=test; column_index_size_in_kb=64; commitlog_directory=/tmp/dtest-x3d6bu/test/node1/commitlogs; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_period_in_ms=10000; compaction_preheat_key_cache=true; compaction_throughput_mb_per_sec=16; concurrent_counter_writes=32; concurrent_reads=32; concurrent_writes=32; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=[/tmp/dtest-x3d6bu/test/node1/data]; disk_failure_policy=stop; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; endpoint_snitch=simplesnitch; flush_directory=/tmp/dtest-x3d6bu/test/node1/flush; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; in_memory_compaction_limit_in_mb=64; incremental_backups=false; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=-9223372036854775808; inter_dc_tcp_nodelay=false; internode_compression=all; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=127.0.0.1; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=0.4; native_transport_port=9042; partitioner=org.apache.cassandra.dht.murmur3partitioner; permissions_validity_in_ms=2000; phi_convict_threshold=5; preheat_kernel_page_cache=false; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=10000; request_scheduler=org.apache.cassandra.scheduler.noscheduler; request_timeout_in_ms=10000; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=127.0.0.1; rpc_keepalive=true; rpc_port=9160; rpc_server_type=sync; saved_caches_directory=/tmp/dtest-x3d6bu/test/node1/saved_caches; seed_provider=[{class_name=org.apache.cassandra.locator.simpleseedprovider, parameters=[{seeds=127.0.0.1}]}]; server_encryption_options=<redacted>; snapshot_before_compaction=false; ssl_storage_port=7001; start_native_transport=true; start_rpc=true; storage_port=7000; thrift_framed_transport_size_in_mb=15; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=10000; write_request_timeout_in_ms=10000] info  [main] 2014-04-08 19:20:33,740 databasedescriptor.java:197 - diskaccessmode 'auto' determined to be mmap, indexaccessmode is mmap info  [main] 2014-04-08 19:20:33,746 databasedescriptor.java:285 - global memtable on-heap threshold is enabled at 249mb info  [main] 2014-04-08 19:20:33,747 databasedescriptor.java:289 - global memtable off-heap threshold is enabled at 249mb info  [main] 2014-04-08 19:20:34,141 cassandradaemon.java:113 - jvm vendor/version: java hotspot(tm) 64-bit server vm/1.7.0_51 info  [main] 2014-04-08 19:20:34,141 cassandradaemon.java:141 - heap size: 1046937600/1046937600 info  [main] 2014-04-08 19:20:34,141 cassandradaemon.java:143 - code cache non-heap memory: init = 2555904(2496k) used = 675712(659k) committed = 2555904(2496k) max = 50331648(49152k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - par eden space heap memory: init = 214827008(209792k) used = 81668768(79754k) committed = 214827008(209792k) max = 214827008(209792k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - par survivor space heap memory: init = 26804224(26176k) used = 0(0k) committed = 26804224(26176k) max = 26804224(26176k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - cms old gen heap memory: init = 805306368(786432k) used = 0(0k) committed = 805306368(786432k) max = 805306368(786432k) info  [main] 2014-04-08 19:20:34,142 cassandradaemon.java:143 - cms perm gen non-heap memory: init = 21757952(21248k) used = 16716776(16324k) committed = 21757952(21248k) max = 85983232(83968k) info  [main] 2014-04-08 19:20:34,143 cassandradaemon.java:144 - classpath: /home/mshuler/git/cassandra/build/cobertura/classes:/tmp/dtest-x3d6bu/test/node1/conf:/home/mshuler/git/cassandra/build/classes/main:/home/mshuler/git/cassandra/build/classes/thrift:/home/mshuler/git/cassandra/lib/airline-0.6.jar:/home/mshuler/git/cassandra/lib/antlr-3.2.jar:/home/mshuler/git/cassandra/lib/commons-cli-1.1.jar:/home/mshuler/git/cassandra/lib/commons-codec-1.2.jar:/home/mshuler/git/cassandra/lib/commons-lang3-3.1.jar:/home/mshuler/git/cassandra/lib/commons-math3-3.2.jar:/home/mshuler/git/cassandra/lib/compress-lzf-0.8.4.jar:/home/mshuler/git/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/home/mshuler/git/cassandra/lib/disruptor-3.0.1.jar:/home/mshuler/git/cassandra/lib/guava-16.0.jar:/home/mshuler/git/cassandra/lib/high-scale-lib-1.1.2.jar:/home/mshuler/git/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar:/home/mshuler/git/cassandra/lib/javax.inject.jar:/home/mshuler/git/cassandra/lib/jbcrypt-0.3m.jar:/home/mshuler/git/cassandra/lib/jline-1.0.jar:/home/mshuler/git/cassandra/lib/jna-4.0.0.jar:/home/mshuler/git/cassandra/lib/json-simple-1.1.jar:/home/mshuler/git/cassandra/lib/libthrift-0.9.1.jar:/home/mshuler/git/cassandra/lib/logback-classic-1.1.2.jar:/home/mshuler/git/cassandra/lib/logback-core-1.1.12.jar:/home/mshuler/git/cassandra/lib/lz4-1.2.0.jar:/home/mshuler/git/cassandra/lib/metrics-core-2.2.0.jar:/home/mshuler/git/cassandra/lib/netty-all-4.0.17.final.jar:/home/mshuler/git/cassandra/lib/reporter-config-2.1.0.jar:/home/mshuler/git/cassandra/lib/slf4j-api-1.7.2.jar:/home/mshuler/git/cassandra/lib/snakeyaml-1.11.jar:/home/mshuler/git/cassandra/lib/snappy-java-1.0.5.jar:/home/mshuler/git/cassandra/lib/stream-2.5.2.jar:/home/mshuler/git/cassandra/lib/super-csv-2.1.0.jar:/home/mshuler/git/cassandra/lib/thrift-server-0.3.3.jar:/home/mshuler/.m2/repository/net/sourceforge/cobertura/cobertura/1.9.4.1/cobertura-1.9.4.1.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar warn  [main] 2014-04-08 19:20:34,204 clibrary.java:130 - unable to lock jvm memory (enomem). this can result in part of the jvm being swapped out, especially with mmapped i/o enabled. increase rlimit_memlock or run cassandra as root. info  [main] 2014-04-08 19:20:34,227 cacheservice.java:111 - initializing key cache with capacity of 49 mbs. info  [main] 2014-04-08 19:20:34,237 cacheservice.java:133 - initializing row cache with capacity of 0 mbs info  [main] 2014-04-08 19:20:34,244 cacheservice.java:150 - initializing counter cache with capacity of 24 mbs info  [main] 2014-04-08 19:20:34,246 cacheservice.java:161 - scheduling counter cache save to every 7200 seconds (going to save all keys). info  [main] 2014-04-08 19:20:34,360 columnfamilystore.java:283 - initializing system.schema_triggers info  [main] 2014-04-08 19:20:35,608 columnfamilystore.java:283 - initializing system.compaction_history info  [main] 2014-04-08 19:20:35,618 columnfamilystore.java:283 - initializing system.batchlog info  [main] 2014-04-08 19:20:35,624 columnfamilystore.java:283 - initializing system.sstable_activity info  [main] 2014-04-08 19:20:35,630 columnfamilystore.java:283 - initializing system.peer_events info  [main] 2014-04-08 19:20:35,643 columnfamilystore.java:283 - initializing system.compactions_in_progress info  [main] 2014-04-08 19:20:35,652 columnfamilystore.java:283 - initializing system.hints info  [main] 2014-04-08 19:20:35,656 columnfamilystore.java:283 - initializing system.schema_keyspaces info  [main] 2014-04-08 19:20:35,660 columnfamilystore.java:283 - initializing system.range_xfers info  [main] 2014-04-08 19:20:35,664 columnfamilystore.java:283 - initializing system.schema_columnfamilies info  [main] 2014-04-08 19:20:35,669 columnfamilystore.java:283 - initializing system.nodeidinfo info  [main] 2014-04-08 19:20:35,677 columnfamilystore.java:283 - initializing system.paxos info  [main] 2014-04-08 19:20:35,682 columnfamilystore.java:283 - initializing system.schema_usertypes info  [main] 2014-04-08 19:20:35,686 columnfamilystore.java:283 - initializing system.schema_columns info  [main] 2014-04-08 19:20:35,691 columnfamilystore.java:283 - initializing system.indexinfo info  [main] 2014-04-08 19:20:35,695 columnfamilystore.java:283 - initializing system.peers info  [main] 2014-04-08 19:20:35,700 columnfamilystore.java:283 - initializing system.local info  [main] 2014-04-08 19:20:35,820 databasedescriptor.java:587 - couldn't detect any schema definitions in local storage. info  [main] 2014-04-08 19:20:35,821 databasedescriptor.java:592 - to create keyspaces and column families, see 'help create' in cqlsh. info  [main] 2014-04-08 19:20:35,903 columnfamilystore.java:853 - enqueuing flush of local: 1109 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:1] 2014-04-08 19:20:35,921 memtable.java:344 - writing memtable-local@1878619947(220 serialized bytes, 5 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:1] 2014-04-08 19:20:36,021 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-1-data.db (171 bytes) for commitlog position replayposition(segmentid=1397002835560, position=403) info  [main] 2014-04-08 19:20:36,038 commitlog.java:108 - no commitlog files found; skipping replay info  [main] 2014-04-08 19:20:36,535 storageservice.java:510 - cassandra version: 2.1.0-beta1-snapshot info  [main] 2014-04-08 19:20:36,535 storageservice.java:511 - thrift api version: 19.39.0 info  [main] 2014-04-08 19:20:36,559 storageservice.java:512 - cql supported versions: 2.0.0,3.1.5 (default: 3.1.5) info  [main] 2014-04-08 19:20:36,595 indexsummarymanager.java:99 - initializing index summary manager with a memory pool size of 49 mb and a resize interval of 60 minutes info  [main] 2014-04-08 19:20:36,607 storageservice.java:537 - loading persisted ring state info  [main] 2014-04-08 19:20:36,679 storageservice.java:858 - saved tokens not found. using configuration value: [-9223372036854775808] info  [main] 2014-04-08 19:20:36,701 migrationmanager.java:206 - create new keyspace: ksmetadata{name=system_traces, strategyclass=simplestrategy, strategyoptions={replication_factor=2}, cfmetadata={sessions=org.apache.cassandra.config.cfmetadata@7892cf41[cfid=c5e99f16-8677-3914-b17e-960613512345,ksname=system_traces,cfname=sessions,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.columntocollectiontype(706172616d6574657273:org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type))),comment=traced sessions,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=duration, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=parameters, type=org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type), kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=started_at, type=org.apache.cassandra.db.marshal.timestamptype, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=7 cap=7]=columndefinition{name=request, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=11 cap=11]=columndefinition{name=coordinator, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={'keys':'all', 'rows_per_partition':'none'},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}], events=org.apache.cassandra.config.cfmetadata@3a437a40[cfid=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksname=system_traces,cfname=events,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.timeuuidtype,org.apache.cassandra.db.marshal.utf8type),comment=,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=source, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=thread, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=activity, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=14 cap=14]=columndefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=event_id, type=org.apache.cassandra.db.marshal.timeuuidtype, kind=clustering_column, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={'keys':'all', 'rows_per_partition':'none'},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}]}, durablewrites=true, usertypes=org.apache.cassandra.config.utmetadata@240f1da2} info  [migrationstage:1] 2014-04-08 19:20:36,826 columnfamilystore.java:853 - enqueuing flush of schema_keyspaces: 990 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:2] 2014-04-08 19:20:36,834 memtable.java:344 - writing memtable-schema_keyspaces@2107075397(251 serialized bytes, 7 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:2] 2014-04-08 19:20:36,915 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-1-data.db (216 bytes) for commitlog position replayposition(segmentid=1397002835560, position=99535) info  [migrationstage:1] 2014-04-08 19:20:36,920 columnfamilystore.java:853 - enqueuing flush of schema_columnfamilies: 164066 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:1] 2014-04-08 19:20:36,921 memtable.java:344 - writing memtable-schema_columnfamilies@1732384250(30202 serialized bytes, 514 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:1] 2014-04-08 19:20:37,019 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-data.db (6720 bytes) for commitlog position replayposition(segmentid=1397002835560, position=99535) info  [migrationstage:1] 2014-04-08 19:20:37,033 columnfamilystore.java:853 - enqueuing flush of schema_columns: 288881 (0%) on-heap, 0 (0%) off-heap info  [memtableflushwriter:2] 2014-04-08 19:20:37,034 memtable.java:344 - writing memtable-schema_columns@985819426(46627 serialized bytes, 904 ops, 0%/0% of on/off-heap limit) info  [memtableflushwriter:2] 2014-04-08 19:20:37,140 memtable.java:378 - completed flushing /tmp/dtest-x3d6bu/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-1-data.db (10835 bytes) for commitlog position replayposition(segmentid=1397002835560, position=99535) info  [migrationstage:1] 2014-04-08 19:20:37,351 defstables.java:388 - loading org.apache.cassandra.config.cfmetadata@40e9da6[cfid=c5e99f16-8677-3914-b17e-960613512345,ksname=system_traces,cfname=sessions,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.columntocollectiontype(706172616d6574657273:org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type))),comment=traced sessions,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=duration, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=parameters, type=org.apache.cassandra.db.marshal.maptype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type), kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=started_at, type=org.apache.cassandra.db.marshal.timestamptype, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=7 cap=7]=columndefinition{name=request, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=0, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=11 cap=11]=columndefinition{name=coordinator, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={'keys':'all', 'rows_per_partition':'none'},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}] info  [migrationstage:1] 2014-04-08 19:20:37,357 columnfamilystore.java:283 - initializing system_traces.sessions info  [migrationstage:1] 2014-04-08 19:20:37,358 defstables.java:388 - loading org.apache.cassandra.config.cfmetadata@5b8fff5e[cfid=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksname=system_traces,cfname=events,cftype=standard,comparator=org.apache.cassandra.db.marshal.compositetype(org.apache.cassandra.db.marshal.timeuuidtype,org.apache.cassandra.db.marshal.utf8type),comment=,readrepairchance=0.0,dclocalreadrepairchance=0.0,gcgraceseconds=0,defaultvalidator=org.apache.cassandra.db.marshal.bytestype,keyvalidator=org.apache.cassandra.db.marshal.uuidtype,mincompactionthreshold=4,maxcompactionthreshold=32,columnmetadata={java.nio.heapbytebuffer[pos=0 lim=10 cap=10]=columndefinition{name=session_id, type=org.apache.cassandra.db.marshal.uuidtype, kind=partition_key, componentindex=null, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=source, type=org.apache.cassandra.db.marshal.inetaddresstype, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=6 cap=6]=columndefinition{name=thread, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=activity, type=org.apache.cassandra.db.marshal.utf8type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=14 cap=14]=columndefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.int32type, kind=regular, componentindex=1, indexname=null, indextype=null}, java.nio.heapbytebuffer[pos=0 lim=8 cap=8]=columndefinition{name=event_id, type=org.apache.cassandra.db.marshal.timeuuidtype, kind=clustering_column, componentindex=0, indexname=null, indextype=null}},compactionstrategyclass=class org.apache.cassandra.db.compaction.sizetieredcompactionstrategy,compactionstrategyoptions={},compressionparameters={sstable_compression=org.apache.cassandra.io.compress.lz4compressor},bloomfilterfpchance=0.01,memtableflushperiod=3600000,caching={'keys':'all', 'rows_per_partition':'none'},defaulttimetolive=0,minindexinterval=128,maxindexinterval=2048,speculativeretry=99.0percentile,populateiocacheonflush=false,droppedcolumns={},triggers={}] info  [migrationstage:1] 2014-04-08 19:20:37,363 columnfamilystore.java:283 - initializing system_traces.events error [migrationstage:1] 2014-04-08 19:20:37,426 cassandradaemon.java:166 - exception in thread thread[migrationstage:1,5,main] java.lang.assertionerror: null         at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) ~[main/:na]         at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) ~[main/:na]         at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_51]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_51]         at java.lang.thread.run(thread.java:744) [na:1.7.0_51] error [main] 2014-04-08 19:20:37,427 cassandradaemon.java:471 - exception encountered during startup java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:411) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:298) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.announcenewkeyspace(migrationmanager.java:207) ~[main/:na]         at org.apache.cassandra.service.storageservice.jointokenring(storageservice.java:915) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:625) ~[main/:na]         at org.apache.cassandra.service.storageservice.initserver(storageservice.java:505) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:335) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:454) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:543) [main/:na] caused by: java.util.concurrent.executionexception: java.lang.assertionerror         at java.util.concurrent.futuretask.report(futuretask.java:122) ~[na:1.7.0_51]         at java.util.concurrent.futuretask.get(futuretask.java:188) ~[na:1.7.0_51]         at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:407) ~[main/:na]         ... 8 common frames omitted caused by: java.lang.assertionerror: null         at org.apache.cassandra.gms.gossiper.addlocalapplicationstate(gossiper.java:1256) ~[main/:na]         at org.apache.cassandra.service.migrationmanager.passiveannounce(migrationmanager.java:340) ~[main/:na]         at org.apache.cassandra.config.schema.updateversionandannounce(schema.java:380) ~[main/:na]         at org.apache.cassandra.db.defstables.mergeschema(defstables.java:188) ~[main/:na]         at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:316) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_51]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_51]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) ~[na:1.7.0_51]         at java.lang.thread.run(thread.java:744) ~[na:1.7.0_51] error [storageserviceshutdownhook] 2014-04-08 19:20:37,437 cassandradaemon.java:166 - exception in thread thread[storageserviceshutdownhook,5,main] java.lang.nullpointerexception: null         at org.apache.cassandra.gms.gossiper.stop(gossiper.java:1270) ~[main/:na]         at org.apache.cassandra.service.storageservice$1.runmaythrow(storageservice.java:581) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at java.lang.thread.run(thread.java:744) ~[na:1.7.0_51] <code> $ export max_heap_size='1g'; export heap_newsize='256m'; print_debug=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py nose.config: info: ignoring files matching ['^//.', '^_', '^setup//.py$'] incompressible_data_in_compressed_table_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-sjdykb ok remove_index_file_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-grtcgb created keyspaces. sleeping 1s for propagation. total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time 10000,1000,1000,11.5,80.3,280.6,5 end ok sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-xneocv testing sstableloader with pre_compression=deflate and post_compression=deflate creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-_ij1td testing sstableloader with pre_compression=deflate and post_compression=none creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-fzaci9 testing sstableloader with pre_compression=deflate and post_compression=snappy creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-asoepn testing sstableloader with pre_compression=none and post_compression=deflate creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-cfiwh8 testing sstableloader with pre_compression=none and post_compression=none creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-ttuqqg testing sstableloader with pre_compression=none and post_compression=snappy creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-la0rxi testing sstableloader with pre_compression=snappy and post_compression=deflate creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-jj5iub testing sstableloader with pre_compression=snappy and post_compression=none creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ... cluster ccm directory: /tmp/dtest-jtmbkg testing sstableloader with pre_compression=snappy and post_compression=snappy creating keyspace and inserting making a copy of the sstables wiping out the data and restarting cluster re-creating the keyspace and column families. calling sstableloader no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]no sstables to stream progress: [total: 100 - 0mb/s (avg: 0mb/s)]reading data back fail ====================================================================== fail: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 108, in sstableloader_compression_deflate_to_deflate_test     self.load_sstable_with_configuration('deflate', 'deflate')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 102, in sstableloader_compression_deflate_to_none_test     self.load_sstable_with_configuration('deflate', none)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 105, in sstableloader_compression_deflate_to_snappy_test     self.load_sstable_with_configuration('deflate', 'snappy')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 90, in sstableloader_compression_none_to_deflate_test     self.load_sstable_with_configuration(none, 'deflate')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 84, in sstableloader_compression_none_to_none_test     self.load_sstable_with_configuration(none, none)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 87, in sstableloader_compression_none_to_snappy_test     self.load_sstable_with_configuration(none, 'snappy')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 99, in sstableloader_compression_snappy_to_deflate_test     self.load_sstable_with_configuration('snappy', 'deflate')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 93, in sstableloader_compression_snappy_to_none_test     self.load_sstable_with_configuration('snappy', none)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ====================================================================== fail: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.testsstablegenerationandloading) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 96, in sstableloader_compression_snappy_to_snappy_test     self.load_sstable_with_configuration('snappy', 'snappy')   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 197, in load_sstable_with_configuration     read_and_validate_data(cursor)   file '/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py', line 190, in read_and_validate_data     self.assertequals([str(i), 'col', str(i)], cursor.fetchone()) assertionerror: ['0', 'col', '0'] != none ---------------------------------------------------------------------- ran 11 tests in 336.704s failed (failures=9) <text> this looks like something fundamentally wrong with the sstable_generation_loading_test.py dtest - 1.2 and 2.0 look like: 2.1 looks like: 2.1 last ccm node1 log (node2 is the same):",
        "label": 321
    },
    {
        "text": "deprecated repair tests fail on windows <description> after fixing the jolokia agent problem, we can now see that these tests are genuinely failing. every test follows the same basic pattern:     def force_repair_async_1_test(self, ):         \"\"\"         test forcerepairasync(string keyspace, boolean issequential,                               collection<string> datacenters,                               collection<string> hosts,                               boolean primaryrange, boolean fullrepair, string... columnfamilies)         \"\"\"         opt = self._deprecated_repair_jmx(\"forcerepairasync(java.lang.string,boolean,java.util.collection,java.util.collection,boolean,boolean,[ljava.lang.string;)\",                                           ['ks', true, [], [], false, false, [\"cf\"]])         self.assertequal(opt[\"parallelism\"], \"sequential\", opt)         self.assertequal(opt[\"primary_range\"], \"false\", opt)         self.assertequal(opt[\"incremental\"], \"true\", opt)         self.assertequal(opt[\"job_threads\"], \"1\", opt)         self.assertequal(opt[\"data_centers\"], \"[]\", opt)         self.assertequal(opt[\"hosts\"], \"[]\", opt)         self.assertequal(opt[\"column_families\"], \"[cf]\", opt) in each test, the response for opt[\"parallelism\"] is incorrect. yuki morishita wrote these tests, and may be able to shed light on what's going on.<stacktrace> <code>     def force_repair_async_1_test(self, ):         '''         test forcerepairasync(string keyspace, boolean issequential,                               collection<string> datacenters,                               collection<string> hosts,                               boolean primaryrange, boolean fullrepair, string... columnfamilies)         '''         opt = self._deprecated_repair_jmx('forcerepairasync(java.lang.string,boolean,java.util.collection,java.util.collection,boolean,boolean,[ljava.lang.string;)',                                           ['ks', true, [], [], false, false, ['cf']])         self.assertequal(opt['parallelism'], 'sequential', opt)         self.assertequal(opt['primary_range'], 'false', opt)         self.assertequal(opt['incremental'], 'true', opt)         self.assertequal(opt['job_threads'], '1', opt)         self.assertequal(opt['data_centers'], '[]', opt)         self.assertequal(opt['hosts'], '[]', opt)         self.assertequal(opt['column_families'], '[cf]', opt) <text> after fixing the jolokia agent problem, we can now see that these tests are genuinely failing. every test follows the same basic pattern: in each test, the response for opt['parallelism'] is incorrect. yuki morishita wrote these tests, and may be able to shed light on what's going on.",
        "label": 428
    },
    {
        "text": "fatal exception in thread thread migrationstage main   leveledcompaction  <description> error 19:29:39,712 fatal exception in thread thread[migrationstage:1,5,main]  java.lang.assertionerror  at org.apache.cassandra.db.compaction.leveledmanifest.promote(leveledmanifest.java:156)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy.handlenotification(leveledcompactionstrategy.java:141)  at org.apache.cassandra.db.datatracker.notifysstableschanged(datatracker.java:488)  at org.apache.cassandra.db.datatracker.removeallsstables(datatracker.java:257)  at org.apache.cassandra.db.columnfamilystore.invalidate(columnfamilystore.java:267)  at org.apache.cassandra.db.table.unloadcf(table.java:361)  at org.apache.cassandra.db.table.dropcf(table.java:343)  at org.apache.cassandra.db.migration.dropcolumnfamily.applymodels(dropcolumnfamily.java:87)  at org.apache.cassandra.db.migration.migration.apply(migration.java:156)  at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:73)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<stacktrace> error 19:29:39,712 fatal exception in thread thread[migrationstage:1,5,main]  java.lang.assertionerror  at org.apache.cassandra.db.compaction.leveledmanifest.promote(leveledmanifest.java:156)  at org.apache.cassandra.db.compaction.leveledcompactionstrategy.handlenotification(leveledcompactionstrategy.java:141)  at org.apache.cassandra.db.datatracker.notifysstableschanged(datatracker.java:488)  at org.apache.cassandra.db.datatracker.removeallsstables(datatracker.java:257)  at org.apache.cassandra.db.columnfamilystore.invalidate(columnfamilystore.java:267)  at org.apache.cassandra.db.table.unloadcf(table.java:361)  at org.apache.cassandra.db.table.dropcf(table.java:343)  at org.apache.cassandra.db.migration.dropcolumnfamily.applymodels(dropcolumnfamily.java:87)  at org.apache.cassandra.db.migration.migration.apply(migration.java:156)  at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:73)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<code> <text> ",
        "label": 274
    },
    {
        "text": "news txt could use an  upgrading  section for <description> past releases contained one of these. the upgrade path from 0.6 to 0.7 is a little muddy (run the converter, stand up your cluster, import your schema, etc.) so i think this would be useful.<stacktrace> <code> <text> past releases contained one of these. the upgrade path from 0.6 to 0.7 is a little muddy (run the converter, stand up your cluster, import your schema, etc.) so i think this would be useful.",
        "label": 186
    },
    {
        "text": "cassandra stress write hangs with default options <description> cassandra stress sits there for incredibly long time after connecting to jmx. to reproduce ./tools/bin/cassandra-stress write if you give it a -n its not as bad which is why dtests etc dont seem to be impacted. does not occur in 3.0 branch but does in 3.11 and trunk<stacktrace> <code> ./tools/bin/cassandra-stress write <text> cassandra stress sits there for incredibly long time after connecting to jmx. to reproduce if you give it a -n its not as bad which is why dtests etc dont seem to be impacted. does not occur in 3.0 branch but does in 3.11 and trunk",
        "label": 234
    },
    {
        "text": "dtest failure in disk balance test testdiskbalance disk balance stress test <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1418/testreport/disk_balance_test/testdiskbalance/disk_balance_stress_test error message 'float' object has no attribute '2f' -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-lxr8vr dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} --------------------- >> end captured logging << --------------------- stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/disk_balance_test.py\", line 31, in disk_balance_stress_test     self.assert_balanced(node)   file \"/home/automaton/cassandra-dtest/disk_balance_test.py\", line 120, in assert_balanced     assert_almost_equal(*sums, error=0.1, error_message=node.name)   file \"/home/automaton/cassandra-dtest/tools/assertions.py\", line 187, in assert_almost_equal     assert vmin > vmax * (1.0 - error) or vmin == vmax, \"values not within {.2f}% of the max: {} ({})\".format(error * 100, args, error_message) \"'float' object has no attribute '2f'\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: /tmp/dtest-lxr8vr\\ndtest: debug: done setting configuration options:\\n{   'initial_token': none,\\n    'num_tokens': '32',\\n    'phi_convict_threshold': 5,\\n    'range_request_timeout_in_ms': 10000,\\n    'read_request_timeout_in_ms': 10000,\\n    'request_timeout_in_ms': 10000,\\n    'truncate_request_timeout_in_ms': 10000,\\n    'write_request_timeout_in_ms': 10000}\\n--------------------- >> end captured logging << ---------------------\"<stacktrace> <code> error message 'float' object has no attribute '2f' -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-lxr8vr dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} --------------------- >> end captured logging << --------------------- stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/disk_balance_test.py', line 31, in disk_balance_stress_test     self.assert_balanced(node)   file '/home/automaton/cassandra-dtest/disk_balance_test.py', line 120, in assert_balanced     assert_almost_equal(*sums, error=0.1, error_message=node.name)   file '/home/automaton/cassandra-dtest/tools/assertions.py', line 187, in assert_almost_equal     assert vmin > vmax * (1.0 - error) or vmin == vmax, 'values not within {.2f}% of the max: {} ({})'.format(error * 100, args, error_message) ''float' object has no attribute '2f'/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: /tmp/dtest-lxr8vr/ndtest: debug: done setting configuration options:/n{   'initial_token': none,/n    'num_tokens': '32',/n    'phi_convict_threshold': 5,/n    'range_request_timeout_in_ms': 10000,/n    'read_request_timeout_in_ms': 10000,/n    'request_timeout_in_ms': 10000,/n    'truncate_request_timeout_in_ms': 10000,/n    'write_request_timeout_in_ms': 10000}/n--------------------- >> end captured logging << ---------------------' http://cassci.datastax.com/job/trunk_dtest/1418/testreport/disk_balance_test/testdiskbalance/disk_balance_stress_test<text> example failure: ",
        "label": 321
    },
    {
        "text": "cqlsh tests py testcqlsh test sub second precision is failing <description> $ print_debug=true nosetests -vs cqlsh_tests/cqlsh_tests.py:testcqlsh.test_sub_second_precision test_sub_second_precision (cqlsh_tests.cqlsh_tests.testcqlsh) ... cluster ccm directory: /tmp/dtest-h0tka6 custom init_config not found. setting defaults. done setting configuration options: {   'initial_token': none,     'num_tokens': '256',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} (ee)  (ee)    id | subid                           | value ----+---------------------------------+-------   1 | 1943-06-19 11:21:01.000000+0000 |   abc (1 rows) fail removing ccm cluster test at: /tmp/dtest-h0tka6 clearing ssl stores from [/tmp/dtest-h0tka6] directory ====================================================================== fail: test_sub_second_precision (cqlsh_tests.cqlsh_tests.testcqlsh) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/tools.py\", line 253, in wrapped     f(obj)   file \"/home/mshuler/git/cassandra-dtest/cqlsh_tests/cqlsh_tests.py\", line 176, in test_sub_second_precision     self.assertin(\"1943-06-19 11:21:01.123000+0000\", output) assertionerror: '1943-06-19 11:21:01.123000+0000' not found in '\\n id | subid                           | value\\n----+---------------------------------+-------\\n  1 | 1943-06-19 11:21:01.000000+0000 |   abc\\n\\n(1 rows)\\n' -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-h0tka6 dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '256',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} dtest: debug:   id | subid                           | value ----+---------------------------------+-------   1 | 1943-06-19 11:21:01.000000+0000 |   abc (1 rows) --------------------- >> end captured logging << --------------------- ---------------------------------------------------------------------- ran 1 test in 13.016s failed (failures=1) http://cassci.datastax.com/job/trunk_dtest/1006/testreport/cqlsh_tests.cqlsh_tests/testcqlsh/test_sub_second_precision/ job changes:  commit 128d144c0d22238a9045cc697daf880452be974b by josh mckenzie  cqlsh: add local timezone support to cqlsh  patch by stefan podkowinski; reviewed by paulo motta for cassandra-10397<stacktrace> <code> $ print_debug=true nosetests -vs cqlsh_tests/cqlsh_tests.py:testcqlsh.test_sub_second_precision test_sub_second_precision (cqlsh_tests.cqlsh_tests.testcqlsh) ... cluster ccm directory: /tmp/dtest-h0tka6 custom init_config not found. setting defaults. done setting configuration options: {   'initial_token': none,     'num_tokens': '256',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} (ee)  (ee)    id | subid                           | value ----+---------------------------------+-------   1 | 1943-06-19 11:21:01.000000+0000 |   abc (1 rows) fail removing ccm cluster test at: /tmp/dtest-h0tka6 clearing ssl stores from [/tmp/dtest-h0tka6] directory ====================================================================== fail: test_sub_second_precision (cqlsh_tests.cqlsh_tests.testcqlsh) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/tools.py', line 253, in wrapped     f(obj)   file '/home/mshuler/git/cassandra-dtest/cqlsh_tests/cqlsh_tests.py', line 176, in test_sub_second_precision     self.assertin('1943-06-19 11:21:01.123000+0000', output) assertionerror: '1943-06-19 11:21:01.123000+0000' not found in '/n id | subid                           | value/n----+---------------------------------+-------/n  1 | 1943-06-19 11:21:01.000000+0000 |   abc/n/n(1 rows)/n' -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-h0tka6 dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '256',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} dtest: debug:   id | subid                           | value ----+---------------------------------+-------   1 | 1943-06-19 11:21:01.000000+0000 |   abc (1 rows) --------------------- >> end captured logging << --------------------- ---------------------------------------------------------------------- ran 1 test in 13.016s failed (failures=1) http://cassci.datastax.com/job/trunk_dtest/1006/testreport/cqlsh_tests.cqlsh_tests/testcqlsh/test_sub_second_precision/ <text> job changes:  commit 128d144c0d22238a9045cc697daf880452be974b by josh mckenzie  cqlsh: add local timezone support to cqlsh  patch by stefan podkowinski; reviewed by paulo motta for cassandra-10397",
        "label": 252
    },
    {
        "text": "mutation replies are not correctly deserialized by originator <description> these lines in writeresponseresolver  object[] body = response.getmessagebody();  writeresponse writeresponse = (writeresponse) body[0]; cause this exception java.lang.classcastexceptionexception: [b cannot be cast to org.apache.cassandra.db.writeresponse  at org.apache.cassandra.service.writeresponseresolver.resolve(writeresponseresolver.java:50)  at org.apache.cassandra.service.writeresponseresolver.resolve(writeresponseresolver.java:31)  at org.apache.cassandra.service.quorumresponsehandler.get(quorumresponsehandler.java:101)  at org.apache.cassandra.service.storageproxy.insertblocking(storageproxy.java:132) because of course only byte[] is sent over the wire<stacktrace> java.lang.classcastexceptionexception: [b cannot be cast to org.apache.cassandra.db.writeresponse  at org.apache.cassandra.service.writeresponseresolver.resolve(writeresponseresolver.java:50)  at org.apache.cassandra.service.writeresponseresolver.resolve(writeresponseresolver.java:31)  at org.apache.cassandra.service.quorumresponsehandler.get(quorumresponsehandler.java:101)  at org.apache.cassandra.service.storageproxy.insertblocking(storageproxy.java:132) <code> object[] body = response.getmessagebody();  writeresponse writeresponse = (writeresponse) body[0]; <text> these lines in writeresponseresolver cause this exception because of course only byte[] is sent over the wire",
        "label": 274
    },
    {
        "text": "nodetool cleanup causes segfault <description> when doing cleanup on c* 2.0.12 following error crashes the java process:  info 17:59:02,800 cleaning up sstablereader(path='/data/sdd/cassandra_prod/vdna/analytics/vdna-analytics-jb-21670-data.db') # # a fatal error has been detected by the java runtime environment: # #  sigsegv (0xb) at pc=0x00007f750890268e, pid=28039, tid=140130222446336 # # jre version: java(tm) se runtime environment (7.0_71-b14) (build 1.7.0_71-b14) # java vm: java hotspot(tm) 64-bit server vm (24.71-b01 mixed mode linux-amd64 compressed oops) # problematic frame: # j 2655 c2 org.apache.cassandra.io.sstable.indexsummary.binarysearch(lorg/apache/cassandra/db/rowposition;)i (88 bytes) @ 0x00007f750890268e [0x00007f7508902580+0x10e] # # failed to write core dump. core dumps have been disabled. to enable core dumping, try \"ulimit -c unlimited\" before starting java again # # an error report file with more information is saved as: # /var/lib/cassandra_prod/hs_err_pid28039.log compiled method (c2) 913167265 4849             org.apache.cassandra.dht.token::maxkeybound (24 bytes)  total in heap  [0x00007f7508572450,0x00007f7508573318] = 3784  relocation     [0x00007f7508572570,0x00007f7508572618] = 168  main code      [0x00007f7508572620,0x00007f7508572cc0] = 1696  stub code      [0x00007f7508572cc0,0x00007f7508572cf8] = 56  oops           [0x00007f7508572cf8,0x00007f7508572d90] = 152  scopes data    [0x00007f7508572d90,0x00007f7508573118] = 904  scopes pcs     [0x00007f7508573118,0x00007f7508573268] = 336  dependencies   [0x00007f7508573268,0x00007f7508573280] = 24  handler table  [0x00007f7508573280,0x00007f75085732e0] = 96  nul chk table  [0x00007f75085732e0,0x00007f7508573318] = 56 # # if you would like to submit a bug report, please visit: #   http://bugreport.sun.com/bugreport/crash.jsp #<stacktrace> <code>  info 17:59:02,800 cleaning up sstablereader(path='/data/sdd/cassandra_prod/vdna/analytics/vdna-analytics-jb-21670-data.db') # # a fatal error has been detected by the java runtime environment: # #  sigsegv (0xb) at pc=0x00007f750890268e, pid=28039, tid=140130222446336 # # jre version: java(tm) se runtime environment (7.0_71-b14) (build 1.7.0_71-b14) # java vm: java hotspot(tm) 64-bit server vm (24.71-b01 mixed mode linux-amd64 compressed oops) # problematic frame: # j 2655 c2 org.apache.cassandra.io.sstable.indexsummary.binarysearch(lorg/apache/cassandra/db/rowposition;)i (88 bytes) @ 0x00007f750890268e [0x00007f7508902580+0x10e] # # failed to write core dump. core dumps have been disabled. to enable core dumping, try 'ulimit -c unlimited' before starting java again # # an error report file with more information is saved as: # /var/lib/cassandra_prod/hs_err_pid28039.log compiled method (c2) 913167265 4849             org.apache.cassandra.dht.token::maxkeybound (24 bytes)  total in heap  [0x00007f7508572450,0x00007f7508573318] = 3784  relocation     [0x00007f7508572570,0x00007f7508572618] = 168  main code      [0x00007f7508572620,0x00007f7508572cc0] = 1696  stub code      [0x00007f7508572cc0,0x00007f7508572cf8] = 56  oops           [0x00007f7508572cf8,0x00007f7508572d90] = 152  scopes data    [0x00007f7508572d90,0x00007f7508573118] = 904  scopes pcs     [0x00007f7508573118,0x00007f7508573268] = 336  dependencies   [0x00007f7508573268,0x00007f7508573280] = 24  handler table  [0x00007f7508573280,0x00007f75085732e0] = 96  nul chk table  [0x00007f75085732e0,0x00007f7508573318] = 56 # # if you would like to submit a bug report, please visit: #   http://bugreport.sun.com/bugreport/crash.jsp # <text> when doing cleanup on c* 2.0.12 following error crashes the java process:",
        "label": 280
    },
    {
        "text": "disable single sstable tombstone compactions for dtcs <description> we should probably disable tombstone compactions by default for dtcs for these reasons: 1. users should not do deletes with dtcs 2. the only way we should get rid of data is by ttl - and then we don't want to trigger a single sstable compaction whenever an sstable is 20%+ expired, we want to drop the whole thing when it is fully expired<stacktrace> <code> <text> we should probably disable tombstone compactions by default for dtcs for these reasons:",
        "label": 321
    },
    {
        "text": "add blocking force compaction  and anything else  calls to nodeprobe <description> there are times when i'd like to get feedback about when compactions complete. for example, if i'm deleting data from cassandra and want to know when it is 100% removed from cassandra (tombstones collected and all). this is completely trivial to implement based on the existing code (the method called by the non-blocking version returns a future, so you could just wait on that, potentially with a timeout).<stacktrace> <code> <text> there are times when i'd like to get feedback about when compactions complete. for example, if i'm deleting data from cassandra and want to know when it is 100% removed from cassandra (tombstones collected and all). this is completely trivial to implement based on the existing code (the method called by the non-blocking version returns a future, so you could just wait on that, potentially with a timeout).",
        "label": 274
    },
    {
        "text": "fix descriptor versioning for bloom filter changes <description> cassandra-2975 introduced changes to the data component, breaking stream compatibility<stacktrace> <code> <text> cassandra-2975 introduced changes to the data component, breaking stream compatibility",
        "label": 555
    },
    {
        "text": "please include a windows batch file to execute the node tool <description> a windows batch file for the node tool would be very handy. i've written my own which is based on cassandra-cli.bat: @rem  @rem licensed to the apache software foundation (asf) under one or more  @rem contributor license agreements. see the notice file distributed with  @rem this work for additional information regarding copyright ownership.  @rem the asf licenses this file to you under the apache license, version 2.0  @rem (the \"license\"); you may not use this file except in compliance with  @rem the license. you may obtain a copy of the license at  @rem  @rem http://www.apache.org/licenses/license-2.0  @rem  @rem unless required by applicable law or agreed to in writing, software  @rem distributed under the license is distributed on an \"as is\" basis,  @rem without warranties or conditions of any kind, either express or implied.  @rem see the license for the specific language governing permissions and  @rem limitations under the license. @echo off  if \"%os%\" == \"windows_nt\" setlocal if not defined cassandra_home set cassandra_home=%cd%  if not defined java_home goto err rem ensure that any user defined classpath variables are not used on startup  set classpath= rem for each jar in the cassandra_home lib directory call append to build the classpath variable.  for %%i in (%cassandra_home%\\lib*.jar) do call :append %%~fi  goto okclasspath :append  set classpath=%classpath%;%1%2  goto :eof :okclasspath  rem include the build\\classes directory so it works in development  set cassandra_classpath=%classpath%;%cassandra_home%\\build\\classes  goto runnodetool :runnodetool  echo starting nodetool  \"%java_home%\\bin\\java\" -cp \"%cassandra_classpath%\" -dstorage-config=%cassandra_home%\\conf\\ -dlog4j.configuration=log4j-tools.properties org.apache.cassandra.tools.nodecmd %*  goto finally :err  echo the java_home environment variable must be set to run this program!  pause :finally endlocal<stacktrace> <code> if not defined cassandra_home set cassandra_home=%cd%  if not defined java_home goto err rem for each jar in the cassandra_home lib directory call append to build the classpath variable.  for %%i in (%cassandra_home%/lib*.jar) do call :append %%~fi  goto okclasspath :append  set classpath=%classpath%;%1%2  goto :eof :okclasspath  rem include the build/classes directory so it works in development  set cassandra_classpath=%classpath%;%cassandra_home%/build/classes  goto runnodetool :runnodetool  echo starting nodetool  '%java_home%/bin/java' -cp '%cassandra_classpath%' -dstorage-config=%cassandra_home%/conf/ -dlog4j.configuration=log4j-tools.properties org.apache.cassandra.tools.nodecmd %*  goto finally :finally endlocal<text> a windows batch file for the node tool would be very handy. i've written my own which is based on cassandra-cli.bat: @rem  @rem licensed to the apache software foundation (asf) under one or more  @rem contributor license agreements. see the notice file distributed with  @rem this work for additional information regarding copyright ownership.  @rem the asf licenses this file to you under the apache license, version 2.0  @rem (the 'license'); you may not use this file except in compliance with  @rem the license. you may obtain a copy of the license at  @rem  @rem http://www.apache.org/licenses/license-2.0  @rem  @rem unless required by applicable law or agreed to in writing, software  @rem distributed under the license is distributed on an 'as is' basis,  @rem without warranties or conditions of any kind, either express or implied.  @rem see the license for the specific language governing permissions and  @rem limitations under the license. @echo off  if '%os%' == 'windows_nt' setlocal rem ensure that any user defined classpath variables are not used on startup  set classpath= :err  echo the java_home environment variable must be set to run this program!  pause ",
        "label": 186
    },
    {
        "text": "can create a table with decimaltype comparator but cql explodes trying to actually use it  <description> create keyspace cassandracqltestkeyspace with strategy_class='org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=1 use cassandracqltestkeyspace create columnfamily comparator_cf_decimal (id text primary key) with comparator='decimaltype' insert into comparator_cf_decimal (id, 15.333) values ('test', 'test') 1. this leads to failure  select 15.333 from comparator_cf_decimal where id = 'test' error 12:22:56,909 internal error processing execute_cql_query  java.nio.bufferunderflowexception  at java.nio.buffer.nextgetindex(buffer.java:480)  at java.nio.heapbytebuffer.getint(heapbytebuffer.java:336)  at org.apache.cassandra.cql.jdbc.jdbcdecimal.compose(jdbcdecimal.java:90)  at org.apache.cassandra.db.marshal.decimaltype.compose(decimaltype.java:43)  at org.apache.cassandra.db.marshal.decimaltype.compare(decimaltype.java:38)  at org.apache.cassandra.db.marshal.decimaltype.compare(decimaltype.java:30)  at java.util.treemap.getentryusingcomparator(treemap.java:351)  at java.util.treemap.getentry(treemap.java:322)  at java.util.treemap.get(treemap.java:255)  at org.apache.cassandra.db.treemapbackedsortedcolumns.getcolumn(treemapbackedsortedcolumns.java:138)  at org.apache.cassandra.db.abstractcolumncontainer.getcolumn(abstractcolumncontainer.java:128)  at org.apache.cassandra.cql.queryprocessor.process(queryprocessor.java:627)  at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1236)  at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.process(cassandra.java:4072)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:2889)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:187)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:680)<stacktrace> error 12:22:56,909 internal error processing execute_cql_query  java.nio.bufferunderflowexception  at java.nio.buffer.nextgetindex(buffer.java:480)  at java.nio.heapbytebuffer.getint(heapbytebuffer.java:336)  at org.apache.cassandra.cql.jdbc.jdbcdecimal.compose(jdbcdecimal.java:90)  at org.apache.cassandra.db.marshal.decimaltype.compose(decimaltype.java:43)  at org.apache.cassandra.db.marshal.decimaltype.compare(decimaltype.java:38)  at org.apache.cassandra.db.marshal.decimaltype.compare(decimaltype.java:30)  at java.util.treemap.getentryusingcomparator(treemap.java:351)  at java.util.treemap.getentry(treemap.java:322)  at java.util.treemap.get(treemap.java:255)  at org.apache.cassandra.db.treemapbackedsortedcolumns.getcolumn(treemapbackedsortedcolumns.java:138)  at org.apache.cassandra.db.abstractcolumncontainer.getcolumn(abstractcolumncontainer.java:128)  at org.apache.cassandra.cql.queryprocessor.process(queryprocessor.java:627)  at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1236)  at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.process(cassandra.java:4072)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:2889)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:187)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:680)<code> create keyspace cassandracqltestkeyspace with strategy_class='org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=1 create columnfamily comparator_cf_decimal (id text primary key) with comparator='decimaltype' insert into comparator_cf_decimal (id, 15.333) values ('test', 'test') <text> use cassandracqltestkeyspace ",
        "label": 449
    },
    {
        "text": "build stress classes in a location that allows tools stress bin stress to find them <description> right now its hard to run stress from a checkout of trunk. you need to do 'ant artifacts' and then run the stress tool in the generated artifacts. a discussion on irc came up with the proposal to just move stress to the main jar, but the stress/stressd bash scripts in bin/, and drop the tools directory altogether. it will be easier for users to find that way and will make running stress from a checkout much easier.<stacktrace> <code> <text> right now its hard to run stress from a checkout of trunk. you need to do 'ant artifacts' and then run the stress tool in the generated artifacts. a discussion on irc came up with the proposal to just move stress to the main jar, but the stress/stressd bash scripts in bin/, and drop the tools directory altogether. it will be easier for users to find that way and will make running stress from a checkout much easier.",
        "label": 555
    },
    {
        "text": "read repair of tombstones on columnfamilies and supercolumns <description> existing rr code only repairs column tombstones (since normally cf/sc do not have values associated w/ them directly).<stacktrace> <code> <text> existing rr code only repairs column tombstones (since normally cf/sc do not have values associated w/ them directly).",
        "label": 274
    },
    {
        "text": " patch  use long math for long values <description> code performs integer math, then extends to a long value to store in a long variable. should do long math instead.<stacktrace> <code> <text> code performs integer math, then extends to a long value to store in a long variable. should do long math instead.",
        "label": 139
    },
    {
        "text": " sasi pre qa    semantics not respected when using standardanalyzer <description> tested from build cassandra-11067 create keyspace music with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; create table music.albums (     id int primary key,     artist text,     title1 text,     title2 text ); create custom index on music.albums (title1) using 'org.apache.cassandra.index.sasi.sasiindex' with options = {'tokenization_skip_stop_words': 'true', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.standardanalyzer', 'case_sensitive': 'false', 'mode': 'prefix', 'tokenization_enable_stemming': 'true'}; create custom index on music.albums (title2) using 'org.apache.cassandra.index.sasi.sasiindex' with options = {'tokenization_skip_stop_words': 'true', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.standardanalyzer', 'case_sensitive': 'false', 'mode': 'contains', 'tokenization_enable_stemming': 'true'}; insert into music.albums(id, artist, title1, title2)  values(1, 'superpitcher', 'yesterday', 'yesterday'); insert into music.albums(id, artist, title1, title2)  values(2, 'hilary duff', 'so yesterday', 'so yesterday'); insert into music.albums(id, artist, title1, title2)  values(3, 'the mr. t experience', 'yesterday rules', 'yesterday rules'); select artist,title1 from music.albums where title1='yesterday';  artist                 | title1 ------------------------+----------------            superpitcher |       yesterday             hilary duff |    so yesterday    the mr. t experience | yesterday rules   (3 rows) select artist,title1 from music.albums where title2='yesterday'; artist                 | title1 ------------------------+----------------            superpitcher |       yesterday             hilary duff |    so yesterday    the mr. t experience | yesterday rules    (3 rows) the semantic of = is not respected. sasi should return only 1 row with exact match. using like would return all 3 rows. it does impact both prefix and contains mode. using nontokenizeranalyzer return 1 row with exact match.  so indeed, the semantics of = depends on the chosen analyzer, which is inconsistent. we should force = to be exact match no matter which analyzer is chosen.<stacktrace> <code> create keyspace music with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; create table music.albums (     id int primary key,     artist text,     title1 text,     title2 text ); create custom index on music.albums (title1) using 'org.apache.cassandra.index.sasi.sasiindex' with options = {'tokenization_skip_stop_words': 'true', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.standardanalyzer', 'case_sensitive': 'false', 'mode': 'prefix', 'tokenization_enable_stemming': 'true'}; create custom index on music.albums (title2) using 'org.apache.cassandra.index.sasi.sasiindex' with options = {'tokenization_skip_stop_words': 'true', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.standardanalyzer', 'case_sensitive': 'false', 'mode': 'contains', 'tokenization_enable_stemming': 'true'}; insert into music.albums(id, artist, title1, title2)  values(1, 'superpitcher', 'yesterday', 'yesterday'); insert into music.albums(id, artist, title1, title2)  values(2, 'hilary duff', 'so yesterday', 'so yesterday'); insert into music.albums(id, artist, title1, title2)  values(3, 'the mr. t experience', 'yesterday rules', 'yesterday rules'); select artist,title1 from music.albums where title1='yesterday';  artist                 | title1 ------------------------+----------------            superpitcher |       yesterday             hilary duff |    so yesterday    the mr. t experience | yesterday rules   (3 rows) select artist,title1 from music.albums where title2='yesterday'; artist                 | title1 ------------------------+----------------            superpitcher |       yesterday             hilary duff |    so yesterday    the mr. t experience | yesterday rules    (3 rows) create keyspace music with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; create table music.albums (     id int primary key,     artist text,     title1 text,     title2 text ); create custom index on music.albums (title1) using 'org.apache.cassandra.index.sasi.sasiindex' with options = {'tokenization_skip_stop_words': 'true', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.standardanalyzer', 'case_sensitive': 'false', 'mode': 'prefix', 'tokenization_enable_stemming': 'true'}; create custom index on music.albums (title2) using 'org.apache.cassandra.index.sasi.sasiindex' with options = {'tokenization_skip_stop_words': 'true', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.standardanalyzer', 'case_sensitive': 'false', 'mode': 'contains', 'tokenization_enable_stemming': 'true'}; insert into music.albums(id, artist, title1, title2)  values(1, 'superpitcher', 'yesterday', 'yesterday'); insert into music.albums(id, artist, title1, title2)  values(2, 'hilary duff', 'so yesterday', 'so yesterday'); insert into music.albums(id, artist, title1, title2)  values(3, 'the mr. t experience', 'yesterday rules', 'yesterday rules'); select artist,title1 from music.albums where title1='yesterday';  artist                 | title1 ------------------------+----------------            superpitcher |       yesterday             hilary duff |    so yesterday    the mr. t experience | yesterday rules   (3 rows) select artist,title1 from music.albums where title2='yesterday'; artist                 | title1 ------------------------+----------------            superpitcher |       yesterday             hilary duff |    so yesterday    the mr. t experience | yesterday rules    (3 rows) <text> tested from build cassandra-11067 the semantic of = is not respected. sasi should return only 1 row with exact match. using like would return all 3 rows. it does impact both prefix and contains mode. using nontokenizeranalyzer return 1 row with exact match. so indeed, the semantics of = depends on the chosen analyzer, which is inconsistent. we should force = to be exact match no matter which analyzer is chosen.",
        "label": 412
    },
    {
        "text": "dtest failure in secondary indexes test testsecondaryindexes test query indexes with vnodes <description> example failure: http://cassci.datastax.com/job/trunk_offheap_dtest/347/testreport/secondary_indexes_test/testsecondaryindexes/test_query_indexes_with_vnodes standard output unexpected error in node2 log, error:  error [readstage-1] 2016-07-20 04:58:27,391 messagedeliverytask.java:74 - the secondary index 'composites_index' is not yet available<stacktrace> <code> standard output unexpected error in node2 log, error:  error [readstage-1] 2016-07-20 04:58:27,391 messagedeliverytask.java:74 - the secondary index 'composites_index' is not yet available http://cassci.datastax.com/job/trunk_offheap_dtest/347/testreport/secondary_indexes_test/testsecondaryindexes/test_query_indexes_with_vnodes<text> example failure: ",
        "label": 428
    },
    {
        "text": "system schema columns sometimes missing for 'system' keyspace <description> occasionally a cassandra node will have missing schema_columns information where keyspace_name='system'. cqlsh> select * from system.schema_columns where keyspace_name='system';  keyspace_name | columnfamily_name | column_name ---------------+-------------------+------------- (0 rows) all keyspace and column family schema info is present for 'system' \u2013 it's only the column information missing. this can occur on an existing cluster following node restart. the data usually appears again after bouncing the node. this is impactful to client drivers that expect column meta for configured tables. reproducible in 2.1.2. have not seen it crop up in 2.0.11.<stacktrace> <code> cqlsh> select * from system.schema_columns where keyspace_name='system';  keyspace_name | columnfamily_name | column_name ---------------+-------------------+------------- (0 rows) <text> occasionally a cassandra node will have missing schema_columns information where keyspace_name='system'. all keyspace and column family schema info is present for 'system' - it's only the column information missing. this can occur on an existing cluster following node restart. the data usually appears again after bouncing the node. this is impactful to client drivers that expect column meta for configured tables. reproducible in 2.1.2. have not seen it crop up in 2.0.11.",
        "label": 18
    },
    {
        "text": " show schema  in cli outputs invalid text structure that cannot be replayed  easily tweakable though  <description> log explaining the problem. trouble happens at the \"and replication_factor = 1\" string [default@unknown] connect cassandra2/9160;   connected to: \"lab1\" on cassandra2/9160 create a keyspace with a pretty simple definition  [default@unknown] create keyspace foo  ... with placement_strategy = 'simplestrategy'  ... and strategy_options = [ {replication_factor : 1} ];  f9e13340-d58f-11e0-0000-e3f60146f2df  waiting for schema agreement...  ... schemas agree across the cluster  [default@unknown] use foo;  authenticated to keyspace: foo copy the schema so we can paste it later  [default@foo] show schema;   create keyspace foo  and replication_factor = 1  with placement_strategy = 'simplestrategy'  and strategy_options = [ {replication_factor : 1} ]; use foo; remove the keyspace, so we can paste the exact same text above  [default@foo] drop keyspace foo;  07c93a70-d590-11e0-0000-e3f60146f2df  waiting for schema agreement...  ... schemas agree across the cluster paste the schema shown above as result of the 'show schema' command  [default@unknown] create keyspace foo  ... and replication_factor = 1  ... with placement_strategy = 'simplestrategy'  ... and strategy_options = [ {replication_factor : 1} ];  no enum constant org.apache.cassandra.cli.cliclient.addkeyspaceargument.replication_factor presented an error that should not occur if show schema generated valid text<stacktrace> <code> [default@unknown] connect cassandra2/9160;   connected to: 'lab1' on cassandra2/9160 ]; ];  no enum constant org.apache.cassandra.cli.cliclient.addkeyspaceargument.replication_factor<text> log explaining the problem. trouble happens at the 'and replication_factor = 1' string ];  f9e13340-d58f-11e0-0000-e3f60146f2df  waiting for schema agreement...  ... schemas agree across the cluster  [default@unknown] use foo;  authenticated to keyspace: foo use foo; ",
        "label": 412
    },
    {
        "text": "support collection  list  set  and map  value types in cql <description> composite columns introduce the ability to have arbitrarily nested data in a cassandra row. we should expose this through cql.<stacktrace> <code> <text> composite columns introduce the ability to have arbitrarily nested data in a cassandra row. we should expose this through cql.",
        "label": 520
    },
    {
        "text": "include snippet of cql query near error in syntaxerror messages <description> when a syntaxerror is returned, including a snippet of the query close to the error would make a lot of error messages easier to understand. for example, if you did this with the python driver: session.execute(select * from users where username='%s', ['joe smith']) you would wind up with an extra set of single quotes (the driver automatically escapes and quotes input). if a snippet like ...where username=''joe smith'' were included in the error message, this would be pretty easy to spot.<stacktrace> <code> session.execute(select * from users where username='%s', ['joe smith']) <text> when a syntaxerror is returned, including a snippet of the query close to the error would make a lot of error messages easier to understand. for example, if you did this with the python driver: you would wind up with an extra set of single quotes (the driver automatically escapes and quotes input). if a snippet like ...where username=''joe smith'' were included in the error message, this would be pretty easy to spot.",
        "label": 69
    },
    {
        "text": "rangetombstones not merging during compaction <description> when performing a compaction on two sstables that contain the same rangetombstone with different timestamps, the tombstones are not merged in the new sstable. this has been tested using cassandra 2.1 with the following table: create table test(   key text,   column text,   data text,   primary key(key, column) ); and then doing the following: insert into test (key, column, data) values (\"1\", \"1\", \"1\"); // if the sstable only contains tombstones during compaction it seems that the sstable either gets removed or isn't created (but that could probably be a separate jira issue). insert into test (key, column, data) values (\"1\", \"2\", \"2\"); // the inserts are not actually needed, since the deletes create tombstones either way. delete from test where key=\"1\" and column=\"2\"; nodetool flush insert into test (key, column, data) values (\"1\", \"2\", \"2\"); delete from test where key=\"1\" and column=\"2\"; nodetool flush nodetool compact when checking with the sstableexport tool two tombstones exists in the compacted sstable. this can be repeated, resulting in more and more tombstones.<stacktrace> <code> create table test(   key text,   column text,   data text,   primary key(key, column) ); insert into test (key, column, data) values ('1', '1', '1'); // if the sstable only contains tombstones during compaction it seems that the sstable either gets removed or isn't created (but that could probably be a separate jira issue). insert into test (key, column, data) values ('1', '2', '2'); // the inserts are not actually needed, since the deletes create tombstones either way. delete from test where key='1' and column='2'; nodetool flush insert into test (key, column, data) values ('1', '2', '2'); delete from test where key='1' and column='2'; nodetool flush nodetool compact <text> when performing a compaction on two sstables that contain the same rangetombstone with different timestamps, the tombstones are not merged in the new sstable. this has been tested using cassandra 2.1 with the following table: and then doing the following: when checking with the sstableexport tool two tombstones exists in the compacted sstable. this can be repeated, resulting in more and more tombstones.",
        "label": 86
    },
    {
        "text": "support user defined compactions through nodetool <description> for a long time, we've supported running user-defined compactions through jmx. this comes in handy fairly often, mostly when dealing with low disk space or tombstone purging, so it would be good to add something to nodetool for this. an extra option for nodetool compact would probably suffice.<stacktrace> <code> <text> for a long time, we've supported running user-defined compactions through jmx. this comes in handy fairly often, mostly when dealing with low disk space or tombstone purging, so it would be good to add something to nodetool for this. an extra option for nodetool compact would probably suffice.",
        "label": 241
    },
    {
        "text": "nodetool describeclusters shows different snitch info as to what is configured  <description> i couldn't find any similar issue as this one so i'm creating one.  i noticed that doing nodetool describecluster shows a different snitch information as to what is being set in the configuration file. my setup is hosted in aws and i am using ec2snitch. cassandra@cassandra3$ nodetool describecluster  cluster information:  name: testv3  snitch: org.apache.cassandra.locator.dynamicendpointsnitch  partitioner: org.apache.cassandra.dht.murmur3partitioner  schema versions:  fc6e8656-ee7a-341b-9782-b569d1fd1a51: [10.0.3.61,10.0.3.62,10.0.3.63] i checked via mx4j and it shows the same, i haven't verified tho using a different snitch and i am using 2.2.6 above and 3.0.x<stacktrace> <code> cassandra@cassandra3$ nodetool describecluster  cluster information:  name: testv3  snitch: org.apache.cassandra.locator.dynamicendpointsnitch  partitioner: org.apache.cassandra.dht.murmur3partitioner  schema versions:  fc6e8656-ee7a-341b-9782-b569d1fd1a51: [10.0.3.61,10.0.3.62,10.0.3.63] <text> i couldn't find any similar issue as this one so i'm creating one.  i noticed that doing nodetool describecluster shows a different snitch information as to what is being set in the configuration file. my setup is hosted in aws and i am using ec2snitch. i checked via mx4j and it shows the same, i haven't verified tho using a different snitch and i am using 2.2.6 above and 3.0.x",
        "label": 309
    },
    {
        "text": "sstable data loss when upgrading with row tombstone present <description> i ran into an issue when upgrading between 2.1.11 to 3.0.0 (and also cassandra-3.0 branch) where subsequent rows were lost within a partition where there is a row tombstone present. here's a scenario that reproduces the issue. using ccm create a single node cluster at 2.1.11: ccm create -n 1 -v 2.1.11 -s financial run the following queries to create schema, populate some data and then delete some data for november: drop keyspace if exists financial; create keyspace if not exists financial with replication = {'class': 'simplestrategy', 'replication_factor' : 1 }; create table if not exists financial.symbol_history (   symbol text,   name text static,   year int,   month int,   day int,   volume bigint,   close double,   open double,   low double,   high double,   primary key((symbol, year), month, day) ) with clustering order by (month desc, day desc); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 1, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 2, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 3, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 4, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 5, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 6, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 7, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 8, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 9, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 10, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 11, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 12, 1, 100); delete from financial.symbol_history where symbol='corp' and year = 2004 and month=11; flush and run sstable2json on the sole data.db file: ccm node1 flush sstable2json /path/to/file.db the output should look like the following: [ {\"key\": \"corp:2004\",  \"cells\": [[\"::name\",\"megacorp\",1449457517033030],            [\"12:1:\",\"\",1449457517033030],            [\"12:1:volume\",\"100\",1449457517033030],            [\"11:_\",\"11:!\",1449457564983269,\"t\",1449457564],            [\"10:1:\",\"\",1449457516313738],            [\"10:1:volume\",\"100\",1449457516313738],            [\"9:1:\",\"\",1449457516310205],            [\"9:1:volume\",\"100\",1449457516310205],            [\"8:1:\",\"\",1449457516235664],            [\"8:1:volume\",\"100\",1449457516235664],            [\"7:1:\",\"\",1449457516233535],            [\"7:1:volume\",\"100\",1449457516233535],            [\"6:1:\",\"\",1449457516231458],            [\"6:1:volume\",\"100\",1449457516231458],            [\"5:1:\",\"\",1449457516228307],            [\"5:1:volume\",\"100\",1449457516228307],            [\"4:1:\",\"\",1449457516225415],            [\"4:1:volume\",\"100\",1449457516225415],            [\"3:1:\",\"\",1449457516222811],            [\"3:1:volume\",\"100\",1449457516222811],            [\"2:1:\",\"\",1449457516220301],            [\"2:1:volume\",\"100\",1449457516220301],            [\"1:1:\",\"\",1449457516210758],            [\"1:1:volume\",\"100\",1449457516210758]]} ] prepare for upgrade ccm node1 nodetool snapshot financial ccm node1 nodetool drain ccm node1 stop upgrade to cassandra-3.0 and start the node ccm node1 setdir -v git:cassandra-3.0 ccm node1 start run command in cqlsh and observe only 1 row is returned! it appears that all data following november is gone. cqlsh> select * from financial.symbol_history;  symbol | year | month | day | name     | close | high | low  | open | volume --------+------+-------+-----+----------+-------+------+------+------+--------    corp | 2004 |    12 |   1 | megacorp |  null | null | null | null |    100 upgrade sstables and query again and you'll observe the same problem. ccm node1 nodetool upgradesstables financial i modified the 2.2 version of sstable2json so that it works with 3.0 (couldn't help myself ), and observed 2 rangetombstoneboundmarker occurrences for 1 delete and the rest of the data missing. [ {  \"key\": \"corp:2004\",  \"static\": {   \"cells\": {     [\"name\",\"megacorp\",1449457517033030]   }  },  \"rows\": [   {    \"clustering\": {\"month\": \"12\", \"day\": \"1\"},    \"cells\": {      [\"volume\",\"100\",1449457517033030]    }   },   {    \"tombstone\": [\"11:*\",1449457564983269,\"t\",1449457564]   },   {    \"tombstone\": [\"11:*\",1449457564983269,\"t\",1449457564]   }  ] } ] i'm not sure why this is happening, but i should point out that i'm using static columns here and that i'm using reverse order for my clustering, so maybe that makes a difference. i'll try without static columns / regular ordering to see if that makes a difference and update the ticket.<stacktrace> <code> [ {'key': 'corp:2004',  'cells': [['::name','megacorp',1449457517033030],            ['12:1:','',1449457517033030],            ['12:1:volume','100',1449457517033030],            ['11:_','11:!',1449457564983269,'t',1449457564],            ['10:1:','',1449457516313738],            ['10:1:volume','100',1449457516313738],            ['9:1:','',1449457516310205],            ['9:1:volume','100',1449457516310205],            ['8:1:','',1449457516235664],            ['8:1:volume','100',1449457516235664],            ['7:1:','',1449457516233535],            ['7:1:volume','100',1449457516233535],            ['6:1:','',1449457516231458],            ['6:1:volume','100',1449457516231458],            ['5:1:','',1449457516228307],            ['5:1:volume','100',1449457516228307],            ['4:1:','',1449457516225415],            ['4:1:volume','100',1449457516225415],            ['3:1:','',1449457516222811],            ['3:1:volume','100',1449457516222811],            ['2:1:','',1449457516220301],            ['2:1:volume','100',1449457516220301],            ['1:1:','',1449457516210758],            ['1:1:volume','100',1449457516210758]]} ] [ {  'key': 'corp:2004',  'static': {   'cells': {     ['name','megacorp',1449457517033030]   }  },  'rows': [   {    'clustering': {'month': '12', 'day': '1'},    'cells': {      ['volume','100',1449457517033030]    }   },   {    'tombstone': ['11:*',1449457564983269,'t',1449457564]   },   {    'tombstone': ['11:*',1449457564983269,'t',1449457564]   }  ] } ] drop keyspace if exists financial; create keyspace if not exists financial with replication = {'class': 'simplestrategy', 'replication_factor' : 1 }; create table if not exists financial.symbol_history (   symbol text,   name text static,   year int,   month int,   day int,   volume bigint,   close double,   open double,   low double,   high double,   primary key((symbol, year), month, day) ) with clustering order by (month desc, day desc); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 1, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 2, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 3, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 4, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 5, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 6, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 7, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 8, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 9, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 10, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 11, 1, 100); insert into financial.symbol_history (symbol, name, year, month, day, volume) values ('corp', 'megacorp', 2004, 12, 1, 100); delete from financial.symbol_history where symbol='corp' and year = 2004 and month=11; cqlsh> select * from financial.symbol_history;  symbol | year | month | day | name     | close | high | low  | open | volume --------+------+-------+-----+----------+-------+------+------+------+--------    corp | 2004 |    12 |   1 | megacorp |  null | null | null | null |    100 ccm create -n 1 -v 2.1.11 -s financial <text> ccm node1 flush sstable2json /path/to/file.db ccm node1 nodetool snapshot financial ccm node1 nodetool drain ccm node1 stop ccm node1 setdir -v git:cassandra-3.0 ccm node1 start ccm node1 nodetool upgradesstables financial i ran into an issue when upgrading between 2.1.11 to 3.0.0 (and also cassandra-3.0 branch) where subsequent rows were lost within a partition where there is a row tombstone present. here's a scenario that reproduces the issue. using ccm create a single node cluster at 2.1.11: run the following queries to create schema, populate some data and then delete some data for november: flush and run sstable2json on the sole data.db file: the output should look like the following: prepare for upgrade upgrade to cassandra-3.0 and start the node run command in cqlsh and observe only 1 row is returned! it appears that all data following november is gone. upgrade sstables and query again and you'll observe the same problem. i modified the 2.2 version of sstable2json so that it works with 3.0 (couldn't help myself ), and observed 2 rangetombstoneboundmarker occurrences for 1 delete and the rest of the data missing. i'm not sure why this is happening, but i should point out that i'm using static columns here and that i'm using reverse order for my clustering, so maybe that makes a difference. i'll try without static columns / regular ordering to see if that makes a difference and update the ticket.",
        "label": 86
    },
    {
        "text": "intermittent npe from recoverymanagertest recoverpit unit test <description>     [junit] testsuite: org.apache.cassandra.db.recoverymanagertest     [junit] tests run: 5, failures: 0, errors: 1, skipped: 0, time elapsed: 7.654 sec     [junit]      [junit] ------------- standard output ---------------     [junit] warn  16:40:38 no host id found, created 2cbd54a8-79a5-40e0-a8e6-c8bf2c575877 (note: this should happen exactly once per node).     [junit] warn  16:40:38 no host id found, created 2cbd54a8-79a5-40e0-a8e6-c8bf2c575877 (note: this should happen exactly once per node).     [junit] warn  16:40:38 encountered bad header at position 16 of commit log /home/mshuler/git/cassandra/build/test/cassandra/commitlog:0/commitlog-4-1414082433807.log, with invalid crc. the end of segment marker should be zero.     [junit] warn  16:40:38 encountered bad header at position 16 of commit log /home/mshuler/git/cassandra/build/test/cassandra/commitlog:0/commitlog-4-1414082433807.log, with invalid crc. the end of segment marker should be zero.     [junit] ------------- ---------------- ---------------     [junit] testcase: testrecoverpit(org.apache.cassandra.db.recoverymanagertest):      caused an error     [junit] null     [junit] java.lang.nullpointerexception     [junit]     at org.apache.cassandra.db.recoverymanagertest.testrecoverpit(recoverymanagertest.java:129)     [junit]      [junit]      [junit] test org.apache.cassandra.db.recoverymanagertest failed test fails roughly 20-25% of ci runs. several 10x and 25x bisections for 2.1 git bisect start cassandra-2.1 f03e505 resulted in first bad commit: [1394b128c65ef1ad59f765e9c9c5058cac04ca69] which is cassandra-6904. that patch went to 2.0 and i still need to dig there to see if we're getting the same error, but i've attached the unit test failure system.log from 2.1.<stacktrace>     [junit] testsuite: org.apache.cassandra.db.recoverymanagertest     [junit] tests run: 5, failures: 0, errors: 1, skipped: 0, time elapsed: 7.654 sec     [junit]      [junit] ------------- standard output ---------------     [junit] warn  16:40:38 no host id found, created 2cbd54a8-79a5-40e0-a8e6-c8bf2c575877 (note: this should happen exactly once per node).     [junit] warn  16:40:38 no host id found, created 2cbd54a8-79a5-40e0-a8e6-c8bf2c575877 (note: this should happen exactly once per node).     [junit] warn  16:40:38 encountered bad header at position 16 of commit log /home/mshuler/git/cassandra/build/test/cassandra/commitlog:0/commitlog-4-1414082433807.log, with invalid crc. the end of segment marker should be zero.     [junit] warn  16:40:38 encountered bad header at position 16 of commit log /home/mshuler/git/cassandra/build/test/cassandra/commitlog:0/commitlog-4-1414082433807.log, with invalid crc. the end of segment marker should be zero.     [junit] ------------- ---------------- ---------------     [junit] testcase: testrecoverpit(org.apache.cassandra.db.recoverymanagertest):      caused an error     [junit] null     [junit] java.lang.nullpointerexception     [junit]     at org.apache.cassandra.db.recoverymanagertest.testrecoverpit(recoverymanagertest.java:129)     [junit]      [junit]      [junit] test org.apache.cassandra.db.recoverymanagertest failed <code> first bad commit: [1394b128c65ef1ad59f765e9c9c5058cac04ca69] <text> test fails roughly 20-25% of ci runs. several 10x and 25x bisections for 2.1 git bisect start cassandra-2.1 f03e505 resulted in which is cassandra-6904. that patch went to 2.0 and i still need to dig there to see if we're getting the same error, but i've attached the unit test failure system.log from 2.1.",
        "label": 474
    },
    {
        "text": "cassandra driver returns different number of results depending on fetchsize <description> i'm trying to fetch all distinct keys from a cf using cassandra-driver (2.1.7.1) and i observed some strange behavior :- the total distinct rows are 498 so if i perform a query get all distinctkeys it returns 503 instead of 498(five keys twice).  but if i define the fetch size in select statement more than 498 then it returns exact 498 rows. and if i execute same statement on dev-center it returns 498 rows (because the default fetch size is 5000). in `cqlsh` it returns 503 rows (because cqlsh uses fetch size=100). some additional and useful information :-   -------------------------------------------------------  cassandra-2.1.13 (c)* version  consistency level: one   local machine(ubuntu 14.04) table schema:-  ---------------------- create table sample (      pk1 text,      pk2 text,     row_id uuid,     value blob,     primary key (( pk1,  pk2)) ) with bloom_filter_fp_chance = 0.01     and caching = '{\"keys\":\"all\", \"rows_per_partition\":\"none\"}'     and comment = ''     and compaction = {'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99.0percentile'; query :-  ------------ select distinct  pk2, pk1 from sample limit 2147483647;<stacktrace> <code> create table sample (      pk1 text,      pk2 text,     row_id uuid,     value blob,     primary key (( pk1,  pk2)) ) with bloom_filter_fp_chance = 0.01     and caching = '{'keys':'all', 'rows_per_partition':'none'}'     and comment = ''     and compaction = {'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99.0percentile'; select distinct  pk2, pk1 from sample limit 2147483647; create table sample (      pk1 text,      pk2 text,     row_id uuid,     value blob,     primary key (( pk1,  pk2)) ) with bloom_filter_fp_chance = 0.01     and caching = '{'keys':'all', 'rows_per_partition':'none'}'     and comment = ''     and compaction = {'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy'}     and compression = {'sstable_compression': 'org.apache.cassandra.io.compress.lz4compressor'}     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99.0percentile'; select distinct  pk2, pk1 from sample limit 2147483647; some additional and useful information :-   -------------------------------------------------------  cassandra-2.1.13 (c)* version  consistency level: one   local machine(ubuntu 14.04) table schema:-  ---------------------- query :-  ------------<text> i'm trying to fetch all distinct keys from a cf using cassandra-driver (2.1.7.1) and i observed some strange behavior :- the total distinct rows are 498 so if i perform a query get all distinctkeys it returns 503 instead of 498(five keys twice).  but if i define the fetch size in select statement more than 498 then it returns exact 498 rows. and if i execute same statement on dev-center it returns 498 rows (because the default fetch size is 5000). in `cqlsh` it returns 503 rows (because cqlsh uses fetch size=100). ",
        "label": 69
    },
    {
        "text": "cqlsh  error running  select   vs  select all columns  <description> install cql 1.0.5 from http://code.google.com/a/apache-extras.org/p/cassandra-dbapi2/downloads/detail?name=cql-1.0.5.tar.gz&can=2&q= query using \"select *\" cqlsh> select * from users; exception: 'utf8' codec can't decode byte 0xb4 in position 7: unexpected code byte query selecting all columns  select key, password, gender, session_token, state, birth_year from users;    key |  password | gender | session_token | state | birth_year |  user1 | ch@ngem3a |      f |          none |    tx |       1968 | test data create keyspace ks1 with    strategy_class =       'org.apache.cassandra.locator.simplestrategy'    and strategy_options:replication_factor=1;    use ks1; create columnfamily users (   key varchar primary key, password varchar, gender varchar,   session_token varchar, state varchar, birth_year bigint);    insert into users (key, password, gender, state, birth_year) values ('user1', 'ch@ngem3a', 'f', 'tx', '1968');    <stacktrace> <code> cqlsh> select * from users; exception: 'utf8' codec can't decode byte 0xb4 in position 7: unexpected code byte  select key, password, gender, session_token, state, birth_year from users;    key |  password | gender | session_token | state | birth_year |  user1 | ch@ngem3a |      f |          none |    tx |       1968 | create keyspace ks1 with    strategy_class =       'org.apache.cassandra.locator.simplestrategy'    and strategy_options:replication_factor=1;    use ks1; create columnfamily users (   key varchar primary key, password varchar, gender varchar,   session_token varchar, state varchar, birth_year bigint);    insert into users (key, password, gender, state, birth_year) values ('user1', 'ch@ngem3a', 'f', 'tx', '1968');    <text> ",
        "label": 412
    },
    {
        "text": "nullpointerexception in compactioninfo getid compactioninfo java  <description> note: the same trace is cited in the last comment of https://issues.apache.org/jira/browse/cassandra-11961 i've noticed that some of my nodes in my 2.1 cluster have fallen way behind on compactions, and have huge numbers (thousands) of uncompacted, tiny sstables (~30mb or so). in diagnosing the issue, i've found that \"nodetool compactionstats\" returns the exception below. restarting cassandra on the node here causes the pending tasks count to jump to ~2000. compactions run properly for about an hour, until this exception occurs again. once it occurs, i see the pending tasks value rapidly drop towards zero, but without any compactions actually running (the logs show no compactions finishing). it would seem that this is causing compactions to fail on this node, which is leading to it running out of space, etc. [redacted]# nodetool compactionstats  xss = -ea -javaagent:/usr/share/cassandra/lib/jamm-0.3.0.jar -xx:+usethreadpriorities -xx:threadprioritypolicy=42 -xms12g -xmx12g -xmn1000m -xss255k  pending tasks: 5  error: null  \u2013 stacktrace \u2013  java.lang.nullpointerexception  at org.apache.cassandra.db.compaction.compactioninfo.getid(compactioninfo.java:65)  at org.apache.cassandra.db.compaction.compactioninfo.asmap(compactioninfo.java:118)  at org.apache.cassandra.db.compaction.compactionmanager.getcompactions(compactionmanager.java:1405)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at sun.reflect.misc.trampoline.invoke(unknown source)  at sun.reflect.generatedmethodaccessor3.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at sun.reflect.misc.methodutil.invoke(unknown source)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(unknown source)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(unknown source)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(unknown source)  at com.sun.jmx.mbeanserver.perinterface.getattribute(unknown source)  at com.sun.jmx.mbeanserver.mbeansupport.getattribute(unknown source)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getattribute(unknown source)  at com.sun.jmx.mbeanserver.jmxmbeanserver.getattribute(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.access$300(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.getattribute(unknown source)  at sun.reflect.generatedmethodaccessor11.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at sun.rmi.server.unicastserverref.dispatch(unknown source)  at sun.rmi.transport.transport$1.run(unknown source)  at sun.rmi.transport.transport$1.run(unknown source)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(unknown source)  at sun.rmi.transport.tcp.tcptransport.handlemessages(unknown source)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(unknown source)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(unknown source)  at java.util.concurrent.threadpoolexecutor.runworker(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)<stacktrace> [redacted]# nodetool compactionstats  xss = -ea -javaagent:/usr/share/cassandra/lib/jamm-0.3.0.jar -xx:+usethreadpriorities -xx:threadprioritypolicy=42 -xms12g -xmx12g -xmn1000m -xss255k  pending tasks: 5  error: null  - stacktrace -  java.lang.nullpointerexception  at org.apache.cassandra.db.compaction.compactioninfo.getid(compactioninfo.java:65)  at org.apache.cassandra.db.compaction.compactioninfo.asmap(compactioninfo.java:118)  at org.apache.cassandra.db.compaction.compactionmanager.getcompactions(compactionmanager.java:1405)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at sun.reflect.misc.trampoline.invoke(unknown source)  at sun.reflect.generatedmethodaccessor3.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at sun.reflect.misc.methodutil.invoke(unknown source)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(unknown source)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(unknown source)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(unknown source)  at com.sun.jmx.mbeanserver.perinterface.getattribute(unknown source)  at com.sun.jmx.mbeanserver.mbeansupport.getattribute(unknown source)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getattribute(unknown source)  at com.sun.jmx.mbeanserver.jmxmbeanserver.getattribute(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.access$300(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(unknown source)  at javax.management.remote.rmi.rmiconnectionimpl.getattribute(unknown source)  at sun.reflect.generatedmethodaccessor11.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(unknown source)  at java.lang.reflect.method.invoke(unknown source)  at sun.rmi.server.unicastserverref.dispatch(unknown source)  at sun.rmi.transport.transport$1.run(unknown source)  at sun.rmi.transport.transport$1.run(unknown source)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(unknown source)  at sun.rmi.transport.tcp.tcptransport.handlemessages(unknown source)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(unknown source)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(unknown source)  at java.util.concurrent.threadpoolexecutor.runworker(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)<code> <text> note: the same trace is cited in the last comment of https://issues.apache.org/jira/browse/cassandra-11961 i've noticed that some of my nodes in my 2.1 cluster have fallen way behind on compactions, and have huge numbers (thousands) of uncompacted, tiny sstables (~30mb or so). in diagnosing the issue, i've found that 'nodetool compactionstats' returns the exception below. restarting cassandra on the node here causes the pending tasks count to jump to ~2000. compactions run properly for about an hour, until this exception occurs again. once it occurs, i see the pending tasks value rapidly drop towards zero, but without any compactions actually running (the logs show no compactions finishing). it would seem that this is causing compactions to fail on this node, which is leading to it running out of space, etc. ",
        "label": 500
    },
    {
        "text": "dtest failure in upgrade tests upgrade through versions test testupgrade current x to indev x rolling upgrade test <description> example failure: http://cassci.datastax.com/job/cassandra-3.x_large_dtest/16/testreport/upgrade_tests.upgrade_through_versions_test/testupgrade_current_2_1_x_to_indev_3_x/rolling_upgrade_test/ error message subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['upgradesstables', '-a']] exited with non-zero status; exit status: 2;  stderr: error: null -- stacktrace -- java.lang.assertionerror at org.apache.cassandra.db.rows.rows.collectstats(rows.java:88) at org.apache.cassandra.io.sstable.format.big.bigtablewriter$statscollector.applytorow(bigtablewriter.java:237) at org.apache.cassandra.db.transform.baserows.applyone(baserows.java:120) at org.apache.cassandra.db.transform.baserows.add(baserows.java:110) at org.apache.cassandra.db.transform.unfilteredrows.add(unfilteredrows.java:44) at org.apache.cassandra.db.transform.transformation.add(transformation.java:174) at org.apache.cassandra.db.transform.transformation.apply(transformation.java:140) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.append(bigtablewriter.java:171) at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:135) at org.apache.cassandra.db.compaction.writers.defaultcompactionwriter.realappend(defaultcompactionwriter.java:65) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.append(compactionawarewriter.java:141) at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:199) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:85) at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:61) at org.apache.cassandra.db.compaction.compactionmanager$5.execute(compactionmanager.java:420) at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:312) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$204(namedthreadfactory.java:81) at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$5/1992550266.run(unknown source) at java.lang.thread.run(thread.java:745) stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py\", line 279, in rolling_upgrade_test     self.upgrade_scenario(rolling=true)   file \"/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py\", line 345, in upgrade_scenario     self.upgrade_to_version(version_meta, partial=true, nodes=(node,))   file \"/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py\", line 446, in upgrade_to_version     node.nodetool('upgradesstables -a')   file \"/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py\", line 783, in nodetool     return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])   file \"/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py\", line 1993, in handle_external_tool_process     raise toolerror(cmd_args, rc, out, err) related failures:  http://cassci.datastax.com/job/cassandra-3.x_large_dtest/16/testreport/upgrade_tests.upgrade_through_versions_test/testupgrade_current_2_2_x_to_indev_3_x/rolling_upgrade_with_internode_ssl_test/<stacktrace> error message subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['upgradesstables', '-a']] exited with non-zero status; exit status: 2;  stderr: error: null -- stacktrace -- java.lang.assertionerror at org.apache.cassandra.db.rows.rows.collectstats(rows.java:88) at org.apache.cassandra.io.sstable.format.big.bigtablewriter$statscollector.applytorow(bigtablewriter.java:237) at org.apache.cassandra.db.transform.baserows.applyone(baserows.java:120) at org.apache.cassandra.db.transform.baserows.add(baserows.java:110) at org.apache.cassandra.db.transform.unfilteredrows.add(unfilteredrows.java:44) at org.apache.cassandra.db.transform.transformation.add(transformation.java:174) at org.apache.cassandra.db.transform.transformation.apply(transformation.java:140) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.append(bigtablewriter.java:171) at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:135) at org.apache.cassandra.db.compaction.writers.defaultcompactionwriter.realappend(defaultcompactionwriter.java:65) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.append(compactionawarewriter.java:141) at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:199) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:85) at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:61) at org.apache.cassandra.db.compaction.compactionmanager$5.execute(compactionmanager.java:420) at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:312) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$204(namedthreadfactory.java:81) at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$5/1992550266.run(unknown source) at java.lang.thread.run(thread.java:745) <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py', line 279, in rolling_upgrade_test     self.upgrade_scenario(rolling=true)   file '/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py', line 345, in upgrade_scenario     self.upgrade_to_version(version_meta, partial=true, nodes=(node,))   file '/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py', line 446, in upgrade_to_version     node.nodetool('upgradesstables -a')   file '/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py', line 783, in nodetool     return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])   file '/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py', line 1993, in handle_external_tool_process     raise toolerror(cmd_args, rc, out, err) http://cassci.datastax.com/job/cassandra-3.x_large_dtest/16/testreport/upgrade_tests.upgrade_through_versions_test/testupgrade_current_2_1_x_to_indev_3_x/rolling_upgrade_test/ related failures:  http://cassci.datastax.com/job/cassandra-3.x_large_dtest/16/testreport/upgrade_tests.upgrade_through_versions_test/testupgrade_current_2_2_x_to_indev_3_x/rolling_upgrade_with_internode_ssl_test/<text> example failure: ",
        "label": 69
    },
    {
        "text": "improve bytebuffer compression interface <description> now that we have a few uses of compression/decompression on bytebuffers it is time to finalize the interface before it becomes set in stone with 2.2. the current code has some shortcomings: the interface uses the buffers' positions and limits instead of accepting offset and length as parameters. this necessitates that the buffers be duplicated before they can be compressed for thread-safety, something that adds burden to the caller, is prone to being forgotten, and we could generally do without for performance. the direct/non-direct buffer support needs to be more clearly defined. the current usedirectoutputbytebuffers is not named well. if we don't want to support non-direct buffers everywhere as a fallback, we should clearly state the decision and rationale. how should wrappedbytebuffer treat direct/indirect buffers? more testing is necessary as e.g. errors in deflatecompressor were only caught in cassandra-6809.<stacktrace> <code> <text> now that we have a few uses of compression/decompression on bytebuffers it is time to finalize the interface before it becomes set in stone with 2.2. the current code has some shortcomings:",
        "label": 86
    },
    {
        "text": "summaries are needlessly rebuilt when the bf fp ratio is changed <description> this is from trunk, but i also saw this happen on 2.0: before: root@bw-1:/srv/cassandra# ls -ltr /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ total 221460 drwxr-xr-x 2 root root      4096 feb 11 23:34 backups -rw-r--r-- 1 root root        80 feb 11 23:50 ma-6-big-toc.txt -rw-r--r-- 1 root root     26518 feb 11 23:50 ma-6-big-summary.db -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-6-big-statistics.db -rw-r--r-- 1 root root   2607705 feb 11 23:50 ma-6-big-index.db -rw-r--r-- 1 root root    192440 feb 11 23:50 ma-6-big-filter.db -rw-r--r-- 1 root root        10 feb 11 23:50 ma-6-big-digest.crc32 -rw-r--r-- 1 root root  35212125 feb 11 23:50 ma-6-big-data.db -rw-r--r-- 1 root root      2156 feb 11 23:50 ma-6-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-7-big-toc.txt -rw-r--r-- 1 root root     26518 feb 11 23:50 ma-7-big-summary.db -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-7-big-statistics.db -rw-r--r-- 1 root root   2607614 feb 11 23:50 ma-7-big-index.db -rw-r--r-- 1 root root    192432 feb 11 23:50 ma-7-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-7-big-digest.crc32 -rw-r--r-- 1 root root  35190400 feb 11 23:50 ma-7-big-data.db -rw-r--r-- 1 root root      2152 feb 11 23:50 ma-7-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-5-big-toc.txt -rw-r--r-- 1 root root    104178 feb 11 23:50 ma-5-big-summary.db -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-5-big-statistics.db -rw-r--r-- 1 root root  10289077 feb 11 23:50 ma-5-big-index.db -rw-r--r-- 1 root root    757384 feb 11 23:50 ma-5-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-5-big-digest.crc32 -rw-r--r-- 1 root root 139201355 feb 11 23:50 ma-5-big-data.db -rw-r--r-- 1 root root      8508 feb 11 23:50 ma-5-big-crc.db root@bw-1:/srv/cassandra# md5sum /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ma-5-big-summary.db 5fca154fc790f7cfa37e8ad6d1c7552c bf ratio changed, node restarted: root@bw-1:/srv/cassandra# ls -ltr /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ total 242168 drwxr-xr-x 2 root root      4096 feb 11 23:34 backups -rw-r--r-- 1 root root        80 feb 11 23:50 ma-6-big-toc.txt -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-6-big-statistics.db -rw-r--r-- 1 root root   2607705 feb 11 23:50 ma-6-big-index.db -rw-r--r-- 1 root root    192440 feb 11 23:50 ma-6-big-filter.db -rw-r--r-- 1 root root        10 feb 11 23:50 ma-6-big-digest.crc32 -rw-r--r-- 1 root root  35212125 feb 11 23:50 ma-6-big-data.db -rw-r--r-- 1 root root      2156 feb 11 23:50 ma-6-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-7-big-toc.txt -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-7-big-statistics.db -rw-r--r-- 1 root root   2607614 feb 11 23:50 ma-7-big-index.db -rw-r--r-- 1 root root    192432 feb 11 23:50 ma-7-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-7-big-digest.crc32 -rw-r--r-- 1 root root  35190400 feb 11 23:50 ma-7-big-data.db -rw-r--r-- 1 root root      2152 feb 11 23:50 ma-7-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-5-big-toc.txt -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-5-big-statistics.db -rw-r--r-- 1 root root  10289077 feb 11 23:50 ma-5-big-index.db -rw-r--r-- 1 root root    757384 feb 11 23:50 ma-5-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-5-big-digest.crc32 -rw-r--r-- 1 root root 139201355 feb 11 23:50 ma-5-big-data.db -rw-r--r-- 1 root root      8508 feb 11 23:50 ma-5-big-crc.db -rw-r--r-- 1 root root        80 feb 12 00:03 ma-8-big-toc.txt -rw-r--r-- 1 root root     14902 feb 12 00:03 ma-8-big-summary.db -rw-r--r-- 1 root root     10264 feb 12 00:03 ma-8-big-statistics.db -rw-r--r-- 1 root root   1458631 feb 12 00:03 ma-8-big-index.db -rw-r--r-- 1 root root     10808 feb 12 00:03 ma-8-big-filter.db -rw-r--r-- 1 root root        10 feb 12 00:03 ma-8-big-digest.crc32 -rw-r--r-- 1 root root  19660275 feb 12 00:03 ma-8-big-data.db -rw-r--r-- 1 root root      1204 feb 12 00:03 ma-8-big-crc.db -rw-r--r-- 1 root root     26518 feb 12 00:04 ma-7-big-summary.db -rw-r--r-- 1 root root     26518 feb 12 00:04 ma-6-big-summary.db -rw-r--r-- 1 root root    104178 feb 12 00:04 ma-5-big-summary.db root@bw-1:/srv/cassandra# md5sum /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ma-5-big-summary.db  5fca154fc790f7cfa37e8ad6d1c7552c  this hurts startup time and appears to do nothing useful whatsoever.<stacktrace> <code> root@bw-1:/srv/cassandra# ls -ltr /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ total 221460 drwxr-xr-x 2 root root      4096 feb 11 23:34 backups -rw-r--r-- 1 root root        80 feb 11 23:50 ma-6-big-toc.txt -rw-r--r-- 1 root root     26518 feb 11 23:50 ma-6-big-summary.db -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-6-big-statistics.db -rw-r--r-- 1 root root   2607705 feb 11 23:50 ma-6-big-index.db -rw-r--r-- 1 root root    192440 feb 11 23:50 ma-6-big-filter.db -rw-r--r-- 1 root root        10 feb 11 23:50 ma-6-big-digest.crc32 -rw-r--r-- 1 root root  35212125 feb 11 23:50 ma-6-big-data.db -rw-r--r-- 1 root root      2156 feb 11 23:50 ma-6-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-7-big-toc.txt -rw-r--r-- 1 root root     26518 feb 11 23:50 ma-7-big-summary.db -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-7-big-statistics.db -rw-r--r-- 1 root root   2607614 feb 11 23:50 ma-7-big-index.db -rw-r--r-- 1 root root    192432 feb 11 23:50 ma-7-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-7-big-digest.crc32 -rw-r--r-- 1 root root  35190400 feb 11 23:50 ma-7-big-data.db -rw-r--r-- 1 root root      2152 feb 11 23:50 ma-7-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-5-big-toc.txt -rw-r--r-- 1 root root    104178 feb 11 23:50 ma-5-big-summary.db -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-5-big-statistics.db -rw-r--r-- 1 root root  10289077 feb 11 23:50 ma-5-big-index.db -rw-r--r-- 1 root root    757384 feb 11 23:50 ma-5-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-5-big-digest.crc32 -rw-r--r-- 1 root root 139201355 feb 11 23:50 ma-5-big-data.db -rw-r--r-- 1 root root      8508 feb 11 23:50 ma-5-big-crc.db root@bw-1:/srv/cassandra# md5sum /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ma-5-big-summary.db 5fca154fc790f7cfa37e8ad6d1c7552c root@bw-1:/srv/cassandra# ls -ltr /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ total 242168 drwxr-xr-x 2 root root      4096 feb 11 23:34 backups -rw-r--r-- 1 root root        80 feb 11 23:50 ma-6-big-toc.txt -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-6-big-statistics.db -rw-r--r-- 1 root root   2607705 feb 11 23:50 ma-6-big-index.db -rw-r--r-- 1 root root    192440 feb 11 23:50 ma-6-big-filter.db -rw-r--r-- 1 root root        10 feb 11 23:50 ma-6-big-digest.crc32 -rw-r--r-- 1 root root  35212125 feb 11 23:50 ma-6-big-data.db -rw-r--r-- 1 root root      2156 feb 11 23:50 ma-6-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-7-big-toc.txt -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-7-big-statistics.db -rw-r--r-- 1 root root   2607614 feb 11 23:50 ma-7-big-index.db -rw-r--r-- 1 root root    192432 feb 11 23:50 ma-7-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-7-big-digest.crc32 -rw-r--r-- 1 root root  35190400 feb 11 23:50 ma-7-big-data.db -rw-r--r-- 1 root root      2152 feb 11 23:50 ma-7-big-crc.db -rw-r--r-- 1 root root        80 feb 11 23:50 ma-5-big-toc.txt -rw-r--r-- 1 root root     10264 feb 11 23:50 ma-5-big-statistics.db -rw-r--r-- 1 root root  10289077 feb 11 23:50 ma-5-big-index.db -rw-r--r-- 1 root root    757384 feb 11 23:50 ma-5-big-filter.db -rw-r--r-- 1 root root         9 feb 11 23:50 ma-5-big-digest.crc32 -rw-r--r-- 1 root root 139201355 feb 11 23:50 ma-5-big-data.db -rw-r--r-- 1 root root      8508 feb 11 23:50 ma-5-big-crc.db -rw-r--r-- 1 root root        80 feb 12 00:03 ma-8-big-toc.txt -rw-r--r-- 1 root root     14902 feb 12 00:03 ma-8-big-summary.db -rw-r--r-- 1 root root     10264 feb 12 00:03 ma-8-big-statistics.db -rw-r--r-- 1 root root   1458631 feb 12 00:03 ma-8-big-index.db -rw-r--r-- 1 root root     10808 feb 12 00:03 ma-8-big-filter.db -rw-r--r-- 1 root root        10 feb 12 00:03 ma-8-big-digest.crc32 -rw-r--r-- 1 root root  19660275 feb 12 00:03 ma-8-big-data.db -rw-r--r-- 1 root root      1204 feb 12 00:03 ma-8-big-crc.db -rw-r--r-- 1 root root     26518 feb 12 00:04 ma-7-big-summary.db -rw-r--r-- 1 root root     26518 feb 12 00:04 ma-6-big-summary.db -rw-r--r-- 1 root root    104178 feb 12 00:04 ma-5-big-summary.db root@bw-1:/srv/cassandra# md5sum /var/lib/cassandra/data/keyspace1/standard1-071efdc0d11811e590c3413ee28a6c90/ma-5-big-summary.db  5fca154fc790f7cfa37e8ad6d1c7552c  <text> this is from trunk, but i also saw this happen on 2.0: before: bf ratio changed, node restarted: this hurts startup time and appears to do nothing useful whatsoever.",
        "label": 305
    },
    {
        "text": "leak detected while running offline scrub <description> i got couple of those: error 05:09:15 leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@3b60e162) to class org.apache.cassandra.io.sstable.sstablereader$instancetidier@1433208674:/var/lib/cassandra/data/sync/entity2-e24b5040199b11e5a30f75bb514ae072/sync-entity2-ka-405434 was not released before the reference was garbage collected and then: exception in thread \"main\" java.lang.outofmemoryerror: java heap space         at org.apache.cassandra.io.compress.compressedrandomaccessreader.decompresschunk(compressedrandomaccessreader.java:99)         at org.apache.cassandra.io.compress.compressedrandomaccessreader.rebuffer(compressedrandomaccessreader.java:81)         at org.apache.cassandra.io.util.randomaccessreader.read(randomaccessreader.java:353)         at java.io.randomaccessfile.readfully(randomaccessfile.java:444)         at java.io.randomaccessfile.readfully(randomaccessfile.java:424)         at org.apache.cassandra.io.util.randomaccessreader.readbytes(randomaccessreader.java:378)         at org.apache.cassandra.utils.bytebufferutil.read(bytebufferutil.java:348)         at org.apache.cassandra.utils.bytebufferutil.readwithshortlength(bytebufferutil.java:327)         at org.apache.cassandra.db.composites.abstractctype$serializer.deserialize(abstractctype.java:397)         at org.apache.cassandra.db.composites.abstractctype$serializer.deserialize(abstractctype.java:381)         at org.apache.cassandra.db.ondiskatom$serializer.deserializefromsstable(ondiskatom.java:75)         at org.apache.cassandra.db.abstractcell$1.computenext(abstractcell.java:52)         at org.apache.cassandra.db.abstractcell$1.computenext(abstractcell.java:46)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.io.sstable.sstableidentityiterator.hasnext(sstableidentityiterator.java:120)         at org.apache.cassandra.utils.mergeiterator$onetoone.computenext(mergeiterator.java:202)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at com.google.common.collect.iterators$7.computenext(iterators.java:645)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.columnindex$builder.buildforcompaction(columnindex.java:165)         at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:121)         at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:192)         at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:127)         at org.apache.cassandra.io.sstable.sstablerewriter.tryappend(sstablerewriter.java:158)         at org.apache.cassandra.db.compaction.scrubber.scrub(scrubber.java:220)         at org.apache.cassandra.tools.standalonescrubber.main(standalonescrubber.java:116)<stacktrace> exception in thread 'main' java.lang.outofmemoryerror: java heap space         at org.apache.cassandra.io.compress.compressedrandomaccessreader.decompresschunk(compressedrandomaccessreader.java:99)         at org.apache.cassandra.io.compress.compressedrandomaccessreader.rebuffer(compressedrandomaccessreader.java:81)         at org.apache.cassandra.io.util.randomaccessreader.read(randomaccessreader.java:353)         at java.io.randomaccessfile.readfully(randomaccessfile.java:444)         at java.io.randomaccessfile.readfully(randomaccessfile.java:424)         at org.apache.cassandra.io.util.randomaccessreader.readbytes(randomaccessreader.java:378)         at org.apache.cassandra.utils.bytebufferutil.read(bytebufferutil.java:348)         at org.apache.cassandra.utils.bytebufferutil.readwithshortlength(bytebufferutil.java:327)         at org.apache.cassandra.db.composites.abstractctype$serializer.deserialize(abstractctype.java:397)         at org.apache.cassandra.db.composites.abstractctype$serializer.deserialize(abstractctype.java:381)         at org.apache.cassandra.db.ondiskatom$serializer.deserializefromsstable(ondiskatom.java:75)         at org.apache.cassandra.db.abstractcell$1.computenext(abstractcell.java:52)         at org.apache.cassandra.db.abstractcell$1.computenext(abstractcell.java:46)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.io.sstable.sstableidentityiterator.hasnext(sstableidentityiterator.java:120)         at org.apache.cassandra.utils.mergeiterator$onetoone.computenext(mergeiterator.java:202)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at com.google.common.collect.iterators$7.computenext(iterators.java:645)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.columnindex$builder.buildforcompaction(columnindex.java:165)         at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:121)         at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:192)         at org.apache.cassandra.io.sstable.sstablerewriter.append(sstablerewriter.java:127)         at org.apache.cassandra.io.sstable.sstablerewriter.tryappend(sstablerewriter.java:158)         at org.apache.cassandra.db.compaction.scrubber.scrub(scrubber.java:220)         at org.apache.cassandra.tools.standalonescrubber.main(standalonescrubber.java:116) <code> error 05:09:15 leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@3b60e162) to class org.apache.cassandra.io.sstable.sstablereader$instancetidier@1433208674:/var/lib/cassandra/data/sync/entity2-e24b5040199b11e5a30f75bb514ae072/sync-entity2-ka-405434 was not released before the reference was garbage collected <text> i got couple of those: and then:",
        "label": 321
    },
    {
        "text": "hadoop compatibility broken <description> some enhancements we did recently for the cqlrecordreader are using the job context object directly to get the configuration. this patch fixes those issues so that we maintain compatibility with old and new mapreduce implementations. attached is a patch for the 2.0 branch.<stacktrace> <code> <text> some enhancements we did recently for the cqlrecordreader are using the job context object directly to get the configuration. this patch fixes those issues so that we maintain compatibility with old and new mapreduce implementations. attached is a patch for the 2.0 branch.",
        "label": 68
    },
    {
        "text": "convert pig smoke tests into real pigunit tests <description> currently, we have some ghetto pig tests in examples/pig/test, but there's currently no way to continuously integrate these since a human needs to check that the output isn't wrong, not just that the tests ran successfully. we've had garbled output problems in the past, so it would be nice to formalize our tests to catch this. pigunit appears to be a good choice for this.<stacktrace> <code> <text> currently, we have some ghetto pig tests in examples/pig/test, but there's currently no way to continuously integrate these since a human needs to check that the output isn't wrong, not just that the tests ran successfully. we've had garbled output problems in the past, so it would be nice to formalize our tests to catch this. pigunit appears to be a good choice for this.",
        "label": 22
    },
    {
        "text": "add binary protocol support for bind variables to non prepared statements <description> currently, the binary protocol allows requests as \"string\" or \"[prepared statement] id + bind vars\". allowing \"string + bind vars\" as well would simplify life for users with one-off statements and not have to choose between adding boilerplate for ps, and having to manually escape parameters, which is particularly painful for binary data.<stacktrace> <code> <text> currently, the binary protocol allows requests as 'string' or '[prepared statement] id + bind vars'. allowing 'string + bind vars' as well would simplify life for users with one-off statements and not have to choose between adding boilerplate for ps, and having to manually escape parameters, which is particularly painful for binary data.",
        "label": 321
    },
    {
        "text": "add all options in cqlshrc sample as commented out choices <description> all the options should be added to the sample cqlshrc file as commented out options. this will provide users with the 15 or so options that can be used in cqlshrc.<stacktrace> <code> <text> all the options should be added to the sample cqlshrc file as commented out options. this will provide users with the 15 or so options that can be used in cqlshrc.",
        "label": 538
    },
    {
        "text": "incorrect log entry during startup in <description> when doing some testing on 4.0 i found this in the log: 2018-10-12t14:06:14.507+0200  info [main] startupclusterconnectivitychecker.java:113 after waiting/processing for 10005 milliseconds, 1 out of 3 peers (0.0%) have been marked alive and had connections established 1 out of 3 is not 0%<stacktrace> <code> 2018-10-12t14:06:14.507+0200  info [main] startupclusterconnectivitychecker.java:113 after waiting/processing for 10005 milliseconds, 1 out of 3 peers (0.0%) have been marked alive and had connections established <text> when doing some testing on 4.0 i found this in the log: 1 out of 3 is not 0%",
        "label": 541
    },
    {
        "text": "concurrent schema changes test snapshot test dtest needs to account for hashed data dirs in <description> ====================================================================== error: snapshot_test (concurrent_schema_changes_test.testconcurrentschemachanges) ---------------------------------------------------------------------- traceback (most recent call last):   file \"/home/mshuler/git/cassandra-dtest/concurrent_schema_changes_test.py\", line 299, in snapshot_test     for f in os.listdir(dirr): oserror: [errno 2] no such file or directory: '/tmp/dtest-vzwotc/test/node1/data/ks_ns2/cf_ns2'<stacktrace> <code> ====================================================================== error: snapshot_test (concurrent_schema_changes_test.testconcurrentschemachanges) ---------------------------------------------------------------------- traceback (most recent call last):   file '/home/mshuler/git/cassandra-dtest/concurrent_schema_changes_test.py', line 299, in snapshot_test     for f in os.listdir(dirr): oserror: [errno 2] no such file or directory: '/tmp/dtest-vzwotc/test/node1/data/ks_ns2/cf_ns2'<text> ",
        "label": 85
    },
    {
        "text": "implement hints compression <description> cassandra-6230 is being implemented with compression in mind, but it's not going to be implemented by the original ticket. adding it on top should be relatively straight-forward, and important, since there are several users in the wild that use compression interface for encryption purposes. dse is one of them (but isn't the only one). losing encryption capabilities would be a regression.<stacktrace> <code> <text> cassandra-6230 is being implemented with compression in mind, but it's not going to be implemented by the original ticket. adding it on top should be relatively straight-forward, and important, since there are several users in the wild that use compression interface for encryption purposes. dse is one of them (but isn't the only one). losing encryption capabilities would be a regression.",
        "label": 79
    },
    {
        "text": "prohibit counter type as part of the pk <description> c* let me do this: create table aggregated.counter1 ( a counter , b int , primary key (b,a)) with clustering order by (a desc); and then treated a as an int! cqlsh> update aggregated.counter1 set a= a+1 where b = 2 ;bad request: invalid operation (a = a + 1) for non counter column a insert into aggregated.counter1 (b, a ) values ( 3, 2) ; (should have given can't insert must update error) even though desc table still shows it as a counter type: create table counter1 (   b int,   a counter,   primary key ((b), a) ) with clustering order by (a desc) and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.100000 and   gc_grace_seconds=864000 and   index_interval=128 and   read_repair_chance=0.000000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   default_time_to_live=0 and   speculative_retry='99.0percentile' and   memtable_flush_period_in_ms=0 and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'lz4compressor'};<stacktrace> <code> create table aggregated.counter1 ( a counter , b int , primary key (b,a)) with clustering order by (a desc); cqlsh> update aggregated.counter1 set a= a+1 where b = 2 ;bad request: invalid operation (a = a + 1) for non counter column a insert into aggregated.counter1 (b, a ) values ( 3, 2) ; create table counter1 (   b int,   a counter,   primary key ((b), a) ) with clustering order by (a desc) and   bloom_filter_fp_chance=0.010000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.100000 and   gc_grace_seconds=864000 and   index_interval=128 and   read_repair_chance=0.000000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   default_time_to_live=0 and   speculative_retry='99.0percentile' and   memtable_flush_period_in_ms=0 and   compaction={'class': 'sizetieredcompactionstrategy'} and   compression={'sstable_compression': 'lz4compressor'}; <text> c* let me do this: and then treated a as an int! (should have given can't insert must update error) even though desc table still shows it as a counter type:",
        "label": 88
    },
    {
        "text": "collectionstest has some commented tests <description> it seems that several test in collectionstest have been commented by mistake.<stacktrace> <code> <text> it seems that several test in collectionstest have been commented by mistake.",
        "label": 69
    },
    {
        "text": "restore automatic snapshot of system keyspace during upgrade <description> since 2.2, the installed version is compared with the version persisted in system.local (if any) at startup. if these versions differ, the system keyspace is snapshotted before proceeding in order to enable a rollback if any other issue prevents startup from completing. although the method to perform this check & snapshot is still present in systemkeyspace, its only callsite was mistakenly removed from cassandradaemon in cassandra-12716.<stacktrace> <code> <text> since 2.2, the installed version is compared with the version persisted in system.local (if any) at startup. if these versions differ, the system keyspace is snapshotted before proceeding in order to enable a rollback if any other issue prevents startup from completing. although the method to perform this check & snapshot is still present in systemkeyspace, its only callsite was mistakenly removed from cassandradaemon in cassandra-12716.",
        "label": 474
    },
    {
        "text": "load balancing does not account for the load of the moving node <description> given a node a (with load 10 gb) and a node b (with load 20 gb), running the loadbalance command against node a will:  1. remove node a from the ring recalculates pending ranges so that node b is responsible for the entire ring  2. pick the most loaded node node b is still reporting 20 gb load, because that is all it has locally  3. choose a token that divides the range of the most loaded node in half since the token calculation doesn't take into account the load that node b is 'inheriting' from node a, the token will divide node b's load in half and swap the loads. instead, the token calculation needs to pretend that b has already inherited the 10 gb from node a, for a total of 30 gb. the token that should be chosen falls at 15 gb of the total load, or 5 gb into node b's load.<stacktrace> <code> <text> given a node a (with load 10 gb) and a node b (with load 20 gb), running the loadbalance command against node a will:  1. remove node a from the ring since the token calculation doesn't take into account the load that node b is 'inheriting' from node a, the token will divide node b's load in half and swap the loads. instead, the token calculation needs to pretend that b has already inherited the 10 gb from node a, for a total of 30 gb. the token that should be chosen falls at 15 gb of the total load, or 5 gb into node b's load.",
        "label": 515
    },
    {
        "text": "npe when running upgradesstables <description> running a test upgrade from 0.7(version f sstables) to 1.0.  upgradesstables runs for about 40 minutes and then npe's when trying to retrieve a key. no files have been succesfully upgraded. likely related is that scrub (without having run upgrade) consumes all ram and ooms. possible theory is that a lot of paths call ipartitioner's decoratekey, and, at least in the randompartitioner's implementation, if any of those callers pass a null bytebuffer, they key will be null in the stack trace below. java.util.concurrent.executionexception: java.lang.nullpointerexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.compaction.compactionmanager.performallsstableoperation(compactionmanager.java:203)  at org.apache.cassandra.db.compaction.compactionmanager.performsstablerewrite(compactionmanager.java:219)  at org.apache.cassandra.db.columnfamilystore.sstablesrewrite(columnfamilystore.java:970)  at org.apache.cassandra.service.storageservice.upgradesstables(storageservice.java:1540)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:93)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:27)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:208)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:120)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:262)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:836)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:761)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1427)  at javax.management.remote.rmi.rmiconnectionimpl.access$200(rmiconnectionimpl.java:72)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1265)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1360)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:788)  at sun.reflect.generatedmethodaccessor39.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:305)  at sun.rmi.transport.transport$1.run(transport.java:159)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:155)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:535)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:790)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:649)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.compaction.precompactedrow.removedeletedandoldshards(precompactedrow.java:65)  at org.apache.cassandra.db.compaction.precompactedrow.<init>(precompactedrow.java:92)  at org.apache.cassandra.db.compaction.compactioncontroller.getcompactedrow(compactioncontroller.java:137)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:102)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:87)  at org.apache.cassandra.utils.mergeiterator$onetoone.computenext(mergeiterator.java:200)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at com.google.common.collect.iterators$7.computenext(iterators.java:614)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:172)  at org.apache.cassandra.db.compaction.compactionmanager$4.perform(compactionmanager.java:229)  at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:182)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more<stacktrace> java.util.concurrent.executionexception: java.lang.nullpointerexception  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.compaction.compactionmanager.performallsstableoperation(compactionmanager.java:203)  at org.apache.cassandra.db.compaction.compactionmanager.performsstablerewrite(compactionmanager.java:219)  at org.apache.cassandra.db.columnfamilystore.sstablesrewrite(columnfamilystore.java:970)  at org.apache.cassandra.service.storageservice.upgradesstables(storageservice.java:1540)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:93)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:27)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:208)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:120)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:262)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:836)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:761)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1427)  at javax.management.remote.rmi.rmiconnectionimpl.access$200(rmiconnectionimpl.java:72)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1265)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1360)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:788)  at sun.reflect.generatedmethodaccessor39.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:305)  at sun.rmi.transport.transport$1.run(transport.java:159)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:155)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:535)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:790)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:649)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.db.compaction.precompactedrow.removedeletedandoldshards(precompactedrow.java:65)  at org.apache.cassandra.db.compaction.precompactedrow.<init>(precompactedrow.java:92)  at org.apache.cassandra.db.compaction.compactioncontroller.getcompactedrow(compactioncontroller.java:137)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:102)  at org.apache.cassandra.db.compaction.compactioniterable$reducer.getreduced(compactioniterable.java:87)  at org.apache.cassandra.utils.mergeiterator$onetoone.computenext(mergeiterator.java:200)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at com.google.common.collect.iterators$7.computenext(iterators.java:614)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:172)  at org.apache.cassandra.db.compaction.compactionmanager$4.perform(compactionmanager.java:229)  at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:182)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more<code> <text> running a test upgrade from 0.7(version f sstables) to 1.0.  upgradesstables runs for about 40 minutes and then npe's when trying to retrieve a key. no files have been succesfully upgraded. likely related is that scrub (without having run upgrade) consumes all ram and ooms. possible theory is that a lot of paths call ipartitioner's decoratekey, and, at least in the randompartitioner's implementation, if any of those callers pass a null bytebuffer, they key will be null in the stack trace below. ",
        "label": 274
    },
    {
        "text": "tombstone records in cassandra are not being deleted <description> i am running into problems with get_key_range.  my command is client.get_key_range(\"keyspace1\", \"datastoredeletionschedule\",  \"\", \"\", 25, consistencylevel.one); after a lot of deletes on the datastore, i am getting error [pool-1-thread-36] 2009-10-19 17:24:28,223 cassandra.java (line  770) internal error processing get_key_range  java.lang.runtimeexception: java.util.concurrent.timeoutexception:  operation timed out.  at org.apache.cassandra.service.storageproxy.getkeyrange(storageproxy.java:560)  at org.apache.cassandra.service.cassandraserver.get_key_range(cassandraserver.java:595)  at org.apache.cassandra.service.cassandra$processor$get_key_range.process(cassandra.java:766)  at org.apache.cassandra.service.cassandra$processor.process(cassandra.java:609)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:885)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619)  caused by: java.util.concurrent.timeoutexception: operation timed out.  at org.apache.cassandra.net.asyncresult.get(asyncresult.java:97)  at org.apache.cassandra.service.storageproxy.getkeyrange(storageproxy.java:556)  ... 7 more turns out that the compaction code removes tombstones, and it runs whenever you have  enough sstable fragments. as an optimization, if there is  only one version of a row it will just copy it to the new sstable.  this means it won't clean out tombstones, which is causing this problem.<stacktrace> error [pool-1-thread-36] 2009-10-19 17:24:28,223 cassandra.java (line  770) internal error processing get_key_range  java.lang.runtimeexception: java.util.concurrent.timeoutexception:  operation timed out.  at org.apache.cassandra.service.storageproxy.getkeyrange(storageproxy.java:560)  at org.apache.cassandra.service.cassandraserver.get_key_range(cassandraserver.java:595)  at org.apache.cassandra.service.cassandra$processor$get_key_range.process(cassandra.java:766)  at org.apache.cassandra.service.cassandra$processor.process(cassandra.java:609)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:885)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:907)  at java.lang.thread.run(thread.java:619)  caused by: java.util.concurrent.timeoutexception: operation timed out.  at org.apache.cassandra.net.asyncresult.get(asyncresult.java:97)  at org.apache.cassandra.service.storageproxy.getkeyrange(storageproxy.java:556)  ... 7 more <code> i am running into problems with get_key_range.  my command is client.get_key_range('keyspace1', 'datastoredeletionschedule',  '', '', 25, consistencylevel.one); <text> after a lot of deletes on the datastore, i am getting turns out that the compaction code removes tombstones, and it runs whenever you have  enough sstable fragments. as an optimization, if there is  only one version of a row it will just copy it to the new sstable.  this means it won't clean out tombstones, which is causing this problem.",
        "label": 274
    },
    {
        "text": "tools java driver needs to be updated <description> when you run stress currently you get a bunch of harmless stacktraces like: error 21:11:51 error parsing schema options for table system_traces.sessions: cluster.getmetadata().getkeyspace(\"system_traces\").gettable(\"sessions\").getoptions() will return null java.lang.illegalargumentexception: populate_io_cache_on_flush is not a column defined in this metadata         at com.datastax.driver.core.columndefinitions.getallidx(columndefinitions.java:273) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.columndefinitions.getfirstidx(columndefinitions.java:279) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.arraybackedrow.isnull(arraybackedrow.java:56) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.tablemetadata$options.<init>(tablemetadata.java:529) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.tablemetadata.build(tablemetadata.java:119) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.metadata.buildtablemetadata(metadata.java:131) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.metadata.rebuildschema(metadata.java:92) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.refreshschema(controlconnection.java:293) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.tryconnect(controlconnection.java:230) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.reconnectinternal(controlconnection.java:170) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.connect(controlconnection.java:78) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.cluster$manager.init(cluster.java:1029) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.cluster.getmetadata(cluster.java:270) [cassandra-driver-core-2.0.1.jar:na]         at org.apache.cassandra.stress.util.javadriverclient.connect(javadriverclient.java:90) [stress/:na]         at org.apache.cassandra.stress.settings.stresssettings.getjavadriverclient(stresssettings.java:177) [stress/:na]         at org.apache.cassandra.stress.settings.stresssettings.getjavadriverclient(stresssettings.java:159) [stress/:na]         at org.apache.cassandra.stress.stressaction$consumer.run(stressaction.java:264) [stress/:na]<stacktrace> error 21:11:51 error parsing schema options for table system_traces.sessions: cluster.getmetadata().getkeyspace('system_traces').gettable('sessions').getoptions() will return null java.lang.illegalargumentexception: populate_io_cache_on_flush is not a column defined in this metadata         at com.datastax.driver.core.columndefinitions.getallidx(columndefinitions.java:273) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.columndefinitions.getfirstidx(columndefinitions.java:279) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.arraybackedrow.isnull(arraybackedrow.java:56) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.tablemetadata$options.<init>(tablemetadata.java:529) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.tablemetadata.build(tablemetadata.java:119) ~[cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.metadata.buildtablemetadata(metadata.java:131) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.metadata.rebuildschema(metadata.java:92) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.refreshschema(controlconnection.java:293) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.tryconnect(controlconnection.java:230) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.reconnectinternal(controlconnection.java:170) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.controlconnection.connect(controlconnection.java:78) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.cluster$manager.init(cluster.java:1029) [cassandra-driver-core-2.0.1.jar:na]         at com.datastax.driver.core.cluster.getmetadata(cluster.java:270) [cassandra-driver-core-2.0.1.jar:na]         at org.apache.cassandra.stress.util.javadriverclient.connect(javadriverclient.java:90) [stress/:na]         at org.apache.cassandra.stress.settings.stresssettings.getjavadriverclient(stresssettings.java:177) [stress/:na]         at org.apache.cassandra.stress.settings.stresssettings.getjavadriverclient(stresssettings.java:159) [stress/:na]         at org.apache.cassandra.stress.stressaction$consumer.run(stressaction.java:264) [stress/:na] <code> <text> when you run stress currently you get a bunch of harmless stacktraces like:",
        "label": 520
    },
    {
        "text": "sizeestimatesrecorder has assertions after decommission sometimes <description> doing some testing with 2.1.8 adding and decommissioning nodes. sometimes after decommissioning the following starts being thrown by the sizeestimatesrecorder. java.lang.assertionerror: -9223372036854775808 not found in -9223372036854775798, 10 at org.apache.cassandra.locator.tokenmetadata.getpredecessor(tokenmetadata.java:683) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at org.apache.cassandra.locator.tokenmetadata.getprimaryrangesfor(tokenmetadata.java:627) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at org.apache.cassandra.db.sizeestimatesrecorder.run(sizeestimatesrecorder.java:68) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_40] at java.util.concurrent.futuretask.runandreset(futuretask.java:308) [na:1.8.0_40] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:180) [na:1.8.0_40] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:294) [na:1.8.0_40] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_40] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_40] at java.lang.thread.run(thread.java:745) [na:1.8.0_40]<stacktrace> java.lang.assertionerror: -9223372036854775808 not found in -9223372036854775798, 10 at org.apache.cassandra.locator.tokenmetadata.getpredecessor(tokenmetadata.java:683) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at org.apache.cassandra.locator.tokenmetadata.getprimaryrangesfor(tokenmetadata.java:627) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at org.apache.cassandra.db.sizeestimatesrecorder.run(sizeestimatesrecorder.java:68) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118) ~[cassandra-all-2.1.8.621.jar:2.1.8.621] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_40] at java.util.concurrent.futuretask.runandreset(futuretask.java:308) [na:1.8.0_40] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:180) [na:1.8.0_40] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:294) [na:1.8.0_40] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_40] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_40] at java.lang.thread.run(thread.java:745) [na:1.8.0_40] <code> <text> doing some testing with 2.1.8 adding and decommissioning nodes. sometimes after decommissioning the following starts being thrown by the sizeestimatesrecorder.",
        "label": 409
    },
    {
        "text": "cqlsh error when using 'login' to switch users <description> using passwordauthenticator and cassandraauthorizer: bin/cqlsh -u cassandra -p cassandra connected to test cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | cassandra 3.0.14-snapshot | cql spec 3.4.0 | native protocol v4] use help for help. cassandra@cqlsh> create role super with superuser = true and password = 'p' and login = true; cassandra@cqlsh> login super; password: super@cqlsh> list roles; 'row' object has no attribute 'values' when we initialize the shell, we configure certain settings on the session object such as self.session.default_timeout = request_timeout self.session.row_factory = ordered_dict_factory self.session.default_consistency_level = cassandra.consistencylevel.one however, once we perform a login cmd, which calls do_login(..), we create a new cluster/session object but actually never set those settings on the new session. it isn't failing on 3.x. as a workaround, it is possible to logout and log back in and things work correctly.<stacktrace> <code> bin/cqlsh -u cassandra -p cassandra connected to test cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | cassandra 3.0.14-snapshot | cql spec 3.4.0 | native protocol v4] use help for help. cassandra@cqlsh> create role super with superuser = true and password = 'p' and login = true; cassandra@cqlsh> login super; password: super@cqlsh> list roles; 'row' object has no attribute 'values' self.session.default_timeout = request_timeout self.session.row_factory = ordered_dict_factory self.session.default_consistency_level = cassandra.consistencylevel.one <text> using passwordauthenticator and cassandraauthorizer: when we initialize the shell, we configure certain settings on the session object such as however, once we perform a login cmd, which calls do_login(..), we create a new cluster/session object but actually never set those settings on the new session. it isn't failing on 3.x. as a workaround, it is possible to logout and log back in and things work correctly.",
        "label": 37
    },
    {
        "text": "add column definition kind to system schema dropped columns <description> both regular and static columns can currently be dropped by users, but this information is currently not stored in schemakeyspace.droppedcolumns. as a consequence, cfmetadata.getdroppedcolumndefinition returns a regular column and this has caused problems such as cassandra-12582. we should add the column kind to schemakeyspace.droppedcolumns so that cfmetadata.getdroppedcolumndefinition can create the correct column definition. however, altering schema tables would cause inter-node communication failures during a rolling upgrade, see cassandra-12236. therefore we should wait for a full schema migration when upgrading to the next major version.<stacktrace> <code> <text> both regular and static columns can currently be dropped by users, but this information is currently not stored in schemakeyspace.droppedcolumns. as a consequence, cfmetadata.getdroppedcolumndefinition returns a regular column and this has caused problems such as cassandra-12582. we should add the column kind to schemakeyspace.droppedcolumns so that cfmetadata.getdroppedcolumndefinition can create the correct column definition. however, altering schema tables would cause inter-node communication failures during a rolling upgrade, see cassandra-12236. therefore we should wait for a full schema migration when upgrading to the next major version.",
        "label": 508
    },
    {
        "text": "probably don't need to do full copy to row cache after un mmap  change <description> 3179 changes from directly using the bytebuffer from mmap(), to copying that buffer, cfs.cacherow() https://github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/columnfamilystore.java line 1126  says it makes a deep copy exactly to prevent issues from unmmap(). maybe this deep copy is not needed now given 3179 if so, maybe slightly better performance in both speed and memory<stacktrace> <code> cfs.cacherow() https://github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/columnfamilystore.java line 1126  says it makes a deep copy exactly to prevent issues from unmmap(). <text> 3179 changes from directly using the bytebuffer from mmap(), to copying that buffer, maybe this deep copy is not needed now given 3179 if so, maybe slightly better performance in both speed and memory",
        "label": 571
    },
    {
        "text": "grep friendly nodetool compactionstats output <description> output from nodetool compactionstats is quite hard to parse with text tools - it would be nice to have one line per compaction<stacktrace> <code> <text> output from nodetool compactionstats is quite hard to parse with text tools - it would be nice to have one line per compaction",
        "label": 570
    },
    {
        "text": "npe thrown while updating speculative execution time if table is removed during task execution <description> cassandra-14338 fixed the scheduling the speculation retry threshold calculation, but if the task happens to be scheduled while a table is being dropped, it triggers an npe. error 2020-07-14t11:34:55,762 [optionaltasks:1] org.apache.cassandra.service.cassandradaemon:446 - exception in thread thread[optionaltasks:1,5,main]  java.lang.nullpointerexception: null         at org.apache.cassandra.db.keyspace.initcf(keyspace.java:444) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace.<init>(keyspace.java:346) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace.open(keyspace.java:139) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace.open(keyspace.java:116) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace$1.apply(keyspace.java:102) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace$1.apply(keyspace.java:99) ~[cassandra-4.0.0.jar:4.0.0]         at com.google.common.collect.iterables$5.lambda$foreach$0(iterables.java:704) ~[guava-27.0-jre.jar:?]         at com.google.common.collect.indexedimmutableset.foreach(indexedimmutableset.java:45) ~[guava-27.0-jre.jar:?]         at com.google.common.collect.iterables$5.foreach(iterables.java:704) ~[guava-27.0-jre.jar:?]         at org.apache.cassandra.service.cassandradaemon.lambda$setup$2(cassandradaemon.java:412) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118) [cassandra-4.0.0.jar:4.0.0]         at java.util.concurrent.executors$runnableadapter.call(executors.java:515) [?:?]         at java.util.concurrent.futuretask.runandreset(futuretask.java:305) [?:?]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:305) [?:?]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128) [?:?]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628) [?:?]         at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) [netty-all-4.1.37.final.jar:4.1.37.final]         at java.lang.thread.run(thread.java:834) [?:?]<stacktrace> error 2020-07-14t11:34:55,762 [optionaltasks:1] org.apache.cassandra.service.cassandradaemon:446 - exception in thread thread[optionaltasks:1,5,main]  java.lang.nullpointerexception: null         at org.apache.cassandra.db.keyspace.initcf(keyspace.java:444) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace.<init>(keyspace.java:346) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace.open(keyspace.java:139) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace.open(keyspace.java:116) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace$1.apply(keyspace.java:102) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.db.keyspace$1.apply(keyspace.java:99) ~[cassandra-4.0.0.jar:4.0.0]         at com.google.common.collect.iterables$5.lambda$foreach$0(iterables.java:704) ~[guava-27.0-jre.jar:?]         at com.google.common.collect.indexedimmutableset.foreach(indexedimmutableset.java:45) ~[guava-27.0-jre.jar:?]         at com.google.common.collect.iterables$5.foreach(iterables.java:704) ~[guava-27.0-jre.jar:?]         at org.apache.cassandra.service.cassandradaemon.lambda$setup$2(cassandradaemon.java:412) ~[cassandra-4.0.0.jar:4.0.0]         at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118) [cassandra-4.0.0.jar:4.0.0]         at java.util.concurrent.executors$runnableadapter.call(executors.java:515) [?:?]         at java.util.concurrent.futuretask.runandreset(futuretask.java:305) [?:?]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:305) [?:?]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128) [?:?]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628) [?:?]         at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) [netty-all-4.1.37.final.jar:4.1.37.final]         at java.lang.thread.run(thread.java:834) [?:?]<code> <text> cassandra-14338 fixed the scheduling the speculation retry threshold calculation, but if the task happens to be scheduled while a table is being dropped, it triggers an npe. ",
        "label": 96
    },
    {
        "text": "add client to cqlsh show session <description> once the python driver supports it, https://datastax-oss.atlassian.net/browse/python-235, add the client to cqlsh show_session as done in this commit: https://github.com/apache/cassandra/commit/249f79d3718fa05347d60e09f9d3fa15059bd3d3 also, update the bundled python driver.<stacktrace> <code> https://github.com/apache/cassandra/commit/249f79d3718fa05347d60e09f9d3fa15059bd3d3 <text> once the python driver supports it, https://datastax-oss.atlassian.net/browse/python-235, add the client to cqlsh show_session as done in this commit: also, update the bundled python driver.",
        "label": 508
    },
    {
        "text": "handle empty unbootstrap ranges <description> if there are no ranges needing transfer during unbootstrap, unbootstrap fails to complete.<stacktrace> <code> <text> if there are no ranges needing transfer during unbootstrap, unbootstrap fails to complete.",
        "label": 219
    },
    {
        "text": "incomingtcpconnection recognizes from by doing socket getinetaddress  instead of broadcastaddress <description> change \"this.from = socket.getinetaddress()\" to understand the broad cast ip, but the problem is we dont know until the first packet is received, this ticket is to work around the problem until it reads the first packet.<stacktrace> <code> <text> change 'this.from = socket.getinetaddress()' to understand the broad cast ip, but the problem is we dont know until the first packet is received, this ticket is to work around the problem until it reads the first packet.",
        "label": 555
    },
    {
        "text": "mx4j does not work in <description> after update from 2.1 to 3.x version mx4j page begin empty $ curl -i cassandra1:8081 http/1.0 200 ok expires: now server: mx4j-httpd/1.0 cache-control: no-cache pragma: no-cache content-type: text/html there are no errors in the log. logs: ~ $ grep -i mx4j /local/apache-cassandra/logs/system.log | tail -2 info  [main] 2016-07-22 13:48:00,352 cassandradaemon.java:432 - jvm arguments: [-xloggc:/local/apache-cassandra//logs/gc.log, -xx:+usethreadpriorities, -xx:threadprioritypolicy=42, -xx:+heapdumponoutofmemoryerror, -xx:heapdumppath=/local/tmp, -xss256k, -xx:stringtablesize=1000003, -xx:+alwayspretouch, -xx:+usetlab, -xx:+resizetlab, -xx:+usenuma, -djava.net.preferipv4stack=true, -xms512m, -xmx1g, -xx:+useg1gc, -xx:g1rsetupdatingpausetimepercent=5, -xx:maxgcpausemillis=500, -xx:initiatingheapoccupancypercent=25, -xx:g1heapregionsize=32m, -xx:parallelgcthreads=16, -xx:+printgcdetails, -xx:+printgcdatestamps, -xx:+printheapatgc, -xx:+printtenuringdistribution, -xx:+printgcapplicationstoppedtime, -xx:+printpromotionfailure, -xx:+usegclogfilerotation, -xx:numberofgclogfiles=10, -xx:gclogfilesize=10m, -xx:compilecommandfile=/local/apache-cassandra//conf/hotspot_compiler, -javaagent:/local/apache-cassandra//lib/jamm-0.3.0.jar, -djava.rmi.server.hostname=cassandra1.d3, -dcom.sun.management.jmxremote.port=7199, -dcom.sun.management.jmxremote.rmi.port=7199, -dcom.sun.management.jmxremote.ssl=false, -dcom.sun.management.jmxremote.authenticate=false, -dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password, -djava.library.path=/local/apache-cassandra//lib/sigar-bin, -dmx4jport=8081, -dlogback.configurationfile=logback.xml, -dcassandra.logdir=/local/apache-cassandra//logs, -dcassandra.storagedir=/local/apache-cassandra//data, -dcassandra-pidfile=/local/apache-cassandra/run/cassandra.pid] info  [main] 2016-07-22 13:48:04,045 mx4jtool.java:63 - mx4j successfuly loaded ~ $ sudo lsof -i:8081 command   pid      user   fd   type    device size/off node name java    14489 cassandra   86u  ipv4 381043582      0t0  tcp cassandra1.d3:sunproxyadmin (listen) i checked versions 3.0.8 and 3.5, result the same - not work.<stacktrace> <code> $ curl -i cassandra1:8081 http/1.0 200 ok expires: now server: mx4j-httpd/1.0 cache-control: no-cache pragma: no-cache content-type: text/html ~ $ grep -i mx4j /local/apache-cassandra/logs/system.log | tail -2 info  [main] 2016-07-22 13:48:00,352 cassandradaemon.java:432 - jvm arguments: [-xloggc:/local/apache-cassandra//logs/gc.log, -xx:+usethreadpriorities, -xx:threadprioritypolicy=42, -xx:+heapdumponoutofmemoryerror, -xx:heapdumppath=/local/tmp, -xss256k, -xx:stringtablesize=1000003, -xx:+alwayspretouch, -xx:+usetlab, -xx:+resizetlab, -xx:+usenuma, -djava.net.preferipv4stack=true, -xms512m, -xmx1g, -xx:+useg1gc, -xx:g1rsetupdatingpausetimepercent=5, -xx:maxgcpausemillis=500, -xx:initiatingheapoccupancypercent=25, -xx:g1heapregionsize=32m, -xx:parallelgcthreads=16, -xx:+printgcdetails, -xx:+printgcdatestamps, -xx:+printheapatgc, -xx:+printtenuringdistribution, -xx:+printgcapplicationstoppedtime, -xx:+printpromotionfailure, -xx:+usegclogfilerotation, -xx:numberofgclogfiles=10, -xx:gclogfilesize=10m, -xx:compilecommandfile=/local/apache-cassandra//conf/hotspot_compiler, -javaagent:/local/apache-cassandra//lib/jamm-0.3.0.jar, -djava.rmi.server.hostname=cassandra1.d3, -dcom.sun.management.jmxremote.port=7199, -dcom.sun.management.jmxremote.rmi.port=7199, -dcom.sun.management.jmxremote.ssl=false, -dcom.sun.management.jmxremote.authenticate=false, -dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password, -djava.library.path=/local/apache-cassandra//lib/sigar-bin, -dmx4jport=8081, -dlogback.configurationfile=logback.xml, -dcassandra.logdir=/local/apache-cassandra//logs, -dcassandra.storagedir=/local/apache-cassandra//data, -dcassandra-pidfile=/local/apache-cassandra/run/cassandra.pid] info  [main] 2016-07-22 13:48:04,045 mx4jtool.java:63 - mx4j successfuly loaded ~ $ sudo lsof -i:8081 command   pid      user   fd   type    device size/off node name java    14489 cassandra   86u  ipv4 381043582      0t0  tcp cassandra1.d3:sunproxyadmin (listen) <text> after update from 2.1 to 3.x version mx4j page begin empty there are no errors in the log. logs: i checked versions 3.0.8 and 3.5, result the same - not work.",
        "label": 453
    },
    {
        "text": "hinted handoff rows never get deleted <description> from the list: \"after the hints are delivered, the hinted keys are deleted from the hinted cf only, but not from the application cf.\" prashant verified that this is a bug that can't be fixed until deletes are fully working. note: when we fix this, see if we can do so w/o compromising the immediate-gc of the hinted cf keys.<stacktrace> <code> <text> from the list: 'after the hints are delivered, the hinted keys are deleted from the hinted cf only, but not from the application cf.' prashant verified that this is a bug that can't be fixed until deletes are fully working. note: when we fix this, see if we can do so w/o compromising the immediate-gc of the hinted cf keys.",
        "label": 285
    },
    {
        "text": "don't mark sstables as repairing when doing sub range repair <description> since cassandra-10422 we don't do anticompaction when a user issues a sub range repair (-st x -et y), but we still mark sstables as repairing. we should avoid marking them as users might want to run many sub range repair sessions in parallel. the reason we mark sstables is that we don't want another repair session to steal the sstables before we do anticompaction, and since we do no anticompaction with sub range repair we have no benefit from the marking.<stacktrace> <code> <text> since cassandra-10422 we don't do anticompaction when a user issues a sub range repair (-st x -et y), but we still mark sstables as repairing. we should avoid marking them as users might want to run many sub range repair sessions in parallel. the reason we mark sstables is that we don't want another repair session to steal the sstables before we do anticompaction, and since we do no anticompaction with sub range repair we have no benefit from the marking.",
        "label": 321
    },
    {
        "text": "nullpointerexception causing query timeout <description> a common select query could not be completed failing with. request did not complete within rpc_timeout. output.log showed this: error 15:38:04,036 exception in thread thread[readstage:170,5,main] java.lang.runtimeexception: java.lang.nullpointerexception         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1867)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:724) caused by: java.lang.nullpointerexception         at org.apache.cassandra.db.index.composites.compositesindexonregular.isstale(compositesindexonregular.java:97)         at org.apache.cassandra.db.index.composites.compositessearcher$1.computenext(compositessearcher.java:247)         at org.apache.cassandra.db.index.composites.compositessearcher$1.computenext(compositessearcher.java:102)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.columnfamilystore.filter(columnfamilystore.java:1651)         at org.apache.cassandra.db.index.composites.compositessearcher.search(compositessearcher.java:50)         at org.apache.cassandra.db.index.secondaryindexmanager.search(secondaryindexmanager.java:525)         at org.apache.cassandra.db.columnfamilystore.search(columnfamilystore.java:1639)         at org.apache.cassandra.db.rangeslicecommand.executelocally(rangeslicecommand.java:135)         at org.apache.cassandra.service.storageproxy$localrangeslicerunnable.runmaythrow(storageproxy.java:1358)         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1863)<stacktrace> error 15:38:04,036 exception in thread thread[readstage:170,5,main] java.lang.runtimeexception: java.lang.nullpointerexception         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1867)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:724) caused by: java.lang.nullpointerexception         at org.apache.cassandra.db.index.composites.compositesindexonregular.isstale(compositesindexonregular.java:97)         at org.apache.cassandra.db.index.composites.compositessearcher$1.computenext(compositessearcher.java:247)         at org.apache.cassandra.db.index.composites.compositessearcher$1.computenext(compositessearcher.java:102)         at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:143)         at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:138)         at org.apache.cassandra.db.columnfamilystore.filter(columnfamilystore.java:1651)         at org.apache.cassandra.db.index.composites.compositessearcher.search(compositessearcher.java:50)         at org.apache.cassandra.db.index.secondaryindexmanager.search(secondaryindexmanager.java:525)         at org.apache.cassandra.db.columnfamilystore.search(columnfamilystore.java:1639)         at org.apache.cassandra.db.rangeslicecommand.executelocally(rangeslicecommand.java:135)         at org.apache.cassandra.service.storageproxy$localrangeslicerunnable.runmaythrow(storageproxy.java:1358)         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1863) <code> <text> request did not complete within rpc_timeout. a common select query could not be completed failing with. output.log showed this:",
        "label": 520
    },
    {
        "text": "converting bytes to hex string is unnecessarily slow <description> bytebufferutil.bytestohex() is unnecessarily slow - it doesn't pre-size the stringbuilder (so several re-sizes will be needed behind the scenes) and it makes quite a few method calls per byte. (ok, this may be a premature optimisation, but i couldn't resist, and it's a small change) will attach patch shortly that speeds it up by about x3, plus benchmarking test.<stacktrace> <code> <text> bytebufferutil.bytestohex() is unnecessarily slow - it doesn't pre-size the stringbuilder (so several re-sizes will be needed behind the scenes) and it makes quite a few method calls per byte. (ok, this may be a premature optimisation, but i couldn't resist, and it's a small change) will attach patch shortly that speeds it up by about x3, plus benchmarking test.",
        "label": 137
    },
    {
        "text": "bug in calculating quorum <description> hello, it seems that there is a bug in calculating quorum in src/java/org/apache/cassandra/service/quorumresponsehandler.java currently the quorum formula in place will return correct quorum if replication factor <= 3. however if you have a replication factor > 3, it will return incorrect result. -----------------------  \u2014 src/java/org/apache/cassandra/service/quorumresponsehandler.java (revision 995482)  +++ src/java/org/apache/cassandra/service/quorumresponsehandler.java (working copy)  @@ -109,7 +109,7 @@  case any:  return 1;  case quorum: return (databasedescriptor.getquorum(table)/ 2) + 1;  + return databasedescriptor.getquorum(table);  case all:  return databasedescriptor.getreplicationfactor(table);  default:  -------------------  in quorumresponsehandler:determineblockfor()  databasedescriptor.getquorum(table) is already returning a quorum value which is further divided by 2 and a one is added. so say if your rf=6, it is suppose to check 4 replicas, (6/2)+1=4 but it ends up checking only 3 replicas as databasedescriptor.getquorum returns 4, so determineblockfor will return (4/2)+1=3. let me know if you have any questions. jignesh<stacktrace> <code> it seems that there is a bug in calculating quorum in src/java/org/apache/cassandra/service/quorumresponsehandler.java -----------------------  - src/java/org/apache/cassandra/service/quorumresponsehandler.java (revision 995482)  +++ src/java/org/apache/cassandra/service/quorumresponsehandler.java (working copy)  @@ -109,7 +109,7 @@  case any:  return 1;  case quorum: <text> hello, currently the quorum formula in place will return correct quorum if replication factor <= 3. however if you have a replication factor > 3, it will return incorrect result. so say if your rf=6, it is suppose to check 4 replicas, (6/2)+1=4 but it ends up checking only 3 replicas as databasedescriptor.getquorum returns 4, so determineblockfor will return (4/2)+1=3. let me know if you have any questions. jignesh",
        "label": 251
    },
    {
        "text": "cql alter table   add causes oob <description> to reproduce: ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 2.3.0 | cassandra 1.2.0-snapshot | cql spec 3.0.0 | thrift protocol 19.35.0]  use help for help.  cqlsh> create keyspace music with replication = {'class' : ... 'simplestrategy', 'replication_factor' : 3} ;  cqlsh> use music  ... ;  cqlsh:music> create table songs (  ... id uuid primary key,  ... title text,  ... album text,  ... artist text,  ... data blob  ... );  cqlsh:music> insert into songs (id, title, artist, album) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'la grange', 'zz top', 'tres hombres');  cqlsh:music> insert into songs (id, title, artist, album) values ('8a172618-b121-4136-bb10-f665cfc469eb', 'moving in stereo', 'fu manchu', 'we must obey');  cqlsh:music> insert into songs (id, title, artist, album) values ('62c36092-82a1-3a00-93d1-46196ee77204', 'outside woman blues', 'back door slam', 'roll away');  cqlsh:music> create table song_tags (  ... id uuid,  ... tag_name text,  ... primary key (id, tag_name)  ... );  cqlsh:music> select * from song_tags;  cqlsh:music> insert into song_tags (id, tag_name) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'blues');  cqlsh:music> insert into song_tags (id, tag_name) values ('8a172618-b121-4136-bb10-f665cfc469eb', 'covers');  cqlsh:music> insert into song_tags (id, tag_name) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', '1973');  cqlsh:music> insert into song_tags (id, tag_name) values ('8a172618-b121-4136-bb10-f665cfc469eb', '2007');  cqlsh:music> select * from song_tags;  id | tag_name  -------------------------------------+---------  a3e64f8f-bd44-4f28-b8d9-6938726e34d4 | 1973  a3e64f8f-bd44-4f28-b8d9-6938726e34d4 | blues  8a172618-b121-4136-bb10-f665cfc469eb | 2007  8a172618-b121-4136-bb10-f665cfc469eb | covers cqlsh:music> drop table song_tags;  cqlsh:music> alter table songs add tags set<text>;  tsocket read 0 bytes<stacktrace> <code> id | tag_name  -------------------------------------+---------  a3e64f8f-bd44-4f28-b8d9-6938726e34d4 | 1973  a3e64f8f-bd44-4f28-b8d9-6938726e34d4 | blues  8a172618-b121-4136-bb10-f665cfc469eb | 2007  8a172618-b121-4136-bb10-f665cfc469eb | covers cqlsh:music> drop table song_tags;  cqlsh:music> alter table songs add tags set<text>;  tsocket read 0 bytes<text> to reproduce: ./cqlsh  connected to test cluster at localhost:9160.  [cqlsh 2.3.0 | cassandra 1.2.0-snapshot | cql spec 3.0.0 | thrift protocol 19.35.0]  use help for help.  cqlsh> create keyspace music with replication = ;  cqlsh> use music  ... ;  cqlsh:music> create table songs (  ... id uuid primary key,  ... title text,  ... album text,  ... artist text,  ... data blob  ... );  cqlsh:music> insert into songs (id, title, artist, album) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'la grange', 'zz top', 'tres hombres');  cqlsh:music> insert into songs (id, title, artist, album) values ('8a172618-b121-4136-bb10-f665cfc469eb', 'moving in stereo', 'fu manchu', 'we must obey');  cqlsh:music> insert into songs (id, title, artist, album) values ('62c36092-82a1-3a00-93d1-46196ee77204', 'outside woman blues', 'back door slam', 'roll away');  cqlsh:music> create table song_tags (  ... id uuid,  ... tag_name text,  ... primary key (id, tag_name)  ... );  cqlsh:music> select * from song_tags;  cqlsh:music> insert into song_tags (id, tag_name) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'blues');  cqlsh:music> insert into song_tags (id, tag_name) values ('8a172618-b121-4136-bb10-f665cfc469eb', 'covers');  cqlsh:music> insert into song_tags (id, tag_name) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', '1973');  cqlsh:music> insert into song_tags (id, tag_name) values ('8a172618-b121-4136-bb10-f665cfc469eb', '2007');  cqlsh:music> select * from song_tags; ",
        "label": 520
    },
    {
        "text": "possible problem with widerow in pig uri <description> i don't have a good way to test this directly, but i'm concerned the uri parsing for widerows isn't going to work. setlocation   1.) calls setlocationfromuri (which sets widerows to the uri value)  2.) sets widerows to a static value (which is defined as false)  3.) sets widerows to the system setting if it exists.   that doesn't seem right... but setlocationfromuri also gets called from setstorelocation, and i don't really know the difference between setlocation and setstorelocation in terms of what is going on in terms of the integration between cassandra/pig/hadoop.<stacktrace> <code> <text> i don't have a good way to test this directly, but i'm concerned the uri parsing for widerows isn't going to work. setlocation   1.) calls setlocationfromuri (which sets widerows to the uri value)  2.) sets widerows to a static value (which is defined as false)  3.) sets widerows to the system setting if it exists.   that doesn't seem right... but setlocationfromuri also gets called from setstorelocation, and i don't really know the difference between setlocation and setstorelocation in terms of what is going on in terms of the integration between cassandra/pig/hadoop.",
        "label": 85
    },
    {
        "text": "match error message with  tombstone failure threshold  config parameter <description> configuration parameter tombstone_failure_threshold is misprinted in error message, which can be confusing.  noformat  error [hintedhandoff:55] 2014-08-05 08:59:59,509 slicequeryfilter.java (line 200) scanned over 100000 tombstones in system.hints; query aborted (see tombstone_fail_threshold)  noformat  patch is trivial. ant test seems to pass.<stacktrace> <code> <text> configuration parameter tombstone_failure_threshold is misprinted in error message, which can be confusing.  noformat  error [hintedhandoff:55] 2014-08-05 08:59:59,509 slicequeryfilter.java (line 200) scanned over 100000 tombstones in system.hints; query aborted (see tombstone_fail_threshold)  noformat  patch is trivial. ant test seems to pass.",
        "label": 381
    },
    {
        "text": "disablebinary nodetool command <description> the following commands are available via `nodetool`:   disablehandoff         - disable the future hints storing on the current node   disablegossip          - disable gossip (effectively marking the node dead)   disablethrift          - disable thrift server is it possible to get disablebinary added to help with the testing of binary client drivers?<stacktrace> <code>   disablehandoff         - disable the future hints storing on the current node   disablegossip          - disable gossip (effectively marking the node dead)   disablethrift          - disable thrift server <text> the following commands are available via `nodetool`: is it possible to get disablebinary added to help with the testing of binary client drivers?",
        "label": 352
    },
    {
        "text": "upgraded cassandra loses all cfs on restart <description> a bit dramatic summary, but hey; if you upgrade cassandra and then restart it, you lose all your cfs, but they come back if you restart again. this is due to fixschemananotimestamp not flushing the new data after truncating the cf and re-doing the mutations.<stacktrace> <code> a bit dramatic summary, but hey; <text> if you upgrade cassandra and then restart it, you lose all your cfs, but they come back if you restart again. this is due to fixschemananotimestamp not flushing the new data after truncating the cf and re-doing the mutations.",
        "label": 321
    },
    {
        "text": "readcommandtest should truncate between test cases <description> readcommandtest writes to the same cf in multiple unit tests, and then counts the results in a partition. this can lead to tests failing in some circumstances:     [junit] testcase: testsinglepartitionnamesabort(org.apache.cassandra.db.readcommandtest): failed     [junit] expected:<2> but was:<1>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<1>     [junit]  at org.apache.cassandra.db.readcommandtest.testsinglepartitionnamesabort(readcommandtest.java:140)     [junit]      [junit]      [junit] testcase: testsinglepartitionsliceabort(org.apache.cassandra.db.readcommandtest): failed     [junit] expected:<2> but was:<3>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<3>     [junit]  at org.apache.cassandra.db.readcommandtest.testsinglepartitionsliceabort(readcommandtest.java:111) this can be fixed trivially by truncating the cf at the beginning of testsinglepartitionsliceabort and testsinglepartitionnamesabort also, once the truncate is in place, a potential typo is exposed in testsinglepartitionnamesabort, where the partition is written with clustering column cc and cdd, but the read command reads cc and dd<stacktrace>     [junit] testcase: testsinglepartitionnamesabort(org.apache.cassandra.db.readcommandtest): failed     [junit] expected:<2> but was:<1>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<1>     [junit]  at org.apache.cassandra.db.readcommandtest.testsinglepartitionnamesabort(readcommandtest.java:140)     [junit]      [junit]      [junit] testcase: testsinglepartitionsliceabort(org.apache.cassandra.db.readcommandtest): failed     [junit] expected:<2> but was:<3>     [junit] junit.framework.assertionfailederror: expected:<2> but was:<3>     [junit]  at org.apache.cassandra.db.readcommandtest.testsinglepartitionsliceabort(readcommandtest.java:111) <code> <text> readcommandtest writes to the same cf in multiple unit tests, and then counts the results in a partition. this can lead to tests failing in some circumstances: this can be fixed trivially by truncating the cf at the beginning of testsinglepartitionsliceabort and testsinglepartitionnamesabort also, once the truncate is in place, a potential typo is exposed in testsinglepartitionnamesabort, where the partition is written with clustering column cc and cdd, but the read command reads cc and dd",
        "label": 241
    },
    {
        "text": "cassandra stress should validate its results in  user  mode <description> <stacktrace> <code> <text> ",
        "label": 67
    },
    {
        "text": "gossipfilepropertysnitch and ec2multiregionsnitch when used in aws gce clusters doesnt use the private ips for intra dc communications   when running nodetool repair <description> neither of these snitches(gossipfilepropertysnitch and ec2multiregionsnitch ) used the private ips for communication between intra-dc nodes in my multi-region multi-dc cluster in cloud(on both aws and gce) when i ran \"nodetool repair -local\". it works fine during regular reads.  here are the various cluster flavors i tried and failed- aws + multi-region + multi-dc + gossippropertyfilesnitch + (prefer_local=true) in rackdc-properties file. aws + multi-region + multi-dc + ec2multiregionsnitch + (prefer_local=true) in rackdc-properties file. gce + multi-region + multi-dc + gossippropertyfilesnitch + (prefer_local=true) in rackdc-properties file. gce + multi-region + multi-dc + ec2multiregionsnitch + (prefer_local=true) in rackdc-properties file. i am expecting with the above setup all of my nodes in a given dc all communicate via private ips since the cloud providers dont charge us for using the private ips and they charge for using public ips. but they can use public ips for inter-dc communications which is working as expected. here is a snippet from my log files when i ran the \"nodetool repair -local\" - node responding to 'node running repair'   info [antientropystage:1] 2014-10-08 14:47:51,628 validator.java (line 254) repair #1439f290-4efa-11e4-bf3a-df845ecf54f8 sending completed merkle tree to /54.172.118.222 for system_traces/sessions  info [antientropystage:1] 2014-10-08 14:47:51,741 validator.java (line 254) repair #1439f290-4efa-11e4-bf3a-df845ecf54f8 sending completed merkle tree to /54.172.118.222 for system_traces/events node running repair -   info [antientropystage:1] 2014-10-08 14:47:51,927 repairsession.java (line 166) repair #1439f290-4efa-11e4-bf3a-df845ecf54f8 received merkle tree for events from /54.172.118.222 note: the ips its communicating is all public ips and it should have used the private ips starting with 172.x.x.x yaml file values :   the listen address is set to: private ip  the broadcast address is set to: public ip  the seeds address is set to: public ips from both dcs  the snitches tried: gpfs and ec2multiregionsnitch  rack-dc: had prefer_local set to true.<stacktrace> <code> aws + multi-region + multi-dc + gossippropertyfilesnitch + (prefer_local=true) in rackdc-properties file. gce + multi-region + multi-dc + gossippropertyfilesnitch + (prefer_local=true) in rackdc-properties file. node responding to 'node running repair'   info [antientropystage:1] 2014-10-08 14:47:51,628 validator.java (line 254) repair #1439f290-4efa-11e4-bf3a-df845ecf54f8 sending completed merkle tree to /54.172.118.222 for system_traces/sessions  info [antientropystage:1] 2014-10-08 14:47:51,741 validator.java (line 254) repair #1439f290-4efa-11e4-bf3a-df845ecf54f8 sending completed merkle tree to /54.172.118.222 for system_traces/events node running repair -   info [antientropystage:1] 2014-10-08 14:47:51,927 repairsession.java (line 166) repair #1439f290-4efa-11e4-bf3a-df845ecf54f8 received merkle tree for events from /54.172.118.222 <text> neither of these snitches(gossipfilepropertysnitch and ec2multiregionsnitch ) used the private ips for communication between intra-dc nodes in my multi-region multi-dc cluster in cloud(on both aws and gce) when i ran 'nodetool repair -local'. it works fine during regular reads. here are the various cluster flavors i tried and failed- aws + multi-region + multi-dc + ec2multiregionsnitch + (prefer_local=true) in rackdc-properties file. gce + multi-region + multi-dc + ec2multiregionsnitch + (prefer_local=true) in rackdc-properties file. i am expecting with the above setup all of my nodes in a given dc all communicate via private ips since the cloud providers dont charge us for using the private ips and they charge for using public ips. but they can use public ips for inter-dc communications which is working as expected. here is a snippet from my log files when i ran the 'nodetool repair -local' - note: the ips its communicating is all public ips and it should have used the private ips starting with 172.x.x.x yaml file values :   the listen address is set to: private ip  the broadcast address is set to: public ip  the seeds address is set to: public ips from both dcs  the snitches tried: gpfs and ec2multiregionsnitch  rack-dc: had prefer_local set to true.",
        "label": 577
    },
    {
        "text": "explicitly examine current c  state on startup to detect incompatibilities before upgrade <description> unfortunately, we cannot rely on users reading, and following, news.txt before upgrading. people don't read, or ignore it, and sometimes have issues as the result (see cassandra-8047, for example, and i know of several cases like that one). we should add an explicit compatibility check on startup, before we modify anything, or write out sstables with the new format. we should fail and complain loudly if we detect a skipped upgrade step. we should also snapshot the schema tables before attempting any conversions (since it's not uncommon to make schema modifications as part of the upgrade).<stacktrace> <code> <text> unfortunately, we cannot rely on users reading, and following, news.txt before upgrading. people don't read, or ignore it, and sometimes have issues as the result (see cassandra-8047, for example, and i know of several cases like that one). we should add an explicit compatibility check on startup, before we modify anything, or write out sstables with the new format. we should fail and complain loudly if we detect a skipped upgrade step. we should also snapshot the schema tables before attempting any conversions (since it's not uncommon to make schema modifications as part of the upgrade).",
        "label": 474
    },
    {
        "text": "update comment link to mx4j in cassandra env sh <description> seems this http://wiki.apache.org/cassandra/operations#monitoring_with_mx4j has moved to http://cassandra.apache.org/doc/latest/operating/metrics.html#jmx    <stacktrace> <code> http://wiki.apache.org/cassandra/operations#monitoring_with_mx4j has moved to http://cassandra.apache.org/doc/latest/operating/metrics.html#jmx <text> seems this    ",
        "label": 236
    },
    {
        "text": "entire sstable transfers don't work over ssl <description> entire sstable transfers do not function when ssl handler is present in the netty pipeline. this is a trivial fix to allow it to proceed over ssl there by extending benefits over ssl. ssl cassandra dtest utests & dtests<stacktrace> <code> <text> entire sstable transfers do not function when ssl handler is present in the netty pipeline. this is a trivial fix to allow it to proceed over ssl there by extending benefits over ssl.",
        "label": 149
    },
    {
        "text": "windows utest  columnfamilystoretest testscrubdatadirectories failing <description>     [junit] testcase: testscrubdatadirectories(org.apache.cassandra.db.columnfamilystoretest):  caused an error     [junit] java.nio.file.accessdeniedexception: build\\test\\cassandra\\data;0\\columnfamilystoretest1\\standard1-a0d5fa503f8b11e58dec831ef068609c\\ma-7-big-index.db     [junit] fswriteerror in build\\test\\cassandra\\data;0\\columnfamilystoretest1\\standard1-a0d5fa503f8b11e58dec831ef068609c\\ma-7-big-index.db     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:135)     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:152)     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:147)     [junit]     at org.apache.cassandra.db.columnfamilystore.scrubdatadirectories(columnfamilystore.java:556)     [junit]     at org.apache.cassandra.db.columnfamilystoretest.testscrubdatadirectories(columnfamilystoretest.java:533)     [junit] caused by: java.nio.file.accessdeniedexception: build\\test\\cassandra\\data;0\\columnfamilystoretest1\\standard1-a0d5fa503f8b11e58dec831ef068609c\\ma-7-big-index.db     [junit]     at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.java:83)     [junit]     at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.java:97)     [junit]     at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.java:102)     [junit]     at sun.nio.fs.windowsfilesystemprovider.impldelete(windowsfilesystemprovider.java:269)     [junit]     at sun.nio.fs.abstractfilesystemprovider.delete(abstractfilesystemprovider.java:103)     [junit]     at java.nio.file.files.delete(files.java:1126)     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:129) test has never passed on windows (history). stefania alborghetti: looks like this is your test. care to take this?<stacktrace>     [junit] testcase: testscrubdatadirectories(org.apache.cassandra.db.columnfamilystoretest):  caused an error     [junit] java.nio.file.accessdeniedexception: build/test/cassandra/data;0/columnfamilystoretest1/standard1-a0d5fa503f8b11e58dec831ef068609c/ma-7-big-index.db     [junit] fswriteerror in build/test/cassandra/data;0/columnfamilystoretest1/standard1-a0d5fa503f8b11e58dec831ef068609c/ma-7-big-index.db     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:135)     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:152)     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:147)     [junit]     at org.apache.cassandra.db.columnfamilystore.scrubdatadirectories(columnfamilystore.java:556)     [junit]     at org.apache.cassandra.db.columnfamilystoretest.testscrubdatadirectories(columnfamilystoretest.java:533)     [junit] caused by: java.nio.file.accessdeniedexception: build/test/cassandra/data;0/columnfamilystoretest1/standard1-a0d5fa503f8b11e58dec831ef068609c/ma-7-big-index.db     [junit]     at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.java:83)     [junit]     at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.java:97)     [junit]     at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.java:102)     [junit]     at sun.nio.fs.windowsfilesystemprovider.impldelete(windowsfilesystemprovider.java:269)     [junit]     at sun.nio.fs.abstractfilesystemprovider.delete(abstractfilesystemprovider.java:103)     [junit]     at java.nio.file.files.delete(files.java:1126)     [junit]     at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:129) <code> <text> test has never passed on windows (history). stefania alborghetti: looks like this is your test. care to take this?",
        "label": 508
    },
    {
        "text": "remove filecacheservice  instead pooling the buffers <description> after cassandra-8893, a rar will be a very lightweight object and will not need caching, so we can eliminate this cache entirely. instead we should have a pool of buffers that are page-aligned.<stacktrace> <code> <text> after cassandra-8893, a rar will be a very lightweight object and will not need caching, so we can eliminate this cache entirely. instead we should have a pool of buffers that are page-aligned.",
        "label": 508
    },
    {
        "text": "repair tests flakiness <description> repair tests come up in test failure reports every now and then. i have tried to repro the latest locally 100 times with no luck.  dtest-novnode.repair_tests.repair_test/testrepair/test_simple_sequential_repair still from experience from fixing other flaky tests i have some intuition where the problems may lie. the proposed fix should add no harm if merged. we can reopen the ticket if repair tests keep failing.<stacktrace> <code> <text> repair tests come up in test failure reports every now and then. i have tried to repro the latest locally 100 times with no luck.  dtest-novnode.repair_tests.repair_test/testrepair/test_simple_sequential_repair still from experience from fixing other flaky tests i have some intuition where the problems may lie. the proposed fix should add no harm if merged. we can reopen the ticket if repair tests keep failing.",
        "label": 73
    },
    {
        "text": "incomplete handling of exceptions when decoding incoming messages <description> messageinhandler.decode() occasionally reads the payload incorrectly, passing the full message to messagein.read() instead of just the payload bytes. you can see the stack trace in the logs from this ci run. caused by: java.lang.assertionerror: null at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:351) at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:371) at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:335) at org.apache.cassandra.net.messagein.read(messagein.java:158) at org.apache.cassandra.net.async.messageinhandler.decode(messageinhandler.java:132) reconstructed, truncated stream passed to messagein.read():  0000000b000743414c5f42414301002a01e1a5c9b089fd11e8b517436ee1243007040000005d10fc50ec  you can clearly see parameters in there encoded before the payload:  [43414c5f424143 - cal_bac] [01 - one_byte] [002a - 42, payload size] 01 e1 a5 c9 b0 89 fd 11 e8 b5 17 43 6e e1 24 30 07 04 00 00 00 1d 10 fc 50 ec<stacktrace> caused by: java.lang.assertionerror: null at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:351) at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:371) at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:335) at org.apache.cassandra.net.messagein.read(messagein.java:158) at org.apache.cassandra.net.async.messageinhandler.decode(messageinhandler.java:132) <code> reconstructed, truncated stream passed to messagein.read():  0000000b000743414c5f42414301002a01e1a5c9b089fd11e8b517436ee1243007040000005d10fc50ec  you can clearly see parameters in there encoded before the payload:  [43414c5f424143 - cal_bac] [01 - one_byte] [002a - 42, payload size] 01 e1 a5 c9 b0 89 fd 11 e8 b5 17 43 6e e1 24 30 07 04 00 00 00 1d 10 fc 50 ec<text> messageinhandler.decode() occasionally reads the payload incorrectly, passing the full message to messagein.read() instead of just the payload bytes. you can see the stack trace in the logs from this ci run. ",
        "label": 232
    },
    {
        "text": "failed aggregate creation breaks server permanently <description> while testing edge cases around aggregates, i tried the following to see if custom types were supported: ccm create v321 -v3.2.1 -n3 ccm updateconf enable_user_defined_functions:true ccm start ccm node1 cqlsh create function id(i 'dynamiccompositetype(s => utf8type, i => int32type)') returns null on null input returns 'dynamiccompositetype(s => utf8type, i => int32type)' language java as 'return i;'; // function created successfully create aggregate ag() sfunc id stype 'dynamiccompositetype(s => utf8type, i => int32type)' initcond 's@foo:i@32'; servererror: <errormessage code=0000 [server error] message=\"java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: failed parsing cql term: [s@foo:i@32] reason: syntaxexception line 1:1 no viable alternative at character '@'\"> despite the error, the aggregate appears in system tables: select * from system_schema.aggregates;  keyspace_name | aggregate_name | ... ---------------+----------------+ ...           test |             ag | ... but you can't drop it, and trying to drop its function produces the server error again: drop aggregate ag; invalidrequest: code=2200 [invalid query] message=\"cannot drop non existing aggregate 'test.ag'\" drop function id; servererror: <errormessage code=0000 [server error] message=\"java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: failed parsing cql term: [s@foo:i@32] reason: syntaxexception line 1:1 no viable alternative at character '@'\"> what's worse, it's now impossible to restart the server: ccm stop; ccm start org.apache.cassandra.exceptions.syntaxexception: failed parsing cql term: [s@foo:i@32] reason: syntaxexception line 1:1 no viable alternative at character '@' at org.apache.cassandra.cql3.cqlfragmentparser.parseany(cqlfragmentparser.java:48) at org.apache.cassandra.cql3.terms.asbytes(terms.java:51) at org.apache.cassandra.schema.schemakeyspace.createudafromrow(schemakeyspace.java:1225) at org.apache.cassandra.schema.schemakeyspace.fetchudas(schemakeyspace.java:1204) at org.apache.cassandra.schema.schemakeyspace.fetchfunctions(schemakeyspace.java:1129) at org.apache.cassandra.schema.schemakeyspace.fetchkeyspace(schemakeyspace.java:897) at org.apache.cassandra.schema.schemakeyspace.fetchkeyspaceswithout(schemakeyspace.java:872) at org.apache.cassandra.schema.schemakeyspace.fetchnonsystemkeyspaces(schemakeyspace.java:860) at org.apache.cassandra.config.schema.loadfromdisk(schema.java:125) at org.apache.cassandra.config.schema.loadfromdisk(schema.java:115) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:229) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:551) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:680)<stacktrace> ccm stop; ccm start org.apache.cassandra.exceptions.syntaxexception: failed parsing cql term: [s@foo:i@32] reason: syntaxexception line 1:1 no viable alternative at character '@' at org.apache.cassandra.cql3.cqlfragmentparser.parseany(cqlfragmentparser.java:48) at org.apache.cassandra.cql3.terms.asbytes(terms.java:51) at org.apache.cassandra.schema.schemakeyspace.createudafromrow(schemakeyspace.java:1225) at org.apache.cassandra.schema.schemakeyspace.fetchudas(schemakeyspace.java:1204) at org.apache.cassandra.schema.schemakeyspace.fetchfunctions(schemakeyspace.java:1129) at org.apache.cassandra.schema.schemakeyspace.fetchkeyspace(schemakeyspace.java:897) at org.apache.cassandra.schema.schemakeyspace.fetchkeyspaceswithout(schemakeyspace.java:872) at org.apache.cassandra.schema.schemakeyspace.fetchnonsystemkeyspaces(schemakeyspace.java:860) at org.apache.cassandra.config.schema.loadfromdisk(schema.java:125) at org.apache.cassandra.config.schema.loadfromdisk(schema.java:115) at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:229) at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:551) at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:680) <code> ccm create v321 -v3.2.1 -n3 ccm updateconf enable_user_defined_functions:true ccm start ccm node1 cqlsh create function id(i 'dynamiccompositetype(s => utf8type, i => int32type)') returns null on null input returns 'dynamiccompositetype(s => utf8type, i => int32type)' language java as 'return i;'; // function created successfully create aggregate ag() sfunc id stype 'dynamiccompositetype(s => utf8type, i => int32type)' initcond 's@foo:i@32'; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: failed parsing cql term: [s@foo:i@32] reason: syntaxexception line 1:1 no viable alternative at character '@''> select * from system_schema.aggregates;  keyspace_name | aggregate_name | ... ---------------+----------------+ ...           test |             ag | ... drop aggregate ag; invalidrequest: code=2200 [invalid query] message='cannot drop non existing aggregate 'test.ag'' drop function id; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: failed parsing cql term: [s@foo:i@32] reason: syntaxexception line 1:1 no viable alternative at character '@''> <text> while testing edge cases around aggregates, i tried the following to see if custom types were supported: despite the error, the aggregate appears in system tables: but you can't drop it, and trying to drop its function produces the server error again: what's worse, it's now impossible to restart the server:",
        "label": 453
    },
    {
        "text": "reintroduce off heap memtables <description> cassandra-8099 removes off heap memtables. we should reintroduce them asap.<stacktrace> <code> <text> cassandra-8099 removes off heap memtables. we should reintroduce them asap.",
        "label": 67
    },
    {
        "text": "leak detected  after nodetool drain <description> 6 node cluster running 2.1.8 sequence of events: 2015-08-14 13:37:07,049 - drain the node  2015-08-14 13:37:11,943 - drained  2015-08-14 13:37:37,055 ref.java:179 - leak detected: error [reference-reaper:1] 2015-08-14 13:37:37,055 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@5534701) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@fab2c71) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-index.db was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@555d8efb) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-index.db was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@7b29bfea) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-index.db was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@2d37dc5a) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@713444527:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@13153552) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@713444527:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@25f51e35) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@713444527:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@3633d3dd) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@2ec81280) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,058 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@144d1dae) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,058 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@1944bda4) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,058 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@31c1386a) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1601396928:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-8229-index.db was not released before the reference was garbage collected see full log here:  https://dl.dropboxusercontent.com/u/4179566/cassandra-system.log<stacktrace> <code> error [reference-reaper:1] 2015-08-14 13:37:37,055 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@5534701) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@fab2c71) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-index.db was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@555d8efb) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-index.db was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@7b29bfea) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-index.db was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@2d37dc5a) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@713444527:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@13153552) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@713444527:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@25f51e35) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@713444527:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@3633d3dd) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,057 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@2ec81280) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,058 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@144d1dae) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,058 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@1944bda4) to class org.apache.cassandra.utils.concurrent.wrappedsharedcloseable$1@194296283:[[offheapbitset]] was not released before the reference was garbage collected error [reference-reaper:1] 2015-08-14 13:37:37,058 ref.java:179 - leak detected: a reference (org.apache.cassandra.utils.concurrent.ref$state@31c1386a) to class org.apache.cassandra.io.util.mmappedsegmentedfile$cleanup@1601396928:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-8229-index.db was not released before the reference was garbage collected 2015-08-14 13:37:07,049 - drain the node  2015-08-14 13:37:11,943 - drained  2015-08-14 13:37:37,055 ref.java:179 - leak detected: see full log here:  https://dl.dropboxusercontent.com/u/4179566/cassandra-system.log<text> 6 node cluster running 2.1.8 sequence of events: ",
        "label": 577
    },
    {
        "text": "test optimized primary range repair   transient replication test testtransientreplication <description> dtest failure.  example:  https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/118/workflows/9e57522d-52fa-4d44-88d8-5cec0e87f517/jobs/585/tests<stacktrace> <code> dtest failure.  example:  https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/118/workflows/9e57522d-52fa-4d44-88d8-5cec0e87f517/jobs/585/tests<text> ",
        "label": 165
    },
    {
        "text": "use map entryset and map values instead of multiple lookups from map keyset <description> in a few places in the codebase there are loops on map containers in which the values are read but the iteration is done on the keys with subsequent calls to map.get(key). using entryset/values instead of keyset should bear a little performance improvement in that kind of loops (10/30%, if i recall correctly) but mostly the patch just gets rid of a few more findbugs warnings. all tests still passing<stacktrace> <code> <text> in a few places in the codebase there are loops on map containers in which the values are read but the iteration is done on the keys with subsequent calls to map.get(key). using entryset/values instead of keyset should bear a little performance improvement in that kind of loops (10/30%, if i recall correctly) but mostly the patch just gets rid of a few more findbugs warnings. all tests still passing",
        "label": 587
    },
    {
        "text": "portability flaw  locale dependent comparison <description> overview:  in may through june of 2016 a static analysis was performed on version 3.0.5 of the cassandra source code. the analysis included an automated analysis using hp fortify v4.21 sca and a manual analysis utilizing scitools understand v4. the results of that analysis includes the issue below. issue:  in the file legacyschemamigrator.java on line 286 there is a portability problem with the call to tolowercase() because it has different locales which may lead to unexpected output. this may also circumvent custom validation routines. legacyschemamigrator.java, lines 286-289: 286 boolean issuper = \"super\".equals(tablerow.getstring(\"type\").tolowercase()); 287 boolean isdense = tablerow.has(\"is_dense\") 288                 ? tablerow.getboolean(\"is_dense\") 289                 : calculateisdense(rawcomparator, columnrows);<stacktrace> <code> legacyschemamigrator.java, lines 286-289: 286 boolean issuper = 'super'.equals(tablerow.getstring('type').tolowercase()); 287 boolean isdense = tablerow.has('is_dense') 288                 ? tablerow.getboolean('is_dense') 289                 : calculateisdense(rawcomparator, columnrows); <text> overview:  in may through june of 2016 a static analysis was performed on version 3.0.5 of the cassandra source code. the analysis included an automated analysis using hp fortify v4.21 sca and a manual analysis utilizing scitools understand v4. the results of that analysis includes the issue below. issue:  in the file legacyschemamigrator.java on line 286 there is a portability problem with the call to tolowercase() because it has different locales which may lead to unexpected output. this may also circumvent custom validation routines.",
        "label": 139
    },
    {
        "text": "compact storage value restriction message confusing <description> i have a compact storage value column name (user) the same as another column family name (user) and was getting the error restricting the value of a compact cf (user) is not supported which was very confusing changed message to restricting the value (user) of a compact cf is not supported (tackling the big problems)<stacktrace> <code> (tackling the big problems)<text> i have a compact storage value column name (user) the same as another column family name (user) and was getting the error restricting the value of a compact cf (user) is not supported which was very confusing changed message to restricting the value (user) of a compact cf is not supported ",
        "label": 274
    },
    {
        "text": "provide graphing tool along with cassandra stress <description> whilst cstar makes some pretty graphs, they're a little limited and also require you to run your tests through it. it would be useful to be able to graph results from any stress run easily.<stacktrace> <code> <text> whilst cstar makes some pretty graphs, they're a little limited and also require you to run your tests through it. it would be useful to be able to graph results from any stress run easily.",
        "label": 470
    },
    {
        "text": "doing a descending range still returns columns in ascending order <description> if i do  result = table.getslicefrom(row, \"standard1:col5\", false, 2);  cf = result.getcolumnfamily(\"standard1\"); i expect to get back columns in the order 5, 4, 3 (at the thrift level, it's turned into a list) but instead i get 3, 4, 5 because using a cf as the return vehicle re-sorts them by the standard comparator. the simplest solution is to allow user-defined column ordering as in cassandra-185 and always return columns in that order (i.e., remove the ascending bool). this also allows us to make the columngroup fetching more efficient in the best case (deserializing one column at a time instead of a group at a time). otoh using one index to allow fetching items relatively efficiently in either directly is cool. but my gut says it's relatively uncommon to want to access in both directions at once, and even more uncommon to not be able to do a reverse() on the client side (because of data volume, for instance). forcing a separate cf for this special case of a special case might be worth the tradeoff.<stacktrace> <code> result = table.getslicefrom(row, 'standard1:col5', false, 2);  cf = result.getcolumnfamily('standard1'); <text> if i do i expect to get back columns in the order 5, 4, 3 (at the thrift level, it's turned into a list) but instead i get 3, 4, 5 because using a cf as the return vehicle re-sorts them by the standard comparator. the simplest solution is to allow user-defined column ordering as in cassandra-185 and always return columns in that order (i.e., remove the ascending bool). this also allows us to make the columngroup fetching more efficient in the best case (deserializing one column at a time instead of a group at a time). otoh using one index to allow fetching items relatively efficiently in either directly is cool. but my gut says it's relatively uncommon to want to access in both directions at once, and even more uncommon to not be able to do a reverse() on the client side (because of data volume, for instance). forcing a separate cf for this special case of a special case might be worth the tradeoff.",
        "label": 285
    },
    {
        "text": "add a rate limit option to stress <description> for certain testing scenarios, you might want to only partially load a cluster. we could do this by wiring guava's ratelimiter into keys sent by stress. the operator would have to play with the limit to find the load they want, but that's unavoidable.<stacktrace> <code> <text> for certain testing scenarios, you might want to only partially load a cluster. we could do this by wiring guava's ratelimiter into keys sent by stress. the operator would have to play with the limit to find the load they want, but that's unavoidable.",
        "label": 232
    },
    {
        "text": "optimize sstables upgrade task scheduling <description> when starting the sstable-rewrite process by running nodetool upgradesstables --jobs n, with n > 1, not all of the provided n slots are used. for example, we were testing with concurrent_compactors=5 and n=4. what we observed both for version 2.2 and 3.0, is that initially all 4 provided slots are used for \"upgrade sstables\" compactions, but later when some of the 4 tasks are finished, no new tasks are scheduled immediately. it takes the last of the 4 tasks to finish before new 4 tasks would be scheduled. this happens on every node we've observed. this doesn't utilize available resources to the full extent allowed by the --jobs n parameter. in the field, on a cluster of 12 nodes with 4-5 tib data each, we've seen that the whole process was taking more than 7 days, instead of estimated 1.5-2 days (provided there would be close to full n slots utilization). instead, new tasks should be scheduled as soon as there is a free compaction slot.  additionally, starting from the biggest sstables could further reduce the total time required for the whole process to finish on any given node.<stacktrace> <code> <text> when starting the sstable-rewrite process by running nodetool upgradesstables --jobs n, with n > 1, not all of the provided n slots are used. for example, we were testing with concurrent_compactors=5 and n=4. what we observed both for version 2.2 and 3.0, is that initially all 4 provided slots are used for 'upgrade sstables' compactions, but later when some of the 4 tasks are finished, no new tasks are scheduled immediately. it takes the last of the 4 tasks to finish before new 4 tasks would be scheduled. this happens on every node we've observed. this doesn't utilize available resources to the full extent allowed by the --jobs n parameter. in the field, on a cluster of 12 nodes with 4-5 tib data each, we've seen that the whole process was taking more than 7 days, instead of estimated 1.5-2 days (provided there would be close to full n slots utilization). instead, new tasks should be scheduled as soon as there is a free compaction slot.  additionally, starting from the biggest sstables could further reduce the total time required for the whole process to finish on any given node.",
        "label": 305
    },
    {
        "text": "sstableloader should use discovered broadcast address to connect intra cluster <description> currently, in the loaderoptions for the bulkloader, the user can give a list of initial host addresses. that's to do the initial connection to the cluster but also to stream the sstables. if you have two physical interfaces, one for rpc, the other for internode traffic, then bulk loader won't currently work. it will throw an error such as: > sstableloader -v -u cassadmin -pw xxx -d 10.133.210.101,10.133.210.102,10.133.210.103,10.133.210.104 /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl  established connection to initial hosts  opening sstables and calculating sections to stream  streaming relevant part of /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-1-big-data.db /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-2-big-data.db to [/10.133.210.101, /10.133.210.103, /10.133.210.102, /10.133.210.104]  progress: total: 100% 0 mb/s(avg: 0 mb/s)error 10:16:05,311 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 streaming error occurred  java.net.connectexception: connection refused  at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:454) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:446) ~[na:1.8.0_101]  at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_101]  at java.nio.channels.socketchannel.open(socketchannel.java:189) ~[na:1.8.0_101]  at org.apache.cassandra.tools.bulkloadconnectionfactory.createconnection(bulkloadconnectionfactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_101]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_101]  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) ~[netty-all-4.0.54.final.jar:4.0.54.final]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_101]  error 10:16:05,312 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 streaming error occurred  java.net.connectexception: connection refused  at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:454) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:446) ~[na:1.8.0_101]  at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_101]  at java.nio.channels.socketchannel.open(socketchannel.java:189) ~[na:1.8.0_101]  at org.apache.cassandra.tools.bulkloadconnectionfactory.createconnection(bulkloadconnectionfactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_101]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_101]  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) ~[netty-all-4.0.54.final.jar:4.0.54.final]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_101]  error 10:16:05,312 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 streaming error occurred  java.net.connectexception: connection refused  at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:454) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:446) ~[na:1.8.0_101]  at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_101]  at java.nio.channels.socketchannel.open(socketchannel.java:189) ~[na:1.8.0_101]  at org.apache.cassandra.tools.bulkloadconnectionfactory.createconnection(bulkloadconnectionfactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_101]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_101]  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) ~[netty-all-4.0.54.final.jar:4.0.54.final]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_101]  progress: total: 100% 0 mb/s(avg: 0 mb/s)warn 10:16:05,320 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 stream failed  streaming to the following hosts failed:  [/10.133.210.101, /10.133.210.103, /10.133.210.102]  java.util.concurrent.executionexception: org.apache.cassandra.streaming.streamexception: stream failed  at com.google.common.util.concurrent.abstractfuture$sync.getvalue(abstractfuture.java:299)  at com.google.common.util.concurrent.abstractfuture$sync.get(abstractfuture.java:286)  at com.google.common.util.concurrent.abstractfuture.get(abstractfuture.java:116)  at org.apache.cassandra.tools.bulkloader.main(bulkloader.java:122)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:498)  at com.datastax.bdp.tools.shelltoolwrapper.main(shelltoolwrapper.java:34)  caused by: org.apache.cassandra.streaming.streamexception: stream failed  at org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85)  at com.google.common.util.concurrent.futures$6.run(futures.java:1310)  at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457)  at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156)  at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145)  at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202)  at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:215)  at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:191)  at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:449)  at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:549)  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:259)  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30)  at java.lang.thread.run(thread.java:745)  warn 10:16:05,322 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 stream failed  warn 10:16:05,322 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 stream failed<stacktrace> > sstableloader -v -u cassadmin -pw xxx -d 10.133.210.101,10.133.210.102,10.133.210.103,10.133.210.104 /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl  established connection to initial hosts  opening sstables and calculating sections to stream  streaming relevant part of /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-1-big-data.db /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-2-big-data.db to [/10.133.210.101, /10.133.210.103, /10.133.210.102, /10.133.210.104]  progress: total: 100% 0 mb/s(avg: 0 mb/s)error 10:16:05,311 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 streaming error occurred  java.net.connectexception: connection refused  at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:454) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:446) ~[na:1.8.0_101]  at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_101]  at java.nio.channels.socketchannel.open(socketchannel.java:189) ~[na:1.8.0_101]  at org.apache.cassandra.tools.bulkloadconnectionfactory.createconnection(bulkloadconnectionfactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_101]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_101]  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) ~[netty-all-4.0.54.final.jar:4.0.54.final]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_101]  error 10:16:05,312 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 streaming error occurred  java.net.connectexception: connection refused  at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:454) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:446) ~[na:1.8.0_101]  at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_101]  at java.nio.channels.socketchannel.open(socketchannel.java:189) ~[na:1.8.0_101]  at org.apache.cassandra.tools.bulkloadconnectionfactory.createconnection(bulkloadconnectionfactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_101]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_101]  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) ~[netty-all-4.0.54.final.jar:4.0.54.final]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_101]  error 10:16:05,312 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 streaming error occurred  java.net.connectexception: connection refused  at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:454) ~[na:1.8.0_101]  at sun.nio.ch.net.connect(net.java:446) ~[na:1.8.0_101]  at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_101]  at java.nio.channels.socketchannel.open(socketchannel.java:189) ~[na:1.8.0_101]  at org.apache.cassandra.tools.bulkloadconnectionfactory.createconnection(bulkloadconnectionfactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_101]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_101]  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30) ~[netty-all-4.0.54.final.jar:4.0.54.final]  at java.lang.thread.run(thread.java:745) ~[na:1.8.0_101]  progress: total: 100% 0 mb/s(avg: 0 mb/s)warn 10:16:05,320 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 stream failed  streaming to the following hosts failed:  [/10.133.210.101, /10.133.210.103, /10.133.210.102]  java.util.concurrent.executionexception: org.apache.cassandra.streaming.streamexception: stream failed  at com.google.common.util.concurrent.abstractfuture$sync.getvalue(abstractfuture.java:299)  at com.google.common.util.concurrent.abstractfuture$sync.get(abstractfuture.java:286)  at com.google.common.util.concurrent.abstractfuture.get(abstractfuture.java:116)  at org.apache.cassandra.tools.bulkloader.main(bulkloader.java:122)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:498)  at com.datastax.bdp.tools.shelltoolwrapper.main(shelltoolwrapper.java:34)  caused by: org.apache.cassandra.streaming.streamexception: stream failed  at org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85)  at com.google.common.util.concurrent.futures$6.run(futures.java:1310)  at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457)  at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156)  at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145)  at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202)  at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:215)  at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:191)  at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:449)  at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:549)  at org.apache.cassandra.streaming.streamsession.start(streamsession.java:259)  at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:212)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)  at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)  at io.netty.util.concurrent.fastthreadlocalrunnable.run(fastthreadlocalrunnable.java:30)  at java.lang.thread.run(thread.java:745)  warn 10:16:05,322 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 stream failed  warn 10:16:05,322 stream #9ed00130-6ff6-11e8-965c-93a78bf96e60 stream failed<code> <text> currently, in the loaderoptions for the bulkloader, the user can give a list of initial host addresses. that's to do the initial connection to the cluster but also to stream the sstables. if you have two physical interfaces, one for rpc, the other for internode traffic, then bulk loader won't currently work. it will throw an error such as: ",
        "label": 245
    },
    {
        "text": "repair of the systemdistributed keyspace creates a non trivial amount of memory pressure <description> when a repair without any particular option is triggered, the systemdistributed keyspace is repaired for all range, and in particular the repair_history table. for every range, that table is written and flushed (as part of normal repair), meaning that every range triggers the creation of a new 1mb slab region (this also triggers quite a few compactions that also write and flush compaction_progress at every start and end). i don't know how much of a big deal this will be in practice, but i wonder if it's really useful to repair the repair_* tables by default so maybe we could exclude the systemdistributed keyspace from default repairs and only repair it if asked explicitly?<stacktrace> <code> <text> when a repair without any particular option is triggered, the systemdistributed keyspace is repaired for all range, and in particular the repair_history table. for every range, that table is written and flushed (as part of normal repair), meaning that every range triggers the creation of a new 1mb slab region (this also triggers quite a few compactions that also write and flush compaction_progress at every start and end). i don't know how much of a big deal this will be in practice, but i wonder if it's really useful to repair the repair_* tables by default so maybe we could exclude the systemdistributed keyspace from default repairs and only repair it if asked explicitly?",
        "label": 321
    },
    {
        "text": "improve serialization in the native protocol <description> message serialization in the native protocol currently make a netty's channelbuffers.wrappedbuffer(). the rational was to avoid copying of the values bytes when such value are biggish. this has a cost however, especially with lots of small values, and as suggested in cassandra-5422, this might well be a more common scenario for cassandra, so let's consider directly serializing in a newly allocated buffer.<stacktrace> <code> <text> message serialization in the native protocol currently make a netty's channelbuffers.wrappedbuffer(). the rational was to avoid copying of the values bytes when such value are biggish. this has a cost however, especially with lots of small values, and as suggested in cassandra-5422, this might well be a more common scenario for cassandra, so let's consider directly serializing in a newly allocated buffer.",
        "label": 520
    },
    {
        "text": "remove deprecated repair method in <description> once we hit 4.0, we should remove all deprecated repair jmx api.  (nodetool has been using only new api since it is introduced.)<stacktrace> <code> once we hit 4.0, we should remove all deprecated repair jmx api.  (nodetool has been using only new api since it is introduced.)<text> ",
        "label": 577
    },
    {
        "text": "improve the ability to run the cassandradaemon as a managed service <description> see a transcript of the discussion here: http://www.mail-archive.com/dev@cassandra.apache.org/msg07529.html<stacktrace> <code> http://www.mail-archive.com/dev@cassandra.apache.org/msg07529.html<text> see a transcript of the discussion here: ",
        "label": 204
    },
    {
        "text": "can't rebuild sasi index <description> there's been no real requirement for that so far. as beobal has pointed out, it's not a big issue, since that only could be needed when index files are lost, data corruption on disk (hardware issue) has occurred or there was a bug that'd require an index rebuild. during rebuild_index task, indexes are only \"marked\" as removed with secondaryindexmanager::markindexremoved and then buildindexesblocking is called. however, since sasi keeps track of sstables for the index, it's going to filter them out with .filter((sstable) -> !sasi.index.hassstable(sstable)) in sasiindexbuildingsupport. if i understand the logic correctly, we have to \"invalidate\" (drop data) right before we re-index them. this is also a blocker for cassandra-11990 since without it we can't have an upgrade path. i have a patch ready in branch, but since it's a bug, it's better to have it released earlier and for all branches affected. cc pavel yaskevich<stacktrace> <code> <text> there's been no real requirement for that so far. as beobal has pointed out, it's not a big issue, since that only could be needed when index files are lost, data corruption on disk (hardware issue) has occurred or there was a bug that'd require an index rebuild. during rebuild_index task, indexes are only 'marked' as removed with secondaryindexmanager::markindexremoved and then buildindexesblocking is called. however, since sasi keeps track of sstables for the index, it's going to filter them out with .filter((sstable) -> !sasi.index.hassstable(sstable)) in sasiindexbuildingsupport. if i understand the logic correctly, we have to 'invalidate' (drop data) right before we re-index them. this is also a blocker for cassandra-11990 since without it we can't have an upgrade path. i have a patch ready in branch, but since it's a bug, it's better to have it released earlier and for all branches affected. cc pavel yaskevich",
        "label": 25
    },
    {
        "text": "rowindexentry deletiontime raises unsupportedoperationexception when upgrading to <description> upgrading from 1.2.5 to 1.2.7 immediately caused the following exception. we stopped the node and reverted to 1.2.5. i'll note that we also run a 1.2.6 cluster that is doing fine, so i think this is caused by a change in 1.2.7. error [mutationstage:59] 2013-07-26 19:35:10,460 cassandradaemon.java (line 192) exception in thread thread[mutationstage:59,5,main] java.lang.exceptionininitializererror at org.apache.cassandra.utils.counterid.localids(counterid.java:49) at org.apache.cassandra.utils.counterid.getlocalid(counterid.java:54) at org.apache.cassandra.db.context.countercontext.create(countercontext.java:105) at org.apache.cassandra.db.counterupdatecolumn.localcopy(counterupdatecolumn.java:84) at org.apache.cassandra.db.counterupdatecolumn.localcopy(counterupdatecolumn.java:34) at org.apache.cassandra.db.countermutation.apply(countermutation.java:133) at org.apache.cassandra.service.storageproxy$7.runmaythrow(storageproxy.java:758) at org.apache.cassandra.service.storageproxy$localmutationrunnable.run(storageproxy.java:1630) at org.apache.cassandra.service.storageproxy$3.apply(storageproxy.java:142) at org.apache.cassandra.service.storageproxy.performwrite(storageproxy.java:385) at org.apache.cassandra.service.storageproxy.applycountermutationonleader(storageproxy.java:733) at org.apache.cassandra.db.countermutationverbhandler.doverb(countermutationverbhandler.java:53) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:56) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) caused by: java.lang.unsupportedoperationexception at org.apache.cassandra.db.rowindexentry.deletiontime(rowindexentry.java:81) at org.apache.cassandra.db.columniterator.indexedslicereader.<init>(indexedslicereader.java:109) at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:68) at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:44) at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:101) at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:68) at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:272) at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:65) at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1390) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1213) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1125) at org.apache.cassandra.db.systemtable.getcurrentlocalcounterid(systemtable.java:595) at org.apache.cassandra.utils.counterid$localcounteridhistory.<init>(counterid.java:194) at org.apache.cassandra.utils.counterid$localids.<clinit>(counterid.java:42) ... 16 more the code causing it seems to have changed in 1.2.7 via https://issues.apache.org/jira/browse/cassandra-5677<stacktrace> error [mutationstage:59] 2013-07-26 19:35:10,460 cassandradaemon.java (line 192) exception in thread thread[mutationstage:59,5,main] java.lang.exceptionininitializererror at org.apache.cassandra.utils.counterid.localids(counterid.java:49) at org.apache.cassandra.utils.counterid.getlocalid(counterid.java:54) at org.apache.cassandra.db.context.countercontext.create(countercontext.java:105) at org.apache.cassandra.db.counterupdatecolumn.localcopy(counterupdatecolumn.java:84) at org.apache.cassandra.db.counterupdatecolumn.localcopy(counterupdatecolumn.java:34) at org.apache.cassandra.db.countermutation.apply(countermutation.java:133) at org.apache.cassandra.service.storageproxy$7.runmaythrow(storageproxy.java:758) at org.apache.cassandra.service.storageproxy$localmutationrunnable.run(storageproxy.java:1630) at org.apache.cassandra.service.storageproxy$3.apply(storageproxy.java:142) at org.apache.cassandra.service.storageproxy.performwrite(storageproxy.java:385) at org.apache.cassandra.service.storageproxy.applycountermutationonleader(storageproxy.java:733) at org.apache.cassandra.db.countermutationverbhandler.doverb(countermutationverbhandler.java:53) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:56) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) caused by: java.lang.unsupportedoperationexception at org.apache.cassandra.db.rowindexentry.deletiontime(rowindexentry.java:81) at org.apache.cassandra.db.columniterator.indexedslicereader.<init>(indexedslicereader.java:109) at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:68) at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:44) at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:101) at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:68) at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:272) at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:65) at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1390) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1213) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1125) at org.apache.cassandra.db.systemtable.getcurrentlocalcounterid(systemtable.java:595) at org.apache.cassandra.utils.counterid$localcounteridhistory.<init>(counterid.java:194) at org.apache.cassandra.utils.counterid$localids.<clinit>(counterid.java:42) ... 16 more <code> <text> upgrading from 1.2.5 to 1.2.7 immediately caused the following exception. we stopped the node and reverted to 1.2.5. i'll note that we also run a 1.2.6 cluster that is doing fine, so i think this is caused by a change in 1.2.7. the code causing it seems to have changed in 1.2.7 via https://issues.apache.org/jira/browse/cassandra-5677",
        "label": 274
    },
    {
        "text": "make compression ratio much more accurate <description> currently in cfstats, it will take an average over the compression ratios of all of the sstables without regard to the data sizes. this can lead to a very inaccurate value. it would be good to factor in the uncompressed and compressed sizes for the sstables to give an accurate number.<stacktrace> <code> <text> currently in cfstats, it will take an average over the compression ratios of all of the sstables without regard to the data sizes. this can lead to a very inaccurate value. it would be good to factor in the uncompressed and compressed sizes for the sstables to give an accurate number.",
        "label": 88
    },
    {
        "text": "tuple columns with udts not updated when the udt is altered <description> when a tuple column contains a udt and the udt is altered, we do not update the tuple column: cqlsh> create keyspace ks1 with replication = {'class': 'simplestrategy', 'replication_factor': '1' }; cqlsh> create type ks1.foo (a int, b int); cqlsh> create table ks1.mytable (a int primary key, b frozen<tuple<int, ks1.foo>>); cqlsh> insert into ks1.mytable (a, b) values (1, (1, {a: 1, b: 1})); cqlsh> alter type ks1.foo add c int; cqlsh> insert into ks1.mytable (a, b) values (1, (1, {a: 1, b: 1})); cqlsh> insert into ks1.mytable (a, b) values (1, (1, {a: 1, b: 1, c: 1})); invalidrequest: code=2200 [invalid query] message=\"unknown field 'c' in value of user defined type foo\"<stacktrace> <code> cqlsh> create keyspace ks1 with replication = {'class': 'simplestrategy', 'replication_factor': '1' }; cqlsh> create type ks1.foo (a int, b int); cqlsh> create table ks1.mytable (a int primary key, b frozen<tuple<int, ks1.foo>>); cqlsh> insert into ks1.mytable (a, b) values (1, (1, {a: 1, b: 1})); cqlsh> alter type ks1.foo add c int; cqlsh> insert into ks1.mytable (a, b) values (1, (1, {a: 1, b: 1})); cqlsh> insert into ks1.mytable (a, b) values (1, (1, {a: 1, b: 1, c: 1})); invalidrequest: code=2200 [invalid query] message='unknown field 'c' in value of user defined type foo' <text> when a tuple column contains a udt and the udt is altered, we do not update the tuple column:",
        "label": 69
    },
    {
        "text": "replace address can  succeed  without actually streaming anything <description> when you do a replace address and the new node has the same ip as the node it is replacing, then the following check can let the replace be successful even if we think all the other nodes are down: https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/dht/rangestreamer.java#l271 as the failuredetectorsourcefilter will exclude the other nodes, so an empty stream plan gets executed.<stacktrace> <code> when you do a replace address and the new node has the same ip as the node it is replacing, then the following check can let the replace be successful even if we think all the other nodes are down: https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/dht/rangestreamer.java#l271 <text> as the failuredetectorsourcefilter will exclude the other nodes, so an empty stream plan gets executed.",
        "label": 409
    },
    {
        "text": "histograms metrics in do not appear recency biased <description> in addition to upgrading to metrics3, cassandra-5657 switched to using a custom histogram implementation. after upgrading to cassandra 2.2 histograms/timer metrics are not suspiciously flat. to be useful for graphing and alerting metrics need to be biased towards recent events. i have attached images that i think illustrate this. the first two are a comparison between latency observed by a c* 2.2 (us) cluster shoring very flat lines and a client (using metrics 2.2.0, ms) showing server performance problems. we can't rule out with total certainty that something else isn't the cause (that's why we measure from both the client & server) but they very rarely disagree. the 3rd image compares jconsole viewing of metrics on a 2.2 and 2.1 cluster over several minutes. not a single digit changed on the 2.2 cluster.<stacktrace> <code> <text> in addition to upgrading to metrics3, cassandra-5657 switched to using a custom histogram implementation. after upgrading to cassandra 2.2 histograms/timer metrics are not suspiciously flat. to be useful for graphing and alerting metrics need to be biased towards recent events. i have attached images that i think illustrate this.",
        "label": 419
    },
    {
        "text": "npe on startup if another cassandra instance is already running <description> after cassandra-7087, if you try to start cassandra while another instance is already running, you'll see something like this: $ bin/cassandra -f error: exception thrown by the agent : java.lang.nullpointerexception this is probably a jvm bug, but we should confirm that, open a jvm ticket, and see if we can give a more useful error message on the c* side.<stacktrace> <code> $ bin/cassandra -f error: exception thrown by the agent : java.lang.nullpointerexception <text> after cassandra-7087, if you try to start cassandra while another instance is already running, you'll see something like this: this is probably a jvm bug, but we should confirm that, open a jvm ticket, and see if we can give a more useful error message on the c* side.",
        "label": 85
    },
    {
        "text": "cql queries using limit sometimes missing results <description> in certain conditions, cql queries using limit clauses are not being given all of the expected results (whether unset column values or missing rows). here are the condition sets i've been able to identify: first mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. conditions: table has a multi-component primary key table has more than one column which is not a component of the primary key the number of results which would be returned by a query is equal to or more than the specified limit second mode: result has fewer rows than it should, lower than both the limit and the actual number of matching rows. conditions: table has a single-column primary key table has more than one column which is not a component of the primary key the number of results which would be returned by a query is equal to or more than the specified limit it would make sense to me that this would have started with cassandra-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing cassandra-3647. test case for the first failure mode: drop keyspace test; create keyspace test     with strategy_class = 'simplestrategy'     and strategy_options:replication_factor = 1; use test; create table testcf (     a int,     b int,     c int,     d int,     e int,     primary key (a, b) ); insert into testcf (a, b, c, d, e) values (1, 11, 111, 1111, 11111); insert into testcf (a, b, c, d, e) values (2, 22, 222, 2222, 22222); insert into testcf (a, b, c, d, e) values (3, 33, 333, 3333, 33333); insert into testcf (a, b, c, d, e) values (4, 44, 444, 4444, 44444); select * from testcf; select * from testcf limit 1; -- columns d and e in result row are null select * from testcf limit 2; -- columns d and e in last result row are null select * from testcf limit 3; -- columns d and e in last result row are null select * from testcf limit 4; -- columns d and e in last result row are null select * from testcf limit 5; -- results are correct (4 rows returned) test case for the second failure mode: create keyspace test     with strategy_class = 'simplestrategy'     and strategy_options:replication_factor = 1; use test; create table testcf (     a int primary key,     b int,     c int, ); insert into testcf (a, b, c) values (1, 11, 111); insert into testcf (a, b, c) values (2, 22, 222); insert into testcf (a, b, c) values (3, 33, 333); insert into testcf (a, b, c) values (4, 44, 444); select * from testcf; select * from testcf limit 1; -- gives 1 row select * from testcf limit 2; -- gives 1 row select * from testcf limit 3; -- gives 2 rows select * from testcf limit 4; -- gives 2 rows select * from testcf limit 5; -- gives 3 rows<stacktrace> <code> drop keyspace test; create keyspace test     with strategy_class = 'simplestrategy'     and strategy_options:replication_factor = 1; use test; create table testcf (     a int,     b int,     c int,     d int,     e int,     primary key (a, b) ); insert into testcf (a, b, c, d, e) values (1, 11, 111, 1111, 11111); insert into testcf (a, b, c, d, e) values (2, 22, 222, 2222, 22222); insert into testcf (a, b, c, d, e) values (3, 33, 333, 3333, 33333); insert into testcf (a, b, c, d, e) values (4, 44, 444, 4444, 44444); select * from testcf; select * from testcf limit 1; -- columns d and e in result row are null select * from testcf limit 2; -- columns d and e in last result row are null select * from testcf limit 3; -- columns d and e in last result row are null select * from testcf limit 4; -- columns d and e in last result row are null select * from testcf limit 5; -- results are correct (4 rows returned) create keyspace test     with strategy_class = 'simplestrategy'     and strategy_options:replication_factor = 1; use test; create table testcf (     a int primary key,     b int,     c int, ); insert into testcf (a, b, c) values (1, 11, 111); insert into testcf (a, b, c) values (2, 22, 222); insert into testcf (a, b, c) values (3, 33, 333); insert into testcf (a, b, c) values (4, 44, 444); select * from testcf; select * from testcf limit 1; -- gives 1 row select * from testcf limit 2; -- gives 1 row select * from testcf limit 3; -- gives 2 rows select * from testcf limit 4; -- gives 2 rows select * from testcf limit 5; -- gives 3 rows <text> in certain conditions, cql queries using limit clauses are not being given all of the expected results (whether unset column values or missing rows). here are the condition sets i've been able to identify: first mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. conditions: second mode: result has fewer rows than it should, lower than both the limit and the actual number of matching rows. conditions: it would make sense to me that this would have started with cassandra-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing cassandra-3647. test case for the first failure mode: test case for the second failure mode:",
        "label": 520
    },
    {
        "text": "cassandra cli command should have  version option <description> implementing \"cassandra-cli --version\" command line option would make it easier to write bug reports and check the versions of tools in use. or if you want to make it a cli command inside the tool, i don't know. --version option seems to be the standard way.<stacktrace> <code> <text> implementing 'cassandra-cli --version' command line option would make it easier to write bug reports and check the versions of tools in use. or if you want to make it a cli command inside the tool, i don't know. --version option seems to be the standard way.",
        "label": 412
    },
    {
        "text": "record token endpoint  ip  pairs in system table <description> each node records its own token after generating one or reading it from the config file. for more redundancy we should record others' tokens too. this should not be regarded as canonical information \u2013 do not use it to take shortcuts instead of using gossip. it is only to provide redundancy in case of failure. for instance, if your cluster loses power and when it comes back up, the disk w/ token information on one of your machines is dead, this will let you get that token information easily from another node instead of having to guess by scanning sstables. this is not a theoretical scenario; something of this nature did happen to fb.<stacktrace> <code> <text> each node records its own token after generating one or reading it from the config file. for more redundancy we should record others' tokens too. this should not be regarded as canonical information - do not use it to take shortcuts instead of using gossip. it is only to provide redundancy in case of failure. for instance, if your cluster loses power and when it comes back up, the disk w/ token information on one of your machines is dead, this will let you get that token information easily from another node instead of having to guess by scanning sstables. this is not a theoretical scenario; something of this nature did happen to fb.",
        "label": 186
    },
    {
        "text": "cassandra does not start with new systemd version <description> after update systemd with  fixed vulnerability https://access.redhat.com/security/cve/cve-2018-16888, the cassandra service does not start correctly. environment: rhel 7, systemd-219-67.el7_7.1, cassandra-3.11.4-1 (https://www.apache.org/dist/cassandra/redhat/311x/cassandra-3.11.4-1.noarch.rpm) --------------------------------------------------------------- systemctl status cassandra  \u25cf cassandra.service - lsb: distributed storage system for structured data  loaded: loaded (/etc/rc.d/init.d/cassandra; bad; vendor preset: disabled)  active: failed (result: resources) since fri 2019-08-09 17:20:26 msk; 1s ago  docs: man:systemd-sysv-generator(8)  process: 2414 execstop=/etc/rc.d/init.d/cassandra stop (code=exited, status=0/success)  process: 2463 execstart=/etc/rc.d/init.d/cassandra start (code=exited, status=0/success)  main pid: 1884 (code=exited, status=143) aug 09 17:20:23 desktop43.example.com systemd[1]: unit cassandra.service entered failed state.  aug 09 17:20:23 desktop43.example.com systemd[1]: cassandra.service failed.  aug 09 17:20:23 desktop43.example.com systemd[1]: starting lsb: distributed storage system for structured data...  aug 09 17:20:23 desktop43.example.com su[2473]: (to cassandra) root on none  aug 09 17:20:26 desktop43.example.com cassandra[2463]: starting cassandra: ok  aug 09 17:20:26 desktop43.example.com systemd[1]: new main pid 2545 does not belong to service, and pid file is not owned by root. refusing.  aug 09 17:20:26 desktop43.example.com systemd[1]: new main pid 2545 does not belong to service, and pid file is not owned by root. refusing.  aug 09 17:20:26 desktop43.example.com systemd[1]: failed to start lsb: distributed storage system for structured data.  aug 09 17:20:26 desktop43.example.com systemd[1]: unit cassandra.service entered failed state.  aug 09 17:20:26 desktop43.example.com systemd[1]: cassandra.service failed.<stacktrace> <code> environment: rhel 7, systemd-219-67.el7_7.1, cassandra-3.11.4-1 (https://www.apache.org/dist/cassandra/redhat/311x/cassandra-3.11.4-1.noarch.rpm) --------------------------------------------------------------- systemctl status cassandra   cassandra.service - lsb: distributed storage system for structured data  loaded: loaded (/etc/rc.d/init.d/cassandra; bad; vendor preset: disabled)  active: failed (result: resources) since fri 2019-08-09 17:20:26 msk; 1s ago  docs: man:systemd-sysv-generator(8)  process: 2414 execstop=/etc/rc.d/init.d/cassandra stop (code=exited, status=0/success)  process: 2463 execstart=/etc/rc.d/init.d/cassandra start (code=exited, status=0/success)  main pid: 1884 (code=exited, status=143) <text> after update systemd with  fixed vulnerability https://access.redhat.com/security/cve/cve-2018-16888, the cassandra service does not start correctly. aug 09 17:20:23 desktop43.example.com systemd[1]: unit cassandra.service entered failed state.  aug 09 17:20:23 desktop43.example.com systemd[1]: cassandra.service failed.  aug 09 17:20:23 desktop43.example.com systemd[1]: starting lsb: distributed storage system for structured data...  aug 09 17:20:23 desktop43.example.com su[2473]: (to cassandra) root on none  aug 09 17:20:26 desktop43.example.com cassandra[2463]: starting cassandra: ok  aug 09 17:20:26 desktop43.example.com systemd[1]: new main pid 2545 does not belong to service, and pid file is not owned by root. refusing.  aug 09 17:20:26 desktop43.example.com systemd[1]: new main pid 2545 does not belong to service, and pid file is not owned by root. refusing.  aug 09 17:20:26 desktop43.example.com systemd[1]: failed to start lsb: distributed storage system for structured data.  aug 09 17:20:26 desktop43.example.com systemd[1]: unit cassandra.service entered failed state.  aug 09 17:20:26 desktop43.example.com systemd[1]: cassandra.service failed.",
        "label": 347
    },
    {
        "text": "decommissioned nodes can remain in gossip <description> this may apply to other dead states as well. dead states should be expired after 3 days. in the case of decom we attach a timestamp to let the other nodes know when it should be expired. it has been observed that sometimes a subset of nodes in the cluster never expire the state, and through heap analysis of these nodes it is revealed that the epstate.isalive check returns true when it should return false, which would allow the state to be evicted. this may have been affected by cassandra-8336.<stacktrace> <code> <text> this may apply to other dead states as well. dead states should be expired after 3 days. in the case of decom we attach a timestamp to let the other nodes know when it should be expired. it has been observed that sometimes a subset of nodes in the cluster never expire the state, and through heap analysis of these nodes it is revealed that the epstate.isalive check returns true when it should return false, which would allow the state to be evicted. this may have been affected by cassandra-8336.",
        "label": 261
    },
    {
        "text": "improve tombstone compactions <description> when there are no other compactions to do, we trigger a single-sstable compaction if there is more than x% droppable tombstones in the sstable. in this ticket we should try to include overlapping sstables in those compactions to be able to actually drop the tombstones. might only be doable with lcs (with stcs we would probably end up including all sstables)<stacktrace> <code> <text> when there are no other compactions to do, we trigger a single-sstable compaction if there is more than x% droppable tombstones in the sstable. in this ticket we should try to include overlapping sstables in those compactions to be able to actually drop the tombstones. might only be doable with lcs (with stcs we would probably end up including all sstables)",
        "label": 86
    },
    {
        "text": "pig tests broken <description> not sure what happened here, but i get a smorgasbord of errors running the pig tests now, from xml errors in xerces to notfoundexceptions.<stacktrace> <code> <text> not sure what happened here, but i get a smorgasbord of errors running the pig tests now, from xml errors in xerces to notfoundexceptions.",
        "label": 474
    },
    {
        "text": "dtest failure in thrift tests testmutations test dynamic indexes with system update cf <description> this has started pretty consistently failing on 2.2+ on ci. i am not reproducing it locally though, so it is flaky. the failures all related to failed queries, whether they don't return data when they're expected to, or they time out. example failure: http://cassci.datastax.com/job/cassandra-3.0_dtest/710/testreport/thrift_tests/testmutations/test_dynamic_indexes_with_system_update_cf failed on cassci build cassandra-3.0_dtest #710 i expect this may be a bug, but someone should go in and bisect, or get slightly more useful debugging data.<stacktrace> <code> http://cassci.datastax.com/job/cassandra-3.0_dtest/710/testreport/thrift_tests/testmutations/test_dynamic_indexes_with_system_update_cf failed on cassci build cassandra-3.0_dtest #710 <text> this has started pretty consistently failing on 2.2+ on ci. i am not reproducing it locally though, so it is flaky. the failures all related to failed queries, whether they don't return data when they're expected to, or they time out. example failure: i expect this may be a bug, but someone should go in and bisect, or get slightly more useful debugging data.",
        "label": 487
    },
    {
        "text": "add describe ring to cli <description> lately i have found the describe_ring feature was needed to debug/analyze issue, but the cli does not have this available. so just in case it is useful, please see the attached patch. here is the sample output: [default@unknown] help; ... ... decr                    decrements a counter column. describe ring           describe the token range information. describe cluster        describe the cluster configuration. ... ... [default@unknown] help describe ring; describe ring <keyspace>; describes the token range settings for the named keyspace. required parameters: - keyspace: name of the keyspace to describe the token range. examples: describe ring <keyspace>; - describes the token range settings for the named keyspace. [default@unknown] describe ring keyspace3; tokenrange:          tokenrange(start_token:9739248273232290250409572410247679660, end_token:9739248273232290250409572410247679660, endpoints:[192.168.0.125], rpc_endpoints:[192.168.0.125], endpoint_details:[endpointdetails(host:192.168.0.125, port:9160, datacenter:168)]) [default@unknown] describe ring fooks; keyspace with name 'fooks' wasn't found, , please, authorize to one of the keyspaces first. [default@unknown] describe ring; syntax error at position 13: mismatched input ';' expecting set null <stacktrace> <code> [default@unknown] help; ... ... decr                    decrements a counter column. describe ring           describe the token range information. describe cluster        describe the cluster configuration. ... ... [default@unknown] help describe ring; describe ring <keyspace>; describes the token range settings for the named keyspace. required parameters: - keyspace: name of the keyspace to describe the token range. examples: describe ring <keyspace>; - describes the token range settings for the named keyspace. [default@unknown] describe ring keyspace3; tokenrange:          tokenrange(start_token:9739248273232290250409572410247679660, end_token:9739248273232290250409572410247679660, endpoints:[192.168.0.125], rpc_endpoints:[192.168.0.125], endpoint_details:[endpointdetails(host:192.168.0.125, port:9160, datacenter:168)]) [default@unknown] describe ring fooks; keyspace with name 'fooks' wasn't found, , please, authorize to one of the keyspaces first. [default@unknown] describe ring; syntax error at position 13: mismatched input ';' expecting set null <text> lately i have found the describe_ring feature was needed to debug/analyze issue, but the cli does not have this available. so just in case it is useful, please see the attached patch. here is the sample output:",
        "label": 221
    },
    {
        "text": "consider storing more informations on peers in system tables <description> currently, the only thing we keep in system tables about other peers is their token and ip addresses. we should probably also record the new ring_id, but since cassandra-4018 makes system table easily queriable, may it could be worth adding some more information (basically most of what we gossip could be a candidate (schema uuid, status, c* version, ...)) as a simple way to expose the ring state to users (even if it's just a \"view\" of the ring state from one specific node i believe it's still nice). of course that means storing information that may not be absolutely needed by the server, but i'm not sure there is much harm to that. note that doing this cleanly may require changing the schema of current system tables but as long as we do that in the 1.2 timeframe it's ok (since the concerned system table 'local' and 'peers' are news anyway).<stacktrace> <code> <text> currently, the only thing we keep in system tables about other peers is their token and ip addresses. we should probably also record the new ring_id, but since cassandra-4018 makes system table easily queriable, may it could be worth adding some more information (basically most of what we gossip could be a candidate (schema uuid, status, c* version, ...)) as a simple way to expose the ring state to users (even if it's just a 'view' of the ring state from one specific node i believe it's still nice). of course that means storing information that may not be absolutely needed by the server, but i'm not sure there is much harm to that. note that doing this cleanly may require changing the schema of current system tables but as long as we do that in the 1.2 timeframe it's ok (since the concerned system table 'local' and 'peers' are news anyway).",
        "label": 520
    },
    {
        "text": "push read repair setting down to the dc level <description> currently, read repair is a global setting. however, when you have two dcs and use one for analytics, it would be nice to turn it off only for that dc so the live dc serving the application can still benefit from it.<stacktrace> <code> <text> currently, read repair is a global setting. however, when you have two dcs and use one for analytics, it would be nice to turn it off only for that dc so the live dc serving the application can still benefit from it.",
        "label": 555
    },
    {
        "text": "cql support for jdbc databasemetadata <description> in order to increase the drop-in capability of cql to existing jdbc app bases, cql must be updated to include at least semi-valid responses to the jdbc metadata portion. without enhancement: >>> com.largecompany.jdbcmanager.getconnection(\"<vague cassandra jndi pointer>\")  resource has error: java.lang.unsupportedoperationexception: method not supported  ... with enhancement:  >>> com.largecompany.jdbcmanager.getconnection(\"<vague cassandra jndi pointer>\")  org.apache.cassandra.cql.jdbc.cassandraconnection@1915470e<stacktrace> <code> with enhancement:  >>> com.largecompany.jdbcmanager.getconnection('<vague cassandra jndi pointer>')  org.apache.cassandra.cql.jdbc.cassandraconnection@1915470e<text> in order to increase the drop-in capability of cql to existing jdbc app bases, cql must be updated to include at least semi-valid responses to the jdbc metadata portion. without enhancement: >>> com.largecompany.jdbcmanager.getconnection('<vague cassandra jndi pointer>')  resource has error: java.lang.unsupportedoperationexception: method not supported  ... ",
        "label": 449
    },
    {
        "text": "compactions don't work while node is bootstrapping <description> it seems that there is a race condition in storageservice that prevents compactions from completing while node is in a bootstrap state. i have been able to reproduce this multiple times by throttling streaming throughput to extend the bootstrap time while simultaneously inserting data to the cluster. the problems lies in the synchronization of initserver(int delay) and reportseverity(double incr) methods as they both try to acquire the instance lock of storageservice through the use of synchronized keyword. as initserver does not return until the bootstrap has completed, all calls to reportseverity will block until that. however, reportseverity is called when starting compactions in compactioninfo and thus all compactions block until bootstrap completes. this might severely degrade node's performance after bootstrap as it might have lots of compactions pending while simultaneously starting to serve reads. i have been able to solve the issue by adding a separate lock for reportseverity and removing its class level synchronization. this of course is not a valid approach if we must assume that any of gossiper's iendpointstatechangesubscribers could potentially end up calling back to storageservice's synchronized methods. however, at least at the moment, that does not seem to be the case. maybe somebody with more experience about the codebase comes up with a better solution? (this might affect dynamicendpointsnitch as well, as it also calls to reportseverity in its setseverity method)<stacktrace> <code> <text> it seems that there is a race condition in storageservice that prevents compactions from completing while node is in a bootstrap state. i have been able to reproduce this multiple times by throttling streaming throughput to extend the bootstrap time while simultaneously inserting data to the cluster. the problems lies in the synchronization of initserver(int delay) and reportseverity(double incr) methods as they both try to acquire the instance lock of storageservice through the use of synchronized keyword. as initserver does not return until the bootstrap has completed, all calls to reportseverity will block until that. however, reportseverity is called when starting compactions in compactioninfo and thus all compactions block until bootstrap completes. this might severely degrade node's performance after bootstrap as it might have lots of compactions pending while simultaneously starting to serve reads. i have been able to solve the issue by adding a separate lock for reportseverity and removing its class level synchronization. this of course is not a valid approach if we must assume that any of gossiper's iendpointstatechangesubscribers could potentially end up calling back to storageservice's synchronized methods. however, at least at the moment, that does not seem to be the case. maybe somebody with more experience about the codebase comes up with a better solution? (this might affect dynamicendpointsnitch as well, as it also calls to reportseverity in its setseverity method)",
        "label": 85
    },
    {
        "text": "compactionmanager improvements <description> we should expose a count of compactions finished the way we did in 0.5 (when we used a simpler executor model in cm). it looks like we could still subclass jmxenabledtpe and get this for almost free. also: we should expose cm in tpstats again.<stacktrace> <code> <text> we should expose a count of compactions finished the way we did in 0.5 (when we used a simpler executor model in cm). it looks like we could still subclass jmxenabledtpe and get this for almost free. also: we should expose cm in tpstats again.",
        "label": 274
    },
    {
        "text": "select keyspace name from system schema keyspaces <description> it is currently possible to \"describe tables\" to list the tables in the current keyspace, or list all tables in all keyspaces if you are not currently in a keyspace. it is also possible to enumerate the keyspaces with a cql command to select from the system.schema_columnfamilies. there should be a simple \"describe keyspaces\" command that enumerates just the keyspaces and is syntactic sugar for \"select keyspace name from schema_keyspaces\".<stacktrace> <code> <text> it is currently possible to 'describe tables' to list the tables in the current keyspace, or list all tables in all keyspaces if you are not currently in a keyspace. it is also possible to enumerate the keyspaces with a cql command to select from the system.schema_columnfamilies. there should be a simple 'describe keyspaces' command that enumerates just the keyspaces and is syntactic sugar for 'select keyspace name from schema_keyspaces'.",
        "label": 18
    },
    {
        "text": "expose dynamic secondary indexes via system update column family <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "cassandra bat fails when cassandra home contains a whitespace  again <description> i installed cassandra into c:\\program files\\apache-cassandra and tried to start cassandra. but cassandra.bat fails with following error.   c:\\program files\\apache-cassandra>bin\\cassandra.bat starting cassandra server error opening zip file or jar manifest missing : c:\\program error occurred during initialization of vm agent library failed to init: instrument this problem is similar to cassandra-2237. i'll post a patch to fix the problem later.<stacktrace> <code>   c:/program files/apache-cassandra>bin/cassandra.bat starting cassandra server error opening zip file or jar manifest missing : c:/program error occurred during initialization of vm agent library failed to init: instrument <text> i installed cassandra into c:/program files/apache-cassandra and tried to start cassandra. but cassandra.bat fails with following error. this problem is similar to cassandra-2237. i'll post a patch to fix the problem later.",
        "label": 304
    },
    {
        "text": "leveledcompactionstrategytest testcompactionrace and testmutatelevel sometimes fail <description> testcompactionrace  http://cassci.datastax.com/view/trunk/job/trunk_testall/136/testreport/junit/org.apache.cassandra.db.compaction/leveledcompactionstrategytest/testcompactionprogress/ testmutatelevel (only seen on my desktop)     [junit] testcase: testmutatelevel(org.apache.cassandra.db.compaction.leveledcompactionstrategytest):        failed     [junit] expected:<6> but was:<0>     [junit] junit.framework.assertionfailederror: expected:<6> but was:<0>     [junit]     at org.apache.cassandra.db.compaction.leveledcompactionstrategytest.testmutatelevel(leveledcompactionstrateg ytest.java:281)<stacktrace>     [junit] testcase: testmutatelevel(org.apache.cassandra.db.compaction.leveledcompactionstrategytest):        failed     [junit] expected:<6> but was:<0>     [junit] junit.framework.assertionfailederror: expected:<6> but was:<0>     [junit]     at org.apache.cassandra.db.compaction.leveledcompactionstrategytest.testmutatelevel(leveledcompactionstrateg ytest.java:281) <code> testcompactionrace  http://cassci.datastax.com/view/trunk/job/trunk_testall/136/testreport/junit/org.apache.cassandra.db.compaction/leveledcompactionstrategytest/testcompactionprogress/ <text> testmutatelevel (only seen on my desktop)",
        "label": 577
    },
    {
        "text": "message  inter node  compression <description> cassandra-3015 adds compression of streams. but it could be useful to also compress some messages.  compressing messages is easy, but what may be little bit trickier is when and what messages to compress to get the best performances. the simple solution would be to just have it either always on or always off. but for very small messages (gossip?) that may be counter-productive. on the other side of the spectrum, this is likely always a good choice to compress for say the exchange of merkle trees across data-centers. we could maybe define a size of messages after which we start to compress. maybe the option to only compress for cross data-center messages would be useful too (but i may also just be getting carried away).<stacktrace> <code> <text> cassandra-3015 adds compression of streams. but it could be useful to also compress some messages.  compressing messages is easy, but what may be little bit trickier is when and what messages to compress to get the best performances. the simple solution would be to just have it either always on or always off. but for very small messages (gossip?) that may be counter-productive. on the other side of the spectrum, this is likely always a good choice to compress for say the exchange of merkle trees across data-centers. we could maybe define a size of messages after which we start to compress. maybe the option to only compress for cross data-center messages would be useful too (but i may also just be getting carried away).",
        "label": 321
    },
    {
        "text": "compaction improvements to optimize time series data <description> there are some unique characteristics of many/most time series use cases that both provide challenges, as well as provide unique opportunities for optimizations. one of the major challenges is in compaction. the existing compaction strategies will tend to re-compact data on disk at least a few times over the lifespan of each data point, greatly increasing the cpu and io costs of that write. compaction exists to  1) ensure that there aren't too many files on disk  2) ensure that data that should be contiguous (part of the same partition) is laid out contiguously  3) deleting data due to ttls or tombstones the special characteristics of time series data allow us to optimize away all three. time series data  1) tends to be delivered in time order, with relatively constrained exceptions  2) often has a pre-determined and fixed expiration date  3) never gets deleted prior to ttl  4) has relatively predictable ingestion rates note that i filed cassandra-5561 and this ticket potentially replaces or lowers the need for it. in that ticket, jbellis reasonably asks, how that compaction strategy is better than disabling compaction. taking that to heart, here is a compaction-strategy-less approach that could be extremely efficient for time-series use cases that follow the above pattern. (for context, i'm thinking of an example use case involving lots of streams of time-series data with a 5gb per day ingestion rate, and a 1000 day retention with ttl, resulting in an eventual steady state of 5tb per node) 1) you have an extremely large memtable (preferably off heap, if/when doable) for the table, and that memtable is sized to be able to hold a lengthy window of time. a typical period might be one day. at the end of that period, you flush the contents of the memtable to an sstable and move to the next one. this is basically identical to current behaviour, but with thresholds adjusted so that you can ensure flushing at predictable intervals. (open question is whether predictable intervals is actually necessary, or whether just waiting until the huge memtable is nearly full is sufficient)  2) combine the behaviour with cassandra-5228 so that sstables will be efficiently dropped once all of the columns have. (another side note, it might be valuable to have a modified version of cassandra-3974 that doesn't bother storing per-column ttl since it is required that all columns have the same ttl)  3) be able to mark column families as read/write only (no explicit deletes), so no tombstones.  4) optionally add back an additional type of delete that would delete all data earlier than a particular timestamp, resulting in immediate dropping of obsoleted sstables. the result is that for in-order delivered data, every cell will be laid out optimally on disk on the first pass, and over the course of 1000 days and 5tb of data, there will \"only\" be 1000 5gb sstables, so the number of filehandles will be reasonable. for exceptions (out-of-order delivery), most cases will be caught by the extended (24 hour+) memtable flush times and merged correctly automatically. for those that were slightly askew at flush time, or were delivered so far out of order that they go in the wrong sstable, there is relatively low overhead to reading from two sstables for a time slice, instead of one, and that overhead would be incurred relatively rarely unless out-of-order delivery was the common case, in which case, this strategy should not be used. another possible optimization to address out-of-order would be to maintain more than one time-centric memtables in memory at a time (e.g. two 12 hour ones), and then you always insert into whichever one of the two \"owns\" the appropriate range of time. by delaying flushing the ahead one until we are ready to roll writes over to a third one, we are able to avoid any fragmentation as long as all deliveries come in no more than 12 hours late (in this example, presumably tunable). anything that triggers compactions will have to be looked at, since there won't be any. the one concern i have is the ramificaiton of repair. initially, at least, i think it would be acceptable to just write one sstable per repair and not bother trying to merge it with other sstables.<stacktrace> <code> <text> there are some unique characteristics of many/most time series use cases that both provide challenges, as well as provide unique opportunities for optimizations. one of the major challenges is in compaction. the existing compaction strategies will tend to re-compact data on disk at least a few times over the lifespan of each data point, greatly increasing the cpu and io costs of that write. compaction exists to  1) ensure that there aren't too many files on disk  2) ensure that data that should be contiguous (part of the same partition) is laid out contiguously  3) deleting data due to ttls or tombstones the special characteristics of time series data allow us to optimize away all three. time series data  1) tends to be delivered in time order, with relatively constrained exceptions  2) often has a pre-determined and fixed expiration date  3) never gets deleted prior to ttl  4) has relatively predictable ingestion rates note that i filed cassandra-5561 and this ticket potentially replaces or lowers the need for it. in that ticket, jbellis reasonably asks, how that compaction strategy is better than disabling compaction. taking that to heart, here is a compaction-strategy-less approach that could be extremely efficient for time-series use cases that follow the above pattern. (for context, i'm thinking of an example use case involving lots of streams of time-series data with a 5gb per day ingestion rate, and a 1000 day retention with ttl, resulting in an eventual steady state of 5tb per node) 1) you have an extremely large memtable (preferably off heap, if/when doable) for the table, and that memtable is sized to be able to hold a lengthy window of time. a typical period might be one day. at the end of that period, you flush the contents of the memtable to an sstable and move to the next one. this is basically identical to current behaviour, but with thresholds adjusted so that you can ensure flushing at predictable intervals. (open question is whether predictable intervals is actually necessary, or whether just waiting until the huge memtable is nearly full is sufficient)  2) combine the behaviour with cassandra-5228 so that sstables will be efficiently dropped once all of the columns have. (another side note, it might be valuable to have a modified version of cassandra-3974 that doesn't bother storing per-column ttl since it is required that all columns have the same ttl)  3) be able to mark column families as read/write only (no explicit deletes), so no tombstones.  4) optionally add back an additional type of delete that would delete all data earlier than a particular timestamp, resulting in immediate dropping of obsoleted sstables. the result is that for in-order delivered data, every cell will be laid out optimally on disk on the first pass, and over the course of 1000 days and 5tb of data, there will 'only' be 1000 5gb sstables, so the number of filehandles will be reasonable. for exceptions (out-of-order delivery), most cases will be caught by the extended (24 hour+) memtable flush times and merged correctly automatically. for those that were slightly askew at flush time, or were delivered so far out of order that they go in the wrong sstable, there is relatively low overhead to reading from two sstables for a time slice, instead of one, and that overhead would be incurred relatively rarely unless out-of-order delivery was the common case, in which case, this strategy should not be used. another possible optimization to address out-of-order would be to maintain more than one time-centric memtables in memory at a time (e.g. two 12 hour ones), and then you always insert into whichever one of the two 'owns' the appropriate range of time. by delaying flushing the ahead one until we are ready to roll writes over to a third one, we are able to avoid any fragmentation as long as all deliveries come in no more than 12 hours late (in this example, presumably tunable). anything that triggers compactions will have to be looked at, since there won't be any. the one concern i have is the ramificaiton of repair. initially, at least, i think it would be acceptable to just write one sstable per repair and not bother trying to merge it with other sstables.",
        "label": 77
    },
    {
        "text": "native protocol sanity check <description> with mutationstatement.execute turned into a no-op, i only get about 33k insert_prepared ops/s on my laptop. that is: this is an upper bound for our performance if cassandra were infinitely fast, limited by netty handling the protocol + connections. this is up from about 13k/s with ms.execute running normally. ~40% overhead from netty seems awfully high to me, especially for insert_prepared where the return value is tiny. (i also used 4-byte column values to minimize that part as well.)<stacktrace> <code> <text> with mutationstatement.execute turned into a no-op, i only get about 33k insert_prepared ops/s on my laptop. that is: this is an upper bound for our performance if cassandra were infinitely fast, limited by netty handling the protocol + connections. this is up from about 13k/s with ms.execute running normally. ~40% overhead from netty seems awfully high to me, especially for insert_prepared where the return value is tiny. (i also used 4-byte column values to minimize that part as well.)",
        "label": 133
    },
    {
        "text": "potential problem with garbagecollectormxbean <description> i am not certain this is definitely a bug, but i thought it might be worth posting to see if someone with more jvm//jmx knowledge could disprove my reasoning. apologies if i've failed to understand something. we've seen an intermittent problem where there is an uncaught exception in the scheduled task of logging gc results in gcinspector.java: ...  error [scheduledtasks:1] 2013-03-08 01:09:06,335 abstractcassandradaemon.java (line 139) fatal exception in thread thread[scheduledtasks:1,5,main] java.lang.reflect.undeclaredthrowableexception         at $proxy0.getname(unknown source)         at org.apache.cassandra.service.gcinspector.loggcresults(gcinspector.java:95)         at org.apache.cassandra.service.gcinspector.access$000(gcinspector.java:41)         at org.apache.cassandra.service.gcinspector$1.run(gcinspector.java:85)         at java.util.concurrent.executors$runnableadapter.call(executors.java:441)         at java.util.concurrent.futuretask$sync.innerrunandreset(futuretask.java:317)         at java.util.concurrent.futuretask.runandreset(futuretask.java:150)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$101(scheduledthreadpoolexecutor.java:98)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.runperiodic(scheduledthreadpoolexecutor.java:180)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:204)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) caused by: javax.management.instancenotfoundexception: java.lang:name=parnew,type=garbagecollector         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getattribute(defaultmbeanserverinterceptor.java:662)         at com.sun.jmx.mbeanserver.jmxmbeanserver.getattribute(jmxmbeanserver.java:638)         at com.sun.jmx.mbeanserver.mxbeanproxy$gethandler.invoke(mxbeanproxy.java:106)         at com.sun.jmx.mbeanserver.mxbeanproxy.invoke(mxbeanproxy.java:148)         at javax.management.mbeanserverinvocationhandler.invoke(mbeanserverinvocationhandler.java:248)         ... 13 more ... i think the problem, may be caused by the following reasoning: in gcinspector we populate a list of mxbeans when the gcinspector instance is instantiated: ... list<garbagecollectormxbean> beans = new arraylist<garbagecollectormxbean>();         mbeanserver server = managementfactory.getplatformmbeanserver();         try         {             objectname gcname = new objectname(managementfactory.garbage_collector_mxbean_domain_type + \",*\");             for (objectname name : server.querynames(gcname, null))             {                 garbagecollectormxbean gc = managementfactory.newplatformmxbeanproxy(server, name.getcanonicalname(), garbagecollectormxbean.class);                 beans.add(gc);             }         }         catch (exception e)         {             throw new runtimeexception(e);         } ... cassandra then periodically calls: ...     private void loggcresults()     {         for (garbagecollectormxbean gc : beans)         {             long previoustotal = gctimes.get(gc.getname()); ... in the oracle javadocs, they seem to suggest that these beans could disappear at any time.(i'm not sure why when or how this might happen)  http://docs.oracle.com/javase/6/docs/api/  see: getgarbagecollectormxbeans ... public static list<garbagecollectormxbean> getgarbagecollectormxbeans() returns a list of garbagecollectormxbean objects in the java virtual machine. the java virtual machine may have one or more garbagecollectormxbean objects. it may add or remove garbagecollectormxbean during execution. returns: a list of garbagecollectormxbean objects. ... correct me if i'm wrong, but do you think this might be causing the problem? that somehow the jvm decides to remove the garbagecollectormxbean temporarily or permanently (causing said exception) and if this is expected behaviour, should it be handled in some way?  also i'd like to point out that this may be an issue on other versions as well as i don't believe this code has changed in quite a long time.  unfortunately i haven't been able to reproduce this outside of the production environment, if you have any tips, questions or are able to explain//disprove my concerns, i'd be very grateful. thanks,  matt<stacktrace> ...  error [scheduledtasks:1] 2013-03-08 01:09:06,335 abstractcassandradaemon.java (line 139) fatal exception in thread thread[scheduledtasks:1,5,main] java.lang.reflect.undeclaredthrowableexception         at $proxy0.getname(unknown source)         at org.apache.cassandra.service.gcinspector.loggcresults(gcinspector.java:95)         at org.apache.cassandra.service.gcinspector.access$000(gcinspector.java:41)         at org.apache.cassandra.service.gcinspector$1.run(gcinspector.java:85)         at java.util.concurrent.executors$runnableadapter.call(executors.java:441)         at java.util.concurrent.futuretask$sync.innerrunandreset(futuretask.java:317)         at java.util.concurrent.futuretask.runandreset(futuretask.java:150)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$101(scheduledthreadpoolexecutor.java:98)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.runperiodic(scheduledthreadpoolexecutor.java:180)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:204)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) caused by: javax.management.instancenotfoundexception: java.lang:name=parnew,type=garbagecollector         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1094)         at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getattribute(defaultmbeanserverinterceptor.java:662)         at com.sun.jmx.mbeanserver.jmxmbeanserver.getattribute(jmxmbeanserver.java:638)         at com.sun.jmx.mbeanserver.mxbeanproxy$gethandler.invoke(mxbeanproxy.java:106)         at com.sun.jmx.mbeanserver.mxbeanproxy.invoke(mxbeanproxy.java:148)         at javax.management.mbeanserverinvocationhandler.invoke(mbeanserverinvocationhandler.java:248)         ... 13 more ... <code> ... list<garbagecollectormxbean> beans = new arraylist<garbagecollectormxbean>();         mbeanserver server = managementfactory.getplatformmbeanserver();         try         {             objectname gcname = new objectname(managementfactory.garbage_collector_mxbean_domain_type + ',*');             for (objectname name : server.querynames(gcname, null))             {                 garbagecollectormxbean gc = managementfactory.newplatformmxbeanproxy(server, name.getcanonicalname(), garbagecollectormxbean.class);                 beans.add(gc);             }         }         catch (exception e)         {             throw new runtimeexception(e);         } ... ...     private void loggcresults()     {         for (garbagecollectormxbean gc : beans)         {             long previoustotal = gctimes.get(gc.getname()); ... ... public static list<garbagecollectormxbean> getgarbagecollectormxbeans() returns a list of garbagecollectormxbean objects in the java virtual machine. the java virtual machine may have one or more garbagecollectormxbean objects. it may add or remove garbagecollectormxbean during execution. returns: a list of garbagecollectormxbean objects. ... <text> i am not certain this is definitely a bug, but i thought it might be worth posting to see if someone with more jvm//jmx knowledge could disprove my reasoning. apologies if i've failed to understand something. we've seen an intermittent problem where there is an uncaught exception in the scheduled task of logging gc results in gcinspector.java: i think the problem, may be caused by the following reasoning: in gcinspector we populate a list of mxbeans when the gcinspector instance is instantiated: cassandra then periodically calls: in the oracle javadocs, they seem to suggest that these beans could disappear at any time.(i'm not sure why when or how this might happen)  http://docs.oracle.com/javase/6/docs/api/  see: getgarbagecollectormxbeans correct me if i'm wrong, but do you think this might be causing the problem? that somehow the jvm decides to remove the garbagecollectormxbean temporarily or permanently (causing said exception) and if this is expected behaviour, should it be handled in some way?  also i'd like to point out that this may be an issue on other versions as well as i don't believe this code has changed in quite a long time.  unfortunately i haven't been able to reproduce this outside of the production environment, if you have any tips, questions or are able to explain//disprove my concerns, i'd be very grateful. thanks,  matt",
        "label": 280
    },
    {
        "text": "add the name of the compressor in the output of sstablemetadata <description> here is a simple patch to add to the output of sstablemetadata the name the compressor used for the sstable.  i also made sstablemetadata embedded into the .deb distribution<stacktrace> <code> <text> here is a simple patch to add to the output of sstablemetadata the name the compressor used for the sstable.  i also made sstablemetadata embedded into the .deb distribution",
        "label": 380
    },
    {
        "text": "describe keyspace error <description> i was testing cassandra 2.1.0rc5 on opscenter and noticed something odd in the diagnostic tarball. the keyspace that i had created show up, but not the name of the column family. so i tried cqlsh and this was the result. [cqlsh 5.0.1 | cassandra 2.1.0-rc5 | cql spec 3.2.0 | native protocol v3] use help for help. cqlsh> describe keyspace funspace; too many values to unpack cqlsh> describe keyspaces; system_traces  funspace  system  \"opscenter\"  \"keyspace1\" my dataset is attached.<stacktrace> <code> [cqlsh 5.0.1 | cassandra 2.1.0-rc5 | cql spec 3.2.0 | native protocol v3] use help for help. cqlsh> describe keyspace funspace; too many values to unpack cqlsh> describe keyspaces; system_traces  funspace  system  'opscenter'  'keyspace1' <text> i was testing cassandra 2.1.0rc5 on opscenter and noticed something odd in the diagnostic tarball. the keyspace that i had created show up, but not the name of the column family. so i tried cqlsh and this was the result. my dataset is attached.",
        "label": 538
    },
    {
        "text": "coordinator's  java lang arrayindexoutofboundsexception    with cl   <description> i've got this error in system.log on all coordinators error [thrift:37555] 2014-01-28 19:53:51,547 customtthreadpoolserver.java (line 212) error occurred during processing of message. java.lang.arrayindexoutofboundsexception: -1         at java.util.arraylist.elementdata(arraylist.java:400)         at java.util.arraylist.remove(arraylist.java:477)         at org.apache.cassandra.db.arraybackedsortedcolumns$reversesortedcollection$1.remove(arraybackedsortedcolumns.java:373)         at org.apache.cassandra.db.filter.slicequeryfilter.trim(slicequeryfilter.java:249)         at org.apache.cassandra.db.slicefromreadcommand.maybetrim(slicefromreadcommand.java:101)         at org.apache.cassandra.service.storageproxy.fetchrows(storageproxy.java:1370)         at org.apache.cassandra.service.storageproxy.read(storageproxy.java:1189)         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:188)         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:163)         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:58)         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:188)         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:222)         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:212)         at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1958)         at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4486)         at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4470)         at org.apache.thrift.processfunction.process(processfunction.java:39)         at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39)         at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:194)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) it's occurred on coordinator (not always primary or secondary of this uid-pk) when i execute query (php or python client got \"tsocket read 0 bytes\" exception): select * from home_timeline where uid = 0x52dcbc794989a6ea2c8b4569 order by tuuid desc limit 32 if limit < 32, then its ok. when order ... asc its ok. when consistencylevel 1 its ok.  on one node data is inconsistent with two others, and read repair won't work (32-nd element is odd). our rf = 3  cassandra version 2.0.4<stacktrace> error [thrift:37555] 2014-01-28 19:53:51,547 customtthreadpoolserver.java (line 212) error occurred during processing of message. java.lang.arrayindexoutofboundsexception: -1         at java.util.arraylist.elementdata(arraylist.java:400)         at java.util.arraylist.remove(arraylist.java:477)         at org.apache.cassandra.db.arraybackedsortedcolumns$reversesortedcollection$1.remove(arraybackedsortedcolumns.java:373)         at org.apache.cassandra.db.filter.slicequeryfilter.trim(slicequeryfilter.java:249)         at org.apache.cassandra.db.slicefromreadcommand.maybetrim(slicefromreadcommand.java:101)         at org.apache.cassandra.service.storageproxy.fetchrows(storageproxy.java:1370)         at org.apache.cassandra.service.storageproxy.read(storageproxy.java:1189)         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:188)         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:163)         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:58)         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:188)         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:222)         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:212)         at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1958)         at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4486)         at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4470)         at org.apache.thrift.processfunction.process(processfunction.java:39)         at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39)         at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:194)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) <code> select * from home_timeline where uid = 0x52dcbc794989a6ea2c8b4569 order by tuuid desc limit 32 select * from home_timeline where uid = 0x52dcbc794989a6ea2c8b4569 order by tuuid desc limit 32 <text> i've got this error in system.log on all coordinators it's occurred on coordinator (not always primary or secondary of this uid-pk) when i execute query (php or python client got 'tsocket read 0 bytes' exception): if limit < 32, then its ok. when order ... asc its ok. when consistencylevel 1 its ok.  on one node data is inconsistent with two others, and read repair won't work (32-nd element is odd). our rf = 3  cassandra version 2.0.4",
        "label": 362
    },
    {
        "text": "respect slice count even if column expire mid request <description> this is a follow-up of cassandra-5099. if a column expire just while a slice query is performed, it is possible for replicas to count said column as live but to have the coordinator seeing it as dead when building the final result. the effect that the query might return strictly less columns that the requested slice count even though there is some live columns matching the slice predicate but not returned in the result.<stacktrace> <code> <text> this is a follow-up of cassandra-5099. if a column expire just while a slice query is performed, it is possible for replicas to count said column as live but to have the coordinator seeing it as dead when building the final result. the effect that the query might return strictly less columns that the requested slice count even though there is some live columns matching the slice predicate but not returned in the result.",
        "label": 18
    },
    {
        "text": "dtest failure in upgrade tests cql tests testcqlnodes2rf1 upgrade current x to indev x list item conditional test <description> <errormessage code=2000 [syntax error in cql query] message=\"line 4:18 no viable alternative at input 'frozen'\"> example failure: http://cassci.datastax.com/job/upgrade_tests-all/46/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_2_0_x_to_indev_2_1_x/list_item_conditional_test failed on cassci build upgrade_tests-all #46 on first glance, i believe this is a test issue.<stacktrace> <code> http://cassci.datastax.com/job/upgrade_tests-all/46/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_2_0_x_to_indev_2_1_x/list_item_conditional_test <text> <errormessage code=2000 [syntax error in cql query] message='line 4:18 no viable alternative at input 'frozen''> example failure: failed on cassci build upgrade_tests-all #46 on first glance, i believe this is a test issue.",
        "label": 487
    },
    {
        "text": "cassandra yaml now unicode instead of ascii after <description> after cassandra-13090, which was commit 5725e2c422d21d8efe5ae3bc4389842939553650, cassandra.yaml now has unicode characters, specifically 0xe2. previously, it was only ascii. this is an admittedly minor change, but it is breaking. it affects (at least) a subset of python yaml parsing tools (which is a large number of tools that use c*).<stacktrace> <code> <text> after cassandra-13090, which was commit 5725e2c422d21d8efe5ae3bc4389842939553650, cassandra.yaml now has unicode characters, specifically 0xe2. previously, it was only ascii. this is an admittedly minor change, but it is breaking. it affects (at least) a subset of python yaml parsing tools (which is a large number of tools that use c*).",
        "label": 52
    },
    {
        "text": "npe on invalid cql delete command <description> the cql command delete from k where key='bar'; causes cassandra to hit a nullpointerexception when the \"k\" column family does not exist, and it subsequently closes the thrift connection instead of reporting an ire or whatever. this is probably wrong.<stacktrace> <code> <text> the cql command delete from k where key='bar'; causes cassandra to hit a nullpointerexception when the 'k' column family does not exist, and it subsequently closes the thrift connection instead of reporting an ire or whatever. this is probably wrong.",
        "label": 139
    },
    {
        "text": "make pig cassandrastorage delete functionality disabled by default and configurable <description> right now, there is a way to delete column with the cassandrastorage loadstorefunc. in practice it is a bad idea to have that enabled by default. a scenario: do an outer join and you don't have a value for something and then you write out to cassandra all of the attributes of that relation. you've just inadvertently deleted a column for all the rows that didn't have that value as a result of the outer join. it can be argued that you want to be careful with how you project after the join. however, i would think disabling by default and having a configurable property to enable it for the instances when you explicitly want to use it is the right plan. fwiw, we had a bug in one of our scripts that did exactly as described above. it's good to fix the bug. it's bad to implicitly delete data.<stacktrace> <code> <text> right now, there is a way to delete column with the cassandrastorage loadstorefunc. in practice it is a bad idea to have that enabled by default. a scenario: do an outer join and you don't have a value for something and then you write out to cassandra all of the attributes of that relation. you've just inadvertently deleted a column for all the rows that didn't have that value as a result of the outer join. it can be argued that you want to be careful with how you project after the join. however, i would think disabling by default and having a configurable property to enable it for the instances when you explicitly want to use it is the right plan. fwiw, we had a bug in one of our scripts that did exactly as described above. it's good to fix the bug. it's bad to implicitly delete data.",
        "label": 85
    },
    {
        "text": "missing string format  in antientropyservice java logs <description> a string.format() is missing in antientropyservice.java (line 625 in 1.2).   this is what is written to the logs: antientropyservice.java (line 625) [repair #%s] no neighbors to repair with on range %s: session completed.<stacktrace> <code> a string.format() is missing in antientropyservice.java (line 625 in 1.2).   this is what is written to the logs: antientropyservice.java (line 625) [repair #%s] no neighbors to repair with on range %s: session completed.<text> ",
        "label": 135
    },
    {
        "text": "validate rack information on startup <description> moving to a new rack means that different data should be stored on a node. we already persist rack information in a system table; we should fail startup if this doesn't match what the snitch thinks it should be. (either the snitch is wrong, and needs to be fixed, or the machine has been moved and needs to be rebootstrapped.)<stacktrace> <code> <text> moving to a new rack means that different data should be stored on a node. we already persist rack information in a system table; we should fail startup if this doesn't match what the snitch thinks it should be. (either the snitch is wrong, and needs to be fixed, or the machine has been moved and needs to be rebootstrapped.)",
        "label": 98
    },
    {
        "text": "duplicate column names with dynamiccompositetype <description> i have a column family whose comparator is dynamiccompositetype and validation is countercolumntype. during automated testing, there have been occasions where a counter column is created twice, throwing off the query results for the column. doing a 'get' via the cli, i see the following output for the row: => (counter=s@language:b@00000001:s@pt_br, value=198)  => (counter=s@language:s@possible, value=200)  => (counter=s@language:b@00000001:s@pt_br, value=0) if i print out the byte value of the column names along with their md5 sum i see: name: [language, java.nio.heapbytebuffer[pos=0 lim=4 cap=4], pt_br]  byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520  md5: 2db353a9a72a0d7cb6cb277ac5125653  name: [language, possible]  byte array: ffffff8073086c616e67756167650ffffff807308706f737369626c650  md5: 82cad9b6a65c794e97cf1d4613e2e367  name: [language, java.nio.heapbytebuffer[pos=0 lim=4 cap=4], pt_br]  byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520  md5: 2db353a9a72a0d7cb6cb277ac5125653 unfortunately, i have been unable to duplicate this manually or via a generic test script and our qa department can only duplicate ~25% of the time.<stacktrace> <code> => (counter=s@language:b@00000001:s@pt_br, value=198)  => (counter=s@language:s@possible, value=200)  => (counter=s@language:b@00000001:s@pt_br, value=0) name: [language, java.nio.heapbytebuffer[pos=0 lim=4 cap=4], pt_br]  byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520  md5: 2db353a9a72a0d7cb6cb277ac5125653  name: [language, possible]  byte array: ffffff8073086c616e67756167650ffffff807308706f737369626c650  md5: 82cad9b6a65c794e97cf1d4613e2e367  name: [language, java.nio.heapbytebuffer[pos=0 lim=4 cap=4], pt_br]  byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520  md5: 2db353a9a72a0d7cb6cb277ac5125653 <text> i have a column family whose comparator is dynamiccompositetype and validation is countercolumntype. during automated testing, there have been occasions where a counter column is created twice, throwing off the query results for the column. doing a 'get' via the cli, i see the following output for the row: if i print out the byte value of the column names along with their md5 sum i see: unfortunately, i have been unable to duplicate this manually or via a generic test script and our qa department can only duplicate ~25% of the time.",
        "label": 520
    },
    {
        "text": "cassandra stress should support multiple table operations <description> <stacktrace> <code> <text> ",
        "label": 66
    },
    {
        "text": "make cfif use rpc endpoint prior to trying endpoint <description> following up on cassandra-3187 , we probably need to attempt to use the rpc_endpoint address first, and then fall back to the gossip endpoint if we don't get what we want.<stacktrace> <code> <text> following up on cassandra-3187 , we probably need to attempt to use the rpc_endpoint address first, and then fall back to the gossip endpoint if we don't get what we want.",
        "label": 166
    },
    {
        "text": "coalescing strategy can enter infinite loop <description> boolean maybesleep(int messages, long averagegap, long maxcoalescewindow, parker parker) maybesleep() can enter an infinite loop if messages or averagegap ends up being 0 because sleep will be 0 and the while loop will never exit. i've noticed that on one of my clusters twice this week. this can happen if in averagegap() sum is bigger than measured_interval, which should be pretty rare but apparently happen to me. even if the diagnostic is wrong (and i'm pretty sure that this thread was using 100% cpu doing nothing), the fix seems pretty safe to apply. diff --git a/src/java/org/apache/cassandra/utils/coalescingstrategies.java b/src/java/org/apache/cassandra/utils/coalescingstrategies.java index 0aa980f..982d4a6 100644 --- a/src/java/org/apache/cassandra/utils/coalescingstrategies.java +++ b/src/java/org/apache/cassandra/utils/coalescingstrategies.java @@ -100,7 +100,7 @@ public class coalescingstrategies      {          // only sleep if we can expect to double the number of messages we're sending in the time interval          long sleep = messages * averagegap; -        if (sleep > maxcoalescewindow) +        if (!sleep || sleep > maxcoalescewindow)              return false;            // assume we receive as many messages as we expect; apply the same logic to the future batch:<stacktrace> <code> boolean maybesleep(int messages, long averagegap, long maxcoalescewindow, parker parker) diff --git a/src/java/org/apache/cassandra/utils/coalescingstrategies.java b/src/java/org/apache/cassandra/utils/coalescingstrategies.java index 0aa980f..982d4a6 100644 --- a/src/java/org/apache/cassandra/utils/coalescingstrategies.java +++ b/src/java/org/apache/cassandra/utils/coalescingstrategies.java @@ -100,7 +100,7 @@ public class coalescingstrategies      {          // only sleep if we can expect to double the number of messages we're sending in the time interval          long sleep = messages * averagegap; -        if (sleep > maxcoalescewindow) +        if (!sleep || sleep > maxcoalescewindow)              return false;            // assume we receive as many messages as we expect; apply the same logic to the future batch: <text> maybesleep() can enter an infinite loop if messages or averagegap ends up being 0 because sleep will be 0 and the while loop will never exit. i've noticed that on one of my clusters twice this week. this can happen if in averagegap() sum is bigger than measured_interval, which should be pretty rare but apparently happen to me. even if the diagnostic is wrong (and i'm pretty sure that this thread was using 100% cpu doing nothing), the fix seems pretty safe to apply.",
        "label": 120
    },
    {
        "text": "thrift interface needs a ruby namespace <description> <stacktrace> <code> <text> ",
        "label": 178
    },
    {
        "text": "avoid dropping messages off the client request path <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "fix buffer length comparison when decompressing in netty based streaming <description> streaming a single partition with ~100k rows fails with the following exception: error [stream-deserializer-/127.0.0.1:35149-a92e5e12] 2017-09-21 04:03:41,237 streamsession.java:617 - [stream #c2e5b640-9eab-11e7-99c0-e9864ca8da8e] streaming error occurred on session with peer 127.0.0.1 org.apache.cassandra.streaming.streamreceiveexception: java.lang.runtimeexception: last written key decoratedkey(-1000328290821038380) >= current key decoratedkey(-1055007227842125139)  writing into /home/paulo/.ccm/test/node2/data0/stresscql/typestest-482ac7b09e8d11e787cf85d073c 8e037/na-1-big-data.db         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:63) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:41) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:55) ~[main/:na]         at org.apache.cassandra.streaming.async.streaminginboundhandler$streamdeserializingtask.run(streaminginboundhandler.java:178) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_121] reproduction steps: create ccm cluster with 2 nodes start only first node, disable hinted handoff run stress with the attached yaml: tools/bin/cassandra-stress \"user profile=largepartition.yaml n=10k ops(insert=1) no-warmup -node whitelist 127.0.0.1 -mode native cql3 compression=lz4 -rate threads=4 -insert visits=fixed(100k) revisit=fixed(100k)\" start second node, run repair on stresscql table - the exception above will be thrown. i investigated briefly and haven't found anything suspicious. this seems to be related to cassandra-12229 as i tested the steps above in a branch without that and the repair completed successfully. i haven't tested with a smaller number of rows per partition to see at which point it starts to be a problem. we should probably add a regression dtest to stream large partitions to catch similar problems in the future.<stacktrace> error [stream-deserializer-/127.0.0.1:35149-a92e5e12] 2017-09-21 04:03:41,237 streamsession.java:617 - [stream #c2e5b640-9eab-11e7-99c0-e9864ca8da8e] streaming error occurred on session with peer 127.0.0.1 org.apache.cassandra.streaming.streamreceiveexception: java.lang.runtimeexception: last written key decoratedkey(-1000328290821038380) >= current key decoratedkey(-1055007227842125139)  writing into /home/paulo/.ccm/test/node2/data0/stresscql/typestest-482ac7b09e8d11e787cf85d073c 8e037/na-1-big-data.db         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:63) ~[main/:na]         at org.apache.cassandra.streaming.messages.incomingfilemessage$1.deserialize(incomingfilemessage.java:41) ~[main/:na]         at org.apache.cassandra.streaming.messages.streammessage.deserialize(streammessage.java:55) ~[main/:na]         at org.apache.cassandra.streaming.async.streaminginboundhandler$streamdeserializingtask.run(streaminginboundhandler.java:178) ~[main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_121] <code> <text> streaming a single partition with ~100k rows fails with the following exception: reproduction steps: i investigated briefly and haven't found anything suspicious. this seems to be related to cassandra-12229 as i tested the steps above in a branch without that and the repair completed successfully. i haven't tested with a smaller number of rows per partition to see at which point it starts to be a problem. we should probably add a regression dtest to stream large partitions to catch similar problems in the future.",
        "label": 232
    },
    {
        "text": "illegalargumentexception when preparing statements <description> when preparing a lot of statements with the python native driver, i occasionally get an error response with an error that corresponds to the following stacktrace in the cassandra logs: error [native-transport-requests:126] 2014-01-11 13:58:05,503 errormessage.java (line 210) unexpected exception during request java.lang.illegalargumentexception         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap.checkargument(concurrentlinkedhashmap.java:259)         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap$boundedentryweigher.weightof(concurrentlinkedhashmap.java:1448)         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap.put(concurrentlinkedhashmap.java:764)         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap.put(concurrentlinkedhashmap.java:743)         at org.apache.cassandra.cql3.queryprocessor.storepreparedstatement(queryprocessor.java:255)         at org.apache.cassandra.cql3.queryprocessor.prepare(queryprocessor.java:221)         at org.apache.cassandra.transport.messages.preparemessage.execute(preparemessage.java:77)         at org.apache.cassandra.transport.message$dispatcher.messagereceived(message.java:287)         at org.jboss.netty.channel.simplechannelupstreamhandler.handleupstream(simplechannelupstreamhandler.java:70)         at org.jboss.netty.channel.defaultchannelpipeline.sendupstream(defaultchannelpipeline.java:564)         at org.jboss.netty.channel.defaultchannelpipeline$defaultchannelhandlercontext.sendupstream(defaultchannelpipeline.java:791)         at org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun(channelupstreameventrunnable.java:43)         at org.jboss.netty.handler.execution.channeleventrunnable.run(channeleventrunnable.java:67)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918)         at java.lang.thread.run(thread.java:662) looking at the clhm source, this means we're giving the statement a weight that's less than 1. i'll also note that these errors frequently happen in clumps of 2 or 3 at a time.<stacktrace> error [native-transport-requests:126] 2014-01-11 13:58:05,503 errormessage.java (line 210) unexpected exception during request java.lang.illegalargumentexception         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap.checkargument(concurrentlinkedhashmap.java:259)         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap$boundedentryweigher.weightof(concurrentlinkedhashmap.java:1448)         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap.put(concurrentlinkedhashmap.java:764)         at com.googlecode.concurrentlinkedhashmap.concurrentlinkedhashmap.put(concurrentlinkedhashmap.java:743)         at org.apache.cassandra.cql3.queryprocessor.storepreparedstatement(queryprocessor.java:255)         at org.apache.cassandra.cql3.queryprocessor.prepare(queryprocessor.java:221)         at org.apache.cassandra.transport.messages.preparemessage.execute(preparemessage.java:77)         at org.apache.cassandra.transport.message$dispatcher.messagereceived(message.java:287)         at org.jboss.netty.channel.simplechannelupstreamhandler.handleupstream(simplechannelupstreamhandler.java:70)         at org.jboss.netty.channel.defaultchannelpipeline.sendupstream(defaultchannelpipeline.java:564)         at org.jboss.netty.channel.defaultchannelpipeline$defaultchannelhandlercontext.sendupstream(defaultchannelpipeline.java:791)         at org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun(channelupstreameventrunnable.java:43)         at org.jboss.netty.handler.execution.channeleventrunnable.run(channeleventrunnable.java:67)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918)         at java.lang.thread.run(thread.java:662) <code> <text> when preparing a lot of statements with the python native driver, i occasionally get an error response with an error that corresponds to the following stacktrace in the cassandra logs: looking at the clhm source, this means we're giving the statement a weight that's less than 1. i'll also note that these errors frequently happen in clumps of 2 or 3 at a time.",
        "label": 520
    },
    {
        "text": "frozen broken   frozen collections in frozen tuple type <description> create table foo (key int primary key, tup frozen<tuple<double, list<double>, set<text>, map<int, boolean>>>) produces an npe in cql3type.freeze(), because lists and sets have no keys. create table bar (key int primary key, tup frozen<tuple<double, frozen<list<double>>, frozen<set<text>>, frozen<map<int, boolean>>>>) produces some npes that prevent the correct error message being printed. simple patch attached.<stacktrace> <code> create table foo (key int primary key, tup frozen<tuple<double, list<double>, set<text>, map<int, boolean>>>) create table bar (key int primary key, tup frozen<tuple<double, frozen<list<double>>, frozen<set<text>>, frozen<map<int, boolean>>>>) <text> produces an npe in cql3type.freeze(), because lists and sets have no keys. produces some npes that prevent the correct error message being printed. simple patch attached.",
        "label": 453
    },
    {
        "text": "read repair does not always work correctly <description> read repair does not always work. at the least, we allow violation of the cl.all contract. to reproduce, create a three node cluster with rf=3, and json2sstable one of the attached json files on each node. this creates a row whose key is 'test' with 9 columns, but only 3 columns are on each machine. if you get_count this row in quick succession at cl.all, sometimes you will receive a count of 6, sometimes 9. after the readrepairmanager has sent the repairs, you will always get 9, which is the desired behavior. i have another data set obtained in the wild which never fully repairs for some reason, but it's a bit large to attach (600ish columns per machine.) i'm still trying to figure out why rr isn't working on this set, but i always get different results when reading at any cl including all, no matter how long i wait or how many reads i do.<stacktrace> <code> <text> read repair does not always work. at the least, we allow violation of the cl.all contract. to reproduce, create a three node cluster with rf=3, and json2sstable one of the attached json files on each node. this creates a row whose key is 'test' with 9 columns, but only 3 columns are on each machine. if you get_count this row in quick succession at cl.all, sometimes you will receive a count of 6, sometimes 9. after the readrepairmanager has sent the repairs, you will always get 9, which is the desired behavior. i have another data set obtained in the wild which never fully repairs for some reason, but it's a bit large to attach (600ish columns per machine.) i'm still trying to figure out why rr isn't working on this set, but i always get different results when reading at any cl including all, no matter how long i wait or how many reads i do.",
        "label": 85
    },
    {
        "text": " dtest   trunk  test tracing does not interfere with digest calculation   cql tracing test testcqltracing failed once   assertionerror  assert   <description> test_tracing_does_not_interfere_with_digest_calculation - cql_tracing_test.testcqltracing failed it's assertion once today in a circleci run. the dtests were running against trunk. although it has failed once so far, a quick read of the comments in the test seems to indicate that the assertion failing this way might mean that cassandra-13964 didn't fully fix the issue. if jmx.has_mbean(rr_count):                 # expect 0 digest mismatches >               assert 0 == jmx.read_attribute(rr_count, 'count') e               assertionerror: assert 0 == 1 e                +  where 1 = <bound method jolokiaagent.read_attribute of <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>>('org.apache.cassandra.metrics:type=readrepair,name=repairedblocking', 'count') e                +    where <bound method jolokiaagent.read_attribute of <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>> = <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>.read_attribute<stacktrace> <code> if jmx.has_mbean(rr_count):                 # expect 0 digest mismatches >               assert 0 == jmx.read_attribute(rr_count, 'count') e               assertionerror: assert 0 == 1 e                +  where 1 = <bound method jolokiaagent.read_attribute of <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>>('org.apache.cassandra.metrics:type=readrepair,name=repairedblocking', 'count') e                +    where <bound method jolokiaagent.read_attribute of <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>> = <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>.read_attribute if jmx.has_mbean(rr_count):                 # expect 0 digest mismatches >               assert 0 == jmx.read_attribute(rr_count, 'count') e               assertionerror: assert 0 == 1 e                +  where 1 = <bound method jolokiaagent.read_attribute of <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>>('org.apache.cassandra.metrics:type=readrepair,name=repairedblocking', 'count') e                +    where <bound method jolokiaagent.read_attribute of <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>> = <tools.jmxutils.jolokiaagent object at 0x7f62d4156898>.read_attribute <text> test_tracing_does_not_interfere_with_digest_calculation - cql_tracing_test.testcqltracing failed it's assertion once today in a circleci run. the dtests were running against trunk. although it has failed once so far, a quick read of the comments in the test seems to indicate that the assertion failing this way might mean that cassandra-13964 didn't fully fix the issue.",
        "label": 8
    },
    {
        "text": "nodetool ring throws java lang assertionerror in tokenmetadata gettopology <description> $ bin/nodetool -h localhost ring exception in thread \"main\" java.lang.assertionerror         at org.apache.cassandra.locator.tokenmetadata.gettopology(tokenmetadata.java:851)         at org.apache.cassandra.service.storageservice.effectiveownership(storageservice.java:2781)         at org.apache.cassandra.service.storageservice.effectiveownership(storageservice.java:70) tokenmetadata.gettopology() can only be called on a clone of tokenmetadata, not the storageservice instance.<stacktrace> $ bin/nodetool -h localhost ring exception in thread 'main' java.lang.assertionerror         at org.apache.cassandra.locator.tokenmetadata.gettopology(tokenmetadata.java:851)         at org.apache.cassandra.service.storageservice.effectiveownership(storageservice.java:2781)         at org.apache.cassandra.service.storageservice.effectiveownership(storageservice.java:70) <code> <text> tokenmetadata.gettopology() can only be called on a clone of tokenmetadata, not the storageservice instance.",
        "label": 473
    },
    {
        "text": "introduce direct unit test coverage for rows <description> as with much of the codebase, we have no direct unit test coverage for rows, and we should remedy this given how central it is to behaviour.<stacktrace> <code> <text> as with much of the codebase, we have no direct unit test coverage for rows, and we should remedy this given how central it is to behaviour.",
        "label": 79
    },
    {
        "text": "some compactions do not works under windows  file in use during rename  <description> compaction do not works under windows due to file rename fails: (pro  es nem\u00df p\u00b0\u00fdstup k souboru, nebo\u0142 jej pr\u00dfv\u00fd vyu\u00d7\u00fdv\u00df jin\u0159 proces = process can not access file because its in use by another process). not all compactions are broken. compactions done during server startup on system tables works fine. info 18:30:27 completed flushing c:\\cassandra-2.1\\data\\system\\compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_progress-ka-6-dat.db (42 bytes) for commitlog position replayposition(segmentid=1402165543361, psition=8024611)  error 18:30:27 exception in thread hread[compactionexecutor:5,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:\\cassandra-2.1\\data\\test\\sipdb-5  f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-7-index.db to c:\\cassandra-2.1  data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-7-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:\\cassandra-2.1\\data\\test\\sipdb-  8f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-7-index.db -> c:\\cassandra-2.  \\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-7-index.db: pro  es nem\u00df p\u00b0\u00fdstup k souboru, nebo\u0142 jej pr\u00dfv\u00fd vyu\u00d7\u00fdv\u00df jin\u0159 proces.  at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted  info 18:30:27 compacting [sstablereader(path='c:\\cassandra-2.1\\data\\system\\com  actions_in_progress-55080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_prog  ess-ka-3-data.db'), sstablereader(path='c:\\cassandra-2.1\\data\\system\\compaction  _in_progress-55080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_progress-ka  5-data.db'), sstablereader(path='c:\\cassandra-2.1\\data\\system\\compactions_in_pr  gress-55080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_progress-ka-4-data  db'), sstablereader(path='c:\\cassandra-2.1\\data\\system\\compactions_in_progress-  5080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_progress-ka-6-data.db')]  error 18:30:27 exception in thread thread[compactionexecutor:5,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:\\cassandra-2.1\\data\\test\\sipdb-5  f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-7-index.db to c:\\cassandra-2.1  data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-7-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:\\cassandra-2.1\\data\\test\\sipdb-  8f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-7-index.db -> c:\\cassandra-2.  \\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-7-index.db: pro  es nem\u00df p\u00b0\u00fdstup k souboru, nebo\u0142 jej pr\u00dfv\u00fd vyu\u00d7\u00fdv\u00df jin\u0159 proces.  at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted  info 18:30:27 compacted 4 sstables to []. 423 bytes to 0 (~0% of original) in  39ms = 0,000000mb/s. 4 total partitions merged to 0. partition merge counts w  re {2:2, } info 18:30:45 enqueuing flush of compactions_in_progress: 1345 (0%) on-heap, 0  (0%) off-heap  info 18:30:45 writing memtable-compactions_in_progress@15659113(153 serialized  bytes, 10 ops, 0%/0% of on/off-heap limit)  info 18:30:45 completed flushing c:\\cassandra-2.1\\data\\system\\compactions_in_p  ogress-55080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_progress-ka-8-dat  .db (173 bytes) for commitlog position replayposition(segmentid=1402165543361,  osition=8025407)  info 18:30:45 compacting [sstablereader(path='c:\\cassandra-2.1\\data\\test\\sipdb  58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-3-data.db'), sstablereader(path=  c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka  1-data.db'), sstablereader(path='c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee651  e3815625991ef2b954\\test-sipdb-ka-4-data.db'), sstablereader(path='c:\\cassandra-  .1\\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-2-data.db'),  stablereader(path='c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511e3815625991ef  b954\\test-sipdb-ka-6-data.db')]  error 18:31:41 unable to delete c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee6511  3815625991ef2b954\\test-sipdb-ka-8-data.db (it will be removed on server restart  we'll also retry after gc)  error 18:31:41 missing component: c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee65  1e3815625991ef2b954\\test-sipdb-tmp-ka-8-digest.sha1  error 18:31:41 missing component: c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee65  1e3815625991ef2b954\\test-sipdb-tmp-ka-8-summary.db  error 18:31:41 missing component: c:\\cassandra-2.1\\data\\test\\sipdb-58f51090ee65  1e3815625991ef2b954\\test-sipdb-tmp-ka-8-statistics.db  info 18:31:41 enqueuing flush of compactions_in_progress: 148 (0%) on-heap, 0  0%) off-heap  info 18:31:41 writing memtable-compactions_in_progress@6888852(0 serialized by  es, 1 ops, 0%/0% of on/off-heap limit)  info 18:31:41 completed flushing c:\\cassandra-2.1\\data\\system\\compactions_in_p  ogress-55080ab05d9c388690a4acb25fe1f77b\\system-compactions_in_progress-ka-9-dat  .db (42 bytes) for commitlog position replayposition(segmentid=1402165543361, p  sition=8025563)  error 18:31:41 exception in thread thread[compactionexecutor:6,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:\\cassandra-2.1\\data\\test\\sipdb-5  f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-8-index.db to c:\\cassandra-2.1  data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-8-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:\\cassandra-2.1\\data\\test\\sipdb-  8f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-8-index.db -> c:\\cassandra-2.  \\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-8-index.db: pro  es nem\u00df p\u00b0\u00fdstup k souboru, nebo\u0142 jej pr\u00dfv\u00fd vyu\u00d7\u00fdv\u00df jin\u0159 proces.  at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted  error 18:31:41 exception in thread thread[compactionexecutor:6,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:\\cassandra-2.1\\data\\test\\sipdb-5  f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-8-index.db to c:\\cassandra-2.1  data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-8-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:\\cassandra-2.1\\data\\test\\sipdb-  8f51090ee6511e3815625991ef2b954\\test-sipdb-tmp-ka-8-index.db -> c:\\cassandra-2.  \\data\\test\\sipdb-58f51090ee6511e3815625991ef2b954\\test-sipdb-ka-8-index.db: pro  es nem\u00df p\u00b0\u00fdstup k souboru, nebo\u0142 jej pr\u00dfv\u00fd vyu\u00d7\u00fdv\u00df jin\u0159 proces.  at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted<stacktrace> info 18:30:27 completed flushing c:/cassandra-2.1/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-6-dat.db (42 bytes) for commitlog position replayposition(segmentid=1402165543361, psition=8024611)  error 18:30:27 exception in thread hread[compactionexecutor:5,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:/cassandra-2.1/data/test/sipdb-5  f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-7-index.db to c:/cassandra-2.1  data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-7-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:/cassandra-2.1/data/test/sipdb-  8f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-7-index.db -> c:/cassandra-2.  /data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-7-index.db: pro  es nem pstup k souboru, nebo jej prv vyuv jin proces. at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted  info 18:30:27 compacting [sstablereader(path='c:/cassandra-2.1/data/system/com  actions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_prog  ess-ka-3-data.db'), sstablereader(path='c:/cassandra-2.1/data/system/compaction  _in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka  5-data.db'), sstablereader(path='c:/cassandra-2.1/data/system/compactions_in_pr  gress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-4-data  db'), sstablereader(path='c:/cassandra-2.1/data/system/compactions_in_progress-  5080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-6-data.db')]  error 18:30:27 exception in thread thread[compactionexecutor:5,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:/cassandra-2.1/data/test/sipdb-5  f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-7-index.db to c:/cassandra-2.1  data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-7-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:/cassandra-2.1/data/test/sipdb-  8f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-7-index.db -> c:/cassandra-2.  /data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-7-index.db: pro  es nem pstup k souboru, nebo jej prv vyuv jin proces. at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted  info 18:30:27 compacted 4 sstables to []. 423 bytes to 0 (~0% of original) in  39ms = 0,000000mb/s. 4 total partitions merged to 0. partition merge counts w  re info 18:30:45 enqueuing flush of compactions_in_progress: 1345 (0%) on-heap, 0  (0%) off-heap  info 18:30:45 writing memtable-compactions_in_progress@15659113(153 serialized  bytes, 10 ops, 0%/0% of on/off-heap limit)  info 18:30:45 completed flushing c:/cassandra-2.1/data/system/compactions_in_p  ogress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-8-dat  .db (173 bytes) for commitlog position replayposition(segmentid=1402165543361,  osition=8025407)  info 18:30:45 compacting [sstablereader(path='c:/cassandra-2.1/data/test/sipdb  58f51090ee6511e3815625991ef2b954/test-sipdb-ka-3-data.db'), sstablereader(path=  c:/cassandra-2.1/data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka  1-data.db'), sstablereader(path='c:/cassandra-2.1/data/test/sipdb-58f51090ee651  e3815625991ef2b954/test-sipdb-ka-4-data.db'), sstablereader(path='c:/cassandra-  .1/data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-2-data.db'),  stablereader(path='c:/cassandra-2.1/data/test/sipdb-58f51090ee6511e3815625991ef  b954/test-sipdb-ka-6-data.db')]  error 18:31:41 unable to delete c:/cassandra-2.1/data/test/sipdb-58f51090ee6511  3815625991ef2b954/test-sipdb-ka-8-data.db (it will be removed on server restart  we'll also retry after gc)  error 18:31:41 missing component: c:/cassandra-2.1/data/test/sipdb-58f51090ee65  1e3815625991ef2b954/test-sipdb-tmp-ka-8-digest.sha1  error 18:31:41 missing component: c:/cassandra-2.1/data/test/sipdb-58f51090ee65  1e3815625991ef2b954/test-sipdb-tmp-ka-8-summary.db  error 18:31:41 missing component: c:/cassandra-2.1/data/test/sipdb-58f51090ee65  1e3815625991ef2b954/test-sipdb-tmp-ka-8-statistics.db  info 18:31:41 enqueuing flush of compactions_in_progress: 148 (0%) on-heap, 0  0%) off-heap  info 18:31:41 writing memtable-compactions_in_progress@6888852(0 serialized by  es, 1 ops, 0%/0% of on/off-heap limit)  info 18:31:41 completed flushing c:/cassandra-2.1/data/system/compactions_in_p  ogress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-9-dat  .db (42 bytes) for commitlog position replayposition(segmentid=1402165543361, p  sition=8025563)  error 18:31:41 exception in thread thread[compactionexecutor:6,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:/cassandra-2.1/data/test/sipdb-5  f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-8-index.db to c:/cassandra-2.1  data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-8-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:/cassandra-2.1/data/test/sipdb-  8f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-8-index.db -> c:/cassandra-2.  /data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-8-index.db: pro  es nem pstup k souboru, nebo jej prv vyuv jin proces. at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted  error 18:31:41 exception in thread thread[compactionexecutor:6,1,rmi runtime]  java.lang.runtimeexception: failed to rename c:/cassandra-2.1/data/test/sipdb-5  f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-8-index.db to c:/cassandra-2.1  data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-8-index.db  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.rename(sstablewriter.j  va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.close(sstablewriter.ja  a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablewriter.closeandopenreader(sst  blewriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.sstable.sstablerewriter.finish(sstablerewrit  r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.runwith(compaction  ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.diskawarerunnable.runmaythrow(diskaware  unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:  8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactiontask.executeinternal(co  pactiontask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(ab  tractcompactiontask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompa  tiontask.run(compactionmanager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0  rc1]  at java.util.concurrent.executors$runnableadapter.call(executors.java:4  1) ~[na:1.7.0_60]  at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_  0]  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor  java:1145) ~[na:1.7.0_60]  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecuto  .java:615) [na:1.7.0_60]  at java.lang.thread.run(thread.java:745) [na:1.7.0_60]  caused by: java.nio.file.filesystemexception: c:/cassandra-2.1/data/test/sipdb-  8f51090ee6511e3815625991ef2b954/test-sipdb-tmp-ka-8-index.db -> c:/cassandra-2.  /data/test/sipdb-58f51090ee6511e3815625991ef2b954/test-sipdb-ka-8-index.db: pro  es nem pstup k souboru, nebo jej prv vyuv jin proces. at sun.nio.fs.windowsexception.translatetoioexception(windowsexception.  ava:86) ~[na:1.7.0_60]  at sun.nio.fs.windowsexception.rethrowasioexception(windowsexception.ja  a:97) ~[na:1.7.0_60]  at sun.nio.fs.windowsfilecopy.move(windowsfilecopy.java:301) ~[na:1.7.0  60]  at sun.nio.fs.windowsfilesystemprovider.move(windowsfilesystemprovider.  ava:287) ~[na:1.7.0_60]  at java.nio.file.files.move(files.java:1347) ~[na:1.7.0_60]  at org.apache.cassandra.io.util.fileutils.atomicmovewithfallback(fileut  ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  at org.apache.cassandra.io.util.fileutils.renamewithconfirm(fileutils.j  va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]  ... 19 common frames omitted<code> <text> compaction do not works under windows due to file rename fails: (pro  es nem pstup k souboru, nebo jej prv vyuv jin proces = process can not access file because its in use by another process). not all compactions are broken. compactions done during server startup on system tables works fine. ",
        "label": 280
    },
    {
        "text": "bootstrapping new node causes rowmutationverbhandler couldn't find cfid <description> existing 0.7.0-beta2 cluster adding 1 new node with no data data on it. enable bootstrapping and start new node and received stream of couldn't find cfid, added keyspaces via cli and errors stopped. node did not bootstrap. error [mutation_stage:6] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation  org.apache.cassandra.db.unserializablecolumnfamilyexception: couldn't find cfid=1011  at org.apache.cassandra.db.columnfamilyserializer.deserialize(columnfamilyserializer.java:113)  at org.apache.cassandra.db.rowmutationserializer.defreezethemaps(rowmutation.java:365)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:375)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:333)  at org.apache.cassandra.db.rowmutationverbhandler.doverb(rowmutationverbhandler.java:49)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:50)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  error [mutation_stage:21] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation  org.apache.cassandra.db.unserializablecolumnfamilyexception: couldn't find cfid=1011  at org.apache.cassandra.db.columnfamilyserializer.deserialize(columnfamilyserializer.java:113)  at org.apache.cassandra.db.rowmutationserializer.defreezethemaps(rowmutation.java:365)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:375)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:333)  at org.apache.cassandra.db.rowmutationverbhandler.doverb(rowmutationverbhandler.java:49)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:50)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  error [mutation_stage:31] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation  org.apache.cassandra.db.unserializablecolumnfamilyexception: couldn't find cfid=1016  at org.apache.cassandra.db.columnfamilyserializer.deserialize(columnfamilyserializer.java:113)  at org.apache.cassandra.db.rowmutationserializer.defreezethemaps(rowmutation.java:365)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:375)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:333)  at org.apache.cassandra.db.rowmutationverbhandler.doverb(rowmutationverbhandler.java:49)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:50)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  error [mutation_stage:4] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation<stacktrace> error [mutation_stage:6] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation  org.apache.cassandra.db.unserializablecolumnfamilyexception: couldn't find cfid=1011  at org.apache.cassandra.db.columnfamilyserializer.deserialize(columnfamilyserializer.java:113)  at org.apache.cassandra.db.rowmutationserializer.defreezethemaps(rowmutation.java:365)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:375)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:333)  at org.apache.cassandra.db.rowmutationverbhandler.doverb(rowmutationverbhandler.java:49)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:50)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  error [mutation_stage:21] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation  org.apache.cassandra.db.unserializablecolumnfamilyexception: couldn't find cfid=1011  at org.apache.cassandra.db.columnfamilyserializer.deserialize(columnfamilyserializer.java:113)  at org.apache.cassandra.db.rowmutationserializer.defreezethemaps(rowmutation.java:365)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:375)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:333)  at org.apache.cassandra.db.rowmutationverbhandler.doverb(rowmutationverbhandler.java:49)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:50)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  error [mutation_stage:31] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation  org.apache.cassandra.db.unserializablecolumnfamilyexception: couldn't find cfid=1016  at org.apache.cassandra.db.columnfamilyserializer.deserialize(columnfamilyserializer.java:113)  at org.apache.cassandra.db.rowmutationserializer.defreezethemaps(rowmutation.java:365)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:375)  at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:333)  at org.apache.cassandra.db.rowmutationverbhandler.doverb(rowmutationverbhandler.java:49)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:50)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  error [mutation_stage:4] 2010-10-21 15:46:58,950 rowmutationverbhandler.java (line 81) error in row mutation<code> <text> existing 0.7.0-beta2 cluster adding 1 new node with no data data on it. enable bootstrapping and start new node and received stream of couldn't find cfid, added keyspaces via cli and errors stopped. node did not bootstrap. ",
        "label": 186
    },
    {
        "text": "truncate of a cf should also delete paxos cf <description> we don't delete data from paxos cf during truncate. this will cause data to come back in the next cas round for incomplete commits.   also i am not sure whether we already do this but should we also not truncate hints for that cf.<stacktrace> <code> <text> we don't delete data from paxos cf during truncate. this will cause data to come back in the next cas round for incomplete commits.   also i am not sure whether we already do this but should we also not truncate hints for that cf.",
        "label": 474
    },
    {
        "text": "make node tool exit code non zero when it fails to create snapshot <description> when node tool snapshot is invoked on a bootstrapping node, it does not create the snapshot as expected. however node tool returns a zero exit code in that case. can we make the node tool return a non zero exit code when create snapshot fails?<stacktrace> <code> <text> when node tool snapshot is invoked on a bootstrapping node, it does not create the snapshot as expected. however node tool returns a zero exit code in that case. can we make the node tool return a non zero exit code when create snapshot fails?",
        "label": 85
    },
    {
        "text": "merkletree mismatch for deleted and non existing rows <description> validation compaction will currently create different hashes for rows that have been deleted compared to nodes that have not seen the rows at all or have already compacted them away. in case this sounds familiar to you, see cassandra-4905 which was supposed to prevent hashing of expired tombstones. this still seems to be in place, but does not address the issue completely. or there was a change in 2.0 that rendered the patch ineffective. the problem is that rowhash() in the validator will return a new hash in any case, whether the precompactedrow did actually update the digest or not. this will lead to the case that a purged, precompactedrow will not change the digest, but we end up with a different tree compared to not having rowhash called at all (such as in case the row already doesn't exist). as an implication, repair jobs will constantly detect mismatches between older sstables containing purgable rows and nodes that have already compacted these rows. after transfering the reported ranges, the newly created sstables will immediately get deleted again during the following compaction. this will happen for each repair run over again until the sstable with the purgable row finally gets compacted.<stacktrace> <code> <text> validation compaction will currently create different hashes for rows that have been deleted compared to nodes that have not seen the rows at all or have already compacted them away. in case this sounds familiar to you, see cassandra-4905 which was supposed to prevent hashing of expired tombstones. this still seems to be in place, but does not address the issue completely. or there was a change in 2.0 that rendered the patch ineffective. the problem is that rowhash() in the validator will return a new hash in any case, whether the precompactedrow did actually update the digest or not. this will lead to the case that a purged, precompactedrow will not change the digest, but we end up with a different tree compared to not having rowhash called at all (such as in case the row already doesn't exist). as an implication, repair jobs will constantly detect mismatches between older sstables containing purgable rows and nodes that have already compacted these rows. after transfering the reported ranges, the newly created sstables will immediately get deleted again during the following compaction. this will happen for each repair run over again until the sstable with the purgable row finally gets compacted.",
        "label": 507
    },
    {
        "text": "fix sstablerewritertest on windows <description> platform specific failures:  org.apache.cassandra.io.sstable.sstablerewritertest.testnumberoffiles_truncate  org.apache.cassandra.io.sstable.sstablerewritertest.testsmallfiles  org.apache.cassandra.io.sstable.sstablerewritertest.testnumberoffiles_dont_clean_readers  org.apache.cassandra.io.sstable.sstablerewritertest.testnumberoffiles_finish_empty_new_writer<stacktrace> <code> platform specific failures:  org.apache.cassandra.io.sstable.sstablerewritertest.testnumberoffiles_truncate  org.apache.cassandra.io.sstable.sstablerewritertest.testsmallfiles  org.apache.cassandra.io.sstable.sstablerewritertest.testnumberoffiles_dont_clean_readers  org.apache.cassandra.io.sstable.sstablerewritertest.testnumberoffiles_finish_empty_new_writer<text> ",
        "label": 508
    },
    {
        "text": "commitlog add doesn't really force to disk <description> commitlog.add does't really force writes to disk. this could result in acked writes being lost.<stacktrace> <code> <text> commitlog.add does't really force writes to disk. this could result in acked writes being lost.",
        "label": 274
    },
    {
        "text": "gcinspector logs very different times after cassandra <description> after the gcinspector rewrite in cassandra-7638 the times reported for cms are the full time (including all the concurrent time), not just the stop the world pause time. in previous versions we reported just the stop the world pause time. this change is kind of scary for someone used to the old logs, and is also not as useful. you can't get \"how long were things really stopped\" from the log message any more. for example, this is a cms that got logged in c* 2.1: info  [service thread] 2015-04-03 23:58:37,583  gcinspector.java:142 - concurrentmarksweep gc in 12926ms.  cms old gen: 5305346280 -> 1106799064; par eden space: 223080 -> 158423560; par survivor space: 42081744 -> 51339584 and here is the corresponding information for that cms from the gc log. 2015-04-03t23:58:24.656+0000: 8064.780: [gc [1 cms-initial-mark: 5181002k(6901760k)] 5222315k(7639040k), 0.0316710 secs] [times: user=0.03 sys=0.00, real=0.03 secs]  2015-04-03t23:58:24.688+0000: 8064.812: total time for which application threads were stopped: 0.0324490 seconds 2015-04-03t23:58:24.688+0000: 8064.812: [cms-concurrent-mark-start] 2015-04-03t23:58:26.939+0000: 8067.062: [cms-concurrent-mark: 2.176/2.250 secs] [times: user=12.94 sys=1.73, real=2.25 secs]  2015-04-03t23:58:26.939+0000: 8067.063: [cms-concurrent-preclean-start] 2015-04-03t23:58:27.209+0000: 8067.333: [cms-concurrent-preclean: 0.187/0.270 secs] [times: user=1.53 sys=0.15, real=0.28 secs]  2015-04-03t23:58:27.210+0000: 8067.333: [cms-concurrent-abortable-preclean-start] 2015-04-03t23:58:27.988+0000: 8068.112: [cms-concurrent-abortable-preclean: 0.759/0.779 secs] [times: user=4.07 sys=0.74, real=0.77 secs]  2015-04-03t23:58:27.989+0000: 8068.113: [gc[yg occupancy: 488441 k (737280 k)]2015-04-03t23:58:27.989+0000: 8068.113: [rescan (parallel) , 0.3688960 secs]2015-04-03t23:58:28.358+0000: 8068.482: [weak refs processing, 0.0009620 secs]2015-04-03t23:58:28.359+0000: 8068.483: [class unloading, 0.0060870 secs]2015-04-03t23:58:28.365+0000: 8068.489: [scrub symbol table, 0.0146010 secs]2015-04-03t23:58:28.380+0000: 8068.504: [scrub string table, 0.0031270 secs] [1 cms-remark: 5231445k(6901760k)] 5719886k(7639040k), 0.3953770 secs] [times: user=2.96 sys=0.00, real=0.39 secs]  2015-04-03t23:58:28.385+0000: 8068.508: total time for which application threads were stopped: 0.3962470 seconds 2015-04-03t23:58:28.385+0000: 8068.509: [cms-concurrent-sweep-start] 2015-04-03t23:58:37.582+0000: 8077.706: [cms-concurrent-sweep: 8.661/9.197 secs] [times: user=44.80 sys=9.58, real=9.20 secs]  2015-04-03t23:58:37.589+0000: 8077.713: [cms-concurrent-reset-start] 2015-04-03t23:58:37.633+0000: 8077.757: [cms-concurrent-reset: 0.044/0.044 secs] [times: user=0.19 sys=0.10, real=0.04 secs]  the entire cms took the 12 seconds reported in the gcispector log message. previously we would have only reported the 0.39 seconds that were spent in stw pauses. at the least we need to change the log message so that people don't think we are still just reporting stw time. but it would be more helpful if we could get the stw time and put that into the log message like we had previously.<stacktrace> <code> info  [service thread] 2015-04-03 23:58:37,583  gcinspector.java:142 - concurrentmarksweep gc in 12926ms.  cms old gen: 5305346280 -> 1106799064; par eden space: 223080 -> 158423560; par survivor space: 42081744 -> 51339584 2015-04-03t23:58:24.656+0000: 8064.780: [gc [1 cms-initial-mark: 5181002k(6901760k)] 5222315k(7639040k), 0.0316710 secs] [times: user=0.03 sys=0.00, real=0.03 secs]  2015-04-03t23:58:24.688+0000: 8064.812: total time for which application threads were stopped: 0.0324490 seconds 2015-04-03t23:58:24.688+0000: 8064.812: [cms-concurrent-mark-start] 2015-04-03t23:58:26.939+0000: 8067.062: [cms-concurrent-mark: 2.176/2.250 secs] [times: user=12.94 sys=1.73, real=2.25 secs]  2015-04-03t23:58:26.939+0000: 8067.063: [cms-concurrent-preclean-start] 2015-04-03t23:58:27.209+0000: 8067.333: [cms-concurrent-preclean: 0.187/0.270 secs] [times: user=1.53 sys=0.15, real=0.28 secs]  2015-04-03t23:58:27.210+0000: 8067.333: [cms-concurrent-abortable-preclean-start] 2015-04-03t23:58:27.988+0000: 8068.112: [cms-concurrent-abortable-preclean: 0.759/0.779 secs] [times: user=4.07 sys=0.74, real=0.77 secs]  2015-04-03t23:58:27.989+0000: 8068.113: [gc[yg occupancy: 488441 k (737280 k)]2015-04-03t23:58:27.989+0000: 8068.113: [rescan (parallel) , 0.3688960 secs]2015-04-03t23:58:28.358+0000: 8068.482: [weak refs processing, 0.0009620 secs]2015-04-03t23:58:28.359+0000: 8068.483: [class unloading, 0.0060870 secs]2015-04-03t23:58:28.365+0000: 8068.489: [scrub symbol table, 0.0146010 secs]2015-04-03t23:58:28.380+0000: 8068.504: [scrub string table, 0.0031270 secs] [1 cms-remark: 5231445k(6901760k)] 5719886k(7639040k), 0.3953770 secs] [times: user=2.96 sys=0.00, real=0.39 secs]  2015-04-03t23:58:28.385+0000: 8068.508: total time for which application threads were stopped: 0.3962470 seconds 2015-04-03t23:58:28.385+0000: 8068.509: [cms-concurrent-sweep-start] 2015-04-03t23:58:37.582+0000: 8077.706: [cms-concurrent-sweep: 8.661/9.197 secs] [times: user=44.80 sys=9.58, real=9.20 secs]  2015-04-03t23:58:37.589+0000: 8077.713: [cms-concurrent-reset-start] 2015-04-03t23:58:37.633+0000: 8077.757: [cms-concurrent-reset: 0.044/0.044 secs] [times: user=0.19 sys=0.10, real=0.04 secs]  <text> after the gcinspector rewrite in cassandra-7638 the times reported for cms are the full time (including all the concurrent time), not just the stop the world pause time. in previous versions we reported just the stop the world pause time. this change is kind of scary for someone used to the old logs, and is also not as useful. you can't get 'how long were things really stopped' from the log message any more. for example, this is a cms that got logged in c* 2.1: and here is the corresponding information for that cms from the gc log. the entire cms took the 12 seconds reported in the gcispector log message. previously we would have only reported the 0.39 seconds that were spent in stw pauses. at the least we need to change the log message so that people don't think we are still just reporting stw time. but it would be more helpful if we could get the stw time and put that into the log message like we had previously.",
        "label": 52
    },
    {
        "text": "audit logging   new feature <description> added a page on audit logging, a new feature.  https://github.com/apache/cassandra/pull/403<stacktrace> <code> <text> added a page on audit logging, a new feature.  https://github.com/apache/cassandra/pull/403",
        "label": 145
    },
    {
        "text": " patch  remove redundant unused code <description> code calculates a delete flag and doesn't use it \u2013 removed.<stacktrace> <code> <text> code calculates a delete flag and doesn't use it - removed.",
        "label": 139
    },
    {
        "text": "orphan hint file gets created while node is being removed from cluster <description> i have found this new issue during my test, whenever node is being removed then hint file for that node gets written and stays inside the hint directory forever. i debugged the code and found that it is due to the race condition between hintswriteexecutor.java::flush and hintswriteexecutor.java::closewriter   . time t1 node is down, as a result hints are being written by hintswriteexecutor.java::flush   time t2 node is removed from cluster as a result it calls hintsservice.java-excisestore which removes hint files for the node being removed  time t3 mutation stage keeps pumping hints through hintservice.java::write which again calls hintswriteexecutor.java::flush and new orphan file gets created i was writing a new dtest for {cassandra-13562, cassandra-13308} and that helped me reproduce this new bug. i will submit patch for this new dtest later. i also tried following to check how this orphan hint file responds:  1. i tried nodetool truncatehints <node> but it fails as node is no longer part of the ring  2. i then tried nodetool truncatehints, that still doesn\u2019t remove hint file because it is not yet included in the dispatchdequeue reproducible steps:  please find dtest python file gossip_hang_test.py attached which reproduces this bug. solution:  this is due to race condition as mentioned above. since hintswriteexecutor.java creates thread pool with only 1 worker, so solution becomes little simple. whenever we hintservice.java::excise a host, just store it in-memory, and check for already evicted host inside hintswriteexecutor.java::flush . if already evicted host is found then ignore hints. jaydeep<stacktrace> <code> time t1 node is down, as a result hints are being written by hintswriteexecutor.java::flush   time t2 node is removed from cluster as a result it calls hintsservice.java-excisestore which removes hint files for the node being removed  time t3 mutation stage keeps pumping hints through hintservice.java::write which again calls hintswriteexecutor.java::flush and new orphan file gets created solution:  this is due to race condition as mentioned above. since hintswriteexecutor.java creates thread pool with only 1 worker, so solution becomes little simple. whenever we hintservice.java::excise a host, just store it in-memory, and check for already evicted host inside hintswriteexecutor.java::flush . if already evicted host is found then ignore hints. <text> i have found this new issue during my test, whenever node is being removed then hint file for that node gets written and stays inside the hint directory forever. i debugged the code and found that it is due to the race condition between hintswriteexecutor.java::flush and hintswriteexecutor.java::closewriter   . i was writing a new dtest for and that helped me reproduce this new bug. i will submit patch for this new dtest later. i also tried following to check how this orphan hint file responds:  1. i tried nodetool truncatehints <node> but it fails as node is no longer part of the ring  2. i then tried nodetool truncatehints, that still doesn't remove hint file because it is not yet included in the dispatchdequeue reproducible steps:  please find dtest python file gossip_hang_test.py attached which reproduces this bug. jaydeep",
        "label": 235
    },
    {
        "text": "testjsonthreadsafety is failing   flapping <description> jsontest::testjsonthreadsafety is failing quite often recently:   https://cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-11540-2.2-testall/lastcompletedbuild/testreport/org.apache.cassandra.cql3.validation.entities/jsontest/testjsonthreadsafety/ output looks like stacktrace java.util.concurrent.timeoutexception at java.util.concurrent.futuretask.get(futuretask.java:201) at org.apache.cassandra.cql3.validation.entities.jsontest.testjsonthreadsafety(jsontest.java:1028) warn  12:19:23 small commitlog volume detected at build/test/cassandra/commitlog:30; setting commitlog_total_space_in_mb to 1982.  you can override this in cassandra.yaml warn  12:19:23 small commitlog volume detected at build/test/cassandra/commitlog:30; setting commitlog_total_space_in_mb to 1982.  you can override this in cassandra.yaml warn  12:19:23 only 5581 mb free across all data volumes. consider adding more capacity to your cluster or removing obsolete snapshots warn  12:19:23 only 5581 mb free across all data volumes. consider adding more capacity to your cluster or removing obsolete snapshots warn  12:19:26 aggregation query used without partition key warn  12:19:26 aggregation query used without partition key warn  12:19:26 aggregation query used without partition key warn  12:19:26 aggregation query used without partition key seed 889742091470<stacktrace> stacktrace java.util.concurrent.timeoutexception at java.util.concurrent.futuretask.get(futuretask.java:201) at org.apache.cassandra.cql3.validation.entities.jsontest.testjsonthreadsafety(jsontest.java:1028) warn  12:19:23 small commitlog volume detected at build/test/cassandra/commitlog:30; setting commitlog_total_space_in_mb to 1982.  you can override this in cassandra.yaml warn  12:19:23 small commitlog volume detected at build/test/cassandra/commitlog:30; setting commitlog_total_space_in_mb to 1982.  you can override this in cassandra.yaml warn  12:19:23 only 5581 mb free across all data volumes. consider adding more capacity to your cluster or removing obsolete snapshots warn  12:19:23 only 5581 mb free across all data volumes. consider adding more capacity to your cluster or removing obsolete snapshots warn  12:19:26 aggregation query used without partition key warn  12:19:26 aggregation query used without partition key warn  12:19:26 aggregation query used without partition key warn  12:19:26 aggregation query used without partition key seed 889742091470 <code> jsontest::testjsonthreadsafety is failing quite often recently:   https://cassci.datastax.com/view/dev/view/ifesdjeen/job/ifesdjeen-11540-2.2-testall/lastcompletedbuild/testreport/org.apache.cassandra.cql3.validation.entities/jsontest/testjsonthreadsafety/ <text> output looks like",
        "label": 538
    },
    {
        "text": "npe in hadoop word count example <description> the partition keys requested in wordcount.java do not match the primary key set up in the table output_words. it looks this patch was not merged properly from cassandra-5622.the attached patch addresses the npe and uses the correct keys defined in #5622. i am assuming there is no need to fix the actual npe like throwing an invalidrequestexception back to user to fix the partition keys, as it would be trivial to get the same from the tablemetadata using the driver api. java.lang.nullpointerexception  at org.apache.cassandra.dht.murmur3partitioner.gettoken(murmur3partitioner.java:92)  at org.apache.cassandra.dht.murmur3partitioner.gettoken(murmur3partitioner.java:40)  at org.apache.cassandra.client.ringcache.getrange(ringcache.java:117)  at org.apache.cassandra.hadoop.cql3.cqlrecordwriter.write(cqlrecordwriter.java:163)  at org.apache.cassandra.hadoop.cql3.cqlrecordwriter.write(cqlrecordwriter.java:63)  at org.apache.hadoop.mapred.reducetask$newtrackingrecordwriter.write(reducetask.java:587)  at org.apache.hadoop.mapreduce.taskinputoutputcontext.write(taskinputoutputcontext.java:80)  at wordcount$reducertocassandra.reduce(unknown source)  at wordcount$reducertocassandra.reduce(unknown source)  at org.apache.hadoop.mapreduce.reducer.run(reducer.java:176)  at org.apache.hadoop.mapred.reducetask.runnewreducer(reducetask.java:649)  at org.apache.hadoop.mapred.reducetask.run(reducetask.java:417)  at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:260)<stacktrace> java.lang.nullpointerexception  at org.apache.cassandra.dht.murmur3partitioner.gettoken(murmur3partitioner.java:92)  at org.apache.cassandra.dht.murmur3partitioner.gettoken(murmur3partitioner.java:40)  at org.apache.cassandra.client.ringcache.getrange(ringcache.java:117)  at org.apache.cassandra.hadoop.cql3.cqlrecordwriter.write(cqlrecordwriter.java:163)  at org.apache.cassandra.hadoop.cql3.cqlrecordwriter.write(cqlrecordwriter.java:63)  at org.apache.hadoop.mapred.reducetask$newtrackingrecordwriter.write(reducetask.java:587)  at org.apache.hadoop.mapreduce.taskinputoutputcontext.write(taskinputoutputcontext.java:80)  at wordcount$reducertocassandra.reduce(unknown source)  at wordcount$reducertocassandra.reduce(unknown source)  at org.apache.hadoop.mapreduce.reducer.run(reducer.java:176)  at org.apache.hadoop.mapred.reducetask.runnewreducer(reducetask.java:649)  at org.apache.hadoop.mapred.reducetask.run(reducetask.java:417)  at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:260)<code> <text> the partition keys requested in wordcount.java do not match the primary key set up in the table output_words. it looks this patch was not merged properly from cassandra-5622.the attached patch addresses the npe and uses the correct keys defined in #5622. i am assuming there is no need to fix the actual npe like throwing an invalidrequestexception back to user to fix the partition keys, as it would be trivial to get the same from the tablemetadata using the driver api. ",
        "label": 100
    },
    {
        "text": "undeclare throwable exception while executing 'nodetool netstats localhost' <description> steps 1. stop cassandra service 2. check netstats of nodetool using 'nodetool netstats localhost' 3. start cassandra service 4. again check netstats of nodetool using 'nodetool netstats localhost' expected output  mode: starting  not sending any streams. (end of output - no further exceptions) observed output  nodetool netstats localhost mode: starting not sending any streams. exception in thread \"main\" java.lang.reflect.undeclaredthrowableexception at com.sun.proxy.$proxy6.getreadrepairattempted(unknown source) at org.apache.cassandra.tools.nodeprobe.getreadrepairattempted(nodeprobe.java:897) at org.apache.cassandra.tools.nodecmd.printnetworkstats(nodecmd.java:726) at org.apache.cassandra.tools.nodecmd.main(nodecmd.java:1281) caused by: javax.management.instancenotfoundexception: org.apache.cassandra.db:type=storageproxy at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1095) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getattribute(defaultmbeanserverinterceptor.java:643) at com.sun.jmx.mbeanserver.jmxmbeanserver.getattribute(jmxmbeanserver.java:678) at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1464) at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97) at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328) at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420) at javax.management.remote.rmi.rmiconnectionimpl.getattribute(rmiconnectionimpl.java:657) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322) at sun.rmi.transport.transport$1.run(transport.java:177) at sun.rmi.transport.transport$1.run(transport.java:174) at java.security.accesscontroller.doprivileged(native method) at sun.rmi.transport.transport.servicecall(transport.java:173) at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:553) at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:808) at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:667) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:724) at sun.rmi.transport.streamremotecall.exceptionreceivedfromserver(streamremotecall.java:273) at sun.rmi.transport.streamremotecall.executecall(streamremotecall.java:251) at sun.rmi.server.unicastref.invoke(unicastref.java:160) at com.sun.jmx.remote.internal.pref.invoke(unknown source) at javax.management.remote.rmi.rmiconnectionimpl_stub.getattribute(unknown source) at javax.management.remote.rmi.rmiconnector$remotembeanserverconnection.getattribute(rmiconnector.java:902) at javax.management.mbeanserverinvocationhandler.invoke(mbeanserverinvocationhandler.java:267) ... 4 more<stacktrace>  nodetool netstats localhost mode: starting not sending any streams. exception in thread 'main' java.lang.reflect.undeclaredthrowableexception at com.sun.proxy.$proxy6.getreadrepairattempted(unknown source) at org.apache.cassandra.tools.nodeprobe.getreadrepairattempted(nodeprobe.java:897) at org.apache.cassandra.tools.nodecmd.printnetworkstats(nodecmd.java:726) at org.apache.cassandra.tools.nodecmd.main(nodecmd.java:1281) caused by: javax.management.instancenotfoundexception: org.apache.cassandra.db:type=storageproxy at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getmbean(defaultmbeanserverinterceptor.java:1095) at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.getattribute(defaultmbeanserverinterceptor.java:643) at com.sun.jmx.mbeanserver.jmxmbeanserver.getattribute(jmxmbeanserver.java:678) at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1464) at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97) at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328) at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420) at javax.management.remote.rmi.rmiconnectionimpl.getattribute(rmiconnectionimpl.java:657) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:606) at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322) at sun.rmi.transport.transport$1.run(transport.java:177) at sun.rmi.transport.transport$1.run(transport.java:174) at java.security.accesscontroller.doprivileged(native method) at sun.rmi.transport.transport.servicecall(transport.java:173) at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:553) at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:808) at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:667) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) at java.lang.thread.run(thread.java:724) at sun.rmi.transport.streamremotecall.exceptionreceivedfromserver(streamremotecall.java:273) at sun.rmi.transport.streamremotecall.executecall(streamremotecall.java:251) at sun.rmi.server.unicastref.invoke(unicastref.java:160) at com.sun.jmx.remote.internal.pref.invoke(unknown source) at javax.management.remote.rmi.rmiconnectionimpl_stub.getattribute(unknown source) at javax.management.remote.rmi.rmiconnector$remotembeanserverconnection.getattribute(rmiconnector.java:902) at javax.management.mbeanserverinvocationhandler.invoke(mbeanserverinvocationhandler.java:267) ... 4 more <code> <text> steps expected output  mode: starting  not sending any streams. (end of output - no further exceptions) observed output",
        "label": 98
    },
    {
        "text": "secondary indexes fail following a system restart <description> create a new cf with a secondary index, and queries with indexes predicates work fine until the server is restarted, after which they error and the following stacktrace is output to the log: java.lang.classcastexception: java.math.biginteger cannot be cast to java.nio.bytebuffer at org.apache.cassandra.dht.localtoken.compareto(localtoken.java:44) at org.apache.cassandra.db.decoratedkey.compareto(decoratedkey.java:88) at org.apache.cassandra.db.decoratedkey.compareto(decoratedkey.java:1) at org.apache.cassandra.utils.intervaltree.comparepoints(intervaltree.java:191) at org.apache.cassandra.utils.intervaltree.contains(intervaltree.java:203) at org.apache.cassandra.utils.intervaltree.access$3(intervaltree.java:201) at org.apache.cassandra.utils.intervaltree$intervalnode.searchinternal(intervaltree.java:293) at org.apache.cassandra.utils.intervaltree.search(intervaltree.java:140) at org.apache.cassandra.utils.intervaltree.search(intervaltree.java:146) at org.apache.cassandra.db.columnfamilystore.markreferenced(columnfamilystore.java:1259) at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:229) at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:65) at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1300) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1174) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1104) at org.apache.cassandra.db.index.keys.keyssearcher$1.computenext(keyssearcher.java:144) at org.apache.cassandra.db.index.keys.keyssearcher$1.computenext(keyssearcher.java:1) at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140) at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135) at org.apache.cassandra.db.columnfamilystore.filter(columnfamilystore.java:1409) at org.apache.cassandra.db.index.keys.keyssearcher.search(keyssearcher.java:88) at org.apache.cassandra.db.index.secondaryindexmanager.search(secondaryindexmanager.java:595) at org.apache.cassandra.db.columnfamilystore.search(columnfamilystore.java:1398) at org.apache.cassandra.service.rangesliceverbhandler.executelocally(rangesliceverbhandler.java:47) at org.apache.cassandra.service.storageproxy.getrangeslice(storageproxy.java:870) at org.apache.cassandra.cql3.statements.selectstatement.multirangeslice(selectstatement.java:259) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:134) at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:108) at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:121) at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1236) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:3542) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:1) at org.apache.thrift.processfunction.process(processfunction.java:32) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:184) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) tested with a single node setup & verified that this behaviour is only present in trunk, cassandra-1.0.10 works as expected.<stacktrace> java.lang.classcastexception: java.math.biginteger cannot be cast to java.nio.bytebuffer at org.apache.cassandra.dht.localtoken.compareto(localtoken.java:44) at org.apache.cassandra.db.decoratedkey.compareto(decoratedkey.java:88) at org.apache.cassandra.db.decoratedkey.compareto(decoratedkey.java:1) at org.apache.cassandra.utils.intervaltree.comparepoints(intervaltree.java:191) at org.apache.cassandra.utils.intervaltree.contains(intervaltree.java:203) at org.apache.cassandra.utils.intervaltree.access$3(intervaltree.java:201) at org.apache.cassandra.utils.intervaltree$intervalnode.searchinternal(intervaltree.java:293) at org.apache.cassandra.utils.intervaltree.search(intervaltree.java:140) at org.apache.cassandra.utils.intervaltree.search(intervaltree.java:146) at org.apache.cassandra.db.columnfamilystore.markreferenced(columnfamilystore.java:1259) at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:229) at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:65) at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1300) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1174) at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1104) at org.apache.cassandra.db.index.keys.keyssearcher$1.computenext(keyssearcher.java:144) at org.apache.cassandra.db.index.keys.keyssearcher$1.computenext(keyssearcher.java:1) at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:140) at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:135) at org.apache.cassandra.db.columnfamilystore.filter(columnfamilystore.java:1409) at org.apache.cassandra.db.index.keys.keyssearcher.search(keyssearcher.java:88) at org.apache.cassandra.db.index.secondaryindexmanager.search(secondaryindexmanager.java:595) at org.apache.cassandra.db.columnfamilystore.search(columnfamilystore.java:1398) at org.apache.cassandra.service.rangesliceverbhandler.executelocally(rangesliceverbhandler.java:47) at org.apache.cassandra.service.storageproxy.getrangeslice(storageproxy.java:870) at org.apache.cassandra.cql3.statements.selectstatement.multirangeslice(selectstatement.java:259) at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:134) at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:108) at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:121) at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1236) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:3542) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:1) at org.apache.thrift.processfunction.process(processfunction.java:32) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:184) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) <code> <text> create a new cf with a secondary index, and queries with indexes predicates work fine until the server is restarted, after which they error and the following stacktrace is output to the log: tested with a single node setup & verified that this behaviour is only present in trunk, cassandra-1.0.10 works as expected.",
        "label": 474
    },
    {
        "text": "asynchresult does not respect timeunit in get  <description> when waiting for a blocking get in asynchresult, the parameter timeunit tu is ignored. the passed parameter long timeout is assumed to be milliseconds. attached you will find my quick fix to convert timeout to milliseconds with respect to tu.<stacktrace> <code> <text> when waiting for a blocking get in asynchresult, the parameter timeunit tu is ignored. the passed parameter long timeout is assumed to be milliseconds. attached you will find my quick fix to convert timeout to milliseconds with respect to tu.",
        "label": 327
    },
    {
        "text": "avoid nuking system classpath <description> this will allow using system packages for jna on debian and redhat<stacktrace> <code> <text> this will allow using system packages for jna on debian and redhat",
        "label": 376
    },
    {
        "text": "timestampreconciler should be a singleton <description> like abstracttype<stacktrace> <code> <text> like abstracttype",
        "label": 181
    },
    {
        "text": "concurrentmodificationexception during quorumresponsehandler <description> using cassandra-0.6.0-beta2/ 2010-03-09 09:17:26,827 error [pool-1-thread-675] [cassandra.java:1166] internal error processing get  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.service.quorumresponsehandler.get(quorumresponsehandler.java:68)  at org.apache.cassandra.service.storageproxy.strongread(storageproxy.java:470)  at org.apache.cassandra.service.storageproxy.readprotocol(storageproxy.java:401)  at org.apache.cassandra.thrift.cassandraserver.readcolumnfamily(cassandraserver.java:101)  at org.apache.cassandra.thrift.cassandraserver.multigetinternal(cassandraserver.java:309)  at org.apache.cassandra.thrift.cassandraserver.get(cassandraserver.java:274)  at org.apache.cassandra.thrift.cassandra$processor$get.process(cassandra.java:1156)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:1114)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)<stacktrace> 2010-03-09 09:17:26,827 error [pool-1-thread-675] [cassandra.java:1166] internal error processing get  java.util.concurrentmodificationexception  at java.util.abstractlist$itr.checkforcomodification(abstractlist.java:372)  at java.util.abstractlist$itr.next(abstractlist.java:343)  at org.apache.cassandra.service.quorumresponsehandler.get(quorumresponsehandler.java:68)  at org.apache.cassandra.service.storageproxy.strongread(storageproxy.java:470)  at org.apache.cassandra.service.storageproxy.readprotocol(storageproxy.java:401)  at org.apache.cassandra.thrift.cassandraserver.readcolumnfamily(cassandraserver.java:101)  at org.apache.cassandra.thrift.cassandraserver.multigetinternal(cassandraserver.java:309)  at org.apache.cassandra.thrift.cassandraserver.get(cassandraserver.java:274)  at org.apache.cassandra.thrift.cassandra$processor$get.process(cassandra.java:1156)  at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:1114)  at org.apache.thrift.server.tthreadpoolserver$workerprocess.run(tthreadpoolserver.java:253)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)<code> using cassandra-0.6.0-beta2/ <text> ",
        "label": 274
    },
    {
        "text": "complete bootstrap code <description> the bootstrap code sends files from databasedescriptor.getbootstrapfilelocation to the node that is joining the ring, but nothing actually puts files into that directory. there are probably other bugs / omissions we will find as we flesh this out.<stacktrace> <code> <text> the bootstrap code sends files from databasedescriptor.getbootstrapfilelocation to the node that is joining the ring, but nothing actually puts files into that directory. there are probably other bugs / omissions we will find as we flesh this out.",
        "label": 481
    },
    {
        "text": "http www apache org dist cassandra debian dists 12x main binary amd64 packages is refering cassandra version  but the file cassandra all deb doesn't exist in http www apache org dist cassandra debian pool main c cassandra  <description> <stacktrace> <code> <text> ",
        "label": 521
    },
    {
        "text": "remove primitive collections for java <description> pcj.jar or primitive collections for java is licensed under lgpl and can't be included in the release. http://sourceforge.net/project/shownotes.php?release_id=180015  http://www.apache.org/legal/3party.html<stacktrace> <code> http://sourceforge.net/project/shownotes.php?release_id=180015  http://www.apache.org/legal/3party.html<text> pcj.jar or primitive collections for java is licensed under lgpl and can't be included in the release. ",
        "label": 274
    },
    {
        "text": "cql textile wasn't updated for cassandra <description> cql.textile wasn't updated after cassandra-6839 added inequalities for lwt's.<stacktrace> <code> <text> cql.textile wasn't updated after cassandra-6839 added inequalities for lwt's.",
        "label": 520
    },
    {
        "text": "log output should state the current version  the thrift version  and  if applicable  svn rev <description> currently cassandra logs don't say what version is running. that can create unpleasant ambiguity. the thrift version and the svn rev are also useful; the log line should look like this: cassandra starting up (version x, thrift version y, svn rev z)...<stacktrace> <code> <text> currently cassandra logs don't say what version is running. that can create unpleasant ambiguity. the thrift version and the svn rev are also useful; the log line should look like this: cassandra starting up (version x, thrift version y, svn rev z)...",
        "label": 334
    },
    {
        "text": "commit log allocator deadlock after first start with empty commitlog directory <description> while testing cassandra-3541 at some point stress completely timed out. i proceeded to shut the cluster down and 2/3 jvms hang infinitely. after a while, one of them logged: warn 19:07:50,133 some hints were not written before shutdown.  this is not supposed to happen.  you should (a) run repair, and (b) file a bug report<stacktrace> <code> <text> warn 19:07:50,133 some hints were not written before shutdown.  this is not supposed to happen.  you should (a) run repair, and (b) file a bug report while testing cassandra-3541 at some point stress completely timed out. i proceeded to shut the cluster down and 2/3 jvms hang infinitely. after a while, one of them logged:",
        "label": 448
    },
    {
        "text": "move system cfs into own table <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "wrong memtable size estimation  liveratio is not honored in edge cases <description> memtable.getlivesize does not honours liveratio the correct way:   allocator.get**size() return sizes allocated only by name and columns data (i.e. no liveratio applied); but conditions, which cap estimated size, compare it with estimatedsize, already multiplied by liveratio.  if liveratio is big enough (i've seen >11 on our dataset), this leads to huge estimation errors and even to outofmemory, because meteredflusher underestimates memtables sizes.<stacktrace> <code> <text> memtable.getlivesize does not honours liveratio the correct way:   allocator.get**size() return sizes allocated only by name and columns data (i.e. no liveratio applied); but conditions, which cap estimated size, compare it with estimatedsize, already multiplied by liveratio.  if liveratio is big enough (i've seen >11 on our dataset), this leads to huge estimation errors and even to outofmemory, because meteredflusher underestimates memtables sizes.",
        "label": 274
    },
    {
        "text": "dtest failure in upgrade tests cql tests testcqlnodes2rf1 upgrade current x to indev x invalid custom timestamp test <description> expecting query to be invalid: got <cassandra.cluster.resultset object at 0x7f1847cd7e90> example failure: http://cassci.datastax.com/job/upgrade_tests-all/46/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_2_0_x_to_indev_2_1_x/invalid_custom_timestamp_test failed on cassci build upgrade_tests-all #46<stacktrace> <code> expecting query to be invalid: got <cassandra.cluster.resultset object at 0x7f1847cd7e90> http://cassci.datastax.com/job/upgrade_tests-all/46/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_2_0_x_to_indev_2_1_x/invalid_custom_timestamp_test <text> example failure: failed on cassci build upgrade_tests-all #46",
        "label": 122
    },
    {
        "text": "column values are only being validated in insert  <description> insert() is the only code path that currently results in validate() being called for column values; it is possible to write invalid column values using batch_mutate()<stacktrace> <code> <text> insert() is the only code path that currently results in validate() being called for column values; it is possible to write invalid column values using batch_mutate()",
        "label": 561
    },
    {
        "text": "nullpointerexception when using prepared statements <description> due to the changes in cassandra-4914, using a prepared statement from multiple threads leads to a race condition where the simple selection may be reset from a different thread, causing the following npe: java.lang.nullpointerexception: null at org.apache.cassandra.cql3.resultset.addrow(resultset.java:63) ~[main/:na] at org.apache.cassandra.cql3.statements.selection$resultsetbuilder.build(selection.java:372) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1120) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:283) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:260) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:213) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:63) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:226) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:481) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:133) ~[main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:438) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:334) [main/:na] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_67] at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:163) [main/:na] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:103) [main/:na] at java.lang.thread.run(thread.java:745) [na:1.7.0_67] reproduced this using the stress tool:  ./tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml ops\\(insert=1,simple1=1\\) you'll need to change the select: line to be /1000 to prevent the illegal query exceptions.<stacktrace> java.lang.nullpointerexception: null at org.apache.cassandra.cql3.resultset.addrow(resultset.java:63) ~[main/:na] at org.apache.cassandra.cql3.statements.selection$resultsetbuilder.build(selection.java:372) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1120) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:283) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:260) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:213) ~[main/:na] at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:63) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:226) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:481) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:133) ~[main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:438) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:334) [main/:na] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_67] at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:163) [main/:na] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:103) [main/:na] at java.lang.thread.run(thread.java:745) [na:1.7.0_67] <code>  ./tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml ops/(insert=1,simple1=1/) <text> select: due to the changes in cassandra-4914, using a prepared statement from multiple threads leads to a race condition where the simple selection may be reset from a different thread, causing the following npe: reproduced this using the stress tool: you'll need to change the line to be /1000 to prevent the illegal query exceptions.",
        "label": 69
    },
    {
        "text": "windows  clitest broken because of  r n <description> somebody thought that windows should emulate a telex machine and we ended up with /r/n.<stacktrace> <code> <text> somebody thought that windows should emulate a telex machine and we ended up with /r/n.",
        "label": 68
    },
    {
        "text": "circleci fix   only collect the xml file from containers where it exists <description> followup from cassandra-13775 - my fix with ant eclipse-warnings obviously does not work since it doesn't generate any xml files push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers  test running here: https://circleci.com/gh/krummas/cassandra/86<stacktrace> <code> <text> followup from cassandra-13775 - my fix with ant eclipse-warnings obviously does not work since it doesn't generate any xml files push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers  test running here: https://circleci.com/gh/krummas/cassandra/86",
        "label": 321
    },
    {
        "text": "hsha test closing connections test flapping on <description> see history here: http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/280/testreport/thrift_hsha_test/thrifthshatest/test_closing_connections/history/ the 3 most recent failures were: http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/272/testreport/junit/thrift_hsha_test/thrifthshatest/test_closing_connections/  http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/272/testreport/junit/thrift_hsha_test/thrifthshatest/test_closing_connections/  http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/280/testreport/junit/thrift_hsha_test/thrifthshatest/test_closing_connections/ i haven't seen this failure on cassci on the 2.2 branch, so it may be a regression; i'm not sure. this is related to cassandra-9369; it presents itself the same way. looks like that ticket was closed after a fix was merged to the dtests, but it didn't fix this particular error.<stacktrace> <code> http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/280/testreport/thrift_hsha_test/thrifthshatest/test_closing_connections/history/ http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/272/testreport/junit/thrift_hsha_test/thrifthshatest/test_closing_connections/  http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/272/testreport/junit/thrift_hsha_test/thrifthshatest/test_closing_connections/  http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/280/testreport/junit/thrift_hsha_test/thrifthshatest/test_closing_connections/ <text> see history here: the 3 most recent failures were: i haven't seen this failure on cassci on the 2.2 branch, so it may be a regression; i'm not sure. this is related to cassandra-9369; it presents itself the same way. looks like that ticket was closed after a fix was merged to the dtests, but it didn't fix this particular error.",
        "label": 98
    },
    {
        "text": "fatclient removal causes concurrentmodificationexception <description> after using a fatclient and killing it, i later receive this st on all nodes:  info 16:04:58,999 fatclient /10.242.4.13 has been silent for 3600000ms, removing from gossip  error 16:04:58,999 fatal exception in thread thread[timer-1,5,main]  java.lang.runtimeexception: java.util.concurrentmodificationexception  at org.apache.cassandra.gms.gossiper$gossiptimertask.run(gossiper.java:96)  at java.util.timerthread.mainloop(timer.java:534)  at java.util.timerthread.run(timer.java:484)  caused by: java.util.concurrentmodificationexception  at java.util.hashtable$enumerator.next(hashtable.java:1048)  at org.apache.cassandra.gms.gossiper.dostatuscheck(gossiper.java:382)  at org.apache.cassandra.gms.gossiper$gossiptimertask.run(gossiper.java:90)  ... 2 more<stacktrace> info 16:04:58,999 fatclient /10.242.4.13 has been silent for 3600000ms, removing from gossip  error 16:04:58,999 fatal exception in thread thread[timer-1,5,main]  java.lang.runtimeexception: java.util.concurrentmodificationexception  at org.apache.cassandra.gms.gossiper$gossiptimertask.run(gossiper.java:96)  at java.util.timerthread.mainloop(timer.java:534)  at java.util.timerthread.run(timer.java:484)  caused by: java.util.concurrentmodificationexception  at java.util.hashtable$enumerator.next(hashtable.java:1048)  at org.apache.cassandra.gms.gossiper.dostatuscheck(gossiper.java:382)  at org.apache.cassandra.gms.gossiper$gossiptimertask.run(gossiper.java:90)  ... 2 more<code> <text> after using a fatclient and killing it, i later receive this st on all nodes: ",
        "label": 85
    },
    {
        "text": "limit user types to the keyspace they are defined in <description> i'm not 100% certain this is a bug. the current syntax for \"alter type rename\" requires the keyspace on the old and new table name (if a keyspace is not active). so, to rename the type 'foo' to 'bar', you have to issue this statement:  alter type ks.foo rename to ks.bar . as a result, this syntax will also allow renaming the type into another existing keyspace, which updates the metadata in system.schema_usertypes. i'm wondering if perhaps we can omit the second keyspace prefix and implicitly rename into the same keyspace. to reproduce: cqlsh> create keyspace user_types with replication = {'class':'simplestrategy', 'replication_factor':3} ; cqlsh> create keyspace user_types2 with replication = {'class':'simplestrategy', 'replication_factor':3} ; cqlsh> create type user_types.simple_type (user_number int); cqlsh> alter type user_types.simple_type rename to user_types2.simple_type; renaming to another keyspace is also possible when a keyspace is active, like so: cqlsh:user_types> alter type simple_type rename to user_types2.simple_type;<stacktrace> <code> cqlsh> create keyspace user_types with replication = {'class':'simplestrategy', 'replication_factor':3} ; cqlsh> create keyspace user_types2 with replication = {'class':'simplestrategy', 'replication_factor':3} ; cqlsh> create type user_types.simple_type (user_number int); cqlsh> alter type user_types.simple_type rename to user_types2.simple_type; cqlsh:user_types> alter type simple_type rename to user_types2.simple_type; <text> i'm not 100% certain this is a bug. the current syntax for 'alter type rename' requires the keyspace on the old and new table name (if a keyspace is not active). so, to rename the type 'foo' to 'bar', you have to issue this statement:  alter type ks.foo rename to ks.bar . as a result, this syntax will also allow renaming the type into another existing keyspace, which updates the metadata in system.schema_usertypes. i'm wondering if perhaps we can omit the second keyspace prefix and implicitly rename into the same keyspace. to reproduce: renaming to another keyspace is also possible when a keyspace is active, like so:",
        "label": 520
    },
    {
        "text": "have the word count contrib example use the new baked in hadoop outputformat <description> the contrib/word_count example currently outputs results to the /tmp directory. it would be nice to give an example of writing back to cassandra with the new baked in output format, based on cassandra-1101.<stacktrace> <code> <text> the contrib/word_count example currently outputs results to the /tmp directory. it would be nice to give an example of writing back to cassandra with the new baked in output format, based on cassandra-1101.",
        "label": 246
    },
    {
        "text": " patch  doc  correct section number of opcode in protocol spec <description> this patch just corrected the section number, opcode detail is described in section 2.4<stacktrace> <code> <text> this patch just corrected the section number, opcode detail is described in section 2.4",
        "label": 36
    },
    {
        "text": "need to set ttl with copy command <description> i can import a chunk of data into cassandra table with copy command like: copy my_table (name, address) from my_file.csv with option='value' ... ; but i am not able to specify a finite ttl in copy command with \"using ttl 3600\", for example.<stacktrace> <code> copy my_table (name, address) from my_file.csv with option='value' ... ; <text> i can import a chunk of data into cassandra table with copy command like: but i am not able to specify a finite ttl in copy command with 'using ttl 3600', for example.",
        "label": 508
    },
    {
        "text": "test failure in snitch test testdynamicendpointsnitch test multidatacenter local quorum <description> example failure: http://cassci.datastax.com/job/trunk_large_dtest/48/testreport/snitch_test/testdynamicendpointsnitch/test_multidatacenter_local_quorum error message 75 != 76 stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/tools/decorators.py\", line 48, in wrapped     f(obj)   file \"/home/automaton/cassandra-dtest/snitch_test.py\", line 168, in test_multidatacenter_local_quorum     bad_jmx.read_attribute(read_stage, 'value'))   file \"/usr/lib/python2.7/unittest/case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 506, in _baseassertequal     raise self.failureexception(msg)<stacktrace> <code> error message 75 != 76 stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/tools/decorators.py', line 48, in wrapped     f(obj)   file '/home/automaton/cassandra-dtest/snitch_test.py', line 168, in test_multidatacenter_local_quorum     bad_jmx.read_attribute(read_stage, 'value'))   file '/usr/lib/python2.7/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file '/usr/lib/python2.7/unittest/case.py', line 506, in _baseassertequal     raise self.failureexception(msg) http://cassci.datastax.com/job/trunk_large_dtest/48/testreport/snitch_test/testdynamicendpointsnitch/test_multidatacenter_local_quorum<text> example failure: ",
        "label": 261
    },
    {
        "text": " patch  fix bad comparison in hadoop cf recorder reader <description> code does rows.get(0).columns.get(0).column.equals(startcolumn) which is a column against a bytebuffer changed to rows.get(0).columns.get(0).column.name.equals(startcolumn)<stacktrace> <code> rows.get(0).columns.get(0).column.equals(startcolumn) rows.get(0).columns.get(0).column.name.equals(startcolumn)<text> code does which is a column against a bytebuffer changed to ",
        "label": 139
    },
    {
        "text": " patch  bufferedinputstream skip only skips bytes that are in the buffer  so keep skipping until done <description> code calls skip(remaining) without checking result. skip isn't guaranteed to skip what you requested, especially bufferedinputstream, so keep skipping until the remaining bytes is 0.<stacktrace> <code> <text> code calls skip(remaining) without checking result. skip isn't guaranteed to skip what you requested, especially bufferedinputstream, so keep skipping until the remaining bytes is 0.",
        "label": 139
    },
    {
        "text": "error saving cache on windows <description> i launch clean cassandra 7.2 instalation, and after few days i look at system.log follow error (more then 10 times): error [compactionexecutor:1] 2011-02-19 02:56:17,965 abstractcassandradaemon.java (line 114) fatal exception in thread thread[compactionexecutor:1,1,main]  java.lang.runtimeexception: java.io.ioexception: unable to rename cache to f:\\cassandra\\7.2\\saved_caches\\system-locationinfo-keycache  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.ioexception: unable to rename cache to f:\\cassandra\\7.2\\saved_caches\\system-locationinfo-keycache  at org.apache.cassandra.io.sstable.cachewriter.savecache(cachewriter.java:85)  at org.apache.cassandra.db.compactionmanager$9.runmaythrow(compactionmanager.java:746)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more<stacktrace> error [compactionexecutor:1] 2011-02-19 02:56:17,965 abstractcassandradaemon.java (line 114) fatal exception in thread thread[compactionexecutor:1,1,main]  java.lang.runtimeexception: java.io.ioexception: unable to rename cache to f:/cassandra/7.2/saved_caches/system-locationinfo-keycache  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.io.ioexception: unable to rename cache to f:/cassandra/7.2/saved_caches/system-locationinfo-keycache  at org.apache.cassandra.io.sstable.cachewriter.savecache(cachewriter.java:85)  at org.apache.cassandra.db.compactionmanager$9.runmaythrow(compactionmanager.java:746)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more<code> <text> i launch clean cassandra 7.2 instalation, and after few days i look at system.log follow error (more then 10 times): ",
        "label": 274
    },
    {
        "text": "bytestype and batch mutate causes encoded bytes of non printable characters to be dropped <description> when running the two tests, individual column insert works with the values generated. however, batch insert with the same values causes an encoding failure on the key. it appears bytes are dropped from the end of the byte array that represents the key value. see the attached unit test<stacktrace> <code> <text> when running the two tests, individual column insert works with the values generated. however, batch insert with the same values causes an encoding failure on the key. it appears bytes are dropped from the end of the byte array that represents the key value. see the attached unit test",
        "label": 181
    },
    {
        "text": "improve concurrency in compactionstrategymanager <description> continue discussion from cassandra-9882. compactionstrategymanager(wrappingcompactionstrategy for <3.0) tracks sstable changes mainly for separating repaired / unrepaired sstables (+ lcs manages level). this is blocking operation, and can lead to block of flush etc. when determining next background task takes longer. explore the way to mitigate this concurrency issue.<stacktrace> <code> <text> continue discussion from cassandra-9882. compactionstrategymanager(wrappingcompactionstrategy for <3.0) tracks sstable changes mainly for separating repaired / unrepaired sstables (+ lcs manages level). this is blocking operation, and can lead to block of flush etc. when determining next background task takes longer. explore the way to mitigate this concurrency issue.",
        "label": 321
    },
    {
        "text": "gossipstage blocks because of race in activerepairservice <description> bad luck caused a kernel panic in a cluster, and that took another node with it because gossipstage stopped responding. i think it's pretty obvious what's happening, here are the relevant excerpts from the stack traces : \"thread-24004\" #393781 daemon prio=5 os_prio=0 tid=0x00007efca9647400 nid=0xe75c waiting on condition [0x00007efaa47fe000]    java.lang.thread.state: timed_waiting (parking)     at sun.misc.unsafe.park(native method)     - parking to wait for  <0x000000052b63a7e8> (a java.util.concurrent.countdownlatch$sync)     at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:215)     at java.util.concurrent.locks.abstractqueuedsynchronizer.doacquiresharednanos(abstractqueuedsynchronizer.java:1037)     at java.util.concurrent.locks.abstractqueuedsynchronizer.tryacquiresharednanos(abstractqueuedsynchronizer.java:1328)     at java.util.concurrent.countdownlatch.await(countdownlatch.java:277)     at org.apache.cassandra.service.activerepairservice.prepareforrepair(activerepairservice.java:332)     - locked <0x00000002e6bc99f0> (a org.apache.cassandra.service.activerepairservice)     at org.apache.cassandra.repair.repairrunnable.runmaythrow(repairrunnable.java:211)     at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)     at java.util.concurrent.executors$runnableadapter.call(executors.java:511)                                                                                                           at java.util.concurrent.futuretask.run(futuretask.java:266)     at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)     at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$3/1498438472.run(unknown source)     at java.lang.thread.run(thread.java:748) \"gossiptasks:1\" #367 daemon prio=5 os_prio=0 tid=0x00007efc5e971000 nid=0x700b waiting for monitor entry [0x00007dfb839fe000]    java.lang.thread.state: blocked (on object monitor)     at org.apache.cassandra.service.activerepairservice.removeparentrepairsession(activerepairservice.java:421)     - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.activerepairservice)     at org.apache.cassandra.service.activerepairservice.convict(activerepairservice.java:776)     at org.apache.cassandra.gms.failuredetector.interpret(failuredetector.java:306)     at org.apache.cassandra.gms.gossiper.dostatuscheck(gossiper.java:775)                                                                                                                at org.apache.cassandra.gms.gossiper.access$800(gossiper.java:67)     at org.apache.cassandra.gms.gossiper$gossiptask.run(gossiper.java:187)     at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118)     at java.util.concurrent.executors$runnableadapter.call(executors.java:511)     at java.util.concurrent.futuretask.runandreset(futuretask.java:308)     at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:180)     at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:294)     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)     at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)     at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$3/1498438472.run(unknown source)     at java.lang.thread.run(thread.java:748) \"gossipstage:1\" #320 daemon prio=5 os_prio=0 tid=0x00007efc5b9f2c00 nid=0x6fcd waiting for monitor entry [0x00007e260186a000]    java.lang.thread.state: blocked (on object monitor)     at org.apache.cassandra.service.activerepairservice.removeparentrepairsession(activerepairservice.java:421)     - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.activerepairservice)                                                                                          at org.apache.cassandra.service.activerepairservice.convict(activerepairservice.java:776)     at org.apache.cassandra.service.activerepairservice.onrestart(activerepairservice.java:744)     at org.apache.cassandra.gms.gossiper.handlemajorstatechange(gossiper.java:1049)     at org.apache.cassandra.gms.gossiper.applystatelocally(gossiper.java:1143)     at org.apache.cassandra.gms.gossipdigestack2verbhandler.doverb(gossipdigestack2verbhandler.java:49)     at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:67)     at java.util.concurrent.executors$runnableadapter.call(executors.java:511)     at java.util.concurrent.futuretask.run(futuretask.java:266)     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)     at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)     at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$3/1498438472.run(unknown source)                                                                                       at java.lang.thread.run(thread.java:748) iow, org.apache.cassandra.service.activerepairservice.prepareforrepair holds a lock until the repair is prepared, which means waiting for other nodes to respond, which may die at exactly that moment, so they won't complete. gossip will at the same time try to mark the node as down, but it requires that same lock<stacktrace> 'thread-24004' #393781 daemon prio=5 os_prio=0 tid=0x00007efca9647400 nid=0xe75c waiting on condition [0x00007efaa47fe000]    java.lang.thread.state: timed_waiting (parking)     at sun.misc.unsafe.park(native method)     - parking to wait for  <0x000000052b63a7e8> (a java.util.concurrent.countdownlatch$sync)     at java.util.concurrent.locks.locksupport.parknanos(locksupport.java:215)     at java.util.concurrent.locks.abstractqueuedsynchronizer.doacquiresharednanos(abstractqueuedsynchronizer.java:1037)     at java.util.concurrent.locks.abstractqueuedsynchronizer.tryacquiresharednanos(abstractqueuedsynchronizer.java:1328)     at java.util.concurrent.countdownlatch.await(countdownlatch.java:277)     at org.apache.cassandra.service.activerepairservice.prepareforrepair(activerepairservice.java:332)     - locked <0x00000002e6bc99f0> (a org.apache.cassandra.service.activerepairservice)     at org.apache.cassandra.repair.repairrunnable.runmaythrow(repairrunnable.java:211)     at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)     at java.util.concurrent.executors$runnableadapter.call(executors.java:511)                                                                                                           at java.util.concurrent.futuretask.run(futuretask.java:266)     at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)     at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$3/1498438472.run(unknown source)     at java.lang.thread.run(thread.java:748) 'gossiptasks:1' #367 daemon prio=5 os_prio=0 tid=0x00007efc5e971000 nid=0x700b waiting for monitor entry [0x00007dfb839fe000]    java.lang.thread.state: blocked (on object monitor)     at org.apache.cassandra.service.activerepairservice.removeparentrepairsession(activerepairservice.java:421)     - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.activerepairservice)     at org.apache.cassandra.service.activerepairservice.convict(activerepairservice.java:776)     at org.apache.cassandra.gms.failuredetector.interpret(failuredetector.java:306)     at org.apache.cassandra.gms.gossiper.dostatuscheck(gossiper.java:775)                                                                                                                at org.apache.cassandra.gms.gossiper.access$800(gossiper.java:67)     at org.apache.cassandra.gms.gossiper$gossiptask.run(gossiper.java:187)     at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118)     at java.util.concurrent.executors$runnableadapter.call(executors.java:511)     at java.util.concurrent.futuretask.runandreset(futuretask.java:308)     at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:180)     at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:294)     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)     at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)     at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$3/1498438472.run(unknown source)     at java.lang.thread.run(thread.java:748) 'gossipstage:1' #320 daemon prio=5 os_prio=0 tid=0x00007efc5b9f2c00 nid=0x6fcd waiting for monitor entry [0x00007e260186a000]    java.lang.thread.state: blocked (on object monitor)     at org.apache.cassandra.service.activerepairservice.removeparentrepairsession(activerepairservice.java:421)     - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.activerepairservice)                                                                                          at org.apache.cassandra.service.activerepairservice.convict(activerepairservice.java:776)     at org.apache.cassandra.service.activerepairservice.onrestart(activerepairservice.java:744)     at org.apache.cassandra.gms.gossiper.handlemajorstatechange(gossiper.java:1049)     at org.apache.cassandra.gms.gossiper.applystatelocally(gossiper.java:1143)     at org.apache.cassandra.gms.gossipdigestack2verbhandler.doverb(gossipdigestack2verbhandler.java:49)     at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:67)     at java.util.concurrent.executors$runnableadapter.call(executors.java:511)     at java.util.concurrent.futuretask.run(futuretask.java:266)     at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)     at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)     at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:79)     at org.apache.cassandra.concurrent.namedthreadfactory$$lambda$3/1498438472.run(unknown source)                                                                                       at java.lang.thread.run(thread.java:748) <code> <text> bad luck caused a kernel panic in a cluster, and that took another node with it because gossipstage stopped responding. i think it's pretty obvious what's happening, here are the relevant excerpts from the stack traces : iow, org.apache.cassandra.service.activerepairservice.prepareforrepair holds a lock until the repair is prepared, which means waiting for other nodes to respond, which may die at exactly that moment, so they won't complete. gossip will at the same time try to mark the node as down, but it requires that same lock",
        "label": 490
    },
    {
        "text": "constant compaction under lcs <description> it appears that tables configured with lcs will completely re-compact themselves over some period of time after upgrading from 2.0 to 2.1 (2.0.11 -> 2.1.2, specifically). it starts out with <10 pending tasks for an hour or so, then starts building up, now with 50-100 tasks pending across the cluster after 12 hours. these nodes are under heavy write load, but were easily able to keep up in 2.0 (they rarely had >5 pending compaction tasks), so i don't think it's lcs in 2.1 actually being worse, just perhaps some different lcs behavior that causes the layout of tables from 2.0 to prompt the compactor to reorganize them? the nodes flushed ~11mb sstables under 2.0. they're currently flushing ~36mb sstables due to the improved memtable setup in 2.1. before i upgraded the entire cluster to 2.1, i noticed the problem and tried several variations on the flush size, thinking perhaps the larger tables in l0 were causing some kind of cascading compactions. even if they're sized roughly like the 2.0 flushes were, same behavior occurs. i also tried both enabling & disabling stcs in l0 with no real change other than l0 began to back up faster, so i left the stcs in l0 enabled. tables are configured with 32mb sstable_size_in_mb, which was found to be an improvement on the 160mb table size for compaction performance. maybe this is wrong now? otherwise, the tables are configured with defaults. compaction has been unthrottled to help them catch-up. the compaction threads stay very busy, with the cluster-wide cpu at 45% \"nice\" time. no nodes have completely caught up yet. i'll update jira with status about their progress if anything interesting happens. from a node around 12 hours ago, around an hour after the upgrade, with 19 pending compaction tasks:  sstables in each level: [6/4, 10, 105/100, 268, 0, 0, 0, 0, 0]  sstables in each level: [6/4, 10, 106/100, 271, 0, 0, 0, 0, 0]  sstables in each level: [1, 16/10, 105/100, 269, 0, 0, 0, 0, 0]  sstables in each level: [5/4, 10, 103/100, 272, 0, 0, 0, 0, 0]  sstables in each level: [4, 11/10, 105/100, 270, 0, 0, 0, 0, 0]  sstables in each level: [1, 12/10, 105/100, 271, 0, 0, 0, 0, 0]  sstables in each level: [1, 14/10, 104/100, 267, 0, 0, 0, 0, 0]  sstables in each level: [9/4, 10, 103/100, 265, 0, 0, 0, 0, 0] recently, with 41 pending compaction tasks:  sstables in each level: [4, 13/10, 106/100, 269, 0, 0, 0, 0, 0]  sstables in each level: [4, 12/10, 106/100, 273, 0, 0, 0, 0, 0]  sstables in each level: [5/4, 11/10, 106/100, 271, 0, 0, 0, 0, 0]  sstables in each level: [4, 12/10, 103/100, 275, 0, 0, 0, 0, 0]  sstables in each level: [2, 13/10, 106/100, 273, 0, 0, 0, 0, 0]  sstables in each level: [3, 10, 104/100, 275, 0, 0, 0, 0, 0]  sstables in each level: [6/4, 11/10, 103/100, 269, 0, 0, 0, 0, 0]  sstables in each level: [4, 16/10, 105/100, 264, 0, 0, 0, 0, 0] more information about the use case: writes are roughly uniform across these tables. the data is \"sharded\" across these 8 tables by key to improve compaction parallelism. each node receives up to 75,000 writes/sec sustained at peak, and a small number of reads. this is a pre-production cluster that's being warmed up with new data, so the low volume of reads (~100/sec per node) is just from automatic sampled data checks, otherwise we'd just use stcs<stacktrace> <code> from a node around 12 hours ago, around an hour after the upgrade, with 19 pending compaction tasks:  sstables in each level: [6/4, 10, 105/100, 268, 0, 0, 0, 0, 0]  sstables in each level: [6/4, 10, 106/100, 271, 0, 0, 0, 0, 0]  sstables in each level: [1, 16/10, 105/100, 269, 0, 0, 0, 0, 0]  sstables in each level: [5/4, 10, 103/100, 272, 0, 0, 0, 0, 0]  sstables in each level: [4, 11/10, 105/100, 270, 0, 0, 0, 0, 0]  sstables in each level: [1, 12/10, 105/100, 271, 0, 0, 0, 0, 0]  sstables in each level: [1, 14/10, 104/100, 267, 0, 0, 0, 0, 0]  sstables in each level: [9/4, 10, 103/100, 265, 0, 0, 0, 0, 0] recently, with 41 pending compaction tasks:  sstables in each level: [4, 13/10, 106/100, 269, 0, 0, 0, 0, 0]  sstables in each level: [4, 12/10, 106/100, 273, 0, 0, 0, 0, 0]  sstables in each level: [5/4, 11/10, 106/100, 271, 0, 0, 0, 0, 0]  sstables in each level: [4, 12/10, 103/100, 275, 0, 0, 0, 0, 0]  sstables in each level: [2, 13/10, 106/100, 273, 0, 0, 0, 0, 0]  sstables in each level: [3, 10, 104/100, 275, 0, 0, 0, 0, 0]  sstables in each level: [6/4, 11/10, 103/100, 269, 0, 0, 0, 0, 0]  sstables in each level: [4, 16/10, 105/100, 264, 0, 0, 0, 0, 0] <text> it appears that tables configured with lcs will completely re-compact themselves over some period of time after upgrading from 2.0 to 2.1 (2.0.11 -> 2.1.2, specifically). it starts out with <10 pending tasks for an hour or so, then starts building up, now with 50-100 tasks pending across the cluster after 12 hours. these nodes are under heavy write load, but were easily able to keep up in 2.0 (they rarely had >5 pending compaction tasks), so i don't think it's lcs in 2.1 actually being worse, just perhaps some different lcs behavior that causes the layout of tables from 2.0 to prompt the compactor to reorganize them? the nodes flushed ~11mb sstables under 2.0. they're currently flushing ~36mb sstables due to the improved memtable setup in 2.1. before i upgraded the entire cluster to 2.1, i noticed the problem and tried several variations on the flush size, thinking perhaps the larger tables in l0 were causing some kind of cascading compactions. even if they're sized roughly like the 2.0 flushes were, same behavior occurs. i also tried both enabling & disabling stcs in l0 with no real change other than l0 began to back up faster, so i left the stcs in l0 enabled. tables are configured with 32mb sstable_size_in_mb, which was found to be an improvement on the 160mb table size for compaction performance. maybe this is wrong now? otherwise, the tables are configured with defaults. compaction has been unthrottled to help them catch-up. the compaction threads stay very busy, with the cluster-wide cpu at 45% 'nice' time. no nodes have completely caught up yet. i'll update jira with status about their progress if anything interesting happens. more information about the use case: writes are roughly uniform across these tables. the data is 'sharded' across these 8 tables by key to improve compaction parallelism. each node receives up to 75,000 writes/sec sustained at peak, and a small number of reads. this is a pre-production cluster that's being warmed up with new data, so the low volume of reads (~100/sec per node) is just from automatic sampled data checks, otherwise we'd just use stcs",
        "label": 321
    },
    {
        "text": "c cqlsh describe keyspace   or table   returns 'nonetype' object has no attribute 'replace' <description> c* 2.1 cqlsh describe keyspace ( or table ) returns:  'nonetype' object has no attribute 'replace'  for thrift cf's originally created in c* 1.2. repro: 1. create cf in cassandra-cli on c* 1.2.x (1.2.9 was used here) [default@ks1] create column family t1 ... with column_type='standard' ... and comparator='compositetype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type)' ... and default_validation_class='utf8type' ... and key_validation_class='utf8type' ... and read_repair_chance=0.1 ... and dclocal_read_repair_chance=0.0 ... and gc_grace=864000 ... and min_compaction_threshold=4 ... and max_compaction_threshold=32 ... and replicate_on_write=true ... and compaction_strategy='leveledcompactionstrategy' and compaction_strategy_options={sstable_size_in_mb: 32} ... and caching='keys_only' ... and compression_options={sstable_compression:snappycompressor, chunk_length_kb:64}; qlsh> describe keyspace ks1; create keyspace ks1 with replication = {   'class': 'networktopologystrategy',   'datacenter1': '1' }; use ks1; create table t1 (   key text,   column1 text,   column2 text,   value text,   primary key (key, column1, column2) ) with compact storage and   bloom_filter_fp_chance=0.100000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'sstable_size_in_mb': '32', 'class': 'leveledcompactionstrategy'} and   compression={'chunk_length_kb': '64', 'sstable_compression': 'snappycompressor'}; cqlsh> select keyspace_name, columnfamily_name,column_aliases,key_aliases from system.schema_columnfamilies where keyspace_name= 'ks1';  keyspace_name | columnfamily_name | column_aliases | key_aliases ---------------+-------------------+----------------+-------------            ks1 |                t1 |             [] |          [] 2/ upgrade -> c* 2.0.9 -> nodetool upgradesstables -a at this stage , describe in cqlsh is working 3/ upgrade -> c* 2.1.12 -> nodetool upgradesstables -a describe now fails: cqlsh> describe table ks1.t1; 'nonetype' object has no attribute 'replace' cqlsh> describe keyspace ks1; 'nonetype' object has no attribute 'replace' you can workaround by manually updating system.schema_columnfamilies  update system.schema_columnfamilies set column_aliases ='[\"column1\",\"column2\"]' where keyspace_name = 'ks1' and columnfamily_name = 't1'; once you exit and restart cqlsh, describe is not working as per c* 1.2 cqlsh> describe keyspace ks1; create keyspace ks1 with replication = {'class': 'networktopologystrategy', 'datacenter1': '1'}  and durable_writes = true; create table ks1.t1 (     key text,     column1 text,     column2 text,     value text,     primary key (key, column1, column2) ) with compact storage     and clustering order by (column1 asc, column2 asc)     and caching = '{\"keys\":\"all\", \"rows_per_partition\":\"none\"}'     and comment = ''     and compaction = {'sstable_size_in_mb': '32', 'class': 'org.apache.cassandra.db.compaction.leveledcompactionstrategy'}     and compression = {'chunk_length_kb': '64', 'sstable_compression': 'org.apache.cassandra.io.compress.snappycompressor'}     and dclocal_read_repair_chance = 0.0     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.1     and speculative_retry = '99.0percentile';<stacktrace> <code>  'nonetype' object has no attribute 'replace'  [default@ks1] create column family t1 ... with column_type='standard' ... and comparator='compositetype(org.apache.cassandra.db.marshal.utf8type,org.apache.cassandra.db.marshal.utf8type)' ... and default_validation_class='utf8type' ... and key_validation_class='utf8type' ... and read_repair_chance=0.1 ... and dclocal_read_repair_chance=0.0 ... and gc_grace=864000 ... and min_compaction_threshold=4 ... and max_compaction_threshold=32 ... and replicate_on_write=true ... and compaction_strategy='leveledcompactionstrategy' and compaction_strategy_options={sstable_size_in_mb: 32} ... and caching='keys_only' ... and compression_options={sstable_compression:snappycompressor, chunk_length_kb:64}; qlsh> describe keyspace ks1; create keyspace ks1 with replication = {   'class': 'networktopologystrategy',   'datacenter1': '1' }; use ks1; create table t1 (   key text,   column1 text,   column2 text,   value text,   primary key (key, column1, column2) ) with compact storage and   bloom_filter_fp_chance=0.100000 and   caching='keys_only' and   comment='' and   dclocal_read_repair_chance=0.000000 and   gc_grace_seconds=864000 and   read_repair_chance=0.100000 and   replicate_on_write='true' and   populate_io_cache_on_flush='false' and   compaction={'sstable_size_in_mb': '32', 'class': 'leveledcompactionstrategy'} and   compression={'chunk_length_kb': '64', 'sstable_compression': 'snappycompressor'}; cqlsh> select keyspace_name, columnfamily_name,column_aliases,key_aliases from system.schema_columnfamilies where keyspace_name= 'ks1';  keyspace_name | columnfamily_name | column_aliases | key_aliases ---------------+-------------------+----------------+-------------            ks1 |                t1 |             [] |          [] 2/ upgrade -> c* 2.0.9 -> nodetool upgradesstables -a at this stage , describe in cqlsh is working 3/ upgrade -> c* 2.1.12 -> nodetool upgradesstables -a describe now fails: cqlsh> describe table ks1.t1; 'nonetype' object has no attribute 'replace' cqlsh> describe keyspace ks1; 'nonetype' object has no attribute 'replace'  update system.schema_columnfamilies set column_aliases ='['column1','column2']' where keyspace_name = 'ks1' and columnfamily_name = 't1'; cqlsh> describe keyspace ks1; create keyspace ks1 with replication = {'class': 'networktopologystrategy', 'datacenter1': '1'}  and durable_writes = true; create table ks1.t1 (     key text,     column1 text,     column2 text,     value text,     primary key (key, column1, column2) ) with compact storage     and clustering order by (column1 asc, column2 asc)     and caching = '{'keys':'all', 'rows_per_partition':'none'}'     and comment = ''     and compaction = {'sstable_size_in_mb': '32', 'class': 'org.apache.cassandra.db.compaction.leveledcompactionstrategy'}     and compression = {'chunk_length_kb': '64', 'sstable_compression': 'org.apache.cassandra.io.compress.snappycompressor'}     and dclocal_read_repair_chance = 0.0     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.1     and speculative_retry = '99.0percentile'; 1. create cf in cassandra-cli on c* 1.2.x (1.2.9 was used here) <text> c* 2.1 cqlsh describe keyspace ( or table ) returns: for thrift cf's originally created in c* 1.2. repro: you can workaround by manually updating system.schema_columnfamilies once you exit and restart cqlsh, describe is not working as per c* 1.2",
        "label": 508
    },
    {
        "text": "resolve local address binding in <description> cassandra-8457/cassandra-12229 introduced a regression against cassandra-12673. this was discovered with cassandra-14362 and moved here for resolution independent of that ticket.<stacktrace> <code> <text> cassandra-8457/cassandra-12229 introduced a regression against cassandra-12673. this was discovered with cassandra-14362 and moved here for resolution independent of that ticket.",
        "label": 149
    },
    {
        "text": "range movement causes cpu   performance impact <description> observing big cpu & latency regressions when doing range movements on clusters with many tens of thousands of vnodes. see cpu usage increase by ~80% when a single node is being replaced. top methods are: 1) ljava/math/biginteger;.compareto in lorg/apache/cassandra/dht/comparableobjecttoken;.compareto   2) lcom/google/common/collect/abstractmapbasedmultimap;.wrapcollection in lcom/google/common/collect/abstractmapbasedmultimap$asmap$asmapiterator;.next  3) lorg/apache/cassandra/db/decoratedkey;.compareto in lorg/apache/cassandra/dht/range;.contains here's a sample stack from a thread dump: \"thrift:50673\" daemon prio=10 tid=0x00007f2f20164800 nid=0x3a04af runnable [0x00007f2d878d0000]    java.lang.thread.state: runnable       at org.apache.cassandra.dht.range.iswraparound(range.java:260)       at org.apache.cassandra.dht.range.contains(range.java:51)       at org.apache.cassandra.dht.range.contains(range.java:110)       at org.apache.cassandra.locator.tokenmetadata.pendingendpointsfor(tokenmetadata.java:916)       at org.apache.cassandra.service.storageproxy.performwrite(storageproxy.java:775)       at org.apache.cassandra.service.storageproxy.mutate(storageproxy.java:541)       at org.apache.cassandra.service.storageproxy.mutatewithtriggers(storageproxy.java:616)       at org.apache.cassandra.thrift.cassandraserver.doinsert(cassandraserver.java:1101)       at org.apache.cassandra.thrift.cassandraserver.doinsert(cassandraserver.java:1083)       at org.apache.cassandra.thrift.cassandraserver.batch_mutate(cassandraserver.java:976)       at org.apache.cassandra.thrift.cassandra$processor$batch_mutate.getresult(cassandra.java:3996)       at org.apache.cassandra.thrift.cassandra$processor$batch_mutate.getresult(cassandra.java:3980)       at org.apache.thrift.processfunction.process(processfunction.java:39)       at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39)       at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:205)       at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)       at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)       at java.lang.thread.run(thread.java:745)<stacktrace> 'thrift:50673' daemon prio=10 tid=0x00007f2f20164800 nid=0x3a04af runnable [0x00007f2d878d0000]    java.lang.thread.state: runnable       at org.apache.cassandra.dht.range.iswraparound(range.java:260)       at org.apache.cassandra.dht.range.contains(range.java:51)       at org.apache.cassandra.dht.range.contains(range.java:110)       at org.apache.cassandra.locator.tokenmetadata.pendingendpointsfor(tokenmetadata.java:916)       at org.apache.cassandra.service.storageproxy.performwrite(storageproxy.java:775)       at org.apache.cassandra.service.storageproxy.mutate(storageproxy.java:541)       at org.apache.cassandra.service.storageproxy.mutatewithtriggers(storageproxy.java:616)       at org.apache.cassandra.thrift.cassandraserver.doinsert(cassandraserver.java:1101)       at org.apache.cassandra.thrift.cassandraserver.doinsert(cassandraserver.java:1083)       at org.apache.cassandra.thrift.cassandraserver.batch_mutate(cassandraserver.java:976)       at org.apache.cassandra.thrift.cassandra$processor$batch_mutate.getresult(cassandra.java:3996)       at org.apache.cassandra.thrift.cassandra$processor$batch_mutate.getresult(cassandra.java:3980)       at org.apache.thrift.processfunction.process(processfunction.java:39)       at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39)       at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:205)       at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)       at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)       at java.lang.thread.run(thread.java:745) <code> 1) ljava/math/biginteger;.compareto in lorg/apache/cassandra/dht/comparableobjecttoken;.compareto   2) lcom/google/common/collect/abstractmapbasedmultimap;.wrapcollection in lcom/google/common/collect/abstractmapbasedmultimap$asmap$asmapiterator;.next  3) lorg/apache/cassandra/db/decoratedkey;.compareto in lorg/apache/cassandra/dht/range;.contains <text> observing big cpu & latency regressions when doing range movements on clusters with many tens of thousands of vnodes. see cpu usage increase by ~80% when a single node is being replaced. top methods are: here's a sample stack from a thread dump:",
        "label": 147
    },
    {
        "text": "abstractreplicationstrategy is not extendable from outside the package <description> the constructor for ars is package-private (no modifier), which means that custom rs implementations cannot call it. that is, calling super(...) fails unless you create your replication strategy inside org.apache.cassandra.locator. this obviously isn't ideal. i propose adding protected to the constructor in ars. the attached patch was against trunk however i believe it should work for 2.1, 2.2, 3.0, and 3.x.<stacktrace> <code> <text> the constructor for ars is package-private (no modifier), which means that custom rs implementations cannot call it. that is, calling super(...) fails unless you create your replication strategy inside org.apache.cassandra.locator. this obviously isn't ideal. i propose adding protected to the constructor in ars. the attached patch was against trunk however i believe it should work for 2.1, 2.2, 3.0, and 3.x.",
        "label": 305
    },
    {
        "text": "nodetool scrub fails on system schema with udts <description> [apache-cassandra-2.1.0-rc4]$ bin/cqlsh connected to test cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | cassandra 2.1.0-rc4 | cql spec 3.2.0 | native protocol v3] use help for help. cqlsh> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': 1 }; cqlsh> use test; cqlsh:test> create type point_t (x double, y double); cqlsh:test> exit [apache-cassandra-2.1.0-rc4]$bin/nodetool scrub info  12:34:57 scrubbing sstablereader(path='/apache-cassandra-2.1.0-rc4/bin/../data/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-data.db') (34135 bytes) info  12:34:57 scrub of sstablereader(path='/apache-cassandra-2.1.0-rc4/bin/../data/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-data.db') complete: 2 rows in new sstable and 0 empty (tombstoned) rows dropped info  12:34:57 scrubbing sstablereader(path='/apache-cassandra-2.1.0-rc4/bin/../data/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-5-data.db') (12515 bytes) warn  12:34:57 error reading row (stacktrace follows): org.apache.cassandra.io.sstable.corruptsstableexception: org.apache.cassandra.serializers.marshalexception: not enough bytes to read a set at org.apache.cassandra.io.sstable.sstableidentityiterator.next(sstableidentityiterator.java:139) ~[apache-cassandra-2.1.0-rc4.jar:2.1.0-rc4]<stacktrace> [apache-cassandra-2.1.0-rc4]$bin/nodetool scrub info  12:34:57 scrubbing sstablereader(path='/apache-cassandra-2.1.0-rc4/bin/../data/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-data.db') (34135 bytes) info  12:34:57 scrub of sstablereader(path='/apache-cassandra-2.1.0-rc4/bin/../data/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-data.db') complete: 2 rows in new sstable and 0 empty (tombstoned) rows dropped info  12:34:57 scrubbing sstablereader(path='/apache-cassandra-2.1.0-rc4/bin/../data/data/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-5-data.db') (12515 bytes) warn  12:34:57 error reading row (stacktrace follows): org.apache.cassandra.io.sstable.corruptsstableexception: org.apache.cassandra.serializers.marshalexception: not enough bytes to read a set at org.apache.cassandra.io.sstable.sstableidentityiterator.next(sstableidentityiterator.java:139) ~[apache-cassandra-2.1.0-rc4.jar:2.1.0-rc4]<code> [apache-cassandra-2.1.0-rc4]$ bin/cqlsh connected to test cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | cassandra 2.1.0-rc4 | cql spec 3.2.0 | native protocol v3] use help for help. cqlsh> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': 1 }; cqlsh> use test; cqlsh:test> create type point_t (x double, y double); cqlsh:test> exit <text> ",
        "label": 577
    },
    {
        "text": "cassandra can accept invalid durations <description> a duration can be positive or negative. if the duration is positive the months, days and nanoseconds must be greater or equals to zero. if the duration is negative the months, days and nanoseconds must be smaller or equals to zero.  currently, it is possible to send to c* a duration which does not respect that rule and the data will not be reject.<stacktrace> <code> <text> a duration can be positive or negative. if the duration is positive the months, days and nanoseconds must be greater or equals to zero. if the duration is negative the months, days and nanoseconds must be smaller or equals to zero.  currently, it is possible to send to c* a duration which does not respect that rule and the data will not be reject.",
        "label": 69
    },
    {
        "text": "ioexception in messagingservice run  causes orphaned storage server socket <description> the refactoring of reading the message header in messagingservice.run() vs incomingtcpconnection seems to mishandle ioexception as the loop is broken and messagingservice.socketthread never seems to get reinitialized. to reproduce: telnet to port 7000 and send random data. this then prevents any new or restarting node in the cluster from handshaking with this defunct storage port.<stacktrace> <code> <text> the refactoring of reading the message header in messagingservice.run() vs incomingtcpconnection seems to mishandle ioexception as the loop is broken and messagingservice.socketthread never seems to get reinitialized. to reproduce: telnet to port 7000 and send random data. this then prevents any new or restarting node in the cluster from handshaking with this defunct storage port.",
        "label": 362
    },
    {
        "text": "row cache   streaming aren't aware of each other <description> sstablewriter.builder.build() takes tables that resulted from streaming, repair, bootstrapping, et cetera and builds the indexes and bloom filters before \"adding\" it so the current node is aware of it. however, if there is data present in the cache for a row that is also present in the streamed table the row cache can over shadow the data in the newly built table. in other words, until the row in row cache is removed from the cache (e.g. because it's pushed out because of size, the node is restarted, the cache is manually cleared) the data in the newly built table will never be returned to clients. the solution that seems most reasonable at this point is to have sstablewriter.builder.build() (or something below it) update the row cache if the row key in the table being built is also present in the cache.<stacktrace> <code> <text> sstablewriter.builder.build() takes tables that resulted from streaming, repair, bootstrapping, et cetera and builds the indexes and bloom filters before 'adding' it so the current node is aware of it. however, if there is data present in the cache for a row that is also present in the streamed table the row cache can over shadow the data in the newly built table. in other words, until the row in row cache is removed from the cache (e.g. because it's pushed out because of size, the node is restarted, the cache is manually cleared) the data in the newly built table will never be returned to clients. the solution that seems most reasonable at this point is to have sstablewriter.builder.build() (or something below it) update the row cache if the row key in the table being built is also present in the cache.",
        "label": 520
    },
    {
        "text": " patch  avoid map look ups in a loop by using entryset <description> code uses a keyset iterator to loop over a map, doing map look ups each iteration. switch to entryset() iterator, to avoid that.<stacktrace> <code> <text> code uses a keyset iterator to loop over a map, doing map look ups each iteration. switch to entryset() iterator, to avoid that.",
        "label": 139
    },
    {
        "text": "select count within a partition does not respect limit <description> cassandra@cqlsh> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': '1'}; cassandra@cqlsh> use test; cassandra@cqlsh:test> create table t (k int, c int, v int, primary key (k, c)); cassandra@cqlsh:test> insert into t (k, c, v) values (0, 0, 0); cassandra@cqlsh:test> insert into t (k, c, v) values (0, 1, 0); cassandra@cqlsh:test> insert into t (k, c, v) values (0, 2, 0); cassandra@cqlsh:test> select * from t where k = 0;  k | c | v ---+---+---  0 | 0 | 0  0 | 1 | 0  0 | 2 | 0 (3 rows) cassandra@cqlsh:test> select count(*) from t where k = 0 limit 2;  count -------      3 (1 rows) expected: count should return 2, according to limit.  actual: count of all rows in partition this manifests in 3.0, does not appear in 2.2<stacktrace> <code> cassandra@cqlsh> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': '1'}; cassandra@cqlsh> use test; cassandra@cqlsh:test> create table t (k int, c int, v int, primary key (k, c)); cassandra@cqlsh:test> insert into t (k, c, v) values (0, 0, 0); cassandra@cqlsh:test> insert into t (k, c, v) values (0, 1, 0); cassandra@cqlsh:test> insert into t (k, c, v) values (0, 2, 0); cassandra@cqlsh:test> select * from t where k = 0;  k | c | v ---+---+---  0 | 0 | 0  0 | 1 | 0  0 | 2 | 0 (3 rows) cassandra@cqlsh:test> select count(*) from t where k = 0 limit 2;  count -------      3 (1 rows) <text> expected: count should return 2, according to limit.  actual: count of all rows in partition this manifests in 3.0, does not appear in 2.2",
        "label": 69
    },
    {
        "text": "add legacy notifications backward support on deprecated repair methods <description> forcerepairrangeasync is deprecated in 2.2/3.x series. it's still available for older clients though. unfortunately it sometimes hangs when you call it. it looks like it completes fine but the notification to the client that the operation is done is never sent. this is easiest to see by using nodetool from 2.1 against a 3.x cluster. [nicks-macbook-pro:16:06:21 cassandra-2.1] cassandra$ ./bin/nodetool repair -st 0 -et 1 opscenter [2016-03-24 16:06:50,165] nothing to repair for keyspace 'opscenter' [nicks-macbook-pro:16:06:50 cassandra-2.1] cassandra$ [nicks-macbook-pro:16:06:55 cassandra-2.1] cassandra$ [nicks-macbook-pro:16:06:55 cassandra-2.1] cassandra$ ./bin/nodetool repair -st 0 -et 1 system_distributed ... ... (i added the ellipses)<stacktrace> <code> (i added the ellipses)<text> [nicks-macbook-pro:16:06:21 cassandra-2.1] cassandra$ ./bin/nodetool repair -st 0 -et 1 opscenter [2016-03-24 16:06:50,165] nothing to repair for keyspace 'opscenter' [nicks-macbook-pro:16:06:50 cassandra-2.1] cassandra$ [nicks-macbook-pro:16:06:55 cassandra-2.1] cassandra$ [nicks-macbook-pro:16:06:55 cassandra-2.1] cassandra$ ./bin/nodetool repair -st 0 -et 1 system_distributed ... ... forcerepairrangeasync is deprecated in 2.2/3.x series. it's still available for older clients though. unfortunately it sometimes hangs when you call it. it looks like it completes fine but the notification to the client that the operation is done is never sent. this is easiest to see by using nodetool from 2.1 against a 3.x cluster. ",
        "label": 409
    },
    {
        "text": "cassandra silently loses data when a single row gets large  under  heavy load  <description> when you insert a large number of columns in a single row, cassandra silently loses some or all of these inserts while flushing memtable to disk (potentialy leaving you with zero-sized data files). this happens when the memtable threshold is violated, i.e. when currentsize_ >= threshold_ (memtablesizeinmb) or currentobjectcount_ >= thresholdcount_ (memtableobjectcountinmillions). this was a problem with the old code in code.google and the code with the jdk7 dependencies also. no outofmemory errors are thrown, there is nothing relevant in the logs. it is not clear why this happens under heavy load (when no throttle is used) as it works fine when when you pace requests. i have confirmed this with another member of the community. in storage-conf.xml:  <hashingstrategy>random</hashingstrategy>  <memtablesizeinmb>32</memtablesizeinmb>  <memtableobjectcountinmillions>1</memtableobjectcountinmillions>  <tables>  <table name=\"mytable\">  <columnfamily columntype=\"super\" columnsort=\"name\" name=\"mysuper\"></columnfamily>  </table>  </tables> you can also test it with different values for thresholdcount_ in db/memtable.java, say:  private int thresholdcount_ = 512*1024; here is a small program that will help you reproduce this (hopefully):  private static void dowrite() throws throwable  {  int numrequests=0;  int numrequestspersecond = 3;  table table = table.open(\"mytable\");  random random = new random();  byte[] bytes = new byte[8];  string key = \"mykey\";  int totalused = 0;  int total = 0;  for (int i = 0; i < 1500; ++i) {  rowmutation rm = new rowmutation(\"mytable\", key);  random.nextbytes(bytes);  int[] used = new int[500*1024];  for (int z=0; z<500*1024;z++) { used[z]=0; } int n = random.nextint(16*1024);  for ( int k = 0; k < n; ++k ) {  int j = random.nextint(500*1024);  if ( used[j] == 0 ) { used[j] = 1; ++totalused; //int w = random.nextint(4); int w = 0; rm.add(\"mysuper:supercolumn-\" + j + \":column-\" + i, bytes, w); } }  rm.apply();  total += n;  system.out.println(\"n=\"n \" total=\"+ total+\" totalused=\"+totalused);  //thread.sleep(1000*numrequests/numrequestspersecond);  numrequests++;  }  system.out.println(\"write done\");  } ps. please note that (a) i'm no java guru and (b) i have tried this initially with a c++ thrift client. the outcome is always the same: zero-sized data files under heavy load \u2014 it works fine when you pace requests.<stacktrace> <code> <hashingstrategy>random</hashingstrategy>  <memtablesizeinmb>32</memtablesizeinmb>  <memtableobjectcountinmillions>1</memtableobjectcountinmillions>  <tables>  <table name='mytable'>  <columnfamily columntype='super' columnsort='name' name='mysuper'></columnfamily>  </table>  </tables> you can also test it with different values for thresholdcount_ in db/memtable.java, say:  private int thresholdcount_ = 512*1024; private static void dowrite() throws throwable  {  int numrequests=0;  int numrequestspersecond = 3;  table table = table.open('mytable');  random random = new random();  byte[] bytes = new byte[8];  string key = 'mykey';  int totalused = 0;  int total = 0;  for (int i = 0; i < 1500; ++i) {  rowmutation rm = new rowmutation('mytable', key);  random.nextbytes(bytes);  int[] used = new int[500*1024];  for (int z=0; z<500*1024;z++) int n = random.nextint(16*1024);  for ( int k = 0; k < n; ++k ) {  int j = random.nextint(500*1024);  if ( used[j] == 0 ) }  rm.apply();  total += n;  system.out.println('n='n ' total='+ total+' totalused='+totalused);  //thread.sleep(1000*numrequests/numrequestspersecond);  numrequests++;  }  system.out.println('write done');  } <text> when you insert a large number of columns in a single row, cassandra silently loses some or all of these inserts while flushing memtable to disk (potentialy leaving you with zero-sized data files). this happens when the memtable threshold is violated, i.e. when currentsize_ >= threshold_ (memtablesizeinmb) or currentobjectcount_ >= thresholdcount_ (memtableobjectcountinmillions). this was a problem with the old code in code.google and the code with the jdk7 dependencies also. no outofmemory errors are thrown, there is nothing relevant in the logs. it is not clear why this happens under heavy load (when no throttle is used) as it works fine when when you pace requests. i have confirmed this with another member of the community. in storage-conf.xml: here is a small program that will help you reproduce this (hopefully): ps. please note that (a) i'm no java guru and (b) i have tried this initially with a c++ thrift client. the outcome is always the same: zero-sized data files under heavy load - it works fine when you pace requests.",
        "label": 274
    },
    {
        "text": "make lz4 compression level configurable <description> we'd like to make the lz4 compressor implementation configurable on a per column family basis. testing has shown a ~4% reduction in file size with the higher compression lz4 implementation vs the standard compressor we currently use instantiated by the default constructor. the attached patch adds the following optional parameters 'lz4_compressor_type' and 'lz4_high_compressor_level' to the lz4compressor. if none of the new optional parameters are specified, the compressor will use the same defaults cassandra has always had for lz4. new lz4compressor optional parameters: lz4_compressor_type can currently be either 'high' (uses lz4hccompressor) or 'fast' (uses lz4compressor) lz4_high_compressor_level can be set between 1 and 17. not specifying a compressor level while specifying lz4_compressor_type as 'high' will use a default level of 9 (as picked by the lz4 library as the \"default\"). currently, we use the default lz4 compressor constructor. this change would just expose the level (and implementation to use) to the user via the schema. there are many potential cases where users may find that the tradeoff in additional cpu and memory usage is worth the on-disk space savings.<stacktrace> <code> <text> we'd like to make the lz4 compressor implementation configurable on a per column family basis. testing has shown a ~4% reduction in file size with the higher compression lz4 implementation vs the standard compressor we currently use instantiated by the default constructor. the attached patch adds the following optional parameters 'lz4_compressor_type' and 'lz4_high_compressor_level' to the lz4compressor. if none of the new optional parameters are specified, the compressor will use the same defaults cassandra has always had for lz4. new lz4compressor optional parameters: currently, we use the default lz4 compressor constructor. this change would just expose the level (and implementation to use) to the user via the schema. there are many potential cases where users may find that the tradeoff in additional cpu and memory usage is worth the on-disk space savings.",
        "label": 344
    },
    {
        "text": "allow compaction throttle to be real time <description> we should allow compaction throttle to be set while compaction is going on. currently, it takes effect on the next compaction. this is bad for large compactions.<stacktrace> <code> <text> we should allow compaction throttle to be set while compaction is going on. currently, it takes effect on the next compaction. this is bad for large compactions.",
        "label": 501
    },
    {
        "text": "indexoutofboundsexception in hintedhandoffmanager <description> after upgrading our cluster to 2.2.0, the following error started showing exectly every 10 minutes on every server in the cluster: info  [compactionexecutor:1381] 2015-08-31 18:31:55,506 compactiontask.java:142 - compacting (8e7e1520-500e-11e5-b1e3-e95897ba4d20) [/cassandra/data/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-540-big-data.db:level=0, ] info  [compactionexecutor:1381] 2015-08-31 18:31:55,599 compactiontask.java:224 - compacted (8e7e1520-500e-11e5-b1e3-e95897ba4d20) 1 sstables to [/cassandra/data/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-541-big,] to level=0.  1,544,495 bytes to 1,544,495 (~100% of original) in 93ms = 15.838121mb/s.  0 total partitions merged to 4.  partition merge counts were {1:4, } error [hintedhandoff:1] 2015-08-31 18:31:55,600 cassandradaemon.java:182 - exception in thread thread[hintedhandoff:1,1,main] java.lang.indexoutofboundsexception: null at java.nio.buffer.checkindex(buffer.java:538) ~[na:1.7.0_79] at java.nio.heapbytebuffer.getlong(heapbytebuffer.java:410) ~[na:1.7.0_79] at org.apache.cassandra.utils.uuidgen.getuuid(uuidgen.java:106) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.db.hintedhandoffmanager.schedulealldeliveries(hintedhandoffmanager.java:515) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:88) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.db.hintedhandoffmanager$1.run(hintedhandoffmanager.java:168) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118) ~[apache-cassandra-2.2.0.jar:2.2.0] at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_79] at java.util.concurrent.futuretask.runandreset(futuretask.java:304) [na:1.7.0_79] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:178) [na:1.7.0_79] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:293) [na:1.7.0_79] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [na:1.7.0_79] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_79] at java.lang.thread.run(thread.java:745) [na:1.7.0_79]<stacktrace> info  [compactionexecutor:1381] 2015-08-31 18:31:55,506 compactiontask.java:142 - compacting (8e7e1520-500e-11e5-b1e3-e95897ba4d20) [/cassandra/data/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-540-big-data.db:level=0, ] info  [compactionexecutor:1381] 2015-08-31 18:31:55,599 compactiontask.java:224 - compacted (8e7e1520-500e-11e5-b1e3-e95897ba4d20) 1 sstables to [/cassandra/data/system/hints-2666e20573ef38b390fefecf96e8f0c7/la-541-big,] to level=0.  1,544,495 bytes to 1,544,495 (~100% of original) in 93ms = 15.838121mb/s.  0 total partitions merged to 4.  partition merge counts were {1:4, } error [hintedhandoff:1] 2015-08-31 18:31:55,600 cassandradaemon.java:182 - exception in thread thread[hintedhandoff:1,1,main] java.lang.indexoutofboundsexception: null at java.nio.buffer.checkindex(buffer.java:538) ~[na:1.7.0_79] at java.nio.heapbytebuffer.getlong(heapbytebuffer.java:410) ~[na:1.7.0_79] at org.apache.cassandra.utils.uuidgen.getuuid(uuidgen.java:106) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.db.hintedhandoffmanager.schedulealldeliveries(hintedhandoffmanager.java:515) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:88) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.db.hintedhandoffmanager$1.run(hintedhandoffmanager.java:168) ~[apache-cassandra-2.2.0.jar:2.2.0] at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:118) ~[apache-cassandra-2.2.0.jar:2.2.0] at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_79] at java.util.concurrent.futuretask.runandreset(futuretask.java:304) [na:1.7.0_79] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:178) [na:1.7.0_79] at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:293) [na:1.7.0_79] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [na:1.7.0_79] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_79] at java.lang.thread.run(thread.java:745) [na:1.7.0_79] <code> <text> after upgrading our cluster to 2.2.0, the following error started showing exectly every 10 minutes on every server in the cluster:",
        "label": 218
    },
    {
        "text": "allow jvm opts to be passed to sstablescrub <description> can you add a feature request to pass jvm_opts to the sstablescrub script \u2013 and other places where java is being called? (among other things, this lets us run java stuff with \"-djava.awt.headless=true\" on os x so that java processes don't pop up into the foreground \u2013 i.e. we have a script that loops over all cfs and runs sstablescrub, and without that flag being passed in the os x machine becomes pretty much unusable as it keeps switching focus to the java processes as they start.) \u2014 a/resources/cassandra/bin/sstablescrub  +++ b/resources/cassandra/bin/sstablescrub  @@ -70,7 +70,7 @@ if [ \"x$max_heap_size\" = \"x\" ]; then  max_heap_size=\"256m\"  fi -$java -ea -cp $classpath -xmx$max_heap_size \\  +$java $jvm_opts -ea -cp $classpath -xmx$max_heap_size \\  -dlog4j.configuration=log4j-tools.properties \\  org.apache.cassandra.tools.standalonescrubber \"$@\"<stacktrace> <code> can you add a feature request to pass jvm_opts to the sstablescrub script - and other places where java is being called? (among other things, this lets us run java stuff with '-djava.awt.headless=true' on os x so that java processes don't pop up into the foreground - i.e. we have a script that loops over all cfs and runs sstablescrub, and without that flag being passed in the os x machine becomes pretty much unusable as it keeps switching focus to the java processes as they start.) - a/resources/cassandra/bin/sstablescrub  +++ b/resources/cassandra/bin/sstablescrub  @@ -70,7 +70,7 @@ if [ 'x$max_heap_size' = 'x' ]; then  max_heap_size='256m'  fi -$java -ea -cp $classpath -xmx$max_heap_size /  +$java $jvm_opts -ea -cp $classpath -xmx$max_heap_size /  -dlog4j.configuration=log4j-tools.properties /  org.apache.cassandra.tools.standalonescrubber '$@'<text> ",
        "label": 508
    },
    {
        "text": "stop teeingappender on shutdown hook <description> stefania discovered that tests that don't produce a lot of log output end up producing 0 debug output to files because the data is not flushed as part of the shutdown hook. i traced through and it looks like the shutdown hook doesn't actually invoke code that does anything useful. it shuts down an executor service in the logging context but doesn't call stop on any appenders. a hackish thing we can do is use a status listener to collect all the appenders and then stop them when the shutdown hook runs. even adding a small delay to the shutdown hook (no code changes on our part) would in let the async appender flush in 90% of cases. we still need to fix it for test which uses a different config file and for which a small delay is not desirable.<stacktrace> <code> <text> stefania discovered that tests that don't produce a lot of log output end up producing 0 debug output to files because the data is not flushed as part of the shutdown hook. i traced through and it looks like the shutdown hook doesn't actually invoke code that does anything useful. it shuts down an executor service in the logging context but doesn't call stop on any appenders. a hackish thing we can do is use a status listener to collect all the appenders and then stop them when the shutdown hook runs. even adding a small delay to the shutdown hook (no code changes on our part) would in let the async appender flush in 90% of cases. we still need to fix it for test which uses a different config file and for which a small delay is not desirable.",
        "label": 52
    },
    {
        "text": "there should be an easy way to find out which sstables a key lives in <description> when debugging, often times on a live server you want to extract a certain key with sst2j, but unfortunately you can't know which sstable(s) you need to run this on, causing you to iterate over much more data than necessary.<stacktrace> <code> <text> when debugging, often times on a live server you want to extract a certain key with sst2j, but unfortunately you can't know which sstable(s) you need to run this on, causing you to iterate over much more data than necessary.",
        "label": 85
    },
    {
        "text": "cqlsh fails with undefined symbol  pyunicodeucs2 decodeutf8 <description> trying to run cqlsh produces: cqlsh  traceback (most recent call last):  file \"/usr/bin/cqlsh.py\", line 170, in <module>  from cqlshlib.copyutil import exporttask, importtask  importerror: /usr/lib/python2.7/site-packages/cqlshlib/copyutil.so: undefined symbol: pyunicodeucs2_decodeutf8 with 3.4 the error does not happen.<stacktrace> <code> cqlsh  traceback (most recent call last):  file '/usr/bin/cqlsh.py', line 170, in <module>  from cqlshlib.copyutil import exporttask, importtask  importerror: /usr/lib/python2.7/site-packages/cqlshlib/copyutil.so: undefined symbol: pyunicodeucs2_decodeutf8 <text> trying to run cqlsh produces: with 3.4 the error does not happen.",
        "label": 348
    },
    {
        "text": "preparedstatements get mixed up between keyspaces <description> i found this behavior while running the same application using two different keyspaces connected to the same node. the prepared statements uses the keyspace that was set while the statement was perpared (public final cfdefinition cfdef).  when reusing the statement only the cql-query is used to create a key and the keyspace is ignored. when the same query is prepared and used for two different keyspaces the wrong keyspace can be used. the fix is not to ignore the keyspace when reusing the statement.<stacktrace> <code> <text> i found this behavior while running the same application using two different keyspaces connected to the same node. the prepared statements uses the keyspace that was set while the statement was perpared (public final cfdefinition cfdef).  when reusing the statement only the cql-query is used to create a key and the keyspace is ignored. when the same query is prepared and used for two different keyspaces the wrong keyspace can be used. the fix is not to ignore the keyspace when reusing the statement.",
        "label": 142
    },
    {
        "text": "removetoken is broken <description> when running removetoken on a dead node, it hangs forever. the debug log shows: debug 19:45:53,794 node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]  debug 19:45:53,795 range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102  debug 19:45:53,798 pending ranges:  cassandra-1/10.179.65.102:(115049868157599339472315320703867977321,62676456546693435176060154681903071729] debug 19:45:53,798 node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]  debug 19:45:53,799 range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102<stacktrace> <code> debug 19:45:53,794 node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]  debug 19:45:53,795 range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102  debug 19:45:53,798 pending ranges:  cassandra-1/10.179.65.102:(115049868157599339472315320703867977321,62676456546693435176060154681903071729] debug 19:45:53,798 node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]  debug 19:45:53,799 range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102<text> when running removetoken on a dead node, it hangs forever. the debug log shows: ",
        "label": 376
    },
    {
        "text": "in flight shadow round requests <description> bootstrapping or replacing a node in the cluster requires to gather and check some host ids or tokens by doing a gossip \"shadow round\" once before joining the cluster. this is done by sending a gossip syn to all seeds until we receive a response with the cluster state, from where we can move on in the bootstrap process. receiving a response will call the shadow round done and calls gossiper.resetendpointstatemap for cleaning up the received state again. the issue here is that at this point there might be other in-flight requests and it's very likely that shadow round responses from other seeds will be received afterwards, while the current state of the bootstrap process doesn't expect this to happen (e.g. gossiper may or may not be enabled). one side effect will be that migrationtasks are spawned for each shadow round reply except the first. tasks might or might not execute based on whether at execution time gossiper.resetendpointstatemap had been called, which effects the outcome of failuredetector.instance.isalive(endpoint)) at start of the task. you'll see error log messages such as follows when this happend: info  [sharedpool-worker-1] 2016-09-08 08:36:39,255 gossiper.java:993 - inetaddress /xx.xx.xx.xx is now up error [migrationstage:1]    2016-09-08 08:36:39,255 failuredetector.java:223 - unknown endpoint /xx.xx.xx.xx although is isn't pretty, i currently don't see any serious harm from this, but it would be good to get a second opinion (feel free to close as \"wont fix\"). /cc stefania alborghetti tom hobbs<stacktrace> <code> info  [sharedpool-worker-1] 2016-09-08 08:36:39,255 gossiper.java:993 - inetaddress /xx.xx.xx.xx is now up error [migrationstage:1]    2016-09-08 08:36:39,255 failuredetector.java:223 - unknown endpoint /xx.xx.xx.xx <text> bootstrapping or replacing a node in the cluster requires to gather and check some host ids or tokens by doing a gossip 'shadow round' once before joining the cluster. this is done by sending a gossip syn to all seeds until we receive a response with the cluster state, from where we can move on in the bootstrap process. receiving a response will call the shadow round done and calls gossiper.resetendpointstatemap for cleaning up the received state again. the issue here is that at this point there might be other in-flight requests and it's very likely that shadow round responses from other seeds will be received afterwards, while the current state of the bootstrap process doesn't expect this to happen (e.g. gossiper may or may not be enabled). one side effect will be that migrationtasks are spawned for each shadow round reply except the first. tasks might or might not execute based on whether at execution time gossiper.resetendpointstatemap had been called, which effects the outcome of failuredetector.instance.isalive(endpoint)) at start of the task. you'll see error log messages such as follows when this happend: although is isn't pretty, i currently don't see any serious harm from this, but it would be good to get a second opinion (feel free to close as 'wont fix'). /cc stefania alborghetti tom hobbs",
        "label": 507
    },
    {
        "text": "fat client nodes dont schedule schema pull on connect <description> so they cannot connect for a long time<stacktrace> <code> <text> so they cannot connect for a long time",
        "label": 392
    },
    {
        "text": "cassandra shuffle not working with authentication <description> when enabling authentication for a cassandra cluster the tool cassandra-shuffle is unable to connect. the reason is, that cassandra-shuffle doesn't take any parameter for username and password for the thrift connection. to solve that problem, parameter for username and password should be added, it should also be able to interpret cqlshrc or a separate file file with authentication data.<stacktrace> <code> <text> when enabling authentication for a cassandra cluster the tool cassandra-shuffle is unable to connect. the reason is, that cassandra-shuffle doesn't take any parameter for username and password for the thrift connection. to solve that problem, parameter for username and password should be added, it should also be able to interpret cqlshrc or a separate file file with authentication data.",
        "label": 18
    },
    {
        "text": "does not preserve compatibility w  index queries against nodes <description> 1.1 merged index + seq scan paths into rangeslicecommand. 1.1 storageproxy always sends a rsc for either scan type. but 1.0 rsvh only does seq scans.<stacktrace> <code> <text> 1.1 merged index + seq scan paths into rangeslicecommand. 1.1 storageproxy always sends a rsc for either scan type. but 1.0 rsvh only does seq scans.",
        "label": 520
    },
    {
        "text": "update build xml and build properties default maven repos <description> only 2 of the 5 urls in build.properties.default are currently valid. java.net2, jclouds, and oauth urls all 404. $ git grep remoterepository build.properties.default:artifact.remoterepository.central:     http://repo1.maven.org/maven2 build.properties.default:artifact.remoterepository.java.net2:   http://download.java.net/maven/2 build.properties.default:artifact.remoterepository.apache:      https://repository.apache.org/content/repositories/releases build.properties.default:artifact.remoterepository.jclouds:     http://jclouds.googlecode.com/svn/repo build.properties.default:artifact.remoterepository.oauth:       http://oauth.googlecode.com/svn/code/maven build.xml:      <artifact:remoterepository id=\"central\"   url=\"${artifact.remoterepository.central}\"/> build.xml:      <artifact:remoterepository id=\"java.net2\" url=\"${artifact.remoterepository.java.net2}\"/> build.xml:      <artifact:remoterepository id=\"apache\"    url=\"${artifact.remoterepository.apache}\"/> build.xml:          <remoterepository refid=\"central\"/> build.xml:          <remoterepository refid=\"apache\"/> build.xml:          <remoterepository refid=\"java.net2\"/> build.xml:          <remoterepository refid=\"central\"/> build.xml:          <remoterepository refid=\"apache\"/> build.xml:          <remoterepository refid=\"java.net2\"/> build.xml:          <remoterepository refid=\"central\"/> build.xml:        <remoterepository refid=\"apache\"/> build.xml:        <remoterepository refid=\"central\"/> build.xml:        <remoterepository refid=\"oauth\"/><stacktrace> <code> $ git grep remoterepository build.properties.default:artifact.remoterepository.central:     http://repo1.maven.org/maven2 build.properties.default:artifact.remoterepository.java.net2:   http://download.java.net/maven/2 build.properties.default:artifact.remoterepository.apache:      https://repository.apache.org/content/repositories/releases build.properties.default:artifact.remoterepository.jclouds:     http://jclouds.googlecode.com/svn/repo build.properties.default:artifact.remoterepository.oauth:       http://oauth.googlecode.com/svn/code/maven build.xml:      <artifact:remoterepository id='central'   url='${artifact.remoterepository.central}'/> build.xml:      <artifact:remoterepository id='java.net2' url='${artifact.remoterepository.java.net2}'/> build.xml:      <artifact:remoterepository id='apache'    url='${artifact.remoterepository.apache}'/> build.xml:          <remoterepository refid='central'/> build.xml:          <remoterepository refid='apache'/> build.xml:          <remoterepository refid='java.net2'/> build.xml:          <remoterepository refid='central'/> build.xml:          <remoterepository refid='apache'/> build.xml:          <remoterepository refid='java.net2'/> build.xml:          <remoterepository refid='central'/> build.xml:        <remoterepository refid='apache'/> build.xml:        <remoterepository refid='central'/> build.xml:        <remoterepository refid='oauth'/> <text> only 2 of the 5 urls in build.properties.default are currently valid. java.net2, jclouds, and oauth urls all 404.",
        "label": 453
    },
    {
        "text": "when loading an abstracttype that does not include an instance field  an unhelpful exception is raised making diagnosis difficult <description> 0.7.0 changes the contract for creating abstracttypes. a custom abstracttype defined against 0.6.x will be incompatible and the error messaging around why the comparator is invalid is obtuse and non-obvious. specifically, when porting a valid abstracttype from 0.6.x to 0.7 that does not include a public static instance field, the thrift system_add_column_family call will throw an exception whose only message is: invalidrequestexception(why:instance) no log messages are generated from the server as to the issue so the root cause is non obvious to developers. i marked as major because types defined against 0.6.x did not require an \"instance\" field so at a minimum migration of abstracttypes to 0.7 should document the change in what is expected of abstracttypes. patch attached for better logging and to create a more helpful exception for better communication to the client as to the issue.<stacktrace> <code> invalidrequestexception(why:instance) <text> 0.7.0 changes the contract for creating abstracttypes. a custom abstracttype defined against 0.6.x will be incompatible and the error messaging around why the comparator is invalid is obtuse and non-obvious. specifically, when porting a valid abstracttype from 0.6.x to 0.7 that does not include a public static instance field, the thrift system_add_column_family call will throw an exception whose only message is: no log messages are generated from the server as to the issue so the root cause is non obvious to developers. i marked as major because types defined against 0.6.x did not require an 'instance' field so at a minimum migration of abstracttypes to 0.7 should document the change in what is expected of abstracttypes. patch attached for better logging and to create a more helpful exception for better communication to the client as to the issue.",
        "label": 177
    },
    {
        "text": "improve dsnitch severity <description> this ticket is to continue the discussion in cassandra-5255. currently dsnitch reports the severity by calculating the amount of data compacting relative to load of the node.   it will be nice to report severity as a factor of load average, n/w throughput and io wait instead. http://www.hyperic.com/products/sigar seem to have it (but personally i have not used it).<stacktrace> <code> <text> this ticket is to continue the discussion in cassandra-5255. currently dsnitch reports the severity by calculating the amount of data compacting relative to load of the node.   it will be nice to report severity as a factor of load average, n/w throughput and io wait instead. http://www.hyperic.com/products/sigar seem to have it (but personally i have not used it).",
        "label": 555
    },
    {
        "text": "nodetool test testnodetool test describecluster more information three datacenters should be only <description> https://builds.apache.org/view/a-d/view/cassandra/job/cassandra-trunk-dtest/541/testreport/junit/nodetool_test/testnodetool/test_describecluster_more_information_three_datacenters/ assertionerror: assert 'cluster info...1=3, dc3=1}\\n' == 'cluster infor...1=3, dc3=1}\\n'     cluster information:      name: test      snitch: org.apache.cassandra.locator.propertyfilesnitch      dynamicendpointsnitch: enabled      partitioner: org.apache.cassandra.dht.murmur3partitioner      schema versions:       37963ad1-c76f-3155-b164-88e3a1b7a86b: [127.0.0.6, 127.0.0.5, 127.0.0.4, 127.0.0.3, 127.0.0.2, 127.0.0.1]          stats for all nodes:      live: 6      joining: 0      moving: 0      leaving: 0      unreachable: 0          data centers:       dc1 #nodes: 2 #down: 0      dc2 #nodes: 3 #down: 0      dc3 #nodes: 1 #down: 0          database versions:      4.0.0: [127.0.0.6:7000, 127.0.0.5:7000, 127.0.0.4:7000, 127.0.0.3:7000, 127.0.0.2:7000, 127.0.0.1:7000]          keyspaces:      system_schema -> replication class: localstrategy {}      system -> replication class: localstrategy {}      system_traces -> replication class: simplestrategy {replication_factor=2}   +  system_auth -> replication class: simplestrategy {replication_factor=1}      system_distributed -> replication class: simplestrategy {replication_factor=3}   -  system_auth -> replication class: simplestrategy {replication_factor=1}      ks1 -> replication class: networktopologystrategy {dc2=5, dc1=3, dc3=1}      ks2 -> replication class: networktopologystrategy {dc2=5, dc1=3, dc3=1}<stacktrace> <code> assertionerror: assert 'cluster info...1=3, dc3=1}/n' == 'cluster infor...1=3, dc3=1}/n'     cluster information:      name: test      snitch: org.apache.cassandra.locator.propertyfilesnitch      dynamicendpointsnitch: enabled      partitioner: org.apache.cassandra.dht.murmur3partitioner      schema versions:       37963ad1-c76f-3155-b164-88e3a1b7a86b: [127.0.0.6, 127.0.0.5, 127.0.0.4, 127.0.0.3, 127.0.0.2, 127.0.0.1]          stats for all nodes:      live: 6      joining: 0      moving: 0      leaving: 0      unreachable: 0          data centers:       dc1 #nodes: 2 #down: 0      dc2 #nodes: 3 #down: 0      dc3 #nodes: 1 #down: 0          database versions:      4.0.0: [127.0.0.6:7000, 127.0.0.5:7000, 127.0.0.4:7000, 127.0.0.3:7000, 127.0.0.2:7000, 127.0.0.1:7000]          keyspaces:      system_schema -> replication class: localstrategy {}      system -> replication class: localstrategy {}      system_traces -> replication class: simplestrategy {replication_factor=2}   +  system_auth -> replication class: simplestrategy {replication_factor=1}      system_distributed -> replication class: simplestrategy {replication_factor=3}   -  system_auth -> replication class: simplestrategy {replication_factor=1}      ks1 -> replication class: networktopologystrategy {dc2=5, dc1=3, dc3=1}      ks2 -> replication class: networktopologystrategy {dc2=5, dc1=3, dc3=1} https://builds.apache.org/view/a-d/view/cassandra/job/cassandra-trunk-dtest/541/testreport/junit/nodetool_test/testnodetool/test_describecluster_more_information_three_datacenters/<text> ",
        "label": 234
    },
    {
        "text": "windows  address potential jvm swapping <description> similar to mlockall() in clibrary.java for linux, it would be nice to lock the virtual address space on windows to prevent page faults. one option: reference api: http://msdn.microsoft.com/en-us/library/windows/desktop/aa366895(v=vs.85).aspx<stacktrace> <code> one option: reference api: http://msdn.microsoft.com/en-us/library/windows/desktop/aa366895(v=vs.85).aspx<text> similar to mlockall() in clibrary.java for linux, it would be nice to lock the virtual address space on windows to prevent page faults. ",
        "label": 280
    },
    {
        "text": "deflate compression corrupts sstables <description> hi, it seems that the deflate compressor corrupts the sstables. 3 out of 3 installations were corrupt. snappy works fine. here is what i did: 1. start a single cassandra node (i was using byteorderedpartitioner)  2. write data into cf that uses deflate compression - i think it has to be enough data so that the data folder contains some files.  3. when i now try to read (i did a range scan) from my application, it fails and the logs show corruptions: caused by: org.apache.cassandra.io.compress.corruptedblockexception: (/home/cspriegel/development/cassandra1/data/test/response-h-2-data.db): corruption detected, chunk at 0 of length 65536. regards,  christian<stacktrace> <code> <text> hi, it seems that the deflate compressor corrupts the sstables. 3 out of 3 installations were corrupt. snappy works fine. here is what i did: 1. start a single cassandra node (i was using byteorderedpartitioner)  2. write data into cf that uses deflate compression - i think it has to be enough data so that the data folder contains some files.  3. when i now try to read (i did a range scan) from my application, it fails and the logs show corruptions: caused by: org.apache.cassandra.io.compress.corruptedblockexception: (/home/cspriegel/development/cassandra1/data/test/response-h-2-data.db): corruption detected, chunk at 0 of length 65536. regards,  christian",
        "label": 520
    },
    {
        "text": "fix flaky test org apache cassandra streaming streamtransfertasktest testfailsessionduringtransfershouldnotreleasereferences <description> junit.framework.assertionfailederror: expected:<0> but was:<1> at org.apache.cassandra.streaming.streamtransfertasktest.testfailsessionduringtransfershouldnotreleasereferences(streamtransfertasktest.java:181) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) failure was on java 11<stacktrace> junit.framework.assertionfailederror: expected:<0> but was:<1> at org.apache.cassandra.streaming.streamtransfertasktest.testfailsessionduringtransfershouldnotreleasereferences(streamtransfertasktest.java:181) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke0(native method) at java.base/jdk.internal.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at java.base/jdk.internal.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) <code> <text> failure was on java 11",
        "label": 85
    },
    {
        "text": "select on table fails after changing user defined type in map <description> in cassandra 3.5 i get the following exception when i run this cqls: --drop keyspace bugtest ; create keyspace bugtest  with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }; use bugtest; create type tt ( a boolean ); create table t1 ( k text, v map<text,frozen<tt>>, primary key(k) ); insert into t1 (k,v) values ('k2',{'mk':{a:false}}); alter type tt add b boolean; update t1 set v['mk'] = { b:true } where k = 'k2'; select * from t1;   the last select fails. warn  [sharedpool-worker-5] 2016-04-19 14:18:49,885 abstractlocalawareexecutorservice.java:169 - uncaught exception on thread thread[sharedpool-worker-5,5,main]: {} java.lang.assertionerror: null         at org.apache.cassandra.db.rows.complexcolumndata$builder.addcell(complexcolumndata.java:254) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.row$merger$columndatareducer.getreduced(row.java:623) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.row$merger$columndatareducer.getreduced(row.java:549) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:217) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:156) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.row$merger.merge(row.java:526) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator$mergereducer.getreduced(unfilteredrowiterators.java:473) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator$mergereducer.getreduced(unfilteredrowiterators.java:437) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:217) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:156) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator.computenext(unfilteredrowiterators.java:419) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator.computenext(unfilteredrowiterators.java:279) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.computenext(lazilyinitializedunfilteredrowiterator.java:100) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.computenext(lazilyinitializedunfilteredrowiterator.java:32) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.transform.baserows.hasnext(baserows.java:112) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.transform.unfilteredrows.isempty(unfilteredrows.java:38) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.partitions.purgefunction.applytopartition(purgefunction.java:64) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.partitions.purgefunction.applytopartition(purgefunction.java:24) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.transform.basepartitions.hasnext(basepartitions.java:76) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$serializer.serialize(unfilteredpartitioniterators.java:289) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse$localdataresponse.build(readresponse.java:134) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse$localdataresponse.<init>(readresponse.java:127) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse$localdataresponse.<init>(readresponse.java:123) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse.createdataresponse(readresponse.java:65) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readcommand.createresponse(readcommand.java:292) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1799) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:2466) ~[apache-cassandra-3.5.jar:3.5]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_72-internal]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:164) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$localsessionfuturetask.run(abstractlocalawareexecutorservice.java:136) [apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [apache-cassandra-3.5.jar:3.5]         at java.lang.thread.run(thread.java:745) [na:1.8.0_72-internal]<stacktrace> warn  [sharedpool-worker-5] 2016-04-19 14:18:49,885 abstractlocalawareexecutorservice.java:169 - uncaught exception on thread thread[sharedpool-worker-5,5,main]: {} java.lang.assertionerror: null         at org.apache.cassandra.db.rows.complexcolumndata$builder.addcell(complexcolumndata.java:254) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.row$merger$columndatareducer.getreduced(row.java:623) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.row$merger$columndatareducer.getreduced(row.java:549) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:217) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:156) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.row$merger.merge(row.java:526) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator$mergereducer.getreduced(unfilteredrowiterators.java:473) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator$mergereducer.getreduced(unfilteredrowiterators.java:437) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.consume(mergeiterator.java:217) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.mergeiterator$manytoone.computenext(mergeiterator.java:156) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator.computenext(unfilteredrowiterators.java:419) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.unfilteredrowiterators$unfilteredrowmergeiterator.computenext(unfilteredrowiterators.java:279) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.computenext(lazilyinitializedunfilteredrowiterator.java:100) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.rows.lazilyinitializedunfilteredrowiterator.computenext(lazilyinitializedunfilteredrowiterator.java:32) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.utils.abstractiterator.hasnext(abstractiterator.java:47) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.transform.baserows.hasnext(baserows.java:112) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.transform.unfilteredrows.isempty(unfilteredrows.java:38) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.partitions.purgefunction.applytopartition(purgefunction.java:64) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.partitions.purgefunction.applytopartition(purgefunction.java:24) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.transform.basepartitions.hasnext(basepartitions.java:76) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.partitions.unfilteredpartitioniterators$serializer.serialize(unfilteredpartitioniterators.java:289) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse$localdataresponse.build(readresponse.java:134) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse$localdataresponse.<init>(readresponse.java:127) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse$localdataresponse.<init>(readresponse.java:123) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readresponse.createdataresponse(readresponse.java:65) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.db.readcommand.createresponse(readcommand.java:292) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:1799) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:2466) ~[apache-cassandra-3.5.jar:3.5]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) ~[na:1.8.0_72-internal]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$futuretask.run(abstractlocalawareexecutorservice.java:164) ~[apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.concurrent.abstractlocalawareexecutorservice$localsessionfuturetask.run(abstractlocalawareexecutorservice.java:136) [apache-cassandra-3.5.jar:3.5]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [apache-cassandra-3.5.jar:3.5]         at java.lang.thread.run(thread.java:745) [na:1.8.0_72-internal] <code> --drop keyspace bugtest ; create keyspace bugtest  with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }; use bugtest; create type tt ( a boolean ); create table t1 ( k text, v map<text,frozen<tt>>, primary key(k) ); insert into t1 (k,v) values ('k2',{'mk':{a:false}}); alter type tt add b boolean; update t1 set v['mk'] = { b:true } where k = 'k2'; select * from t1;   <text> in cassandra 3.5 i get the following exception when i run this cqls: the last select fails.",
        "label": 25
    },
    {
        "text": "cqlsh bash script cannot be called through symlink <description> cqlsh bash script cannot be called through a symlink<stacktrace> <code> <text> cqlsh bash script cannot be called through a symlink",
        "label": 71
    },
    {
        "text": "testall failure inorg apache cassandra io sstable indexsummarymanagertest testcancelindex <description> example failure:  http://cassci.datastax.com/job/cassandra-2.2_testall/594/testreport/org.apache.cassandra.io.sstable/indexsummarymanagertest/testcancelindex/ error message expected compaction interrupted exception stacktrace junit.framework.assertionfailederror: expected compaction interrupted exception at org.apache.cassandra.io.sstable.indexsummarymanagertest.testcancelindex(indexsummarymanagertest.java:641) related failure:  http://cassci.datastax.com/job/cassandra-2.2_testall/600/testreport/org.apache.cassandra.io.sstable/indexsummarymanagertest/testcancelindex_compression/<stacktrace> stacktrace junit.framework.assertionfailederror: expected compaction interrupted exception at org.apache.cassandra.io.sstable.indexsummarymanagertest.testcancelindex(indexsummarymanagertest.java:641) <code> error message expected compaction interrupted exception example failure:  http://cassci.datastax.com/job/cassandra-2.2_testall/594/testreport/org.apache.cassandra.io.sstable/indexsummarymanagertest/testcancelindex/ related failure:  http://cassci.datastax.com/job/cassandra-2.2_testall/600/testreport/org.apache.cassandra.io.sstable/indexsummarymanagertest/testcancelindex_compression/<text> ",
        "label": 474
    },
    {
        "text": "estimated row cache entry size incorrect  always  <description> after running for several hours the rowcachesize was suspicious low (ie 70 something mb) i used cassandra-4859 to measure the size and number of entries on a node: in [3]: 1560504./65021  out[3]: 24.0 in [4]: 2149464./89561  out[4]: 24.0 in [6]: 7216096./300785  out[6]: 23.990877204647838 that's rowcachesize/rowcachenumentires . just to prove i don't have crazy small rows the mean size of the row keys in the saved cache is 67 and compacted row mean size: 355. no jamm errors in the log config notes:  row_cache_provider: concurrentlinkedhashcacheprovider  row_cache_size_in_mb: 2048 version info: c*: 1.1.6 centos 2.6.32-220.13.1.el6.x86_64 java 6u31 java hotspot(tm) 64-bit server vm (build 20.6-b01, mixed mode)<stacktrace> <code> in [3]: 1560504./65021  out[3]: 24.0 in [4]: 2149464./89561  out[4]: 24.0 in [6]: 7216096./300785  out[6]: 23.990877204647838 config notes:  row_cache_provider: concurrentlinkedhashcacheprovider  row_cache_size_in_mb: 2048 <text> after running for several hours the rowcachesize was suspicious low (ie 70 something mb) i used cassandra-4859 to measure the size and number of entries on a node: that's rowcachesize/rowcachenumentires . just to prove i don't have crazy small rows the mean size of the row keys in the saved cache is 67 and compacted row mean size: 355. no jamm errors in the log version info:",
        "label": 555
    },
    {
        "text": "add  caching  to cql cf options <description> \"caching\" option is missing from cql columnfamily options.<stacktrace> <code> <text> 'caching' option is missing from cql columnfamily options.",
        "label": 520
    },
    {
        "text": "ant eclipse warnings fails in trunk <description> eclipse-warnings:     [mkdir] created dir: /home/mshuler/git/cassandra/build/ecj      [echo] running eclipse code analysis.  output logged to /home/mshuler/git/cassandra/build/ecj/eclipse_compiler_checks.txt      [java] incorrect classpath: /home/mshuler/git/cassandra/build/cobertura/classes      [java] ----------      [java] 1. error in /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/randomaccessreader.java (at line 81)      [java]     super(new channelproxy(file), default_buffer_size, -1l, buffertype.off_heap);      [java]           ^^^^^^^^^^^^^^^^^^^^^^      [java] potential resource leak: '<unassigned closeable value>' may not be closed      [java] ----------      [java] 1 problem (1 error) build failed (checked 2.2 and did not find this issue)  git blame on line 81 shows commit 17dd4cc for cassandra-8897<stacktrace> <code> eclipse-warnings:     [mkdir] created dir: /home/mshuler/git/cassandra/build/ecj      [echo] running eclipse code analysis.  output logged to /home/mshuler/git/cassandra/build/ecj/eclipse_compiler_checks.txt      [java] incorrect classpath: /home/mshuler/git/cassandra/build/cobertura/classes      [java] ----------      [java] 1. error in /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/randomaccessreader.java (at line 81)      [java]     super(new channelproxy(file), default_buffer_size, -1l, buffertype.off_heap);      [java]           ^^^^^^^^^^^^^^^^^^^^^^      [java] potential resource leak: '<unassigned closeable value>' may not be closed      [java] ----------      [java] 1 problem (1 error) build failed <text> (checked 2.2 and did not find this issue)  git blame on line 81 shows commit 17dd4cc for cassandra-8897",
        "label": 508
    },
    {
        "text": "native protocol v2 spec is missing column type definition for text <description> native protocol v2 spec is missing column type definition for text. should be 0x000a. https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v2.spec#l526<stacktrace> <code> https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v2.spec#l526<text> native protocol v2 spec is missing column type definition for text. should be 0x000a. ",
        "label": 520
    },
    {
        "text": "fix logback configuration in scripts and debian packaging for trunk <description> <stacktrace> <code> <text> ",
        "label": 348
    },
    {
        "text": "compaction throttling can be too slow <description> compaction throttling needs to know how many active compactions are running (to divide bandwith for each active compaction). the way active compaction is counted can be broken because it counts the number of active threads in the executor but the thread starts by acquiring a lock.  if the lock can't be acquired immediately : the thread is seen as \"active\" but does not participate in io operations.  the case can happen when major compaction are triggered (major compaction acquire a write lock, while minor compactions acquire a read lock). having compaction througput to 16mb/s, we observed is the following (two times) : only 1 active compaction (a long one for a few hours) starting at 16mb/s, then after some time running at 2mb/s, thus taking a very long time to complete many pending compactions using jmx and monitoring the stack trace of the compaction threads showed that : 1 thread was effectively compacting 1 thread was waiting to acquire the write lock (due to a major compaction) 6 threads were waiting to acquire the read lock (probably due to the thread above trying to acquire the write lock) attached is a proposed patch (very simple, not yet tested) which counts only active compactions.<stacktrace> <code> <text> compaction throttling needs to know how many active compactions are running (to divide bandwith for each active compaction). the way active compaction is counted can be broken because it counts the number of active threads in the executor but the thread starts by acquiring a lock.  if the lock can't be acquired immediately : the thread is seen as 'active' but does not participate in io operations.  the case can happen when major compaction are triggered (major compaction acquire a write lock, while minor compactions acquire a read lock). having compaction througput to 16mb/s, we observed is the following (two times) : using jmx and monitoring the stack trace of the compaction threads showed that : attached is a proposed patch (very simple, not yet tested) which counts only active compactions.",
        "label": 520
    },
    {
        "text": "assertionerror in hintedhandoff   <description> we are running a 8 node cassandra cluster running cassandra 1.0.5.  all our cf use leveled compaction. we ran a test where we did a lot  of inserts for 3 days. after that we started to run tests where some  of the reads could ask for information that was inserted a while back.  in this scenario we are seeing this assertion error in hintedhandoff. error [hintedhandoff:3] 2011-12-05 15:42:04,324  abstractcassandradaemon.java (line 133) fatal exception in thread  thread[hintedhandoff:3,1,main]  java.lang.runtimeexception: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:330)  at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:81)  at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:353)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more  caused by: java.util.concurrent.executionexception:  java.lang.assertionerror: originally calculated column size of  470937164 but now it is 470294247  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:326)  ... 6 more  caused by: java.lang.assertionerror: originally calculated column size  of 470937164 but now it is 470294247  at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:124)  at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:160)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:158)  at org.apache.cassandra.db.compaction.compactionmanager$6.call(compactionmanager.java:275)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more  error [hintedhandoff:3] 2011-12-05 15:42:04,333  abstractcassandradaemon.java (line 133) fatal exception in thread  thread[hintedhandoff:3,1,main]  java.lang.runtimeexception: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:330)  at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:81)  at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:353)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more  caused by: java.util.concurrent.executionexception:  java.lang.assertionerror: originally calculated column size of  470937164 but now it is 470294247  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:326)  ... 6 more  caused by: java.lang.assertionerror: originally calculated column size  of 470937164 but now it is 470294247  at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:124)  at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:160)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:158)  at org.apache.cassandra.db.compaction.compactionmanager$6.call(compactionmanager.java:275)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more  error [compactionexecutor:9931] 2011-12-05 15:42:04,333  abstractcassandradaemon.java (line 133) fatal exception in thread  thread[compactionexecutor:9931,1,main]  java.lang.assertionerror: originally calculated column size of  470937164 but now it is 470294247  at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:124)  at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:160)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:158)  at org.apache.cassandra.db.compaction.compactionmanager$6.call(compactionmanager.java:275)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<stacktrace> error [hintedhandoff:3] 2011-12-05 15:42:04,324  abstractcassandradaemon.java (line 133) fatal exception in thread  thread[hintedhandoff:3,1,main]  java.lang.runtimeexception: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:330)  at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:81)  at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:353)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more  caused by: java.util.concurrent.executionexception:  java.lang.assertionerror: originally calculated column size of  470937164 but now it is 470294247  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:326)  ... 6 more  caused by: java.lang.assertionerror: originally calculated column size  of 470937164 but now it is 470294247  at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:124)  at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:160)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:158)  at org.apache.cassandra.db.compaction.compactionmanager$6.call(compactionmanager.java:275)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more  error [hintedhandoff:3] 2011-12-05 15:42:04,333  abstractcassandradaemon.java (line 133) fatal exception in thread  thread[hintedhandoff:3,1,main]  java.lang.runtimeexception: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.runtimeexception:  java.util.concurrent.executionexception: java.lang.assertionerror:  originally calculated column size of 470937164 but now it is 470294247  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:330)  at org.apache.cassandra.db.hintedhandoffmanager.access$100(hintedhandoffmanager.java:81)  at org.apache.cassandra.db.hintedhandoffmanager$2.runmaythrow(hintedhandoffmanager.java:353)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more  caused by: java.util.concurrent.executionexception:  java.lang.assertionerror: originally calculated column size of  470937164 but now it is 470294247  at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222)  at java.util.concurrent.futuretask.get(futuretask.java:83)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:326)  ... 6 more  caused by: java.lang.assertionerror: originally calculated column size  of 470937164 but now it is 470294247  at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:124)  at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:160)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:158)  at org.apache.cassandra.db.compaction.compactionmanager$6.call(compactionmanager.java:275)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  ... 3 more  error [compactionexecutor:9931] 2011-12-05 15:42:04,333  abstractcassandradaemon.java (line 133) fatal exception in thread  thread[compactionexecutor:9931,1,main]  java.lang.assertionerror: originally calculated column size of  470937164 but now it is 470294247  at org.apache.cassandra.db.compaction.lazilycompactedrow.write(lazilycompactedrow.java:124)  at org.apache.cassandra.io.sstable.sstablewriter.append(sstablewriter.java:160)  at org.apache.cassandra.db.compaction.compactiontask.execute(compactiontask.java:158)  at org.apache.cassandra.db.compaction.compactionmanager$6.call(compactionmanager.java:275)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<code> <text> we are running a 8 node cassandra cluster running cassandra 1.0.5.  all our cf use leveled compaction. we ran a test where we did a lot  of inserts for 3 days. after that we started to run tests where some  of the reads could ask for information that was inserted a while back.  in this scenario we are seeing this assertion error in hintedhandoff. ",
        "label": 520
    },
    {
        "text": "cqlsh copy uses wrong variable name for time format <description> diff --git a/bin/cqlsh b/bin/cqlsh index ca45be3..ddf0314 100755 --- a/bin/cqlsh +++ b/bin/cqlsh @@ -1816,7 +1816,7 @@ class shell(cmd.cmd):          encoding = opts.pop('encoding', 'utf8')          nullval = opts.pop('null', '')          header = bool(opts.pop('header', '').lower() == 'true') -        timestamp_format = opts.pop('time_format', self.display_timestamp_format) +        timestamp_format = opts.pop('time_format', self.display_time_format)          if dialect_options['quotechar'] == dialect_options['escapechar']:              dialect_options['doublequote'] = true              del dialect_options['escape char']<stacktrace> <code> diff --git a/bin/cqlsh b/bin/cqlsh index ca45be3..ddf0314 100755 --- a/bin/cqlsh +++ b/bin/cqlsh @@ -1816,7 +1816,7 @@ class shell(cmd.cmd):          encoding = opts.pop('encoding', 'utf8')          nullval = opts.pop('null', '')          header = bool(opts.pop('header', '').lower() == 'true') -        timestamp_format = opts.pop('time_format', self.display_timestamp_format) +        timestamp_format = opts.pop('time_format', self.display_time_format)          if dialect_options['quotechar'] == dialect_options['escapechar']:              dialect_options['doublequote'] = true              del dialect_options['escape char']<text> ",
        "label": 508
    },
    {
        "text": "change protocol to allow sending key space independent of query string <description> currently keyspace is either embedded in the query string or set through \"use keyspace\" on a connection by client driver. there are practical use cases where client user has query and keyspace independently. in order for that scenario to work, they will have to create one client session per keyspace or have to resort to some string replace hackery. it will be nice if protocol allowed sending keyspace separately from the query.<stacktrace> <code> <text> currently keyspace is either embedded in the query string or set through 'use keyspace' on a connection by client driver. there are practical use cases where client user has query and keyspace independently. in order for that scenario to work, they will have to create one client session per keyspace or have to resort to some string replace hackery. it will be nice if protocol allowed sending keyspace separately from the query.",
        "label": 480
    },
    {
        "text": "update py test to use strategy options <description> ksdef was changed in cassandra.thrift to accept a hash of options as strategy_options. py_test/stress.py needs to be updated with the new method arguments. cassandra-1263 changed the parameters to ksdef.  cassandra-2462 fixed this issue in the native java stress package.<stacktrace> <code> <text> ksdef was changed in cassandra.thrift to accept a hash of options as strategy_options. py_test/stress.py needs to be updated with the new method arguments. cassandra-1263 changed the parameters to ksdef.  cassandra-2462 fixed this issue in the native java stress package.",
        "label": 452
    },
    {
        "text": "breaks cql compatibility with super columns families <description> this is a follow-up to cassandra-12335 to fix the cql side of super column compatibility. the details and a proposed solution can be found in the comments of cassandra-12335 but the crux of the issue is that super column famillies show up differently in cql in 3.0.x/3.x compared to 2.x, hence breaking backward compatibilty.<stacktrace> <code> <text> this is a follow-up to cassandra-12335 to fix the cql side of super column compatibility. the details and a proposed solution can be found in the comments of cassandra-12335 but the crux of the issue is that super column famillies show up differently in cql in 3.0.x/3.x compared to 2.x, hence breaking backward compatibilty.",
        "label": 25
    },
    {
        "text": "high heap consumption due to high number of sstablereader <description> given a workload with quite a lot of reads, i recently encountered high heap memory consumption. given 2gb of heap, it appears i have 750.000+ tasks in sstablereader.syncexecutor, consuming more than 1.2gb. these tasks have type sstablereader$5, which i guess corresponds to : readmetersyncfuture = syncexecutor.scheduleatfixedrate(new runnable() { public void run() { if (!iscompacted.get()) { metersyncthrottle.acquire(); systemkeyspace.persistsstablereadmeter(desc.ksname, desc.cfname, desc.generation, readmeter); } } }, 1, 5, timeunit.minutes); i do not have have to the environment right now, but i could provide a threaddump later if necessary.<stacktrace> <code> readmetersyncfuture = syncexecutor.scheduleatfixedrate(new runnable() { public void run() { if (!iscompacted.get()) { metersyncthrottle.acquire(); systemkeyspace.persistsstablereadmeter(desc.ksname, desc.cfname, desc.generation, readmeter); } } }, 1, 5, timeunit.minutes); <text> given a workload with quite a lot of reads, i recently encountered high heap memory consumption. given 2gb of heap, it appears i have 750.000+ tasks in sstablereader.syncexecutor, consuming more than 1.2gb. these tasks have type sstablereader$5, which i guess corresponds to : i do not have have to the environment right now, but i could provide a threaddump later if necessary.",
        "label": 521
    },
    {
        "text": "cve security vulnerability and redefine default log rotation policy <description> cassandra 3.11.1 is patched with logback 1.1.3, which contains the security vulnerability described here. https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2017-5929 also update to logback allows a simple date and size rotation policy to  replace the default fixed policy, which is broken by design.<stacktrace> <code> cassandra 3.11.1 is patched with logback 1.1.3, which contains the security vulnerability described here. https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2017-5929 <text> also update to logback allows a simple date and size rotation policy to  replace the default fixed policy, which is broken by design.",
        "label": 526
    },
    {
        "text": "faster sequential io  on compaction  streaming  etc  <description> when node is doing a lot of sequencial io (streaming, compacting, etc) a lot of cpu is lost in calls to raf's int read() and dataoutputstream's write(int).  this is because default implementations of readshort,readlong, etc as well as their matching write* are implemented with numerous calls of byte by byte read and write.   this makes a lot of syscalls as well. a quick microbench shows than just reimplementation of these methods in either way gives 8x speed increase. a patch attached implements randomaccessreader.read<type> and sequencialwriter.write<type> methods in more efficient way.  i also eliminated some extra byte copies in compositetype.split and columnnamehelper.maxcomponents, which were on my profiler's hotspot method list during tests. a stress tests on my laptop show that this patch makes compaction 25-30% faster on uncompressed sstables and 15% faster for compressed ones. a deployment to production shows much less cpu load for compaction.   (i attached a cpu load graph from one of our production, orange is niced cpu load - i.e. compaction; yellow is user - i.e. not compaction related tasks)<stacktrace> <code> <text> when node is doing a lot of sequencial io (streaming, compacting, etc) a lot of cpu is lost in calls to raf's int read() and dataoutputstream's write(int).  this is because default implementations of readshort,readlong, etc as well as their matching write* are implemented with numerous calls of byte by byte read and write.   this makes a lot of syscalls as well. a quick microbench shows than just reimplementation of these methods in either way gives 8x speed increase. a patch attached implements randomaccessreader.read<type> and sequencialwriter.write<type> methods in more efficient way.  i also eliminated some extra byte copies in compositetype.split and columnnamehelper.maxcomponents, which were on my profiler's hotspot method list during tests. a stress tests on my laptop show that this patch makes compaction 25-30% faster on uncompressed sstables and 15% faster for compressed ones. a deployment to production shows much less cpu load for compaction.   (i attached a cpu load graph from one of our production, orange is niced cpu load - i.e. compaction; yellow is user - i.e. not compaction related tasks)",
        "label": 508
    },
    {
        "text": "repair streaming forwarding loop <description> i am able to reproduce what appears to be a streaming forwarding loop when running repairs. this affect only nodes using broadcast_address (ec2 external ip) & listen_address of 0.0.0.0. (configuration is using property file snitch in a multi dc nts where some dc's are ec2 and others are not). the hosts in the other dc's not using broadcast_address do not experience this symptom. on ec2 host dc1host1:  info [antientropystage:1] 2011-09-13 06:34:01,673 streamingrepairtask.java (line 211) streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb received task from /0.0.0.0 to stream 12259 ranges to /external.ec2.ip.dc1host3  info [antientropystage:1] 2011-09-13 06:34:01,673 streamingrepairtask.java (line 136) streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb forwarding streaming repair of 12259 ranges to /external.ec2.ip.of.dc1host1 (to be streamed with /external.ip.of.host3) the above appears to trigger another streaming task and results in saturating the network interfaces dc1host1. the above log entries are repeated until cassandra is killed.<stacktrace> <code> on ec2 host dc1host1:  info [antientropystage:1] 2011-09-13 06:34:01,673 streamingrepairtask.java (line 211) streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb received task from /0.0.0.0 to stream 12259 ranges to /external.ec2.ip.dc1host3  info [antientropystage:1] 2011-09-13 06:34:01,673 streamingrepairtask.java (line 136) streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb forwarding streaming repair of 12259 ranges to /external.ec2.ip.of.dc1host1 (to be streamed with /external.ip.of.host3) <text> i am able to reproduce what appears to be a streaming forwarding loop when running repairs. this affect only nodes using broadcast_address (ec2 external ip) & listen_address of 0.0.0.0. (configuration is using property file snitch in a multi dc nts where some dc's are ec2 and others are not). the hosts in the other dc's not using broadcast_address do not experience this symptom. the above appears to trigger another streaming task and results in saturating the network interfaces dc1host1. the above log entries are repeated until cassandra is killed.",
        "label": 520
    },
    {
        "text": "cassandra cli cannot connect <description> i cannot connect to any of my nodes using cassandra-cli. i think this has happened about 2 weeks ago: [agoudarzi@cas-test1 bin]$ cassandra-cli --host 10.50.26.132 --port 9160 --debug  exception retrieving information about the cassandra node, check you have connected to the thrift port.  org.apache.thrift.transport.ttransportexception  at org.apache.thrift.transport.tiostreamtransport.read(tiostreamtransport.java:132)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.transport.tframedtransport.readframe(tframedtransport.java:129)  at org.apache.thrift.transport.tframedtransport.read(tframedtransport.java:101)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.protocol.tbinaryprotocol.readall(tbinaryprotocol.java:369)  at org.apache.thrift.protocol.tbinaryprotocol.readi32(tbinaryprotocol.java:295)  at org.apache.thrift.protocol.tbinaryprotocol.readmessagebegin(tbinaryprotocol.java:202)  at org.apache.cassandra.thrift.cassandra$client.recv_describe_cluster_name(cassandra.java:1117)  at org.apache.cassandra.thrift.cassandra$client.describe_cluster_name(cassandra.java:1103)  at org.apache.cassandra.cli.climain.connect(climain.java:164)  at org.apache.cassandra.cli.climain.main(climain.java:255)  welcome to cassandra cli. type 'help' or '?' for help. type 'quit' or 'exit' to quit.  [default@unknown] exit however using thrift php client i have no problem connecting and executing describe_cluster_name(). i have configured cassandra rpc port and ip as follows: 1. the address to bind the thrift rpc service to  rpc_address: 10.50.26.132 2. port for thrift to listen on  rpc_port: 9160 steps to reproduce:  1. start from a clean setup;  2. run py_stress to insert some keys and create the default keyspace;  3. try connecting using cassandra-cli like command above. you'll get the exception.<stacktrace> [agoudarzi@cas-test1 bin]$ cassandra-cli --host 10.50.26.132 --port 9160 --debug  exception retrieving information about the cassandra node, check you have connected to the thrift port.  org.apache.thrift.transport.ttransportexception  at org.apache.thrift.transport.tiostreamtransport.read(tiostreamtransport.java:132)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.transport.tframedtransport.readframe(tframedtransport.java:129)  at org.apache.thrift.transport.tframedtransport.read(tframedtransport.java:101)  at org.apache.thrift.transport.ttransport.readall(ttransport.java:84)  at org.apache.thrift.protocol.tbinaryprotocol.readall(tbinaryprotocol.java:369)  at org.apache.thrift.protocol.tbinaryprotocol.readi32(tbinaryprotocol.java:295)  at org.apache.thrift.protocol.tbinaryprotocol.readmessagebegin(tbinaryprotocol.java:202)  at org.apache.cassandra.thrift.cassandra$client.recv_describe_cluster_name(cassandra.java:1117)  at org.apache.cassandra.thrift.cassandra$client.describe_cluster_name(cassandra.java:1103)  at org.apache.cassandra.cli.climain.connect(climain.java:164)  at org.apache.cassandra.cli.climain.main(climain.java:255)  welcome to cassandra cli. <code> <text> i cannot connect to any of my nodes using cassandra-cli. i think this has happened about 2 weeks ago: type 'help' or '?' for help. type 'quit' or 'exit' to quit.  [default@unknown] exit however using thrift php client i have no problem connecting and executing describe_cluster_name(). i have configured cassandra rpc port and ip as follows: steps to reproduce:  1. start from a clean setup;  2. run py_stress to insert some keys and create the default keyspace;  3. try connecting using cassandra-cli like command above. you'll get the exception.",
        "label": 274
    },
    {
        "text": "indexerror 'list index out of range'  when trying to connect to cassandra cluster with cqlsh <description> cassandra by default uses a python driver to connect >>> cluster = cluster(['ip'], protocol_version=3) >>> session = cluster.connect() traceback (most recent call last):   file \"<stdin>\", line 1, in <module>   file \"/usr/local/lib/python2.7/dist-packages/cassandra/cluster.py\", line 839, in connect     self.control_connection.connect()   file \"/usr/local/lib/python2.7/dist-packages/cassandra/cluster.py\", line 2075, in connect     self._set_new_connection(self._reconnect_internal())   file \"/usr/local/lib/python2.7/dist-packages/cassandra/cluster.py\", line 2110, in _reconnect_internal     raise nohostavailable(\"unable to connect to any servers\", errors) cassandra.cluster.nohostavailable: ('unable to connect to any servers', {'ip': indexerror('list index out of range',)})<stacktrace> <code> >>> cluster = cluster(['ip'], protocol_version=3) >>> session = cluster.connect() traceback (most recent call last):   file '<stdin>', line 1, in <module>   file '/usr/local/lib/python2.7/dist-packages/cassandra/cluster.py', line 839, in connect     self.control_connection.connect()   file '/usr/local/lib/python2.7/dist-packages/cassandra/cluster.py', line 2075, in connect     self._set_new_connection(self._reconnect_internal())   file '/usr/local/lib/python2.7/dist-packages/cassandra/cluster.py', line 2110, in _reconnect_internal     raise nohostavailable('unable to connect to any servers', errors) cassandra.cluster.nohostavailable: ('unable to connect to any servers', {'ip': indexerror('list index out of range',)}) <text> cassandra by default uses a python driver to connect",
        "label": 8
    },
    {
        "text": "during repair   incorrect data size     connection reset  errors  repair unable to complete  <description> this has been happening since 1.0.2. i wasn't on 1.0 for very long but i'm fairly certain repair was working ok. repair worked decently for me in 0.8 (data bloat sucked). all my sstables are version h. on one node: java.lang.assertionerror: incorrect row data size 596045 written to /mnt/cassandra/data/trprod/metrics1m-tmp-h-25036-data.db; correct is 586675  at org.apache.cassandra.io.sstable.sstablewriter.appendfromstream(sstablewriter.java:253)  at org.apache.cassandra.streaming.incomingstreamreader.streamin(incomingstreamreader.java:146)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:87)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:184)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:81) on the other node: 4999 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-24953-data.db sections=1707 progress=0/1513497639 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25000-data.db sections=635 progress=0/53400713 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25002-data.db sections=570 progress=0/709993 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25003-data.db sections=550 progress=0/449498 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25005-data.db sections=516 progress=0/316301 - 0%], 6 sstables.  info [streamstage:1] 2011-11-09 19:45:22,795 streamoutsession.java (line 203) streaming to /10.38.69.192  error [streaming:1] 2011-11-09 19:47:47,964 abstractcassandradaemon.java (line 133) fatal exception in thread thread[streaming:1,1,main]  java.lang.runtimeexception: java.net.socketexception: connection reset  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.net.socketexception: connection reset  at java.net.socketoutputstream.socketwrite(socketoutputstream.java:96)  at java.net.socketoutputstream.write(socketoutputstream.java:136)  at com.ning.compress.lzf.chunkencoder.encodeandwritechunk(chunkencoder.java:133)  at com.ning.compress.lzf.lzfoutputstream.writecompressedblock(lzfoutputstream.java:203)  at com.ning.compress.lzf.lzfoutputstream.write(lzfoutputstream.java:97)  at org.apache.cassandra.streaming.filestreamtask.write(filestreamtask.java:181)  at org.apache.cassandra.streaming.filestreamtask.stream(filestreamtask.java:145)  at org.apache.cassandra.streaming.filestreamtask.runmaythrow(filestreamtask.java:91)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more  error [streaming:1] 2011-11-09 19:47:47,970 abstractcassandradaemon.java (line 133) fatal exception in thread thread[streaming:1,1,main]  java.lang.runtimeexception: java.net.socketexception: connection reset  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.net.socketexception: connection reset  at java.net.socketoutputstream.socketwrite(socketoutputstream.java:96)  at java.net.socketoutputstream.write(socketoutputstream.java:136)  at com.ning.compress.lzf.chunkencoder.encodeandwritechunk(chunkencoder.java:133)  at com.ning.compress.lzf.lzfoutputstream.writecompressedblock(lzfoutputstream.java:203)  at com.ning.compress.lzf.lzfoutputstream.write(lzfoutputstream.java:97)  at org.apache.cassandra.streaming.filestreamtask.write(filestreamtask.java:181)  at org.apache.cassandra.streaming.filestreamtask.stream(filestreamtask.java:145)  at org.apache.cassandra.streaming.filestreamtask.runmaythrow(filestreamtask.java:91)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more<stacktrace> java.lang.assertionerror: incorrect row data size 596045 written to /mnt/cassandra/data/trprod/metrics1m-tmp-h-25036-data.db; correct is 586675  at org.apache.cassandra.io.sstable.sstablewriter.appendfromstream(sstablewriter.java:253)  at org.apache.cassandra.streaming.incomingstreamreader.streamin(incomingstreamreader.java:146)  at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:87)  at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:184)  at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:81) 4999 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-24953-data.db sections=1707 progress=0/1513497639 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25000-data.db sections=635 progress=0/53400713 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25002-data.db sections=570 progress=0/709993 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25003-data.db sections=550 progress=0/449498 - 0%, /mnt/cassandra/data/trprod/metrics1m-h-25005-data.db sections=516 progress=0/316301 - 0%], 6 sstables.  info [streamstage:1] 2011-11-09 19:45:22,795 streamoutsession.java (line 203) streaming to /10.38.69.192  error [streaming:1] 2011-11-09 19:47:47,964 abstractcassandradaemon.java (line 133) fatal exception in thread thread[streaming:1,1,main]  java.lang.runtimeexception: java.net.socketexception: connection reset  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.net.socketexception: connection reset  at java.net.socketoutputstream.socketwrite(socketoutputstream.java:96)  at java.net.socketoutputstream.write(socketoutputstream.java:136)  at com.ning.compress.lzf.chunkencoder.encodeandwritechunk(chunkencoder.java:133)  at com.ning.compress.lzf.lzfoutputstream.writecompressedblock(lzfoutputstream.java:203)  at com.ning.compress.lzf.lzfoutputstream.write(lzfoutputstream.java:97)  at org.apache.cassandra.streaming.filestreamtask.write(filestreamtask.java:181)  at org.apache.cassandra.streaming.filestreamtask.stream(filestreamtask.java:145)  at org.apache.cassandra.streaming.filestreamtask.runmaythrow(filestreamtask.java:91)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more  error [streaming:1] 2011-11-09 19:47:47,970 abstractcassandradaemon.java (line 133) fatal exception in thread thread[streaming:1,1,main]  java.lang.runtimeexception: java.net.socketexception: connection reset  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)  caused by: java.net.socketexception: connection reset  at java.net.socketoutputstream.socketwrite(socketoutputstream.java:96)  at java.net.socketoutputstream.write(socketoutputstream.java:136)  at com.ning.compress.lzf.chunkencoder.encodeandwritechunk(chunkencoder.java:133)  at com.ning.compress.lzf.lzfoutputstream.writecompressedblock(lzfoutputstream.java:203)  at com.ning.compress.lzf.lzfoutputstream.write(lzfoutputstream.java:97)  at org.apache.cassandra.streaming.filestreamtask.write(filestreamtask.java:181)  at org.apache.cassandra.streaming.filestreamtask.stream(filestreamtask.java:145)  at org.apache.cassandra.streaming.filestreamtask.runmaythrow(filestreamtask.java:91)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 3 more<code> <text> this has been happening since 1.0.2. i wasn't on 1.0 for very long but i'm fairly certain repair was working ok. repair worked decently for me in 0.8 (data bloat sucked). all my sstables are version h. on one node: on the other node: ",
        "label": 520
    },
    {
        "text": "query returns different number of results depending on fetchsize <description> i issue a query across the set of partitioned wide rows for one logical row, where s, l, and partition specify the composite primary key for the row:  select ec, ea, rd from sr where s = ? and partition in ? and l = ? allow filtering; if i set fetchsize to only 1000 when the cluster is configured, the query sometimes does not return all the results. in the particular case i am chasing, it returns a total of 98586 rows. if i increase the fetchsize to 100000, all the 99999 actual rows are returned. this suggests there is some problem with fetchsize re-establishing the position on the next segment of the result set, at least when multiple partitions are being accessed.<stacktrace> <code> i issue a query across the set of partitioned wide rows for one logical row, where s, l, and partition specify the composite primary key for the row:  select ec, ea, rd from sr where s = ? and partition in ? and l = ? allow filtering; <text> if i set fetchsize to only 1000 when the cluster is configured, the query sometimes does not return all the results. in the particular case i am chasing, it returns a total of 98586 rows. if i increase the fetchsize to 100000, all the 99999 actual rows are returned. this suggests there is some problem with fetchsize re-establishing the position on the next segment of the result set, at least when multiple partitions are being accessed.",
        "label": 520
    },
    {
        "text": "creating a materialized view on a table with  token  column breaks the cluster <description> on a new cassandra cluster, if we create a table with a field called \"token\" (with quotes) and then create a materialized view that uses \"token\", the cluster breaks. a servererror is returned, and no further nodetool operations on the sstables work. restarting the cassandra server will also fail. it seems like the entire cluster is hosed. we tried this on cassandra 3.3 and 3.5. here's how to produce (on an new, empty cassandra 3.5 docker container): [cqlsh 5.0.1 | cassandra 3.5 | cql spec 3.4.0 | native protocol v4] use help for help. cqlsh> create keyspace account with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }; cqlsh> create table account.session  (    ...   \"token\" blob,    ...   account_id uuid,    ...   primary key(\"token\")    ... )with compaction={'class': 'leveledcompactionstrategy'} and    ...   compression={'sstable_compression': 'lz4compressor'}; cqlsh> create materialized view account.account_session as    ...        select account_id,\"token\" from account.session    ...        where \"token\" is not null and account_id is not null    ...        primary key (account_id, \"token\"); servererror: <errormessage code=0000 [server error] message=\"java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: line 1:25 no viable alternative at input 'from' (select account_id, token [from]...)\"> cqlsh> drop table account.session; servererror: <errormessage code=0000 [server error] message=\"java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: line 1:25 no viable alternative at input 'from' (select account_id, token [from]...)\"> when any sstable*, nodetool, or when the cassandra process is restarted, this is emitted on startup and cassandra exits (copied from a server w/ data): info  [main] 2016-05-12 23:25:30,074 columnfamilystore.java:395 - initializing system_schema.indexes debug [sstablebatchopen:1] 2016-05-12 23:25:30,075 sstablereader.java:480 - opening /mnt/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/ma-4-big (91 bytes) error [main] 2016-05-12 23:25:30,143 cassandradaemon.java:697 - exception encountered during startup org.apache.cassandra.exceptions.syntaxexception: line 1:59 no viable alternative at input 'from' (..., expire_at, last_used, token [from]...)         at org.apache.cassandra.cql3.errorcollector.throwfirstsyntaxerror(errorcollector.java:101) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.cql3.cqlfragmentparser.parseanyunhandled(cqlfragmentparser.java:80) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.cql3.queryprocessor.parsestatement(queryprocessor.java:512) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchview(schemakeyspace.java:1128) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchviews(schemakeyspace.java:1092) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchkeyspace(schemakeyspace.java:903) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchkeyspaceswithout(schemakeyspace.java:879) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchnonsystemkeyspaces(schemakeyspace.java:867) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:134) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:124) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:229) [apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:551) [apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:680) [apache-cassandra-3.5.0.jar:3.5.0]<stacktrace> info  [main] 2016-05-12 23:25:30,074 columnfamilystore.java:395 - initializing system_schema.indexes debug [sstablebatchopen:1] 2016-05-12 23:25:30,075 sstablereader.java:480 - opening /mnt/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/ma-4-big (91 bytes) error [main] 2016-05-12 23:25:30,143 cassandradaemon.java:697 - exception encountered during startup org.apache.cassandra.exceptions.syntaxexception: line 1:59 no viable alternative at input 'from' (..., expire_at, last_used, token [from]...)         at org.apache.cassandra.cql3.errorcollector.throwfirstsyntaxerror(errorcollector.java:101) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.cql3.cqlfragmentparser.parseanyunhandled(cqlfragmentparser.java:80) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.cql3.queryprocessor.parsestatement(queryprocessor.java:512) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchview(schemakeyspace.java:1128) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchviews(schemakeyspace.java:1092) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchkeyspace(schemakeyspace.java:903) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchkeyspaceswithout(schemakeyspace.java:879) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.schema.schemakeyspace.fetchnonsystemkeyspaces(schemakeyspace.java:867) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:134) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:124) ~[apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:229) [apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:551) [apache-cassandra-3.5.0.jar:3.5.0]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:680) [apache-cassandra-3.5.0.jar:3.5.0] <code> [cqlsh 5.0.1 | cassandra 3.5 | cql spec 3.4.0 | native protocol v4] use help for help. cqlsh> create keyspace account with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }; cqlsh> create table account.session  (    ...   'token' blob,    ...   account_id uuid,    ...   primary key('token')    ... )with compaction={'class': 'leveledcompactionstrategy'} and    ...   compression={'sstable_compression': 'lz4compressor'}; cqlsh> create materialized view account.account_session as    ...        select account_id,'token' from account.session    ...        where 'token' is not null and account_id is not null    ...        primary key (account_id, 'token'); servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: line 1:25 no viable alternative at input 'from' (select account_id, token [from]...)'> cqlsh> drop table account.session; servererror: <errormessage code=0000 [server error] message='java.lang.runtimeexception: java.util.concurrent.executionexception: org.apache.cassandra.exceptions.syntaxexception: line 1:25 no viable alternative at input 'from' (select account_id, token [from]...)'> <text> on a new cassandra cluster, if we create a table with a field called 'token' (with quotes) and then create a materialized view that uses 'token', the cluster breaks. a servererror is returned, and no further nodetool operations on the sstables work. restarting the cassandra server will also fail. it seems like the entire cluster is hosed. we tried this on cassandra 3.3 and 3.5. here's how to produce (on an new, empty cassandra 3.5 docker container): when any sstable*, nodetool, or when the cassandra process is restarted, this is emitted on startup and cassandra exits (copied from a server w/ data):",
        "label": 98
    },
    {
        "text": " windows  dtest failure in cqlsh tests cqlsh copy tests cqlshcopytest test reading with skip and max rows <description> looks to be an assertion problem, so could be test or cassandra related: e.g.: 10000 != 331 http://cassci.datastax.com/job/trunk_dtest_win32/404/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_reading_with_skip_and_max_rows failed on cassci build trunk_dtest_win32 #404<stacktrace> <code> 10000 != 331 e.g.: http://cassci.datastax.com/job/trunk_dtest_win32/404/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_reading_with_skip_and_max_rows failed on cassci build trunk_dtest_win32 #404<text> looks to be an assertion problem, so could be test or cassandra related: ",
        "label": 508
    },
    {
        "text": "delete only workloads crash cassandra <description> the size of a tombstone is not properly accounted for in the memtable. a memtable which has only tombstones will never get flushed. it will grow until the jvm runs out of memory. the following program easily demonstrates the problem. cluster.builder builder = cluster.builder(); cluster c = builder.addcontactpoints(\"cas121.devf3.com\").build(); session s = c.connect(); s.execute(\"create keyspace if not exists test with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }\"); s.execute(\"create table if not exists test.test(id int primary key)\"); preparedstatement stmt = s.prepare(\"delete from test.test where id = :id\"); int id = 0; while (true) { s.execute(stmt.bind(id)); id++; } this program should run forever, but eventually cassandra runs out of heap and craps out. you needn't wait for cassandra to crash. if you run \"nodetool cfstats test.test\" while it is running, you'll see memtable cell count grow, but memtable data size will remain 0. this issue was fixed once before. i received a patch for version 2.0.5 (i believe), which contained the fix, but the fix has apparently been lost, because it is clearly broken, and i don't see the fix in the change logs.<stacktrace> <code> cluster.builder builder = cluster.builder(); cluster c = builder.addcontactpoints('cas121.devf3.com').build(); session s = c.connect(); s.execute('create keyspace if not exists test with replication = { 'class' : 'simplestrategy', 'replication_factor' : 1 }'); s.execute('create table if not exists test.test(id int primary key)'); preparedstatement stmt = s.prepare('delete from test.test where id = :id'); int id = 0; while (true) { s.execute(stmt.bind(id)); id++; } <text> the size of a tombstone is not properly accounted for in the memtable. a memtable which has only tombstones will never get flushed. it will grow until the jvm runs out of memory. the following program easily demonstrates the problem. this program should run forever, but eventually cassandra runs out of heap and craps out. you needn't wait for cassandra to crash. if you run 'nodetool cfstats test.test' while it is running, you'll see memtable cell count grow, but memtable data size will remain 0. this issue was fixed once before. i received a patch for version 2.0.5 (i believe), which contained the fix, but the fix has apparently been lost, because it is clearly broken, and i don't see the fix in the change logs.",
        "label": 67
    },
    {
        "text": "assertionerror in sizeestimatesrecorder <description> one of the dtests of cassandra-8236 (https://github.com/stef1927/cassandra-dtest/tree/8236) raises the following exception unless i set -dcassandra.size_recorder_interval=0: error [optionaltasks:1] 2015-03-25 12:58:47,015 cassandradaemon.java:179 - exception in thread thread[optionaltasks:1,5,main] java.lang.assertionerror: null         at org.apache.cassandra.service.storageservice.getlocaltokens(storageservice.java:2235) ~[main/:na]         at org.apache.cassandra.db.sizeestimatesrecorder.run(sizeestimatesrecorder.java:61) ~[main/:na]         at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:82) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_76]         at java.util.concurrent.futuretask.runandreset(futuretask.java:304) [na:1.7.0_76]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:178) [na:1.7.0_76]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:293) [na:1.7.0_76]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [na:1.7.0_76]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_76]         at java.lang.thread.run(thread.java:745) [na:1.7.0_76] info  [rmi tcp connection(2)-127.0.0.1] 2015-03-25 12:59:23,189 storageservice.java:863 - joining ring by operator request the test is start_node_without_join_test in pushed_notifications_test.py but starting a node that won't join the ring might be sufficient to reproduce the exception (i haven't tried though).<stacktrace> error [optionaltasks:1] 2015-03-25 12:58:47,015 cassandradaemon.java:179 - exception in thread thread[optionaltasks:1,5,main] java.lang.assertionerror: null         at org.apache.cassandra.service.storageservice.getlocaltokens(storageservice.java:2235) ~[main/:na]         at org.apache.cassandra.db.sizeestimatesrecorder.run(sizeestimatesrecorder.java:61) ~[main/:na]         at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor$uncomplainingrunnable.run(debuggablescheduledthreadpoolexecutor.java:82) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) [na:1.7.0_76]         at java.util.concurrent.futuretask.runandreset(futuretask.java:304) [na:1.7.0_76]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$301(scheduledthreadpoolexecutor.java:178) [na:1.7.0_76]         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:293) [na:1.7.0_76]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) [na:1.7.0_76]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_76]         at java.lang.thread.run(thread.java:745) [na:1.7.0_76] info  [rmi tcp connection(2)-127.0.0.1] 2015-03-25 12:59:23,189 storageservice.java:863 - joining ring by operator request <code> <text> one of the dtests of cassandra-8236 (https://github.com/stef1927/cassandra-dtest/tree/8236) raises the following exception unless i set -dcassandra.size_recorder_interval=0: the test is start_node_without_join_test in pushed_notifications_test.py but starting a node that won't join the ring might be sufficient to reproduce the exception (i haven't tried though).",
        "label": 98
    },
    {
        "text": "get range slices is broken <description> hi,  we just recently tried to use 0.6.4 and 0.6.5 in our production environment and  had some serious problem.  the getrangeslices functionality is broken.  we have a cluster of 5 machines.  we use getrangeslices to iterate over all of the keys in a cf (2062 keys total).  we are using orderpreservingpartitioner.  we use getrangeslices with keyrange using keys (not tokens).  if we set the requestblockcount (aka: keyrange.setcount()) to a number  greater than 2062 we get all keys in one shot (all is good).  if we try to fetch the keys in smaller blocks (requestblockcount=100)  we get bad results.  we get only 800 unique keys back.  we start with (startkey=\"\" and endkey=\"\") then, after each iteration, we use the lastkey to set the startkey for the next page.  except on first page, we always skip the first item of the page (knowing that it is a repeat, the last one, of the prior page).  to get the lastkey we tried two strategies: [1] set the lastkey to the last item in the page, and [2] use string.compareto to get the largest ley. neither strategy worked.  our keys are strings (obviously the only option in 0.6) that represent numbers.  some sample keys are: (in correct lexi order)  -1  11113  11457  6831  7035  8060  8839  ------  this code (without any changes) was working correctly under 0.6.3 (we  got same response from getrangeslices if using requestblockcounts of  10,000 or 100).  we tried it under 0.6.4 and 0.6.5 and it stopped working.  we reverted back to 0.6.3 and (again, without changing the code) it  started working again.  ------  i tried inserting all the keys into a test cluster of one (1 machine) and it worked fine.  so this must be related to how the page is build in a cluster of more than 1 nodes.  we have a cluster of 5 nodes with replication factor of 3.<stacktrace> <code> <text> hi,  we just recently tried to use 0.6.4 and 0.6.5 in our production environment and  had some serious problem.  the getrangeslices functionality is broken.  we have a cluster of 5 machines.  we use getrangeslices to iterate over all of the keys in a cf (2062 keys total).  we are using orderpreservingpartitioner.  we use getrangeslices with keyrange using keys (not tokens).  if we set the requestblockcount (aka: keyrange.setcount()) to a number  greater than 2062 we get all keys in one shot (all is good).  if we try to fetch the keys in smaller blocks (requestblockcount=100)  we get bad results.  we get only 800 unique keys back.  we start with (startkey='' and endkey='') then, after each iteration, we use the lastkey to set the startkey for the next page.  except on first page, we always skip the first item of the page (knowing that it is a repeat, the last one, of the prior page).  to get the lastkey we tried two strategies: [1] set the lastkey to the last item in the page, and [2] use string.compareto to get the largest ley. neither strategy worked.  our keys are strings (obviously the only option in 0.6) that represent numbers.  some sample keys are: (in correct lexi order)  -1  11113  11457  6831  7035  8060  8839  ------  this code (without any changes) was working correctly under 0.6.3 (we  got same response from getrangeslices if using requestblockcounts of  10,000 or 100).  we tried it under 0.6.4 and 0.6.5 and it stopped working.  we reverted back to 0.6.3 and (again, without changing the code) it  started working again.  ------  i tried inserting all the keys into a test cluster of one (1 machine) and it worked fine.  so this must be related to how the page is build in a cluster of more than 1 nodes.  we have a cluster of 5 nodes with replication factor of 3.",
        "label": 515
    },
    {
        "text": "create release ant target <description> in order to simplify nightly builds and releases we should create an ant target that builds the project and rolls a tarball.<stacktrace> <code> <text> in order to simplify nightly builds and releases we should create an ant target that builds the project and rolls a tarball.",
        "label": 264
    },
    {
        "text": "paxos replay of in progress update is incorrect <description> when we replay inprogress, we need to refresh it with the newly prepared ballot, or it will be (correctly) rejected.<stacktrace> <code> <text> when we replay inprogress, we need to refresh it with the newly prepared ballot, or it will be (correctly) rejected.",
        "label": 274
    },
    {
        "text": "switch to jdk7's standardcharsets <description> instead of using charset.forname() take advantage of jdk7's standardcharsets that provides utf-8 and us-ascii character sets as static fields. http://docs.oracle.com/javase/7/docs/api/java/nio/charset/standardcharsets.html<stacktrace> <code> http://docs.oracle.com/javase/7/docs/api/java/nio/charset/standardcharsets.html<text> instead of using charset.forname() take advantage of jdk7's standardcharsets that provides utf-8 and us-ascii character sets as static fields. ",
        "label": 78
    },
    {
        "text": "meteredflusher should ignore memtables not affected by it <description> before metered flusher runs, count up the number of bytes used by memtables unaffected by metered flusher and subtract that from the maximum allowed bytes.<stacktrace> <code> <text> before metered flusher runs, count up the number of bytes used by memtables unaffected by metered flusher and subtract that from the maximum allowed bytes.",
        "label": 18
    },
    {
        "text": "migrating clusters with gossip tables that have old dead nodes causes npe  inability to join cluster <description> i had done a removetoken on this cluster when it was 1.1.x, and it had a \"ghost\" entry for the removed node still in the stored ring data. when the nodes loaded the table up after conversion to 1.2 and attempting to migrate to vnodes, i got the following traceback: error [write-/10.0.0.0] 2013-01-31 18:35:44,788 cassandradaemon.java (line 133) exception in thread thread[write-/10.0.0.0,5,main]  java.lang.nullpointerexception  at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:167)  at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:124)  at org.apache.cassandra.cql.jdbc.jdbcutf8.getstring(jdbcutf8.java:73)  at org.apache.cassandra.cql.jdbc.jdbcutf8.compose(jdbcutf8.java:93)  at org.apache.cassandra.db.marshal.utf8type.compose(utf8type.java:32)  at org.apache.cassandra.cql3.untypedresultset$row.getstring(untypedresultset.java:96)  at org.apache.cassandra.db.systemtable.loaddcrackinfo(systemtable.java:402)  at org.apache.cassandra.locator.ec2snitch.getdatacenter(ec2snitch.java:117)  at org.apache.cassandra.locator.dynamicendpointsnitch.getdatacenter(dynamicendpointsnitch.java:127)  at org.apache.cassandra.net.outboundtcpconnection.islocaldc(outboundtcpconnection.java:74)  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:270)  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:142) this is because these ghost nodes had a null tokens list in the system/peers table. a workaround was to delete the offending row in the system/peers table and restart the node.<stacktrace> error [write-/10.0.0.0] 2013-01-31 18:35:44,788 cassandradaemon.java (line 133) exception in thread thread[write-/10.0.0.0,5,main]  java.lang.nullpointerexception  at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:167)  at org.apache.cassandra.utils.bytebufferutil.string(bytebufferutil.java:124)  at org.apache.cassandra.cql.jdbc.jdbcutf8.getstring(jdbcutf8.java:73)  at org.apache.cassandra.cql.jdbc.jdbcutf8.compose(jdbcutf8.java:93)  at org.apache.cassandra.db.marshal.utf8type.compose(utf8type.java:32)  at org.apache.cassandra.cql3.untypedresultset$row.getstring(untypedresultset.java:96)  at org.apache.cassandra.db.systemtable.loaddcrackinfo(systemtable.java:402)  at org.apache.cassandra.locator.ec2snitch.getdatacenter(ec2snitch.java:117)  at org.apache.cassandra.locator.dynamicendpointsnitch.getdatacenter(dynamicendpointsnitch.java:127)  at org.apache.cassandra.net.outboundtcpconnection.islocaldc(outboundtcpconnection.java:74)  at org.apache.cassandra.net.outboundtcpconnection.connect(outboundtcpconnection.java:270)  at org.apache.cassandra.net.outboundtcpconnection.run(outboundtcpconnection.java:142) <code> <text> i had done a removetoken on this cluster when it was 1.1.x, and it had a 'ghost' entry for the removed node still in the stored ring data. when the nodes loaded the table up after conversion to 1.2 and attempting to migrate to vnodes, i got the following traceback: this is because these ghost nodes had a null tokens list in the system/peers table. a workaround was to delete the offending row in the system/peers table and restart the node.",
        "label": 85
    },
    {
        "text": "the  read defragmentation  optimization does not work <description> the so-called \"read defragmentation\" that has been added way back with cassandra-2503 actually does not work, and never has. that is, the defragmentation writes do happen, but they only additional load on the nodes without helping anything, and are thus a clear negative. the \"read defragmentation\" (which only impact so-called \"names queries\") kicks in when a read hits \"too many\" sstables (> 4 by default), and when it does, it writes down the result of that read. the assumption being that the next read for that data would only read the newly written data, which if not still in memtable would at least be in a single sstable, thus speeding that next read. unfortunately, this is not how this work. when we defrag and write the result of our original read, we do so with the timestamp of the data read (as we should, changing the timestamp would be plain wrong). and as a result, following reads will read that data first, but will have no way to tell that no more sstables should be read. technically, the reducefilter call will not return null because the currentmaxts will be higher than at least some of the data in the result, and this until we've read from as many sstables than in the original read. i see no easy way to fix this. it might be possible to make it work with additional per-sstable metadata, but nothing sufficiently simple and cheap to be worth it comes to mind. and i thus suggest simply removing that code. for the record, i'll note that there is actually a 2nd problem with that code: currently, we \"defrag\" a read even if we didn't got data for everything that the query requests. this also is \"wrong\" even if we ignore the first issue: a following read that would read the defragmented data would also have no way to know to not read more sstables to try to get the missing parts. this problem would be fixeable, but is obviously overshadowed by the previous one anyway. anyway, as mentioned, i suggest to just remove the \"optimization\" (which again, never optimized anything) altogether, and happy to provide the simple patch. the only question might be in which versions? this impact all versions, but this isn't a correction bug either, \"just\" a performance one. so do we want 4.0 only or is there appetite for earlier?<stacktrace> <code> <text> the so-called 'read defragmentation' that has been added way back with cassandra-2503 actually does not work, and never has. that is, the defragmentation writes do happen, but they only additional load on the nodes without helping anything, and are thus a clear negative. the 'read defragmentation' (which only impact so-called 'names queries') kicks in when a read hits 'too many' sstables (> 4 by default), and when it does, it writes down the result of that read. the assumption being that the next read for that data would only read the newly written data, which if not still in memtable would at least be in a single sstable, thus speeding that next read. unfortunately, this is not how this work. when we defrag and write the result of our original read, we do so with the timestamp of the data read (as we should, changing the timestamp would be plain wrong). and as a result, following reads will read that data first, but will have no way to tell that no more sstables should be read. technically, the reducefilter call will not return null because the currentmaxts will be higher than at least some of the data in the result, and this until we've read from as many sstables than in the original read. i see no easy way to fix this. it might be possible to make it work with additional per-sstable metadata, but nothing sufficiently simple and cheap to be worth it comes to mind. and i thus suggest simply removing that code. for the record, i'll note that there is actually a 2nd problem with that code: currently, we 'defrag' a read even if we didn't got data for everything that the query requests. this also is 'wrong' even if we ignore the first issue: a following read that would read the defragmented data would also have no way to know to not read more sstables to try to get the missing parts. this problem would be fixeable, but is obviously overshadowed by the previous one anyway. anyway, as mentioned, i suggest to just remove the 'optimization' (which again, never optimized anything) altogether, and happy to provide the simple patch. the only question might be in which versions? this impact all versions, but this isn't a correction bug either, 'just' a performance one. so do we want 4.0 only or is there appetite for earlier?",
        "label": 520
    },
    {
        "text": "create long running test suite <description> we need to start running tests that run for at least several hours. our current dtest suite is inadequate at catching data loss bugs and compaction problems.<stacktrace> <code> <text> we need to start running tests that run for at least several hours. our current dtest suite is inadequate at catching data loss bugs and compaction problems.",
        "label": 428
    },
    {
        "text": "index caching <description> currently, the \"hidden\" cfs backing secondary indexes are created with caching.none, which can result in slower-than-expected index queries.<stacktrace> <code> <text> currently, the 'hidden' cfs backing secondary indexes are created with caching.none, which can result in slower-than-expected index queries.",
        "label": 577
    },
    {
        "text": "copy from fails when importing blob <description> when we try to copy to a table containing a blob, we get this error   copy test.blobtable from '/tmp/test1.csv' with null='null' and delimiter=',' and quote='\"'; /opt/apache-cassandra-2.1.13.4/bin/../pylib/cqlshlib/copyutil.py:1602: deprecationwarning: baseexception.message has been deprecated as of python 2.6  /opt/apache-cassandra-2.1.13.4/bin/../pylib/cqlshlib/copyutil.py:1850: deprecationwarning: baseexception.message has been deprecated as of python 2.6  failed to import 5 rows: parseerror - fromhex() argument 1 must be unicode, not str - given up without retries  failed to process 5 rows; failed rows written to import_test_blobtable.err same copy to function worked fine with 2.1.9  the csv is generated by doing a copy from on the same table.  is there any work around this issue?<stacktrace> <code> when we try to copy to a table containing a blob, we get this error   copy test.blobtable from '/tmp/test1.csv' with null='null' and delimiter=',' and quote='''; /opt/apache-cassandra-2.1.13.4/bin/../pylib/cqlshlib/copyutil.py:1602: deprecationwarning: baseexception.message has been deprecated as of python 2.6  /opt/apache-cassandra-2.1.13.4/bin/../pylib/cqlshlib/copyutil.py:1850: deprecationwarning: baseexception.message has been deprecated as of python 2.6  failed to import 5 rows: parseerror - fromhex() argument 1 must be unicode, not str - given up without retries  failed to process 5 rows; failed rows written to import_test_blobtable.err <text> same copy to function worked fine with 2.1.9  the csv is generated by doing a copy from on the same table.  is there any work around this issue?",
        "label": 508
    },
    {
        "text": "cql  errors when running unqualified  select column  statement  no where clause  <description> seed data create keyspace cqldb with strategy_class = 'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=2; use cqldb; create columnfamily users (key varchar primary key, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint); insert into users (key, password) values ('user0', 'ch@ngem3'); insert into users (key, password, gender, state, birth_year) values ('user1', 'ch@ngem3a', 'f', 'tx', '1968'); insert into users (key, password) values ('user2', 'ch@ngem3b'); insert into users (key, password) values ('user3', 'ch@ngem3c'); query #1 - select varchar column cqlsh> select state from users; u'user1' | u'state',u'tx' exception: 'nonetype' object has no attribute 'decode' cqlsh> select state from users where key='user1'; u'user1' | u'state',u'tx' query #2 - select bigint column cqlsh> select birth_year from users; exception: unpack requires a string argument of length 8 cqlsh> select birth_year from users where key='user1'; u'user1' | u'birth_year',1968 a simple 'select *' with no where clause works fine cqlsh> select * from users; u'user1' | u'birth_year',1968 | u'gender',u'f' | u'password',u'ch@ngem3a' | u'state',u'tx' u'user0' | u'password',u'ch@ngem3' u'user3' | u'password',u'ch@ngem3c' u'user2' | u'password',u'ch@ngem3b'<stacktrace> <code> create keyspace cqldb with strategy_class = 'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=2; use cqldb; create columnfamily users (key varchar primary key, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint); insert into users (key, password) values ('user0', 'ch@ngem3'); insert into users (key, password, gender, state, birth_year) values ('user1', 'ch@ngem3a', 'f', 'tx', '1968'); insert into users (key, password) values ('user2', 'ch@ngem3b'); insert into users (key, password) values ('user3', 'ch@ngem3c'); cqlsh> select state from users; u'user1' | u'state',u'tx' exception: 'nonetype' object has no attribute 'decode' cqlsh> select state from users where key='user1'; u'user1' | u'state',u'tx' cqlsh> select birth_year from users; exception: unpack requires a string argument of length 8 cqlsh> select birth_year from users where key='user1'; u'user1' | u'birth_year',1968 cqlsh> select * from users; u'user1' | u'birth_year',1968 | u'gender',u'f' | u'password',u'ch@ngem3a' | u'state',u'tx' u'user0' | u'password',u'ch@ngem3' u'user3' | u'password',u'ch@ngem3c' u'user2' | u'password',u'ch@ngem3b' <text> seed data query #1 - select varchar column query #2 - select bigint column a simple 'select *' with no where clause works fine",
        "label": 412
    },
    {
        "text": "sstable2json doesn't work for secondary index sstable due to partitioner mismatch <description> sstable2json doesn't work for secondary index sstable in 1.0.6 while it worked in version 0.8.x. $ bin/sstable2json $data/data/keyspace1/users-hc-1-data.db   {  \"1111\": [[\"birth_year\",\"1973\",1326450301786000], [\"full_name\",\"patrick rothfuss\",1326450301782000]],  \"1020\": [[\"birth_year\",\"1975\",1326450301776000], [\"full_name\",\"brandon sanderson\",1326450301716000]]  } $ bin/sstable2json $data/data/keyspace1/users.users_birth_year_idx-hc-1-data.db   exception in thread \"main\" java.lang.runtimeexception: cannot open data/keyspace1/users.users_birth_year_idx-hc-1 because partitioner does not match org.apache.cassandra.dht.randompartitioner  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:145)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:123)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:118)  at org.apache.cassandra.tools.sstableexport.export(sstableexport.java:360)  at org.apache.cassandra.tools.sstableexport.export(sstableexport.java:373)  at org.apache.cassandra.tools.sstableexport.main(sstableexport.java:431) i tested with following sample data via cli: create keyspace keyspace1;  use keyspace1;  create column family users with comparator=utf8type and  column_metadata=[ {column_name: full_name, validation_class: utf8type} , {column_name: email, validation_class: utf8type} , {column_name: birth_year, validation_class: longtype, index_type: keys} , {column_name: state, validation_class: utf8type, index_type: keys} ];  set users[1020][full_name] = 'brandon sanderson';  set users[1020][birth_year] = 1975;   set users[1111][full_name] = 'patrick rothfuss';   set users[1111][birth_year] = 1973;  get users where birth_year = 1973;<stacktrace> $ bin/sstable2json $data/data/keyspace1/users.users_birth_year_idx-hc-1-data.db   exception in thread 'main' java.lang.runtimeexception: cannot open data/keyspace1/users.users_birth_year_idx-hc-1 because partitioner does not match org.apache.cassandra.dht.randompartitioner  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:145)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:123)  at org.apache.cassandra.io.sstable.sstablereader.open(sstablereader.java:118)  at org.apache.cassandra.tools.sstableexport.export(sstableexport.java:360)  at org.apache.cassandra.tools.sstableexport.export(sstableexport.java:373)  at org.apache.cassandra.tools.sstableexport.main(sstableexport.java:431) <code> $ bin/sstable2json $data/data/keyspace1/users-hc-1-data.db   {  '1111': [['birth_year','1973',1326450301786000], ['full_name','patrick rothfuss',1326450301782000]],  '1020': [['birth_year','1975',1326450301776000], ['full_name','brandon sanderson',1326450301716000]]  } , , , ];  set users[1020][full_name] = 'brandon sanderson';  set users[1020][birth_year] = 1975;   set users[1111][full_name] = 'patrick rothfuss';   set users[1111][birth_year] = 1973;  get users where birth_year = 1973;<text> sstable2json doesn't work for secondary index sstable in 1.0.6 while it worked in version 0.8.x. i tested with following sample data via cli: create keyspace keyspace1;  use keyspace1;  create column family users with comparator=utf8type and  column_metadata=[ ",
        "label": 577
    },
    {
        "text": "documentation cites three but only two rpc server types provided <description> <stacktrace> <code> <text> ",
        "label": 594
    },
    {
        "text": "user types allow counters <description> from the conversation on cassandra-6312 it seems we should not allow user types to contain counters. presently, user types can be defined with field types of counter, and these user types can also be associated with tables without error. i'm not certain if there's a compelling case for counters within user types, but i don't think there's any syntax existing presently that would allow updating them anyway. to repro: create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; use test; create type t_item (   sub_one counter );  create type item (   sub_one counter );  create table test.mytable (     value1 text primary key,     item t_item,     value2 text ); cqlsh:test> insert into mytable (value1, value2) values ( 'foo', 'bar'); cqlsh:test> select * from mytable;  value1 | item | value2 --------+------+--------     foo | null |    bar <stacktrace> <code> create keyspace test with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; use test; create type t_item (   sub_one counter );  create type item (   sub_one counter );  create table test.mytable (     value1 text primary key,     item t_item,     value2 text ); cqlsh:test> insert into mytable (value1, value2) values ( 'foo', 'bar'); cqlsh:test> select * from mytable;  value1 | item | value2 --------+------+--------     foo | null |    bar <text> from the conversation on cassandra-6312 it seems we should not allow user types to contain counters. presently, user types can be defined with field types of counter, and these user types can also be associated with tables without error. i'm not certain if there's a compelling case for counters within user types, but i don't think there's any syntax existing presently that would allow updating them anyway. to repro:",
        "label": 18
    },
    {
        "text": "cqlsh copy functionality doesn't work together with source or with cqlsh  f <description> executing a copy command from an external file using the cqlsh -f or the source command results in the error: filename.cql:7:descriptor 'lower' requires a 'str' object but received a 'unicode' looks like there was a change in the cqlsh code from 2.1.2 to 2.1.3 which makes use of codecs.open() instead of open(), which returns a unicode object. the offending line of code that returns the error seems to be in cqlsh, line 1415:  copyoptnames = map(str.lower, parsed.get_binding('optnames', ()))<stacktrace> <code> the offending line of code that returns the error seems to be in cqlsh, line 1415:  copyoptnames = map(str.lower, parsed.get_binding('optnames', ()))<text> executing a copy command from an external file using the cqlsh -f or the source command results in the error: filename.cql:7:descriptor 'lower' requires a 'str' object but received a 'unicode' looks like there was a change in the cqlsh code from 2.1.2 to 2.1.3 which makes use of codecs.open() instead of open(), which returns a unicode object. ",
        "label": 538
    },
    {
        "text": "assertionerror in dk <description> when running the dtests: error [pool-2-thread-1] 2011-12-05 16:22:53,940 cassandra.java (line 4082) internal error processing execute_cql_query java.lang.assertionerror         at org.apache.cassandra.db.decoratedkey.<init>(decoratedkey.java:56)         at org.apache.cassandra.dht.randompartitioner.decoratekey(randompartitioner.java:47)         at org.apache.cassandra.cql.queryprocessor.multirangeslice(queryprocessor.java:161)         at org.apache.cassandra.cql.queryprocessor.process(queryprocessor.java:549)         at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1249)         at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.process(cassandra.java:4072)         at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:2889)         at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:187)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) i suspect cassandra-1034 is to blame.<stacktrace> error [pool-2-thread-1] 2011-12-05 16:22:53,940 cassandra.java (line 4082) internal error processing execute_cql_query java.lang.assertionerror         at org.apache.cassandra.db.decoratedkey.<init>(decoratedkey.java:56)         at org.apache.cassandra.dht.randompartitioner.decoratekey(randompartitioner.java:47)         at org.apache.cassandra.cql.queryprocessor.multirangeslice(queryprocessor.java:161)         at org.apache.cassandra.cql.queryprocessor.process(queryprocessor.java:549)         at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1249)         at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.process(cassandra.java:4072)         at org.apache.cassandra.thrift.cassandra$processor.process(cassandra.java:2889)         at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:187)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) <code> <text> when running the dtests: i suspect cassandra-1034 is to blame.",
        "label": 520
    },
    {
        "text": "clean up thread pool and queue sizes <description> most of cassandra assumes that threadpoolexecutor handles tasks by starting with core threads, adding threads up to max as tasks arrive, then queuing any additional. this is not correct:     if fewer than corepoolsize threads are running, the executor always prefers adding a new thread rather than queuing.     if corepoolsize or more threads are running, the executor always prefers queuing a request rather than adding a new thread.     if a request cannot be queued, a new thread is created unless this would exceed maximumpoolsize, in which case, the task will be rejected. cassandra-2178 fixed this in one place but made it worse by default since most people run with a single data dir, meaning as soon as you have multiple cfs flushing (or a single one with indexes) then you will start blocking writes.<stacktrace> <code> <text>     if fewer than corepoolsize threads are running, the executor always prefers adding a new thread rather than queuing.     if corepoolsize or more threads are running, the executor always prefers queuing a request rather than adding a new thread.     if a request cannot be queued, a new thread is created unless this would exceed maximumpoolsize, in which case, the task will be rejected. most of cassandra assumes that threadpoolexecutor handles tasks by starting with core threads, adding threads up to max as tasks arrive, then queuing any additional. this is not correct: cassandra-2178 fixed this in one place but made it worse by default since most people run with a single data dir, meaning as soon as you have multiple cfs flushing (or a single one with indexes) then you will start blocking writes.",
        "label": 274
    },
    {
        "text": "sstable2json always returns default value validator <description> when exporting to json tables created using cqlsh, values are always exported to hexa : the serializecolumn function fails to get the correct value validator from the cfmetadata, since getcolumndefinition(column) returns null. i'm not a java expert and doesn't really understand the use of bytebuffer for map : the workaround i found is to pass to cfmetadata.getvaluevalidator a new wrapped bytebuffer of the column.name argument, then the validator is correctly found from the cfmetadata.column_metadata.<stacktrace> <code> <text> when exporting to json tables created using cqlsh, values are always exported to hexa : the serializecolumn function fails to get the correct value validator from the cfmetadata, since getcolumndefinition(column) returns null. i'm not a java expert and doesn't really understand the use of bytebuffer for map : the workaround i found is to pass to cfmetadata.getvaluevalidator a new wrapped bytebuffer of the column.name argument, then the validator is correctly found from the cfmetadata.column_metadata.",
        "label": 520
    },
    {
        "text": "islocaldc  in outboundtcpconnection class retrieves local ip in wrong way <description> my question from dev mailing list: can someone explain me why islocaldc() in outboundtcpconnection class uses databasedescriptor.getrpcaddress() for retrieving \"local\" ip, instead of using dd.getlistenaddress() or - even better - fbutilities.getlocaladdress()? i mean - i don't get why rpc address is checked before initializing internode (so not rpc-based) communication, which will use ip address returned by (in otcpool) fbutilities.getlocaladdress()? and response by marcus eriksson: that should probably be fbutilities.getbroadcastaddress even, could you file a ticket?<stacktrace> <code> <text> my question from dev mailing list: can someone explain me why islocaldc() in outboundtcpconnection class uses databasedescriptor.getrpcaddress() for retrieving 'local' ip, instead of using dd.getlistenaddress() or - even better - fbutilities.getlocaladdress()? i mean - i don't get why rpc address is checked before initializing internode (so not rpc-based) communication, which will use ip address returned by (in otcpool) fbutilities.getlocaladdress()? and response by marcus eriksson: that should probably be fbutilities.getbroadcastaddress even, could you file a ticket?",
        "label": 321
    },
    {
        "text": "replace trivial uses of string replace replaceall split with stringutils methods <description> there are places in the code where those regex based methods are used with plain, not regexp, strings, so stringutils alternatives should be faster.<stacktrace> <code> <text> there are places in the code where those regex based methods are used with plain, not regexp, strings, so stringutils alternatives should be faster.",
        "label": 27
    },
    {
        "text": "print sensible units for all log messages <description> like cassandra-9691, this has bugged me too long. it also adversely impacts log analysis. i've introduced some improvements to the bits i touched for cassandra-9681, but we should do this across the codebase. it's a small investment for a lot of long term clarity in the logs.<stacktrace> <code> <text> like cassandra-9691, this has bugged me too long. it also adversely impacts log analysis. i've introduced some improvements to the bits i touched for cassandra-9681, but we should do this across the codebase. it's a small investment for a lot of long term clarity in the logs.",
        "label": 190
    },
    {
        "text": "oom intermittently during compaction <description> jvm crashes intermittently during compaction. our test data set is not that big, less than 10 gb.  when jvm is about to crash, we see that it consumes a lot of memory (exceeding the max heap size). the excessive memory usage during compaction is caused by the maintenance of blockindexes_ in sstable. this blockindexes_ was only introduced to the apache version.<stacktrace> <code> <text> jvm crashes intermittently during compaction. our test data set is not that big, less than 10 gb.  when jvm is about to crash, we see that it consumes a lot of memory (exceeding the max heap size). the excessive memory usage during compaction is caused by the maintenance of blockindexes_ in sstable. this blockindexes_ was only introduced to the apache version.",
        "label": 274
    },
    {
        "text": "prepared statement on defunct cf can impact cluster availability <description> synopsis: misbehaving clients can cause dos on a cluster with a defunct prepared statement scenario:   1.) create prepared insert statement on existing table x  2.) table x is dropped  3.) continue using prepared statement from (1) result:   a.) on coordinator node: commit-log-writer + mutationstage errors  b.) on other nodes: \"unknowncolumnfamilyexception reading from socket; closing\" --> leads to thrashing inter-node connections  c.) other clients of the cluster suffer from i/o timeouts, presumably a result of (b) other observations: on single-node clusters, clients return from insert without error because mutation errors are swallowed. on multiple-node clusters, clients receive a confounded 'read timeout' error because the closed internode connections do not propagate the error back. with prepared select statements (as opposed to insert described above). a nullpointerexception is caused on the server, and no meaninful error is returned to the client. besides the obvious \"don't do that\" to the integrator, it would be good if the cluster could handle this error case more gracefully and avoid undue impact.<stacktrace> <code> scenario:   1.) create prepared insert statement on existing table x  2.) table x is dropped  3.) continue using prepared statement from (1) result:   a.) on coordinator node: commit-log-writer + mutationstage errors  b.) on other nodes: 'unknowncolumnfamilyexception reading from socket; closing' --> leads to thrashing inter-node connections  c.) other clients of the cluster suffer from i/o timeouts, presumably a result of (b) <text> synopsis: misbehaving clients can cause dos on a cluster with a defunct prepared statement other observations: besides the obvious 'don't do that' to the integrator, it would be good if the cluster could handle this error case more gracefully and avoid undue impact.",
        "label": 8
    },
    {
        "text": "all test targets to set jvm arg  java io tmpdir  according to  tmp dir  <description> make all test targets declare the temp directory (java.io.tmpdir) as defined by tmp.dir this was originally done in cassandra-7712 but has been eroded over time as the test targets evolved. the attached patch moves the <jvmarg value=\"-djava.io.tmpdir=${tmp.dir}\"/> into the testmacrohelper macrodef. like cassandra-7712, jenkins agents are filling up their root volumes because of what's left behind under \"/tmp\" the build scripts have already been configured to define the temp directory to be within the jenkins job's workspace: https://github.com/apache/cassandra-builds/commit/33ba1e30ea196180f7d70f8e6ec47fdf6844f3f6#diff-91876f5f158ec50dab9a70cc06c06922<stacktrace> <code> <jvmarg value='-djava.io.tmpdir=${tmp.dir}'/> <text> make all test targets declare the temp directory (java.io.tmpdir) as defined by tmp.dir this was originally done in cassandra-7712 but has been eroded over time as the test targets evolved. the attached patch moves the into the testmacrohelper macrodef. like cassandra-7712, jenkins agents are filling up their root volumes because of what's left behind under '/tmp' the build scripts have already been configured to define the temp directory to be within the jenkins job's workspace: https://github.com/apache/cassandra-builds/commit/33ba1e30ea196180f7d70f8e6ec47fdf6844f3f6#diff-91876f5f158ec50dab9a70cc06c06922",
        "label": 165
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh tests testcqlsh test describe <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1283/testreport/cqlsh_tests.cqlsh_tests/testcqlsh/test_describe failed on cassci build trunk_dtest #1283 logs are attached. stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py\", line 726, in test_describe     self.execute(cql=\"describe test.users2\", expected_err=\"'users2' not found in keyspace 'test'\")   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py\", line 986, in execute     self.check_response(err, expected_err)   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py\", line 999, in check_response     self.assertequal(expected_lines, lines)   file \"/usr/lib/python2.7/unittest/case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 742, in assertlistequal     self.assertsequenceequal(list1, list2, msg, seq_type=list)   file \"/usr/lib/python2.7/unittest/case.py\", line 724, in assertsequenceequal     self.fail(msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 410, in fail     raise self.failureexception(msg) error message lists differ: [\"'users2' not found in keyspa... != [\"error: ('unable to complete ... first differing element 0: 'users2' not found in keyspace 'test' error: ('unable to complete the operation against any hosts', {}) - [\"'users2' not found in keyspace 'test'\"] + [\"error: ('unable to complete the operation against any hosts', {})\"]<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py', line 726, in test_describe     self.execute(cql='describe test.users2', expected_err=''users2' not found in keyspace 'test'')   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py', line 986, in execute     self.check_response(err, expected_err)   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_tests.py', line 999, in check_response     self.assertequal(expected_lines, lines)   file '/usr/lib/python2.7/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file '/usr/lib/python2.7/unittest/case.py', line 742, in assertlistequal     self.assertsequenceequal(list1, list2, msg, seq_type=list)   file '/usr/lib/python2.7/unittest/case.py', line 724, in assertsequenceequal     self.fail(msg)   file '/usr/lib/python2.7/unittest/case.py', line 410, in fail     raise self.failureexception(msg) error message lists differ: [''users2' not found in keyspa... != ['error: ('unable to complete ... first differing element 0: 'users2' not found in keyspace 'test' error: ('unable to complete the operation against any hosts', {}) - [''users2' not found in keyspace 'test''] + ['error: ('unable to complete the operation against any hosts', {})'] http://cassci.datastax.com/job/trunk_dtest/1283/testreport/cqlsh_tests.cqlsh_tests/testcqlsh/test_describe <text> example failure: failed on cassci build trunk_dtest #1283 logs are attached.",
        "label": 261
    },
    {
        "text": "add missing  bat files for tools <description> clustertool, config-converter, schematool and sstablekeys don't have windows versions.<stacktrace> <code> <text> clustertool, config-converter, schematool and sstablekeys don't have windows versions.",
        "label": 562
    },
    {
        "text": "fix flaky test test expiration overflow policy cap   ttl test testttl <description> https://app.circleci.com/pipelines/github/dcapwell/cassandra/622/workflows/adcd463c-156a-43c7-a9bc-7f3e4938dbe8/jobs/3514 >           assert warning, 'log message should be print for cap and cap_nowarn policy' e           assertionerror: log message should be print for cap and cap_nowarn policy e           assert [] ttl_test.py:410: assertionerror<stacktrace> <code> >           assert warning, 'log message should be print for cap and cap_nowarn policy' e           assertionerror: log message should be print for cap and cap_nowarn policy e           assert [] ttl_test.py:410: assertionerror https://app.circleci.com/pipelines/github/dcapwell/cassandra/622/workflows/adcd463c-156a-43c7-a9bc-7f3e4938dbe8/jobs/3514<text> ",
        "label": 8
    },
    {
        "text": "user defined compaction is broken <description> currently forceuserdefinedcompaction takes (keyspace, datafile) cassandra tries to look for ks/ks-cf-hf-80-data.db when the sstable actually exists at ks/cf/ks-cf-hf-80-data.db fix would be for user defined compaction to look for the sstable datafile in the correct location<stacktrace> <code> currently forceuserdefinedcompaction takes (keyspace, datafile) <text> cassandra tries to look for ks/ks-cf-hf-80-data.db when the sstable actually exists at ks/cf/ks-cf-hf-80-data.db fix would be for user defined compaction to look for the sstable datafile in the correct location",
        "label": 577
    },
    {
        "text": "remove unused on heap bloomfilter implementation <description> seems like it's just dead code, should that be removed?<stacktrace> <code> <text> seems like it's just dead code, should that be removed?",
        "label": 234
    },
    {
        "text": "corrupted commit logs <description> two of our nodes had a hard failure. they both came up with a corrupted commit log. on startup we get this: 011-02-07_19:34:03.95124 info - finished reading /var/lib/cassandra/commitlog/commitlog-1297099954252.log  2011-02-07_19:34:03.95400 error - exception encountered during startup.  2011-02-07_19:34:03.95403 java.io.eofexception  2011-02-07_19:34:03.95403 at java.io.datainputstream.readunsignedshort(datainputstream.java:323)  2011-02-07_19:34:03.95404 at java.io.datainputstream.readutf(datainputstream.java:572)  2011-02-07_19:34:03.95405 at java.io.datainputstream.readutf(datainputstream.java:547)  2011-02-07_19:34:03.95406 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:363)  2011-02-07_19:34:03.95407 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:318)  2011-02-07_19:34:03.95408 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:240)  2011-02-07_19:34:03.95409 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:172)  2011-02-07_19:34:03.95409 at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:115)  2011-02-07_19:34:03.95410 at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  2011-02-07_19:34:03.95422 exception encountered during startup.  2011-02-07_19:34:03.95436 java.io.eofexception  2011-02-07_19:34:03.95447 at java.io.datainputstream.readunsignedshort(datainputstream.java:323)  2011-02-07_19:34:03.95458 at java.io.datainputstream.readutf(datainputstream.java:572)  2011-02-07_19:34:03.95468 at java.io.datainputstream.readutf(datainputstream.java:547)  2011-02-07_19:34:03.95478 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:363)  2011-02-07_19:34:03.95489 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:318)  2011-02-07_19:34:03.95499 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:240)  2011-02-07_19:34:03.95510 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:172)  2011-02-07_19:34:03.95521 at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:115)  2011-02-07_19:34:03.95531 at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224) on node a, the commit log in question is 100mb. on node b, the commit log in question is 60mb. an ideal resolution would be if eof is hit early, log something, but don't stop the startup. instead process everything that we have done so far, and keep going.<stacktrace> 011-02-07_19:34:03.95124 info - finished reading /var/lib/cassandra/commitlog/commitlog-1297099954252.log  2011-02-07_19:34:03.95400 error - exception encountered during startup.  2011-02-07_19:34:03.95403 java.io.eofexception  2011-02-07_19:34:03.95403 at java.io.datainputstream.readunsignedshort(datainputstream.java:323)  2011-02-07_19:34:03.95404 at java.io.datainputstream.readutf(datainputstream.java:572)  2011-02-07_19:34:03.95405 at java.io.datainputstream.readutf(datainputstream.java:547)  2011-02-07_19:34:03.95406 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:363)  2011-02-07_19:34:03.95407 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:318)  2011-02-07_19:34:03.95408 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:240)  2011-02-07_19:34:03.95409 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:172)  2011-02-07_19:34:03.95409 at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:115)  2011-02-07_19:34:03.95410 at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224)  2011-02-07_19:34:03.95422 exception encountered during startup.  2011-02-07_19:34:03.95436 java.io.eofexception  2011-02-07_19:34:03.95447 at java.io.datainputstream.readunsignedshort(datainputstream.java:323)  2011-02-07_19:34:03.95458 at java.io.datainputstream.readutf(datainputstream.java:572)  2011-02-07_19:34:03.95468 at java.io.datainputstream.readutf(datainputstream.java:547)  2011-02-07_19:34:03.95478 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:363)  2011-02-07_19:34:03.95489 at org.apache.cassandra.db.rowmutationserializer.deserialize(rowmutation.java:318)  2011-02-07_19:34:03.95499 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:240)  2011-02-07_19:34:03.95510 at org.apache.cassandra.db.commitlog.commitlog.recover(commitlog.java:172)  2011-02-07_19:34:03.95521 at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:115)  2011-02-07_19:34:03.95531 at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:224) <code> <text> two of our nodes had a hard failure. they both came up with a corrupted commit log. on startup we get this: on node a, the commit log in question is 100mb. on node b, the commit log in question is 60mb. an ideal resolution would be if eof is hit early, log something, but don't stop the startup. instead process everything that we have done so far, and keep going.",
        "label": 274
    },
    {
        "text": "row cache does not cache partitions on tables without clustering keys <description> mlsea-jjirsa01:~ jjirsa$ ccm start mlsea-jjirsa01:~ jjirsa$ echo \"describe table test.test; \" | ccm node1 cqlsh create table test.test (     id int primary key,     v text ) with bloom_filter_fp_chance = 0.01     and caching = {'keys': 'all', 'rows_per_partition': '100'}     and comment = ''     and compaction = {'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy', 'max_threshold': '32', 'min_threshold': '4'}     and compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.lz4compressor'}     and crc_check_chance = 1.0     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99percentile'; mlsea-jjirsa01:~ jjirsa$ ccm node1 nodetool info | grep row row cache              : entries 0, size 0 bytes, capacity 100 mib, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds mlsea-jjirsa01:~ jjirsa$ echo \"insert into test.test(id,v) values(1, 'a'); \" | ccm node1 cqlsh mlsea-jjirsa01:~ jjirsa$ echo \"select * from test.test where id=1; \" | ccm node1 cqlsh  id | v ----+---   1 | a (1 rows) mlsea-jjirsa01:~ jjirsa$ ccm node1 nodetool info | grep row row cache              : entries 0, size 0 bytes, capacity 100 mib, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds mlsea-jjirsa01:~ jjirsa$ echo \"select * from test.test where id=1; \" | ccm node1 cqlsh  id | v ----+---   1 | a (1 rows) mlsea-jjirsa01:~ jjirsa$ ccm node1 nodetool info | grep row row cache              : entries 0, size 0 bytes, capacity 100 mib, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds mlsea-jjirsa01:~ jjirsa$<stacktrace> <code> mlsea-jjirsa01:~ jjirsa$ ccm start mlsea-jjirsa01:~ jjirsa$ echo 'describe table test.test; ' | ccm node1 cqlsh create table test.test (     id int primary key,     v text ) with bloom_filter_fp_chance = 0.01     and caching = {'keys': 'all', 'rows_per_partition': '100'}     and comment = ''     and compaction = {'class': 'org.apache.cassandra.db.compaction.sizetieredcompactionstrategy', 'max_threshold': '32', 'min_threshold': '4'}     and compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.lz4compressor'}     and crc_check_chance = 1.0     and dclocal_read_repair_chance = 0.1     and default_time_to_live = 0     and gc_grace_seconds = 864000     and max_index_interval = 2048     and memtable_flush_period_in_ms = 0     and min_index_interval = 128     and read_repair_chance = 0.0     and speculative_retry = '99percentile'; mlsea-jjirsa01:~ jjirsa$ ccm node1 nodetool info | grep row row cache              : entries 0, size 0 bytes, capacity 100 mib, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds mlsea-jjirsa01:~ jjirsa$ echo 'insert into test.test(id,v) values(1, 'a'); ' | ccm node1 cqlsh mlsea-jjirsa01:~ jjirsa$ echo 'select * from test.test where id=1; ' | ccm node1 cqlsh  id | v ----+---   1 | a (1 rows) mlsea-jjirsa01:~ jjirsa$ ccm node1 nodetool info | grep row row cache              : entries 0, size 0 bytes, capacity 100 mib, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds mlsea-jjirsa01:~ jjirsa$ echo 'select * from test.test where id=1; ' | ccm node1 cqlsh  id | v ----+---   1 | a (1 rows) mlsea-jjirsa01:~ jjirsa$ ccm node1 nodetool info | grep row row cache              : entries 0, size 0 bytes, capacity 100 mib, 0 hits, 0 requests, nan recent hit rate, 0 save period in seconds mlsea-jjirsa01:~ jjirsa$<text> ",
        "label": 241
    },
    {
        "text": "test coverage for conditional ddl statements <description> we only have minimal test coverage of if [not] exists conditions for ddl statements. i think dtests are the right place to add those tests. we need to cover: create keyspace if not exists drop keyspace if exists create table if not exists drop table if exists create index if not exists drop index if exists create type if not exists drop type if exists the tests should also ensure that invalidrequestexceptions are thrown if, for example, you try to drop an index from a keyspace that doesn't exist (regardless of whether if exists is used).<stacktrace> <code> <text> we only have minimal test coverage of if [not] exists conditions for ddl statements. i think dtests are the right place to add those tests. we need to cover: the tests should also ensure that invalidrequestexceptions are thrown if, for example, you try to drop an index from a keyspace that doesn't exist (regardless of whether if exists is used).",
        "label": 465
    },
    {
        "text": "cassandra cli  keyspace option doesn't work properly when used with authentication <description> the logic to select the keyspace is applied before authentication credentials are processed in cassandra-cli. this leads to a \"keyspace foo not found\" message at login for a keyspace that exists.<stacktrace> <code> <text> the logic to select the keyspace is applied before authentication credentials are processed in cassandra-cli. this leads to a 'keyspace foo not found' message at login for a keyspace that exists.",
        "label": 226
    },
    {
        "text": "user created with debian packaging is unable to increase memlock <description> to reproduce: install a fresh copy of ubuntu 10.04. install sun's java6 jdk. install libjna-java 3.2.7 into /usr/share/java. install cassandra 0.7.0 from the apache debian packages. start cassandra using /etc/init.d/cassandra  in the output.log there will be the following error: unable to lock jvm memory (enomem). this can result in part of the jvm being swapped out, especially with mmapped i/o enabled. increase rlimit_memlock or run cassandra as root. this shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'. i tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf. i was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but i could never get the init.d script to work and get beyond that error. based on all the trial and error, i think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'.<stacktrace> <code> <text> to reproduce: unable to lock jvm memory (enomem). this can result in part of the jvm being swapped out, especially with mmapped i/o enabled. increase rlimit_memlock or run cassandra as root. this shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'. i tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf. i was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but i could never get the init.d script to work and get beyond that error. based on all the trial and error, i think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'.",
        "label": 246
    },
    {
        "text": "row keys should be byte s  not strings <description> this issue has come up numerous times, and we've dealt with a lot of pain because of it: let's get it knocked out. keys being java strings can make it painful to use cassandra from other languages, encoding binary data like integers as strings is very inefficient, and there is a disconnect between our column data types and the plain string treatment we give row keys. the key design decision that needs discussion is: should we apply the column abstracttypes to row keys? if so, how do partitioners change?<stacktrace> <code> <text> this issue has come up numerous times, and we've dealt with a lot of pain because of it: let's get it knocked out. keys being java strings can make it painful to use cassandra from other languages, encoding binary data like integers as strings is very inefficient, and there is a disconnect between our column data types and the plain string treatment we give row keys. the key design decision that needs discussion is: should we apply the column abstracttypes to row keys? if so, how do partitioners change?",
        "label": 515
    },
    {
        "text": "increase precision of elapsed time in casandra cli <description> cassandra often return response in less then < 1 ms from another datacenter  [default@test] get sipdb[23];  returned 0 results.  elapsed time: 219 msec(s). from own dataceter  [default@test] get sipdb[13];  returned 0 results.  elapsed time: 0 msec(s). precession of this timer needs to be increased a bit, while looking at network latency adding 1 number should be enough like : 0.2 ms should be sufficient. it would be good to display sub milliseconds only if time elapsed is < 1 ms.<stacktrace> <code> <text> cassandra often return response in less then < 1 ms from another datacenter  [default@test] get sipdb[23];  returned 0 results.  elapsed time: 219 msec(s). from own dataceter  [default@test] get sipdb[13];  returned 0 results.  elapsed time: 0 msec(s). precession of this timer needs to be increased a bit, while looking at network latency adding 1 number should be enough like : 0.2 ms should be sufficient. it would be good to display sub milliseconds only if time elapsed is < 1 ms.",
        "label": 437
    },
    {
        "text": "save compaction merge counts in a system table <description> we should save the compaction merge stats from cassandra-4894 in the system table and probably expose them via jmx (and nodetool)<stacktrace> <code> <text> we should save the compaction merge stats from cassandra-4894 in the system table and probably expose them via jmx (and nodetool)",
        "label": 592
    },
    {
        "text": "allow coordinator failover for cursors <description> with cassandra-4415 if a coordinator fails or gets slow, causing the next request to timeout, the client application won't be able to complete its browsing of the result. that implies that most of the time when the developer will rely on cursors he will have to write some logic to handle a retry request for results starting where the iteration failed. this will quickly become painful. ideally the driver should handle this failover by itself by transparently issuing this retry query when next fail, but as the driver doesn't understand cql queries, the only thing it's aware of is the number of rows already read. therefore we should allow an optional parameter <initial_row_number> in query and execute messages that would allow a kind of stateless failover of cursors. with such an option, developers wouldn't have to write any failover/retry logic on failure as they would know that everything has already been tried by the driver.<stacktrace> <code> <text> with cassandra-4415 if a coordinator fails or gets slow, causing the next request to timeout, the client application won't be able to complete its browsing of the result. that implies that most of the time when the developer will rely on cursors he will have to write some logic to handle a retry request for results starting where the iteration failed. this will quickly become painful. ideally the driver should handle this failover by itself by transparently issuing this retry query when next fail, but as the driver doesn't understand cql queries, the only thing it's aware of is the number of rows already read. therefore we should allow an optional parameter <initial_row_number> in query and execute messages that would allow a kind of stateless failover of cursors. with such an option, developers wouldn't have to write any failover/retry logic on failure as they would know that everything has already been tried by the driver.",
        "label": 520
    },
    {
        "text": "consider providing error code with exceptions  and documenting them  <description> it could be a good idea to assign documented error code for the different exception raised. currently, one may have to parse the exception string (say if one wants to know if its 'create keyspace' failed because the keyspace already exists versus other kind of exception), but it means we cannot improve the error message at the risk of breaking client code. adding documented error codes with the message would avoid this.<stacktrace> <code> <text> it could be a good idea to assign documented error code for the different exception raised. currently, one may have to parse the exception string (say if one wants to know if its 'create keyspace' failed because the keyspace already exists versus other kind of exception), but it means we cannot improve the error message at the risk of breaking client code. adding documented error codes with the message would avoid this.",
        "label": 520
    },
    {
        "text": "once a host has been hinted to  log messages for it repeat every mins even if no hints are delivered <description>  info 15:36:03,977 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 15:36:03,978 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 15:46:31,248 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 15:46:31,249 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 15:56:29,448 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 15:56:29,449 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 16:06:09,949 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 16:06:09,950 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 16:16:21,529 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 16:16:21,530 finished hinted handoff of 0 rows to endpoint /10.179.111.137 introduced by cassandra-3554. the problem is that until a compaction on hints occurs, tombstones are present causing the isempty() check to be false.<stacktrace> <code> <text>  info 15:36:03,977 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 15:36:03,978 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 15:46:31,248 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 15:46:31,249 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 15:56:29,448 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 15:56:29,449 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 16:06:09,949 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 16:06:09,950 finished hinted handoff of 0 rows to endpoint /10.179.111.137  info 16:16:21,529 started hinted handoff for token: 170141183460469231731687303715884105726 with ip: /10.179.111.137  info 16:16:21,530 finished hinted handoff of 0 rows to endpoint /10.179.111.137 introduced by cassandra-3554. the problem is that until a compaction on hints occurs, tombstones are present causing the isempty() check to be false.",
        "label": 85
    },
    {
        "text": "add insert support to cql <description> there are two reasons to support insert: it helps new users feel comfortable (everyone's first statement will be to try to insert something, we should make that a positive experience instead of slapping them) even though it is synonymous with update it is still useful in your code to have both to help communicate intent, similar to choosing good variable names the only downside is explaining how insert isn't a \"true\" insert because it doesn't error out if the row already exists \u2013 but we already have to explain that same concept for update; the cognitive load is extremely minor.<stacktrace> <code> <text> there are two reasons to support insert: the only downside is explaining how insert isn't a 'true' insert because it doesn't error out if the row already exists - but we already have to explain that same concept for update; the cognitive load is extremely minor.",
        "label": 412
    },
    {
        "text": "'internal application error' on select   where col1 val and col2 in   <description> query with error: select * from user where login='nsv' and st in ('1','2') allow filtering; query works:  select * from user where login='nsv' and st in ('1') allow filtering;  \u2013 single item inside in table definition:   create columnfamily user (  key uuid primary key,  name text,  avatar text,  email text,  phone text,  login text,  pw text,  st text  ); from /var/log/cassandra/output.log:  error 11:58:52,454 internal error processing execute_cql3_query  java.lang.assertionerror  at org.apache.cassandra.cql3.statements.selectstatement.getindexexpressions(selectstatement.java:749)  at org.apache.cassandra.cql3.statements.selectstatement.getrangecommand(selectstatement.java:303)  at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:155)  at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:56)  at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:101)  at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:117)  at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:108)  at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1920)  at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4372)  at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4356)  at org.apache.thrift.processfunction.process(processfunction.java:39)  at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:194)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:724)<stacktrace> from /var/log/cassandra/output.log:  error 11:58:52,454 internal error processing execute_cql3_query  java.lang.assertionerror  at org.apache.cassandra.cql3.statements.selectstatement.getindexexpressions(selectstatement.java:749)  at org.apache.cassandra.cql3.statements.selectstatement.getrangecommand(selectstatement.java:303)  at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:155)  at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:56)  at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:101)  at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:117)  at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:108)  at org.apache.cassandra.thrift.cassandraserver.execute_cql3_query(cassandraserver.java:1920)  at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4372)  at org.apache.cassandra.thrift.cassandra$processor$execute_cql3_query.getresult(cassandra.java:4356)  at org.apache.thrift.processfunction.process(processfunction.java:39)  at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:39)  at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:194)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:724)<code> table definition:   create columnfamily user (  key uuid primary key,  name text,  avatar text,  email text,  phone text,  login text,  pw text,  st text  ); <text> query with error: select * from user where login='nsv' and st in ('1','2') allow filtering; query works:  select * from user where login='nsv' and st in ('1') allow filtering;  - single item inside in ",
        "label": 520
    },
    {
        "text": "column bloomfilter not exploited <description> there is a bloomfilter created for all columns per row in sstables. the bloomfilter is deserialized during read, but is not checked.<stacktrace> <code> <text> there is a bloomfilter created for all columns per row in sstables. the bloomfilter is deserialized during read, but is not checked.",
        "label": 274
    },
    {
        "text": "incorrect compaction log information on totalsourcerows in c  pre versions <description> i was looking at some confusing compaction log information on c* 3.0.7 and realized that we have a bug that was trivially fixed in c* 3.8.  basically here is the log entry in debug.log (as most of the compaction related log have been moved to debug.log due to adjustment in cassandra-10241). debug [compactionexecutor:6] 2016-07-12 15:38:28,471  compactiontask.java:217 - compacted (96aa1ba6-4846-11e6-adb7-17866fa8ddfd) 4 sstables to [/var/lib/cassandra/data/keyspace1/standard1-713f7920484411e6adb717866fa8ddfd/mb-5-big,] to level=0.  267,974,735 bytes to 78,187,400 (~29% of original) in 39,067ms = 1.908652mb/s.  0 total partitions merged to 332,904.  partition merge counts were {1:9008, 2:34822, 3:74505, 4:214569, } debug [compactionexecutor:4] 2016-07-12 20:51:56,578  compactiontask.java:217 - compacted (786cd9d0-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mb-25-big,] to level=0.  620 bytes to 498 (~80% of original) in 51ms = 0.009312mb/s.  0 total partitions merged to 6.  partition merge counts were {1:4, 3:2, } debug [compactionexecutor:4] 2016-07-12 20:51:58,345  compactiontask.java:217 - compacted (79771de0-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mb-65-big,] to level=0.  14,113 bytes to 9,553 (~67% of original) in 70ms = 0.130149mb/s.  0 total partitions merged to 16.  partition merge counts were {1:13, 2:2, 3:1, } debug [compactionexecutor:3] 2016-07-12 20:52:00,415  compactiontask.java:217 - compacted (7ab6a2c0-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mb-85-big,] to level=0.  1,066 bytes to 611 (~57% of original) in 48ms = 0.012139mb/s.  0 total partitions merged to 16.  partition merge counts were {1:13, 2:2, 4:1, } debug [compactionexecutor:4] 2016-07-12 20:52:00,442  compactiontask.java:217 - compacted (7abae880-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-77-big,] to level=0.  6,910 bytes to 4,396 (~63% of original) in 48ms = 0.087341mb/s.  0 total partitions merged to 16.  partition merge counts were {1:13, 2:2, 3:1, } note no matter if it's system table or user table, it's always showing \"0 total partitions merged to xx\", which is incorrect information due to this code segment https://github.com/apache/cassandra/blob/cassandra-3.0.7/src/java/org/apache/cassandra/db/compaction/compactiontask.java#l215-217. basically it only initialized the totalsourcerows value with 0 and never assigned a real calculated value to it. looks like the latest commit from cassandra-12080 fixed this problem, but it only got checked into 3.8 branch. since this can make people doubt the accuracy of compaction related log entries, and the changes made in cassandra-12080 are only log metrics related, low impact changes, i'd advocate we backport the change from cassandra-12080 into c*-3.0 branch as many people's production c*-3.0 version can benefit from the bug fix, along with better compaction log information in general. i realize that cassandra-12080 may be based on the c*-3.6 changes in cassandra-10805, so this means we may have to bring in changes from cassandra-10805 as well if cassandra-12080 cannot be cleanly rebased on c*-3.0 branch, but both are going to benefit compaction observability in production on c*-3.0.x versions, so both should be welcomed changes in c*-3.0 branch.<stacktrace> <code> debug [compactionexecutor:6] 2016-07-12 15:38:28,471  compactiontask.java:217 - compacted (96aa1ba6-4846-11e6-adb7-17866fa8ddfd) 4 sstables to [/var/lib/cassandra/data/keyspace1/standard1-713f7920484411e6adb717866fa8ddfd/mb-5-big,] to level=0.  267,974,735 bytes to 78,187,400 (~29% of original) in 39,067ms = 1.908652mb/s.  0 total partitions merged to 332,904.  partition merge counts were {1:9008, 2:34822, 3:74505, 4:214569, } debug [compactionexecutor:4] 2016-07-12 20:51:56,578  compactiontask.java:217 - compacted (786cd9d0-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mb-25-big,] to level=0.  620 bytes to 498 (~80% of original) in 51ms = 0.009312mb/s.  0 total partitions merged to 6.  partition merge counts were {1:4, 3:2, } debug [compactionexecutor:4] 2016-07-12 20:51:58,345  compactiontask.java:217 - compacted (79771de0-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mb-65-big,] to level=0.  14,113 bytes to 9,553 (~67% of original) in 70ms = 0.130149mb/s.  0 total partitions merged to 16.  partition merge counts were {1:13, 2:2, 3:1, } debug [compactionexecutor:3] 2016-07-12 20:52:00,415  compactiontask.java:217 - compacted (7ab6a2c0-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mb-85-big,] to level=0.  1,066 bytes to 611 (~57% of original) in 48ms = 0.012139mb/s.  0 total partitions merged to 16.  partition merge counts were {1:13, 2:2, 4:1, } debug [compactionexecutor:4] 2016-07-12 20:52:00,442  compactiontask.java:217 - compacted (7abae880-4872-11e6-8755-79a37e6d8141) 4 sstables to [/var/lib/cassandra/data/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mb-77-big,] to level=0.  6,910 bytes to 4,396 (~63% of original) in 48ms = 0.087341mb/s.  0 total partitions merged to 16.  partition merge counts were {1:13, 2:2, 3:1, } <text> i was looking at some confusing compaction log information on c* 3.0.7 and realized that we have a bug that was trivially fixed in c* 3.8.  basically here is the log entry in debug.log (as most of the compaction related log have been moved to debug.log due to adjustment in cassandra-10241). note no matter if it's system table or user table, it's always showing '0 total partitions merged to xx', which is incorrect information due to this code segment https://github.com/apache/cassandra/blob/cassandra-3.0.7/src/java/org/apache/cassandra/db/compaction/compactiontask.java#l215-217. basically it only initialized the totalsourcerows value with 0 and never assigned a real calculated value to it. looks like the latest commit from cassandra-12080 fixed this problem, but it only got checked into 3.8 branch. since this can make people doubt the accuracy of compaction related log entries, and the changes made in cassandra-12080 are only log metrics related, low impact changes, i'd advocate we backport the change from cassandra-12080 into c*-3.0 branch as many people's production c*-3.0 version can benefit from the bug fix, along with better compaction log information in general. i realize that cassandra-12080 may be based on the c*-3.6 changes in cassandra-10805, so this means we may have to bring in changes from cassandra-10805 as well if cassandra-12080 cannot be cleanly rebased on c*-3.0 branch, but both are going to benefit compaction observability in production on c*-3.0.x versions, so both should be welcomed changes in c*-3.0 branch.",
        "label": 98
    },
    {
        "text": "need  describe cluster  functionality in nodetool or cqlsh <description> in order to be able to completely get rid of cassandra-cli we still need something like \"describe cluster;\" callable from nodetool or cqlsh. describe cluster is different from just querying the system tables in cqlsh in that it actually contacts each node. having a command which does that is useful.<stacktrace> <code> <text> in order to be able to completely get rid of cassandra-cli we still need something like 'describe cluster;' callable from nodetool or cqlsh. describe cluster is different from just querying the system tables in cqlsh in that it actually contacts each node. having a command which does that is useful.",
        "label": 315
    },
    {
        "text": "deletes get lost <description> the mentioned commit introduced a bug which is not easy to reproduce: workload description: one insert into a table multiple concurrent selects against different tables (one select returns a result) one update against the same table as the insert (same) multiple concurrent selects against different tables (one select returns a result) one delete against the same table as the insert (same) multiple concurrent selects against different tables expected is that the last bunch of selects returns no result. but since commit sha the delete gets not processed.  to clarify - the delete is not delayed - it is not executed at all. checked against a single node c* \"cluster\". does only affect unreleased 2.1 - not 2.0 nor 1.2.<stacktrace> <code> <text> the mentioned commit introduced a bug which is not easy to reproduce: workload description: expected is that the last bunch of selects returns no result. but since commit sha the delete gets not processed.  to clarify - the delete is not delayed - it is not executed at all. checked against a single node c* 'cluster'. does only affect unreleased 2.1 - not 2.0 nor 1.2.",
        "label": 521
    },
    {
        "text": "cli does not support removing compression options from a columnfamily <description> this may be an issue with thriftvalidator as well - not accepting a null or empty compression properties map as a disable flag.<stacktrace> <code> <text> this may be an issue with thriftvalidator as well - not accepting a null or empty compression properties map as a disable flag.",
        "label": 412
    },
    {
        "text": "transient replication  implement cheap quorum write optimizations <description> writes should never be sent to transient replicas unless necessary to satisfy the requested consistency level. such as rf not being sufficient for strong consistency or not enough full replicas marked as alive. if a write doesn't receive sufficient responses in time additional replicas should be sent the write similar to rapid read protection. hints should never be written for a transient replica.<stacktrace> <code> <text> writes should never be sent to transient replicas unless necessary to satisfy the requested consistency level. such as rf not being sufficient for strong consistency or not enough full replicas marked as alive. if a write doesn't receive sufficient responses in time additional replicas should be sent the write similar to rapid read protection. hints should never be written for a transient replica.",
        "label": 79
    },
    {
        "text": "log when a  repair  operation completes <description> operators who run \"repair\" operations need to know when they are completed. nickmbailey suggests that this depends on his work to add callbacks for streaming operations. when this is done, a simple log message at level info when all differencers are done would be of use!<stacktrace> <code> <text> operators who run 'repair' operations need to know when they are completed. nickmbailey suggests that this depends on his work to add callbacks for streaming operations. when this is done, a simple log message at level info when all differencers are done would be of use!",
        "label": 515
    },
    {
        "text": "writing mostly deletes to a memtable results in undercounting the table's occupancy so it may not flush <description> in the extreme case of only deletes the memtable will never flush, and we will oom.<stacktrace> <code> <text> in the extreme case of only deletes the memtable will never flush, and we will oom.",
        "label": 67
    },
    {
        "text": "isinclusive and boundsascomposites in restriction take bounds in different order <description> after we've upgraded our cluster to version 2.1.11, we started getting the bellow exceptions for some of our queries. issue seems to be very similar to cassandra-7284. code to reproduce:         createtable(\"create table %s (\" +                     \"    a text,\" +                     \"    b int,\" +                     \"    primary key (a, b)\" +                     \") with compact storage\" +                     \"    and clustering order by (b desc)\");         execute(\"insert into %s (a, b) values ('a', 2)\");         execute(\"select * from %s where a = 'a' and b > 0\"); java.lang.classcastexception: org.apache.cassandra.db.composites.composites$emptycomposite cannot be cast to org.apache.cassandra.db.composites.cellname         at org.apache.cassandra.db.composites.abstractcellnametype.cellfrombytebuffer(abstractcellnametype.java:188) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.db.composites.abstractsimplecellnametype.makecellname(abstractsimplecellnametype.java:125) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.db.composites.abstractcellnametype.makecellname(abstractcellnametype.java:254) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.makeexclusiveslicebound(selectstatement.java:1197) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.applyslicerestriction(selectstatement.java:1205) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.processcolumnfamily(selectstatement.java:1283) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1250) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:299) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:276) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:224) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:67) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:238) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:493) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:138) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:439) [apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:335) [apache-cassandra-2.1.11.jar:2.1.11]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_66]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [apache-cassandra-2.1.11.jar:2.1.11]         at java.lang.thread.run(thread.java:745) [na:1.8.0_66]<stacktrace> java.lang.classcastexception: org.apache.cassandra.db.composites.composites$emptycomposite cannot be cast to org.apache.cassandra.db.composites.cellname         at org.apache.cassandra.db.composites.abstractcellnametype.cellfrombytebuffer(abstractcellnametype.java:188) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.db.composites.abstractsimplecellnametype.makecellname(abstractsimplecellnametype.java:125) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.db.composites.abstractcellnametype.makecellname(abstractcellnametype.java:254) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.makeexclusiveslicebound(selectstatement.java:1197) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.applyslicerestriction(selectstatement.java:1205) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.processcolumnfamily(selectstatement.java:1283) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1250) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:299) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:276) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:224) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:67) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:238) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:493) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:138) ~[apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:439) [apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:335) [apache-cassandra-2.1.11.jar:2.1.11]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_66]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [apache-cassandra-2.1.11.jar:2.1.11]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [apache-cassandra-2.1.11.jar:2.1.11]         at java.lang.thread.run(thread.java:745) [na:1.8.0_66] <code>         createtable('create table %s (' +                     '    a text,' +                     '    b int,' +                     '    primary key (a, b)' +                     ') with compact storage' +                     '    and clustering order by (b desc)');         execute('insert into %s (a, b) values ('a', 2)');         execute('select * from %s where a = 'a' and b > 0'); <text> after we've upgraded our cluster to version 2.1.11, we started getting the bellow exceptions for some of our queries. issue seems to be very similar to cassandra-7284. code to reproduce:",
        "label": 25
    },
    {
        "text": "triggerexecutor should group mutations by row key <description> triggerexecutor doesn't currently group mutations returned by triggers even if belonging to the same row key: while harmless per se (at least, i think so), this is definitely a performance problem, because each mutation is a cluster mutation, generating more network traffic, more disk io and more index calls (if present).<stacktrace> <code> <text> triggerexecutor doesn't currently group mutations returned by triggers even if belonging to the same row key: while harmless per se (at least, i think so), this is definitely a performance problem, because each mutation is a cluster mutation, generating more network traffic, more disk io and more index calls (if present).",
        "label": 18
    },
    {
        "text": "add secondary index ops to stress py <description> secondary indexes need some basic benchmarks.<stacktrace> <code> <text> secondary indexes need some basic benchmarks.",
        "label": 515
    },
    {
        "text": "multiget returns empty columnorsupercolumn instead of null <description> the later is more intuitive, and the former violates the rule that cosc should have exactly one of {column, super_column} set.<stacktrace> <code> <text> the later is more intuitive, and the former violates the rule that cosc should have exactly one of set.",
        "label": 274
    },
    {
        "text": "cassandrastorage can't cast fields from a cf correctly <description> included scripts demonstrate the problem. regardless of whether the key is cast as a chararray or not, the pig scripts fail with java.lang.classcastexception: org.apache.pig.data.databytearray cannot be cast to java.lang.string at org.apache.pig.backend.hadoop.hdatatype.getwritablecomparabletypes(hdatatype.java:72) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapreduce$map.collect(piggenericmapreduce.java:117) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapbase.runpipeline(piggenericmapbase.java:269) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapbase.map(piggenericmapbase.java:262) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapbase.map(piggenericmapbase.java:64) at org.apache.hadoop.mapreduce.mapper.run(mapper.java:144) at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:764) at org.apache.hadoop.mapred.maptask.run(maptask.java:370) at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:212)<stacktrace> java.lang.classcastexception: org.apache.pig.data.databytearray cannot be cast to java.lang.string at org.apache.pig.backend.hadoop.hdatatype.getwritablecomparabletypes(hdatatype.java:72) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapreduce$map.collect(piggenericmapreduce.java:117) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapbase.runpipeline(piggenericmapbase.java:269) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapbase.map(piggenericmapbase.java:262) at org.apache.pig.backend.hadoop.executionengine.mapreducelayer.piggenericmapbase.map(piggenericmapbase.java:64) at org.apache.hadoop.mapreduce.mapper.run(mapper.java:144) at org.apache.hadoop.mapred.maptask.runnewmapper(maptask.java:764) at org.apache.hadoop.mapred.maptask.run(maptask.java:370) at org.apache.hadoop.mapred.localjobrunner$job.run(localjobrunner.java:212) <code> <text> included scripts demonstrate the problem. regardless of whether the key is cast as a chararray or not, the pig scripts fail with",
        "label": 85
    },
    {
        "text": "expose nodetool scrub for 2is <description> continuation of cassandra-4464, where many other nodetool operations were added for 2is. this ticket supports scrub fo 2is and is in its own ticket due to the riskiness of deleting data on a bad bug.<stacktrace> <code> <text> continuation of cassandra-4464, where many other nodetool operations were added for 2is. this ticket supports scrub fo 2is and is in its own ticket due to the riskiness of deleting data on a bad bug.",
        "label": 508
    },
    {
        "text": "streamout doesn't correctly handle wrapped ranges <description> streamout doesn't normalize ranges, causing abstractviewsstablefinder to miss sstables when the requested range is wrapped, and hence breaking node bootstrapping/unbootstrapping on such ranges.<stacktrace> <code> <text> streamout doesn't normalize ranges, causing abstractviewsstablefinder to miss sstables when the requested range is wrapped, and hence breaking node bootstrapping/unbootstrapping on such ranges.",
        "label": 491
    },
    {
        "text": "org apache cassandra concurrent type commitlog should actually be org apache cassandra db type commitlog <description> within ./db/commitlogexecutorservice.java, the commitlog mbean is currently registering itself as \"org.apache.cassandra.concurrent:type=commitlog\", when it should be registering itself as \"org.apache.cassandra.db:type=commitlog\". per jbellis this is a legacy issue and can/should be corrected. attached find a unified diff patch which does so!<stacktrace> <code> <text> within ./db/commitlogexecutorservice.java, the commitlog mbean is currently registering itself as 'org.apache.cassandra.concurrent:type=commitlog', when it should be registering itself as 'org.apache.cassandra.db:type=commitlog'. per jbellis this is a legacy issue and can/should be corrected. attached find a unified diff patch which does so!",
        "label": 451
    },
    {
        "text": "batch cas does not support local serial <description> the batch cas feature introduced in cassandra 2.0.6 does not support the local_serial consistency level, and always uses serial. create a cluster with 4 nodes with the following topology: datacenter: dc2 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack un  127.0.0.3  269 kb     256     26.3%  ae92d997-6042-42d9-b447-943080569742  rac1 un  127.0.0.4  197.81 kb  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  rac1 datacenter: dc1 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack un  127.0.0.1  226.92 kb  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  rac1 un  127.0.0.2  179.27 kb  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  rac1 in cqlsh: cqlsh> create keyspace foo with replication = {'class': 'networktopologystrategy', 'dc1': 2, 'dc2': 2}; cqlsh> use foo; cqlsh:foo> create table bar (x text, y bigint, z bigint, t bigint, primary key(x,y)); kill nodes 127.0.0.3 and 127.0.0.4: datacenter: dc2 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack dn  127.0.0.3  262.37 kb  256     26.3%  ae92d997-6042-42d9-b447-943080569742  rac1 dn  127.0.0.4  208.04 kb  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  rac1 datacenter: dc1 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack un  127.0.0.1  214.82 kb  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  rac1 un  127.0.0.2  178.23 kb  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  rac1 connect to 127.0.0.1 in dc1 and run a cas batch at cl.local_serial+local_quorum:         final cluster cluster = new cluster.builder()                 .addcontactpoint(\"127.0.0.1\")                 .withloadbalancingpolicy(new dcawareroundrobinpolicy(\"dc1\"))                 .build();         final session session = cluster.connect(\"foo\");         batch batch = querybuilder.batch();         batch.add(new simplestatement(\"insert into bar (x,y,z) values ('abc', 123, 1) if not exists\"));         batch.add(new simplestatement(\"update bar set t=2 where x='abc' and y=123\"));         batch.setconsistencylevel(consistencylevel.local_quorum);         batch.setserialconsistencylevel(consistencylevel.local_serial);         session.execute(batch); the batch fails with: caused by: com.datastax.driver.core.exceptions.unavailableexception: not enough replica available for query at consistency serial (3 required but only 2 alive) at com.datastax.driver.core.responses$error$1.decode(responses.java:44) at com.datastax.driver.core.responses$error$1.decode(responses.java:33) at com.datastax.driver.core.message$protocoldecoder.decode(message.java:182) at org.jboss.netty.handler.codec.oneone.onetoonedecoder.handleupstream(onetoonedecoder.java:66) ... 21 more<stacktrace> caused by: com.datastax.driver.core.exceptions.unavailableexception: not enough replica available for query at consistency serial (3 required but only 2 alive) at com.datastax.driver.core.responses$error$1.decode(responses.java:44) at com.datastax.driver.core.responses$error$1.decode(responses.java:33) at com.datastax.driver.core.message$protocoldecoder.decode(message.java:182) at org.jboss.netty.handler.codec.oneone.onetoonedecoder.handleupstream(onetoonedecoder.java:66) ... 21 more <code> datacenter: dc2 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack un  127.0.0.3  269 kb     256     26.3%  ae92d997-6042-42d9-b447-943080569742  rac1 un  127.0.0.4  197.81 kb  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  rac1 datacenter: dc1 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack un  127.0.0.1  226.92 kb  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  rac1 un  127.0.0.2  179.27 kb  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  rac1 cqlsh> create keyspace foo with replication = {'class': 'networktopologystrategy', 'dc1': 2, 'dc2': 2}; cqlsh> use foo; cqlsh:foo> create table bar (x text, y bigint, z bigint, t bigint, primary key(x,y)); datacenter: dc2 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack dn  127.0.0.3  262.37 kb  256     26.3%  ae92d997-6042-42d9-b447-943080569742  rac1 dn  127.0.0.4  208.04 kb  256     25.1%  3edc92d7-9d1b-472a-8452-24dddbc4502c  rac1 datacenter: dc1 =============== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens  owns   host id                               rack un  127.0.0.1  214.82 kb  256     24.8%  dbc17bd7-1ede-47a2-9b31-6063752d6eb3  rac1 un  127.0.0.2  178.23 kb  256     23.7%  bb0ad285-34d2-4989-a664-b068986ab6fa  rac1         final cluster cluster = new cluster.builder()                 .addcontactpoint('127.0.0.1')                 .withloadbalancingpolicy(new dcawareroundrobinpolicy('dc1'))                 .build();         final session session = cluster.connect('foo');         batch batch = querybuilder.batch();         batch.add(new simplestatement('insert into bar (x,y,z) values ('abc', 123, 1) if not exists'));         batch.add(new simplestatement('update bar set t=2 where x='abc' and y=123'));         batch.setconsistencylevel(consistencylevel.local_quorum);         batch.setserialconsistencylevel(consistencylevel.local_serial);         session.execute(batch); <text> the batch cas feature introduced in cassandra 2.0.6 does not support the local_serial consistency level, and always uses serial. create a cluster with 4 nodes with the following topology: in cqlsh: kill nodes 127.0.0.3 and 127.0.0.4: connect to 127.0.0.1 in dc1 and run a cas batch at cl.local_serial+local_quorum: the batch fails with:",
        "label": 520
    },
    {
        "text": "remove list iterators <description> we allocate list iterators in several places in hot paths. this converts them to get by index. this provides a ~4% improvement in relvant workloads.<stacktrace> <code> <text> we allocate list iterators in several places in hot paths. this converts them to get by index. this provides a ~4% improvement in relvant workloads.",
        "label": 79
    },
    {
        "text": "setting severity via jmx broken <description> looks like setting the severity attribute in the dynamicendpointsnitch via jmx is a no-op.<stacktrace> <code> <text> looks like setting the severity attribute in the dynamicendpointsnitch via jmx is a no-op.",
        "label": 555
    },
    {
        "text": "improve cas propose cql query <description> propose stage only requires us to read the latest ballet we prepared. there is no need to read an entire cql row.   this will be very useful when cassandra-7085 is implemented. as this improved select query will always come from mem table.<stacktrace> <code> <text> propose stage only requires us to read the latest ballet we prepared. there is no need to read an entire cql row.   this will be very useful when cassandra-7085 is implemented. as this improved select query will always come from mem table.",
        "label": 482
    },
    {
        "text": "upgrade test on path fails with configuration problems <description> the following test fails on the uprgrade path from 2.1 to 3.0: http://cassci.datastax.com/view/upgrades/job/cassandra_upgrade_2.1_to_3.0_proto_v3/10/testreport/upgrade_through_versions_test/testupgrade_from_3_0_latest_tag_to_3_0_head/bootstrap_multidc_test/ i believe it's basically a configuration error; the cluster likely just needs to be reconfigured in the test: code=2200 [invalid query] message=\"user-defined functions are disabled in cassandra.yaml - set enable_user_defined_functions=true to enable\" assigning to russ hatch for now.<stacktrace> <code> code=2200 [invalid query] message='user-defined functions are disabled in cassandra.yaml - set enable_user_defined_functions=true to enable' http://cassci.datastax.com/view/upgrades/job/cassandra_upgrade_2.1_to_3.0_proto_v3/10/testreport/upgrade_through_versions_test/testupgrade_from_3_0_latest_tag_to_3_0_head/bootstrap_multidc_test/ <text> the following test fails on the uprgrade path from 2.1 to 3.0: i believe it's basically a configuration error; the cluster likely just needs to be reconfigured in the test: assigning to russ hatch for now.",
        "label": 39
    },
    {
        "text": "nothing appearing to happen during bootstrap while waiting for anticompaction is still confusing <description> as seen on the recent ml thread can we turn one or more of the .debug statements in streamout into a jmx notification?<stacktrace> <code> <text> as seen on the recent ml thread can we turn one or more of the .debug statements in streamout into a jmx notification?",
        "label": 186
    },
    {
        "text": "dtest failure in user types test testusertypes test nested user types <description> this is a single flap: http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/217/testreport/user_types_test/testusertypes/test_nested_user_types failed on cassci build cassandra-2.2_dtest_win32 #217 error message lists differ: [none] != [[u'test', u'test2']] first differing element 0: none [u'test', u'test2'] - [none] + [[u'test', u'test2']] -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: d:\\temp\\dtest-vgkgwi dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} --------------------- >> end captured logging << --------------------- stacktrace   file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 329, in run     testmethod()   file \"d:\\jenkins\\workspace\\cassandra-2.2_dtest_win32\\cassandra-dtest\\user_types_test.py\", line 289, in test_nested_user_types     self.assertequal(listify(primary_item), [[u'test', u'test2']])   file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 742, in assertlistequal     self.assertsequenceequal(list1, list2, msg, seq_type=list)   file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 724, in assertsequenceequal     self.fail(msg)   file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 410, in fail     raise self.failureexception(msg) \"lists differ: [none] != [[u'test', u'test2']]\\n\\nfirst differing element 0:\\nnone\\n[u'test', u'test2']\\n\\n- [none]\\n+ [[u'test', u'test2']]\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: d:\\\\temp\\\\dtest-vgkgwi\\ndtest: debug: custom init_config not found. setting defaults.\\ndtest: debug: done setting configuration options:\\n{   'initial_token': none,\\n    'num_tokens': '32',\\n    'phi_convict_threshold': 5,\\n    'range_request_timeout_in_ms': 10000,\\n    'read_request_timeout_in_ms': 10000,\\n    'request_timeout_in_ms': 10000,\\n    'truncate_request_timeout_in_ms': 10000,\\n    'write_request_timeout_in_ms': 10000}\\n--------------------- >> end captured logging << ---------------------\" standard error started: node1 with pid: 4328 started: node3 with pid: 7568 started: node2 with pid: 7504<stacktrace> <code> error message lists differ: [none] != [[u'test', u'test2']] first differing element 0: none [u'test', u'test2'] - [none] + [[u'test', u'test2']] -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: d:/temp/dtest-vgkgwi dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} --------------------- >> end captured logging << --------------------- stacktrace   file 'c:/tools/python2/lib/unittest/case.py', line 329, in run     testmethod()   file 'd:/jenkins/workspace/cassandra-2.2_dtest_win32/cassandra-dtest/user_types_test.py', line 289, in test_nested_user_types     self.assertequal(listify(primary_item), [[u'test', u'test2']])   file 'c:/tools/python2/lib/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file 'c:/tools/python2/lib/unittest/case.py', line 742, in assertlistequal     self.assertsequenceequal(list1, list2, msg, seq_type=list)   file 'c:/tools/python2/lib/unittest/case.py', line 724, in assertsequenceequal     self.fail(msg)   file 'c:/tools/python2/lib/unittest/case.py', line 410, in fail     raise self.failureexception(msg) 'lists differ: [none] != [[u'test', u'test2']]/n/nfirst differing element 0:/nnone/n[u'test', u'test2']/n/n- [none]/n+ [[u'test', u'test2']]/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: d://temp//dtest-vgkgwi/ndtest: debug: custom init_config not found. setting defaults./ndtest: debug: done setting configuration options:/n{   'initial_token': none,/n    'num_tokens': '32',/n    'phi_convict_threshold': 5,/n    'range_request_timeout_in_ms': 10000,/n    'read_request_timeout_in_ms': 10000,/n    'request_timeout_in_ms': 10000,/n    'truncate_request_timeout_in_ms': 10000,/n    'write_request_timeout_in_ms': 10000}/n--------------------- >> end captured logging << ---------------------' standard error started: node1 with pid: 4328 started: node3 with pid: 7568 started: node2 with pid: 7504 http://cassci.datastax.com/job/cassandra-2.2_dtest_win32/217/testreport/user_types_test/testusertypes/test_nested_user_types failed on cassci build cassandra-2.2_dtest_win32 #217<text> this is a single flap: ",
        "label": 428
    },
    {
        "text": "minor slicerange documentation fix <description> minor doc typo, patch attached<stacktrace> <code> <text> minor doc typo, patch attached",
        "label": 317
    },
    {
        "text": "rte from new cdc column breaks in flight queries  <description> this rte is not harmless. it will cause the internode connection to break which will cause all in flight requests between these nodes to die/timeout.     - due to changes in schema migration handling and the storage format after 3.0, you will       see error messages such as:          \"java.lang.runtimeexception: unknown column cdc during deserialization\"       in your system logs on a mixed-version cluster during upgrades. this error message       is harmless and due to the 3.8 nodes having cdc added to their schema tables while       the <3.8 nodes do not. this message should cease once all nodes are upgraded to 3.8.       as always, refrain from schema changes during cluster upgrades.<stacktrace> <code>     - due to changes in schema migration handling and the storage format after 3.0, you will       see error messages such as:          'java.lang.runtimeexception: unknown column cdc during deserialization'       in your system logs on a mixed-version cluster during upgrades. this error message       is harmless and due to the 3.8 nodes having cdc added to their schema tables while       the <3.8 nodes do not. this message should cease once all nodes are upgraded to 3.8.       as always, refrain from schema changes during cluster upgrades. <text> this rte is not harmless. it will cause the internode connection to break which will cause all in flight requests between these nodes to die/timeout.",
        "label": 520
    },
    {
        "text": "cassandraservicedatacleaner prepare  fails with ioexception  <description> cassandraservicedatacleaner.prepare() fails with an ioexception if run in isolation. it seems that initializing the datadescriptor creates a new commitlog file, and then the cleaner tries to delete this file and fails. 16:06:07.204 [main] info o.a.c.config.databasedescriptor - loading settings from file:/c:/workspace/sandbox/target/classes/cassandra.yaml  16:06:07.282 [main] debug o.a.c.config.databasedescriptor - syncing log with a period of 10000  16:06:07.282 [main] info o.a.c.config.databasedescriptor - diskaccessmode 'auto' determined to be standard, indexaccessmode is standard  16:06:07.797 [main] debug o.a.c.config.databasedescriptor - setting auto_bootstrap to false  16:06:07.797 [main] info o.a.c.db.commitlog.commitlogsegment - creating new commitlog segment target/var/lib/cassandra/commitlog\\commitlog-1294934767797.log  16:06:07.813 [main] debug o.apache.cassandra.io.util.fileutils - deleting commitlog-1294934767797.log  exception in thread \"main\" java.io.ioexception: failed to delete c:\\workspace\\sandbox\\target\\var\\lib\\cassandra\\commitlog\\commitlog-1294934767797.log  at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:54)  at org.apache.cassandra.io.util.fileutils.deleterecursive(fileutils.java:201)  at org.apache.cassandra.contrib.utils.service.cassandraservicedatacleaner.cleandir(cassandraservicedatacleaner.java:99)  at org.apache.cassandra.contrib.utils.service.cassandraservicedatacleaner.cleanupdatadirectories(cassandraservicedatacleaner.java:53)  at org.apache.cassandra.contrib.utils.service.cassandraservicedatacleaner.prepare(cassandraservicedatacleaner.java:44)  at cng.sandbox.app.main(app.java:15) this also seems to leave a bunch of threads running in the background, so the process has to be manually killed. this was tested with the javautils in the 0.7.0 branch.<stacktrace> 16:06:07.204 [main] info o.a.c.config.databasedescriptor - loading settings from file:/c:/workspace/sandbox/target/classes/cassandra.yaml  16:06:07.282 [main] debug o.a.c.config.databasedescriptor - syncing log with a period of 10000  16:06:07.282 [main] info o.a.c.config.databasedescriptor - diskaccessmode 'auto' determined to be standard, indexaccessmode is standard  16:06:07.797 [main] debug o.a.c.config.databasedescriptor - setting auto_bootstrap to false  16:06:07.797 [main] info o.a.c.db.commitlog.commitlogsegment - creating new commitlog segment target/var/lib/cassandra/commitlog/commitlog-1294934767797.log  16:06:07.813 [main] debug o.apache.cassandra.io.util.fileutils - deleting commitlog-1294934767797.log  exception in thread 'main' java.io.ioexception: failed to delete c:/workspace/sandbox/target/var/lib/cassandra/commitlog/commitlog-1294934767797.log  at org.apache.cassandra.io.util.fileutils.deletewithconfirm(fileutils.java:54)  at org.apache.cassandra.io.util.fileutils.deleterecursive(fileutils.java:201)  at org.apache.cassandra.contrib.utils.service.cassandraservicedatacleaner.cleandir(cassandraservicedatacleaner.java:99)  at org.apache.cassandra.contrib.utils.service.cassandraservicedatacleaner.cleanupdatadirectories(cassandraservicedatacleaner.java:53)  at org.apache.cassandra.contrib.utils.service.cassandraservicedatacleaner.prepare(cassandraservicedatacleaner.java:44)  at cng.sandbox.app.main(app.java:15) <code> <text> cassandraservicedatacleaner.prepare() fails with an ioexception if run in isolation. it seems that initializing the datadescriptor creates a new commitlog file, and then the cleaner tries to delete this file and fails. this also seems to leave a bunch of threads running in the background, so the process has to be manually killed. this was tested with the javautils in the 0.7.0 branch.",
        "label": 373
    },
    {
        "text": "improve error handling when table is queried before the schema has fully propagated <description> this error occurs during a rolling upgrade between 2.0.14 and 2.1.4. repo with all the nodes on 2.0.14 make the following tables create keyspace test with replication = {   'class': 'simplestrategy',   'replication_factor': '2' }; use test; create table compact (   k int,   c int,   d int,   primary key ((k), c) ) with compact storage; create table norm (   k int,   c int,   d int,   primary key ((k), c) ) ; then load some data into these tables. i used the python driver from cassandra.cluster import cluster s = cluster().connect() for x in range (1000):     for y in range (1000):        s.execute_async(\"insert into test.compact (k,c,d) values (%d,%d,%d)\"%(x,y,y))        s.execute_async(\"insert into test.norm (k,c,d) values (%d,%d,%d)\"%(x,y,y)) upgrade one node from 2.0.14 -> 2.1.4 from the 2.1.4 node, create a new table. query that table on the 2.0.14 nodes you get these exceptions because the schema didn't propagate there. this exception kills the tcp connection between the nodes. error [thread-19] 2015-04-08 18:48:45,337 cassandradaemon.java (line 258) exception in thread thread[thread-19,5,main] java.lang.nullpointerexception at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:247) at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:156) at org.apache.cassandra.net.messagein.read(messagein.java:99) at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:149) at org.apache.cassandra.net.incomingtcpconnection.receivemessages(incomingtcpconnection.java:131) at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:74) run cqlsh on the upgraded node and queries will fail until the tcp connection is established again, easiest to repo with cl = all cqlsh> select count(*) from test.norm where k = 22 ; readtimeout: code=1200 [coordinator node timed out waiting for replica nodes' responses] message=\"operation timed out - received only 1 responses.\" info={'received_responses': 1, 'required_responses': 2, 'consistency': 'all'} cqlsh> select count(*) from test.norm where k = 21 ; readtimeout: code=1200 [coordinator node timed out waiting for replica nodes' responses] message=\"operation timed out - received only 1 responses.\" info={'received_responses': 1, 'required_responses': 2, 'consistency': 'all'} so connection made: debug [thread-227] 2015-04-09 05:09:02,718 incomingtcpconnection.java (line 107) set version for /10.240.14.115 to 8 (will use 7) connection broken by query of table before schema propagated: error [thread-227] 2015-04-09 05:10:24,015 cassandradaemon.java (line 258) exception in thread thread[thread-227,5,main] java.lang.nullpointerexception at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:247) at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:156) at org.apache.cassandra.net.messagein.read(messagein.java:99) at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:149) at org.apache.cassandra.net.incomingtcpconnection.receivemessages(incomingtcpconnection.java:131) at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:74) all query to that node will fail with timeouts now until...  connection re-established debug [thread-228] 2015-04-09 05:11:00,323 incomingtcpconnection.java (line 107) set version for /10.240.14.115 to 8 (will use 7) now queries work again.<stacktrace> error [thread-19] 2015-04-08 18:48:45,337 cassandradaemon.java (line 258) exception in thread thread[thread-19,5,main] java.lang.nullpointerexception at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:247) at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:156) at org.apache.cassandra.net.messagein.read(messagein.java:99) at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:149) at org.apache.cassandra.net.incomingtcpconnection.receivemessages(incomingtcpconnection.java:131) at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:74) error [thread-227] 2015-04-09 05:10:24,015 cassandradaemon.java (line 258) exception in thread thread[thread-227,5,main] java.lang.nullpointerexception at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:247) at org.apache.cassandra.db.rangeslicecommandserializer.deserialize(rangeslicecommand.java:156) at org.apache.cassandra.net.messagein.read(messagein.java:99) at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:149) at org.apache.cassandra.net.incomingtcpconnection.receivemessages(incomingtcpconnection.java:131) at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:74) <code> create keyspace test with replication = {   'class': 'simplestrategy',   'replication_factor': '2' }; use test; create table compact (   k int,   c int,   d int,   primary key ((k), c) ) with compact storage; create table norm (   k int,   c int,   d int,   primary key ((k), c) ) ; from cassandra.cluster import cluster s = cluster().connect() for x in range (1000):     for y in range (1000):        s.execute_async('insert into test.compact (k,c,d) values (%d,%d,%d)'%(x,y,y))        s.execute_async('insert into test.norm (k,c,d) values (%d,%d,%d)'%(x,y,y)) cqlsh> select count(*) from test.norm where k = 22 ; readtimeout: code=1200 [coordinator node timed out waiting for replica nodes' responses] message='operation timed out - received only 1 responses.' info={'received_responses': 1, 'required_responses': 2, 'consistency': 'all'} cqlsh> select count(*) from test.norm where k = 21 ; readtimeout: code=1200 [coordinator node timed out waiting for replica nodes' responses] message='operation timed out - received only 1 responses.' info={'received_responses': 1, 'required_responses': 2, 'consistency': 'all'} debug [thread-227] 2015-04-09 05:09:02,718 incomingtcpconnection.java (line 107) set version for /10.240.14.115 to 8 (will use 7) debug [thread-228] 2015-04-09 05:11:00,323 incomingtcpconnection.java (line 107) set version for /10.240.14.115 to 8 (will use 7) upgrade one node from 2.0.14 -> 2.1.4 <text> this error occurs during a rolling upgrade between 2.0.14 and 2.1.4. with all the nodes on 2.0.14 make the following tables then load some data into these tables. i used the python driver from the 2.1.4 node, create a new table. query that table on the 2.0.14 nodes you get these exceptions because the schema didn't propagate there. this exception kills the tcp connection between the nodes. run cqlsh on the upgraded node and queries will fail until the tcp connection is established again, easiest to repo with cl = all so connection made: connection broken by query of table before schema propagated: all query to that node will fail with timeouts now until...  connection re-established now queries work again.",
        "label": 538
    },
    {
        "text": "expand breaks when the result has rows in cqlsh <description> cqlsh> expand on; now expanded output is enabled cqlsh> select * from system.local; @ row 1 -------------------------+---------------------------------------------  key                     | local  bootstrapped            | completed  broadcast_address       | 127.0.0.1  cluster_name            | dse_50_graph  cql_version             | 3.4.0  data_center             | graph  dse_version             | 5.0.0  gossip_generation       | 1454032824  graph                   | true  host_id                 | ad30ccb2-04a1-4511-98b6-a72e4ea182c0  listen_address          | 127.0.0.1  native_protocol_version | 4  partitioner             | org.apache.cassandra.dht.murmur3partitioner  rack                    | rack1  release_version         | 3.0.1.816  rpc_address             | 127.0.0.1  schema_version          | 5667501a-4ac3-3f00-ab35-9040efb927ad  server_id               | a0-ce-c8-01-cc-ca  thrift_version          | 20.1.0  tokens                  | {'-9223372036854775808'}  truncated_at            | null  workload                | cassandra (1 rows) cqlsh> select * from system.peers; max() arg is an empty sequence cqlsh> expand off; disabled expanded output. cqlsh> select * from system.peers;  peer | data_center | dse_version | graph | host_id | preferred_ip | rack | release_version | rpc_address | schema_version | server_id | tokens | workload ------+-------------+-------------+-------+---------+--------------+------+-----------------+-------------+----------------+-----------+--------+---------- (0 rows) <stacktrace> <code> cqlsh> expand on; now expanded output is enabled cqlsh> select * from system.local; @ row 1 -------------------------+---------------------------------------------  key                     | local  bootstrapped            | completed  broadcast_address       | 127.0.0.1  cluster_name            | dse_50_graph  cql_version             | 3.4.0  data_center             | graph  dse_version             | 5.0.0  gossip_generation       | 1454032824  graph                   | true  host_id                 | ad30ccb2-04a1-4511-98b6-a72e4ea182c0  listen_address          | 127.0.0.1  native_protocol_version | 4  partitioner             | org.apache.cassandra.dht.murmur3partitioner  rack                    | rack1  release_version         | 3.0.1.816  rpc_address             | 127.0.0.1  schema_version          | 5667501a-4ac3-3f00-ab35-9040efb927ad  server_id               | a0-ce-c8-01-cc-ca  thrift_version          | 20.1.0  tokens                  | {'-9223372036854775808'}  truncated_at            | null  workload                | cassandra (1 rows) cqlsh> select * from system.peers; max() arg is an empty sequence cqlsh> expand off; disabled expanded output. cqlsh> select * from system.peers;  peer | data_center | dse_version | graph | host_id | preferred_ip | rack | release_version | rpc_address | schema_version | server_id | tokens | workload ------+-------------+-------------+-------+---------+--------------+------+-----------------+-------------+----------------+-----------+--------+---------- (0 rows) <text> ",
        "label": 577
    },
    {
        "text": "fsync the directory after new sstable or commit log segment are created <description> the mannual of fsync said: calling fsync() does not necessarily ensure that the entry in the directory containing the file has also reached disk. for that an explicit fsync() on a file descriptor for the directory is also needed. at least on ext4, syncing the directory is a must to have step, as described by [1]. otherwise, the new sstables or commit logs could be missed after crash even if itself is synced. unfortunately, jvm does not provide an approach to sync the directory... [1] http://www.linuxfoundation.org/news-media/blogs/browse/2009/03/don%e2%80%99t-fear-fsync<stacktrace> <code> [1] http://www.linuxfoundation.org/news-media/blogs/browse/2009/03/don%e2%80%99t-fear-fsync<text> the mannual of fsync said: calling fsync() does not necessarily ensure that the entry in the directory containing the file has also reached disk. for that an explicit fsync() on a file descriptor for the directory is also needed. at least on ext4, syncing the directory is a must to have step, as described by [1]. otherwise, the new sstables or commit logs could be missed after crash even if itself is synced. unfortunately, jvm does not provide an approach to sync the directory... ",
        "label": 412
    },
    {
        "text": "cassandra should expose connected client state via jmx <description> there is currently no good way to determine or estimate how many clients are connected to a cassandra node without using netstat or (if using sync thrift server) counting threads. there is also no way to understand what state any given connection is in. people regularly come into #cassandra/cassandra-user@ and ask how to get the equivalent of a mysql \"show full processlist.\" while i understand that feature parity with show full processlist/information_schema.processlist is unlikely, even a few basic metrics like \"number of connected clients\" or \"number of active clients\" would greatly help with this operational information need.<stacktrace> <code> <text> there is currently no good way to determine or estimate how many clients are connected to a cassandra node without using netstat or (if using sync thrift server) counting threads. there is also no way to understand what state any given connection is in. people regularly come into #cassandra/cassandra-user@ and ask how to get the equivalent of a mysql 'show full processlist.' while i understand that feature parity with show full processlist/information_schema.processlist is unlikely, even a few basic metrics like 'number of connected clients' or 'number of active clients' would greatly help with this operational information need.",
        "label": 362
    },
    {
        "text": "improve debug logging for local inserts <description> i found this patch useful for debugging clients.<stacktrace> <code> <text> i found this patch useful for debugging clients.",
        "label": 469
    },
    {
        "text": "update news txt to explain system schema exceptions during partial cluster upgrade <description> upgrade tests found this exception occuring during upgrades: node2: error [messagingservice-incoming-/127.0.0.1] 2016-06-16 20:14:59,268 cassandradaemon.java:217 - exception in thread thread[messagingservice-incoming-/127.0.0.1,5,main] java.lang.runtimeexception: unknown column cdc during deserialization at org.apache.cassandra.db.columns$serializer.deserialize(columns.java:433) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.serializationheader$serializer.deserializeformessaging(serializationheader.java:407) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.rows.unfilteredrowiteratorserializer.deserializeheader(unfilteredrowiteratorserializer.java:192) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.partitions.partitionupdate$partitionupdateserializer.deserialize30(partitionupdate.java:668) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.partitions.partitionupdate$partitionupdateserializer.deserialize(partitionupdate.java:656) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:341) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:350) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.service.migrationmanager$migrationsserializer.deserialize(migrationmanager.java:610) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.service.migrationmanager$migrationsserializer.deserialize(migrationmanager.java:593) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.messagein.read(messagein.java:114) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:190) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.incomingtcpconnection.receivemessages(incomingtcpconnection.java:178) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:92) ~[apache-cassandra-3.7.jar:3.7] which is apparently normal and should subside after full cluster upgrade to post-3.0 versions. news.txt needs an update to let users know this is not a problem during their upgrade.<stacktrace> node2: error [messagingservice-incoming-/127.0.0.1] 2016-06-16 20:14:59,268 cassandradaemon.java:217 - exception in thread thread[messagingservice-incoming-/127.0.0.1,5,main] java.lang.runtimeexception: unknown column cdc during deserialization at org.apache.cassandra.db.columns$serializer.deserialize(columns.java:433) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.serializationheader$serializer.deserializeformessaging(serializationheader.java:407) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.rows.unfilteredrowiteratorserializer.deserializeheader(unfilteredrowiteratorserializer.java:192) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.partitions.partitionupdate$partitionupdateserializer.deserialize30(partitionupdate.java:668) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.partitions.partitionupdate$partitionupdateserializer.deserialize(partitionupdate.java:656) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:341) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.db.mutation$mutationserializer.deserialize(mutation.java:350) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.service.migrationmanager$migrationsserializer.deserialize(migrationmanager.java:610) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.service.migrationmanager$migrationsserializer.deserialize(migrationmanager.java:593) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.messagein.read(messagein.java:114) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.incomingtcpconnection.receivemessage(incomingtcpconnection.java:190) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.incomingtcpconnection.receivemessages(incomingtcpconnection.java:178) ~[apache-cassandra-3.7.jar:3.7] at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:92) ~[apache-cassandra-3.7.jar:3.7] <code> <text> upgrade tests found this exception occuring during upgrades: which is apparently normal and should subside after full cluster upgrade to post-3.0 versions. news.txt needs an update to let users know this is not a problem during their upgrade.",
        "label": 280
    },
    {
        "text": "clean out dd inmemorycompactionlimit <description> still used in a couple places<stacktrace> <code> <text> still used in a couple places",
        "label": 274
    },
    {
        "text": "tests for cqlsh <description> cqlsh has become big enough and tries to cover enough situations that it's time to start acting like a responsible adult and make this bugger some unit tests to guard against regressions.<stacktrace> <code> <text> cqlsh has become big enough and tries to cover enough situations that it's time to start acting like a responsible adult and make this bugger some unit tests to guard against regressions.",
        "label": 593
    },
    {
        "text": "compaction doesn't remove index entries as designed <description> percolumnindexupdater ignores updates where the new value is a tombstone. it should still remove the index entry on oldcolumn. (note that this will not affect user-visible correctness, since keyssearcher/compositesearcher will issue deletes against stale index entries, but having more stale entries than we \"should\" could affect performance.)<stacktrace> <code> <text> percolumnindexupdater ignores updates where the new value is a tombstone. it should still remove the index entry on oldcolumn. (note that this will not affect user-visible correctness, since keyssearcher/compositesearcher will issue deletes against stale index entries, but having more stale entries than we 'should' could affect performance.)",
        "label": 274
    },
    {
        "text": "test coverage and related bug fixes for abstractbtreepartition and hierarchy <description> follow up to cassandra-9932. the test coverage for abstractbtreepartition and its hierarchy is entirely indirect. that is not to say it is not covered, but we may have some unexplored behaviour. coverage for btree is also missing around a couple of edges, and the gaps should be filled in.<stacktrace> <code> <text> follow up to cassandra-9932. the test coverage for abstractbtreepartition and its hierarchy is entirely indirect. that is not to say it is not covered, but we may have some unexplored behaviour. coverage for btree is also missing around a couple of edges, and the gaps should be filled in.",
        "label": 86
    },
    {
        "text": "add command  list snapshots  to nodetool <description> it would be nice if the nodetool could tell me which snapshots are present on the system instead of me having to browse the filesystem to fetch the names of the snapshots.<stacktrace> <code> <text> it would be nice if the nodetool could tell me which snapshots are present on the system instead of me having to browse the filesystem to fetch the names of the snapshots.",
        "label": 482
    },
    {
        "text": "create index if not exists throws error when index already exists <description> while testing with trunk, i see that issuing the following queries throws an invalidrequest, despite being valid. create keyspace k with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; use k; create table k.t (     id int primary key,     v int,     v2 int,     v3 text ); create index if not exists on t (v2); create index if not exists on t (v2); invalidrequest: code=2200 [invalid query] message=\"index t_v2_idx_1 is a duplicate of existing index t_v2_idx\" the second create index if not exists should work fine.<stacktrace> <code> create keyspace k with replication = {'class': 'simplestrategy', 'replication_factor': '1'}  and durable_writes = true; use k; create table k.t (     id int primary key,     v int,     v2 int,     v3 text ); create index if not exists on t (v2); create index if not exists on t (v2); invalidrequest: code=2200 [invalid query] message='index t_v2_idx_1 is a duplicate of existing index t_v2_idx' <text> while testing with trunk, i see that issuing the following queries throws an invalidrequest, despite being valid. the second create index if not exists should work fine.",
        "label": 474
    },
    {
        "text": "preserve commitlog size cap when recycling segments at startup <description> 1. create a single node cluster, use default configuration, use cassandra.bat to start the server: 2. run the following commands in cli: create keyspace toto; use toto; create column family titi; truncate titi; 3. the node dies with this error: error 23:23:02,118 exception in thread thread[commit-log-allocator,5,main] java.io.ioerror: java.io.ioexception: map failed         at org.apache.cassandra.db.commitlog.commitlogsegment.<init>(commitlogsegment.java:127)         at org.apache.cassandra.db.commitlog.commitlogsegment.recycle(commitlogsegment.java:202)         at org.apache.cassandra.db.commitlog.commitlogallocator$2.run(commitlogallocator.java:159)         at org.apache.cassandra.db.commitlog.commitlogallocator$1.runmaythrow(commitlogallocator.java:95)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)         at java.lang.thread.run(unknown source) caused by: java.io.ioexception: map failed         at sun.nio.ch.filechannelimpl.map(unknown source)         at org.apache.cassandra.db.commitlog.commitlogsegment.<init>(commitlogsegment.java:119)         ... 5 more caused by: java.lang.outofmemoryerror: map failed         at sun.nio.ch.filechannelimpl.map0(native method)         ... 7 more  info 23:23:02,122 stop listening to thrift clients  info 23:23:02,123 waiting for messaging service to quiesce  info 23:23:02,125 messagingservice shutting down server thread.<stacktrace> error 23:23:02,118 exception in thread thread[commit-log-allocator,5,main] java.io.ioerror: java.io.ioexception: map failed         at org.apache.cassandra.db.commitlog.commitlogsegment.<init>(commitlogsegment.java:127)         at org.apache.cassandra.db.commitlog.commitlogsegment.recycle(commitlogsegment.java:202)         at org.apache.cassandra.db.commitlog.commitlogallocator$2.run(commitlogallocator.java:159)         at org.apache.cassandra.db.commitlog.commitlogallocator$1.runmaythrow(commitlogallocator.java:95)         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)         at java.lang.thread.run(unknown source) caused by: java.io.ioexception: map failed         at sun.nio.ch.filechannelimpl.map(unknown source)         at org.apache.cassandra.db.commitlog.commitlogsegment.<init>(commitlogsegment.java:119)         ... 5 more caused by: java.lang.outofmemoryerror: map failed         at sun.nio.ch.filechannelimpl.map0(native method)         ... 7 more  info 23:23:02,122 stop listening to thrift clients  info 23:23:02,123 waiting for messaging service to quiesce  info 23:23:02,125 messagingservice shutting down server thread. <code> create keyspace toto; use toto; create column family titi; truncate titi; <text> 1. create a single node cluster, use default configuration, use cassandra.bat to start the server: 2. run the following commands in cli: 3. the node dies with this error:",
        "label": 274
    },
    {
        "text": "cqlsstablewriter example fails on 0rc1 <description> cqlsstablewriter which works with 2.2.1 does not work with 3.0rc1.  something like https://github.com/yukim/cassandra-bulkload-example should be added to the test suite. exception in thread \"main\" java.lang.runtimeexception: java.lang.exceptionininitializererror  at org.apache.cassandra.io.sstable.sstablesimpleunsortedwriter.close(sstablesimpleunsortedwriter.java:136)  at org.apache.cassandra.io.sstable.cqlsstablewriter.close(cqlsstablewriter.java:274)  at com.metawiring.sandbox.bulkloadexample.main(bulkloadexample.java:160)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:497)  at com.intellij.rt.execution.application.appmain.main(appmain.java:140)  caused by: java.lang.exceptionininitializererror  at org.apache.cassandra.db.keyspace.initcf(keyspace.java:372)  at org.apache.cassandra.db.keyspace.<init>(keyspace.java:309)  at org.apache.cassandra.db.keyspace.open(keyspace.java:133)  at org.apache.cassandra.db.keyspace.open(keyspace.java:110)  at org.apache.cassandra.io.sstable.sstabletxnwriter.create(sstabletxnwriter.java:97)  at org.apache.cassandra.io.sstable.abstractsstablesimplewriter.createwriter(abstractsstablesimplewriter.java:63)  at org.apache.cassandra.io.sstable.sstablesimpleunsortedwriter$diskwriter.run(sstablesimpleunsortedwriter.java:206)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.config.databasedescriptor.getflushwriters(databasedescriptor.java:1153)  at org.apache.cassandra.db.columnfamilystore.<clinit>(columnfamilystore.java:116)  ... 7 more<stacktrace> exception in thread 'main' java.lang.runtimeexception: java.lang.exceptionininitializererror  at org.apache.cassandra.io.sstable.sstablesimpleunsortedwriter.close(sstablesimpleunsortedwriter.java:136)  at org.apache.cassandra.io.sstable.cqlsstablewriter.close(cqlsstablewriter.java:274)  at com.metawiring.sandbox.bulkloadexample.main(bulkloadexample.java:160)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:497)  at com.intellij.rt.execution.application.appmain.main(appmain.java:140)  caused by: java.lang.exceptionininitializererror  at org.apache.cassandra.db.keyspace.initcf(keyspace.java:372)  at org.apache.cassandra.db.keyspace.<init>(keyspace.java:309)  at org.apache.cassandra.db.keyspace.open(keyspace.java:133)  at org.apache.cassandra.db.keyspace.open(keyspace.java:110)  at org.apache.cassandra.io.sstable.sstabletxnwriter.create(sstabletxnwriter.java:97)  at org.apache.cassandra.io.sstable.abstractsstablesimplewriter.createwriter(abstractsstablesimplewriter.java:63)  at org.apache.cassandra.io.sstable.sstablesimpleunsortedwriter$diskwriter.run(sstablesimpleunsortedwriter.java:206)  caused by: java.lang.nullpointerexception  at org.apache.cassandra.config.databasedescriptor.getflushwriters(databasedescriptor.java:1153)  at org.apache.cassandra.db.columnfamilystore.<clinit>(columnfamilystore.java:116)  ... 7 more<code> <text> cqlsstablewriter which works with 2.2.1 does not work with 3.0rc1.  something like https://github.com/yukim/cassandra-bulkload-example should be added to the test suite. ",
        "label": 98
    },
    {
        "text": "longtype should be network endian <description> that's all<stacktrace> <code> <text> that's all",
        "label": 274
    },
    {
        "text": "simultaneous bootstrap test fails on windows <description> bootstrap_test.py:testbootstrap.simultaneous_bootstrap_test fails on windows on c* 3.0 and 2.2: http://cassci.datastax.com/view/win32/job/cassandra-3.0_dtest_win32/99/testreport/junit/bootstrap_test/testbootstrap/simultaneous_bootstrap_test/ http://cassci.datastax.com/view/win32/job/cassandra-2.2_dtest_win32/127/testreport/junit/bootstrap_test/testbootstrap/simultaneous_bootstrap_test/ it expects there to be a warning in the stderr of the node attempting to start while another node boostraps, but the stderr is empty. it fails on this line: https://github.com/riptano/cassandra-dtest/blob/master/bootstrap_test.py#l469 yuki morishita i believe you're the person to handle this ticket? i'll assign you, but feel free to reassign.<stacktrace> <code> http://cassci.datastax.com/view/win32/job/cassandra-3.0_dtest_win32/99/testreport/junit/bootstrap_test/testbootstrap/simultaneous_bootstrap_test/ http://cassci.datastax.com/view/win32/job/cassandra-2.2_dtest_win32/127/testreport/junit/bootstrap_test/testbootstrap/simultaneous_bootstrap_test/ https://github.com/riptano/cassandra-dtest/blob/master/bootstrap_test.py#l469 <text> bootstrap_test.py:testbootstrap.simultaneous_bootstrap_test fails on windows on c* 3.0 and 2.2: it expects there to be a warning in the stderr of the node attempting to start while another node boostraps, but the stderr is empty. it fails on this line: yuki morishita i believe you're the person to handle this ticket? i'll assign you, but feel free to reassign.",
        "label": 280
    },
    {
        "text": "dtest failure in bootstrap test testbootstrap simultaneous bootstrap test <description> failed 11 times in the last 30 runs. flakiness: 62%, stability: 63% error message errors={<host: 127.0.0.2 datacenter1>: readtimeout('error from server: code=1200 [coordinator node timed out waiting for replica nodes\\' responses] message=\"operation timed out - received only 0 responses.\" info={\\'received_responses\\': 0, \\'required_responses\\': 1, \\'consistency\\': \\'one\\'}',)}, last_host=127.0.0.2 -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-vsuthg dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} cassandra.cluster: info: new cassandra host <host: 127.0.0.1 datacenter1> discovered cassandra.protocol: warning: server warning: aggregation query used without partition key dtest: debug: retrying read after timeout. attempt #0 --------------------- >> end captured logging << --------------------- stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/cassandra-dtest/tools/decorators.py\", line 48, in wrapped     f(obj)   file \"/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/cassandra-dtest/bootstrap_test.py\", line 659, in simultaneous_bootstrap_test     assert_one(session, \"select count(*) from keyspace1.standard1\", [500000], cl=consistencylevel.one)   file \"/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/cassandra-dtest/tools/assertions.py\", line 128, in assert_one     res = session.execute(simple_query)   file \"/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py\", line 2018, in execute     return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()   file \"/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py\", line 3822, in result     raise self._final_exception 'errors={<host: 127.0.0.2 datacenter1>: readtimeout(\\'error from server: code=1200 [coordinator node timed out waiting for replica nodes\\\\\\' responses] message=\"operation timed out - received only 0 responses.\" info={\\\\\\'received_responses\\\\\\': 0, \\\\\\'required_responses\\\\\\': 1, \\\\\\'consistency\\\\\\': \\\\\\'one\\\\\\'}\\',)}, last_host=127.0.0.2\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: /tmp/dtest-vsuthg\\ndtest: debug: done setting configuration options:\\n{   \\'initial_token\\': none,\\n    \\'num_tokens\\': \\'32\\',\\n    \\'phi_convict_threshold\\': 5,\\n    \\'range_request_timeout_in_ms\\': 10000,\\n    \\'read_request_timeout_in_ms\\': 10000,\\n    \\'request_timeout_in_ms\\': 10000,\\n    \\'truncate_request_timeout_in_ms\\': 10000,\\n    \\'write_request_timeout_in_ms\\': 10000}\\ncassandra.cluster: info: new cassandra host <host: 127.0.0.1 datacenter1> discovered\\ncassandra.protocol: warning: server warning: aggregation query used without partition key\\ndtest: debug: retrying read after timeout. attempt #0\\n--------------------- >> end captured logging << ---------------------'<stacktrace> <code> failed 11 times in the last 30 runs. flakiness: 62%, stability: 63% error message errors={<host: 127.0.0.2 datacenter1>: readtimeout('error from server: code=1200 [coordinator node timed out waiting for replica nodes/' responses] message='operation timed out - received only 0 responses.' info={/'received_responses/': 0, /'required_responses/': 1, /'consistency/': /'one/'}',)}, last_host=127.0.0.2 -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-vsuthg dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} cassandra.cluster: info: new cassandra host <host: 127.0.0.1 datacenter1> discovered cassandra.protocol: warning: server warning: aggregation query used without partition key dtest: debug: retrying read after timeout. attempt #0 --------------------- >> end captured logging << --------------------- stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/cassandra-dtest/tools/decorators.py', line 48, in wrapped     f(obj)   file '/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/cassandra-dtest/bootstrap_test.py', line 659, in simultaneous_bootstrap_test     assert_one(session, 'select count(*) from keyspace1.standard1', [500000], cl=consistencylevel.one)   file '/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/cassandra-dtest/tools/assertions.py', line 128, in assert_one     res = session.execute(simple_query)   file '/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py', line 2018, in execute     return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()   file '/home/jenkins/jenkins-slave/workspace/cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py', line 3822, in result     raise self._final_exception 'errors={<host: 127.0.0.2 datacenter1>: readtimeout(/'error from server: code=1200 [coordinator node timed out waiting for replica nodes///' responses] message='operation timed out - received only 0 responses.' info={///'received_responses///': 0, ///'required_responses///': 1, ///'consistency///': ///'one///'}/',)}, last_host=127.0.0.2/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: /tmp/dtest-vsuthg/ndtest: debug: done setting configuration options:/n{   /'initial_token/': none,/n    /'num_tokens/': /'32/',/n    /'phi_convict_threshold/': 5,/n    /'range_request_timeout_in_ms/': 10000,/n    /'read_request_timeout_in_ms/': 10000,/n    /'request_timeout_in_ms/': 10000,/n    /'truncate_request_timeout_in_ms/': 10000,/n    /'write_request_timeout_in_ms/': 10000}/ncassandra.cluster: info: new cassandra host <host: 127.0.0.1 datacenter1> discovered/ncassandra.protocol: warning: server warning: aggregation query used without partition key/ndtest: debug: retrying read after timeout. attempt #0/n--------------------- >> end captured logging << ---------------------'<text> ",
        "label": 79
    },
    {
        "text": "clogged rrs rms stages can hold up processing of gossip messages and request acks <description> the message deserialization process can become a bottleneck that prevents efficient resource utilization because the executor that manages the deserialization process will never grow beyond a single thread. the message deserializer executor is instantiated in the messagingservice constructor as a jmxenablethreadpoolexecutor, which extends java.util.concurrent.threadpoolexecutor. the thread pool is instantiated with a corepoolsize of 1 and a maximumpoolsize of runtime.getruntime().availableprocessors(). but, according to the threadpoolexecutor documentation \"using an unbounded queue (for example a linkedblockingqueue without a predefined capacity) will cause new tasks to be queued in cases where all corepoolsize threads are busy. thus, no more than corepoolsize threads will ever be created. (and the value of the maximumpoolsize therefore doesn't have any effect.)\" the message deserializer pool uses a linkedblockingqueue, so there will never be more than one deserialization thread. this issue became a problem in our production cluster when the message-deserializer-pool began to back up on a node that was only lightly loaded. we increased the core pool size to 4 and the situation improved, but the deserializer pool was still backing up while the machine was not fully utilized (less than 100% cpu utilization). this leads me to think that the deserializer thread is blocking on some sort of i/o, which seems like it shouldn't happen.<stacktrace> <code> <text> the message deserialization process can become a bottleneck that prevents efficient resource utilization because the executor that manages the deserialization process will never grow beyond a single thread. the message deserializer executor is instantiated in the messagingservice constructor as a jmxenablethreadpoolexecutor, which extends java.util.concurrent.threadpoolexecutor. the thread pool is instantiated with a corepoolsize of 1 and a maximumpoolsize of runtime.getruntime().availableprocessors(). but, according to the threadpoolexecutor documentation 'using an unbounded queue (for example a linkedblockingqueue without a predefined capacity) will cause new tasks to be queued in cases where all corepoolsize threads are busy. thus, no more than corepoolsize threads will ever be created. (and the value of the maximumpoolsize therefore doesn't have any effect.)' the message deserializer pool uses a linkedblockingqueue, so there will never be more than one deserialization thread. this issue became a problem in our production cluster when the message-deserializer-pool began to back up on a node that was only lightly loaded. we increased the core pool size to 4 and the situation improved, but the deserializer pool was still backing up while the machine was not fully utilized (less than 100% cpu utilization). this leads me to think that the deserializer thread is blocking on some sort of i/o, which seems like it shouldn't happen.",
        "label": 274
    },
    {
        "text": "data center quorum <description> need a cassandra datacenter quorum read and datacenter quorum write. add 1 new enum dc_quorum. basically reads with this will not span across the datacenter it will use the existing nodes in the datacenter which has this data and read from it.  for writes - all the data centers need to get this data, (datac enters will be configured in the storage-config.xml and number of replicas in it). once configured write will basically write to all the nodes in all the datacenter but will wait only for the write in the current datacenter.   example: we have 3 datacenter a,b,c a has a replication factor of 3, b has 2 and c has 2. dc_quorum write will make sure to write on 2 of 3 nodes in a.... b and c (total 4 +1) nodes will be eventually consistent. changes will be in rackaware, storage, read and write classes. thanks  vijay<stacktrace> <code> <text> need a cassandra datacenter quorum read and datacenter quorum write. add 1 new enum dc_quorum. basically reads with this will not span across the datacenter it will use the existing nodes in the datacenter which has this data and read from it.  for writes - all the data centers need to get this data, (datac enters will be configured in the storage-config.xml and number of replicas in it). once configured write will basically write to all the nodes in all the datacenter but will wait only for the write in the current datacenter.   example: we have 3 datacenter a,b,c a has a replication factor of 3, b has 2 and c has 2. dc_quorum write will make sure to write on 2 of 3 nodes in a.... b and c (total 4 +1) nodes will be eventually consistent. changes will be in rackaware, storage, read and write classes. thanks  vijay",
        "label": 555
    },
    {
        "text": "refactor write path <description> as part of the pluggable storage engine effort, we'd like to modularize the write path related code, make it to be independent from existing storage engine implementation details. for now, refer to https://docs.google.com/document/d/1suzlvhzgb6niybnpm9nxohxz_ri7qam-ueo8v8aifsc for high level designs.<stacktrace> <code> <text> as part of the pluggable storage engine effort, we'd like to modularize the write path related code, make it to be independent from existing storage engine implementation details. for now, refer to https://docs.google.com/document/d/1suzlvhzgb6niybnpm9nxohxz_ri7qam-ueo8v8aifsc for high level designs.",
        "label": 79
    },
    {
        "text": "in the cli  update column family  cf  with comparator  create column metadata <description> using cassandra-cli, i can't update the comparator of a column family with the type i want and when i did it with bytestype, column metadata appear for each of my existing columns.  step to reproduce: [default@unknown] create keyspace test     with placement_strategy = 'org.apache.cassandra.locator.simplestrategy'     and strategy_options = [{replication_factor:1}]; [default@unknown] use test; authenticated to keyspace: test [default@test] create column family test; [default@test] describe keyspace; ...     columnfamily: test       key validation class: org.apache.cassandra.db.marshal.bytestype       default column value validator: org.apache.cassandra.db.marshal.bytestype       columns sorted by: org.apache.cassandra.db.marshal.bytestype       row cache size / save period in seconds: 0.0/0       key cache size / save period in seconds: 200000.0/14400       memtable thresholds: 0.571875/122/1440 (millions of ops/mb/minutes)       gc grace seconds: 864000       compaction min/max thresholds: 4/32       read repair chance: 1.0       replicate on write: false       built indexes: [] ... [default@test] update column family test with comparator = 'longtype'; comparators do not match. why?? the cf is empty [default@test] update column family test with comparator = 'bytestype'; f8e4dcb0-9cca-11e0-0000-d0583497e7ff waiting for schema agreement... ... schemas agree across the cluster [default@test] describe keyspace; ...     columnfamily: test       key validation class: org.apache.cassandra.db.marshal.bytestype       default column value validator: org.apache.cassandra.db.marshal.bytestype       columns sorted by: org.apache.cassandra.db.marshal.bytestype       row cache size / save period in seconds: 0.0/0       key cache size / save period in seconds: 200000.0/14400       memtable thresholds: 0.571875/122/1440 (millions of ops/mb/minutes)       gc grace seconds: 864000       compaction min/max thresholds: 4/32       read repair chance: 1.0       replicate on write: false       built indexes: [] ... [default@test] set test[ascii('row1')][long(1)]=integer(35); set test[ascii('row1')][long(2)]=integer(36); set test[ascii('row1')][long(3)]=integer(38); set test[ascii('row2')][long(1)]=integer(45); set test[ascii('row2')][long(2)]=integer(42); set test[ascii('row2')][long(3)]=integer(33); [default@test] list test; using default limit of 100 ------------------- rowkey: 726f7731 => (column=0000000000000001, value=35, timestamp=1308744931122000) => (column=0000000000000002, value=36, timestamp=1308744931124000) => (column=0000000000000003, value=38, timestamp=1308744931125000) ------------------- rowkey: 726f7732 => (column=0000000000000001, value=45, timestamp=1308744931127000) => (column=0000000000000002, value=42, timestamp=1308744931128000) => (column=0000000000000003, value=33, timestamp=1308744932722000) 2 rows returned. [default@test] update column family test with comparator = 'longtype'; comparators do not match. same question than before, my columns contains only long, why i can't? [default@test] update column family test with comparator = 'bytestype'; [default@test] describe keyspace;                                       keyspace: test:   replication strategy: org.apache.cassandra.locator.simplestrategy     options: [replication_factor:1]   column families:     columnfamily: test       key validation class: org.apache.cassandra.db.marshal.bytestype       default column value validator: org.apache.cassandra.db.marshal.bytestype       columns sorted by: org.apache.cassandra.db.marshal.bytestype       row cache size / save period in seconds: 0.0/0       key cache size / save period in seconds: 200000.0/14400       memtable thresholds: 0.571875/122/1440 (millions of ops/mb/minutes)       gc grace seconds: 864000       compaction min/max thresholds: 4/32       read repair chance: 1.0       replicate on write: false       built indexes: []       column metadata:         column name:  (0000000000000001)           validation class: org.apache.cassandra.db.marshal.integertype         column name:  (0000000000000003)           validation class: org.apache.cassandra.db.marshal.integertype         column name:  (0000000000000002)           validation class: org.apache.cassandra.db.marshal.integertype column metadata appear from nowhere. i don't think that it's expected.<stacktrace> <code> [default@unknown] create keyspace test     with placement_strategy = 'org.apache.cassandra.locator.simplestrategy'     and strategy_options = [{replication_factor:1}]; [default@unknown] use test; authenticated to keyspace: test [default@test] create column family test; [default@test] describe keyspace; ...     columnfamily: test       key validation class: org.apache.cassandra.db.marshal.bytestype       default column value validator: org.apache.cassandra.db.marshal.bytestype       columns sorted by: org.apache.cassandra.db.marshal.bytestype       row cache size / save period in seconds: 0.0/0       key cache size / save period in seconds: 200000.0/14400       memtable thresholds: 0.571875/122/1440 (millions of ops/mb/minutes)       gc grace seconds: 864000       compaction min/max thresholds: 4/32       read repair chance: 1.0       replicate on write: false       built indexes: [] ... [default@test] update column family test with comparator = 'longtype'; comparators do not match. [default@test] update column family test with comparator = 'bytestype'; f8e4dcb0-9cca-11e0-0000-d0583497e7ff waiting for schema agreement... ... schemas agree across the cluster [default@test] describe keyspace; ...     columnfamily: test       key validation class: org.apache.cassandra.db.marshal.bytestype       default column value validator: org.apache.cassandra.db.marshal.bytestype       columns sorted by: org.apache.cassandra.db.marshal.bytestype       row cache size / save period in seconds: 0.0/0       key cache size / save period in seconds: 200000.0/14400       memtable thresholds: 0.571875/122/1440 (millions of ops/mb/minutes)       gc grace seconds: 864000       compaction min/max thresholds: 4/32       read repair chance: 1.0       replicate on write: false       built indexes: [] ... [default@test] set test[ascii('row1')][long(1)]=integer(35); set test[ascii('row1')][long(2)]=integer(36); set test[ascii('row1')][long(3)]=integer(38); set test[ascii('row2')][long(1)]=integer(45); set test[ascii('row2')][long(2)]=integer(42); set test[ascii('row2')][long(3)]=integer(33); [default@test] list test; using default limit of 100 ------------------- rowkey: 726f7731 => (column=0000000000000001, value=35, timestamp=1308744931122000) => (column=0000000000000002, value=36, timestamp=1308744931124000) => (column=0000000000000003, value=38, timestamp=1308744931125000) ------------------- rowkey: 726f7732 => (column=0000000000000001, value=45, timestamp=1308744931127000) => (column=0000000000000002, value=42, timestamp=1308744931128000) => (column=0000000000000003, value=33, timestamp=1308744932722000) 2 rows returned. [default@test] update column family test with comparator = 'longtype'; comparators do not match. [default@test] update column family test with comparator = 'bytestype'; [default@test] describe keyspace;                                       keyspace: test:   replication strategy: org.apache.cassandra.locator.simplestrategy     options: [replication_factor:1]   column families:     columnfamily: test       key validation class: org.apache.cassandra.db.marshal.bytestype       default column value validator: org.apache.cassandra.db.marshal.bytestype       columns sorted by: org.apache.cassandra.db.marshal.bytestype       row cache size / save period in seconds: 0.0/0       key cache size / save period in seconds: 200000.0/14400       memtable thresholds: 0.571875/122/1440 (millions of ops/mb/minutes)       gc grace seconds: 864000       compaction min/max thresholds: 4/32       read repair chance: 1.0       replicate on write: false       built indexes: []       column metadata:         column name:  (0000000000000001)           validation class: org.apache.cassandra.db.marshal.integertype         column name:  (0000000000000003)           validation class: org.apache.cassandra.db.marshal.integertype         column name:  (0000000000000002)           validation class: org.apache.cassandra.db.marshal.integertype <text> using cassandra-cli, i can't update the comparator of a column family with the type i want and when i did it with bytestype, column metadata appear for each of my existing columns.  step to reproduce: why?? the cf is empty same question than before, my columns contains only long, why i can't? column metadata appear from nowhere. i don't think that it's expected.",
        "label": 412
    },
    {
        "text": "setting rr chance via cliclient results in chance being too low <description> running a command like \"update column family shorturls with read_repair_chance=0.4;\" results in the value being set to 0.0040. was expecting it to be 0.4. affects 0.7.6.-2; seems to be fixed on trunk/0.8.<stacktrace> <code> 'update column family shorturls with read_repair_chance=0.4;' <text> running a command like results in the value being set to 0.0040. was expecting it to be 0.4. affects 0.7.6.-2; seems to be fixed on trunk/0.8.",
        "label": 270
    },
    {
        "text": "add possibility for command line client to connect using ip address <description> the \"connect \" command of the command line client should accept an ip-address (not only host names)<stacktrace> <code> <text> the 'connect ' command of the command line client should accept an ip-address (not only host names)",
        "label": 418
    },
    {
        "text": "word count example fails with invalidrequestexception why start key's token sorts after end token  <description> tried with the latest 1.2 branch (commit d64dc2eb3a1a3c3771bbe3218af9ce9629ec67bf) and got this error. seems related to but different than cassandra-5106.<stacktrace> <code> <text> tried with the latest 1.2 branch (commit d64dc2eb3a1a3c3771bbe3218af9ce9629ec67bf) and got this error. seems related to but different than cassandra-5106.",
        "label": 85
    },
    {
        "text": "hashlib instead of md5 lib in stress py <description> the current stress.py script uses md5-lib that is depreciated. hashlib is the recommended replacement. the two libraries can have different performance profiles so a new stress.py should be refactored to use hashlib<stacktrace> <code> <text> the current stress.py script uses md5-lib that is depreciated. hashlib is the recommended replacement. the two libraries can have different performance profiles so a new stress.py should be refactored to use hashlib",
        "label": 503
    },
    {
        "text": "add support for reversedtype <description> it would be nice to add a native syntax for the use of reversedtype. i'm sure there is anything in sql that we inspired ourselves from, so i would propose something like: create table timeseries (   key text,   time uuid,   value text,   primary key (key, time desc) ) alternatively, the desc could also be put after the column name definition but one argument for putting it in the pk instead is that this only apply to keys.<stacktrace> <code> create table timeseries (   key text,   time uuid,   value text,   primary key (key, time desc) ) <text> it would be nice to add a native syntax for the use of reversedtype. i'm sure there is anything in sql that we inspired ourselves from, so i would propose something like: alternatively, the desc could also be put after the column name definition but one argument for putting it in the pk instead is that this only apply to keys.",
        "label": 520
    },
    {
        "text": "add tests for rackawarestrategy <description> rackunawarestrategy has tests (perhaps some of these could be made more generic); ras does not, nor does datacentershardstrategy<stacktrace> <code> <text> rackunawarestrategy has tests (perhaps some of these could be made more generic); ras does not, nor does datacentershardstrategy",
        "label": 454
    },
    {
        "text": "alter type  name  rename to  name  no longer parses as valid cql <description> type renaming seems to be broken. the error looks like perhaps the syntax has changed or there's a problem parsing the cql. cqlsh:test> create type foo (somefield int); cqlsh:test> alter type foo rename to bar; <errormessage code=2000 [syntax error in cql query] message=\"line 1:22 no viable alternative at input 'to' (alter type foo rename [to] bar...)\"><stacktrace> <code> cqlsh:test> create type foo (somefield int); cqlsh:test> alter type foo rename to bar; <errormessage code=2000 [syntax error in cql query] message='line 1:22 no viable alternative at input 'to' (alter type foo rename [to] bar...)'> <text> type renaming seems to be broken. the error looks like perhaps the syntax has changed or there's a problem parsing the cql.",
        "label": 362
    },
    {
        "text": "update columnfamilyoutputformat to use new bulkload api <description> the bulk loading interface added in cassandra-1278 is a great fit for hadoop jobs.<stacktrace> <code> <text> the bulk loading interface added in cassandra-1278 is a great fit for hadoop jobs.",
        "label": 85
    },
    {
        "text": "live ratio limit is way too low <description> currently live ratio is limited to 64.0. this is way too low. warn [memorymeter:1] memtable.java (line 181) setting live ratio to maximum of 64 instead of xxxx values seen in log are ofter larger than 64.0 limit. i propose to use 100.0 as new limit. ponto:(admin)log/cassandra>grep \"to maximum of 64\" system.log.1  warn [memorymeter:1] 2012-02-03 00:00:19,444 memtable.java (line 181) setting live ratio to maximum of 64 instead of 64.9096047648211  warn [memorymeter:1] 2012-02-08 00:00:17,379 memtable.java (line 181) setting live ratio to maximum of 64 instead of 68.81016452376322  warn [memorymeter:1] 2012-02-08 00:00:32,358 memtable.java (line 181) setting live ratio to maximum of 64 instead of 88.49747308025415  warn [memorymeter:1] 2012-02-09 00:00:08,448 memtable.java (line 181) setting live ratio to maximum of 64 instead of 76.2444888765154  warn [memorymeter:1] 2012-02-10 18:18:52,677 memtable.java (line 181) setting live ratio to maximum of 64 instead of 142.22477982642255  warn [memorymeter:1] 2012-02-20 00:00:53,753 memtable.java (line 181) setting live ratio to maximum of 64 instead of 88.19832386767173  warn [memorymeter:1] 2012-03-02 10:41:00,232 memtable.java (line 181) setting live ratio to maximum of 64 instead of 419.9607495592804  warn [memorymeter:1] 2012-03-07 14:13:15,141 memtable.java (line 181) setting live ratio to maximum of 64 instead of infinity  warn [memorymeter:1] 2012-03-08 00:01:12,766 memtable.java (line 181) setting live ratio to maximum of 64 instead of 94.20215772717702  warn [memorymeter:1] 2012-03-09 00:00:38,633 memtable.java (line 181) setting live ratio to maximum of 64 instead of 98.54003447121715  warn [memorymeter:1] 2012-03-11 00:00:13,243 memtable.java (line 181) setting live ratio to maximum of 64 instead of 193.14262214179965  warn [memorymeter:1] 2012-03-14 00:00:26,709 memtable.java (line 181) setting live ratio to maximum of 64 instead of 103.88360138951437<stacktrace> <code> warn [memorymeter:1] memtable.java (line 181) setting live ratio to maximum of 64 instead of xxxx ponto:(admin)log/cassandra>grep 'to maximum of 64' system.log.1  warn [memorymeter:1] 2012-02-03 00:00:19,444 memtable.java (line 181) setting live ratio to maximum of 64 instead of 64.9096047648211  warn [memorymeter:1] 2012-02-08 00:00:17,379 memtable.java (line 181) setting live ratio to maximum of 64 instead of 68.81016452376322  warn [memorymeter:1] 2012-02-08 00:00:32,358 memtable.java (line 181) setting live ratio to maximum of 64 instead of 88.49747308025415  warn [memorymeter:1] 2012-02-09 00:00:08,448 memtable.java (line 181) setting live ratio to maximum of 64 instead of 76.2444888765154  warn [memorymeter:1] 2012-02-10 18:18:52,677 memtable.java (line 181) setting live ratio to maximum of 64 instead of 142.22477982642255  warn [memorymeter:1] 2012-02-20 00:00:53,753 memtable.java (line 181) setting live ratio to maximum of 64 instead of 88.19832386767173  warn [memorymeter:1] 2012-03-02 10:41:00,232 memtable.java (line 181) setting live ratio to maximum of 64 instead of 419.9607495592804  warn [memorymeter:1] 2012-03-07 14:13:15,141 memtable.java (line 181) setting live ratio to maximum of 64 instead of infinity  warn [memorymeter:1] 2012-03-08 00:01:12,766 memtable.java (line 181) setting live ratio to maximum of 64 instead of 94.20215772717702  warn [memorymeter:1] 2012-03-09 00:00:38,633 memtable.java (line 181) setting live ratio to maximum of 64 instead of 98.54003447121715  warn [memorymeter:1] 2012-03-11 00:00:13,243 memtable.java (line 181) setting live ratio to maximum of 64 instead of 193.14262214179965  warn [memorymeter:1] 2012-03-14 00:00:26,709 memtable.java (line 181) setting live ratio to maximum of 64 instead of 103.88360138951437<text> currently live ratio is limited to 64.0. this is way too low. values seen in log are ofter larger than 64.0 limit. i propose to use 100.0 as new limit. ",
        "label": 520
    },
    {
        "text": "dsnitch severity is not correctly set for compaction info <description> we're doing two things wrong in ci. first, load can change between calls, which can cause a negative severity even though it meant to subtract whatever it added before. second, we should report based on how much io we're using, since a 1t throttled to 5mb/s is less impactful than a 100mb running at full speed.<stacktrace> <code> <text> we're doing two things wrong in ci. first, load can change between calls, which can cause a negative severity even though it meant to subtract whatever it added before. second, we should report based on how much io we're using, since a 1t throttled to 5mb/s is less impactful than a 100mb running at full speed.",
        "label": 555
    },
    {
        "text": "remove cold reads to omit from stcs <description> while i upgrading my cluster to 2.1.3, i find some nodes (not all) may have gc issue after the node restarting successfully. old gen grows very fast and most of the space can not be recycled after setting its status to normal immediately. the qps of both reading and writing are very low and there is no heavy compaction. jmap result seems strange that there are too many java.util.hashmap$entry objects in heap, where in my experience the \"[b\" is usually the no1. if i downgrade it to 2.1.1, this issue will not appear. i uploaded conf files and jstack/jmap outputs. i'll upload heap dump if someone need it.<stacktrace> <code> <text> while i upgrading my cluster to 2.1.3, i find some nodes (not all) may have gc issue after the node restarting successfully. old gen grows very fast and most of the space can not be recycled after setting its status to normal immediately. the qps of both reading and writing are very low and there is no heavy compaction. jmap result seems strange that there are too many java.util.hashmap$entry objects in heap, where in my experience the '[b' is usually the no1. if i downgrade it to 2.1.1, this issue will not appear. i uploaded conf files and jstack/jmap outputs. i'll upload heap dump if someone need it.",
        "label": 321
    },
    {
        "text": "repairtask doesn't send a correct message in a jmx notifcation in case of illegalargumentexception <description> from cassandra-7317: the behavior when manually trying to repair the full range of 127.0.0.01 definitely needs improvement though. repair command: [nicks-macbook-pro:21:50:44 cassandra-2.0] cassandra$ bin/nodetool -p 7100 repair -st -10 -et -9223372036854775808 system_traces [2014-05-28 21:50:55,803] starting repair command #17, repairing 1 ranges for keyspace system_traces [2014-05-28 21:50:55,804] starting repair command #17, repairing 1 ranges for keyspace system_traces [2014-05-28 21:50:55,804] repair command #17 finished [nicks-macbook-pro:21:50:56 cassandra-2.0] cassandra$ echo $? 1 system.log: error [thread-96] 2014-05-28 21:40:05,921 storageservice.java (line 2621) repair session failed: java.lang.illegalargumentexception: requested range intersects a local range but is not fully contained in one; this would lead to imprecise repair the actual output of the repair command doesn't really indicate that there was an issue. although the command does return with a non zero exit status. the error here is invisible if you are using the synchronous jmx repair api. it will appear as though the repair completed successfully.<stacktrace> <code> [nicks-macbook-pro:21:50:44 cassandra-2.0] cassandra$ bin/nodetool -p 7100 repair -st -10 -et -9223372036854775808 system_traces [2014-05-28 21:50:55,803] starting repair command #17, repairing 1 ranges for keyspace system_traces [2014-05-28 21:50:55,804] starting repair command #17, repairing 1 ranges for keyspace system_traces [2014-05-28 21:50:55,804] repair command #17 finished [nicks-macbook-pro:21:50:56 cassandra-2.0] cassandra$ echo $? 1 error [thread-96] 2014-05-28 21:40:05,921 storageservice.java (line 2621) repair session failed: java.lang.illegalargumentexception: requested range intersects a local range but is not fully contained in one; this would lead to imprecise repair <text> from cassandra-7317: the behavior when manually trying to repair the full range of 127.0.0.01 definitely needs improvement though. repair command: system.log:",
        "label": 362
    },
    {
        "text": "copy to improvements <description> copy from has gotten a lot of love. copy to not so much. one obvious improvement could be to parallelize reading and writing (write one page of data while fetching the next).<stacktrace> <code> <text> copy from has gotten a lot of love. copy to not so much. one obvious improvement could be to parallelize reading and writing (write one page of data while fetching the next).",
        "label": 508
    },
    {
        "text": "speculative retry can sometimes violate consistency <description> this is most evident with intermittent failures of the short_read dtests. i'll focus on short_read_reversed_test for explanation, since that's what i used to bisect. this test inserts some columns into a row, then deletes a subset, but it performs each delete on a different node, with another node down (hints are disabled.) finally it reads the row back at quorum and checks that it doesn't see any deleted columns, however with speculative retry on this often fails. i bisected this to the change that made 99th percentile sr the default reliably by looping the test enough times at each iteration to be sure it was passing or failing.<stacktrace> <code> <text> this is most evident with intermittent failures of the short_read dtests. i'll focus on short_read_reversed_test for explanation, since that's what i used to bisect. this test inserts some columns into a row, then deletes a subset, but it performs each delete on a different node, with another node down (hints are disabled.) finally it reads the row back at quorum and checks that it doesn't see any deleted columns, however with speculative retry on this often fails. i bisected this to the change that made 99th percentile sr the default reliably by looping the test enough times at each iteration to be sure it was passing or failing.",
        "label": 274
    },
    {
        "text": "avoid repetetive sorting in cfs getcolumnfamilyfromdisk <description> getcolumnfamilyfromdisk starts out with  list<string> files = new arraylist<string>();   lock_.readlock().lock();  try { files.addall(sstables_); collections.sort(files, new filenamecomparator(filenamecomparator.descending)); } finally { lock_.readlock().unlock(); } this is silly, we should really keep the list sorted instead of re-sorting for each read.<stacktrace> <code> list<string> files = new arraylist<string>();   lock_.readlock().lock();  try <text> getcolumnfamilyfromdisk starts out with finally this is silly, we should really keep the list sorted instead of re-sorting for each read.",
        "label": 274
    },
    {
        "text": "set qt shrinks project wide <description> for many of our tests quicktheories shrinker is not sufficient and costs time and memory when shrinking large problem spaces. disable it by default.<stacktrace> <code> <text> for many of our tests quicktheories shrinker is not sufficient and costs time and memory when shrinking large problem spaces. disable it by default.",
        "label": 85
    },
    {
        "text": "nosetests   system tests fail <description> cql driver version used: 1.0.8. eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee ====================================================================== error: system.test_thrift_server.testmutations.test_bad_batch_calls ---------------------------------------------------------------------- traceback (most recent call last):   file \"/usr/local/lib/python2.7/site-packages/nose/case.py\", line 381, in setup     try_run(self.inst, ('setup', 'setup'))   file \"/usr/local/lib/python2.7/site-packages/nose/util.py\", line 478, in try_run     return func()   file \"/var/lib/jenkins/jobs/cassandra/workspace/test/system/__init__.py\", line 113, in setup     self.define_schema()   file \"/var/lib/jenkins/jobs/cassandra/workspace/test/system/__init__.py\", line 158, in define_schema     cassandra.cfdef('keyspace1', 'super1', column_type='super', subcomparator_type='longtype', row_cache_size=1000, key_cache_size=0), typeerror: __init__() got an unexpected keyword argument 'key_cache_size'<stacktrace> <code> eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee ====================================================================== error: system.test_thrift_server.testmutations.test_bad_batch_calls ---------------------------------------------------------------------- traceback (most recent call last):   file '/usr/local/lib/python2.7/site-packages/nose/case.py', line 381, in setup     try_run(self.inst, ('setup', 'setup'))   file '/usr/local/lib/python2.7/site-packages/nose/util.py', line 478, in try_run     return func()   file '/var/lib/jenkins/jobs/cassandra/workspace/test/system/__init__.py', line 113, in setup     self.define_schema()   file '/var/lib/jenkins/jobs/cassandra/workspace/test/system/__init__.py', line 158, in define_schema     cassandra.cfdef('keyspace1', 'super1', column_type='super', subcomparator_type='longtype', row_cache_size=1000, key_cache_size=0), typeerror: __init__() got an unexpected keyword argument 'key_cache_size' <text> cql driver version used: 1.0.8.",
        "label": 412
    },
    {
        "text": "creating materialized views concurrently leads to missing data <description> andrew hust was writing dtests that create multiple tables concurrently. he also wrote a test that creates multiple mv but has not been able to get it works properly. after some debugging outside of dtest, it seems that there is an issue if we create more than 1 mv at the same time. there is no errors in the log but the mv are never entirely populated and are missing data. i've attached 2 scripts: mv_test_bad.sh: is the one that reproduce the issue. it creates 4 mvs at the same time. at the end, some data are missing in the mvs and there is nothing in system.hints or system.batchlog. mv_test_good.sh: is the same script but that waits 10 seconds between each mv creation, which results in 4 mvs with all the data. some more notes from andrew: - lowering the number of rows inserted below ~1000 won't exhibit the inconsistent behavior - adding more columns/mv make it worse -- more of the mvs counts are consistently wrong - multiple runs will range in disagreement -- usually one of the mvs is correct though - the describe cluster and system.mv* queries always \"look\" good thanks andrew for finding this bug and providing the test scripts! //cc carl yeksigian t jake luciani ryan mcguire<stacktrace> <code> - lowering the number of rows inserted below ~1000 won't exhibit the inconsistent behavior - adding more columns/mv make it worse -- more of the mvs counts are consistently wrong - multiple runs will range in disagreement -- usually one of the mvs is correct though - the describe cluster and system.mv* queries always 'look' good <text> andrew hust was writing dtests that create multiple tables concurrently. he also wrote a test that creates multiple mv but has not been able to get it works properly. after some debugging outside of dtest, it seems that there is an issue if we create more than 1 mv at the same time. there is no errors in the log but the mv are never entirely populated and are missing data. i've attached 2 scripts: mv_test_bad.sh: is the one that reproduce the issue. it creates 4 mvs at the same time. at the end, some data are missing in the mvs and there is nothing in system.hints or system.batchlog. mv_test_good.sh: is the same script but that waits 10 seconds between each mv creation, which results in 4 mvs with all the data. some more notes from andrew: thanks andrew for finding this bug and providing the test scripts! //cc carl yeksigian t jake luciani ryan mcguire",
        "label": 521
    },
    {
        "text": "dtest failure in upgrade tests cql tests testcqlnodes2rf1 upgrade current x to indev x conditional update test <description> example failure: http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/9/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_2_0_x_to_indev_2_1_x/conditional_update_test error message <error from server: code=2000 [syntax error in cql query] message=\"line 1:35 no viable alternative at input 'in'\"> stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py\", line 3028, in conditional_update_test     assert_one(cursor, \"delete from test where k = 0 if v1 in (null)\", [true])   file \"/home/automaton/cassandra-dtest/tools/assertions.py\", line 128, in assert_one     res = session.execute(simple_query)   file \"cassandra/cluster.py\", line 1998, in cassandra.cluster.session.execute (cassandra/cluster.c:34869)     return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()   file \"cassandra/cluster.py\", line 3781, in cassandra.cluster.responsefuture.result (cassandra/cluster.c:73073)     raise self._final_exception<stacktrace> <code> error message <error from server: code=2000 [syntax error in cql query] message='line 1:35 no viable alternative at input 'in''> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py', line 3028, in conditional_update_test     assert_one(cursor, 'delete from test where k = 0 if v1 in (null)', [true])   file '/home/automaton/cassandra-dtest/tools/assertions.py', line 128, in assert_one     res = session.execute(simple_query)   file 'cassandra/cluster.py', line 1998, in cassandra.cluster.session.execute (cassandra/cluster.c:34869)     return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()   file 'cassandra/cluster.py', line 3781, in cassandra.cluster.responsefuture.result (cassandra/cluster.c:73073)     raise self._final_exception http://cassci.datastax.com/job/cassandra-2.1_dtest_upgrade/9/testreport/upgrade_tests.cql_tests/testcqlnodes2rf1_upgrade_current_2_0_x_to_indev_2_1_x/conditional_update_test<text> example failure: ",
        "label": 428
    },
    {
        "text": "unable to delete after running scrub <description> another problem with sstable deletions on 1.0. running scrub produces lot of unable to delete messages on windows. error 16:16:37,562 unable to delete \\var\\lib\\cassandra\\data\\test\\sipdb-h-711-dat  a.db (it will be removed on server restart; we'll also retry after gc)  info 16:16:37,577 scrub of sstablereader(path='\\var\\lib\\cassandra\\data\\test\\sip  db-h-711-data.db') complete: 48396 rows in new sstable and 0 empty (tombstoned)  rows dropped<stacktrace> <code> <text> another problem with sstable deletions on 1.0. running scrub produces lot of unable to delete messages on windows. error 16:16:37,562 unable to delete /var/lib/cassandra/data/test/sipdb-h-711-dat  a.db (it will be removed on server restart; we'll also retry after gc)  info 16:16:37,577 scrub of sstablereader(path='/var/lib/cassandra/data/test/sip  db-h-711-data.db') complete: 48396 rows in new sstable and 0 empty (tombstoned)  rows dropped",
        "label": 274
    },
    {
        "text": "cassandra and later require java 7u25 or later   jdk <description> we have been running the cassandr server version 2.1.5. friday, we applied the latest java patch, java(tm) se runtime environment (build 1.7.0_101-b14). cassandra cannot start with this patch. the cassandra log states: cassandra 2.0 and later require java 7u25 or later.<stacktrace> <code> <text> we have been running the cassandr server version 2.1.5. friday, we applied the latest java patch, java(tm) se runtime environment (build 1.7.0_101-b14). cassandra cannot start with this patch. the cassandra log states: cassandra 2.0 and later require java 7u25 or later.",
        "label": 161
    },
    {
        "text": "o a c db marshal collectiontypetest unit test failing in <description> bisecting..<stacktrace> <code> <text> bisecting..",
        "label": 520
    },
    {
        "text": "get mucher higher load and latencies after upgrading from to cassandra <description> after upgrading our cassandra staging cluster version from 2.1.6 to 2.1.7, the average load grows from 0.1-0.3 to 1.8. latencies did increase as well. we see an increase of pending compactions, probably due to cassandra-9592. this cluster has almost no workload (staging environment)<stacktrace> <code> <text> after upgrading our cassandra staging cluster version from 2.1.6 to 2.1.7, the average load grows from 0.1-0.3 to 1.8. latencies did increase as well. we see an increase of pending compactions, probably due to cassandra-9592. this cluster has almost no workload (staging environment)",
        "label": 52
    },
    {
        "text": "flaky test org apache cassandra distributed test simplereadwritetest readwithschemadisagreement <description> this fails infrequently, last seen failure was on java 8 junit.framework.assertionfailederror at org.apache.cassandra.distributed.test.distributedreadwritepathtest.readwithschemadisagreement(distributedreadwritepathtest.java:276)<stacktrace> junit.framework.assertionfailederror at org.apache.cassandra.distributed.test.distributedreadwritepathtest.readwithschemadisagreement(distributedreadwritepathtest.java:276) <code> <text> this fails infrequently, last seen failure was on java 8",
        "label": 299
    },
    {
        "text": "create table accepts value for default time to live on counter table <description> i can create a counter table (via cqlsh) with a default_time_to_live: create table if not exists metrics2(  time timestamp,  value counter,  primary key ((time))  ) with default_time_to_live=10; upsert a row that increments the counter: update metrics2 set value=value+1 where timestamp='2015-01-24 10:48 -0600'; wait 10 seconds, and select, and the row is (of course) still there. there should probably be a warning or error preventing the creation of a table that has both counter columns and a value set for default_time_to_live.<stacktrace> <code> create table if not exists metrics2(  time timestamp,  value counter,  primary key ((time))  ) with default_time_to_live=10; update metrics2 set value=value+1 where timestamp='2015-01-24 10:48 -0600'; <text> i can create a counter table (via cqlsh) with a default_time_to_live: upsert a row that increments the counter: wait 10 seconds, and select, and the row is (of course) still there. there should probably be a warning or error preventing the creation of a table that has both counter columns and a value set for default_time_to_live.",
        "label": 241
    },
    {
        "text": "inconsistent  position  numbering for keys in  system schema columns  <description> a single component partition key starts off with a -1 position. cqlsh> create table test.table1 (key1 text, value1 text, value2 text, primary key(key1)); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table1' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table1 |        key1 | partition_key |       -1           test |     table1 |      value1 |       regular |       -1           test |     table1 |      value2 |       regular |       -1 a single component clustering key starts off with a 0 position. cqlsh> create table test.table2 (key1 text, value1 text, value2 text, primary key(key1, value1)); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table2' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table2 |        key1 | partition_key |       -1           test |     table2 |      value1 |    clustering |        0           test |     table2 |      value2 |       regular |       -1 when another component is added to the partition key it starts at 0. cqlsh> create table test.table3 (key1 text, value1 text, value2 text, primary key((key1, value1))); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table3' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table3 |        key1 | partition_key |        0           test |     table3 |      value1 | partition_key |        1           test |     table3 |      value2 |       regular |       -1 which is the same as a multiple component clustering key. cqlsh> create table test.table4 (key1 text, value1 text, value2 text, primary key(key1, value1, value2)); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table4' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table4 |        key1 | partition_key |       -1           test |     table4 |      value1 |    clustering |        0           test |     table4 |      value2 |    clustering |        1 shouldn't a single component partition key start off with a position of 0?<stacktrace> <code> cqlsh> create table test.table1 (key1 text, value1 text, value2 text, primary key(key1)); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table1' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table1 |        key1 | partition_key |       -1           test |     table1 |      value1 |       regular |       -1           test |     table1 |      value2 |       regular |       -1 cqlsh> create table test.table2 (key1 text, value1 text, value2 text, primary key(key1, value1)); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table2' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table2 |        key1 | partition_key |       -1           test |     table2 |      value1 |    clustering |        0           test |     table2 |      value2 |       regular |       -1 cqlsh> create table test.table3 (key1 text, value1 text, value2 text, primary key((key1, value1))); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table3' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table3 |        key1 | partition_key |        0           test |     table3 |      value1 | partition_key |        1           test |     table3 |      value2 |       regular |       -1 cqlsh> create table test.table4 (key1 text, value1 text, value2 text, primary key(key1, value1, value2)); cqlsh> select keyspace_name, table_name, column_name, kind, position from system_schema.columns where keyspace_name = 'test' and  table_name = 'table4' ;  keyspace_name | table_name | column_name | kind          | position ---------------+------------+-------------+---------------+----------           test |     table4 |        key1 | partition_key |       -1           test |     table4 |      value1 |    clustering |        0           test |     table4 |      value2 |    clustering |        1 <text> a single component partition key starts off with a -1 position. a single component clustering key starts off with a 0 position. when another component is added to the partition key it starts at 0. which is the same as a multiple component clustering key. shouldn't a single component partition key start off with a position of 0?",
        "label": 520
    },
    {
        "text": "make mv index creation robust for wide referent rows <description> when creating an index for a materialized view for extant data, heap pressure is very dependent on the cardinality of of rows associated with each index value. with the way that per-index value rows are created within the index, this can cause unbounded heap pressure, which can cause oom. this appears to be a side-effect of how each index row is applied atomically as with batches. the commit logs can accumulate enough during the process to prevent the node from being restarted. given that this occurs during global index creation, this can happen on multiple nodes, making stable recovery of a node set difficult, as co-replicas become unavailable to assist in back-filling data from commitlogs. while it is understandable that you want to avoid having relatively wide rows even in materialized views, this represents a particularly difficult scenario for triage. the basic recommendation for improving this is to sub-group the index creation into smaller chunks internally, providing a maximal bound against the heap pressure when it is needed.<stacktrace> <code> <text> when creating an index for a materialized view for extant data, heap pressure is very dependent on the cardinality of of rows associated with each index value. with the way that per-index value rows are created within the index, this can cause unbounded heap pressure, which can cause oom. this appears to be a side-effect of how each index row is applied atomically as with batches. the commit logs can accumulate enough during the process to prevent the node from being restarted. given that this occurs during global index creation, this can happen on multiple nodes, making stable recovery of a node set difficult, as co-replicas become unavailable to assist in back-filling data from commitlogs. while it is understandable that you want to avoid having relatively wide rows even in materialized views, this represents a particularly difficult scenario for triage. the basic recommendation for improving this is to sub-group the index creation into smaller chunks internally, providing a maximal bound against the heap pressure when it is needed.",
        "label": 98
    },
    {
        "text": "compactionstest fails with timeout <description>     [junit] testsuite: org.apache.cassandra.db.compaction.compactionstest     [junit] testsuite: org.apache.cassandra.db.compaction.compactionstest     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0 sec     [junit]      [junit] testcase: org.apache.cassandra.db.compaction.compactionstest:teststandardcolumncompactions: caused an error     [junit] timeout occurred. please note the time in the report does not reflect the time until the timeout.     [junit] junit.framework.assertionfailederror: timeout occurred. please note the time in the report does not reflect the time until the timeout.     [junit]      [junit]      [junit] test org.apache.cassandra.db.compaction.compactionstest failed (timeout)<stacktrace> <code>     [junit] testsuite: org.apache.cassandra.db.compaction.compactionstest     [junit] testsuite: org.apache.cassandra.db.compaction.compactionstest     [junit] tests run: 1, failures: 0, errors: 1, time elapsed: 0 sec     [junit]      [junit] testcase: org.apache.cassandra.db.compaction.compactionstest:teststandardcolumncompactions: caused an error     [junit] timeout occurred. please note the time in the report does not reflect the time until the timeout.     [junit] junit.framework.assertionfailederror: timeout occurred. please note the time in the report does not reflect the time until the timeout.     [junit]      [junit]      [junit] test org.apache.cassandra.db.compaction.compactionstest failed (timeout)<text> ",
        "label": 28
    },
    {
        "text": "data integrity flappers in upgrade tests dtests <description> a number of the upgrade_tests in dtest are flapping with data integrity bugs. for instance: http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/422/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3/conditional_delete_test/history/ http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/421/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3/cas_and_list_index_test/history/ this needs further digging to get good debug information; it could well be, e.g., a race condition in the test. i have not reproduced locally. /cc philip thompson<stacktrace> <code> http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/422/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3/conditional_delete_test/history/ http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/421/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3/cas_and_list_index_test/history/ <text> a number of the upgrade_tests in dtest are flapping with data integrity bugs. for instance: this needs further digging to get good debug information; it could well be, e.g., a race condition in the test. i have not reproduced locally. /cc philip thompson",
        "label": 520
    },
    {
        "text": "introduce leaf only iterator <description> in many cases we have small btrees, small enough to fit in a single leaf page. in this case it may be more efficient to specialise our iterator.<stacktrace> <code> <text> in many cases we have small btrees, small enough to fit in a single leaf page. in this case it may be more efficient to specialise our iterator.",
        "label": 234
    },
    {
        "text": "cannot drop keyspace keyspace1 after running cassandra stress <description> steps to reproduce: 1. set max_heap=\"2g\", heap_newsize=\"400m\" 2. run ./cassandra-stress -n 50000 -c 400 -s 256 3. the test should complete despite several warnings about low heap memory. 4. try to drop keyspace: cqlsh> drop keyspace keyspace1; tsocket read 0 bytes system.log:  info 15:10:46,516 enqueuing flush of memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)  info 15:10:46,516 writing memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)  info 15:10:46,690 completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-6-data.db (38 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794574)  info 15:10:46,692 enqueuing flush of memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)  info 15:10:46,693 writing memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)  info 15:10:46,857 completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-ic-6-data.db (38 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794574)  info 15:10:46,897 enqueuing flush of memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)  info 15:10:46,898 writing memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)  info 15:10:47,064 completed flushing /var/lib/cassandra/data/system/local/system-local-ic-12-data.db (139 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794845)  info 15:10:48,956 enqueuing flush of memtable-local@432522279(46/46 serialized/live bytes, 1 ops)  info 15:10:48,957 writing memtable-local@432522279(46/46 serialized/live bytes, 1 ops)  info 15:10:49,132 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 400882073/1094043713)bytes  info 15:10:49,175 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 147514075/645675954)bytes  info 15:10:49,185 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 223249644/609072261)bytes  info 15:10:49,202 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 346471085/990388210)bytes  info 15:10:49,215 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 294748503/2092376617)bytes  info 15:10:49,257 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 692722235/739328646)bytes  info 15:10:49,285 completed flushing /var/lib/cassandra/data/system/local/system-local-ic-13-data.db (82 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794974)  info 15:10:49,286 compacting [sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-10-data.db'), sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-13-data.db'), sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-12-data.db'), sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-11-data.db')] error 15:10:49,287 error occurred during processing of message. java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:378) at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:281) at org.apache.cassandra.service.migrationmanager.announcekeyspacedrop(migrationmanager.java:262) at org.apache.cassandra.cql.queryprocessor.processstatement(queryprocessor.java:718) at org.apache.cassandra.cql.queryprocessor.process(queryprocessor.java:775) at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1668) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:4048) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:4036) at org.apache.thrift.processfunction.process(processfunction.java:32) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:199) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662) caused by: java.util.concurrent.executionexception: java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222) at java.util.concurrent.futuretask.get(futuretask.java:83) at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:374) ... 13 more caused by: java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at org.apache.cassandra.db.datatracker.removeoldsstablessize(datatracker.java:354) at org.apache.cassandra.db.datatracker.postreplace(datatracker.java:325) at org.apache.cassandra.db.datatracker.unreferencesstables(datatracker.java:264) at org.apache.cassandra.db.columnfamilystore.invalidate(columnfamilystore.java:302) at org.apache.cassandra.db.table.unloadcf(table.java:314) at org.apache.cassandra.db.table.dropcf(table.java:296) at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:607) at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:469) at org.apache.cassandra.db.defstable.mergeschema(defstable.java:355) at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:299) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) ... 3 more error 15:10:49,287 exception in thread thread[migrationstage:1,5,main] java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at org.apache.cassandra.db.datatracker.removeoldsstablessize(datatracker.java:354) at org.apache.cassandra.db.datatracker.postreplace(datatracker.java:325) at org.apache.cassandra.db.datatracker.unreferencesstables(datatracker.java:264) at org.apache.cassandra.db.columnfamilystore.invalidate(columnfamilystore.java:302) at org.apache.cassandra.db.table.unloadcf(table.java:314) at org.apache.cassandra.db.table.dropcf(table.java:296) at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:607) at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:469) at org.apache.cassandra.db.defstable.mergeschema(defstable.java:355) at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:299) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662)  info 15:10:49,471 compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ic-14,].  829 bytes to 501 (~60% of original) in 184ms = 0,002597mb/s.  4 total rows, 1 unique.  row merge counts were {1:0, 2:0, 3:0, 4:1, }<stacktrace>  info 15:10:46,516 enqueuing flush of memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)  info 15:10:46,516 writing memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)  info 15:10:46,690 completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-6-data.db (38 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794574)  info 15:10:46,692 enqueuing flush of memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)  info 15:10:46,693 writing memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)  info 15:10:46,857 completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-ic-6-data.db (38 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794574)  info 15:10:46,897 enqueuing flush of memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)  info 15:10:46,898 writing memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)  info 15:10:47,064 completed flushing /var/lib/cassandra/data/system/local/system-local-ic-12-data.db (139 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794845)  info 15:10:48,956 enqueuing flush of memtable-local@432522279(46/46 serialized/live bytes, 1 ops)  info 15:10:48,957 writing memtable-local@432522279(46/46 serialized/live bytes, 1 ops)  info 15:10:49,132 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 400882073/1094043713)bytes  info 15:10:49,175 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 147514075/645675954)bytes  info 15:10:49,185 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 223249644/609072261)bytes  info 15:10:49,202 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 346471085/990388210)bytes  info 15:10:49,215 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 294748503/2092376617)bytes  info 15:10:49,257 compaction interrupted: compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(keyspace1, standard1, 692722235/739328646)bytes  info 15:10:49,285 completed flushing /var/lib/cassandra/data/system/local/system-local-ic-13-data.db (82 bytes) for commitlog position replayposition(segmentid=1377867520699, position=19794974)  info 15:10:49,286 compacting [sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-10-data.db'), sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-13-data.db'), sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-12-data.db'), sstablereader(path='/var/lib/cassandra/data/system/local/system-local-ic-11-data.db')] error 15:10:49,287 error occurred during processing of message. java.lang.runtimeexception: java.util.concurrent.executionexception: java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:378) at org.apache.cassandra.service.migrationmanager.announce(migrationmanager.java:281) at org.apache.cassandra.service.migrationmanager.announcekeyspacedrop(migrationmanager.java:262) at org.apache.cassandra.cql.queryprocessor.processstatement(queryprocessor.java:718) at org.apache.cassandra.cql.queryprocessor.process(queryprocessor.java:775) at org.apache.cassandra.thrift.cassandraserver.execute_cql_query(cassandraserver.java:1668) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:4048) at org.apache.cassandra.thrift.cassandra$processor$execute_cql_query.getresult(cassandra.java:4036) at org.apache.thrift.processfunction.process(processfunction.java:32) at org.apache.thrift.tbaseprocessor.process(tbaseprocessor.java:34) at org.apache.cassandra.thrift.customtthreadpoolserver$workerprocess.run(customtthreadpoolserver.java:199) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662) caused by: java.util.concurrent.executionexception: java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at java.util.concurrent.futuretask$sync.innerget(futuretask.java:222) at java.util.concurrent.futuretask.get(futuretask.java:83) at org.apache.cassandra.utils.fbutilities.waitonfuture(fbutilities.java:374) ... 13 more caused by: java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at org.apache.cassandra.db.datatracker.removeoldsstablessize(datatracker.java:354) at org.apache.cassandra.db.datatracker.postreplace(datatracker.java:325) at org.apache.cassandra.db.datatracker.unreferencesstables(datatracker.java:264) at org.apache.cassandra.db.columnfamilystore.invalidate(columnfamilystore.java:302) at org.apache.cassandra.db.table.unloadcf(table.java:314) at org.apache.cassandra.db.table.dropcf(table.java:296) at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:607) at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:469) at org.apache.cassandra.db.defstable.mergeschema(defstable.java:355) at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:299) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) ... 3 more error 15:10:49,287 exception in thread thread[migrationstage:1,5,main] java.lang.assertionerror: sstablereader(path='/var/lib/cassandra/data/keyspace1/standard1/keyspace1-standard1-ic-78-data.db') was already marked compacted at org.apache.cassandra.db.datatracker.removeoldsstablessize(datatracker.java:354) at org.apache.cassandra.db.datatracker.postreplace(datatracker.java:325) at org.apache.cassandra.db.datatracker.unreferencesstables(datatracker.java:264) at org.apache.cassandra.db.columnfamilystore.invalidate(columnfamilystore.java:302) at org.apache.cassandra.db.table.unloadcf(table.java:314) at org.apache.cassandra.db.table.dropcf(table.java:296) at org.apache.cassandra.db.defstable.dropcolumnfamily(defstable.java:607) at org.apache.cassandra.db.defstable.mergecolumnfamilies(defstable.java:469) at org.apache.cassandra.db.defstable.mergeschema(defstable.java:355) at org.apache.cassandra.service.migrationmanager$2.runmaythrow(migrationmanager.java:299) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.util.concurrent.executors$runnableadapter.call(executors.java:439) at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303) at java.util.concurrent.futuretask.run(futuretask.java:138) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:895) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:918) at java.lang.thread.run(thread.java:662)  info 15:10:49,471 compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ic-14,].  829 bytes to 501 (~60% of original) in 184ms = 0,002597mb/s.  4 total rows, 1 unique.  row merge counts were {1:0, 2:0, 3:0, 4:1, } <code> <text> cqlsh> drop keyspace keyspace1; tsocket read 0 bytes steps to reproduce: system.log:",
        "label": 538
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh copy tests cqlshcopytest test bulk round trip non prepared statements <description> example failure: http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/447/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_bulk_round_trip_non_prepared_statements error message 100000 != 96848 -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-bryyns dtest: debug: done setting configuration options: {   'initial_token': none,     'memtable_allocation_type': 'offheap_objects',     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} dtest: debug: running stress without any user profile dtest: debug: generated 100000 records dtest: debug: exporting to csv file: /tmp/tmpreohbz dtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpreohbz' with pagetimeout = 10 and pagesize = 1000 dtest: debug: copy to took 0:00:04.598829 to export 100000 records dtest: debug: truncating keyspace1.standard1... dtest: debug: importing from csv file: /tmp/tmpreohbz dtest: debug: copy keyspace1.standard1 from '/tmp/tmpreohbz' with preparedstatements = false dtest: debug: copy from took 0:00:10.348123 to import 100000 records dtest: debug: exporting to csv file: /tmp/tmpexlptz dtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpexlptz' with pagetimeout = 10 and pagesize = 1000 dtest: debug: copy to took 0:00:11.681829 to export 100000 records --------------------- >> end captured logging << --------------------- stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 2482, in test_bulk_round_trip_non_prepared_statements     copy_from_options={'preparedstatements': false})   file \"/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py\", line 2461, in _test_bulk_round_trip     sum(1 for _ in open(tempfile2.name)))   file \"/usr/lib/python2.7/unittest/case.py\", line 513, in assertequal     assertion_func(first, second, msg=msg)   file \"/usr/lib/python2.7/unittest/case.py\", line 506, in _baseassertequal     raise self.failureexception(msg) \"100000 != 96848\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: /tmp/dtest-bryyns\\ndtest: debug: done setting configuration options:\\n{   'initial_token': none,\\n    'memtable_allocation_type': 'offheap_objects',\\n    'num_tokens': '32',\\n    'phi_convict_threshold': 5,\\n    'range_request_timeout_in_ms': 10000,\\n    'read_request_timeout_in_ms': 10000,\\n    'request_timeout_in_ms': 10000,\\n    'truncate_request_timeout_in_ms': 10000,\\n    'write_request_timeout_in_ms': 10000}\\ndtest: debug: running stress without any user profile\\ndtest: debug: generated 100000 records\\ndtest: debug: exporting to csv file: /tmp/tmpreohbz\\ndtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpreohbz' with pagetimeout = 10 and pagesize = 1000\\ndtest: debug: copy to took 0:00:04.598829 to export 100000 records\\ndtest: debug: truncating keyspace1.standard1...\\ndtest: debug: importing from csv file: /tmp/tmpreohbz\\ndtest: debug: copy keyspace1.standard1 from '/tmp/tmpreohbz' with preparedstatements = false\\ndtest: debug: copy from took 0:00:10.348123 to import 100000 records\\ndtest: debug: exporting to csv file: /tmp/tmpexlptz\\ndtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpexlptz' with pagetimeout = 10 and pagesize = 1000\\ndtest: debug: copy to took 0:00:11.681829 to export 100000 records\\n--------------------- >> end captured logging << ---------------------\"<stacktrace> <code> error message 100000 != 96848 -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: /tmp/dtest-bryyns dtest: debug: done setting configuration options: {   'initial_token': none,     'memtable_allocation_type': 'offheap_objects',     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} dtest: debug: running stress without any user profile dtest: debug: generated 100000 records dtest: debug: exporting to csv file: /tmp/tmpreohbz dtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpreohbz' with pagetimeout = 10 and pagesize = 1000 dtest: debug: copy to took 0:00:04.598829 to export 100000 records dtest: debug: truncating keyspace1.standard1... dtest: debug: importing from csv file: /tmp/tmpreohbz dtest: debug: copy keyspace1.standard1 from '/tmp/tmpreohbz' with preparedstatements = false dtest: debug: copy from took 0:00:10.348123 to import 100000 records dtest: debug: exporting to csv file: /tmp/tmpexlptz dtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpexlptz' with pagetimeout = 10 and pagesize = 1000 dtest: debug: copy to took 0:00:11.681829 to export 100000 records --------------------- >> end captured logging << --------------------- stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 2482, in test_bulk_round_trip_non_prepared_statements     copy_from_options={'preparedstatements': false})   file '/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py', line 2461, in _test_bulk_round_trip     sum(1 for _ in open(tempfile2.name)))   file '/usr/lib/python2.7/unittest/case.py', line 513, in assertequal     assertion_func(first, second, msg=msg)   file '/usr/lib/python2.7/unittest/case.py', line 506, in _baseassertequal     raise self.failureexception(msg) '100000 != 96848/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: /tmp/dtest-bryyns/ndtest: debug: done setting configuration options:/n{   'initial_token': none,/n    'memtable_allocation_type': 'offheap_objects',/n    'num_tokens': '32',/n    'phi_convict_threshold': 5,/n    'range_request_timeout_in_ms': 10000,/n    'read_request_timeout_in_ms': 10000,/n    'request_timeout_in_ms': 10000,/n    'truncate_request_timeout_in_ms': 10000,/n    'write_request_timeout_in_ms': 10000}/ndtest: debug: running stress without any user profile/ndtest: debug: generated 100000 records/ndtest: debug: exporting to csv file: /tmp/tmpreohbz/ndtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpreohbz' with pagetimeout = 10 and pagesize = 1000/ndtest: debug: copy to took 0:00:04.598829 to export 100000 records/ndtest: debug: truncating keyspace1.standard1.../ndtest: debug: importing from csv file: /tmp/tmpreohbz/ndtest: debug: copy keyspace1.standard1 from '/tmp/tmpreohbz' with preparedstatements = false/ndtest: debug: copy from took 0:00:10.348123 to import 100000 records/ndtest: debug: exporting to csv file: /tmp/tmpexlptz/ndtest: debug: consistency all; copy keyspace1.standard1 to '/tmp/tmpexlptz' with pagetimeout = 10 and pagesize = 1000/ndtest: debug: copy to took 0:00:11.681829 to export 100000 records/n--------------------- >> end captured logging << ---------------------' http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/447/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_bulk_round_trip_non_prepared_statements<text> example failure: ",
        "label": 508
    },
    {
        "text": "sstableloader broken in <description> i don't see this happen on 1.2.6. to reproduce (on a fresh single node cluster): [nicks-macbook-pro:11:33:06 (cassandra-1.2.7)*] cassandra$ bin/cqlsh connected to test cluster at localhost:9160. [cqlsh 3.1.4 | cassandra 1.2.7-snapshot | cql spec 3.0.0 | thrift protocol 19.36.0] cqlsh> create keyspace test_backup_restore with replication = {'class': 'simplestrategy', 'replication_factor': 1}; cqlsh> use test_backup_restore; cqlsh:test_backup_restore> create table cf0 (                        ...   a text primary key,                        ...   b text,                        ...   c text                        ... ); cqlsh:test_backup_restore> insert into cf0 (a, b, c) values ( 'a', 'b', 'c'); cqlsh:test_backup_restore> select * from cf0;  a | b | c ---+---+---  a | b | c cqlsh:test_backup_restore> ^d [nicks-macbook-pro:11:34:22 (cassandra-1.2.7)*] cassandra$ bin/nodetool snapshot requested creating snapshot for: all keyspaces snapshot directory: 1375115668449 [nicks-macbook-pro:11:34:40 (cassandra-1.2.7)*] cassandra$ mkdir -p test_backup_restore/snapshots [nicks-macbook-pro:11:34:48 (cassandra-1.2.7)*] cassandra$ cp /var/lib/cassandra/data/test_backup_restore/cf0/snapshots/1375115668449/* test_backup_restore/snapshots/ [nicks-macbook-pro:11:35:14 (cassandra-1.2.7)*] cassandra$ bin/sstableloader --debug -v -d 127.0.0.1 test_backup_restore/snapshots streaming revelant part of test_backup_restore/snapshots/test_backup_restore-cf0-ic-1-data.db  to [/127.0.0.1] org.apache.cassandra.io.util.compressedsegmentedfile cannot be cast to org.apache.cassandra.io.util.compressedpoolingsegmentedfile java.lang.classcastexception: org.apache.cassandra.io.util.compressedsegmentedfile cannot be cast to org.apache.cassandra.io.util.compressedpoolingsegmentedfile at org.apache.cassandra.io.sstable.sstablereader.getcompressionmetadata(sstablereader.java:574) at org.apache.cassandra.streaming.streamout.creatependingfiles(streamout.java:179) at org.apache.cassandra.streaming.streamout.transfersstables(streamout.java:154) at org.apache.cassandra.io.sstable.sstableloader.stream(sstableloader.java:145) at org.apache.cassandra.tools.bulkloader.main(bulkloader.java:67)<stacktrace> [nicks-macbook-pro:11:33:06 (cassandra-1.2.7)*] cassandra$ bin/cqlsh connected to test cluster at localhost:9160. [cqlsh 3.1.4 | cassandra 1.2.7-snapshot | cql spec 3.0.0 | thrift protocol 19.36.0] cqlsh> create keyspace test_backup_restore with replication = {'class': 'simplestrategy', 'replication_factor': 1}; cqlsh> use test_backup_restore; cqlsh:test_backup_restore> create table cf0 (                        ...   a text primary key,                        ...   b text,                        ...   c text                        ... ); cqlsh:test_backup_restore> insert into cf0 (a, b, c) values ( 'a', 'b', 'c'); cqlsh:test_backup_restore> select * from cf0;  a | b | c ---+---+---  a | b | c cqlsh:test_backup_restore> ^d [nicks-macbook-pro:11:34:22 (cassandra-1.2.7)*] cassandra$ bin/nodetool snapshot requested creating snapshot for: all keyspaces snapshot directory: 1375115668449 [nicks-macbook-pro:11:34:40 (cassandra-1.2.7)*] cassandra$ mkdir -p test_backup_restore/snapshots [nicks-macbook-pro:11:34:48 (cassandra-1.2.7)*] cassandra$ cp /var/lib/cassandra/data/test_backup_restore/cf0/snapshots/1375115668449/* test_backup_restore/snapshots/ [nicks-macbook-pro:11:35:14 (cassandra-1.2.7)*] cassandra$ bin/sstableloader --debug -v -d 127.0.0.1 test_backup_restore/snapshots streaming revelant part of test_backup_restore/snapshots/test_backup_restore-cf0-ic-1-data.db  to [/127.0.0.1] org.apache.cassandra.io.util.compressedsegmentedfile cannot be cast to org.apache.cassandra.io.util.compressedpoolingsegmentedfile java.lang.classcastexception: org.apache.cassandra.io.util.compressedsegmentedfile cannot be cast to org.apache.cassandra.io.util.compressedpoolingsegmentedfile at org.apache.cassandra.io.sstable.sstablereader.getcompressionmetadata(sstablereader.java:574) at org.apache.cassandra.streaming.streamout.creatependingfiles(streamout.java:179) at org.apache.cassandra.streaming.streamout.transfersstables(streamout.java:154) at org.apache.cassandra.io.sstable.sstableloader.stream(sstableloader.java:145) at org.apache.cassandra.tools.bulkloader.main(bulkloader.java:67) <code> <text> i don't see this happen on 1.2.6. to reproduce (on a fresh single node cluster):",
        "label": 538
    },
    {
        "text": "cassandraversion complains about x version strings <description> the utest systemkeyspacetest.snapshotsystemkeyspaceifupgrading fails with java.lang.illegalargumentexception: invalid version value: 3.2 (e.g. here). the question is just: 1. change the regex pattern in cassandraversion and silently assume .0 as the patch version or 2. go with x.y.0 version strings instead of x.y<stacktrace> <code> the utest systemkeyspacetest.snapshotsystemkeyspaceifupgrading fails with java.lang.illegalargumentexception: invalid version value: 3.2 (e.g. here). <text> the question is just:",
        "label": 577
    },
    {
        "text": "coalescing strategy sleeps too much <description> with the current code maybesleep is called even if we managed to take maxitems out of the backlog. in this case we should really avoid sleeping because it means that backlog is building up. i'll send a patch shortly.<stacktrace> <code> <text> with the current code maybesleep is called even if we managed to take maxitems out of the backlog. in this case we should really avoid sleeping because it means that backlog is building up. i'll send a patch shortly.",
        "label": 120
    },
    {
        "text": "mx4j does not load <description> adding mx4j-tools.jar (latest) to the library causes following exception:  warn 20:22:25,123 could not start register mbean in jmx java.lang.reflect.invocationtargetexception at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.cassandra.utils.mx4jtool.maybeload(mx4jtool.java:67) at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:169) at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:55) at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:216) at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:134) caused by: java.net.bindexception: address already in use at java.net.plainsocketimpl.socketbind(native method) at java.net.plainsocketimpl.bind(plainsocketimpl.java:365) at java.net.serversocket.bind(serversocket.java:319) at java.net.serversocket.<init>(serversocket.java:185) at mx4j.tools.adaptor.plainadaptorserversocketfactory.createserversocket(plainadaptorserversocketfactory.java:24) at mx4j.tools.adaptor.http.httpadaptor.createserversocket(httpadaptor.java:672) at mx4j.tools.adaptor.http.httpadaptor.start(httpadaptor.java:478) ... 9 more<stacktrace>  warn 20:22:25,123 could not start register mbean in jmx java.lang.reflect.invocationtargetexception at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25) at java.lang.reflect.method.invoke(method.java:597) at org.apache.cassandra.utils.mx4jtool.maybeload(mx4jtool.java:67) at org.apache.cassandra.service.abstractcassandradaemon.setup(abstractcassandradaemon.java:169) at org.apache.cassandra.thrift.cassandradaemon.setup(cassandradaemon.java:55) at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:216) at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:134) caused by: java.net.bindexception: address already in use at java.net.plainsocketimpl.socketbind(native method) at java.net.plainsocketimpl.bind(plainsocketimpl.java:365) at java.net.serversocket.bind(serversocket.java:319) at java.net.serversocket.<init>(serversocket.java:185) at mx4j.tools.adaptor.plainadaptorserversocketfactory.createserversocket(plainadaptorserversocketfactory.java:24) at mx4j.tools.adaptor.http.httpadaptor.createserversocket(httpadaptor.java:672) at mx4j.tools.adaptor.http.httpadaptor.start(httpadaptor.java:478) ... 9 more <code> <text> adding mx4j-tools.jar (latest) to the library causes following exception:",
        "label": 440
    },
    {
        "text": "usertype  tupletype and collections in udfs <description> is java driver as a dependency required ? is it possible to extract parts of the java driver for udt/tt/coll support ? cql drop type must check udfs must check keyspace access permissions (if those exist)<stacktrace> <code> <text> ",
        "label": 453
    },
    {
        "text": "random port   hardcoded <description> the endpoint class contains a \"random port\" currently hard coded to 5555. the cassandra process will attempt to bind on this port, another process already using 5555 will interfere with this. the port should be configurable.<stacktrace> <code> <text> the endpoint class contains a 'random port' currently hard coded to 5555. the cassandra process will attempt to bind on this port, another process already using 5555 will interfere with this. the port should be configurable.",
        "label": 274
    },
    {
        "text": "read repair on cl one regression <description> read repair w/ cl.one had a regression. the repaircallback was dropped (in the background for cl.one), so readresponseresolver : resolve() was never called.<stacktrace> <code> <text> read repair w/ cl.one had a regression. the repaircallback was dropped (in the background for cl.one), so readresponseresolver : resolve() was never called.",
        "label": 274
    },
    {
        "text": "cassandra broke running cassandra and tests in intellij under java <description> cassandra-9608 added a couple hard-coded options to workspace.xml that are not supported in java 8: https://github.com/apache/cassandra/commit/6ba2fb9395226491872b41312d978a169f36fcdb#diff-59e65c5abf01f83a11989765ada76841. unrecognized option: --add-exports error: could not create the java virtual machine. error: a fatal exception has occurred. program will exit. to reproduce:  1. update to the most recent trunk  2. rm -rf .idea && ant generate-idea-files  3. re-open the project in intellij (using java 8) and run cassandra or a test.<stacktrace> <code> unrecognized option: --add-exports error: could not create the java virtual machine. error: a fatal exception has occurred. program will exit. <text> cassandra-9608 added a couple hard-coded options to workspace.xml that are not supported in java 8: https://github.com/apache/cassandra/commit/6ba2fb9395226491872b41312d978a169f36fcdb#diff-59e65c5abf01f83a11989765ada76841. to reproduce:  1. update to the most recent trunk  2. rm -rf .idea && ant generate-idea-files  3. re-open the project in intellij (using java 8) and run cassandra or a test.",
        "label": 278
    },
    {
        "text": "timestamptype doesn't support pre epoch long <description> org.apache.cassandra.db.marshal.timestamptype.datestringtotimestamp() contains a regular expression that checks to see if the string argument contains a number. if so it parses it as a long timestamp. however pre-epoch timestamps are negative and the code doesn't account for this so it tries to parse it as a formatted date. a tweak to the regular expression in timestamptype.datestringtotimestamp() would solve this issue. i could use formatted date strings instead, but the timestamptype date parser uses iso8601 patterns which would cause the timestamp to be rounded to the nearest second. currently i get the following exception message: unable to coerce '-86400000' to a formatted date (long)<stacktrace> <code> unable to coerce '-86400000' to a formatted date (long)<text> org.apache.cassandra.db.marshal.timestamptype.datestringtotimestamp() contains a regular expression that checks to see if the string argument contains a number. if so it parses it as a long timestamp. however pre-epoch timestamps are negative and the code doesn't account for this so it tries to parse it as a formatted date. a tweak to the regular expression in timestamptype.datestringtotimestamp() would solve this issue. i could use formatted date strings instead, but the timestamptype date parser uses iso8601 patterns which would cause the timestamp to be rounded to the nearest second. currently i get the following exception message: ",
        "label": 362
    },
    {
        "text": "abort bootstrap if our ip is already in the token ring <description> trying to bootstrap a node w/ the same ip as one that is down but not removed from the ring should give an error.<stacktrace> <code> <text> trying to bootstrap a node w/ the same ip as one that is down but not removed from the ring should give an error.",
        "label": 274
    },
    {
        "text": "expose global columnfamily metrics <description> it would be very useful to have cassandra expose columnfamily metrics that span all column families. a general purpose cassandra monitoring system built up around the current columnfamily metrics really only has a couple of choices right now: publish metrics for all column families or fetch metrics for all column families, aggregate them and then publish the aggregated metrics. the first can be quite expensive for the downstream monitoring system and the second is a piece of work that it seems is better pushed into cassandra itself. perhaps these global columnfamily metrics could be published under a name of: org.apache.cassandra.metrics:type=(columnfamily|indexcolumnfamily),name=(metric name) (same as existing columnfamily metrics, but with no keyspace or scope.)<stacktrace> <code> org.apache.cassandra.metrics:type=(columnfamily|indexcolumnfamily),name=(metric name) <text> it would be very useful to have cassandra expose columnfamily metrics that span all column families. a general purpose cassandra monitoring system built up around the current columnfamily metrics really only has a couple of choices right now: publish metrics for all column families or fetch metrics for all column families, aggregate them and then publish the aggregated metrics. the first can be quite expensive for the downstream monitoring system and the second is a piece of work that it seems is better pushed into cassandra itself. perhaps these global columnfamily metrics could be published under a name of: (same as existing columnfamily metrics, but with no keyspace or scope.)",
        "label": 106
    },
    {
        "text": "improve startup time by making row cache population multi threaded <description> attached patch multi threads the row cache population my small tests show improvements atleast; - 4 cores                                                                                                                                                                                               - 11g stress-generated data - page cache dropped between runs                                                                                                                                                                                                                                                                                                                                                                                     single platter spinning disk:                                                                                                                                                    single threaded:                                                                                                                                                                                         info 10:18:21,365 completed loading (245562 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                          info 10:32:43,738 completed loading (255106 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                         multi threaded:                                                                                                                                                                                          info 10:22:47,567 completed loading (213905 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                          info 10:27:26,873 completed loading (214514 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                                                                                                                                                                                                                                 ssd;                                                                                                                                                                                                    single threaded:                                                                                                                                                                                         info 10:40:49,079 completed loading (103883 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                          info 10:43:45,799 completed loading (106913 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                         multi threaded:                                                                                                                                                                                          info 10:38:20,798 completed loading (57617 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                           info 10:47:20,339 completed loading (56682 ms; 311381 keys) row cache for keyspace1.standard1<stacktrace> <code> - 4 cores                                                                                                                                                                                               - 11g stress-generated data - page cache dropped between runs                                                                                                                                                                                                                                                                                                                                                                                     single platter spinning disk:                                                                                                                                                    single threaded:                                                                                                                                                                                         info 10:18:21,365 completed loading (245562 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                          info 10:32:43,738 completed loading (255106 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                         multi threaded:                                                                                                                                                                                          info 10:22:47,567 completed loading (213905 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                          info 10:27:26,873 completed loading (214514 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                                                                                                                                                                                                                                 ssd;                                                                                                                                                                                                    single threaded:                                                                                                                                                                                         info 10:40:49,079 completed loading (103883 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                          info 10:43:45,799 completed loading (106913 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                         multi threaded:                                                                                                                                                                                          info 10:38:20,798 completed loading (57617 ms; 311381 keys) row cache for keyspace1.standard1                                                                                                           info 10:47:20,339 completed loading (56682 ms; 311381 keys) row cache for keyspace1.standard1 <text> attached patch multi threads the row cache population my small tests show improvements atleast;",
        "label": 274
    },
    {
        "text": "gccompactiontest is flaky <description> gccompactiontest was introduced by cassandra-7019 and appears to be flaky, see for example here. i think it's the same root cause as cassandra-12282: the tables in the test keyspace are dropped asynchronously after each test, and this might cause additional flush operations for all dirty tables in the keyspace. see the callstack in 12282. a possible solution is to use keyspace_per_test, which is instead dropped synchronously.<stacktrace> <code> <text> gccompactiontest was introduced by cassandra-7019 and appears to be flaky, see for example here. i think it's the same root cause as cassandra-12282: the tables in the test keyspace are dropped asynchronously after each test, and this might cause additional flush operations for all dirty tables in the keyspace. see the callstack in 12282. a possible solution is to use keyspace_per_test, which is instead dropped synchronously.",
        "label": 86
    },
    {
        "text": "minor cleanup   unused code removal <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "hsha broken at startup <description> error 09:10:21,781 exception encountered during startup java.lang.illegalargumentexception         at java.util.concurrent.threadpoolexecutor.<init>(threadpoolexecutor.java:589)         at java.util.concurrent.threadpoolexecutor.<init>(threadpoolexecutor.java:514)         at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.<init>(debuggablethreadpoolexecutor.java:90)         at org.apache.cassandra.concurrent.jmxenabledthreadpoolexecutor.<init>(jmxenabledthreadpoolexecutor.java:76)         at org.apache.cassandra.thrift.cassandradaemon$thriftserver.<init>(cassandradaemon.java:192)         at org.apache.cassandra.thrift.cassandradaemon.startserver(cassandradaemon.java:75)         at org.apache.cassandra.service.abstractcassandradaemon.startrpcserver(abstractcassandradaemon.java:281)         at org.apache.cassandra.service.abstractcassandradaemon.start(abstractcassandradaemon.java:253)         at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:350)         at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106)<stacktrace> error 09:10:21,781 exception encountered during startup java.lang.illegalargumentexception         at java.util.concurrent.threadpoolexecutor.<init>(threadpoolexecutor.java:589)         at java.util.concurrent.threadpoolexecutor.<init>(threadpoolexecutor.java:514)         at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor.<init>(debuggablethreadpoolexecutor.java:90)         at org.apache.cassandra.concurrent.jmxenabledthreadpoolexecutor.<init>(jmxenabledthreadpoolexecutor.java:76)         at org.apache.cassandra.thrift.cassandradaemon$thriftserver.<init>(cassandradaemon.java:192)         at org.apache.cassandra.thrift.cassandradaemon.startserver(cassandradaemon.java:75)         at org.apache.cassandra.service.abstractcassandradaemon.startrpcserver(abstractcassandradaemon.java:281)         at org.apache.cassandra.service.abstractcassandradaemon.start(abstractcassandradaemon.java:253)         at org.apache.cassandra.service.abstractcassandradaemon.activate(abstractcassandradaemon.java:350)         at org.apache.cassandra.thrift.cassandradaemon.main(cassandradaemon.java:106)<code> <text> ",
        "label": 85
    },
    {
        "text": "batch statements with multiple updates to partition error when table is indexed <description> if a batch statement contains multiple update statements that update the same partition, and a secondary index exists on that table, the batch statement will error: servererror: <errormessage code=0000 [server error] message=\"java.lang.illegalstateexception: an update should not be written again once it has been read\"> with the following traceback in the logs: error 20:53:46 unexpected exception during request java.lang.illegalstateexception: an update should not be written again once it has been read at org.apache.cassandra.db.partitions.partitionupdate.assertnotbuilt(partitionupdate.java:504) ~[main/:na] at org.apache.cassandra.db.partitions.partitionupdate.add(partitionupdate.java:535) ~[main/:na] at org.apache.cassandra.cql3.statements.updatestatement.addupdateforkey(updatestatement.java:96) ~[main/:na] at org.apache.cassandra.cql3.statements.modificationstatement.addupdates(modificationstatement.java:667) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.getmutations(batchstatement.java:234) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:335) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:321) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:316) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:205) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:471) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:448) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:130) ~[main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45] at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] this is due to secondaryindexmanager.validate() triggering a build of the partitionupdate (stacktrace from debugging the build() call): at org.apache.cassandra.db.partitions.partitionupdate.build(partitionupdate.java:571) [main/:na] at org.apache.cassandra.db.partitions.partitionupdate.maybebuild(partitionupdate.java:561) [main/:na] at org.apache.cassandra.db.partitions.partitionupdate.iterator(partitionupdate.java:418) [main/:na] at org.apache.cassandra.index.internal.cassandraindex.validaterows(cassandraindex.java:560) [main/:na] at org.apache.cassandra.index.internal.cassandraindex.validate(cassandraindex.java:314) [main/:na] at org.apache.cassandra.index.secondaryindexmanager.lambda$validate$75(secondaryindexmanager.java:642) [main/:na] at org.apache.cassandra.index.secondaryindexmanager$$lambda$166/1388080038.accept(unknown source) [main/:na] at java.util.stream.foreachops$foreachop$ofref.accept(foreachops.java:184) [na:1.8.0_45] at java.util.stream.referencepipeline$2$1.accept(referencepipeline.java:175) [na:1.8.0_45] at java.util.concurrent.concurrenthashmap$valuespliterator.foreachremaining(concurrenthashmap.java:3566) [na:1.8.0_45] at java.util.stream.abstractpipeline.copyinto(abstractpipeline.java:512) [na:1.8.0_45] at java.util.stream.abstractpipeline.wrapandcopyinto(abstractpipeline.java:502) [na:1.8.0_45] at java.util.stream.foreachops$foreachop.evaluatesequential(foreachops.java:151) [na:1.8.0_45] at java.util.stream.foreachops$foreachop$ofref.evaluatesequential(foreachops.java:174) [na:1.8.0_45] at java.util.stream.abstractpipeline.evaluate(abstractpipeline.java:234) [na:1.8.0_45] at java.util.stream.referencepipeline.foreach(referencepipeline.java:418) [na:1.8.0_45] at org.apache.cassandra.index.secondaryindexmanager.validate(secondaryindexmanager.java:642) [main/:na] at org.apache.cassandra.cql3.updateparameters.validateindexedcolumns(updateparameters.java:105) [main/:na] at org.apache.cassandra.cql3.statements.updatestatement.addupdateforkey(updatestatement.java:107) [main/:na] at org.apache.cassandra.cql3.statements.modificationstatement.addupdates(modificationstatement.java:667) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.getmutations(batchstatement.java:234) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:335) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:321) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:316) [main/:na] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:205) [main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:471) [main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:448) [main/:na] at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:130) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45] at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] you can reproduce the issue with a table and index like this: create table foo (a int, b int, c int, d int, primary key (a, b)); create index on foo(c); and a batch update like this: begin batch update foo set c = 0 where a = 0 and b = 0; update foo set d = 0 where a = 0 and b = 0; apply batch assigning sylvain lebresne for now, since he probably has the best idea of how to avoid this.<stacktrace> error 20:53:46 unexpected exception during request java.lang.illegalstateexception: an update should not be written again once it has been read at org.apache.cassandra.db.partitions.partitionupdate.assertnotbuilt(partitionupdate.java:504) ~[main/:na] at org.apache.cassandra.db.partitions.partitionupdate.add(partitionupdate.java:535) ~[main/:na] at org.apache.cassandra.cql3.statements.updatestatement.addupdateforkey(updatestatement.java:96) ~[main/:na] at org.apache.cassandra.cql3.statements.modificationstatement.addupdates(modificationstatement.java:667) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.getmutations(batchstatement.java:234) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:335) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:321) ~[main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:316) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:205) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:471) ~[main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:448) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:130) ~[main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45] at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] at org.apache.cassandra.db.partitions.partitionupdate.build(partitionupdate.java:571) [main/:na] at org.apache.cassandra.db.partitions.partitionupdate.maybebuild(partitionupdate.java:561) [main/:na] at org.apache.cassandra.db.partitions.partitionupdate.iterator(partitionupdate.java:418) [main/:na] at org.apache.cassandra.index.internal.cassandraindex.validaterows(cassandraindex.java:560) [main/:na] at org.apache.cassandra.index.internal.cassandraindex.validate(cassandraindex.java:314) [main/:na] at org.apache.cassandra.index.secondaryindexmanager.lambda$validate$75(secondaryindexmanager.java:642) [main/:na] at org.apache.cassandra.index.secondaryindexmanager$$lambda$166/1388080038.accept(unknown source) [main/:na] at java.util.stream.foreachops$foreachop$ofref.accept(foreachops.java:184) [na:1.8.0_45] at java.util.stream.referencepipeline$2$1.accept(referencepipeline.java:175) [na:1.8.0_45] at java.util.concurrent.concurrenthashmap$valuespliterator.foreachremaining(concurrenthashmap.java:3566) [na:1.8.0_45] at java.util.stream.abstractpipeline.copyinto(abstractpipeline.java:512) [na:1.8.0_45] at java.util.stream.abstractpipeline.wrapandcopyinto(abstractpipeline.java:502) [na:1.8.0_45] at java.util.stream.foreachops$foreachop.evaluatesequential(foreachops.java:151) [na:1.8.0_45] at java.util.stream.foreachops$foreachop$ofref.evaluatesequential(foreachops.java:174) [na:1.8.0_45] at java.util.stream.abstractpipeline.evaluate(abstractpipeline.java:234) [na:1.8.0_45] at java.util.stream.referencepipeline.foreach(referencepipeline.java:418) [na:1.8.0_45] at org.apache.cassandra.index.secondaryindexmanager.validate(secondaryindexmanager.java:642) [main/:na] at org.apache.cassandra.cql3.updateparameters.validateindexedcolumns(updateparameters.java:105) [main/:na] at org.apache.cassandra.cql3.statements.updatestatement.addupdateforkey(updatestatement.java:107) [main/:na] at org.apache.cassandra.cql3.statements.modificationstatement.addupdates(modificationstatement.java:667) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.getmutations(batchstatement.java:234) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:335) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:321) [main/:na] at org.apache.cassandra.cql3.statements.batchstatement.execute(batchstatement.java:316) [main/:na] at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:205) [main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:471) [main/:na] at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:448) [main/:na] at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:130) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:507) [main/:na] at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:401) [main/:na] at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final] at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final] at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45] at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na] at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] <code> servererror: <errormessage code=0000 [server error] message='java.lang.illegalstateexception: an update should not be written again once it has been read'> create table foo (a int, b int, c int, d int, primary key (a, b)); create index on foo(c); begin batch update foo set c = 0 where a = 0 and b = 0; update foo set d = 0 where a = 0 and b = 0; apply batch <text> if a batch statement contains multiple update statements that update the same partition, and a secondary index exists on that table, the batch statement will error: with the following traceback in the logs: this is due to secondaryindexmanager.validate() triggering a build of the partitionupdate (stacktrace from debugging the build() call): you can reproduce the issue with a table and index like this: and a batch update like this: assigning sylvain lebresne for now, since he probably has the best idea of how to avoid this.",
        "label": 474
    },
    {
        "text": "tab completion in cqlsh doesn't work for capitalized letters <description> tab completion in cqlsh doesn't work for capitalized letters, either in keyspace names or table names. typing quotes and a corresponding capital letter should complete the table/keyspace name and the closing quote. cqlsh> create keyspace \"test\" with replication = {'class': 'simplestrategy', 'replication_factor': 1}; cqlsh> use \"tes cqlsh> use tes cqlsh> use test; invalidrequest: code=2200 [invalid query] message=\"keyspace 'test' does not exist\" cqlsh> use \"test\"; cqlsh:test> drop keyspace \"test\" cqlsh:test> create table \"testtable\" (a text primary key, b text); cqlsh:test> select * from \"testtable\";  a | b ---+--- (0 rows) cqlsh:test> select * from \"test<stacktrace> <code> cqlsh> create keyspace 'test' with replication = {'class': 'simplestrategy', 'replication_factor': 1}; cqlsh> use 'tes cqlsh> use tes cqlsh> use test; invalidrequest: code=2200 [invalid query] message='keyspace 'test' does not exist' cqlsh> use 'test'; cqlsh:test> drop keyspace 'test' cqlsh:test> create table 'testtable' (a text primary key, b text); cqlsh:test> select * from 'testtable';  a | b ---+--- (0 rows) cqlsh:test> select * from 'test <text> tab completion in cqlsh doesn't work for capitalized letters, either in keyspace names or table names. typing quotes and a corresponding capital letter should complete the table/keyspace name and the closing quote.",
        "label": 316
    },
    {
        "text": "testall failure in org apache cassandra db compaction compactionscqltest testtriggerminorcompactiondtcs compression <description> example failure:  http://cassci.datastax.com/job/trunk_testall/1243/testreport/org.apache.cassandra.db.compaction/compactionscqltest/testtriggerminorcompactiondtcs_compression/ error message no minor compaction triggered in 5000ms stacktrace junit.framework.assertionfailederror: no minor compaction triggered in 5000ms at org.apache.cassandra.db.compaction.compactionscqltest.waitforminor(compactionscqltest.java:247) at org.apache.cassandra.db.compaction.compactionscqltest.testtriggerminorcompactiondtcs(compactionscqltest.java:72) related failure:  http://cassci.datastax.com/job/cassandra-3.x_testall/47/testreport/org.apache.cassandra.db.compaction/compactionscqltest/testtriggerminorcompactiondtcs/<stacktrace> stacktrace junit.framework.assertionfailederror: no minor compaction triggered in 5000ms at org.apache.cassandra.db.compaction.compactionscqltest.waitforminor(compactionscqltest.java:247) at org.apache.cassandra.db.compaction.compactionscqltest.testtriggerminorcompactiondtcs(compactionscqltest.java:72) <code> error message no minor compaction triggered in 5000ms example failure:  http://cassci.datastax.com/job/trunk_testall/1243/testreport/org.apache.cassandra.db.compaction/compactionscqltest/testtriggerminorcompactiondtcs_compression/ related failure:  http://cassci.datastax.com/job/cassandra-3.x_testall/47/testreport/org.apache.cassandra.db.compaction/compactionscqltest/testtriggerminorcompactiondtcs/<text> ",
        "label": 321
    },
    {
        "text": "fix smallish problems find by findbugs <description> i've just run (the newly released) findbugs 2 out of curiosity. attaching a number of patches related to issue raised by it. there is nothing major at all so all patches are against trunk. i've tried keep each issue to it's own patch with a self describing title. it far from covers all findbugs alerts, but it's a picky tool so i've tried to address only what felt at least vaguely useful. those are still mostly nits (only patch 2 is probably an actual bug).<stacktrace> <code> <text> i've just run (the newly released) findbugs 2 out of curiosity. attaching a number of patches related to issue raised by it. there is nothing major at all so all patches are against trunk. i've tried keep each issue to it's own patch with a self describing title. it far from covers all findbugs alerts, but it's a picky tool so i've tried to address only what felt at least vaguely useful. those are still mostly nits (only patch 2 is probably an actual bug).",
        "label": 520
    },
    {
        "text": "possible early deletion of commit logs <description> i ran my cluster for about 2 days. the cluster has 2 nodes. i restarted one box several times, and the other one was always running. the one always running ended up accumulating 100gb of commit logs. this is 1.0.0 code from about sept 15 in github. i kept the original setting for   #commitlog_total_space_in_mb: 4096  i.e. commented out here is some sample of the output: rw-rr- 1 yyang yyang 134217857 2011-09-28 03:51 commitlog-1317181834810.log  rw-rr- 1 yyang yyang 134217869 2011-09-28 03:50 commitlog-1317181764105.log  rw-rr- 1 yyang yyang 134217783 2011-09-28 03:49 commitlog-1317181694633.log  rw-rr- 1 yyang yyang 134217750 2011-09-28 02:39 commitlog-1317176955102.log  yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib//cassandra/commitlog/|wc -l  727  yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ du -s /mnt/cass/lib/cassandra/commitlog/   95095316 /mnt/cass/lib/cassandra/commitlog/<stacktrace> <code> rw-rr- 1 yyang yyang 134217857 2011-09-28 03:51 commitlog-1317181834810.log  rw-rr- 1 yyang yyang 134217869 2011-09-28 03:50 commitlog-1317181764105.log  rw-rr- 1 yyang yyang 134217783 2011-09-28 03:49 commitlog-1317181694633.log  rw-rr- 1 yyang yyang 134217750 2011-09-28 02:39 commitlog-1317176955102.log  yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib//cassandra/commitlog/|wc -l  727  yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ du -s /mnt/cass/lib/cassandra/commitlog/   95095316 /mnt/cass/lib/cassandra/commitlog/<text> i ran my cluster for about 2 days. the cluster has 2 nodes. i restarted one box several times, and the other one was always running. the one always running ended up accumulating 100gb of commit logs. this is 1.0.0 code from about sept 15 in github. i kept the original setting for   #commitlog_total_space_in_mb: 4096  i.e. commented out here is some sample of the output: ",
        "label": 520
    },
    {
        "text": "materialized view definition regression in clustering key <description> this bug was reported on the users mailing list. the following definitions work in 3.0.3 but fail in 3.0.7. create table ks.pa (     id bigint,     sub_id text,     name text,     class text,     r_id bigint,     k_id bigint,     created timestamp,     priority int,     updated timestamp,     value text,     primary key (id, sub_id, name) ); create ks.mv_pa as     select k_id, name, value, sub_id, id, class, r_id     from ks.pa     where k_id is not null and name is not null and value is not null and sub_id is not null and id is not null     primary key ((k_id, name), value, sub_id, id); after running bisect, i've narrowed it down to commit 86ba227 from cassandra-11475.<stacktrace> <code> create table ks.pa (     id bigint,     sub_id text,     name text,     class text,     r_id bigint,     k_id bigint,     created timestamp,     priority int,     updated timestamp,     value text,     primary key (id, sub_id, name) ); create ks.mv_pa as     select k_id, name, value, sub_id, id, class, r_id     from ks.pa     where k_id is not null and name is not null and value is not null and sub_id is not null and id is not null     primary key ((k_id, name), value, sub_id, id); <text> this bug was reported on the users mailing list. the following definitions work in 3.0.3 but fail in 3.0.7. after running bisect, i've narrowed it down to commit 86ba227 from cassandra-11475.",
        "label": 98
    },
    {
        "text": "ancestors are not cleared in sstablemetadata after compactions are done and old sstables are removed <description> we are using lcs and have total of 38000 sstables for one cf. during lcs, there could be over a thousand sstable involved. all those sstable ids are stored in ancestors field of sstablemetatdata for the new table. in our case, it consumes more than 1g of heap memory for those field. put it in perspective, the ancestors consume 2 - 3 times more memory than bloomfilter (fp = 0.1 by default) in lcs.   we should remove those ancestors from sstablemetadata after the compaction is finished and the old sstable is removed. it might be a big deal for sized compaction since there are small number of sstable involved. but it consumes a lot of memory for lcs.   at least, we shouldn't load those ancestors to the memory during startup if the files are removed.   i would love to contribute and provide patch. please let me know how to start.<stacktrace> <code> <text> we are using lcs and have total of 38000 sstables for one cf. during lcs, there could be over a thousand sstable involved. all those sstable ids are stored in ancestors field of sstablemetatdata for the new table. in our case, it consumes more than 1g of heap memory for those field. put it in perspective, the ancestors consume 2 - 3 times more memory than bloomfilter (fp = 0.1 by default) in lcs.   we should remove those ancestors from sstablemetadata after the compaction is finished and the old sstable is removed. it might be a big deal for sized compaction since there are small number of sstable involved. but it consumes a lot of memory for lcs.   at least, we shouldn't load those ancestors to the memory during startup if the files are removed.   i would love to contribute and provide patch. please let me know how to start.",
        "label": 321
    },
    {
        "text": "cqlrecordreader does not work with password authentication <description> cqlrecordreader initialises a cluster with:  cluster = cqlconfighelper.getinputcluster(location, conf); cqlconfighelper gets class name for auth provider and then tries to initialise it without any parameter. plaintextauthprovider does not have no-args constructor, so it fails in this case. there is no other way to initialise cqlrecordreader with password authentication. one solution which can be considered is to modify the method which instantiate auth provider, so that if it detects plaintextauthprovider, it will retrieve additional parameters to pass to the constructor. or, it can be done more generic.<stacktrace> <code> cqlrecordreader initialises a cluster with:  cluster = cqlconfighelper.getinputcluster(location, conf); <text> cqlconfighelper gets class name for auth provider and then tries to initialise it without any parameter. plaintextauthprovider does not have no-args constructor, so it fails in this case. there is no other way to initialise cqlrecordreader with password authentication. one solution which can be considered is to modify the method which instantiate auth provider, so that if it detects plaintextauthprovider, it will retrieve additional parameters to pass to the constructor. or, it can be done more generic.",
        "label": 220
    },
    {
        "text": "disk failure policy ignores corruptblockexception <description> if cassandra is using compression and has a bad drive or stable, it will throw an corruptblockexception.   disk failure policy only works if it is an fserror and does not work for ioexceptions like this.   we need to better handle such exceptions as it causes nodes to not respond to the co-ordinator causing the client to timeout.<stacktrace> <code> <text> if cassandra is using compression and has a bad drive or stable, it will throw an corruptblockexception.   disk failure policy only works if it is an fserror and does not work for ioexceptions like this.   we need to better handle such exceptions as it causes nodes to not respond to the co-ordinator causing the client to timeout.",
        "label": 321
    },
    {
        "text": "repair doesn't do anything when a cf isn't specified <description> invoking repair on a node does not work. with rf > 1 all i get is: info 15:04:59,987 waiting for repair requests to: [] and nothing happens.<stacktrace> <code> info 15:04:59,987 waiting for repair requests to: [] <text> invoking repair on a node does not work. with rf > 1 all i get is: and nothing happens.",
        "label": 85
    },
    {
        "text": "restore columnsort time on super columnfamilies <description> setting columnsort=time on a super columnfamily should control the sort order of the subcolumns.<stacktrace> <code> <text> setting columnsort=time on a super columnfamily should control the sort order of the subcolumns.",
        "label": 274
    },
    {
        "text": "repair  pr with vnodes incompatibilty <description> hi, i have a cluster on 1.2.2 . this cluster is composed of 16 nodes in two datacenters (8 and 8) with an rf 3 :3.  i used virtual nodes, 256 on each node. when i do \u201crepair \u2013pr\" on a node, i can see that it\u2019s doing repair only on the first vnode : [2013-03-07 14:42:56,922] starting repair command #7, repairing 1 ranges for keyspace pns_fr [2013-03-07 14:42:57,835] repair session eb38dfa0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished [2013-03-07 14:42:57,835] repair command #7 finished [2013-03-07 14:42:57,852] starting repair command #8, repairing 1 ranges for keyspace hbxtest [2013-03-07 14:42:59,307] repair session ebc6c7c0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished so if i understand well, when i do a \"repair \u2013pr\" on each node, i will repair only the first vnode on each node. (16 token ranges on 4096 ranges). this method doesn\u2019t guarantee the consistency of the dataset. it seems to me that the \"repair \u2013pr\" is not compatible with vnode cluster.<stacktrace> <code> [2013-03-07 14:42:56,922] starting repair command #7, repairing 1 ranges for keyspace pns_fr [2013-03-07 14:42:57,835] repair session eb38dfa0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished [2013-03-07 14:42:57,835] repair command #7 finished [2013-03-07 14:42:57,852] starting repair command #8, repairing 1 ranges for keyspace hbxtest [2013-03-07 14:42:59,307] repair session ebc6c7c0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished <text> hi, i have a cluster on 1.2.2 . this cluster is composed of 16 nodes in two datacenters (8 and 8) with an rf 3 :3. i used virtual nodes, 256 on each node. when i do 'repair -pr' on a node, i can see that it's doing repair only on the first vnode : so if i understand well, when i do a 'repair -pr' on each node, i will repair only the first vnode on each node. (16 token ranges on 4096 ranges). this method doesn't guarantee the consistency of the dataset. it seems to me that the 'repair -pr' is not compatible with vnode cluster.",
        "label": 577
    },
    {
        "text": "while dropping and recreating an index  incremental snapshotting can hang <description> when creating a hard link (at list with jna), link() hang if the target of the  link already exists. in theory though, we should not hit that situation  because we use a new directory for each manual snapshot and the generation  number of the sstables should prevent this from hapenning with increment  snapshot. however, when you drop, then recreate a secondary index, if the sstables are  deleted after the drop and before we recreate the index, the recreated index  sstables will start with a generation to 0. thus, when we start backuping them  incrementally, it will conflict with the sstables of the previously dropped  index. first, we should check for the target existance because calling link() to at  least avoid hanging. but then we must make sure that when we drop, then  recreate an index, we will either not name the sstables the same way or the  incremental snapshot use a different directory.<stacktrace> <code> <text> when creating a hard link (at list with jna), link() hang if the target of the  link already exists. in theory though, we should not hit that situation  because we use a new directory for each manual snapshot and the generation  number of the sstables should prevent this from hapenning with increment  snapshot. however, when you drop, then recreate a secondary index, if the sstables are  deleted after the drop and before we recreate the index, the recreated index  sstables will start with a generation to 0. thus, when we start backuping them  incrementally, it will conflict with the sstables of the previously dropped  index. first, we should check for the target existance because calling link() to at  least avoid hanging. but then we must make sure that when we drop, then  recreate an index, we will either not name the sstables the same way or the  incremental snapshot use a different directory.",
        "label": 274
    },
    {
        "text": "test compression could run multiple unit tests in parallel like test <description> <stacktrace> <code> <text> ",
        "label": 52
    },
    {
        "text": "dtcs configuration proposals for handling consequences of repairs <description> this is a document bringing up some issues when dtcs is used to compact time series data in a three node cluster. the dtcs is currently configured with a few parameters that are making the configuration fairly simple, but might cause problems in certain special cases like recovering from the flood of small sstables due to repair operation. we are suggesting some ideas that might be a starting point for further discussions. following sections are containing: description of the cassandra setup feeding process of the data failure testing issues caused by the repair operations for the dtcs proposal for the dtcs configuration parameters attachments are included to support the discussion and there is a separate section giving explanation for those. cassandra setup and data model cluster is composed from three nodes running cassandra 2.1.2. replication factor is two and read and write consistency levels are one. data is time series data. data is saved so that one row contains a certain time span of data for a given metric ( 20 days in this case). the row key contains information about the start time of the time span and metrix name. column name gives the offset from the beginning of time span. column time stamp is set to correspond time stamp when adding together the timestamp from the row key and the offset (the actual time stamp of data point). data model is analog to kairosdb implementation. average sampling rate is 10 seconds varying significantly from metric to metric. 100 000 metrics are fed to the cassandra. max_sstable_age_days is set to 5 days (objective is to keep sstable files in manageable size, around 50 gb) ttl is not in use in the test. procedure for the failure test. data is first dumped to cassandra for 11 days and the data dumping is stopped so that dtcs will have a change to finish all compactions. data is dumped with \"fake timestamps\" so that column time stamp is set when data is written to cassandra. one of the nodes is taken down and new data is dumped on top of the earlier data covering couple of hours worth of data (faked time stamps). dumping is stopped and the node is kept down for few hours. node is taken up and the \"nodetool repair\" is applied on the node that was down. consequences repair operation will lead to massive amount of new sstables far back in the history. new sstables are covering similar time spans than the files that were created by dtcs before the shutdown of one of the nodes. to be able to compact the small files the max_sstable_age_days should be increased to allow compaction to handle the files. however, the in a practical case the time window will increase so large that generated files will be huge that is not desirable. the compaction also combines together one very large file with a bunch of small files in several phases that is not effective. generating really large files may also lead to out of disc space problems. see the list of time graphs later in the document. improvement proposals for the dtcs configuration below is a list of desired properties for the configuration. current parameters are mentioned if available. initial window size (currently:base_time_seconds) the amount of similar size windows for the bucketing (currently: min_threshold) the multiplier for the window size when increased (currently: min_threshold). this we would like to be independent from the min_threshold parameter so that you could actually control the rate how fast the window size is increased. maximum length of the time window inside which the files are assigned for a certain bucket (not currently defined). this means that expansion of time window length is restricted. when the limit is reached the window size will be same all the way back in the history (e.g. one week) the maximum horizon in which sstables are candidates for buckets (currently: max_sstable_age_days) maximum file size of sstable allowed to be in a set of files to be compacted (not possible currently). preventing out of disk space situations. optional strategies to select the most interesting bucket: minimum amount of sstables in the time window before it is a candidate for the most interesting bucket (currently: min_threshold for the most recent window, otherwise two). being able set this value independently would allow to put most of the efforts on those areas where a large amount of small files should be compacted together instead of few new files. optionally, the criteria for the most interesting bucket could be set: e.g. select the window with most files to be compacted. inside the bucket when the amount of files is limited by max_threshold, the compaction would select first small files instead of one huge file and a bunch of small files. the above set of parameters allows to recover from repair operations producing large amount of small sstables. maximum length of the time window for compactions would keep the compacted sstable size in reasonable range and would allow to extend the horizon far back in the history combining small files together instead of combining one huge file with e.g. 31 small files again and again is more disk efficient in addition to the previous advantages the above parameters would also allow: dumping of more data in the history (e.g. new metrics) by assigning the correct timestamp for the column (fake time stamp) and proper compaction of new and existing sstables. expiring reasonable size sstable with ttl even if the compactions would be intermittently executed far back in the history. in this case the new data has to fed with ttl calculated dynamically. note: being able to give the absolute time stamp for the column expiry time would be beneficial when data is dumped back in the history. this is the case when you move data from some legacy system to cassandra with faked time stamps and would like to keep the data only a certain time period. currently the absolute time stamp is calculated by cassandra from the system time and given ttl. ttl has to be calculated dynamically based on the current time and desired expiry moment making things more complex. one interesting question is that why those duplicate sstable files are created? the duplication problem could not be produced when the data was dumped with following spec: 100 metrics 20 days of data in one row one year of data max_sstable_age_days = 15 memtable_offheap_space_in_mb was decreased so that small sstables were created (to create something to be compacted) one node was taken down and one more day of data was dumped on top of the earlier data \"nodetool repair -pr\" was executed on each node => duplicates were checked in each step => no duplicates \"nodetool repair\" was executed on a node that was down => no duplicates were generated -------------------------------------------------------------------------------------------------------------------- attachments time graphs of content of sstables from different phases of the test run: ************************************************************* fields in the below time graphs are following: order number from the sstable file name minimum column timestamp in the sstable file timespan representation graphically maximum column time stamp in sstable the size of the sstable in megabytes time graphs after dumping the 11 days of data and letting all compactions to run through node0_20150621_1646_time_graph.txt  node1_20150621_1646_time_graph.txt  node2_20150621_1646_time_graph.txt (error: same as for node1, but the behavior is same) time graphs after taking one node down (node2) and dumping couple of hours of mode data node0_20150621_2320_time_graph.txt  node1_20150621_2320_time_graph.txt  node2_20150621_2320_time_graph.txt time graphs when the repair operation has finished and compactions are done. compactions will naturally handle only the files inside the max_sstable_age_days range. ==> now there is a large amount of small files covering pretty much same areas as the original sstables node0_20150623_1526_time_graph.txt  node1_20150623_1526_time_graph.txt  node2_20150623_1526_time_graph.txt -----------  trend from the sstable count as a function of time on each node.  sstable_counts.jpg vertical lines: 1) clearing the database and dumping the 11 days worth of data  2) stopping the dumping and letting compactions run  3) taking one node down (top bottom one in figure) and dumping few hours of new data on top of earlier data  4) starting the repair operation  5) repair operation finished --------  nodetool status prints before and after repair operation  nodetool status infos.txt --------------  tracing compactions log files were parsed to demonstrate the creation of new small sstables and the combination of one large file with a bunch of small ones. this is done from the time range where the max_sstable_age_days is able to reach (in this case 5 days). the hierarchy of the files is shown in the file \"sstable_compaction_trace.txt\". the first flushed file can be found from the line 10. each line represents either a flushed sstable or the sstable created by dtcs. for flushed files the timestamp indicates the time period the file represents. for compacted files (marked with c) the first timestamp represents the moment when the compaction was done (wall clock). time stamps are faked when written to the database. the size of the file is the last field. the first field with number in parenthesis shows the level of the file. top level files marked with (0) are those that don't have any predecessors and should be found from the disk also. sstables that are created by the repair operation are not mentioned in the log files so they are handled as phantom files. the existence of file can be concluded from the predecessor list of compacted sstable. those are marked with none,none in timestamps. in the file \"sstable_compaction_trace_snipped.txt\" is one portion that shows the compaction hierarchy for the small files originating from the repair operation. max_threshold is in the default value of 32. in each step 31 tiny files are compacted together with 46 gb file. sstable_compaction_trace.txt  sstable_compaction_trace_snipped.txt<stacktrace> <code> -------------------------------------------------------------------------------------------------------------------- ************************************************************* node0_20150621_1646_time_graph.txt  node1_20150621_1646_time_graph.txt  node2_20150621_1646_time_graph.txt (error: same as for node1, but the behavior is same) node0_20150621_2320_time_graph.txt  node1_20150621_2320_time_graph.txt  node2_20150621_2320_time_graph.txt node0_20150623_1526_time_graph.txt  node1_20150623_1526_time_graph.txt  node2_20150623_1526_time_graph.txt --------------  tracing compactions sstable_compaction_trace.txt  sstable_compaction_trace_snipped.txt<text> this is a document bringing up some issues when dtcs is used to compact time series data in a three node cluster. the dtcs is currently configured with a few parameters that are making the configuration fairly simple, but might cause problems in certain special cases like recovering from the flood of small sstables due to repair operation. we are suggesting some ideas that might be a starting point for further discussions. following sections are containing: attachments are included to support the discussion and there is a separate section giving explanation for those. cassandra setup and data model procedure for the failure test. consequences improvement proposals for the dtcs configuration below is a list of desired properties for the configuration. current parameters are mentioned if available. the above set of parameters allows to recover from repair operations producing large amount of small sstables. in addition to the previous advantages the above parameters would also allow: one interesting question is that why those duplicate sstable files are created? the duplication problem could not be produced when the data was dumped with following spec: attachments time graphs of content of sstables from different phases of the test run: fields in the below time graphs are following: time graphs after dumping the 11 days of data and letting all compactions to run through time graphs after taking one node down (node2) and dumping couple of hours of mode data time graphs when the repair operation has finished and compactions are done. compactions will naturally handle only the files inside the max_sstable_age_days range. ==> now there is a large amount of small files covering pretty much same areas as the original sstables -----------  trend from the sstable count as a function of time on each node.  sstable_counts.jpg vertical lines: 1) clearing the database and dumping the 11 days worth of data  2) stopping the dumping and letting compactions run  3) taking one node down (top bottom one in figure) and dumping few hours of new data on top of earlier data  4) starting the repair operation  5) repair operation finished --------  nodetool status prints before and after repair operation  nodetool status infos.txt log files were parsed to demonstrate the creation of new small sstables and the combination of one large file with a bunch of small ones. this is done from the time range where the max_sstable_age_days is able to reach (in this case 5 days). the hierarchy of the files is shown in the file 'sstable_compaction_trace.txt'. the first flushed file can be found from the line 10. each line represents either a flushed sstable or the sstable created by dtcs. for flushed files the timestamp indicates the time period the file represents. for compacted files (marked with c) the first timestamp represents the moment when the compaction was done (wall clock). time stamps are faked when written to the database. the size of the file is the last field. the first field with number in parenthesis shows the level of the file. top level files marked with (0) are those that don't have any predecessors and should be found from the disk also. sstables that are created by the repair operation are not mentioned in the log files so they are handled as phantom files. the existence of file can be concluded from the predecessor list of compacted sstable. those are marked with none,none in timestamps. in the file 'sstable_compaction_trace_snipped.txt' is one portion that shows the compaction hierarchy for the small files originating from the repair operation. max_threshold is in the default value of 32. in each step 31 tiny files are compacted together with 46 gb file. ",
        "label": 321
    },
    {
        "text": "invalid results are returned while secondary index are being build <description> if you request an index creation and then execute a query that use the index the results returned might be invalid until the index is fully build. this is caused by the fact that the table column will be marked as indexed before the index is ready. the following unit tests can be use to reproduce the problem:     @test     public void testindexcreatedafterinsert() throws throwable     {         createtable(\"create table %s (a int, b int, c int, primary key((a, b)))\");         execute(\"insert into %s (a, b, c) values (0, 0, 0);\");         execute(\"insert into %s (a, b, c) values (0, 1, 1);\");         execute(\"insert into %s (a, b, c) values (0, 2, 2);\");         execute(\"insert into %s (a, b, c) values (1, 0, 3);\");         execute(\"insert into %s (a, b, c) values (1, 1, 4);\");                  createindex(\"create index on %s(b)\");                  assertrows(execute(\"select * from %s where b = ?;\", 1),                    row(0, 1, 1),                    row(1, 1, 4));     }          @test     public void testindexcreatedbeforeinsert() throws throwable     {         createtable(\"create table %s (a int, b int, c int, primary key((a, b)))\");         createindex(\"create index on %s(b)\");                  execute(\"insert into %s (a, b, c) values (0, 0, 0);\");         execute(\"insert into %s (a, b, c) values (0, 1, 1);\");         execute(\"insert into %s (a, b, c) values (0, 2, 2);\");         execute(\"insert into %s (a, b, c) values (1, 0, 3);\");         execute(\"insert into %s (a, b, c) values (1, 1, 4);\");         assertrows(execute(\"select * from %s where b = ?;\", 1),                    row(0, 1, 1),                    row(1, 1, 4));     } the first test will fail while the second will work. in my opinion the first test should reject the request as invalid (as if the index was not existing) until the index is fully build.<stacktrace> <code>     @test     public void testindexcreatedafterinsert() throws throwable     {         createtable('create table %s (a int, b int, c int, primary key((a, b)))');         execute('insert into %s (a, b, c) values (0, 0, 0);');         execute('insert into %s (a, b, c) values (0, 1, 1);');         execute('insert into %s (a, b, c) values (0, 2, 2);');         execute('insert into %s (a, b, c) values (1, 0, 3);');         execute('insert into %s (a, b, c) values (1, 1, 4);');                  createindex('create index on %s(b)');                  assertrows(execute('select * from %s where b = ?;', 1),                    row(0, 1, 1),                    row(1, 1, 4));     }          @test     public void testindexcreatedbeforeinsert() throws throwable     {         createtable('create table %s (a int, b int, c int, primary key((a, b)))');         createindex('create index on %s(b)');                  execute('insert into %s (a, b, c) values (0, 0, 0);');         execute('insert into %s (a, b, c) values (0, 1, 1);');         execute('insert into %s (a, b, c) values (0, 2, 2);');         execute('insert into %s (a, b, c) values (1, 0, 3);');         execute('insert into %s (a, b, c) values (1, 1, 4);');         assertrows(execute('select * from %s where b = ?;', 1),                    row(0, 1, 1),                    row(1, 1, 4));     } <text> if you request an index creation and then execute a query that use the index the results returned might be invalid until the index is fully build. this is caused by the fact that the table column will be marked as indexed before the index is ready. the following unit tests can be use to reproduce the problem: the first test will fail while the second will work. in my opinion the first test should reject the request as invalid (as if the index was not existing) until the index is fully build.",
        "label": 69
    },
    {
        "text": "faster uuid comparisons <description> as i explained on the mailing list, doing slice queries on cfs that are sorted by timeuuidtype gets slower as the value of count increases. according to my profiles, this is largely due to the fact that concurrentskiplistmap calls the comparator very frequently, which is extremely inefficient. in order to compare two uuids, it has to materialize them both in to java.util.uuid objects which is quite slow. the more uuids to compare, the slower it is. attached is a patch that compares uuids by extracting the timestamp directly from the byte[] representation. according to my tests, it's close to a 50% performance improvement.<stacktrace> <code> <text> as i explained on the mailing list, doing slice queries on cfs that are sorted by timeuuidtype gets slower as the value of count increases. according to my profiles, this is largely due to the fact that concurrentskiplistmap calls the comparator very frequently, which is extremely inefficient. in order to compare two uuids, it has to materialize them both in to java.util.uuid objects which is quite slow. the more uuids to compare, the slower it is. attached is a patch that compares uuids by extracting the timestamp directly from the byte[] representation. according to my tests, it's close to a 50% performance improvement.",
        "label": 224
    },
    {
        "text": "udt   allow null   non existant attributes <description> c* 2.1 cql user-defined-types are really fine and useful. but it lacks the possibility to omit attributes or set them to null. would be great to have the possibility to create udt \"instances\" with some attributes missing. also changing the udt definition (for example: alter type add new_attr) will break running applications that rely on the \"previous\" definition of the udt. for exmple: create type foo (    attr_one text,    attr_two int ); create table bar (    id int,    comp foo ); insert into bar (id, com) values (1, {attr_one: 'cassandra', attr_two: 2}); works insert into bar (id, com) values (1, {attr_one: 'cassandra'}); does not work alter type foo add attr_three timestamp; insert into bar (id, com) values (1, {attr_one: 'cassandra', attr_two: 2}); will no longer work (missing attribute)<stacktrace> <code> create type foo (    attr_one text,    attr_two int ); create table bar (    id int,    comp foo ); insert into bar (id, com) values (1, {attr_one: 'cassandra', attr_two: 2}); insert into bar (id, com) values (1, {attr_one: 'cassandra'}); alter type foo add attr_three timestamp; insert into bar (id, com) values (1, {attr_one: 'cassandra', attr_two: 2}); <text> c* 2.1 cql user-defined-types are really fine and useful. but it lacks the possibility to omit attributes or set them to null. would be great to have the possibility to create udt 'instances' with some attributes missing. also changing the udt definition (for example: alter type add new_attr) will break running applications that rely on the 'previous' definition of the udt. for exmple: works does not work will no longer work (missing attribute)",
        "label": 520
    },
    {
        "text": "force minumum timeout value <description> granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong: error 17:13:28,726 exception encountered during startup java.lang.exceptionininitializererror  at org.apache.cassandra.net.messagingservice.instance(messagingservice.java:310)  at org.apache.cassandra.service.storageservice.<init>(storageservice.java:233)  at org.apache.cassandra.service.storageservice.<clinit>(storageservice.java:141)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:87)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:63)  at org.apache.cassandra.config.databasedescriptor.createendpointsnitch(databasedescriptor.java:518)  at org.apache.cassandra.config.databasedescriptor.applyconfig(databasedescriptor.java:350)  at org.apache.cassandra.config.databasedescriptor.<clinit>(databasedescriptor.java:112)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:213)  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:567)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:656) caused by: java.lang.illegalargumentexception  at java.util.concurrent.scheduledthreadpoolexecutor.schedulewithfixeddelay(scheduledthreadpoolexecutor.java:586)  at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor.schedulewithfixeddelay(debuggablescheduledthreadpoolexecutor.java:64)  at org.apache.cassandra.utils.expiringmap.<init>(expiringmap.java:103)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:360)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:68)  at org.apache.cassandra.net.messagingservice$mshandle.<clinit>(messagingservice.java:306)  ... 11 more java.lang.exceptionininitializererror  at org.apache.cassandra.net.messagingservice.instance(messagingservice.java:310)  at org.apache.cassandra.service.storageservice.<init>(storageservice.java:233)  at org.apache.cassandra.service.storageservice.<clinit>(storageservice.java:141)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:87)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:63)  at org.apache.cassandra.config.databasedescriptor.createendpointsnitch(databasedescriptor.java:518)  at org.apache.cassandra.config.databasedescriptor.applyconfig(databasedescriptor.java:350)  at org.apache.cassandra.config.databasedescriptor.<clinit>(databasedescriptor.java:112)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:213)  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:567)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:656) caused by: java.lang.illegalargumentexception  at java.util.concurrent.scheduledthreadpoolexecutor.schedulewithfixeddelay(scheduledthreadpoolexecutor.java:586)  at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor.schedulewithfixeddelay(debuggablescheduledthreadpoolexecutor.java:64)  at org.apache.cassandra.utils.expiringmap.<init>(expiringmap.java:103)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:360)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:68)  at org.apache.cassandra.net.messagingservice$mshandle.<clinit>(messagingservice.java:306)  ... 11 more exception encountered during startup: null<stacktrace> error 17:13:28,726 exception encountered during startup java.lang.exceptionininitializererror  at org.apache.cassandra.net.messagingservice.instance(messagingservice.java:310)  at org.apache.cassandra.service.storageservice.<init>(storageservice.java:233)  at org.apache.cassandra.service.storageservice.<clinit>(storageservice.java:141)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:87)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:63)  at org.apache.cassandra.config.databasedescriptor.createendpointsnitch(databasedescriptor.java:518)  at org.apache.cassandra.config.databasedescriptor.applyconfig(databasedescriptor.java:350)  at org.apache.cassandra.config.databasedescriptor.<clinit>(databasedescriptor.java:112)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:213)  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:567)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:656) caused by: java.lang.illegalargumentexception  at java.util.concurrent.scheduledthreadpoolexecutor.schedulewithfixeddelay(scheduledthreadpoolexecutor.java:586)  at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor.schedulewithfixeddelay(debuggablescheduledthreadpoolexecutor.java:64)  at org.apache.cassandra.utils.expiringmap.<init>(expiringmap.java:103)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:360)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:68)  at org.apache.cassandra.net.messagingservice$mshandle.<clinit>(messagingservice.java:306)  ... 11 more java.lang.exceptionininitializererror  at org.apache.cassandra.net.messagingservice.instance(messagingservice.java:310)  at org.apache.cassandra.service.storageservice.<init>(storageservice.java:233)  at org.apache.cassandra.service.storageservice.<clinit>(storageservice.java:141)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:87)  at org.apache.cassandra.locator.dynamicendpointsnitch.<init>(dynamicendpointsnitch.java:63)  at org.apache.cassandra.config.databasedescriptor.createendpointsnitch(databasedescriptor.java:518)  at org.apache.cassandra.config.databasedescriptor.applyconfig(databasedescriptor.java:350)  at org.apache.cassandra.config.databasedescriptor.<clinit>(databasedescriptor.java:112)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:213)  at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:567)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:656) caused by: java.lang.illegalargumentexception  at java.util.concurrent.scheduledthreadpoolexecutor.schedulewithfixeddelay(scheduledthreadpoolexecutor.java:586)  at org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor.schedulewithfixeddelay(debuggablescheduledthreadpoolexecutor.java:64)  at org.apache.cassandra.utils.expiringmap.<init>(expiringmap.java:103)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:360)  at org.apache.cassandra.net.messagingservice.<init>(messagingservice.java:68)  at org.apache.cassandra.net.messagingservice$mshandle.<clinit>(messagingservice.java:306)  ... 11 more exception encountered during startup: null <code> <text> granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong:",
        "label": 551
    },
    {
        "text": "determine if a materialized view is finished building  without having to query each node <description> since mvs are eventually consistent with its base table, it would nice if we could easily know the state of the mv after its creation, so we could wait until the mv is built before doing some operations. // cc matthias broecheler t jake luciani carl yeksigian ryan mcguire<stacktrace> <code> <text> since mvs are eventually consistent with its base table, it would nice if we could easily know the state of the mv after its creation, so we could wait until the mv is built before doing some operations. // cc matthias broecheler t jake luciani carl yeksigian ryan mcguire",
        "label": 98
    },
    {
        "text": "truncate operation doesn't delete rows from hintscolumnfamily  <description> steps to reproduce:  1. start writing of data to some column family, let name it 'mycf'  2. stop 1 node  3. wait some time (until some data will be collected in hintscolumnfamily)  4. start node (hintedhandoff will be started automatically for 'mycf')  5. run 'truncate' command for 'mycf' column family from command from cli  6. wait until truncate will be finished  7. you will see that 'mycf' is not empty because hintedhandoff is copying data so, i suggest to clean hintscolumnfamily (for truncated column family) before we had started to discard sstables.   i think it should be done in compactionmanager#submittrucate() method. i can try to create patch but i need to know right way of cleaning hintscolumnfamily. could you clarify it?<stacktrace> <code> <text> steps to reproduce:  1. start writing of data to some column family, let name it 'mycf'  2. stop 1 node  3. wait some time (until some data will be collected in hintscolumnfamily)  4. start node (hintedhandoff will be started automatically for 'mycf')  5. run 'truncate' command for 'mycf' column family from command from cli  6. wait until truncate will be finished  7. you will see that 'mycf' is not empty because hintedhandoff is copying data so, i suggest to clean hintscolumnfamily (for truncated column family) before we had started to discard sstables.   i think it should be done in compactionmanager#submittrucate() method. i can try to create patch but i need to know right way of cleaning hintscolumnfamily. could you clarify it?",
        "label": 274
    },
    {
        "text": "support java <description> this ticket is intended to group all issues found to support java 9 in the future. from what i've found out so far: maven dependency com.sun:tools:jar:0 via cobertura cannot be resolved. it can be easily solved using this patch: -        <dependency groupid=\"net.sourceforge.cobertura\" artifactid=\"cobertura\"/> +        <dependency groupid=\"net.sourceforge.cobertura\" artifactid=\"cobertura\"> +          <exclusion groupid=\"com.sun\" artifactid=\"tools\"/> +        </dependency> another issue is that sun.misc.unsafe no longer contains the methods monitorenter + monitorexit. these methods are used by o.a.c.utils.concurrent.locks which is only used by o.a.c.db.atomicbtreecolumns. i don't mind to start working on this yet since java 9 is in a too early development phase.<stacktrace> <code> -        <dependency groupid='net.sourceforge.cobertura' artifactid='cobertura'/> +        <dependency groupid='net.sourceforge.cobertura' artifactid='cobertura'> +          <exclusion groupid='com.sun' artifactid='tools'/> +        </dependency> <text> this ticket is intended to group all issues found to support java 9 in the future. from what i've found out so far: i don't mind to start working on this yet since java 9 is in a too early development phase.",
        "label": 453
    },
    {
        "text": "faster range tombstones on wide partitions <description> having wide cql rows (~1m in single partition) and after deleting some of them, we found inefficiencies in handling of range tombstones on both write and read paths. i attached 2 patches here, one for write path (rangetombstoneswriteoptimization.diff) and another on read (rangetombstonesreadoptimization.diff). on write path, when you have some cql rows deletions by primary key, each of deletion is represented by range tombstone. on put of this tombstone to memtable the original code takes all columns from memtable from partition and checks deletioninfo.isdeleted by brute for loop to decide, should this column stay in memtable or it was deleted by new tombstone. needless to say, more columns you have on partition the slower deletions you have heating your cpu with brute range tombstones check.   the rangetombstoneswriteoptimization.diff patch for partitions with more than 10000 columns loops by tombstones instead and checks existance of columns for each of them. also it copies of whole memtable range tombstone list only if there are changes to be made there (original code copies range tombstone list on every write). on read path, original code scans whole range tombstone list of a partition to match sstable columns to their range tomstones. the rangetombstonesreadoptimization.diff patch scans only necessary range of tombstones, according to filter used for read.<stacktrace> <code> <text> having wide cql rows (~1m in single partition) and after deleting some of them, we found inefficiencies in handling of range tombstones on both write and read paths. i attached 2 patches here, one for write path (rangetombstoneswriteoptimization.diff) and another on read (rangetombstonesreadoptimization.diff). on write path, when you have some cql rows deletions by primary key, each of deletion is represented by range tombstone. on put of this tombstone to memtable the original code takes all columns from memtable from partition and checks deletioninfo.isdeleted by brute for loop to decide, should this column stay in memtable or it was deleted by new tombstone. needless to say, more columns you have on partition the slower deletions you have heating your cpu with brute range tombstones check.   the rangetombstoneswriteoptimization.diff patch for partitions with more than 10000 columns loops by tombstones instead and checks existance of columns for each of them. also it copies of whole memtable range tombstone list only if there are changes to be made there (original code copies range tombstone list on every write). on read path, original code scans whole range tombstone list of a partition to match sstable columns to their range tomstones. the rangetombstonesreadoptimization.diff patch scans only necessary range of tombstones, according to filter used for read.",
        "label": 392
    },
    {
        "text": "stress java cardinality option parsing typo <description> session.java               if (cmd.hasoption(\"c\"))                 cardinality = integer.parseint(cmd.getoptionvalue(\"t\"));<stacktrace> <code>               if (cmd.hasoption('c'))                 cardinality = integer.parseint(cmd.getoptionvalue('t')); session.java<text> ",
        "label": 103
    },
    {
        "text": "new authentication module does not wok in multi datacenters in case of network outage <description> with 1.2.2, i am using the new authentication backend passwordauthenticator with the authorizer cassandraauthorizer in case of network outage, we are no more able to connect to cassandra. here is the error message we get when i want to connect through cqlsh:  traceback (most recent call last):  file \"./cqlsh\", line 2262, in <module>  main(*read_options(sys.argv[1:], os.environ))  file \"./cqlsh\", line 2248, in main  display_float_precision=options.float_precision)  file \"./cqlsh\", line 483, in _init_  cql_version=cqlver, transport=transport) file \"./../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/connection.py\", line 143, in connect  file \"./../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/connection.py\", line 59, in _init_  file \"./../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/thrifteries.py\", line 157, in establish_connection  file \"./../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/cassandra.py\", line 455, in login  file \"./../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/cassandra.py\", line 476, in recv_login  cql.cassandra.ttypes.authenticationexception: authenticationexception(why='org.apache.cassandra.exceptions.unavailableexception: cannot achieve consistency level quorum')<stacktrace> <code> here is the error message we get when i want to connect through cqlsh:  traceback (most recent call last):  file './cqlsh', line 2262, in <module>  main(*read_options(sys.argv[1:], os.environ))  file './cqlsh', line 2248, in main  display_float_precision=options.float_precision)  file './cqlsh', line 483, in _init_  cql_version=cqlver, transport=transport) file './../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/connection.py', line 143, in connect  file './../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/connection.py', line 59, in _init_  file './../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/thrifteries.py', line 157, in establish_connection  file './../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/cassandra.py', line 455, in login  file './../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/cassandra.py', line 476, in recv_login  cql.cassandra.ttypes.authenticationexception: authenticationexception(why='org.apache.cassandra.exceptions.unavailableexception: cannot achieve consistency level quorum')<text> with 1.2.2, i am using the new authentication backend passwordauthenticator with the authorizer cassandraauthorizer in case of network outage, we are no more able to connect to cassandra. ",
        "label": 18
    },
    {
        "text": "nodetool describecluster should be more informative <description> additional information we should be displaying: total node count list of datacenters, rf, with number of nodes per dc, how many are down, version(s)<stacktrace> <code> <text> additional information we should be displaying:",
        "label": 436
    },
    {
        "text": "create compaction stress <description> a tool like cassandra-stress that works with stress yaml but: writes directly to a specified dir using cqlsstablewriter. lets you run just compaction on that directory and generates a report on compaction throughput.<stacktrace> <code> <text> a tool like cassandra-stress that works with stress yaml but:",
        "label": 521
    },
    {
        "text": "dtest failure in materialized views test testmaterializedviews base replica repair test <description> base_replica_repair_test has failed on trunk with the following exception in the log of node2: error [main] 2016-03-31 08:48:46,949 cassandradaemon.java:708 - exception encountered during startup java.lang.runtimeexception: failed to list files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985         at org.apache.cassandra.db.lifecycle.logawarefilelister.list(logawarefilelister.java:53) ~[main/:na]         at org.apache.cassandra.db.lifecycle.lifecycletransaction.getfiles(lifecycletransaction.java:547) ~[main/:na]         at org.apache.cassandra.db.directories$sstablelister.filter(directories.java:725) ~[main/:na]         at org.apache.cassandra.db.directories$sstablelister.list(directories.java:690) ~[main/:na]         at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:567) ~[main/:na]         at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:555) ~[main/:na]         at org.apache.cassandra.db.keyspace.initcf(keyspace.java:383) ~[main/:na]         at org.apache.cassandra.db.keyspace.<init>(keyspace.java:320) ~[main/:na]         at org.apache.cassandra.db.keyspace.open(keyspace.java:130) ~[main/:na]         at org.apache.cassandra.db.keyspace.open(keyspace.java:107) ~[main/:na]         at org.apache.cassandra.cql3.restrictions.statementrestrictions.<init>(statementrestrictions.java:139) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.preparerestrictions(selectstatement.java:864) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:811) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:799) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.getstatement(queryprocessor.java:505) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.parsestatement(queryprocessor.java:242) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.prepareinternal(queryprocessor.java:286) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.executeinternal(queryprocessor.java:294) ~[main/:na]         at org.apache.cassandra.schema.schemakeyspace.query(schemakeyspace.java:1246) ~[main/:na]         at org.apache.cassandra.schema.schemakeyspace.fetchkeyspaceswithout(schemakeyspace.java:875) ~[main/:na]         at org.apache.cassandra.schema.schemakeyspace.fetchnonsystemkeyspaces(schemakeyspace.java:867) ~[main/:na]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:134) ~[main/:na]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:124) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:229) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:562) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:691) [main/:na] caused by: java.lang.runtimeexception: failed to list directory files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985, inconsistent disk state for transaction [ma_txn_flush_58db56b0-f71d-11e5-bf68-03a01adb9f11.log in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985]         at org.apache.cassandra.db.lifecycle.logawarefilelister.classifyfiles(logawarefilelister.java:149) ~[main/:na]         at org.apache.cassandra.db.lifecycle.logawarefilelister.classifyfiles(logawarefilelister.java:103) ~[main/:na]         at org.apache.cassandra.db.lifecycle.logawarefilelister$$lambda$48/35984028.accept(unknown source) ~[na:na]         at java.util.stream.foreachops$foreachop$ofref.accept(foreachops.java:184) ~[na:1.8.0_45]         at java.util.stream.referencepipeline$2$1.accept(referencepipeline.java:175) ~[na:1.8.0_45]         at java.util.arraylist$arraylistspliterator.foreachremaining(arraylist.java:1374) ~[na:1.8.0_45]         at java.util.stream.abstractpipeline.copyinto(abstractpipeline.java:512) ~[na:1.8.0_45]         at java.util.stream.abstractpipeline.wrapandcopyinto(abstractpipeline.java:502) ~[na:1.8.0_45]         at java.util.stream.foreachops$foreachop.evaluatesequential(foreachops.java:151) ~[na:1.8.0_45]         at java.util.stream.foreachops$foreachop$ofref.evaluatesequential(foreachops.java:174) ~[na:1.8.0_45]         at java.util.stream.abstractpipeline.evaluate(abstractpipeline.java:234) ~[na:1.8.0_45]         at java.util.stream.referencepipeline.foreach(referencepipeline.java:418) ~[na:1.8.0_45]         at org.apache.cassandra.db.lifecycle.logawarefilelister.innerlist(logawarefilelister.java:71) ~[main/:na]         at org.apache.cassandra.db.lifecycle.logawarefilelister.list(logawarefilelister.java:49) ~[main/:na]         ... 25 common frames omitted example failure: http://cassci.datastax.com/job/trunk_dtest/1092/testreport/materialized_views_test/testmaterializedviews/base_replica_repair_test failed on cassci build trunk_dtest #1092 i've attached the logs from the failure in build 1092.<stacktrace> error [main] 2016-03-31 08:48:46,949 cassandradaemon.java:708 - exception encountered during startup java.lang.runtimeexception: failed to list files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985         at org.apache.cassandra.db.lifecycle.logawarefilelister.list(logawarefilelister.java:53) ~[main/:na]         at org.apache.cassandra.db.lifecycle.lifecycletransaction.getfiles(lifecycletransaction.java:547) ~[main/:na]         at org.apache.cassandra.db.directories$sstablelister.filter(directories.java:725) ~[main/:na]         at org.apache.cassandra.db.directories$sstablelister.list(directories.java:690) ~[main/:na]         at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:567) ~[main/:na]         at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:555) ~[main/:na]         at org.apache.cassandra.db.keyspace.initcf(keyspace.java:383) ~[main/:na]         at org.apache.cassandra.db.keyspace.<init>(keyspace.java:320) ~[main/:na]         at org.apache.cassandra.db.keyspace.open(keyspace.java:130) ~[main/:na]         at org.apache.cassandra.db.keyspace.open(keyspace.java:107) ~[main/:na]         at org.apache.cassandra.cql3.restrictions.statementrestrictions.<init>(statementrestrictions.java:139) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.preparerestrictions(selectstatement.java:864) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:811) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement$rawstatement.prepare(selectstatement.java:799) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.getstatement(queryprocessor.java:505) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.parsestatement(queryprocessor.java:242) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.prepareinternal(queryprocessor.java:286) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.executeinternal(queryprocessor.java:294) ~[main/:na]         at org.apache.cassandra.schema.schemakeyspace.query(schemakeyspace.java:1246) ~[main/:na]         at org.apache.cassandra.schema.schemakeyspace.fetchkeyspaceswithout(schemakeyspace.java:875) ~[main/:na]         at org.apache.cassandra.schema.schemakeyspace.fetchnonsystemkeyspaces(schemakeyspace.java:867) ~[main/:na]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:134) ~[main/:na]         at org.apache.cassandra.config.schema.loadfromdisk(schema.java:124) ~[main/:na]         at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:229) [main/:na]         at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:562) [main/:na]         at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:691) [main/:na] caused by: java.lang.runtimeexception: failed to list directory files in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985, inconsistent disk state for transaction [ma_txn_flush_58db56b0-f71d-11e5-bf68-03a01adb9f11.log in /mnt/tmp/dtest-du964e/test/node2/data0/system_schema/views-9786ac1cdd583201a7cdad556410c985]         at org.apache.cassandra.db.lifecycle.logawarefilelister.classifyfiles(logawarefilelister.java:149) ~[main/:na]         at org.apache.cassandra.db.lifecycle.logawarefilelister.classifyfiles(logawarefilelister.java:103) ~[main/:na]         at org.apache.cassandra.db.lifecycle.logawarefilelister$$lambda$48/35984028.accept(unknown source) ~[na:na]         at java.util.stream.foreachops$foreachop$ofref.accept(foreachops.java:184) ~[na:1.8.0_45]         at java.util.stream.referencepipeline$2$1.accept(referencepipeline.java:175) ~[na:1.8.0_45]         at java.util.arraylist$arraylistspliterator.foreachremaining(arraylist.java:1374) ~[na:1.8.0_45]         at java.util.stream.abstractpipeline.copyinto(abstractpipeline.java:512) ~[na:1.8.0_45]         at java.util.stream.abstractpipeline.wrapandcopyinto(abstractpipeline.java:502) ~[na:1.8.0_45]         at java.util.stream.foreachops$foreachop.evaluatesequential(foreachops.java:151) ~[na:1.8.0_45]         at java.util.stream.foreachops$foreachop$ofref.evaluatesequential(foreachops.java:174) ~[na:1.8.0_45]         at java.util.stream.abstractpipeline.evaluate(abstractpipeline.java:234) ~[na:1.8.0_45]         at java.util.stream.referencepipeline.foreach(referencepipeline.java:418) ~[na:1.8.0_45]         at org.apache.cassandra.db.lifecycle.logawarefilelister.innerlist(logawarefilelister.java:71) ~[main/:na]         at org.apache.cassandra.db.lifecycle.logawarefilelister.list(logawarefilelister.java:49) ~[main/:na]         ... 25 common frames omitted <code> http://cassci.datastax.com/job/trunk_dtest/1092/testreport/materialized_views_test/testmaterializedviews/base_replica_repair_test <text> base_replica_repair_test has failed on trunk with the following exception in the log of node2: example failure: failed on cassci build trunk_dtest #1092 i've attached the logs from the failure in build 1092.",
        "label": 508
    },
    {
        "text": "fix consistencylevel during bootstrap <description> as originally designed, bootstrap nodes should always get all writes under any consistencylevel, so when bootstrap finishes the operator can run cleanup on the old nodes w/o fear that he might lose data. but if a bootstrap operation fails or is aborted, that means all writes will fail until the ex-bootstrapping node is decommissioned. so starting in cassandra-722, we just ignore dead nodes in consistencylevel calculations. but this breaks the original design. cassandra-822 adds a partial fix for this (just adding bootstrap targets into the rf targets and hinting normally), but this is still broken under certain conditions. the real fix is to consider consistencylevel for two sets of nodes:  1. the rf targets as currently existing (no pending ranges)  2. the rf targets as they will exist after all movement ops are done if we satisfy cl for both sets then we will always be in good shape. i'm not sure if we can easily calculate 2. from the current tokenmetadata, though.<stacktrace> <code> <text> as originally designed, bootstrap nodes should always get all writes under any consistencylevel, so when bootstrap finishes the operator can run cleanup on the old nodes w/o fear that he might lose data. but if a bootstrap operation fails or is aborted, that means all writes will fail until the ex-bootstrapping node is decommissioned. so starting in cassandra-722, we just ignore dead nodes in consistencylevel calculations. but this breaks the original design. cassandra-822 adds a partial fix for this (just adding bootstrap targets into the rf targets and hinting normally), but this is still broken under certain conditions. the real fix is to consider consistencylevel for two sets of nodes: 1. the rf targets as currently existing (no pending ranges)  2. the rf targets as they will exist after all movement ops are done if we satisfy cl for both sets then we will always be in good shape. i'm not sure if we can easily calculate 2. from the current tokenmetadata, though.",
        "label": 274
    },
    {
        "text": "hsha doesn't handle large messages gracefully <description> hsha doesn't seem to enforce any kind of max message length, and when messages are too large, it doesn't fail gracefully. with debug logs enabled, you'll see this: debug 13:13:31,805 unexpected state 16 which seems to mean that there's a selectionkey that's valid, but isn't ready for reading, writing, or accepting. client-side, you'll get this thrift error (while trying to read a frame as part of recv_batch_mutate): ttransportexception: tsocket read 0 bytes<stacktrace> <code> debug 13:13:31,805 unexpected state 16 <text> hsha doesn't seem to enforce any kind of max message length, and when messages are too large, it doesn't fail gracefully. with debug logs enabled, you'll see this: which seems to mean that there's a selectionkey that's valid, but isn't ready for reading, writing, or accepting. client-side, you'll get this thrift error (while trying to read a frame as part of recv_batch_mutate): ttransportexception: tsocket read 0 bytes",
        "label": 412
    },
    {
        "text": "cql protocol does not include schema information <description> the following statement fails when used with a statement or preparedstatement res = stmt.executequery(\"select bar from users\");   res.next(); error message     [junit] testcase: simpleselect(com.datastax.cql.reprobugtest): caused an error     [junit] null     [junit] java.lang.nullpointerexception     [junit]  at org.apache.cassandra.cql.jdbc.columndecoder.makekeycolumn(columndecoder.java:136)     [junit]  at org.apache.cassandra.cql.jdbc.cresultset.next(cresultset.java:388)     [junit]  at com.datastax.cql.reprobugtest.simpleselect(reprobugtest.java:57)     [junit]      [junit]      [junit] test com.datastax.cql.reprobugtest failed here is a quick repro. showing that res.next() works with other statements but not select.  also notice that resultset.getmetadata().getcolumncount() always returns zero.   i noticed in the existing driver tests similar test cases, so not sure the issue. steps to run script you will need to drop this in your test directory change the package declaration ant test -dtest.name=reprobugtest package com.datastax.cql; import java.sql.drivermanager; import java.sql.connection; import java.sql.resultset; import java.sql.sqlexception; import java.sql.statement; import org.junit.test; public class reprobugtest {          @test     public void simpleselect() throws exception {            connection connection = null;         resultset res;         statement stmt;         int colcount = 0;                  try {             class.forname(\"org.apache.cassandra.cql.jdbc.cassandradriver\");                          // check create keyspace             connection = drivermanager.getconnection(\"jdbc:cassandra:root/root@127.0.0.1:9160/default\");                  stmt = connection.createstatement();             try {               system.out.println(\"running drop ks statement\");                 res = stmt.executequery(\"drop keyspace ks1\");                 res.next();                              system.out.println(\"running create ks statement\");               res = stmt.executequery(\"create keyspace ks1 with strategy_class =  'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=1\");                 res.next();             } catch (sqlexception e) {                 if (e.getmessage().startswith(\"keyspace does not exist\"))                  {                     res = stmt.executequery(\"create keyspace ks1 with strategy_class =  'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=1\");                   }              }                connection.close();                              // run test             connection = drivermanager.getconnection(\"jdbc:cassandra:root/root@127.0.0.1:9160/ks1\");                  stmt = connection.createstatement();             system.out.print(\"running create cf statement\");             res = stmt.executequery(\"create columnfamily users (key varchar primary key, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)\");                 colcount = res.getmetadata().getcolumncount();             system.out.println(\" -- column count: \" + colcount);              res.next();                          system.out.print(\"running insert statement\");             res = stmt.executequery(\"insert into users (key, password) values ('user1', 'ch@nge')\");               colcount = res.getmetadata().getcolumncount();             system.out.println(\" -- column count: \" + colcount);              res.next();                          system.out.print(\"running select statement\");             res = stmt.executequery(\"select bar from users\");               colcount = res.getmetadata().getcolumncount();             system.out.println(\" -- column count: \" + colcount);              res.getrow();             res.next();                              connection.close();                        } catch (sqlexception e) {             e.printstacktrace();         }     }         }<stacktrace>     [junit] testcase: simpleselect(com.datastax.cql.reprobugtest): caused an error     [junit] null     [junit] java.lang.nullpointerexception     [junit]  at org.apache.cassandra.cql.jdbc.columndecoder.makekeycolumn(columndecoder.java:136)     [junit]  at org.apache.cassandra.cql.jdbc.cresultset.next(cresultset.java:388)     [junit]  at com.datastax.cql.reprobugtest.simpleselect(reprobugtest.java:57)     [junit]      [junit]      [junit] test com.datastax.cql.reprobugtest failed <code> res = stmt.executequery('select bar from users');   res.next(); package com.datastax.cql; import java.sql.drivermanager; import java.sql.connection; import java.sql.resultset; import java.sql.sqlexception; import java.sql.statement; import org.junit.test; public class reprobugtest {          @test     public void simpleselect() throws exception {            connection connection = null;         resultset res;         statement stmt;         int colcount = 0;                  try {             class.forname('org.apache.cassandra.cql.jdbc.cassandradriver');                          // check create keyspace             connection = drivermanager.getconnection('jdbc:cassandra:root/root@127.0.0.1:9160/default');                  stmt = connection.createstatement();             try {               system.out.println('running drop ks statement');                 res = stmt.executequery('drop keyspace ks1');                 res.next();                              system.out.println('running create ks statement');               res = stmt.executequery('create keyspace ks1 with strategy_class =  'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=1');                 res.next();             } catch (sqlexception e) {                 if (e.getmessage().startswith('keyspace does not exist'))                  {                     res = stmt.executequery('create keyspace ks1 with strategy_class =  'org.apache.cassandra.locator.simplestrategy' and strategy_options:replication_factor=1');                   }              }                connection.close();                              // run test             connection = drivermanager.getconnection('jdbc:cassandra:root/root@127.0.0.1:9160/ks1');                  stmt = connection.createstatement();             system.out.print('running create cf statement');             res = stmt.executequery('create columnfamily users (key varchar primary key, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)');                 colcount = res.getmetadata().getcolumncount();             system.out.println(' -- column count: ' + colcount);              res.next();                          system.out.print('running insert statement');             res = stmt.executequery('insert into users (key, password) values ('user1', 'ch@nge')');               colcount = res.getmetadata().getcolumncount();             system.out.println(' -- column count: ' + colcount);              res.next();                          system.out.print('running select statement');             res = stmt.executequery('select bar from users');               colcount = res.getmetadata().getcolumncount();             system.out.println(' -- column count: ' + colcount);              res.getrow();             res.next();                              connection.close();                        } catch (sqlexception e) {             e.printstacktrace();         }     }         } <text> the following statement fails when used with a statement or preparedstatement error message here is a quick repro. showing that res.next() works with other statements but not select.  also notice that resultset.getmetadata().getcolumncount() always returns zero.   i noticed in the existing driver tests similar test cases, so not sure the issue. steps to run script",
        "label": 274
    },
    {
        "text": "dtest failure in json tools test testjson json tools test <description> example failure: http://cassci.datastax.com/job/cassandra-2.1_dtest/508/testreport/json_tools_test/testjson/json_tools_test/ stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/json_tools_test.py\", line 23, in json_tools_test     debug(\"version: \" + cluster.version()) \"cannot concatenate 'str' and 'instance' objects<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/json_tools_test.py', line 23, in json_tools_test     debug('version: ' + cluster.version()) 'cannot concatenate 'str' and 'instance' objects http://cassci.datastax.com/job/cassandra-2.1_dtest/508/testreport/json_tools_test/testjson/json_tools_test/<text> example failure: ",
        "label": 428
    },
    {
        "text": "dtest failure in user types test testusertypes test type as part of pkey <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1365/testreport/user_types_test/testusertypes/test_type_as_part_of_pkey error message regexp didn't match: 'partition key parts: name must be restricted as other parts are' not found in 'error from server: code=2200 [invalid query] message=\"cannot execute this query as it might involve data filtering and thus may have unpredictable performance. if you want to execute this query despite the performance unpredictability, use allow filtering\"' stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/user_types_test.py\", line 382, in test_type_as_part_of_pkey     assert_invalid(session, stmt, 'partition key parts: name must be restricted as other parts are')   file \"/home/automaton/cassandra-dtest/tools/assertions.py\", line 98, in assert_invalid     assert_exception(session, query, matching=matching, expected=expected)   file \"/home/automaton/cassandra-dtest/tools/assertions.py\", line 71, in assert_exception     _assert_exception(session.execute, query, matching=matching, expected=expected)   file \"/home/automaton/cassandra-dtest/tools/assertions.py\", line 60, in _assert_exception     assert_regexp_matches(str(e), matching)   file \"/usr/lib/python2.7/unittest/case.py\", line 1002, in assertregexpmatches     raise self.failureexception(msg)<stacktrace> <code> error message regexp didn't match: 'partition key parts: name must be restricted as other parts are' not found in 'error from server: code=2200 [invalid query] message='cannot execute this query as it might involve data filtering and thus may have unpredictable performance. if you want to execute this query despite the performance unpredictability, use allow filtering'' stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/user_types_test.py', line 382, in test_type_as_part_of_pkey     assert_invalid(session, stmt, 'partition key parts: name must be restricted as other parts are')   file '/home/automaton/cassandra-dtest/tools/assertions.py', line 98, in assert_invalid     assert_exception(session, query, matching=matching, expected=expected)   file '/home/automaton/cassandra-dtest/tools/assertions.py', line 71, in assert_exception     _assert_exception(session.execute, query, matching=matching, expected=expected)   file '/home/automaton/cassandra-dtest/tools/assertions.py', line 60, in _assert_exception     assert_regexp_matches(str(e), matching)   file '/usr/lib/python2.7/unittest/case.py', line 1002, in assertregexpmatches     raise self.failureexception(msg) http://cassci.datastax.com/job/trunk_dtest/1365/testreport/user_types_test/testusertypes/test_type_as_part_of_pkey<text> example failure: ",
        "label": 25
    },
    {
        "text": "add java rmi server randomid and java home to system properties vtable  follow up on cassandra  <description> add java.rmi.server.randomid to the systems_table  system.getproperty(\"java.home\")   to be switched to  system.getenv(\"java_home\")     oracle jvm might use the jre which do not contain jmap. <stacktrace> <code> add java.rmi.server.randomid to the systems_table  system.getproperty('java.home')   to be switched to  system.getenv('java_home')     oracle jvm might use the jre which do not contain jmap. <text> ",
        "label": 165
    },
    {
        "text": "create a jenkins  devbranch  job for each of the different unit test types  as is done for branches <description> <stacktrace> <code> <text> ",
        "label": 347
    },
    {
        "text": "filter out columnfamily rows that aren't part of the query  using a keyrange  <description> currently, when running a mapreduce job against data in a cassandra data store, it reads through all the data for a particular columnfamily. this could be optimized to only read through those rows that have to do with the query. it's a small change but wanted to put it in jira so that it didn't fall through the cracks.<stacktrace> <code> <text> currently, when running a mapreduce job against data in a cassandra data store, it reads through all the data for a particular columnfamily. this could be optimized to only read through those rows that have to do with the query. it's a small change but wanted to put it in jira so that it didn't fall through the cracks.",
        "label": 347
    },
    {
        "text": "add rowlevel support to secondary index api and methods need to throw exceptions <description> the new secondary index api needs to throw ioexception on some of the methods (where they're missing). for index types that index an entire row (like lucene) we need to change the api to handle one instance of the index per columnfamily. this can be done by adding a percolumnsecondary and perrowsecondaryindex base classes. when the index manager sees this it will add all columns of the same index class to the same index instance. and on search it will send the entire row to the class vs updating each column.<stacktrace> <code> <text> the new secondary index api needs to throw ioexception on some of the methods (where they're missing). for index types that index an entire row (like lucene) we need to change the api to handle one instance of the index per columnfamily. this can be done by adding a percolumnsecondary and perrowsecondaryindex base classes. when the index manager sees this it will add all columns of the same index class to the same index instance. and on search it will send the entire row to the class vs updating each column.",
        "label": 521
    },
    {
        "text": "debuggablescheduledthreadpoolexecutor only schedules a task once <description> debuggablescheduledthreadpoolexecutor only schedules a task exactly once, instead of periodically. this affects scheduled flushers and periodic hints delivery.<stacktrace> <code> <text> debuggablescheduledthreadpoolexecutor only schedules a task exactly once, instead of periodically. this affects scheduled flushers and periodic hints delivery.",
        "label": 274
    },
    {
        "text": "allow custom time format on cqlsh copy to <description> when executing a copy to from cqlsh, the user is currently has no control over the format of exported timestamp columns. if the user has indicated a time_format in their cqlshrc file, that format will be used. otherwise, the system default format will be used. the problem comes into play when the timestamp format used on a copy to, is not valid when the data is sent back into cassandra with a copy from. for instance, if a user has time_format = %y-%m-%d %h:%m:%s%z specified in their cqlshrc, copy to will format timestamp columns like this: userid|posttime|postcontent  0|2015-03-14 14:59:00cdt|rtyeryerweh  0|2015-03-14 14:58:00cdt|sdfsdfsdgfjdsgojr  0|2015-03-12 14:27:00cdt|sdgfjdsgojr executing a copy from on that same file will produce an \"unable to coerce to formatted date(long)\" error. right now, the only way to change the way timestamps are formatted is to exit cqlsh, modify the time_format property in cqlshrc, and restart cqlsh. the ability to specify a copy option of time_format with a python strftime format, would allow the user to quickly alter the timestamp format for export, without reconfiguring cqlsh. aploetz@cqlsh:stackoverflow> copy posts1 to '/home/aploetz/posts1.csv' with delimiter='|' and header=true and time_format='%y-%m-%d %h:%m:%s%z;<stacktrace> <code> userid|posttime|postcontent  0|2015-03-14 14:59:00cdt|rtyeryerweh  0|2015-03-14 14:58:00cdt|sdfsdfsdgfjdsgojr  0|2015-03-12 14:27:00cdt|sdgfjdsgojr aploetz@cqlsh:stackoverflow> copy posts1 to '/home/aploetz/posts1.csv' with delimiter='|' and header=true and time_format='%y-%m-%d %h:%m:%s%z;<text> when executing a copy to from cqlsh, the user is currently has no control over the format of exported timestamp columns. if the user has indicated a time_format in their cqlshrc file, that format will be used. otherwise, the system default format will be used. the problem comes into play when the timestamp format used on a copy to, is not valid when the data is sent back into cassandra with a copy from. for instance, if a user has time_format = %y-%m-%d %h:%m:%s%z specified in their cqlshrc, copy to will format timestamp columns like this: executing a copy from on that same file will produce an 'unable to coerce to formatted date(long)' error. right now, the only way to change the way timestamps are formatted is to exit cqlsh, modify the time_format property in cqlshrc, and restart cqlsh. the ability to specify a copy option of time_format with a python strftime format, would allow the user to quickly alter the timestamp format for export, without reconfiguring cqlsh. ",
        "label": 2
    },
    {
        "text": "add rss support for cassandra blog <description> it would be convenient to add rss support to cassandra blog: http://cassandra.apache.org/blog/2018/08/07/faster_streaming_in_cassandra.html and maybe also for other resources like new versions, but this ticket is about blog.   from: scott andreas sent: wednesday, august 08, 2018 6:53 pm to: dev@cassandra.apache.org subject: re: apache cassandra blog is now live   please feel free to file a ticket (label: documentation and website).   it looks like jekyll, the static site generator used to build the website, has a plugin that generates atom feeds if someone would like to work on adding one: https://github.com/jekyll/jekyll-feed<stacktrace> <code> http://cassandra.apache.org/blog/2018/08/07/faster_streaming_in_cassandra.html sent: wednesday, august 08, 2018 6:53 pm to: dev@cassandra.apache.org <text> it would be convenient to add rss support to cassandra blog: and maybe also for other resources like new versions, but this ticket is about blog.   from: scott andreas subject: re: apache cassandra blog is now live   please feel free to file a ticket (label: documentation and website).   it looks like jekyll, the static site generator used to build the website, has a plugin that generates atom feeds if someone would like to work on adding one: https://github.com/jekyll/jekyll-feed",
        "label": 238
    },
    {
        "text": "dtest failure in materialized views test testmaterializedviews clustering column test <description> single failure, but might be worth looking into to see if it repros at all. http://cassci.datastax.com/job/cassandra-3.0_dtest/669/testreport/materialized_views_test/testmaterializedviews/clustering_column_test failed on cassci build cassandra-3.0_dtest #669<stacktrace> <code> http://cassci.datastax.com/job/cassandra-3.0_dtest/669/testreport/materialized_views_test/testmaterializedviews/clustering_column_test failed on cassci build cassandra-3.0_dtest #669<text> single failure, but might be worth looking into to see if it repros at all. ",
        "label": 428
    },
    {
        "text": "lwts keep failing in trunk after immutable refactor <description> in the paxosstate, the original assert check is in the form of: assert promised.update.metadata() == accepted.update.metadata() && accepted.update.metadata() == mostrecentcommit.update.metadata(); however, after the change to make tablemetadata immutable this no longer works as these instances are not necessarily the same (or never). this causes the lwts to fail although they're still correctly targetting the same table. from irc: <pcmanus> it's a bug alright. though really, the assertion should be on the metadata ids, cause tablemetadata#equals does more than what we want.  <pcmanus> that is, replacing by .equals() is not ok. that would reject throw on any change to a table metadata, while the spirit of the assumption was to sanity check both update were on the same table.<stacktrace> <code> assert promised.update.metadata() == accepted.update.metadata() && accepted.update.metadata() == mostrecentcommit.update.metadata(); <text> in the paxosstate, the original assert check is in the form of: however, after the change to make tablemetadata immutable this no longer works as these instances are not necessarily the same (or never). this causes the lwts to fail although they're still correctly targetting the same table. from irc: <pcmanus> it's a bug alright. though really, the assertion should be on the metadata ids, cause tablemetadata#equals does more than what we want.  <pcmanus> that is, replacing by .equals() is not ok. that would reject throw on any change to a table metadata, while the spirit of the assumption was to sanity check both update were on the same table.",
        "label": 341
    },
    {
        "text": "cas may return false but still commit the insert <description> if a paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. paxos guarantees that we'll agree on \"a\" value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it. in particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer. currently, if a proposer a proposes some update u but it is rejected, a will sleep a bit and retry u. but if u was accepted by at least one acceptor, some other proposer b might replay u, succeed and commit it. if a does its retry after that happens, he will prepare, check the condition, and probably find that the conditions don't apply anymore since u has been committed already. it will thus return false, even though u has been in fact committed. unfortunately i'm not sure there is an easy way for a proposer whose propose fails to know if the update will prevail or not eventually. which mean the only acceptable solution i can see would be to return to the user \"i don't know\" (through some exception for instance). which is annoying because having a proposal rejected won't be an extremely rare occurrence, even with relatively light contention, and returning \"i don't know\" often is a bit unfriendly.<stacktrace> <code> <text> if a paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. paxos guarantees that we'll agree on 'a' value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it. in particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer. currently, if a proposer a proposes some update u but it is rejected, a will sleep a bit and retry u. but if u was accepted by at least one acceptor, some other proposer b might replay u, succeed and commit it. if a does its retry after that happens, he will prepare, check the condition, and probably find that the conditions don't apply anymore since u has been committed already. it will thus return false, even though u has been in fact committed. unfortunately i'm not sure there is an easy way for a proposer whose propose fails to know if the update will prevail or not eventually. which mean the only acceptable solution i can see would be to return to the user 'i don't know' (through some exception for instance). which is annoying because having a proposal rejected won't be an extremely rare occurrence, even with relatively light contention, and returning 'i don't know' often is a bit unfriendly.",
        "label": 274
    },
    {
        "text": "using a cassandra seed doesn't allow a new cassandra node to bootstrap <description> if you spin up a cassandra 2.0 cluster with some seeds, and then attempt to attach a cassandra 2.1 node to it, you get the following message: outboundtcpconnection.java:429 - handshaking version with /10.24.0.10 turning on debug, you get a few additional messages: debug [write-/(ip)] messagingservice.java:789 - setting version 7 for /10.24.0.10 debug [write-/(ip)] outboundtcpconnection.java:369 - target max version is 7; will reconnect with that version however, the code never reconnects. see the comments as to why.<stacktrace> <code> outboundtcpconnection.java:429 - handshaking version with /10.24.0.10 debug [write-/(ip)] messagingservice.java:789 - setting version 7 for /10.24.0.10 debug [write-/(ip)] outboundtcpconnection.java:369 - target max version is 7; will reconnect with that version <text> if you spin up a cassandra 2.0 cluster with some seeds, and then attempt to attach a cassandra 2.1 node to it, you get the following message: turning on debug, you get a few additional messages: however, the code never reconnects. see the comments as to why.",
        "label": 85
    },
    {
        "text": "cqlsh describe needs to show 'sstable compression'  '' <description> for uncompressed tables cqlsh describe schema should show \"and compression = {'sstable_compression': ''} \" otherwise when you replay the schema you get the default of lz4.<stacktrace> <code> <text> for uncompressed tables cqlsh describe schema should show 'and compression = ' otherwise when you replay the schema you get the default of lz4.",
        "label": 538
    },
    {
        "text": "incorrect complexcolumndata hashcode implementation <description> i went through some of the logs from cassandra-13175 and one of the more serious issues that we should address seem to be the complexcolumndata.hashcode() implementation. as objects.hashcode is not using deep hashing for array arguments, hashed will be based on the identity instead of the array's content. see patch for simple fix.<stacktrace> <code> <text> i went through some of the logs from cassandra-13175 and one of the more serious issues that we should address seem to be the complexcolumndata.hashcode() implementation. as objects.hashcode is not using deep hashing for array arguments, hashed will be based on the identity instead of the array's content. see patch for simple fix.",
        "label": 507
    },
    {
        "text": "indexoutofboundsexception thrown during repair <description> i was running repair command with moderate read and write load at the same time. and i found tens of indexoutofboundsexception in system log as follows: error [thread-6056] 2013-05-22 14:47:59,416 cassandradaemon.java (line132) exception in thread thread[thread-6056,5,main]  java.lang.indexoutofboundsexception  at sun.nio.ch.channelinputstream.read(channelinputstream.java:75)  at org.apache.cassandra.streaming.compress.compressedinputstream$reader.runmaythrow(compressedinputstream.java:151)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)  at java.lang.thread.run(thread.java:662) i read the source code of compressedinputstream.java and found there surely will throw indexoutofboundsexception in the following situation: compressedinputstream.java // part of compressedinputstream.java start from line 139         protected void runmaythrow() throws exception         {             byte[] compressedwithcrc;             while (chunks.hasnext())             {                 compressionmetadata.chunk chunk = chunks.next();                 int readlength = chunk.length + 4; // read with crc                 compressedwithcrc = new byte[readlength];                 int bufferread = 0;                 while (bufferread < readlength)                     bufferread += source.read(compressedwithcrc, bufferread, readlength - bufferread);                 databuffer.put(compressedwithcrc);             }         } if read function read nothing because the end of the stream has been reached, it will return -1, thus bufferread can be negetive. in the next circle, read function will throw indexoutofboundsexception because bufferread is negetive.<stacktrace> error [thread-6056] 2013-05-22 14:47:59,416 cassandradaemon.java (line132) exception in thread thread[thread-6056,5,main]  java.lang.indexoutofboundsexception  at sun.nio.ch.channelinputstream.read(channelinputstream.java:75)  at org.apache.cassandra.streaming.compress.compressedinputstream$reader.runmaythrow(compressedinputstream.java:151)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)  at java.lang.thread.run(thread.java:662) <code> // part of compressedinputstream.java start from line 139         protected void runmaythrow() throws exception         {             byte[] compressedwithcrc;             while (chunks.hasnext())             {                 compressionmetadata.chunk chunk = chunks.next();                 int readlength = chunk.length + 4; // read with crc                 compressedwithcrc = new byte[readlength];                 int bufferread = 0;                 while (bufferread < readlength)                     bufferread += source.read(compressedwithcrc, bufferread, readlength - bufferread);                 databuffer.put(compressedwithcrc);             }         } <text> i was running repair command with moderate read and write load at the same time. and i found tens of indexoutofboundsexception in system log as follows: i read the source code of compressedinputstream.java and found there surely will throw indexoutofboundsexception in the following situation: if read function read nothing because the end of the stream has been reached, it will return -1, thus bufferread can be negetive. in the next circle, read function will throw indexoutofboundsexception because bufferread is negetive.",
        "label": 321
    },
    {
        "text": "assertionerror in repair <description> i increased replication factor and run repair, some token ranges were repaired okay, but one failed with:  info 13:03:52,234 repair #dd7937a0-ebab-11e2-ba07-c38e7fba9d51 session completed successfully  error 13:03:52,343 exception in thread thread[validationexecutor:2,1,main]  java.lang.assertionerror: (max(9099058114996150811),max(-5486100704702537010)]  at org.apache.cassandra.db.datarange.<init>(datarange.java:50)  at org.apache.cassandra.db.datarange.forkeyrange(datarange.java:74)  at org.apache.cassandra.io.sstable.sstablereader.getscanner(sstablereade  r.java:1033)  at org.apache.cassandra.db.compaction.abstractcompactionstrategy.getscan  ners(abstractcompactionstrategy.java:214)  at org.apache.cassandra.db.compaction.compactionmanager$validationcompac  tioniterable.<init>(compactionmanager.java:751)  at org.apache.cassandra.db.compaction.compactionmanager.dovalidationcomp  action(compactionmanager.java:657)<stacktrace> info 13:03:52,234 repair #dd7937a0-ebab-11e2-ba07-c38e7fba9d51 session completed successfully  error 13:03:52,343 exception in thread thread[validationexecutor:2,1,main]  java.lang.assertionerror: (max(9099058114996150811),max(-5486100704702537010)]  at org.apache.cassandra.db.datarange.<init>(datarange.java:50)  at org.apache.cassandra.db.datarange.forkeyrange(datarange.java:74)  at org.apache.cassandra.io.sstable.sstablereader.getscanner(sstablereade  r.java:1033)  at org.apache.cassandra.db.compaction.abstractcompactionstrategy.getscan  ners(abstractcompactionstrategy.java:214)  at org.apache.cassandra.db.compaction.compactionmanager$validationcompac  tioniterable.<init>(compactionmanager.java:751)  at org.apache.cassandra.db.compaction.compactionmanager.dovalidationcomp  action(compactionmanager.java:657)<code> <text> i increased replication factor and run repair, some token ranges were repaired okay, but one failed with: ",
        "label": 520
    },
    {
        "text": "add cdc to describe table <description> currently we do not output cdc with describe table, but should include that for 3.8+ tables.<stacktrace> <code> <text> currently we do not output cdc with describe table, but should include that for 3.8+ tables.",
        "label": 508
    },
    {
        "text": "use  allow list  or  safe list  instead of the term  whitelist  <description> language matters. i'd like to remove all references in apache airflow to whitelist or black list, and the cassandra python api has some that we can't easily remove. the recent global events have made this even more relevant, but this has been on my radar for a while now. here is a well written article for why i think it matters https://www.ncsc.gov.uk/blog-post/terminology-its-not-black-and-white it's fairly common to say whitelisting and blacklisting to describe desirable and undesirable things in cyber security. however, there's an issue with the terminology. it only makes sense if you equate white with 'good, permitted, safe' and black with 'bad, dangerous, forbidden'. there are some obvious problems with this. my exposure to is via the python api where there is the cassandra.pollicies.whitelistroundrobinpolicy class. i propose that this be renamed to allowlistroundrobinpolicy instead. i do not know if there are other references.<stacktrace> <code> https://www.ncsc.gov.uk/blog-post/terminology-its-not-black-and-white <text> language matters. i'd like to remove all references in apache airflow to whitelist or black list, and the cassandra python api has some that we can't easily remove. the recent global events have made this even more relevant, but this has been on my radar for a while now. here is a well written article for why i think it matters it's fairly common to say whitelisting and blacklisting to describe desirable and undesirable things in cyber security. however, there's an issue with the terminology. it only makes sense if you equate white with 'good, permitted, safe' and black with 'bad, dangerous, forbidden'. there are some obvious problems with this. my exposure to is via the python api where there is the cassandra.pollicies.whitelistroundrobinpolicy class. i propose that this be renamed to allowlistroundrobinpolicy instead. i do not know if there are other references.",
        "label": 474
    },
    {
        "text": "cqlpagingrecordreader throws illegalstateexception <description> getting the following exception when running a spark job that does not specify cassandra.input.page.row.size: 14/05/08 14:30:43 error executor.executor: exception in task id 12 java.lang.illegalstateexception: optional.get() cannot be called on an absent value         at com.google.common.base.absent.get(absent.java:47)         at org.apache.cassandra.hadoop.cql3.cqlpagingrecordreader.initialize(cqlpagingrecordreader.java:120)         at com.tuplejump.calliope.cql3.cql3cassandrardd$$anon$1.<init>(cql3cassandrardd.scala:65)         at com.tuplejump.calliope.cql3.cql3cassandrardd.compute(cql3cassandrardd.scala:53)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.rdd.mappedrdd.compute(mappedrdd.scala:31)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.scheduler.resulttask.runtask(resulttask.scala:109)         at org.apache.spark.scheduler.task.run(task.scala:53)         at org.apache.spark.executor.executor$taskrunner$$anonfun$run$1.apply$mcv$sp(executor.scala:213)         at org.apache.spark.deploy.sparkhadooputil.runasuser(sparkhadooputil.scala:49)         at org.apache.spark.executor.executor$taskrunner.run(executor.scala:178)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) 14/05/08 14:30:43 error executor.executor: exception in task id 21 java.lang.illegalstateexception: optional.get() cannot be called on an absent value         at com.google.common.base.absent.get(absent.java:47)         at org.apache.cassandra.hadoop.cql3.cqlpagingrecordreader.initialize(cqlpagingrecordreader.java:120)         at com.tuplejump.calliope.cql3.cql3cassandrardd$$anon$1.<init>(cql3cassandrardd.scala:65)         at com.tuplejump.calliope.cql3.cql3cassandrardd.compute(cql3cassandrardd.scala:53)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.rdd.mappedrdd.compute(mappedrdd.scala:31)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.scheduler.resulttask.runtask(resulttask.scala:109)         at org.apache.spark.scheduler.task.run(task.scala:53)         at org.apache.spark.executor.executor$taskrunner$$anonfun$run$1.apply$mcv$sp(executor.scala:213)         at org.apache.spark.deploy.sparkhadooputil.runasuser(sparkhadooputil.scala:49)         at org.apache.spark.executor.executor$taskrunner.run(executor.scala:178)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) reason why is cqlpagingrecordreader catching the wrong exception type. patch attached.<stacktrace> 14/05/08 14:30:43 error executor.executor: exception in task id 12 java.lang.illegalstateexception: optional.get() cannot be called on an absent value         at com.google.common.base.absent.get(absent.java:47)         at org.apache.cassandra.hadoop.cql3.cqlpagingrecordreader.initialize(cqlpagingrecordreader.java:120)         at com.tuplejump.calliope.cql3.cql3cassandrardd$$anon$1.<init>(cql3cassandrardd.scala:65)         at com.tuplejump.calliope.cql3.cql3cassandrardd.compute(cql3cassandrardd.scala:53)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.rdd.mappedrdd.compute(mappedrdd.scala:31)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.scheduler.resulttask.runtask(resulttask.scala:109)         at org.apache.spark.scheduler.task.run(task.scala:53)         at org.apache.spark.executor.executor$taskrunner$$anonfun$run$1.apply$mcv$sp(executor.scala:213)         at org.apache.spark.deploy.sparkhadooputil.runasuser(sparkhadooputil.scala:49)         at org.apache.spark.executor.executor$taskrunner.run(executor.scala:178)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) 14/05/08 14:30:43 error executor.executor: exception in task id 21 java.lang.illegalstateexception: optional.get() cannot be called on an absent value         at com.google.common.base.absent.get(absent.java:47)         at org.apache.cassandra.hadoop.cql3.cqlpagingrecordreader.initialize(cqlpagingrecordreader.java:120)         at com.tuplejump.calliope.cql3.cql3cassandrardd$$anon$1.<init>(cql3cassandrardd.scala:65)         at com.tuplejump.calliope.cql3.cql3cassandrardd.compute(cql3cassandrardd.scala:53)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.rdd.mappedrdd.compute(mappedrdd.scala:31)         at org.apache.spark.rdd.rdd.computeorreadcheckpoint(rdd.scala:241)         at org.apache.spark.rdd.rdd.iterator(rdd.scala:232)         at org.apache.spark.scheduler.resulttask.runtask(resulttask.scala:109)         at org.apache.spark.scheduler.task.run(task.scala:53)         at org.apache.spark.executor.executor$taskrunner$$anonfun$run$1.apply$mcv$sp(executor.scala:213)         at org.apache.spark.deploy.sparkhadooputil.runasuser(sparkhadooputil.scala:49)         at org.apache.spark.executor.executor$taskrunner.run(executor.scala:178)         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)         at java.lang.thread.run(thread.java:744) <code> <text> getting the following exception when running a spark job that does not specify cassandra.input.page.row.size: reason why is cqlpagingrecordreader catching the wrong exception type. patch attached.",
        "label": 87
    },
    {
        "text": "cqlsh support for user types <description> we need cqlsh support for several things:  1. autocomplete for update/insert/select  2. autocomplete for alter type/create type/drop type  3. proper decoding of usertype values (currently showing the encoded blob)  4. support usertypes in describe  5. separate describe types|type <name> cmds<stacktrace> <code> <text> we need cqlsh support for several things:  1. autocomplete for update/insert/select  2. autocomplete for alter type/create type/drop type  3. proper decoding of usertype values (currently showing the encoded blob)  4. support usertypes in describe  5. separate describe types|type <name> cmds",
        "label": 362
    },
    {
        "text": "improve permissions to allow control over creation removal listing of keyspaces <description> we'd like to improve resources/permissions so that they can be applied to the global scope, instead of just individual keyspaces. iauthority currently only has one concept of a resource that it can authorize for: a keyspace. at the very least, this ticket needs to deal with one additional resource: \"the keyspace list\". these resources should be mapped into a hierarchy, and an object representing the path to the resource will be passed to iauthority. a resource hierarchy to represent all possible resources in cassandra might look like: /cassandra/<cluster_name>/keyspaces/<ks_name>/...  in table form: resource   checked perms   explanation /cassandra/   n/a   separates cassandra-internal resources from resources that might be provided by plugins. <cluster_name>/   n/a   organizations might have many clusters keyspaces/   read, write   the list of keyspaces: read/write for this resource mean the ability to view/modify the list of keyspaces. <ks_name>/   read, write, read_value, write_value   an individual keyspace: read/write mean the ability to view/modify the list of column families. since this is the last entry in the current hierarchy, read/write_value apply recursively to ancestor data of this keyspace. over time cassandra may add additional authorize calls for resources higher or lower in the chain, which iauthority backends can choose to ignore, but this initial patch will only make authorize calls for the keyspaces list, and individual keyspaces. as authorize calls are added for child resources like <cf_name>/, the read/write_value permissions will move to the lowest checked level, and will be deprecated at higher levels. (note that /cassandra/ and <cluster_name>/ will not yet be checked for permissions via a call to iauthority.authorize, so while it would be possible for an iauthority backend to store permissions for these top level resources, they will only be able to deny access when a user attempts to access an ancestor resource.)<stacktrace> <code> <text> we'd like to improve resources/permissions so that they can be applied to the global scope, instead of just individual keyspaces. iauthority currently only has one concept of a resource that it can authorize for: a keyspace. at the very least, this ticket needs to deal with one additional resource: 'the keyspace list'. these resources should be mapped into a hierarchy, and an object representing the path to the resource will be passed to iauthority. a resource hierarchy to represent all possible resources in cassandra might look like: /cassandra/<cluster_name>/keyspaces/<ks_name>/...  in table form: over time cassandra may add additional authorize calls for resources higher or lower in the chain, which iauthority backends can choose to ignore, but this initial patch will only make authorize calls for the keyspaces list, and individual keyspaces. as authorize calls are added for child resources like <cf_name>/, the read/write_value permissions will move to the lowest checked level, and will be deprecated at higher levels. (note that /cassandra/ and <cluster_name>/ will not yet be checked for permissions via a call to iauthority.authorize, so while it would be possible for an iauthority backend to store permissions for these top level resources, they will only be able to deny access when a user attempts to access an ancestor resource.)",
        "label": 169
    },
    {
        "text": "possible deadlock for counter mutations <description> storageproxy.applycountermutation is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. if there are more than a few concurrent mutations, this can lead to deadlock.<stacktrace> <code> <text> storageproxy.applycountermutation is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. if there are more than a few concurrent mutations, this can lead to deadlock.",
        "label": 297
    },
    {
        "text": "support thrift ssl socket <description> thrift has supported ssl encryption for a while now (thrift-106); we should allow configuring that in cassandra.yaml<stacktrace> <code> <text> thrift has supported ssl encryption for a while now (thrift-106); we should allow configuring that in cassandra.yaml",
        "label": 232
    },
    {
        "text": "cql metadata has inconsistent schema nomenclature <description> the dumped object below shows that the default_name_type and the default_value_type are referenced inconsistently .. default_name_type should probably use the shortened version like everything else. \u2014 !ruby/object:cassandracql::thrift::cqlresult   rows: !ruby/object:cassandracql::thrift::cqlrow  columns: !ruby/object:cassandracql::thrift::column  name: id  timestamp: -1  value: test string !ruby/object:cassandracql::thrift::column  name: test_column  timestamp: 1320097088551000  value: test  key: test string  schema: !ruby/object:cassandracql::thrift::cqlmetadata   default_name_type: org.apache.cassandra.db.marshal.utf8type  default_value_type: utf8type  name_types:   id: asciitype  value_types:   id: asciitype  test_column: utf8type  type: 1<stacktrace> <code> <text> the dumped object below shows that the default_name_type and the default_value_type are referenced inconsistently .. default_name_type should probably use the shortened version like everything else. - !ruby/object:cassandracql::thrift::cqlresult   rows:",
        "label": 274
    },
    {
        "text": " patch  estimatedhistogram doesn't overide equals correctly <description> estimatedhistogram declares equals with an estimatedhistogram parameter, instead of object, thus only working correctly for statically typed estimatedhistogram references. fixed to take object parm.<stacktrace> <code> <text> estimatedhistogram declares equals with an estimatedhistogram parameter, instead of object, thus only working correctly for statically typed estimatedhistogram references. fixed to take object parm.",
        "label": 139
    },
    {
        "text": " patch  help out gc  by making inner classes static where possible <description> by making inner classes static, the parent class can go out of scope and be gc'ed even if the inner class does not. simplifies gc logic, and makes it easier to process.<stacktrace> <code> <text> by making inner classes static, the parent class can go out of scope and be gc'ed even if the inner class does not. simplifies gc logic, and makes it easier to process.",
        "label": 139
    },
    {
        "text": "duplicate columns in selection causes assertionerror <description> prior to cassandra-9532, unaliased duplicate fields in a selection would be silently ignored. now, they trigger a server side exception and an unfriendly error response, which we should clean up. duplicate columns with aliases are not affected. create keyspace ks with replication = {'class': 'simplestrategy', 'replication_factor': 1}; create table ks.t1 (k int primary key, v int); insert into ks.t2 (k, v) values (0, 0); select k, v from ks.t2; select k, v, v as other_v from ks.t2; select k, v, v from ks.t2; the final statement results in this error response & server side stacktrace: servererror: <errormessage code=0000 [server error] message=\"java.lang.assertionerror\"> error 13:01:30 unexpected exception during request; channel = [id: 0x44d22e61, /127.0.0.1:39463 => /127.0.0.1:9042] java.lang.assertionerror: null         at org.apache.cassandra.cql3.resultset.addrow(resultset.java:63) ~[main/:na]         at org.apache.cassandra.cql3.statements.selection$resultsetbuilder.build(selection.java:355) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1226) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:299) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:238) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:67) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:238) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:260) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:119) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:439) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:335) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] this issue also presents on the head of the 2.2 branch and on 2.0.16. however, the prior behaviour is different on both of those branches.  in the 2.0 line prior to cassandra-9532, duplicate columns would actually be included in the results, as opposed to being silently dropped as per 2.1.x  in 2.2, the assertion error seen above precedes cassandra-9532 and is also triggered for both aliased and unaliased duplicate columns.<stacktrace> servererror: <errormessage code=0000 [server error] message='java.lang.assertionerror'> error 13:01:30 unexpected exception during request; channel = [id: 0x44d22e61, /127.0.0.1:39463 => /127.0.0.1:9042] java.lang.assertionerror: null         at org.apache.cassandra.cql3.resultset.addrow(resultset.java:63) ~[main/:na]         at org.apache.cassandra.cql3.statements.selection$resultsetbuilder.build(selection.java:355) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.process(selectstatement.java:1226) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.processresults(selectstatement.java:299) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:238) ~[main/:na]         at org.apache.cassandra.cql3.statements.selectstatement.execute(selectstatement.java:67) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:238) ~[main/:na]         at org.apache.cassandra.cql3.queryprocessor.process(queryprocessor.java:260) ~[main/:na]         at org.apache.cassandra.transport.messages.querymessage.execute(querymessage.java:119) ~[main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:439) [main/:na]         at org.apache.cassandra.transport.message$dispatcher.channelread0(message.java:335) [main/:na]         at io.netty.channel.simplechannelinboundhandler.channelread(simplechannelinboundhandler.java:105) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:333) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext.access$700(abstractchannelhandlercontext.java:32) [netty-all-4.0.23.final.jar:4.0.23.final]         at io.netty.channel.abstractchannelhandlercontext$8.run(abstractchannelhandlercontext.java:324) [netty-all-4.0.23.final.jar:4.0.23.final]         at java.util.concurrent.executors$runnableadapter.call(executors.java:511) [na:1.8.0_45]         at org.apache.cassandra.concurrent.abstracttracingawareexecutorservice$futuretask.run(abstracttracingawareexecutorservice.java:164) [main/:na]         at org.apache.cassandra.concurrent.sepworker.run(sepworker.java:105) [main/:na]         at java.lang.thread.run(thread.java:745) [na:1.8.0_45] <code> create keyspace ks with replication = {'class': 'simplestrategy', 'replication_factor': 1}; create table ks.t1 (k int primary key, v int); insert into ks.t2 (k, v) values (0, 0); select k, v from ks.t2; select k, v, v as other_v from ks.t2; select k, v, v from ks.t2; <text> prior to cassandra-9532, unaliased duplicate fields in a selection would be silently ignored. now, they trigger a server side exception and an unfriendly error response, which we should clean up. duplicate columns with aliases are not affected. the final statement results in this error response & server side stacktrace: this issue also presents on the head of the 2.2 branch and on 2.0.16. however, the prior behaviour is different on both of those branches.  in the 2.0 line prior to cassandra-9532, duplicate columns would actually be included in the results, as opposed to being silently dropped as per 2.1.x  in 2.2, the assertion error seen above precedes cassandra-9532 and is also triggered for both aliased and unaliased duplicate columns.",
        "label": 474
    },
    {
        "text": "allow lwt operation on static column with only partition keys <description> schema create table if not exists achilles_embedded.entity_with_static_column( id bigint, uuid uuid, static_col text static, value text, primary key(id, uuid)); when trying to prepare the following query delete static_col from achilles_embedded.entity_with_static_column where id=:id_eq if static_col=:static_col; i got the error delete statements must restrict all primary key columns with equality relations in order to use if conditions, but column 'uuid' is not restricted since the mutation only impacts the static column and the cas check is on the static column, it makes sense to provide only partition key<stacktrace> <code> create table if not exists achilles_embedded.entity_with_static_column( id bigint, uuid uuid, static_col text static, value text, primary key(id, uuid)); delete static_col from achilles_embedded.entity_with_static_column where id=:id_eq if static_col=:static_col; create table if not exists achilles_embedded.entity_with_static_column( id bigint, uuid uuid, static_col text static, value text, primary key(id, uuid)); delete static_col from achilles_embedded.entity_with_static_column where id=:id_eq if static_col=:static_col; <text> schema when trying to prepare the following query i got the error delete statements must restrict all primary key columns with equality relations in order to use if conditions, but column 'uuid' is not restricted since the mutation only impacts the static column and the cas check is on the static column, it makes sense to provide only partition key",
        "label": 98
    },
    {
        "text": "document how setting internode send recv buff size in bytes might give you worse performance <description> by setting so_sndbuf (or rcvbuf) (which is what internode_ {send|recv}buff_size_in_bytes does), the actual buffers will be limited by net.core.wmem_max and net.core.rmem_max which defaults to 131071 on debian, but if you dont set it, it will use net.ipv4.tcp(w|r)mem which by default has a max value of 4194304.    so, if you set internode_{send|recv} _buff_size_in_bytes, you will most likely not get the buffer size you want unless you have also tweaked your os. patch adds a few comments about this. unsure if it should go in the config file, but it should at least be mentioned somewhere see:  /proc/sys/net/core/wmem_max  /proc/sys/net/core/rmem_max  /proc/sys/net/ipv4/tcp_wmem  /proc/sys/net/ipv4/tcp_rmem  man tcp<stacktrace> <code> see:  /proc/sys/net/core/wmem_max  /proc/sys/net/core/rmem_max  /proc/sys/net/ipv4/tcp_wmem  /proc/sys/net/ipv4/tcp_rmem  man tcp<text> by setting so_sndbuf (or rcvbuf) (which is what internode_ _buff_size_in_bytes, you will most likely not get the buffer size you want unless you have also tweaked your os. patch adds a few comments about this. unsure if it should go in the config file, but it should at least be mentioned somewhere ",
        "label": 321
    },
    {
        "text": "when dropping a keyspace you're currently authenticated to  might be nice to de authenticate upon completion <description> i found that when i'm authenticated to mykeyspace, then do 'drop keyspace mykeyspace;', i'm still authenticated to it. it's trivial i know, but seems reasonable to unauthenticate from it.<stacktrace> <code> <text> i found that when i'm authenticated to mykeyspace, then do 'drop keyspace mykeyspace;', i'm still authenticated to it. it's trivial i know, but seems reasonable to unauthenticate from it.",
        "label": 259
    },
    {
        "text": "strange result of several list updates in a single request <description> let's assume that we have a row with the 'listcolumn' column and value {1,2,3,4}.  for me it looks logical to expect that the following two pieces of code will ends up with the same result but it isn't so.  code1: update t set listcolumn[2] = 7, listcolumn[2] = 8  where id = 1; expected result: listcolumn={1,2,8,4}   actual result: listcolumn={1,2,7,8,4} code2: update t set listcolumn[2] = 7  where id = 1; update t set listcolumn[2] = 8  where id = 1; expected result: listcolumn={1,2,8,4}   actual result: listcolumn={1,2,8,4} so the question is why code1 and code2 give different results?  looks like code1 should give the same result as code2.<stacktrace> <code> update t set listcolumn[2] = 7, listcolumn[2] = 8  where id = 1; update t set listcolumn[2] = 7  where id = 1; update t set listcolumn[2] = 8  where id = 1; expected result: listcolumn={1,2,8,4}   actual result: listcolumn={1,2,7,8,4} expected result: listcolumn={1,2,8,4}   actual result: listcolumn={1,2,8,4} <text> let's assume that we have a row with the 'listcolumn' column and value {1,2,3,4}.  for me it looks logical to expect that the following two pieces of code will ends up with the same result but it isn't so.  code1: code2: so the question is why code1 and code2 give different results?  looks like code1 should give the same result as code2.",
        "label": 69
    },
    {
        "text": "eliminate temporary object  allocations in columndefinition hashcode <description> columndefinition::hashcode currently calls objects.hashcode(object...)  this triggers the allocation of a short lived object[] which is not eliminated by escapeanalysis. i have implemented a fix by inlining the hashcode logic and also added a caching hashcode field. this improved performance on the read workload.  fix is available here:  https://github.com/nitsanw/cassandra/tree/objects-hashcode-fix<stacktrace> <code> <text> columndefinition::hashcode currently calls objects.hashcode(object...)  this triggers the allocation of a short lived object[] which is not eliminated by escapeanalysis. i have implemented a fix by inlining the hashcode logic and also added a caching hashcode field. this improved performance on the read workload.  fix is available here:  https://github.com/nitsanw/cassandra/tree/objects-hashcode-fix",
        "label": 386
    },
    {
        "text": "cql3 does handle list append or prepend with a  prepared  list <description> i can successfully update a list using the \"literal\" syntax: update testcollection set l = [98,99,100] + l where k = 1; and i can successfully \"upsert\" a list using the \"prepared\" syntax: update testcollection set l = ? where k = 1 by providing a decoded list<integer> in the bind values. but using the \"prepared\" syntax for an prepend like: update testcollection set l = ? + l where k = 1 fails with the following message: java.sql.sqlsyntaxerrorexception: invalidrequestexception(why:line 1:33 mismatched input '+' expecting k_where) at org.apache.cassandra.cql.jdbc.cassandrapreparedstatement.<init>(cassandrapreparedstatement.java:92) ... ... and an append of a \"prepared\" syntax like: update testcollection set l = l + ? where k = 1 fails as follows: java.sql.sqlsyntaxerrorexception: invalidrequestexception(why:invalid operation for non commutative columnfamily testcollection) at org.apache.cassandra.cql.jdbc.cassandrapreparedstatement.<init>(cassandrapreparedstatement.java:92) ... ...<stacktrace> java.sql.sqlsyntaxerrorexception: invalidrequestexception(why:line 1:33 mismatched input '+' expecting k_where) at org.apache.cassandra.cql.jdbc.cassandrapreparedstatement.<init>(cassandrapreparedstatement.java:92) ... ... java.sql.sqlsyntaxerrorexception: invalidrequestexception(why:invalid operation for non commutative columnfamily testcollection) at org.apache.cassandra.cql.jdbc.cassandrapreparedstatement.<init>(cassandrapreparedstatement.java:92) ... ... <code> update testcollection set l = [98,99,100] + l where k = 1; update testcollection set l = ? where k = 1 update testcollection set l = ? + l where k = 1 update testcollection set l = l + ? where k = 1 <text> i can successfully update a list using the 'literal' syntax: and i can successfully 'upsert' a list using the 'prepared' syntax: by providing a decoded list<integer> in the bind values. but using the 'prepared' syntax for an prepend like: fails with the following message: and an append of a 'prepared' syntax like: fails as follows:",
        "label": 520
    },
    {
        "text": "expose tasks queue length via jmx <description> cassandra-11363 introduced cassandra.max_queued_native_transport_requests to set the ntr queue length.  currently cassandra lacks of a jmx mbean which exposes this value which would allow to:  1. be sure this value has been set  2. plot this value in a monitoring application to make correlations with other graphs when we make changes<stacktrace> <code> <text> cassandra-11363 introduced cassandra.max_queued_native_transport_requests to set the ntr queue length.  currently cassandra lacks of a jmx mbean which exposes this value which would allow to: 1. be sure this value has been set  2. plot this value in a monitoring application to make correlations with other graphs when we make changes",
        "label": 348
    },
    {
        "text": "windows dtest  commitlog test py testcommitlog small segment size test fails <description>   file \"d:\\python27\\lib\\unittest\\case.py\", line 329, in run     testmethod()   file \"d:\\jenkins\\workspace\\cassandra-3.0_dtest_win32\\cassandra-dtest\\tools.py\", line 243, in wrapped     f(obj)   file \"d:\\jenkins\\workspace\\cassandra-3.0_dtest_win32\\cassandra-dtest\\commitlog_test.py\", line 226, in small_segment_size_test     self._commitlog_test(segment_size_in_mb, 62.5, 13, files_error=0.2)   file \"d:\\jenkins\\workspace\\cassandra-3.0_dtest_win32\\cassandra-dtest\\commitlog_test.py\", line 99, in _commitlog_test     error=files_error)   file \"d:\\jenkins\\workspace\\cassandra-3.0_dtest_win32\\cassandra-dtest\\assertions.py\", line 62, in assert_almost_equal     assert vmin > vmax * (1.0 - error) or vmin == vmax, \"values not within %.2f%% of the max: %s\" % (error * 100, args) 'values not within 20.00% of the max: (10, 13)\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: d:\\\\temp\\\\dtest-qnguzs\\n--------------------- >> end captured logging << ---------------------' failure history: consistent env: both ci and local<stacktrace> <code>   file 'd:/python27/lib/unittest/case.py', line 329, in run     testmethod()   file 'd:/jenkins/workspace/cassandra-3.0_dtest_win32/cassandra-dtest/tools.py', line 243, in wrapped     f(obj)   file 'd:/jenkins/workspace/cassandra-3.0_dtest_win32/cassandra-dtest/commitlog_test.py', line 226, in small_segment_size_test     self._commitlog_test(segment_size_in_mb, 62.5, 13, files_error=0.2)   file 'd:/jenkins/workspace/cassandra-3.0_dtest_win32/cassandra-dtest/commitlog_test.py', line 99, in _commitlog_test     error=files_error)   file 'd:/jenkins/workspace/cassandra-3.0_dtest_win32/cassandra-dtest/assertions.py', line 62, in assert_almost_equal     assert vmin > vmax * (1.0 - error) or vmin == vmax, 'values not within %.2f%% of the max: %s' % (error * 100, args) 'values not within 20.00% of the max: (10, 13)/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: d://temp//dtest-qnguzs/n--------------------- >> end captured logging << ---------------------' env: both ci and local<text> failure history: consistent ",
        "label": 409
    },
    {
        "text": "static columns returning null for pages after first <description> when paging is used for a query containing a static column, the first page contains the right value for the static column, but subsequent pages have null null for the static column instead of the expected value. repro steps: create a table with a static column create a partition with 500 cells using cqlsh, query that partition actual result: you will see that first, the static column appears as expected, but if you press a key after \"--more--\", the static columns will appear as null. see the attached file for a repro of the output. i am using a single node cluster.<stacktrace> <code> <text> when paging is used for a query containing a static column, the first page contains the right value for the static column, but subsequent pages have null null for the static column instead of the expected value. repro steps: actual result: see the attached file for a repro of the output. i am using a single node cluster.",
        "label": 538
    },
    {
        "text": "fix dtest cqlsh tests cqlsh copy tests cqlshcopytest test bulk round trip <description> test failing on 2.2 after fixing cassandra-10507: http://cassci.datastax.com/view/dev/view/stef1927/job/stef1927-10507-2.2-dtest/lastcompletedbuild/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_bulk_round_trip/<stacktrace> <code> http://cassci.datastax.com/view/dev/view/stef1927/job/stef1927-10507-2.2-dtest/lastcompletedbuild/testreport/cqlsh_tests.cqlsh_copy_tests/cqlshcopytest/test_bulk_round_trip/<text> test failing on 2.2 after fixing cassandra-10507: ",
        "label": 508
    },
    {
        "text": "add usecondcardmark xx jvm settings on jdk <description> found by jbellis adding jvm extension setting usecondcardmark as defined here http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7029167 for better lock handling especially on hotspot with multicore processors.<stacktrace> <code> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7029167 <text> found by jbellis adding jvm extension setting usecondcardmark as defined here for better lock handling especially on hotspot with multicore processors.",
        "label": 135
    },
    {
        "text": "use caffeine  w tinylfu  for on heap caches <description> cassandra currently uses concurrentlinkedhashmap for performance critical caches (key, counter) and guava's cache for non-critical (auth, metrics, security). all of these usages have been replaced by caffeine, written by the author of the previously mentioned libraries. the primary incentive is to switch from lru policy to w-tinylfu, which provides near optimal hit rates. it performs particularly well in database and search traces, is scan resistant, and as adds a very small time/space overhead to lru. secondarily, guava's caches never obtained similar performance to clhm due to some optimizations not being ported over. this change results in faster reads and not creating garbage as a side-effect.<stacktrace> <code> <text> cassandra currently uses concurrentlinkedhashmap for performance critical caches (key, counter) and guava's cache for non-critical (auth, metrics, security). all of these usages have been replaced by caffeine, written by the author of the previously mentioned libraries. the primary incentive is to switch from lru policy to w-tinylfu, which provides near optimal hit rates. it performs particularly well in database and search traces, is scan resistant, and as adds a very small time/space overhead to lru. secondarily, guava's caches never obtained similar performance to clhm due to some optimizations not being ported over. this change results in faster reads and not creating garbage as a side-effect.",
        "label": 65
    },
    {
        "text": "gossiptimertask stops running if an exception occurs <description> the gossiptimertask run() method has a try/catch around its body, but it re-throws all exceptions as runtimeexceptions. this causes the gossiptimertask to no longer run (due to the way the underlying java timer implementation works), stopping the periodic gossip status checks. combine this problem with a bug like cassandra-757 (not yet fixed in 0.6.x) and you get into a state where the server keeps running, but gossip is no longer occurring, preventing node addition / removal from happening. i see two potential choices:  1) log the error but don't re-throw it so that the gossiptimertask will continue to run on its next interval.  2) shutdown the server, since continuing to run without gossip subtly breaks other functionality / knowledge of other nodes.<stacktrace> <code> <text> the gossiptimertask run() method has a try/catch around its body, but it re-throws all exceptions as runtimeexceptions. this causes the gossiptimertask to no longer run (due to the way the underlying java timer implementation works), stopping the periodic gossip status checks. combine this problem with a bug like cassandra-757 (not yet fixed in 0.6.x) and you get into a state where the server keeps running, but gossip is no longer occurring, preventing node addition / removal from happening. i see two potential choices:  1) log the error but don't re-throw it so that the gossiptimertask will continue to run on its next interval.  2) shutdown the server, since continuing to run without gossip subtly breaks other functionality / knowledge of other nodes.",
        "label": 85
    },
    {
        "text": "even faster uuid comparisons <description> patch replaces \"long\" creation and comparion in timeuuidtype with direct byte comparison in correct order. (addition to cassandra-1043).<stacktrace> <code> <text> patch replaces 'long' creation and comparion in timeuuidtype with direct byte comparison in correct order. (addition to cassandra-1043).",
        "label": 181
    },
    {
        "text": "allow paging through non ordered partitioner results in cql3 <description> cql < 3 silently turns a \"key >= x\" into \"token(key) >= token(x)\". this is not what users will expect, since many of the rows returned will not in fact satisfy the requested key inequality. we should add syntax that makes the difference between keys and tokens explicit, possibly with a token() \"function\" as imagined here.<stacktrace> <code> <text> cql < 3 silently turns a 'key >= x' into 'token(key) >= token(x)'. this is not what users will expect, since many of the rows returned will not in fact satisfy the requested key inequality. we should add syntax that makes the difference between keys and tokens explicit, possibly with a token() 'function' as imagined here.",
        "label": 520
    },
    {
        "text": "start script occasionally causes flapping when used with runit <description> starting cassandra in the foreground is the preferred way of running it under something like runit or daemontools. currently, the start script will execute java in a subprocess when starting in the foreground. this is a problem for runit and daemontools since they both register the pid of start script. occasionally the start script process will fail, leaving the java process running. the runit supervisor will then start flapping since it thinks cassandra is down even though there is a java process running. the attached patch will run cassandra in the same process as the start script when run in the foreground. this will cause runit and daemontools to have the correct behavior.<stacktrace> <code> <text> starting cassandra in the foreground is the preferred way of running it under something like runit or daemontools. currently, the start script will execute java in a subprocess when starting in the foreground. this is a problem for runit and daemontools since they both register the pid of start script. occasionally the start script process will fail, leaving the java process running. the runit supervisor will then start flapping since it thinks cassandra is down even though there is a java process running. the attached patch will run cassandra in the same process as the start script when run in the foreground. this will cause runit and daemontools to have the correct behavior.",
        "label": 116
    },
    {
        "text": "fix the endpointstate time bookmarking in gossiper <description> in gossiper#dostatuscheck(line 485 to line 489)  the code is  long l = now - epstate.getupdatetimestamp(); --> should be : \"long l = epstate.getupdatetimestamp();\"  long duration = now - l;  if ( !epstate.isalive() && (duration > averylongtime_) ) { evictfrommembership(endpoint); }<stacktrace> <code> in gossiper#dostatuscheck(line 485 to line 489)  the code is  long l = now - epstate.getupdatetimestamp(); --> should be : 'long l = epstate.getupdatetimestamp();'  long duration = now - l;  if ( !epstate.isalive() && (duration > averylongtime_) )<text> ",
        "label": 274
    },
    {
        "text": "cassandra startup takes an hour because of n n operation <description> (there's a previous version of this ticket, which was very wrong about the actual cause. original is quoted below) in java.org.cassandra.db.columnfamilystore, the function scrubdatadirectories loops over all sstables and then for each sstable it cleans temporary files from its directory. since there are many sstables in a directory, this ends up cleaning the same directory many times. when using leveledcompactionstrategy on a data set that is ~4tb per node, you can easily end up with 200k files. add n and n, and we get a n*n operation (scrubdatadirectories) which ends up taking an hour (or more). (at this point i should probably point out that no, i am not sure about that. at all. but i do know this takes an hour and jstack blames this function) as promised, original ticket below : a cassandra cluster of ours has nodes with up to 4tb of data, in a single table using leveled compaction having 200k files. while upgrading from 2.2.6 to 3.0.7 we noticed that it took a while to restart a node. and with \"a while\" i mean we measured it at more than 60 minutes. jstack shows something interesting : \"main\" #1 prio=5 os_prio=0 tid=0x00007f30db0ea400 nid=0xdb22 runnable [0x00007f30de122000]    java.lang.thread.state: runnable     at java.io.unixfilesystem.list(native method)     at java.io.file.list(file.java:1122)     at java.io.file.listfiles(file.java:1248)     at org.apache.cassandra.io.sstable.descriptor.gettemporaryfiles(descriptor.java:172)     at org.apache.cassandra.db.columnfamilystore.scrubdatadirectories(columnfamilystore.java:599)     at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:245)     at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:557)     at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:685) going by the source of file.listfiles, it puts every file in a directory into an array and then applies the filter. this is actually a known java issue from 1999: http://bugs.java.com/view_bug.do?bug_id=4285834 \u2013 their \"solution\" was to introduce new apis in jre7. i guess that makes listfiles deprecated for larger directories (like when using leveledcompactionstrategy). tl;dr: because cassandra uses java.io.file.listfiles, service startup can take an hour for larger data sets.<stacktrace> 'main' #1 prio=5 os_prio=0 tid=0x00007f30db0ea400 nid=0xdb22 runnable [0x00007f30de122000]    java.lang.thread.state: runnable     at java.io.unixfilesystem.list(native method)     at java.io.file.list(file.java:1122)     at java.io.file.listfiles(file.java:1248)     at org.apache.cassandra.io.sstable.descriptor.gettemporaryfiles(descriptor.java:172)     at org.apache.cassandra.db.columnfamilystore.scrubdatadirectories(columnfamilystore.java:599)     at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:245)     at org.apache.cassandra.service.cassandradaemon.activate(cassandradaemon.java:557)     at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:685) <code> <text> (there's a previous version of this ticket, which was very wrong about the actual cause. original is quoted below) in java.org.cassandra.db.columnfamilystore, the function scrubdatadirectories loops over all sstables and then for each sstable it cleans temporary files from its directory. since there are many sstables in a directory, this ends up cleaning the same directory many times. when using leveledcompactionstrategy on a data set that is ~4tb per node, you can easily end up with 200k files. add n and n, and we get a n*n operation (scrubdatadirectories) which ends up taking an hour (or more). (at this point i should probably point out that no, i am not sure about that. at all. but i do know this takes an hour and jstack blames this function) as promised, original ticket below : a cassandra cluster of ours has nodes with up to 4tb of data, in a single table using leveled compaction having 200k files. while upgrading from 2.2.6 to 3.0.7 we noticed that it took a while to restart a node. and with 'a while' i mean we measured it at more than 60 minutes. jstack shows something interesting : going by the source of file.listfiles, it puts every file in a directory into an array and then applies the filter. this is actually a known java issue from 1999: http://bugs.java.com/view_bug.do?bug_id=4285834 - their 'solution' was to introduce new apis in jre7. i guess that makes listfiles deprecated for larger directories (like when using leveledcompactionstrategy). tl;dr: because cassandra uses java.io.file.listfiles, service startup can take an hour for larger data sets.",
        "label": 241
    },
    {
        "text": "divide by zero in hinted handoff <description> got this on a node shortly after upgrading it to 2.0.8. the other node went down and up (triggering the hinted handoff) because it was being migrated from 2.0.7 to 2.0.8. info [hintedhandoff:1] 2014-06-05 07:26:35,447 hintedhandoffmanager.java (line 344) started hinted handoff for host: b12e6d71-e189-4fe8-b00a-8ff2cc9848fd wit  h ip: /192.168.60.137  error [hintedhandoff:1] 2014-06-05 07:26:35,448 cassandradaemon.java (line 199) exception in thread thread[hintedhandoff:1,1,main]  java.lang.arithmeticexception: / by zero  at org.apache.cassandra.db.hintedhandoffmanager.calculatepagesize(hintedhandoffmanager.java:493)  at org.apache.cassandra.db.hintedhandoffmanager.dodeliverhintstoendpoint(hintedhandoffmanager.java:351)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:330)  at org.apache.cassandra.db.hintedhandoffmanager.access$300(hintedhandoffmanager.java:91)  at org.apache.cassandra.db.hintedhandoffmanager$5.run(hintedhandoffmanager.java:547)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)<stacktrace> info [hintedhandoff:1] 2014-06-05 07:26:35,447 hintedhandoffmanager.java (line 344) started hinted handoff for host: b12e6d71-e189-4fe8-b00a-8ff2cc9848fd wit  h ip: /192.168.60.137  error [hintedhandoff:1] 2014-06-05 07:26:35,448 cassandradaemon.java (line 199) exception in thread thread[hintedhandoff:1,1,main]  java.lang.arithmeticexception: / by zero  at org.apache.cassandra.db.hintedhandoffmanager.calculatepagesize(hintedhandoffmanager.java:493)  at org.apache.cassandra.db.hintedhandoffmanager.dodeliverhintstoendpoint(hintedhandoffmanager.java:351)  at org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:330)  at org.apache.cassandra.db.hintedhandoffmanager.access$300(hintedhandoffmanager.java:91)  at org.apache.cassandra.db.hintedhandoffmanager$5.run(hintedhandoffmanager.java:547)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)<code> <text> got this on a node shortly after upgrading it to 2.0.8. the other node went down and up (triggering the hinted handoff) because it was being migrated from 2.0.7 to 2.0.8. ",
        "label": 18
    },
    {
        "text": "hsha dtest for closing connections almost always fails on cassci developer branches <description> the test consistently succeeds on 2.1/2.2 branches, but has been failing almost every time on developer ones for quite a while, for example 9258 dtest, 10817 dtest, 10059 (committed) dtest, unchanged 2.2 dtest and many others. the failures cannot be genuine if the same code committed to the main branch no longer fails the test. this is making it hard to identify genuine failures and blocking cassandra-9669 in particular.<stacktrace> <code> <text> the test consistently succeeds on 2.1/2.2 branches, but has been failing almost every time on developer ones for quite a while, for example 9258 dtest, 10817 dtest, 10059 (committed) dtest, unchanged 2.2 dtest and many others. the failures cannot be genuine if the same code committed to the main branch no longer fails the test. this is making it hard to identify genuine failures and blocking cassandra-9669 in particular.",
        "label": 98
    },
    {
        "text": "add memtableflushafterminutes  a global replacement for flushperiodinminutes <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "cql queries should allow cf names to be qualified by keyspace  part <description> see cassandra-3130. the same treatment should be applied to the other related cql commands dealing with column family names, viz: insert update delete truncate also, if the intent is to make it possible to go without use entirely (i'm not sure if it is): create columnfamily create index drop columnfamily alter columnfamily drop index (support keyspacename.indexname?)<stacktrace> <code> <text> see cassandra-3130. the same treatment should be applied to the other related cql commands dealing with column family names, viz: also, if the intent is to make it possible to go without use entirely (i'm not sure if it is):",
        "label": 412
    },
    {
        "text": "allow the cassandradaemon to be managed externally <description> this is related to cassandra-7998 and deals with the control flow, if the cassandradaemon is managed by another java process. in that case it should not exit the vm, but instead delegate that decision to the process that created the daemon in the first place.<stacktrace> <code> <text> this is related to cassandra-7998 and deals with the control flow, if the cassandradaemon is managed by another java process. in that case it should not exit the vm, but instead delegate that decision to the process that created the daemon in the first place.",
        "label": 204
    },
    {
        "text": "large supercolumn deserialization invokes cslm worst case scenario <description> supercolumn deserialization hits a worst case insert scenario for cslm: inserting pre-sorted entries one at a time. inside of cslm this requires scanning to the end of the list and doing a comparison at every step for every item inserted. this patch supplies a sortedmap interface to the supercolumn deserialization. cslm will do a bulk insert from a sortedmap interface supplied in the constructor.<stacktrace> <code> <text> supercolumn deserialization hits a worst case insert scenario for cslm: inserting pre-sorted entries one at a time. inside of cslm this requires scanning to the end of the list and doing a comparison at every step for every item inserted. this patch supplies a sortedmap interface to the supercolumn deserialization. cslm will do a bulk insert from a sortedmap interface supplied in the constructor.",
        "label": 116
    },
    {
        "text": "read defragmentation can cause unnecessary repairs <description> after applying the fix from cassandra-10299 to the cluster we started having a problem of ~20k small sstables appearing for the table with static data when running incremental repair. in the logs there were several messages about flushes for that table, one for each repaired range. the flushed sstables were 0.000kb in size with < 100 ops in each. when checking cfstats there were several writes to that table, even though we were only reading from it and read repair did not repair anything. after digging around in the codebase i noticed that defragmentation of data can occur while reading, depending on the query and some other conditions. this causes the read data to be inserted again to have it in a more recent sstable, which can be a problem if that data was repaired using incremental repair. the defragmentation is done in collationcontroller.java. i guess this wasn't a problem with full repairs since i assume that the digest should be the same even if you have two copies of the same data. but with incremental repair this will most probably cause a mismatch between nodes if that data already was repaired, since the other nodes probably won't have that data in their unrepaired set. ------ i can add that the problems on our cluster was probably due to the fact that cassandra-10299 caused the same data to be streamed multiple times and ending up in several sstables. one of the conditions for the defragmentation is that the number of sstables read during a read request have to be more than the minimum number of sstables needed for a compaction(> 4 in our case). so normally i don't think this would cause ~20k sstables to appear, we probably hit an extreme. one workaround for this is to use another compaction strategy than stcs(it seems to be the only affected strategy, atleast in 2.1), but the solution might be to either make defragmentation configurable per table or avoid reinserting the data if any of the sstables involved in the read are repaired.<stacktrace> <code> ------ <text> after applying the fix from cassandra-10299 to the cluster we started having a problem of ~20k small sstables appearing for the table with static data when running incremental repair. in the logs there were several messages about flushes for that table, one for each repaired range. the flushed sstables were 0.000kb in size with < 100 ops in each. when checking cfstats there were several writes to that table, even though we were only reading from it and read repair did not repair anything. after digging around in the codebase i noticed that defragmentation of data can occur while reading, depending on the query and some other conditions. this causes the read data to be inserted again to have it in a more recent sstable, which can be a problem if that data was repaired using incremental repair. the defragmentation is done in collationcontroller.java. i guess this wasn't a problem with full repairs since i assume that the digest should be the same even if you have two copies of the same data. but with incremental repair this will most probably cause a mismatch between nodes if that data already was repaired, since the other nodes probably won't have that data in their unrepaired set. i can add that the problems on our cluster was probably due to the fact that cassandra-10299 caused the same data to be streamed multiple times and ending up in several sstables. one of the conditions for the defragmentation is that the number of sstables read during a read request have to be more than the minimum number of sstables needed for a compaction(> 4 in our case). so normally i don't think this would cause ~20k sstables to appear, we probably hit an extreme. one workaround for this is to use another compaction strategy than stcs(it seems to be the only affected strategy, atleast in 2.1), but the solution might be to either make defragmentation configurable per table or avoid reinserting the data if any of the sstables involved in the read are repaired.",
        "label": 321
    },
    {
        "text": "fix help text for stress counterwrite <description> the help output for counterwrite shows 'counteradd' in the syntax instead of 'counterwrite'. rhatch@whatup:~/git/cstar/cassandra/tools$ ./bin/cassandra-stress help counterwrite usage: counteradd [err<?] [n>?] [n<?] [tries=?] [ignore_errors] [cl=?]  or  usage: counteradd n=? [tries=?] [ignore_errors] [cl=?]   err<? (default=0.02)                     run until the standard error of the mean is below this fraction   n>? (default=30)                         run at least this many iterations before accepting uncertainty convergence   n<? (default=200)                        run at most this many iterations before accepting uncertainty convergence   tries=? (default=9)                      number of tries to perform for each operation before failing   ignore_errors                            do not print/log errors   cl=? (default=one)                       consistency level to use   n=?                                      number of operations to perform<stacktrace> <code> rhatch@whatup:~/git/cstar/cassandra/tools$ ./bin/cassandra-stress help counterwrite usage: counteradd [err<?] [n>?] [n<?] [tries=?] [ignore_errors] [cl=?]  or  usage: counteradd n=? [tries=?] [ignore_errors] [cl=?]   err<? (default=0.02)                     run until the standard error of the mean is below this fraction   n>? (default=30)                         run at least this many iterations before accepting uncertainty convergence   n<? (default=200)                        run at most this many iterations before accepting uncertainty convergence   tries=? (default=9)                      number of tries to perform for each operation before failing   ignore_errors                            do not print/log errors   cl=? (default=one)                       consistency level to use   n=?                                      number of operations to perform <text> the help output for counterwrite shows 'counteradd' in the syntax instead of 'counterwrite'.",
        "label": 67
    },
    {
        "text": "unify support across different map reduce related classes for comma seperated list of hosts for initial thrift port connection  <description> unify support across different map/reduce related classes for comma seperated list of hosts for initial thrift port connection. column family input format already had support for this since cassandra-2807, and ringcache did it the same way, but it was coded seperately. this jira should add pig support, and abstract a common method to iterate over these seeds.<stacktrace> <code> <text> unify support across different map/reduce related classes for comma seperated list of hosts for initial thrift port connection. column family input format already had support for this since cassandra-2807, and ringcache did it the same way, but it was coded seperately. this jira should add pig support, and abstract a common method to iterate over these seeds.",
        "label": 166
    },
    {
        "text": "windows dtest  jmx test py testjmx netstats test fails <description> unexpected error in node1 node log: ['error [hintedhandoff:2] 2015-08-16 23:14:04,419 cassandradaemon.java:191 - exception in thread thread[hintedhandoff:2,1,main] org.apache.cassandra.exceptions.writefailureexception: operation failed - received 0 responses and 1 failures \\tat org.apache.cassandra.service.abstractwriteresponsehandler.get(abstractwriteresponsehandler.java:106) ~[main/:na] \\tat org.apache.cassandra.db.hintedhandoffmanager.checkdelivered(hintedhandoffmanager.java:358) ~[main/:na] \\tat org.apache.cassandra.db.hintedhandoffmanager.dodeliverhintstoendpoint(hintedhandoffmanager.java:414) ~[main/:na] \\tat org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:346) ~[main/:na] \\tat org.apache.cassandra.db.hintedhandoffmanager.access$400(hintedhandoffmanager.java:91) ~[main/:na] \\tat org.apache.cassandra.db.hintedhandoffmanager$5.run(hintedhandoffmanager.java:537) ~[main/:na] \\tat java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_45] \\tat java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) ~[na:1.8.0_45] \\tat java.lang.thread.run(thread.java:745) ~[na:1.8.0_45]'] -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: d:\\temp\\dtest-j1ttp3 dtest: debug: nodetool command 'd:\\jenkins\\workspace\\cassandra-3.0_dtest_win32\\cassandra\\bin\\nodetool.bat -h localhost -p 7100 netstats' failed; exit status: 1; stdout: starting nodetool ; stderr: nodetool: failed to connect to 'localhost:7100' - connectexception: 'connection refused: connect'. dtest: debug: removing ccm cluster test at: d:\\temp\\dtest-j1ttp3 dtest: debug: clearing ssl stores from [d:\\temp\\dtest-j1ttp3] directory --------------------- >> end captured logging << --------------------- failure history: consistent. looks to have regressed on build #5 which seems unlikely given the commit. env: both, though on a local run the test fails due to: traceback (most recent call last):   file \"c:\\src\\cassandra-dtest\\dtest.py\", line 532, in teardown     raise assertionerror('unexpected error in %s node log: %s' % (node.name, errors)) assertionerror: unexpected error in node1 node log: ['error [main] 2015-08-17 15:42:07,717 nospamlogger.java:97 - this platform does not support atomic directory streams (securedirectorystream); race conditions when loading sstable files could occurr', 'error [main] 2015-08-17 15:50:43,978 nospamlogger.java:97 - this platform does not support atomic directory streams (securedirectorystream); race conditions when loading sstable files could occurr']<stacktrace> unexpected error in node1 node log: ['error [hintedhandoff:2] 2015-08-16 23:14:04,419 cassandradaemon.java:191 - exception in thread thread[hintedhandoff:2,1,main] org.apache.cassandra.exceptions.writefailureexception: operation failed - received 0 responses and 1 failures /tat org.apache.cassandra.service.abstractwriteresponsehandler.get(abstractwriteresponsehandler.java:106) ~[main/:na] /tat org.apache.cassandra.db.hintedhandoffmanager.checkdelivered(hintedhandoffmanager.java:358) ~[main/:na] /tat org.apache.cassandra.db.hintedhandoffmanager.dodeliverhintstoendpoint(hintedhandoffmanager.java:414) ~[main/:na] /tat org.apache.cassandra.db.hintedhandoffmanager.deliverhintstoendpoint(hintedhandoffmanager.java:346) ~[main/:na] /tat org.apache.cassandra.db.hintedhandoffmanager.access$400(hintedhandoffmanager.java:91) ~[main/:na] /tat org.apache.cassandra.db.hintedhandoffmanager$5.run(hintedhandoffmanager.java:537) ~[main/:na] /tat java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) ~[na:1.8.0_45] /tat java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) ~[na:1.8.0_45] /tat java.lang.thread.run(thread.java:745) ~[na:1.8.0_45]'] -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: d:/temp/dtest-j1ttp3 dtest: debug: nodetool command 'd:/jenkins/workspace/cassandra-3.0_dtest_win32/cassandra/bin/nodetool.bat -h localhost -p 7100 netstats' failed; exit status: 1; stdout: starting nodetool ; stderr: nodetool: failed to connect to 'localhost:7100' - connectexception: 'connection refused: connect'. dtest: debug: removing ccm cluster test at: d:/temp/dtest-j1ttp3 dtest: debug: clearing ssl stores from [d:/temp/dtest-j1ttp3] directory --------------------- >> end captured logging << --------------------- <code> traceback (most recent call last):   file 'c:/src/cassandra-dtest/dtest.py', line 532, in teardown     raise assertionerror('unexpected error in %s node log: %s' % (node.name, errors)) assertionerror: unexpected error in node1 node log: ['error [main] 2015-08-17 15:42:07,717 nospamlogger.java:97 - this platform does not support atomic directory streams (securedirectorystream); race conditions when loading sstable files could occurr', 'error [main] 2015-08-17 15:50:43,978 nospamlogger.java:97 - this platform does not support atomic directory streams (securedirectorystream); race conditions when loading sstable files could occurr'] <text> failure history: consistent. looks to have regressed on build #5 which seems unlikely given the commit. env: both, though on a local run the test fails due to:",
        "label": 409
    },
    {
        "text": "dtest failure in cqlsh tests cqlsh tests testcqlsh test unicode syntax error <description> example failure: http://cassci.datastax.com/job/cassandra-3.0_dtest/703/testreport/cqlsh_tests.cqlsh_tests/testcqlsh/test_unicode_syntax_error failed on cassci build cassandra-3.0_dtest #703 also failing is cqlsh_tests.cqlsh_tests.testcqlsh.test_unicode_invalid_request_error the relevant failure is 'ascii' codec can't encode character u'\\xe4' in position 12: ordinal not in range(128) these are failing on 2.2, 3.0 and trunk.<stacktrace> <code> 'ascii' codec can't encode character u'/xe4' in position 12: ordinal not in range(128) http://cassci.datastax.com/job/cassandra-3.0_dtest/703/testreport/cqlsh_tests.cqlsh_tests/testcqlsh/test_unicode_syntax_error failed on cassci build cassandra-3.0_dtest #703 also failing is cqlsh_tests.cqlsh_tests.testcqlsh.test_unicode_invalid_request_error <text> example failure: the relevant failure is these are failing on 2.2, 3.0 and trunk.",
        "label": 538
    },
    {
        "text": " patch  equal objects should have equal hashcodes <description> ksmetadata should define hashcode, as it defines equals. also instanceof does null checking so don't need the null check for equals.<stacktrace> <code> <text> ksmetadata should define hashcode, as it defines equals. also instanceof does null checking so don't need the null check for equals.",
        "label": 139
    },
    {
        "text": "sampling bug in metrics core jar used by cassandra <description> there is a sampling bug in the version of the metrics library we're using in cassandra. see https://github.com/codahale/metrics/issues/421. exponentiallydecayingsample is used by the timer's histogram that is used in stress tool, and according to brandon williams it is also in a few other places like the dynamic snitch. the statistical theory involved in this bug goes over my head so i'm not sure if this would bug would meaningfully affect its usage by cassandra. one of the comments on the bug mentions that it affects slow sampling rates (10 samples/min was the example given). we're currently distributing metrics-core-2.0.3.jar and according to the release nodes, this bug is fixed in 2.1.3: http://metrics.codahale.com/about/release-notes/#v2-1-3-aug-06-2012<stacktrace> <code> <text> there is a sampling bug in the version of the metrics library we're using in cassandra. see https://github.com/codahale/metrics/issues/421. exponentiallydecayingsample is used by the timer's histogram that is used in stress tool, and according to brandon williams it is also in a few other places like the dynamic snitch. the statistical theory involved in this bug goes over my head so i'm not sure if this would bug would meaningfully affect its usage by cassandra. one of the comments on the bug mentions that it affects slow sampling rates (10 samples/min was the example given). we're currently distributing metrics-core-2.0.3.jar and according to the release nodes, this bug is fixed in 2.1.3: http://metrics.codahale.com/about/release-notes/#v2-1-3-aug-06-2012",
        "label": 577
    },
    {
        "text": "java io ioerror  java io eofexception with version <description> i use the following data-model column_metadata: []  name: customers  column_type: super  gc_grace_seconds: 60 i have a super-column-family with a single row.  within this row i have a single super-column.  within this super-column, i concurrently create, read and delete columns. i have three threads: do in a loop: add a column to the super-column. do in a loop: delete a random column from the super-column. do in a loop: read the super-column (with all columns). after running the above threads concurrently, i always receive one of the following errors: error 17:09:57,036 fatal exception in thread thread[readstage:81,5,main]  java.io.ioerror: java.io.eofexception  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:252)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:268)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:227)  at java.util.concurrent.concurrentskiplistmap.buildfromsorted(unknown source)  at java.util.concurrent.concurrentskiplistmap.<init>(unknown source)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:379)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:362)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:322)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:79)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:40)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.hasnext(sstablesliceiterator.java:108)  at org.apache.commons.collections.iterators.collatingiterator.set(collatingiterator.java:283)  at org.apache.commons.collections.iterators.collatingiterator.least(collatingiterator.java:326)  at org.apache.commons.collections.iterators.collatingiterator.next(collatingiterator.java:230)  at org.apache.cassandra.utils.reducingiterator.computenext(reducingiterator.java:69)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:116)  at org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(queryfilter.java:130)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1390)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1267)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1195)  at org.apache.cassandra.db.table.getrow(table.java:324)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:451)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)  caused by: java.io.eofexception  at java.io.randomaccessfile.readbyte(unknown source)  at org.apache.cassandra.utils.bytebufferutil.readshortlength(bytebufferutil.java:324)  at org.apache.cassandra.utils.bytebufferutil.readwithshortlength(bytebufferutil.java:335)  at org.apache.cassandra.db.columnserializer.deserialize(columnserializer.java:71)  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:248)  ... 30 more java.io.ioerror: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:252)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:268)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:227)  at java.util.concurrent.concurrentskiplistmap.buildfromsorted(unknown source)  at java.util.concurrent.concurrentskiplistmap.<init>(unknown source)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:379)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:362)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:322)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:79)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:40)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.hasnext(sstablesliceiterator.java:108)  at org.apache.commons.collections.iterators.collatingiterator.set(collatingiterator.java:283)  at org.apache.commons.collections.iterators.collatingiterator.least(collatingiterator.java:326)  at org.apache.commons.collections.iterators.collatingiterator.next(collatingiterator.java:230)  at org.apache.cassandra.utils.reducingiterator.computenext(reducingiterator.java:69)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:116)  at org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(queryfilter.java:130)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1385)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1262)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1190)  at org.apache.cassandra.db.table.getrow(table.java:324)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:451)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)  caused by: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0  at org.apache.cassandra.db.columnserializer.deserialize(columnserializer.java:73)  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:248)  ... 30 more error 11:02:19,824 fatal exception in thread thread[readstage:3404,5,main]  java.io.ioerror: java.io.ioexception: mmap segment underflow; remaining is 660267 but 758592100 requested  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:252)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:268)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:227)  at java.util.concurrent.concurrentskiplistmap.buildfromsorted(unknown source)  at java.util.concurrent.concurrentskiplistmap.<init>(unknown source)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:379)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:362)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:322)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:79)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:40)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.hasnext(sstablesliceiterator.java:108)  at org.apache.commons.collections.iterators.collatingiterator.set(collatingiterator.java:283)  at org.apache.commons.collections.iterators.collatingiterator.least(collatingiterator.java:326)  at org.apache.commons.collections.iterators.collatingiterator.next(collatingiterator.java:230)  at org.apache.cassandra.utils.reducingiterator.computenext(reducingiterator.java:69)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:116)  at org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(queryfilter.java:130)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1390)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1267)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1195)  at org.apache.cassandra.db.table.getrow(table.java:324)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:451)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)<stacktrace> error 17:09:57,036 fatal exception in thread thread[readstage:81,5,main]  java.io.ioerror: java.io.eofexception  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:252)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:268)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:227)  at java.util.concurrent.concurrentskiplistmap.buildfromsorted(unknown source)  at java.util.concurrent.concurrentskiplistmap.<init>(unknown source)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:379)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:362)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:322)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:79)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:40)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.hasnext(sstablesliceiterator.java:108)  at org.apache.commons.collections.iterators.collatingiterator.set(collatingiterator.java:283)  at org.apache.commons.collections.iterators.collatingiterator.least(collatingiterator.java:326)  at org.apache.commons.collections.iterators.collatingiterator.next(collatingiterator.java:230)  at org.apache.cassandra.utils.reducingiterator.computenext(reducingiterator.java:69)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:116)  at org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(queryfilter.java:130)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1390)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1267)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1195)  at org.apache.cassandra.db.table.getrow(table.java:324)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:451)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)  caused by: java.io.eofexception  at java.io.randomaccessfile.readbyte(unknown source)  at org.apache.cassandra.utils.bytebufferutil.readshortlength(bytebufferutil.java:324)  at org.apache.cassandra.utils.bytebufferutil.readwithshortlength(bytebufferutil.java:335)  at org.apache.cassandra.db.columnserializer.deserialize(columnserializer.java:71)  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:248)  ... 30 more java.io.ioerror: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:252)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:268)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:227)  at java.util.concurrent.concurrentskiplistmap.buildfromsorted(unknown source)  at java.util.concurrent.concurrentskiplistmap.<init>(unknown source)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:379)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:362)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:322)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:79)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:40)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.hasnext(sstablesliceiterator.java:108)  at org.apache.commons.collections.iterators.collatingiterator.set(collatingiterator.java:283)  at org.apache.commons.collections.iterators.collatingiterator.least(collatingiterator.java:326)  at org.apache.commons.collections.iterators.collatingiterator.next(collatingiterator.java:230)  at org.apache.cassandra.utils.reducingiterator.computenext(reducingiterator.java:69)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:116)  at org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(queryfilter.java:130)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1385)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1262)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1190)  at org.apache.cassandra.db.table.getrow(table.java:324)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:451)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)  caused by: org.apache.cassandra.db.columnserializer$corruptcolumnexception: invalid column name length 0  at org.apache.cassandra.db.columnserializer.deserialize(columnserializer.java:73)  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:248)  ... 30 more error 11:02:19,824 fatal exception in thread thread[readstage:3404,5,main]  java.io.ioerror: java.io.ioexception: mmap segment underflow; remaining is 660267 but 758592100 requested  at org.apache.cassandra.io.util.columniterator.deserializenext(columnsortedmap.java:252)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:268)  at org.apache.cassandra.io.util.columniterator.next(columnsortedmap.java:227)  at java.util.concurrent.concurrentskiplistmap.buildfromsorted(unknown source)  at java.util.concurrent.concurrentskiplistmap.<init>(unknown source)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:379)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:362)  at org.apache.cassandra.db.supercolumnserializer.deserialize(supercolumn.java:322)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:79)  at org.apache.cassandra.db.columniterator.simpleslicereader.computenext(simpleslicereader.java:40)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.hasnext(sstablesliceiterator.java:108)  at org.apache.commons.collections.iterators.collatingiterator.set(collatingiterator.java:283)  at org.apache.commons.collections.iterators.collatingiterator.least(collatingiterator.java:326)  at org.apache.commons.collections.iterators.collatingiterator.next(collatingiterator.java:230)  at org.apache.cassandra.utils.reducingiterator.computenext(reducingiterator.java:69)  at com.google.common.collect.abstractiterator.trytocomputenext(abstractiterator.java:136)  at com.google.common.collect.abstractiterator.hasnext(abstractiterator.java:131)  at org.apache.cassandra.db.filter.slicequeryfilter.collectreducedcolumns(slicequeryfilter.java:116)  at org.apache.cassandra.db.filter.queryfilter.collectcollatedcolumns(queryfilter.java:130)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1390)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1267)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1195)  at org.apache.cassandra.db.table.getrow(table.java:324)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)  at org.apache.cassandra.service.storageproxy$localreadrunnable.runmaythrow(storageproxy.java:451)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  at java.util.concurrent.threadpoolexecutor$worker.runtask(unknown source)  at java.util.concurrent.threadpoolexecutor$worker.run(unknown source)  at java.lang.thread.run(unknown source)<code> column_metadata: []  name: customers  column_type: super  gc_grace_seconds: 60 <text> i use the following data-model i have a super-column-family with a single row.  within this row i have a single super-column.  within this super-column, i concurrently create, read and delete columns. i have three threads: after running the above threads concurrently, i always receive one of the following errors: ",
        "label": 520
    },
    {
        "text": "memtable flush after mins setting not working <description> we have observed the behavior that memtable_flush_after_mins setting not working occasionally. after some testing and code digging, we finally figured out what going on.  the memtable_flush_after_mins won't work on certain condition with current implementation in cassandra. in org.apache.cassandra.db.table, the scheduled flush task is setup by the following code during construction. ------------------------------------------------------------------------------------------------------------------  int mincheckms = integer.max_value; for (columnfamilystore cfs : columnfamilystores.values())   {  mincheckms = math.min(mincheckms, cfs.getmemtableflushaftermins() * 60 * 1000);  } runnable runnable = new runnable()  {  public void run()  {  for (columnfamilystore cfs : columnfamilystores.values()) { cfs.forceflushifexpired(); } }  };  flushtask = storageservice.scheduledtasks.schedulewithfixeddelay(runnable, mincheckms, mincheckms, timeunit.milliseconds);  ------------------------------------------------------------------------------------------------------------------------------ now for our application, we will create a keyspacewithout without any columnfamily first. and only add needed columnfamily later depends on request. however, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with integer.max_value ms  since there is no columnfamily yet. later when you add columnfamily to this empty keyspace, the initcf() method in table.java doesn't check whether the scheduled flush check task interval need  to be updated or not. to fix this, we'd need to restart the cassandra after columnfamily added into the keyspace. i would suggest that add additional logic in initcf() method to recreate a scheduled flush check task if needed.<stacktrace> <code> ------------------------------------------------------------------------------------------------------------------  int mincheckms = integer.max_value; for (columnfamilystore cfs : columnfamilystores.values())   {  mincheckms = math.min(mincheckms, cfs.getmemtableflushaftermins() * 60 * 1000);  } runnable runnable = new runnable()  {  public void run()  {  for (columnfamilystore cfs : columnfamilystores.values()) }  };  flushtask = storageservice.scheduledtasks.schedulewithfixeddelay(runnable, mincheckms, mincheckms, timeunit.milliseconds);  ------------------------------------------------------------------------------------------------------------------------------ <text> we have observed the behavior that memtable_flush_after_mins setting not working occasionally. after some testing and code digging, we finally figured out what going on.  the memtable_flush_after_mins won't work on certain condition with current implementation in cassandra. in org.apache.cassandra.db.table, the scheduled flush task is setup by the following code during construction. now for our application, we will create a keyspacewithout without any columnfamily first. and only add needed columnfamily later depends on request. however, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with integer.max_value ms  since there is no columnfamily yet. later when you add columnfamily to this empty keyspace, the initcf() method in table.java doesn't check whether the scheduled flush check task interval need  to be updated or not. to fix this, we'd need to restart the cassandra after columnfamily added into the keyspace. i would suggest that add additional logic in initcf() method to recreate a scheduled flush check task if needed.",
        "label": 274
    },
    {
        "text": "sstabledump and sstableverify need to be added to deb packages <description> command-line tool sstabledump is not installed on debian. i used the following source: deb http://www.apache.org/dist/cassandra/debian 35x main with the following installation commands: sudo apt-get install cassandra sudo apt-get install cassandra-tools<stacktrace> <code> deb http://www.apache.org/dist/cassandra/debian 35x main sudo apt-get install cassandra sudo apt-get install cassandra-tools <text> command-line tool sstabledump is not installed on debian. i used the following source: with the following installation commands:",
        "label": 521
    },
    {
        "text": "lost found directory in the data dir causes problems again <description> looks like we've regressed from cassandra-1547 and mounting a fs directly on the data dir is a problem again. info [main] 2012-08-22 23:30:03,710 directories.java (line 475) upgrade from pre-1.1 version detected: migrating sstables to new directory layout error [main] 2012-08-22 23:30:03,712 abstractcassandradaemon.java (line 370) exception encountered during startup                  java.lang.nullpointerexception         at org.apache.cassandra.db.directories.migratesstables(directories.java:487)<stacktrace> info [main] 2012-08-22 23:30:03,710 directories.java (line 475) upgrade from pre-1.1 version detected: migrating sstables to new directory layout error [main] 2012-08-22 23:30:03,712 abstractcassandradaemon.java (line 370) exception encountered during startup                  java.lang.nullpointerexception         at org.apache.cassandra.db.directories.migratesstables(directories.java:487) <code> <text> looks like we've regressed from cassandra-1547 and mounting a fs directly on the data dir is a problem again.",
        "label": 577
    },
    {
        "text": "bulkloader is broken in trunk <description> after cassandra-5015 and cassandra-5521, we need cfmetadata to open sstable(reader), especially to get bloom_filter_fp_chance and index_interval. when using bulkloader, cfmetadata is not available, so we cannot open sstable to be streamed.<stacktrace> <code> <text> after cassandra-5015 and cassandra-5521, we need cfmetadata to open sstable(reader), especially to get bloom_filter_fp_chance and index_interval. when using bulkloader, cfmetadata is not available, so we cannot open sstable to be streamed.",
        "label": 577
    },
    {
        "text": "nodetool scrub hangs or throws an exception <description> trying to run nodetool scrub hung or (only happened one time) threw the following exception: error [compactionexecutor:1] 2011-02-28 10:26:26,620 abstractcassandradaemon.java (line 114) fatal exception in thread thread[compactionexecutor:1,1,main]  java.lang.assertionerror  at org.apache.cassandra.dht.randompartitioner.convertfromdiskformat(randompartitioner.java:62)  at org.apache.cassandra.io.sstable.sstablereader.decodekey(sstablereader.java:627)  at org.apache.cassandra.db.compactionmanager.doscrub(compactionmanager.java:538)  at org.apache.cassandra.db.compactionmanager.access$600(compactionmanager.java:55)  at org.apache.cassandra.db.compactionmanager$3.call(compactionmanager.java:194)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334)  at java.util.concurrent.futuretask.run(futuretask.java:166)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:636)<stacktrace> error [compactionexecutor:1] 2011-02-28 10:26:26,620 abstractcassandradaemon.java (line 114) fatal exception in thread thread[compactionexecutor:1,1,main]  java.lang.assertionerror  at org.apache.cassandra.dht.randompartitioner.convertfromdiskformat(randompartitioner.java:62)  at org.apache.cassandra.io.sstable.sstablereader.decodekey(sstablereader.java:627)  at org.apache.cassandra.db.compactionmanager.doscrub(compactionmanager.java:538)  at org.apache.cassandra.db.compactionmanager.access$600(compactionmanager.java:55)  at org.apache.cassandra.db.compactionmanager$3.call(compactionmanager.java:194)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:334)  at java.util.concurrent.futuretask.run(futuretask.java:166)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603)  at java.lang.thread.run(thread.java:636)<code> <text> trying to run nodetool scrub hung or (only happened one time) threw the following exception: ",
        "label": 274
    },
    {
        "text": "commitlogarchiver thread pool name inconsistent with others <description> pretty trivial... the names of all threadpoolexecutors are in camelcase except the commitlogarchiver as commitlog_archiver. this shows up a little more obvious in tpstats output: nodetool tpstats pool name                    active   pending      completed   blocked   readstage                         0         0         113702         0                requestresponsestage              0         0              0         0                ... pendingrangecalculator            0         0              1         0                  commitlog_archiver                0         0              0         0                  internalresponsestage             0         0              0         0                  hintedhandoff                     0         0              0         0                  seems minor enough to update this to be commitlogarchiver but it may mean changes in any monitoring applications (although i don't think this particular pool has had much runtime or monitoring needs).<stacktrace> <code> nodetool tpstats pool name                    active   pending      completed   blocked   readstage                         0         0         113702         0                requestresponsestage              0         0              0         0                ... pendingrangecalculator            0         0              1         0                  commitlog_archiver                0         0              0         0                  internalresponsestage             0         0              0         0                  hintedhandoff                     0         0              0         0                  <text> pretty trivial... the names of all threadpoolexecutors are in camelcase except the commitlogarchiver as commitlog_archiver. this shows up a little more obvious in tpstats output: seems minor enough to update this to be commitlogarchiver but it may mean changes in any monitoring applications (although i don't think this particular pool has had much runtime or monitoring needs).",
        "label": 106
    },
    {
        "text": "enable gc logging by default <description> overhead for the gc logging is very small (with cycling logs in 7+) and it provides a ton of useful information. this will open up more for c* diagnostic tools to provide feedback as well without requiring restarts.<stacktrace> <code> <text> overhead for the gc logging is very small (with cycling logs in 7+) and it provides a ton of useful information. this will open up more for c* diagnostic tools to provide feedback as well without requiring restarts.",
        "label": 106
    },
    {
        "text": "cleanup fails with indexoutofboundsexception exception <description> originally we had cassandra cluster with the following configuration: initial tokens:  10.1.11.51: -9223372036854775808  10.1.11.52: 0  10.2.11.51: -9223372036854775807  10.2.11.52: 1 endpoint snitch: propertyfilesnitch  10.1.11.51=dc1:rac1  10.1.11.52=dc1:rac1   10.2.11.51=dc2:rac1  10.2.11.52=dc2:rac1 we have created keyspace with network topology strategy and replication factor 2: create keyspace our_keyspace with replication = {   'class': 'networktopologystrategy',   'dc2': '2',   'dc1': '2' }; all column families have been created with sizetiered compaction strategy. \u0430fter a while column families have been migrated to leveledcompactionstrategy and then replication factor was decreased to 1. we started cleanup, but on non-seed nodes (10.1.11.52,10.2.11.52) it failed with assertion error. assertion stacktrace was the same as described in cassandra-6774 issue.  then we've stopped all compactions, started cleanup again and got another exception: error occurred during cleanup  java.util.concurrent.executionexception: java.lang.indexoutofboundsexception: index: 1, size: 1  at java.util.concurrent.futuretask.report(futuretask.java:122)  at java.util.concurrent.futuretask.get(futuretask.java:188)  at org.apache.cassandra.db.compaction.compactionmanager.performallsstableoperation(compactionmanager.java:227)  at org.apache.cassandra.db.compaction.compactionmanager.performcleanup(compactionmanager.java:265)  at org.apache.cassandra.db.columnfamilystore.forcecleanup(columnfamilystore.java:1115)  at org.apache.cassandra.service.storageservice.forcekeyspacecleanup(storageservice.java:2152)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.trampoline.invoke(methodutil.java:75)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.methodutil.invoke(methodutil.java:279)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)  at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)  at sun.reflect.generatedmethodaccessor20.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)  at sun.rmi.transport.transport$1.run(transport.java:177)  at sun.rmi.transport.transport$1.run(transport.java:174)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:173)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)  caused by: java.lang.indexoutofboundsexception: index: 1, size: 1  at java.util.arraylist.rangecheck(arraylist.java:635)  at java.util.arraylist.get(arraylist.java:411)  at org.apache.cassandra.db.compaction.compactionmanager.needscleanup(compactionmanager.java:502)  at org.apache.cassandra.db.compaction.compactionmanager.docleanupcompaction(compactionmanager.java:540)  at org.apache.cassandra.db.compaction.compactionmanager.access$400(compactionmanager.java:62)  at org.apache.cassandra.db.compaction.compactionmanager$5.perform(compactionmanager.java:274)  at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:222)  at java.util.concurrent.futuretask.run(futuretask.java:262)  ... 3 more it seems there is typo in needscleanup method of compactionmanager class and the supplied patch should fix the issue.  get method on the line 502 (compactionmanager.java) throws exception, because of the wrong comparison on the line 496: there should be sortedranges size check, not ownedranges.<stacktrace> error occurred during cleanup  java.util.concurrent.executionexception: java.lang.indexoutofboundsexception: index: 1, size: 1  at java.util.concurrent.futuretask.report(futuretask.java:122)  at java.util.concurrent.futuretask.get(futuretask.java:188)  at org.apache.cassandra.db.compaction.compactionmanager.performallsstableoperation(compactionmanager.java:227)  at org.apache.cassandra.db.compaction.compactionmanager.performcleanup(compactionmanager.java:265)  at org.apache.cassandra.db.columnfamilystore.forcecleanup(columnfamilystore.java:1115)  at org.apache.cassandra.service.storageservice.forcekeyspacecleanup(storageservice.java:2152)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:57)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.trampoline.invoke(methodutil.java:75)  at sun.reflect.generatedmethodaccessor12.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.reflect.misc.methodutil.invoke(methodutil.java:279)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:112)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:46)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:237)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:138)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:252)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:819)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:801)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1487)  at javax.management.remote.rmi.rmiconnectionimpl.access$300(rmiconnectionimpl.java:97)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1328)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1420)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:848)  at sun.reflect.generatedmethodaccessor20.invoke(unknown source)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)  at java.lang.reflect.method.invoke(method.java:606)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:322)  at sun.rmi.transport.transport$1.run(transport.java:177)  at sun.rmi.transport.transport$1.run(transport.java:174)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:173)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:556)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:811)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:670)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)  caused by: java.lang.indexoutofboundsexception: index: 1, size: 1  at java.util.arraylist.rangecheck(arraylist.java:635)  at java.util.arraylist.get(arraylist.java:411)  at org.apache.cassandra.db.compaction.compactionmanager.needscleanup(compactionmanager.java:502)  at org.apache.cassandra.db.compaction.compactionmanager.docleanupcompaction(compactionmanager.java:540)  at org.apache.cassandra.db.compaction.compactionmanager.access$400(compactionmanager.java:62)  at org.apache.cassandra.db.compaction.compactionmanager$5.perform(compactionmanager.java:274)  at org.apache.cassandra.db.compaction.compactionmanager$2.call(compactionmanager.java:222)  at java.util.concurrent.futuretask.run(futuretask.java:262)  ... 3 more <code> create keyspace our_keyspace with replication = {   'class': 'networktopologystrategy',   'dc2': '2',   'dc1': '2' }; initial tokens:  10.1.11.51: -9223372036854775808  10.1.11.52: 0  10.2.11.51: -9223372036854775807  10.2.11.52: 1 endpoint snitch: propertyfilesnitch  10.1.11.51=dc1:rac1  10.1.11.52=dc1:rac1   10.2.11.51=dc2:rac1  10.2.11.52=dc2:rac1 <text> originally we had cassandra cluster with the following configuration: we have created keyspace with network topology strategy and replication factor 2: all column families have been created with sizetiered compaction strategy. fter a while column families have been migrated to leveledcompactionstrategy and then replication factor was decreased to 1. we started cleanup, but on non-seed nodes (10.1.11.52,10.2.11.52) it failed with assertion error. assertion stacktrace was the same as described in cassandra-6774 issue.  then we've stopped all compactions, started cleanup again and got another exception: it seems there is typo in needscleanup method of compactionmanager class and the supplied patch should fix the issue.  get method on the line 502 (compactionmanager.java) throws exception, because of the wrong comparison on the line 496: there should be sortedranges size check, not ownedranges.",
        "label": 153
    },
    {
        "text": "cqlsh  color option doesn't allow you to disable colors <description> there's no way to disable colors with cqlsh, despite it having a --color option, because that option can only enable color if present, not disable it, and the default is that color is enabled. (incidentally, if the --file option is used, it will disable color.)<stacktrace> <code> (incidentally, if the --file option is used, it will disable color.)<text> there's no way to disable colors with cqlsh, despite it having a --color option, because that option can only enable color if present, not disable it, and the default is that color is enabled. ",
        "label": 538
    },
    {
        "text": "cassandra stress hangs on error <description> after encountering a fatal error, cassandra-stress hangs. having not run a previous stress write, can be reproduced with: cassandra-stress read n=1000 -rate threads=2 here's the full output ******************** stress settings ******************** command:   type: read   count: 1,000   no warmup: false   consistency level: local_one   target uncertainty: not applicable   key size (bytes): 10   counter increment distibution: add=fixed(1) rate:   auto: false   thread count: 2   opsper sec: 0 population:   distribution: gaussian:  min=1,max=1000,mean=500.500000,stdev=166.500000   order: arbitrary   wrap: false insert:   revisits: uniform:  min=1,max=1000000   visits: fixed:  key=1   row population ratio: ratio: divisor=1.000000;delegate=fixed:  key=1   batch type: not batching columns:   max columns per key: 5   column names: [c0, c1, c2, c3, c4]   comparator: asciitype   timestamp: null   variable column count: false   slice: false   size distribution: fixed:  key=34   count distribution: fixed:  key=5 errors:   ignore: false   tries: 10 log:   no summary: false   no settings: false   file: null   interval millis: 1000   level: normal mode:   api: java_driver_native   connection style: cql_prepared   cql version: cql3   protocol version: v4   username: null   password: null   auth provide class: null   max pending per connection: 128   connections per host: 8   compression: none node:   nodes: [localhost]   is white list: false   datacenter: null schema:   keyspace: keyspace1   replication strategy: org.apache.cassandra.locator.simplestrategy   replication strategy pptions: {replication_factor=1}   table compression: null   table compaction strategy: null   table compaction strategy options: {} transport:   factory=org.apache.cassandra.thrift.tframedtransportfactory; truststore=null; truststore-password=null; keystore=null; keystore-password=null; ssl-protocol=tls; ssl-alg=sunx509; store-type=jks; ssl-ciphers=tls_rsa_with_aes_128_cbc_sha,tls_rsa_with_aes_256_cbc_sha;  port:   native port: 9042   thrift port: 9160   jmx port: 9042 send to daemon:   *not set* graph:   file: null   revision: unknown   title: null   operation: read tokenrange:   wrap: false   split factor: 1 sleeping 2s... warming up read with 250 iterations... connected to cluster: falcon-test2, max pending requests per connection 128, max connections per host 8 datatacenter: cassandra; host: localhost/127.0.0.1; rack: rack1 failed to connect over jmx; not collecting these stats connected to cluster: falcon-test2, max pending requests per connection 128, max connections per host 8 datatacenter: cassandra; host: localhost/127.0.0.1; rack: rack1 com.datastax.driver.core.exceptions.invalidqueryexception: keyspace 'keyspace1' does not exist connected to cluster: falcon-test2, max pending requests per connection 128, max connections per host 8 datatacenter: cassandra; host: localhost/127.0.0.1; rack: rack1 com.datastax.driver.core.exceptions.invalidqueryexception: keyspace 'keyspace1' does not exist<stacktrace> <code> cassandra-stress read n=1000 -rate threads=2 ******************** stress settings ******************** command:   type: read   count: 1,000   no warmup: false   consistency level: local_one   target uncertainty: not applicable   key size (bytes): 10   counter increment distibution: add=fixed(1) rate:   auto: false   thread count: 2   opsper sec: 0 population:   distribution: gaussian:  min=1,max=1000,mean=500.500000,stdev=166.500000   order: arbitrary   wrap: false insert:   revisits: uniform:  min=1,max=1000000   visits: fixed:  key=1   row population ratio: ratio: divisor=1.000000;delegate=fixed:  key=1   batch type: not batching columns:   max columns per key: 5   column names: [c0, c1, c2, c3, c4]   comparator: asciitype   timestamp: null   variable column count: false   slice: false   size distribution: fixed:  key=34   count distribution: fixed:  key=5 errors:   ignore: false   tries: 10 log:   no summary: false   no settings: false   file: null   interval millis: 1000   level: normal mode:   api: java_driver_native   connection style: cql_prepared   cql version: cql3   protocol version: v4   username: null   password: null   auth provide class: null   max pending per connection: 128   connections per host: 8   compression: none node:   nodes: [localhost]   is white list: false   datacenter: null schema:   keyspace: keyspace1   replication strategy: org.apache.cassandra.locator.simplestrategy   replication strategy pptions: {replication_factor=1}   table compression: null   table compaction strategy: null   table compaction strategy options: {} transport:   factory=org.apache.cassandra.thrift.tframedtransportfactory; truststore=null; truststore-password=null; keystore=null; keystore-password=null; ssl-protocol=tls; ssl-alg=sunx509; store-type=jks; ssl-ciphers=tls_rsa_with_aes_128_cbc_sha,tls_rsa_with_aes_256_cbc_sha;  port:   native port: 9042   thrift port: 9160   jmx port: 9042 send to daemon:   *not set* graph:   file: null   revision: unknown   title: null   operation: read tokenrange:   wrap: false   split factor: 1 sleeping 2s... warming up read with 250 iterations... connected to cluster: falcon-test2, max pending requests per connection 128, max connections per host 8 datatacenter: cassandra; host: localhost/127.0.0.1; rack: rack1 failed to connect over jmx; not collecting these stats connected to cluster: falcon-test2, max pending requests per connection 128, max connections per host 8 datatacenter: cassandra; host: localhost/127.0.0.1; rack: rack1 com.datastax.driver.core.exceptions.invalidqueryexception: keyspace 'keyspace1' does not exist connected to cluster: falcon-test2, max pending requests per connection 128, max connections per host 8 datatacenter: cassandra; host: localhost/127.0.0.1; rack: rack1 com.datastax.driver.core.exceptions.invalidqueryexception: keyspace 'keyspace1' does not exist <text> after encountering a fatal error, cassandra-stress hangs. having not run a previous stress write, can be reproduced with: here's the full output",
        "label": 161
    },
    {
        "text": "aggregation functions in cql <description> the requirement is to do aggregation of data in cassandra (wide row of column values of int, double, float etc). with some basic agree gate functions like avg, sum, mean, min, max, etc (for the columns within a row). example: select * from emp where empid in (130) order by deptid desc;   empid | deptid | first_name | last_name | salary  --------------------------------------  130 | 3 | joe | doe | 10.1  130 | 2 | joe | doe | 100  130 | 1 | joe | doe | 1e+03 select sum(salary), empid from emp where empid in (130);   sum(salary) | empid  ------------+-------  1110.1 | 130<stacktrace> <code> select * from emp where empid in (130) order by deptid desc;   empid | deptid | first_name | last_name | salary  --------------------------------------  130 | 3 | joe | doe | 10.1  130 | 2 | joe | doe | 100  130 | 1 | joe | doe | 1e+03 select sum(salary), empid from emp where empid in (130);   sum(salary) | empid  ------------+-------  1110.1 | 130<text> the requirement is to do aggregation of data in cassandra (wide row of column values of int, double, float etc). with some basic agree gate functions like avg, sum, mean, min, max, etc (for the columns within a row). example: ",
        "label": 69
    },
    {
        "text": "cfs loadnewsstables  broken for pre sstables <description> while working on cassandra-10236 i discovered that cfs.loadnewsstables() doesn't work for pre-3.0 sstables - just for version ma sstables. tbc: starting c* with 2.0, 2.1 or 2.2 sstables works - but loading new sstables during runtime doesn't. issues with cfs.loadnewsstables() discovered so far: 1. metadataserializer.deserialize(descriptor,filedatainput,enumset) returns null for metadatatype.header which results in a npe later in metadataserializer.serialize executing collections.sort. 2. after working around the previous issue, it turns out that it couldn't load the digest file, since component.digest is a singleton which refers to crc32, but pre-3.0 sstables use adler32. 3. after working around that one, it fails in streaminghistogram$streaminghistogramserializer.deserialize as maxbinsize==integer.max_value. as loading legacy sstables works fine during startup, i assume my workarounds are not correct. for reference, this commit contains a ton of legacy sstables (simple, counter, clustered and clustered+counter) for 2.0, 2.1 and 2.2. i've extended legacysstablestest to read these tables using cfs.loadnewsstables(). legacysstablestest.txt diff --git a/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java b/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java index d2922cc..1be6450 100644 --- a/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java +++ b/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java @@ -18,6 +18,9 @@  package org.apache.cassandra.io.sstable;    import java.io.file; +import java.io.fileinputstream; +import java.io.fileoutputstream; +import java.io.ioexception;  import java.nio.bytebuffer;  import java.util.arraylist;  import java.util.hashset; @@ -27,10 +30,15 @@ import java.util.set;  import org.junit.beforeclass;  import org.junit.test;   +import org.slf4j.logger; +import org.slf4j.loggerfactory; +  import org.apache.cassandra.schemaloader;  import org.apache.cassandra.util;  import org.apache.cassandra.config.cfmetadata; +import org.apache.cassandra.cql3.queryprocessor;  import org.apache.cassandra.db.columnfamilystore; +import org.apache.cassandra.db.consistencylevel;  import org.apache.cassandra.db.deletiontime;  import org.apache.cassandra.db.keyspace;  import org.apache.cassandra.db.rows.sliceableunfilteredrowiterator; @@ -43,6 +51,7 @@ import org.apache.cassandra.exceptions.configurationexception;  import org.apache.cassandra.io.sstable.format.sstableformat;  import org.apache.cassandra.io.sstable.format.sstablereader;  import org.apache.cassandra.io.sstable.format.version; +import org.apache.cassandra.io.sstable.format.big.bigformat;  import org.apache.cassandra.schema.keyspaceparams;  import org.apache.cassandra.service.storageservice;  import org.apache.cassandra.streaming.streamplan; @@ -57,6 +66,8 @@ import static org.apache.cassandra.utils.bytebufferutil.bytes;   */  public class legacysstabletest  { +    private static final logger logger = loggerfactory.getlogger(legacysstabletest.class); +      public static final string legacy_sstable_prop = \"legacy-sstable-root\";      public static final string ksname = \"keyspace1\";      public static final string cfname = \"standard1\"; @@ -64,6 +75,8 @@ public class legacysstabletest      public static set<string> test_data;      public static file legacy_sstable_root;   +    public static final string[] legacyversions = {\"jb\", \"ka\", \"la\"}; +      @beforeclass      public static void defineschema() throws configurationexception      { @@ -208,4 +221,65 @@ public class legacysstabletest              throw e;          }      } + +    @test +    public void testlegacycqltables() throws exception +    { +        queryprocessor.executeinternal(\"create keyspace legacy_tables with replication = {'class': 'simplestrategy', 'replication_factor': '1'}\"); + +        loadlegacytables(); +    } + +    private void loadlegacytables() throws ioexception +    { +        for (string legacyversion : legacyversions) +        { +            logger.info(\"preparing legacy version {}\", legacyversion); + +            queryprocessor.executeinternal(string.format(\"create table legacy_tables.legacy_%s_simple (pk text primary key, val text)\", legacyversion)); +            queryprocessor.executeinternal(string.format(\"create table legacy_tables.legacy_%s_simple_counter (pk text primary key, val counter)\", legacyversion)); +            queryprocessor.executeinternal(string.format(\"create table legacy_tables.legacy_%s_clust (pk text, ck text, val text, primary key (pk, ck))\", legacyversion)); +            queryprocessor.executeinternal(string.format(\"create table legacy_tables.legacy_%s_clust_counter (pk text, ck text, val counter, primary key (pk, ck))\", legacyversion)); + +            loadlegacytable(\"legacy_%s_simple\", legacyversion); +            loadlegacytable(\"legacy_%s_simple_counter\", legacyversion); +            loadlegacytable(\"legacy_%s_clust\", legacyversion); +            loadlegacytable(\"legacy_%s_clust_counter\", legacyversion); + +        } +    } + +    private void loadlegacytable(string tablepattern, string legacyversion) throws ioexception +    { +        string table = string.format(tablepattern, legacyversion); + +        logger.info(\"loading legacy table {}\", table); + +        columnfamilystore cfs = keyspace.open(\"legacy_tables\").getcolumnfamilystore(table); + +        for (file cfdir : cfs.getdirectories().getcfdirectories()) +        { +            copysstables(legacyversion, table, cfdir); +        } + +        cfs.loadnewsstables(); +    } + +    private static void copysstables(string legacyversion, string table, file cfdir) throws ioexception +    { +        byte[] buf = new byte[65536]; + +        for (file file : new file(legacy_sstable_root, string.format(\"%s/legacy_tables/%s\", legacyversion, table)).listfiles()) +        { +            if (file.isfile()) +            { +                file target = new file(cfdir, file.getname()); +                int rd; +                fileinputstream is = new fileinputstream(file); +                fileoutputstream os = new fileoutputstream(target); +                while ((rd = is.read(buf)) >= 0) +                    os.write(buf, 0, rd); +            } +        } +    }  } broken-workaround diff --git a/src/java/org/apache/cassandra/db/compaction/verifier.java b/src/java/org/apache/cassandra/db/compaction/verifier.java index 554c782..e953b1d 100644 --- a/src/java/org/apache/cassandra/db/compaction/verifier.java +++ b/src/java/org/apache/cassandra/db/compaction/verifier.java @@ -96,7 +96,7 @@ public class verifier implements closeable          {              validator = null;   -            if (new file(sstable.descriptor.filenamefor(component.digest)).exists()) +            if (new file(sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent())).exists())              {                  validator = dataintegritymetadata.filedigestvalidator(sstable.descriptor);                  validator.validate(); diff --git a/src/java/org/apache/cassandra/io/sstable/component.java b/src/java/org/apache/cassandra/io/sstable/component.java index 54dd35b..d0405e4 100644 --- a/src/java/org/apache/cassandra/io/sstable/component.java +++ b/src/java/org/apache/cassandra/io/sstable/component.java @@ -34,6 +34,7 @@ public class component      public static final char separator = '-';        final static enumset<type> types = enumset.allof(type.class); +      public enum type      {          // the base data for an sstable: the remaining components can be regenerated @@ -79,13 +80,17 @@ public class component          }      }   +    private static final string digest_crc32_name = \"digest.crc32\"; +    private static final string digest_adler32_name = \"digest.adler32\"; +      // singleton components for types that don't need ids      public final static component data = new component(type.data);      public final static component primary_index = new component(type.primary_index);      public final static component filter = new component(type.filter);      public final static component compression_info = new component(type.compression_info);      public final static component stats = new component(type.stats); -    public final static component digest = new component(type.digest); +    public final static component digest_crc32 = new component(type.digest, digest_crc32_name); +    public final static component digest_adler32 = new component(type.digest, digest_adler32_name);      public final static component crc = new component(type.crc);      public final static component summary = new component(type.summary);      public final static component toc = new component(type.toc); @@ -138,11 +143,23 @@ public class component              case filter:            component = component.filter;                       break;              case compression_info:  component = component.compression_info;             break;              case stats:             component = component.stats;                        break; -            case digest:            component = component.digest;                       break;              case crc:               component = component.crc;                          break;              case summary:           component = component.summary;                      break;              case toc:               component = component.toc;                          break;              case custom:            component = new component(type.custom, path.right); break; +            case digest: +                switch (path.right) +                { +                    case digest_crc32_name: +                        component = component.digest_crc32; +                        break; +                    case digest_adler32_name: +                        component = component.digest_adler32; +                        break; +                    default: +                        throw new illegalstateexception(); +                } +                break;              default:                   throw new illegalstateexception();          } diff --git a/src/java/org/apache/cassandra/io/sstable/descriptor.java b/src/java/org/apache/cassandra/io/sstable/descriptor.java index 38829df..0db6f00 100644 --- a/src/java/org/apache/cassandra/io/sstable/descriptor.java +++ b/src/java/org/apache/cassandra/io/sstable/descriptor.java @@ -32,6 +32,7 @@ import org.apache.cassandra.io.sstable.metadata.imetadataserializer;  import org.apache.cassandra.io.sstable.metadata.legacymetadataserializer;  import org.apache.cassandra.io.sstable.metadata.metadataserializer;  import org.apache.cassandra.utils.pair; +import org.apache.hadoop.mapred.jobtracker;    import static org.apache.cassandra.io.sstable.component.separator;   @@ -344,4 +345,16 @@ public class descriptor      {          return hashcode;      } + +    public component digestcomponent() +    { +        switch (version.compressedchecksumtype()) +        { +            case adler32: +                return component.digest_adler32; +            case crc32: +                return component.digest_crc32; +        } +        throw new illegalstateexception(); +    }  } diff --git a/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java b/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java index bd21536..74e4b56 100644 --- a/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java +++ b/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java @@ -131,7 +131,7 @@ public abstract class sstablewriter extends sstable implements transactional                  component.stats,                  component.summary,                  component.toc, -                component.digest)); +                component.digest_crc32));            if (metadata.params.bloomfilterfpchance < 1.0)              components.add(component.filter); diff --git a/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java b/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java index 9a5eae8..a40c37a 100644 --- a/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java +++ b/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java @@ -122,7 +122,12 @@ public class metadataserializer implements imetadataserializer                  in.seek(offset);                  component = type.serializer.deserialize(descriptor.version, in);              } -            components.put(type, component); +            if (component == null) +            { +                assert type != metadatatype.header || !descriptor.version.storerows(); +            } +            else +                components.put(type, component);          }          return components;      } diff --git a/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java b/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java index 70cd860..b88f4f2 100644 --- a/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java +++ b/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java @@ -109,7 +109,7 @@ public class dataintegritymetadata          {              this.descriptor = descriptor;              checksum = descriptor.version.uncompressedchecksumtype().newinstance(); -            digestreader = randomaccessreader.open(new file(descriptor.filenamefor(component.digest))); +            digestreader = randomaccessreader.open(new file(descriptor.filenamefor(descriptor.digestcomponent())));              datareader = randomaccessreader.open(new file(descriptor.filenamefor(component.data)));              try              { @@ -211,7 +211,7 @@ public class dataintegritymetadata            public void writefullchecksum(descriptor descriptor)          { -            file outfile = new file(descriptor.filenamefor(component.digest)); +            file outfile = new file(descriptor.filenamefor(descriptor.digestcomponent()));              try (bufferedwriter out =files.newbufferedwriter(outfile.topath(), charsets.utf_8))              {                  out.write(string.valueof(fullchecksum.getvalue())); diff --git a/src/java/org/apache/cassandra/io/util/fileutils.java b/src/java/org/apache/cassandra/io/util/fileutils.java index 920eee0..1420cae 100644 --- a/src/java/org/apache/cassandra/io/util/fileutils.java +++ b/src/java/org/apache/cassandra/io/util/fileutils.java @@ -173,7 +173,7 @@ public class fileutils        public static void renamewithconfirm(file from, file to)      { -        assert from.exists(); +        assert from.exists() : string.format(\"file to rename does not exist: %s\", from.getpath());          if (logger.isdebugenabled())              logger.debug((string.format(\"renaming %s to %s\", from.getpath(), to.getpath())));          // this is not fswe because usually when we see it it's because we didn't close the file before renaming it, diff --git a/test/unit/org/apache/cassandra/db/verifytest.java b/test/unit/org/apache/cassandra/db/verifytest.java index 13ce0c1..0233169 100644 --- a/test/unit/org/apache/cassandra/db/verifytest.java +++ b/test/unit/org/apache/cassandra/db/verifytest.java @@ -275,11 +275,11 @@ public class verifytest          sstablereader sstable = cfs.getlivesstables().iterator().next();     -        randomaccessfile file = new randomaccessfile(sstable.descriptor.filenamefor(component.digest), \"rw\"); +        randomaccessfile file = new randomaccessfile(sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent()), \"rw\");          long correctchecksum = long.parselong(file.readline());          file.close();   -        writechecksum(++correctchecksum, sstable.descriptor.filenamefor(component.digest)); +        writechecksum(++correctchecksum, sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent()));            try (verifier verifier = new verifier(cfs, sstable, false))          { @@ -315,7 +315,7 @@ public class verifytest          file.close();            // update the digest to have the right checksum -        writechecksum(simplefullchecksum(sstable.getfilename()), sstable.descriptor.filenamefor(component.digest)); +        writechecksum(simplefullchecksum(sstable.getfilename()), sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent()));            try (verifier verifier = new verifier(cfs, sstable, false))          {<stacktrace> <code> diff --git a/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java b/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java index d2922cc..1be6450 100644 --- a/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java +++ b/test/unit/org/apache/cassandra/io/sstable/legacysstabletest.java @@ -18,6 +18,9 @@  package org.apache.cassandra.io.sstable;    import java.io.file; +import java.io.fileinputstream; +import java.io.fileoutputstream; +import java.io.ioexception;  import java.nio.bytebuffer;  import java.util.arraylist;  import java.util.hashset; @@ -27,10 +30,15 @@ import java.util.set;  import org.junit.beforeclass;  import org.junit.test;   +import org.slf4j.logger; +import org.slf4j.loggerfactory; +  import org.apache.cassandra.schemaloader;  import org.apache.cassandra.util;  import org.apache.cassandra.config.cfmetadata; +import org.apache.cassandra.cql3.queryprocessor;  import org.apache.cassandra.db.columnfamilystore; +import org.apache.cassandra.db.consistencylevel;  import org.apache.cassandra.db.deletiontime;  import org.apache.cassandra.db.keyspace;  import org.apache.cassandra.db.rows.sliceableunfilteredrowiterator; @@ -43,6 +51,7 @@ import org.apache.cassandra.exceptions.configurationexception;  import org.apache.cassandra.io.sstable.format.sstableformat;  import org.apache.cassandra.io.sstable.format.sstablereader;  import org.apache.cassandra.io.sstable.format.version; +import org.apache.cassandra.io.sstable.format.big.bigformat;  import org.apache.cassandra.schema.keyspaceparams;  import org.apache.cassandra.service.storageservice;  import org.apache.cassandra.streaming.streamplan; @@ -57,6 +66,8 @@ import static org.apache.cassandra.utils.bytebufferutil.bytes;   */  public class legacysstabletest  { +    private static final logger logger = loggerfactory.getlogger(legacysstabletest.class); +      public static final string legacy_sstable_prop = 'legacy-sstable-root';      public static final string ksname = 'keyspace1';      public static final string cfname = 'standard1'; @@ -64,6 +75,8 @@ public class legacysstabletest      public static set<string> test_data;      public static file legacy_sstable_root;   +    public static final string[] legacyversions = {'jb', 'ka', 'la'}; +      @beforeclass      public static void defineschema() throws configurationexception      { @@ -208,4 +221,65 @@ public class legacysstabletest              throw e;          }      } + +    @test +    public void testlegacycqltables() throws exception +    { +        queryprocessor.executeinternal('create keyspace legacy_tables with replication = {'class': 'simplestrategy', 'replication_factor': '1'}'); + +        loadlegacytables(); +    } + +    private void loadlegacytables() throws ioexception +    { +        for (string legacyversion : legacyversions) +        { +            logger.info('preparing legacy version {}', legacyversion); + +            queryprocessor.executeinternal(string.format('create table legacy_tables.legacy_%s_simple (pk text primary key, val text)', legacyversion)); +            queryprocessor.executeinternal(string.format('create table legacy_tables.legacy_%s_simple_counter (pk text primary key, val counter)', legacyversion)); +            queryprocessor.executeinternal(string.format('create table legacy_tables.legacy_%s_clust (pk text, ck text, val text, primary key (pk, ck))', legacyversion)); +            queryprocessor.executeinternal(string.format('create table legacy_tables.legacy_%s_clust_counter (pk text, ck text, val counter, primary key (pk, ck))', legacyversion)); + +            loadlegacytable('legacy_%s_simple', legacyversion); +            loadlegacytable('legacy_%s_simple_counter', legacyversion); +            loadlegacytable('legacy_%s_clust', legacyversion); +            loadlegacytable('legacy_%s_clust_counter', legacyversion); + +        } +    } + +    private void loadlegacytable(string tablepattern, string legacyversion) throws ioexception +    { +        string table = string.format(tablepattern, legacyversion); + +        logger.info('loading legacy table {}', table); + +        columnfamilystore cfs = keyspace.open('legacy_tables').getcolumnfamilystore(table); + +        for (file cfdir : cfs.getdirectories().getcfdirectories()) +        { +            copysstables(legacyversion, table, cfdir); +        } + +        cfs.loadnewsstables(); +    } + +    private static void copysstables(string legacyversion, string table, file cfdir) throws ioexception +    { +        byte[] buf = new byte[65536]; + +        for (file file : new file(legacy_sstable_root, string.format('%s/legacy_tables/%s', legacyversion, table)).listfiles()) +        { +            if (file.isfile()) +            { +                file target = new file(cfdir, file.getname()); +                int rd; +                fileinputstream is = new fileinputstream(file); +                fileoutputstream os = new fileoutputstream(target); +                while ((rd = is.read(buf)) >= 0) +                    os.write(buf, 0, rd); +            } +        } +    }  } diff --git a/src/java/org/apache/cassandra/db/compaction/verifier.java b/src/java/org/apache/cassandra/db/compaction/verifier.java index 554c782..e953b1d 100644 --- a/src/java/org/apache/cassandra/db/compaction/verifier.java +++ b/src/java/org/apache/cassandra/db/compaction/verifier.java @@ -96,7 +96,7 @@ public class verifier implements closeable          {              validator = null;   -            if (new file(sstable.descriptor.filenamefor(component.digest)).exists()) +            if (new file(sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent())).exists())              {                  validator = dataintegritymetadata.filedigestvalidator(sstable.descriptor);                  validator.validate(); diff --git a/src/java/org/apache/cassandra/io/sstable/component.java b/src/java/org/apache/cassandra/io/sstable/component.java index 54dd35b..d0405e4 100644 --- a/src/java/org/apache/cassandra/io/sstable/component.java +++ b/src/java/org/apache/cassandra/io/sstable/component.java @@ -34,6 +34,7 @@ public class component      public static final char separator = '-';        final static enumset<type> types = enumset.allof(type.class); +      public enum type      {          // the base data for an sstable: the remaining components can be regenerated @@ -79,13 +80,17 @@ public class component          }      }   +    private static final string digest_crc32_name = 'digest.crc32'; +    private static final string digest_adler32_name = 'digest.adler32'; +      // singleton components for types that don't need ids      public final static component data = new component(type.data);      public final static component primary_index = new component(type.primary_index);      public final static component filter = new component(type.filter);      public final static component compression_info = new component(type.compression_info);      public final static component stats = new component(type.stats); -    public final static component digest = new component(type.digest); +    public final static component digest_crc32 = new component(type.digest, digest_crc32_name); +    public final static component digest_adler32 = new component(type.digest, digest_adler32_name);      public final static component crc = new component(type.crc);      public final static component summary = new component(type.summary);      public final static component toc = new component(type.toc); @@ -138,11 +143,23 @@ public class component              case filter:            component = component.filter;                       break;              case compression_info:  component = component.compression_info;             break;              case stats:             component = component.stats;                        break; -            case digest:            component = component.digest;                       break;              case crc:               component = component.crc;                          break;              case summary:           component = component.summary;                      break;              case toc:               component = component.toc;                          break;              case custom:            component = new component(type.custom, path.right); break; +            case digest: +                switch (path.right) +                { +                    case digest_crc32_name: +                        component = component.digest_crc32; +                        break; +                    case digest_adler32_name: +                        component = component.digest_adler32; +                        break; +                    default: +                        throw new illegalstateexception(); +                } +                break;              default:                   throw new illegalstateexception();          } diff --git a/src/java/org/apache/cassandra/io/sstable/descriptor.java b/src/java/org/apache/cassandra/io/sstable/descriptor.java index 38829df..0db6f00 100644 --- a/src/java/org/apache/cassandra/io/sstable/descriptor.java +++ b/src/java/org/apache/cassandra/io/sstable/descriptor.java @@ -32,6 +32,7 @@ import org.apache.cassandra.io.sstable.metadata.imetadataserializer;  import org.apache.cassandra.io.sstable.metadata.legacymetadataserializer;  import org.apache.cassandra.io.sstable.metadata.metadataserializer;  import org.apache.cassandra.utils.pair; +import org.apache.hadoop.mapred.jobtracker;    import static org.apache.cassandra.io.sstable.component.separator;   @@ -344,4 +345,16 @@ public class descriptor      {          return hashcode;      } + +    public component digestcomponent() +    { +        switch (version.compressedchecksumtype()) +        { +            case adler32: +                return component.digest_adler32; +            case crc32: +                return component.digest_crc32; +        } +        throw new illegalstateexception(); +    }  } diff --git a/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java b/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java index bd21536..74e4b56 100644 --- a/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java +++ b/src/java/org/apache/cassandra/io/sstable/format/sstablewriter.java @@ -131,7 +131,7 @@ public abstract class sstablewriter extends sstable implements transactional                  component.stats,                  component.summary,                  component.toc, -                component.digest)); +                component.digest_crc32));            if (metadata.params.bloomfilterfpchance < 1.0)              components.add(component.filter); diff --git a/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java b/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java index 9a5eae8..a40c37a 100644 --- a/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java +++ b/src/java/org/apache/cassandra/io/sstable/metadata/metadataserializer.java @@ -122,7 +122,12 @@ public class metadataserializer implements imetadataserializer                  in.seek(offset);                  component = type.serializer.deserialize(descriptor.version, in);              } -            components.put(type, component); +            if (component == null) +            { +                assert type != metadatatype.header || !descriptor.version.storerows(); +            } +            else +                components.put(type, component);          }          return components;      } diff --git a/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java b/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java index 70cd860..b88f4f2 100644 --- a/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java +++ b/src/java/org/apache/cassandra/io/util/dataintegritymetadata.java @@ -109,7 +109,7 @@ public class dataintegritymetadata          {              this.descriptor = descriptor;              checksum = descriptor.version.uncompressedchecksumtype().newinstance(); -            digestreader = randomaccessreader.open(new file(descriptor.filenamefor(component.digest))); +            digestreader = randomaccessreader.open(new file(descriptor.filenamefor(descriptor.digestcomponent())));              datareader = randomaccessreader.open(new file(descriptor.filenamefor(component.data)));              try              { @@ -211,7 +211,7 @@ public class dataintegritymetadata            public void writefullchecksum(descriptor descriptor)          { -            file outfile = new file(descriptor.filenamefor(component.digest)); +            file outfile = new file(descriptor.filenamefor(descriptor.digestcomponent()));              try (bufferedwriter out =files.newbufferedwriter(outfile.topath(), charsets.utf_8))              {                  out.write(string.valueof(fullchecksum.getvalue())); diff --git a/src/java/org/apache/cassandra/io/util/fileutils.java b/src/java/org/apache/cassandra/io/util/fileutils.java index 920eee0..1420cae 100644 --- a/src/java/org/apache/cassandra/io/util/fileutils.java +++ b/src/java/org/apache/cassandra/io/util/fileutils.java @@ -173,7 +173,7 @@ public class fileutils        public static void renamewithconfirm(file from, file to)      { -        assert from.exists(); +        assert from.exists() : string.format('file to rename does not exist: %s', from.getpath());          if (logger.isdebugenabled())              logger.debug((string.format('renaming %s to %s', from.getpath(), to.getpath())));          // this is not fswe because usually when we see it it's because we didn't close the file before renaming it, diff --git a/test/unit/org/apache/cassandra/db/verifytest.java b/test/unit/org/apache/cassandra/db/verifytest.java index 13ce0c1..0233169 100644 --- a/test/unit/org/apache/cassandra/db/verifytest.java +++ b/test/unit/org/apache/cassandra/db/verifytest.java @@ -275,11 +275,11 @@ public class verifytest          sstablereader sstable = cfs.getlivesstables().iterator().next();     -        randomaccessfile file = new randomaccessfile(sstable.descriptor.filenamefor(component.digest), 'rw'); +        randomaccessfile file = new randomaccessfile(sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent()), 'rw');          long correctchecksum = long.parselong(file.readline());          file.close();   -        writechecksum(++correctchecksum, sstable.descriptor.filenamefor(component.digest)); +        writechecksum(++correctchecksum, sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent()));            try (verifier verifier = new verifier(cfs, sstable, false))          { @@ -315,7 +315,7 @@ public class verifytest          file.close();            // update the digest to have the right checksum -        writechecksum(simplefullchecksum(sstable.getfilename()), sstable.descriptor.filenamefor(component.digest)); +        writechecksum(simplefullchecksum(sstable.getfilename()), sstable.descriptor.filenamefor(sstable.descriptor.digestcomponent()));            try (verifier verifier = new verifier(cfs, sstable, false))          { <text> while working on cassandra-10236 i discovered that cfs.loadnewsstables() doesn't work for pre-3.0 sstables - just for version ma sstables. tbc: starting c* with 2.0, 2.1 or 2.2 sstables works - but loading new sstables during runtime doesn't. issues with cfs.loadnewsstables() discovered so far: as loading legacy sstables works fine during startup, i assume my workarounds are not correct. for reference, this commit contains a ton of legacy sstables (simple, counter, clustered and clustered+counter) for 2.0, 2.1 and 2.2. i've extended legacysstablestest to read these tables using cfs.loadnewsstables().",
        "label": 453
    },
    {
        "text": "refactor to singlequote reuse java util regex pattern instead of string replaceall for tablecqlhelper java <description> singlequote uses `string.replaceall ()` a lot in loop. and 'string.replaceall()' compiles the regular expression each time it is called.   public string replaceall(string regex, string replacement) {     return pattern.compile(regex).matcher(this).replaceall(replacement);  }   it is suggested in javadoc (https://docs.oracle.com/javase/8/docs/api/java/util/regex/pattern.html#matches(java.lang.string,%20java.lang.charsequence)) that it is efficient to reuse the pattern.<stacktrace> <code> public string replaceall(string regex, string replacement) {     return pattern.compile(regex).matcher(this).replaceall(replacement);  } it is suggested in javadoc (https://docs.oracle.com/javase/8/docs/api/java/util/regex/pattern.html#matches(java.lang.string,%20java.lang.charsequence)) that it is efficient to reuse the pattern.<text> singlequote uses `string.replaceall ()` a lot in loop. and 'string.replaceall()' compiles the regular expression each time it is called.     ",
        "label": 125
    },
    {
        "text": "new token allocator is broken <description> it looks like the logic in the if statment in tokenallocation.getstrategy() is broken. if (replicas >= racks) {...} else if (racks == 1) {...} else     throw new configurationexception(string.format(\"token allocation failed: the number of racks %d in datacenter %s is lower than its replication factor %d.\", racks, dc, replicas)); i think the first if statment should be (replicas <= racks), otherwise this will be true if we have fewer racks then replicas and i guess that should result in an exception. also the if and the else if blocks should change place, it should be if (racks == 1) {...} else if (replicas <= racks) {...} else if replicas==1 and racks==1 i think you want the strategyadapter from (racks==1).<stacktrace> <code> if (replicas >= racks) {...} else if (racks == 1) {...} else     throw new configurationexception(string.format('token allocation failed: the number of racks %d in datacenter %s is lower than its replication factor %d.', racks, dc, replicas)); if (racks == 1) {...} else if (replicas <= racks) {...} else <text> it looks like the logic in the if statment in tokenallocation.getstrategy() is broken. i think the first if statment should be (replicas <= racks), otherwise this will be true if we have fewer racks then replicas and i guess that should result in an exception. also the if and the else if blocks should change place, it should be if replicas==1 and racks==1 i think you want the strategyadapter from (racks==1).",
        "label": 86
    },
    {
        "text": "re populate token metadata after commit log replay <description> i've noticed this while working on another task: a node that was currently down was not showed in my nodetool status. the node had joined the cluster and was displayed in the status before. i've attached mv_repair_test.sh to reproduce the issue. after the execution of the script, just try a: ccm node3 nodetool status<stacktrace> <code> <text> i've noticed this while working on another task: a node that was currently down was not showed in my nodetool status. the node had joined the cluster and was displayed in the status before. i've attached mv_repair_test.sh to reproduce the issue. after the execution of the script, just try a: ccm node3 nodetool status",
        "label": 409
    },
    {
        "text": "audit logging for database activity <description> we would like a way to enable cassandra to log database activity being done on our server. it should show username, remote address, timestamp, action type, keyspace, column family, and the query statement.  it should also be able to log connection attempt and changes to the user/roles. i was thinking of making a new keyspace and insert an entry for every activity that occurs.  then it would be possible to query for specific activity or a query targeting a specific keyspace and column family.<stacktrace> <code> <text> we would like a way to enable cassandra to log database activity being done on our server. it should show username, remote address, timestamp, action type, keyspace, column family, and the query statement.  it should also be able to log connection attempt and changes to the user/roles. i was thinking of making a new keyspace and insert an entry for every activity that occurs.  then it would be possible to query for specific activity or a query targeting a specific keyspace and column family.",
        "label": 558
    },
    {
        "text": "explicitly expose ongoing per node drain status and est    done <description> <stacktrace> <code> <text> ",
        "label": 376
    },
    {
        "text": "if sstable flushes complete out of order  on restart we can fail to replay necessary commit log records <description> while postflushexecutor ensures it never expires cl entries out-of-order, on restart we simply take the maximum replay position of any sstable on disk, and ignore anything prior. it is quite possible for there to be two flushes triggered for a given table, and for the second to finish first by virtue of containing a much smaller quantity of live data (or perhaps the disk is just under less pressure). if we crash before the first sstable has been written, then on restart the data it would have represented will disappear, since we will not replay the cl records. this looks to be a bug present since time immemorial, and also seems pretty serious.<stacktrace> <code> <text> while postflushexecutor ensures it never expires cl entries out-of-order, on restart we simply take the maximum replay position of any sstable on disk, and ignore anything prior. it is quite possible for there to be two flushes triggered for a given table, and for the second to finish first by virtue of containing a much smaller quantity of live data (or perhaps the disk is just under less pressure). if we crash before the first sstable has been written, then on restart the data it would have represented will disappear, since we will not replay the cl records. this looks to be a bug present since time immemorial, and also seems pretty serious.",
        "label": 86
    },
    {
        "text": "cql protocol should allow multiple preparedstatements to be atomically executed <description> currently the only way to insert multiple records on the same partition key, atomically and using preparedstatements is to use a cql batch command. unfortunately when doing so the amount of records to be inserted must be known prior to prepare the statement which is rarely the case. thus the only workaround if one want to keep atomicity is currently to use unprepared statements which send a bulk of cql strings and is fairly inefficient. therefore cql protocol should allow clients to send multiple preparedstatements to be executed with similar guarantees and semantic as cql batch command.<stacktrace> <code> <text> currently the only way to insert multiple records on the same partition key, atomically and using preparedstatements is to use a cql batch command. unfortunately when doing so the amount of records to be inserted must be known prior to prepare the statement which is rarely the case. thus the only workaround if one want to keep atomicity is currently to use unprepared statements which send a bulk of cql strings and is fairly inefficient. therefore cql protocol should allow clients to send multiple preparedstatements to be executed with similar guarantees and semantic as cql batch command.",
        "label": 520
    },
    {
        "text": "allow plugging jemalloc for off heap memtables <description> off-heap memtables (memtable_allocation_type:offheap_objects) introduced by cassandra-6694 uses native gcc allocator. provide an option to use jemalloc allocator (http://www.canonware.com/jemalloc/) which is good to reduce fragmentation. cassandra-3997 adds below option for off-heap caches and metadata. but it's not in effect for off-heap memtables. should we use the same option or add another? memory_allocator: jemallocallocator<stacktrace> <code> memory_allocator: jemallocallocator<text> off-heap memtables (memtable_allocation_type:offheap_objects) introduced by cassandra-6694 uses native gcc allocator. provide an option to use jemalloc allocator (http://www.canonware.com/jemalloc/) which is good to reduce fragmentation. cassandra-3997 adds below option for off-heap caches and metadata. but it's not in effect for off-heap memtables. should we use the same option or add another? ",
        "label": 233
    },
    {
        "text": "index scan uses incorrect comparator on non indexed expressions <description> when multiple index expressions are specified, the column name comparator is used when evaluating secondary (non-indexed) expressions after an indexed expression match.<stacktrace> <code> <text> when multiple index expressions are specified, the column name comparator is used when evaluating secondary (non-indexed) expressions after an indexed expression match.",
        "label": 456
    },
    {
        "text": "cql deletes  aka delete  <description> cql specification and implementation for data removal. this corresponds to the following rpc methods: remove() batch_mutate() (deleting, not updating) truncate() my thoughts on the syntax are that it can probably closely mirror a subset of `select': delete (from)? <cf> [using consistency.<lvl>] where <expression> optionally, you could support a form that makes the `where' clause optional, statements without the clause would be interpreted as a column family truncation.<stacktrace> <code> delete (from)? <cf> [using consistency.<lvl>] where <expression> delete (from)? <cf> [using consistency.<lvl>] where <expression> <text> cql specification and implementation for data removal. this corresponds to the following rpc methods: my thoughts on the syntax are that it can probably closely mirror a subset of `select': optionally, you could support a form that makes the `where' clause optional, statements without the clause would be interpreted as a column family truncation.",
        "label": 169
    },
    {
        "text": "remove old get slice  replace with get slice from <description> <stacktrace> <code> <text> ",
        "label": 274
    },
    {
        "text": "ability for cql3 to list partition keys <description> it can be useful to know the set of in-use partition keys (storage engine row keys). one example given to me was where application data was modeled as a few 10s of 1000s of wide rows, where the app required presenting these rows to the user sorted based on information in the partition key. the partition count is small enough to do the sort client-side in memory, which is what the app did with the thrift api--a range slice with an empty columns list. this was a problem when migrating to cql3. select mykey from mytable includes all the logical rows, which makes the resultset too large to make this a reasonable approach, even with paging. one way to add support would be to allow distinct in the special case of select distinct mykey from mytable.<stacktrace> <code> <text> it can be useful to know the set of in-use partition keys (storage engine row keys). one example given to me was where application data was modeled as a few 10s of 1000s of wide rows, where the app required presenting these rows to the user sorted based on information in the partition key. the partition count is small enough to do the sort client-side in memory, which is what the app did with the thrift api--a range slice with an empty columns list. this was a problem when migrating to cql3. select mykey from mytable includes all the logical rows, which makes the resultset too large to make this a reasonable approach, even with paging. one way to add support would be to allow distinct in the special case of select distinct mykey from mytable.",
        "label": 18
    },
    {
        "text": "java lang arrayindexoutofboundsexception while executing repair on a freshly added node   <description> hi, i added a node to the cluster (20 nodes in total) and ran repair on it after a while. the repair still runs, but there are errors in the log file (see below). some of the data in the clsuter has been filled with rc-4. the cluster runs on version 0.7.0 (the release linked on the main cassandra web site). any ideas what might cause this?  (ps. 0.7.0 turns up as unreleased version in affects version/s info [compactionexecutor:1] 2011-01-10 19:55:20,684 sstablereader.java (line 158) opening /hd2/cassandra_md5/data/table_x/table_x-e-4  info [compactionexecutor:1] 2011-01-10 19:55:20,775 sstablereader.java (line 158) opening /hd2/cassandra_md5/data/table_x/table_x_meta-e-14  error [requestresponsestage:3] 2011-01-10 19:55:20,856 debuggablethreadpoolexecutor.java (line 103) error in threadpoolexecutor  java.lang.arrayindexoutofboundsexception: -1  at java.util.arraylist.fastremove(arraylist.java:441)  at java.util.arraylist.remove(arraylist.java:424)  at com.google.common.collect.abstractmultimap.remove(abstractmultimap.java:219)  at com.google.common.collect.arraylistmultimap.remove(arraylistmultimap.java:60)  at org.apache.cassandra.net.messagingservice.responsereceivedfrom(messagingservice.java:436)  at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:40)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:63)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  error [requestresponsestage:3] 2011-01-10 19:55:20,856 abstractcassandradaemon.java (line 91) fatal exception in thread thread[requestresponsestage:3,5,main]  java.lang.arrayindexoutofboundsexception: -1  at java.util.arraylist.fastremove(arraylist.java:441)  at java.util.arraylist.remove(arraylist.java:424)  at com.google.common.collect.abstractmultimap.remove(abstractmultimap.java:219)  at com.google.common.collect.arraylistmultimap.remove(arraylistmultimap.java:60)  at org.apache.cassandra.net.messagingservice.responsereceivedfrom(messagingservice.java:436)  at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:40)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:63)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<stacktrace> info [compactionexecutor:1] 2011-01-10 19:55:20,684 sstablereader.java (line 158) opening /hd2/cassandra_md5/data/table_x/table_x-e-4  info [compactionexecutor:1] 2011-01-10 19:55:20,775 sstablereader.java (line 158) opening /hd2/cassandra_md5/data/table_x/table_x_meta-e-14  error [requestresponsestage:3] 2011-01-10 19:55:20,856 debuggablethreadpoolexecutor.java (line 103) error in threadpoolexecutor  java.lang.arrayindexoutofboundsexception: -1  at java.util.arraylist.fastremove(arraylist.java:441)  at java.util.arraylist.remove(arraylist.java:424)  at com.google.common.collect.abstractmultimap.remove(abstractmultimap.java:219)  at com.google.common.collect.arraylistmultimap.remove(arraylistmultimap.java:60)  at org.apache.cassandra.net.messagingservice.responsereceivedfrom(messagingservice.java:436)  at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:40)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:63)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  error [requestresponsestage:3] 2011-01-10 19:55:20,856 abstractcassandradaemon.java (line 91) fatal exception in thread thread[requestresponsestage:3,5,main]  java.lang.arrayindexoutofboundsexception: -1  at java.util.arraylist.fastremove(arraylist.java:441)  at java.util.arraylist.remove(arraylist.java:424)  at com.google.common.collect.abstractmultimap.remove(abstractmultimap.java:219)  at com.google.common.collect.arraylistmultimap.remove(arraylistmultimap.java:60)  at org.apache.cassandra.net.messagingservice.responsereceivedfrom(messagingservice.java:436)  at org.apache.cassandra.net.responseverbhandler.doverb(responseverbhandler.java:40)  at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:63)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)<code> <text> hi, i added a node to the cluster (20 nodes in total) and ran repair on it after a while. the repair still runs, but there are errors in the log file (see below). some of the data in the clsuter has been filled with rc-4. the cluster runs on version 0.7.0 (the release linked on the main cassandra web site). any ideas what might cause this?  (ps. 0.7.0 turns up as unreleased version in affects version/s ",
        "label": 274
    },
    {
        "text": "repeated incremental nodetool repair results in failed repairs due to running anticompaction <description> i'm trying to synchronize incremental repairs over multiple nodes in a cassandra cluster, and it does not seem to easily achievable. in principle, the process iterates through the nodes of the cluster and performs `nodetool -h $node repair --incremental`, but that sometimes fails on subsequent nodes. the reason for failing seems to be that the repair returns as soon as the repair and the local anticompaction has completed, but does not guarantee that remote anticompactions are complete. if i subsequently try to issue another repair command, they fail to start (and terminate with failure after about one minute). it usually isn't a problem, as the local anticompaction typically involves as much (or more) data as the remote ones, but sometimes not.<stacktrace> <code> <text> i'm trying to synchronize incremental repairs over multiple nodes in a cassandra cluster, and it does not seem to easily achievable. in principle, the process iterates through the nodes of the cluster and performs `nodetool -h $node repair --incremental`, but that sometimes fails on subsequent nodes. the reason for failing seems to be that the repair returns as soon as the repair and the local anticompaction has completed, but does not guarantee that remote anticompactions are complete. if i subsequently try to issue another repair command, they fail to start (and terminate with failure after about one minute). it usually isn't a problem, as the local anticompaction typically involves as much (or more) data as the remote ones, but sometimes not.",
        "label": 577
    },
    {
        "text": "allow literal value as parameter of udf   uda <description> i have defined the following udf create or replace function  maxof(current int, testvalue int) returns null on null input  returns int  language java  as  'return math.max(current,testvalue);' create table maxvalue(id int primary key, val int); insert into maxvalue(id, val) values(1, 100); select maxof(val, 101) from maxvalue where id=1; i got the following error message: syntaxexception: <errormessage code=2000 [syntax error in cql query] message=\"line 1:19 no viable alternative at input '101' (select maxof(val1, [101]...)\"> it would be nice to allow literal value as parameter of udf and uda too.  i was thinking about an use-case for an uda groupby() function where the end user can inject at runtime a literal value to select which aggregation he want to display, something similar to group by ... having <filter clause><stacktrace> <code> create or replace function  maxof(current int, testvalue int) returns null on null input  returns int  language java  as  'return math.max(current,testvalue);' create table maxvalue(id int primary key, val int); insert into maxvalue(id, val) values(1, 100); select maxof(val, 101) from maxvalue where id=1; syntaxexception: <errormessage code=2000 [syntax error in cql query] message='line 1:19 no viable alternative at input '101' (select maxof(val1, [101]...)'> create or replace function  maxof(current int, testvalue int) returns null on null input  returns int  language java  as  'return math.max(current,testvalue);' create table maxvalue(id int primary key, val int); insert into maxvalue(id, val) values(1, 100); select maxof(val, 101) from maxvalue where id=1; <text> i have defined the following udf i got the following error message: it would be nice to allow literal value as parameter of udf and uda too. i was thinking about an use-case for an uda groupby() function where the end user can inject at runtime a literal value to select which aggregation he want to display, something similar to group by ... having <filter clause>",
        "label": 520
    },
    {
        "text": "add  count  and  del  commands to cli <description> currently cassandra-cli does not allow removal of columns allow visualization of column count for a row (useful for large sets) the attached patch fixes this issue by adding three more commands, del <key>, del <key><col>, count <key>. no new tests were addded as i could not find any for the cli, if i have time i'll maybe add some but this is not likely to happen soon. as an aside, imvho the cassandra cli would be better as a small lib loaded within jirb/jython, parsing would come for free, flexibility would be much larger and it could be autogenerated from the thrift api. {{ { connected to localhost/9160 cassandra> get influences.influencer['k'] (column=kewl2, value=10; timestamp=1259756945822) (column=kewl, value=10; timestamp=1259753635959) returned 2 rows. cassandra> del influences.influencer['k'] done removal cassandra> get influences.influencer['k'] returned 0 rows. cassandra> set influences.influencer['k']['kewl2'] = '10' value inserted. cassandra> count influences.influencer['k'] 1 columns cassandra> del influences.influencer['k']['kewl2'] done removal cassandra> count influences.influencer['k'] 1 columns cassandra> get influences.influencer['k'] returned 0 rows. cassandra> count influences.influencer['k'] 1 columns cassandra> del influences.influencer['k'] done removal cassandra> count influences.influencer['k'] 0 columns cassandra> get influences.influencer['k'] returned 0 rows. }} }<stacktrace> <code> {{ }<text> currently cassandra-cli does not the attached patch fixes this issue by adding three more commands, del <key>, del <key><col>, count <key>. no new tests were addded as i could not find any for the cli, if i have time i'll maybe add some but this is not likely to happen soon. as an aside, imvho the cassandra cli would be better as a small lib loaded within jirb/jython, parsing would come for free, flexibility would be much larger and it could be autogenerated from the thrift api. ",
        "label": 587
    },
    {
        "text": "better error handling for commitlogsyncdelay <description> when commitlogsyncdelay is missing from the config, cassandra throws a numberformatexception. since this required directive is new for 0.4, it should probably have better handling, (like commitlogsync does for example).<stacktrace> <code> <text> when commitlogsyncdelay is missing from the config, cassandra throws a numberformatexception. since this required directive is new for 0.4, it should probably have better handling, (like commitlogsync does for example).",
        "label": 274
    },
    {
        "text": "failure to flush commit log <description> the following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup: info - replaying /var/lib/cassandra/commitlog/commitlog-1262855754427.log, /var/lib/cassandra/commitlog/commitlog-1262832689989.log, /var/lib/cassandra/commitlog/commitlog-1262885833186.log, /var/lib/cassandra/commitlog/commitlog-1262900845019.log, /var/lib/cassandra/commitlog/commitlog-1262913267844.log, /var/lib/cassandra/commitlog/commitlog-1262927898170.log, /var/lib/cassandra/commitlog/commitlog-1262961421039.log, /var/lib/cassandra/commitlog/commitlog-1262977175175.log, /var/lib/cassandra/commitlog/commitlog-1262989588783.log, /var/lib/cassandra/commitlog/commitlog-1263000573676.log, /var/lib/cassandra/commitlog/commitlog-1263013691393.log, /var/lib/cassandra/commitlog/commitlog-1263044706108.log, /var/lib/cassandra/commitlog/commitlog-1263060004191.log, /var/lib/cassandra/commitlog/commitlog-1263071446342.log, /var/lib/cassandra/commitlog/commitlog-1263082950154.log, /var/lib/cassandra/commitlog/commitlog-1263095400814.log, /var/lib/cassandra/commitlog/commitlog-1263118331046.log, /var/lib/cassandra/commitlog/commitlog-1263143402963.log, /var/lib/cassandra/commitlog/commitlog-1263155294308.log, /var/lib/cassandra/commitlog/commitlog-1263166154352.log, /var/lib/cassandra/commitlog/commitlog-1263178359247.log, /var/lib/cassandra/commitlog/commitlog-1263202112017.log, /var/lib/cassandra/commitlog/commitlog-1263230932274.log, /var/lib/cassandra/commitlog/commitlog-1263250726505.log, /var/lib/cassandra/commitlog/commitlog-1263264159438.log, /var/lib/cassandra/commitlog/commitlog-1263289964249.log, /var/lib/cassandra/commitlog/commitlog-1263317974387.log, /var/lib/cassandra/commitlog/commitlog-1263331989090.log, /var/lib/cassandra/commitlog/commitlog-1263344147667.log, /var/lib/cassandra/commitlog/commitlog-1263359751527.log, /var/lib/cassandra/commitlog/commitlog-1263395707008.log, /var/lib/cassandra/commitlog/commitlog-1263397833524.log, /var/lib/cassandra/commitlog/commitlog-1263398736183.log, /var/lib/cassandra/commitlog/commitlog-1263399753707.log, /var/lib/cassandra/commitlog/commitlog-1263401667504.log, /var/lib/cassandra/commitlog/commitlog-1263404640782.log, /var/lib/cassandra/commitlog/commitlog-1263405827234.log, /var/lib/cassandra/commitlog/commitlog-1263406901115.log  info - locationinfo has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(locationinfo)@25934689  info - hintscolumnfamily has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(hintscolumnfamily)@4766820  info - adxrequeststatistics has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(adxrequeststatistics)@21521158  info - tokengoogleidcf has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(tokengoogleidcf)@22889075  java.lang.reflect.invocationtargetexception  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.commons.daemon.support.daemonloader.load(daemonloader.java:160)  caused by: java.lang.assertionerror: blocking serialized executor is not yet implemented  at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor$1.rejectedexecution(debuggablethreadpoolexecutor.java:84)  at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:767)  at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:658)  at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:78)  at org.apache.cassandra.db.columnfamilystore.submitflush(columnfamilystore.java:1045)  at org.apache.cassandra.db.columnfamilystore.switchmemtable(columnfamilystore.java:395)  at org.apache.cassandra.db.columnfamilystore.forceflush(columnfamilystore.java:448)  at org.apache.cassandra.db.table.flush(table.java:464)  at org.apache.cassandra.db.commitlog.recover(commitlog.java:397)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:65)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:90)  at org.apache.cassandra.service.cassandradaemon.init(cassandradaemon.java:135)  ... 5 more and the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush': root@domu-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush logger  exception in thread \"main\" java.lang.assertionerror: blocking serialized executor is not yet implemented  at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor$1.rejectedexecution(debuggablethreadpoolexecutor.java:84)  at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:767)  at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:658)  at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:78)  at org.apache.cassandra.db.columnfamilystore.submitflush(columnfamilystore.java:1045)  at org.apache.cassandra.db.columnfamilystore.switchmemtable(columnfamilystore.java:395)  at org.apache.cassandra.db.columnfamilystore.forceflush(columnfamilystore.java:448)  at org.apache.cassandra.service.storageservice.forcetableflush(storageservice.java:984)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:93)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:27)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:208)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:120)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:262)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:836)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:761)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1426)  at javax.management.remote.rmi.rmiconnectionimpl.access$200(rmiconnectionimpl.java:72)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1264)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1359)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:788)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:305)  at sun.rmi.transport.transport$1.run(transport.java:159)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:155)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:535)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:790)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:649)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)<stacktrace> info - replaying /var/lib/cassandra/commitlog/commitlog-1262855754427.log, /var/lib/cassandra/commitlog/commitlog-1262832689989.log, /var/lib/cassandra/commitlog/commitlog-1262885833186.log, /var/lib/cassandra/commitlog/commitlog-1262900845019.log, /var/lib/cassandra/commitlog/commitlog-1262913267844.log, /var/lib/cassandra/commitlog/commitlog-1262927898170.log, /var/lib/cassandra/commitlog/commitlog-1262961421039.log, /var/lib/cassandra/commitlog/commitlog-1262977175175.log, /var/lib/cassandra/commitlog/commitlog-1262989588783.log, /var/lib/cassandra/commitlog/commitlog-1263000573676.log, /var/lib/cassandra/commitlog/commitlog-1263013691393.log, /var/lib/cassandra/commitlog/commitlog-1263044706108.log, /var/lib/cassandra/commitlog/commitlog-1263060004191.log, /var/lib/cassandra/commitlog/commitlog-1263071446342.log, /var/lib/cassandra/commitlog/commitlog-1263082950154.log, /var/lib/cassandra/commitlog/commitlog-1263095400814.log, /var/lib/cassandra/commitlog/commitlog-1263118331046.log, /var/lib/cassandra/commitlog/commitlog-1263143402963.log, /var/lib/cassandra/commitlog/commitlog-1263155294308.log, /var/lib/cassandra/commitlog/commitlog-1263166154352.log, /var/lib/cassandra/commitlog/commitlog-1263178359247.log, /var/lib/cassandra/commitlog/commitlog-1263202112017.log, /var/lib/cassandra/commitlog/commitlog-1263230932274.log, /var/lib/cassandra/commitlog/commitlog-1263250726505.log, /var/lib/cassandra/commitlog/commitlog-1263264159438.log, /var/lib/cassandra/commitlog/commitlog-1263289964249.log, /var/lib/cassandra/commitlog/commitlog-1263317974387.log, /var/lib/cassandra/commitlog/commitlog-1263331989090.log, /var/lib/cassandra/commitlog/commitlog-1263344147667.log, /var/lib/cassandra/commitlog/commitlog-1263359751527.log, /var/lib/cassandra/commitlog/commitlog-1263395707008.log, /var/lib/cassandra/commitlog/commitlog-1263397833524.log, /var/lib/cassandra/commitlog/commitlog-1263398736183.log, /var/lib/cassandra/commitlog/commitlog-1263399753707.log, /var/lib/cassandra/commitlog/commitlog-1263401667504.log, /var/lib/cassandra/commitlog/commitlog-1263404640782.log, /var/lib/cassandra/commitlog/commitlog-1263405827234.log, /var/lib/cassandra/commitlog/commitlog-1263406901115.log  info - locationinfo has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(locationinfo)@25934689  info - hintscolumnfamily has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(hintscolumnfamily)@4766820  info - adxrequeststatistics has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(adxrequeststatistics)@21521158  info - tokengoogleidcf has reached its threshold; switching in a fresh memtable  info - enqueuing flush of memtable(tokengoogleidcf)@22889075  java.lang.reflect.invocationtargetexception  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at org.apache.commons.daemon.support.daemonloader.load(daemonloader.java:160)  caused by: java.lang.assertionerror: blocking serialized executor is not yet implemented  at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor$1.rejectedexecution(debuggablethreadpoolexecutor.java:84)  at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:767)  at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:658)  at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:78)  at org.apache.cassandra.db.columnfamilystore.submitflush(columnfamilystore.java:1045)  at org.apache.cassandra.db.columnfamilystore.switchmemtable(columnfamilystore.java:395)  at org.apache.cassandra.db.columnfamilystore.forceflush(columnfamilystore.java:448)  at org.apache.cassandra.db.table.flush(table.java:464)  at org.apache.cassandra.db.commitlog.recover(commitlog.java:397)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:65)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:90)  at org.apache.cassandra.service.cassandradaemon.init(cassandradaemon.java:135)  ... 5 more root@domu-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush logger  exception in thread 'main' java.lang.assertionerror: blocking serialized executor is not yet implemented  at org.apache.cassandra.concurrent.debuggablethreadpoolexecutor$1.rejectedexecution(debuggablethreadpoolexecutor.java:84)  at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:767)  at java.util.concurrent.threadpoolexecutor.execute(threadpoolexecutor.java:658)  at java.util.concurrent.abstractexecutorservice.submit(abstractexecutorservice.java:78)  at org.apache.cassandra.db.columnfamilystore.submitflush(columnfamilystore.java:1045)  at org.apache.cassandra.db.columnfamilystore.switchmemtable(columnfamilystore.java:395)  at org.apache.cassandra.db.columnfamilystore.forceflush(columnfamilystore.java:448)  at org.apache.cassandra.service.storageservice.forcetableflush(storageservice.java:984)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:93)  at com.sun.jmx.mbeanserver.standardmbeanintrospector.invokem2(standardmbeanintrospector.java:27)  at com.sun.jmx.mbeanserver.mbeanintrospector.invokem(mbeanintrospector.java:208)  at com.sun.jmx.mbeanserver.perinterface.invoke(perinterface.java:120)  at com.sun.jmx.mbeanserver.mbeansupport.invoke(mbeansupport.java:262)  at com.sun.jmx.interceptor.defaultmbeanserverinterceptor.invoke(defaultmbeanserverinterceptor.java:836)  at com.sun.jmx.mbeanserver.jmxmbeanserver.invoke(jmxmbeanserver.java:761)  at javax.management.remote.rmi.rmiconnectionimpl.dooperation(rmiconnectionimpl.java:1426)  at javax.management.remote.rmi.rmiconnectionimpl.access$200(rmiconnectionimpl.java:72)  at javax.management.remote.rmi.rmiconnectionimpl$privilegedoperation.run(rmiconnectionimpl.java:1264)  at javax.management.remote.rmi.rmiconnectionimpl.doprivilegedoperation(rmiconnectionimpl.java:1359)  at javax.management.remote.rmi.rmiconnectionimpl.invoke(rmiconnectionimpl.java:788)  at sun.reflect.nativemethodaccessorimpl.invoke0(native method)  at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:39)  at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:25)  at java.lang.reflect.method.invoke(method.java:597)  at sun.rmi.server.unicastserverref.dispatch(unicastserverref.java:305)  at sun.rmi.transport.transport$1.run(transport.java:159)  at java.security.accesscontroller.doprivileged(native method)  at sun.rmi.transport.transport.servicecall(transport.java:155)  at sun.rmi.transport.tcp.tcptransport.handlemessages(tcptransport.java:535)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run0(tcptransport.java:790)  at sun.rmi.transport.tcp.tcptransport$connectionhandler.run(tcptransport.java:649)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:619)<code> <text> the following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup: and the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush': ",
        "label": 274
    },
    {
        "text": "drop user is not case sensitive <description> as per the summary drop user is not case sensitive, so: create user 'test'; list users;  name      | super -----------+-------       test | false  cassandra |  true drop user 'test'; invalidrequest: code=2200 [invalid query] message=\"test doesn't exist\" drop role is case-sensitive and will drop the above user.<stacktrace> <code> create user 'test'; list users;  name      | super -----------+-------       test | false  cassandra |  true drop user 'test'; invalidrequest: code=2200 [invalid query] message='test doesn't exist' <text> as per the summary drop user is not case sensitive, so: drop role is case-sensitive and will drop the above user.",
        "label": 321
    },
    {
        "text": "avoid id conflicts from concurrent schema changes <description> change columnfamily identifiers to be uuids instead of sequential integers. would be useful in the situation when nodes simultaneously trying to create columnfamilies.<stacktrace> <code> <text> change columnfamily identifiers to be uuids instead of sequential integers. would be useful in the situation when nodes simultaneously trying to create columnfamilies.",
        "label": 412
    },
    {
        "text": "race condition between flushing and compaction stalls compaction indefinitely <description> seen on cassandra 3.11.4 with openjdk 8u212, although i've seen this a few times before, also on 3.11.3. it's a rare issue so i've not bothered with trying to trace it until now. debug [nativepoolcleaner] 2019-07-18 01:12:41,799 columnfamilystore.java:1325 - flushing largest cfs(keyspace='keyspacename', columnfamily='tablename') to free up room. used total: 0.10/0.33, live: 0.10/0.33, flushing: 0.00/0.00, this: 0.09/0.19 debug [nativepoolcleaner] 2019-07-18 01:12:41,800 columnfamilystore.java:935 - enqueuing flush of tablename: 267.930mib (9%) on-heap, 575.580mib (19%) off-heap debug [perdiskmemtableflushwriter_0:204] 2019-07-18 01:12:42,480 memtable.java:456 - writing memtable-tablename@498336646(520.721mib serialized bytes, 870200 ops, 9%/19% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)] info  [service thread] 2019-07-18 01:12:43,616 gcinspector.java:284 - g1 young generation gc in 227ms.  g1 eden space: 14713618432 -> 0; g1 old gen: 13240876928 -> 13259198848; g1 survivor space: 276824064 -> 268435456; info  [service thread] 2019-07-18 01:12:56,251 gcinspector.java:284 - g1 young generation gc in 206ms.  g1 eden space: 14713618432 -> 0; g1 old gen: 13259198848 -> 13285123456; g1 survivor space: 268435456 -> 285212672; debug [perdiskmemtableflushwriter_0:204] 2019-07-18 01:12:56,693 memtable.java:485 - completed flushing /cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db (524.023mib) for commitlog position commitlogposition(segmentid=1563386911266, position=32127822) debug [memtableflushwriter:204] 2019-07-18 01:12:57,620 columnfamilystore.java:1233 - flushed to [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] (1 sstables, 518.714mib), biggest 518.714mib, smallest 518.714mib warn  [compactionexecutor:1617] 2019-07-18 01:12:57,628 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. this final line then starts repeating about once per minute: warn  [compactionexecutor:1610] 2019-07-18 01:13:18,898 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1611] 2019-07-18 01:14:18,899 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1622] 2019-07-18 01:15:18,899 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1436] 2019-07-18 01:16:15,073 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1618] 2019-07-18 01:16:18,899 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1611] 2019-07-18 01:17:18,900 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1606] 2019-07-18 01:18:18,900 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1630] 2019-07-18 01:19:18,902 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1627] 2019-07-18 01:20:18,904 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1638] 2019-07-18 01:21:18,904 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1631] 2019-07-18 01:22:18,905 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1636] 2019-07-18 01:22:58,220 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1625] 2019-07-18 01:23:18,905 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. it will keep going like this for days, until restarted, but compaction won't run until then, so sstables pile up.<stacktrace> <code> debug [nativepoolcleaner] 2019-07-18 01:12:41,799 columnfamilystore.java:1325 - flushing largest cfs(keyspace='keyspacename', columnfamily='tablename') to free up room. used total: 0.10/0.33, live: 0.10/0.33, flushing: 0.00/0.00, this: 0.09/0.19 debug [nativepoolcleaner] 2019-07-18 01:12:41,800 columnfamilystore.java:935 - enqueuing flush of tablename: 267.930mib (9%) on-heap, 575.580mib (19%) off-heap debug [perdiskmemtableflushwriter_0:204] 2019-07-18 01:12:42,480 memtable.java:456 - writing memtable-tablename@498336646(520.721mib serialized bytes, 870200 ops, 9%/19% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)] info  [service thread] 2019-07-18 01:12:43,616 gcinspector.java:284 - g1 young generation gc in 227ms.  g1 eden space: 14713618432 -> 0; g1 old gen: 13240876928 -> 13259198848; g1 survivor space: 276824064 -> 268435456; info  [service thread] 2019-07-18 01:12:56,251 gcinspector.java:284 - g1 young generation gc in 206ms.  g1 eden space: 14713618432 -> 0; g1 old gen: 13259198848 -> 13285123456; g1 survivor space: 268435456 -> 285212672; debug [perdiskmemtableflushwriter_0:204] 2019-07-18 01:12:56,693 memtable.java:485 - completed flushing /cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db (524.023mib) for commitlog position commitlogposition(segmentid=1563386911266, position=32127822) debug [memtableflushwriter:204] 2019-07-18 01:12:57,620 columnfamilystore.java:1233 - flushed to [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] (1 sstables, 518.714mib), biggest 518.714mib, smallest 518.714mib warn  [compactionexecutor:1617] 2019-07-18 01:12:57,628 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1610] 2019-07-18 01:13:18,898 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1611] 2019-07-18 01:14:18,899 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1622] 2019-07-18 01:15:18,899 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1436] 2019-07-18 01:16:15,073 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1618] 2019-07-18 01:16:18,899 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1611] 2019-07-18 01:17:18,900 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1606] 2019-07-18 01:18:18,900 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1630] 2019-07-18 01:19:18,902 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1627] 2019-07-18 01:20:18,904 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1638] 2019-07-18 01:21:18,904 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1631] 2019-07-18 01:22:18,905 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1636] 2019-07-18 01:22:58,220 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. warn  [compactionexecutor:1625] 2019-07-18 01:23:18,905 leveledcompactionstrategy.java:144 - could not acquire references for compacting sstables [bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-data.db'), bigtablereader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. will retry later. <text> seen on cassandra 3.11.4 with openjdk 8u212, although i've seen this a few times before, also on 3.11.3. it's a rare issue so i've not bothered with trying to trace it until now. this final line then starts repeating about once per minute: it will keep going like this for days, until restarted, but compaction won't run until then, so sstables pile up.",
        "label": 321
    },
    {
        "text": "dtest failure in upgrade tests cql tests testcqlnodes3rf3 upgrade next x to current x cql3 non compound range tombstones test <description> example failure: http://cassci.datastax.com/job/upgrade_tests-all-custom_branch_runs/37/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3_upgrade_next_2_1_x_to_current_3_x/cql3_non_compound_range_tombstones_test failed on cassci build upgrade_tests-all-custom_branch_runs #37 failing here:   file \"/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py\", line 1667, in cql3_non_compound_range_tombstones_test     self.assertequal(6, len(row), row) as we see, the row has more data returned. this implies that data isn't properly being shadowed by the tombstone. as such, i'm filing this directly as a bug.<stacktrace> <code>   file '/home/automaton/cassandra-dtest/upgrade_tests/cql_tests.py', line 1667, in cql3_non_compound_range_tombstones_test     self.assertequal(6, len(row), row) http://cassci.datastax.com/job/upgrade_tests-all-custom_branch_runs/37/testreport/upgrade_tests.cql_tests/testcqlnodes3rf3_upgrade_next_2_1_x_to_current_3_x/cql3_non_compound_range_tombstones_test failed on cassci build upgrade_tests-all-custom_branch_runs #37 <text> example failure: failing here: as we see, the row has more data returned. this implies that data isn't properly being shadowed by the tombstone. as such, i'm filing this directly as a bug.",
        "label": 538
    },
    {
        "text": "classcastexception during hinted handoff <description> error 08:51:00,200 fatal exception in thread thread[optionaltasks:1,5,main] java.lang.classcastexception: org.apache.cassandra.dht.bigintegertoken cannot be cast to org.apache.cassandra.db.rowposition         at org.apache.cassandra.db.columnfamilystore.getsequentialiterator(columnfamilystore.java:1286)         at org.apache.cassandra.db.columnfamilystore.getrangeslice(columnfamilystore.java:1356)         at org.apache.cassandra.db.hintedhandoffmanager.schedulealldeliveries(hintedhandoffmanager.java:351)         at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:84)         at org.apache.cassandra.db.hintedhandoffmanager$1.run(hintedhandoffmanager.java:119)         at java.util.concurrent.executors$runnableadapter.call(executors.java:441)         at java.util.concurrent.futuretask$sync.innerrunandreset(futuretask.java:317)         at java.util.concurrent.futuretask.runandreset(futuretask.java:150)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$101(scheduledthreadpoolexecutor.java:98)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.runperiodic(scheduledthreadpoolexecutor.java:180)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:204)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662)<stacktrace> error 08:51:00,200 fatal exception in thread thread[optionaltasks:1,5,main] java.lang.classcastexception: org.apache.cassandra.dht.bigintegertoken cannot be cast to org.apache.cassandra.db.rowposition         at org.apache.cassandra.db.columnfamilystore.getsequentialiterator(columnfamilystore.java:1286)         at org.apache.cassandra.db.columnfamilystore.getrangeslice(columnfamilystore.java:1356)         at org.apache.cassandra.db.hintedhandoffmanager.schedulealldeliveries(hintedhandoffmanager.java:351)         at org.apache.cassandra.db.hintedhandoffmanager.access$000(hintedhandoffmanager.java:84)         at org.apache.cassandra.db.hintedhandoffmanager$1.run(hintedhandoffmanager.java:119)         at java.util.concurrent.executors$runnableadapter.call(executors.java:441)         at java.util.concurrent.futuretask$sync.innerrunandreset(futuretask.java:317)         at java.util.concurrent.futuretask.runandreset(futuretask.java:150)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.access$101(scheduledthreadpoolexecutor.java:98)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.runperiodic(scheduledthreadpoolexecutor.java:180)         at java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask.run(scheduledthreadpoolexecutor.java:204)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662)<code> <text> ",
        "label": 520
    },
    {
        "text": "sstables may not be properly removed from original strategy when repair status changed <description> in csm.handlerepairstatuschangednotification(), cassandra-14621 changed the original semantic of removing sstables in repaired first to adding sstables into unrepaired first... in case of lcs, adding sstables may modify their levels, so they won't be removed from repaired which locates sstables by level.  trunk   circle<stacktrace> <code> <text> in csm.handlerepairstatuschangednotification(), cassandra-14621 changed the original semantic of removing sstables in repaired first to adding sstables into unrepaired first... in case of lcs, adding sstables may modify their levels, so they won't be removed from repaired which locates sstables by level. ",
        "label": 580
    },
    {
        "text": "always flush on truncate <description> commit log grows infinitely after cf truncate operation via cassandra-cli, regardless cf receives writes or not thereafter.  cf's could be non-cql standard and super column type. creation of snapshots after truncate is turned off.  commit log may start grow promptly, may start grow later, on a few only or on all nodes at once.  nothing special in the system log. no idea how to reproduce.  after rolling restart commit logs are cleared and back to normal. just annoying to do rolling restart after each truncate.<stacktrace> <code> <text> commit log grows infinitely after cf truncate operation via cassandra-cli, regardless cf receives writes or not thereafter.  cf's could be non-cql standard and super column type. creation of snapshots after truncate is turned off.  commit log may start grow promptly, may start grow later, on a few only or on all nodes at once.  nothing special in the system log. no idea how to reproduce.  after rolling restart commit logs are cleared and back to normal. just annoying to do rolling restart after each truncate.",
        "label": 244
    },
    {
        "text": "queryprocessor never removes internal statements from its cache <description> queryprocessor holds a reference to internalstatements, a map for prepared statements used internally. those commands don't mix with the ones created by the user, but the problem is that if a ks/cf is dropped and then recreated internalstatements entries will point to invalid preparedstatements (old cf_id) so any operation on those statements will fail thereafter. in fact, as of today, this map is never actually cleaned, no matter what. this problem is similar to the ones addressed by https://issues.apache.org/jira/browse/cassandra-8652 and https://issues.apache.org/jira/browse/cassandra-7566, so those issues provide any further context. i am attaching a patch to this issue.<stacktrace> <code> <text> queryprocessor holds a reference to internalstatements, a map for prepared statements used internally. those commands don't mix with the ones created by the user, but the problem is that if a ks/cf is dropped and then recreated internalstatements entries will point to invalid preparedstatements (old cf_id) so any operation on those statements will fail thereafter. in fact, as of today, this map is never actually cleaned, no matter what. this problem is similar to the ones addressed by https://issues.apache.org/jira/browse/cassandra-8652 and https://issues.apache.org/jira/browse/cassandra-7566, so those issues provide any further context. i am attaching a patch to this issue.",
        "label": 164
    },
    {
        "text": "dtest failure in largecolumn test testlargecolumn cleanup test <description> example failure at: http://cassci.datastax.com/job/trunk_offheap_dtest/200/testreport/largecolumn_test/testlargecolumn/cleanup_test/ node 1 contains the following oom in its log: error [sharedpool-worker-1] 2016-05-16 22:54:10,112 message.java:611 - unexpected exception during request; channel = [id: 0xb97f2640, l:/127.0.0.1:9042 - r:/127.0.0.1:48190] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error [sharedpool-worker-1] 2016-05-16 22:54:10,756 message.java:611 - unexpected exception during request; channel = [id: 0xba0be401, l:/127.0.0.1:9042 - r:/127.0.0.1:48191] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error [sharedpool-worker-1] 2016-05-16 22:54:11,397 message.java:611 - unexpected exception during request; channel = [id: 0x6a690f39, l:/127.0.0.1:9042 - r:/127.0.0.1:48193] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error [sharedpool-worker-1] 2016-05-16 22:54:12,006 message.java:611 - unexpected exception during request; channel = [id: 0xc05601f1, l:/127.0.0.1:9042 - r:/127.0.0.1:48192] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] given that this test has been stable for a long time, only runs two nodes, and we haven't made any recent changes to the environment, i'm filing this directly as a bug. node logs are attached. please take note that this was run with offheap_memtables, as that may be relevant.<stacktrace> error [sharedpool-worker-1] 2016-05-16 22:54:10,112 message.java:611 - unexpected exception during request; channel = [id: 0xb97f2640, l:/127.0.0.1:9042 - r:/127.0.0.1:48190] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error [sharedpool-worker-1] 2016-05-16 22:54:10,756 message.java:611 - unexpected exception during request; channel = [id: 0xba0be401, l:/127.0.0.1:9042 - r:/127.0.0.1:48191] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error [sharedpool-worker-1] 2016-05-16 22:54:11,397 message.java:611 - unexpected exception during request; channel = [id: 0x6a690f39, l:/127.0.0.1:9042 - r:/127.0.0.1:48193] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] error [sharedpool-worker-1] 2016-05-16 22:54:12,006 message.java:611 - unexpected exception during request; channel = [id: 0xc05601f1, l:/127.0.0.1:9042 - r:/127.0.0.1:48192] java.lang.outofmemoryerror: java heap space at org.apache.cassandra.transport.cbutil.readrawbytes(cbutil.java:533) ~[main/:na] at org.apache.cassandra.transport.cbutil.readboundvalue(cbutil.java:407) ~[main/:na] at org.apache.cassandra.transport.cbutil.readvaluelist(cbutil.java:462) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:417) ~[main/:na] at org.apache.cassandra.cql3.queryoptions$codec.decode(queryoptions.java:365) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:45) ~[main/:na] at org.apache.cassandra.transport.messages.executemessage$1.decode(executemessage.java:41) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:280) ~[main/:na] at org.apache.cassandra.transport.message$protocoldecoder.decode(message.java:261) ~[main/:na] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:89) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.messagetomessagedecoder.channelread(messagetomessagedecoder.java:103) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.firechannelread(bytetomessagedecoder.java:277) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.handler.codec.bytetomessagedecoder.channelread(bytetomessagedecoder.java:264) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.invokechannelread(abstractchannelhandlercontext.java:292) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.abstractchannelhandlercontext.firechannelread(abstractchannelhandlercontext.java:278) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.defaultchannelpipeline.firechannelread(defaultchannelpipeline.java:962) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.abstractepollstreamchannel$epollstreamunsafe.epollinready(abstractepollstreamchannel.java:879) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.processready(epolleventloop.java:360) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.channel.epoll.epolleventloop.run(epolleventloop.java:276) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.singlethreadeventexecutor$2.run(singlethreadeventexecutor.java:112) ~[netty-all-4.0.36.final.jar:4.0.36.final] at io.netty.util.concurrent.defaultthreadfactory$defaultrunnabledecorator.run(defaultthreadfactory.java:137) ~[netty-all-4.0.36.final.jar:4.0.36.final] at java.lang.thread.run(thread.java:745) [na:1.8.0_45] <code> http://cassci.datastax.com/job/trunk_offheap_dtest/200/testreport/largecolumn_test/testlargecolumn/cleanup_test/ <text> example failure at: node 1 contains the following oom in its log: given that this test has been stable for a long time, only runs two nodes, and we haven't made any recent changes to the environment, i'm filing this directly as a bug. node logs are attached. please take note that this was run with offheap_memtables, as that may be relevant.",
        "label": 25
    },
    {
        "text": "statement concerning default parallelgcthreads in jvm options is not correct <description> from jvm.options: the jvm maximum is 8 pgc threads and 1/4 of that for concgc. this is not correct. if there are more than eight cpus, the default becomes 5/8 of the number of cpus rounded up to the nearest even number (it seems - see below). see -xx:parallelgcthreads=n secion of http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html pretty easy to test with > 16 cores (as 5/8 of such is 10): turn on gc logging, leave the defaults, and the g1gc output will show something like: [parallel time: 342.6 ms, gc workers: 16] on a 24 core system in this case.<stacktrace> <code> [parallel time: 342.6 ms, gc workers: 16] <text> from jvm.options: the jvm maximum is 8 pgc threads and 1/4 of that for concgc. this is not correct. if there are more than eight cpus, the default becomes 5/8 of the number of cpus rounded up to the nearest even number (it seems - see below). see -xx:parallelgcthreads=n secion of http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html pretty easy to test with > 16 cores (as 5/8 of such is 10): turn on gc logging, leave the defaults, and the g1gc output will show something like: on a 24 core system in this case.",
        "label": 373
    },
    {
        "text": "fix batch test testbatch logged batch gcgs below threshold multi single table test on cassci <description> this is a recent regression: http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastcompletedbuild/testreport/batch_test/testbatch/logged_batch_gcgs_below_threshold_multi_table_test/history/ based on when it started failing, it looks like the regression was introduced by the recent logging changes. pinging benjamin lerer and paulo motta (deprecated) to have a look. my initial guess was that the logs the test depends on were moved to the new debug log. however, the tests passes when i run it manually on openstack, so it could just be a configuration issue on cassci.<stacktrace> <code> http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastcompletedbuild/testreport/batch_test/testbatch/logged_batch_gcgs_below_threshold_multi_table_test/history/ <text> this is a recent regression: based on when it started failing, it looks like the regression was introduced by the recent logging changes. pinging benjamin lerer and paulo motta (deprecated) to have a look. my initial guess was that the logs the test depends on were moved to the new debug log. however, the tests passes when i run it manually on openstack, so it could just be a configuration issue on cassci.",
        "label": 409
    },
    {
        "text": "support a maven release <description> <stacktrace> <code> <text> ",
        "label": 206
    },
    {
        "text": "fromjson null  throws java lang nullpointerexception on cassandra <description> basically, fromjson throws a java.lang.nullpointerexception when null is passed, instead of just returning a null itself. say i create a udt and a table as follows: create type type1 ( id int, name text ); create table table1 ( id int, t frozen<type1>, primary key (id) ); and then try and insert a row as such: insert into table1 (id, t) values (1, fromjson(null)); i get the error: java.lang.nullpointerexception this works as expected: insert into table1 (id, t) values (1, null); programmatically, one does not always know when a udt will be null, hence me expecting fromjson to just return null.<stacktrace> <code> create type type1 ( id int, name text ); create table table1 ( id int, t frozen<type1>, primary key (id) ); insert into table1 (id, t) values (1, fromjson(null)); i get the error: java.lang.nullpointerexception this works as expected: insert into table1 (id, t) values (1, null); <text> basically, fromjson throws a java.lang.nullpointerexception when null is passed, instead of just returning a null itself. say i create a udt and a table as follows: and then try and insert a row as such: programmatically, one does not always know when a udt will be null, hence me expecting fromjson to just return null.",
        "label": 164
    },
    {
        "text": "udf cleanups  follow up  <description> the current code for udf is largely not reusing the pre-existing mechanics/code for native/hardcoded functions. i don't see a good reason for that but i do see downsides: it's more code to maintain and makes it much easier to have inconsitent behavior between hard-coded and user-defined function. more concretely, udfregistery/udffunctionoverloads fundamentally do the same thing than functions, we should just merge both. i'm also not sure there is a need for both ufmetadata and udfunction since ufmetadata really only store infos on a given function (contrarly to what the javadoc pretends). i suggest we consolidate all this to cleanup the code, but also as a way to fix 2 problems that the udf code has but that the existing code for \"native\" functions don't: if there is multiple overloads of a function, the udf code picks the first version whose argument types are compatible with the concrete arguments provided. this is broken for bind markers: we don't know the type of markers and so the first function match may not at all be what the user want. the only sensible choice is to detect that type of ambiguity and reject the query, asking the user to explicitly type-cast their bind marker (which is what the code for hard-coded function does). the udf code builds a function signature using the cql type names of the arguments and use that to distinguish multiple overrides in the schema. this means in particular that f(v text) and f(v varchar) are considered distinct, which is wrong since cql considers varchar as a simple alias of text. and in fact, the function resolution does consider them aliases leading to seemingly broken behavior. there is a few other small problems that i'm proposing to fix while doing this cleanup: function creation only use the function name when checking if the function exists, which is not enough since we allow multiple over-loadings. you can bypass the check by using \"or replace\" but that's obviously broken. if not exists for function creation is broken. the code allows to replace a function (with or replace) by a new function with an incompatible return type. imo that's dodgy and we should refuse it (users can still drop and re-create the method if they really want).<stacktrace> <code> <text> the current code for udf is largely not reusing the pre-existing mechanics/code for native/hardcoded functions. i don't see a good reason for that but i do see downsides: it's more code to maintain and makes it much easier to have inconsitent behavior between hard-coded and user-defined function. more concretely, udfregistery/udffunctionoverloads fundamentally do the same thing than functions, we should just merge both. i'm also not sure there is a need for both ufmetadata and udfunction since ufmetadata really only store infos on a given function (contrarly to what the javadoc pretends). i suggest we consolidate all this to cleanup the code, but also as a way to fix 2 problems that the udf code has but that the existing code for 'native' functions don't: there is a few other small problems that i'm proposing to fix while doing this cleanup:",
        "label": 520
    },
    {
        "text": "dtest failure in cql tracing test testcqltracing tracing default impl test and tracing unknown impl test <description> example failure: http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/174/testreport/cql_tracing_test/testcqltracing/tracing_default_impl_test failed on cassci build cassandra-3.0_novnode_dtest #174<stacktrace> <code> http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/174/testreport/cql_tracing_test/testcqltracing/tracing_default_impl_test failed on cassci build cassandra-3.0_novnode_dtest #174<text> example failure: ",
        "label": 252
    },
    {
        "text": "secondary index not dropped until restart <description> when dropping the secondary index (via cassandra-cli), the describe keyspace still shows the built index entry. only after a restart of the cassandradaemon then the built index entry is gone. this seems indicate a problem with the index not really been dropped completed. to test, use a single node, create an index, then drop it from the cli (issue an update column family ... with metadata fields but not the index info) below is the original:  column families:  columnfamily: inode  \"stores file meta data\"  key validation class: org.apache.cassandra.db.marshal.bytestype  default column value validator: org.apache.cassandra.db.marshal.bytestype  columns sorted by: org.apache.cassandra.db.marshal.bytestype  row cache size / save period in seconds: 0.0/0  key cache size / save period in seconds: 0.0/14400  memtable thresholds: 0.103125/22/1440 (millions of ops/mb/minutes)  gc grace seconds: 60  compaction min/max thresholds: 4/32  read repair chance: 1.0  replicate on write: false  built indexes: [inode.path, inode.sentinel]  column metadata:  column name: path (70617468)  validation class: org.apache.cassandra.db.marshal.bytestype  index name: path  index type: keys  column name: sentinel (73656e74696e656c)  validation class: org.apache.cassandra.db.marshal.bytestype  index name: sentinel  index type: keys issue an update: [default@unknown] use cfs; authenticated to keyspace: cfs [default@cfs] update column family inode with comparator=bytestype and column_metadata=[{column_name:70617468, validation_class:bytestype}, {column_name:73656e74696e656c,validation_class:bytestype}]; fca46d00-783c-11e0-0000-242d50cf1fff waiting for schema agreement... ... schemas agree across the cluster describe the keyspace again:  keyspace: cfs:  replication strategy: org.apache.cassandra.locator.networktopologystrategy  options: [brisk:1, cassandra:0]  column families:  columnfamily: inode  \"stores file meta data\"  key validation class: org.apache.cassandra.db.marshal.bytestype  default column value validator: org.apache.cassandra.db.marshal.bytestype  columns sorted by: org.apache.cassandra.db.marshal.bytestype  row cache size / save period in seconds: 0.0/0  key cache size / save period in seconds: 0.0/14400  memtable thresholds: 0.103125/22/1440 (millions of ops/mb/minutes)  gc grace seconds: 60  compaction min/max thresholds: 4/32  read repair chance: 1.0  replicate on write: false  built indexes: [inode.path, inode.sentinel]  column metadata:  column name: path (70617468)  validation class: org.apache.cassandra.db.marshal.bytestype  column name: sentinel (73656e74696e656c)  validation class: org.apache.cassandra.db.marshal.bytestype notice the red line on built indexes restart cassandradaemon, describe again: keyspace: cfs:  replication strategy: org.apache.cassandra.locator.networktopologystrategy  options: [brisk:1, cassandra:0]  column families:  columnfamily: inode  \"stores file meta data\"  key validation class: org.apache.cassandra.db.marshal.bytestype  default column value validator: org.apache.cassandra.db.marshal.bytestype  columns sorted by: org.apache.cassandra.db.marshal.bytestype  row cache size / save period in seconds: 0.0/0  key cache size / save period in seconds: 0.0/14400  memtable thresholds: 0.103125/22/1440 (millions of ops/mb/minutes)  gc grace seconds: 60  compaction min/max thresholds: 4/32  read repair chance: 1.0  replicate on write: false  built indexes: []  column metadata:  column name: path (70617468)  validation class: org.apache.cassandra.db.marshal.bytestype  column name: sentinel (73656e74696e656c)  validation class: org.apache.cassandra.db.marshal.bytestype on another note, upon re-create the index, it does not appear the index is actually rebuilt. there is no need to restart cassandradaemon for the built index to show up from the describe. but the update goes very fast. we could tell the index is not being rebuilt because we were getting npe from: java.lang.runtimeexception: java.lang.nullpointerexception at org.apache.cassandra.service.indexscanverbhandler.doverb(indexscanverbhandler.java:51) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:72) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.lang.nullpointerexception at org.apache.cassandra.db.columnfamilystore.satisfies(columnfamilystore.java:1647) at org.apache.cassandra.db.columnfamilystore.scan(columnfamilystore.java:1594) at org.apache.cassandra.service.indexscanverbhandler.doverb(indexscanverbhandler.java:42) and after re-create the index, the exception resurface (the exception does not surface upon drop). if we drop the index files and remove them, then re-create the index, the npe is resolved: $ find /var/lib/cassandra/data/cfs -name \"*path*\" -o -name \"*sentinel* -exec rm {} \\;\"<stacktrace> java.lang.runtimeexception: java.lang.nullpointerexception at org.apache.cassandra.service.indexscanverbhandler.doverb(indexscanverbhandler.java:51) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:72) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.lang.nullpointerexception at org.apache.cassandra.db.columnfamilystore.satisfies(columnfamilystore.java:1647) at org.apache.cassandra.db.columnfamilystore.scan(columnfamilystore.java:1594) at org.apache.cassandra.service.indexscanverbhandler.doverb(indexscanverbhandler.java:42) <code> [default@unknown] use cfs; authenticated to keyspace: cfs [default@cfs] update column family inode with comparator=bytestype and column_metadata=[{column_name:70617468, validation_class:bytestype}, {column_name:73656e74696e656c,validation_class:bytestype}]; fca46d00-783c-11e0-0000-242d50cf1fff waiting for schema agreement... ... schemas agree across the cluster $ find /var/lib/cassandra/data/cfs -name '*path*' -o -name '*sentinel* -exec rm {} /;' column families:  columnfamily: inode  'stores file meta data'  key validation class: org.apache.cassandra.db.marshal.bytestype  default column value validator: org.apache.cassandra.db.marshal.bytestype  columns sorted by: org.apache.cassandra.db.marshal.bytestype  row cache size / save period in seconds: 0.0/0  key cache size / save period in seconds: 0.0/14400  memtable thresholds: 0.103125/22/1440 (millions of ops/mb/minutes)  gc grace seconds: 60  compaction min/max thresholds: 4/32  read repair chance: 1.0  replicate on write: false  built indexes: [inode.path, inode.sentinel]  column metadata:  column name: path (70617468)  validation class: org.apache.cassandra.db.marshal.bytestype  index name: path  index type: keys  column name: sentinel (73656e74696e656c)  validation class: org.apache.cassandra.db.marshal.bytestype  index name: sentinel  index type: keys describe the keyspace again:  keyspace: cfs:  replication strategy: org.apache.cassandra.locator.networktopologystrategy  options: [brisk:1, cassandra:0]  column families:  columnfamily: inode  'stores file meta data'  key validation class: org.apache.cassandra.db.marshal.bytestype  default column value validator: org.apache.cassandra.db.marshal.bytestype  columns sorted by: org.apache.cassandra.db.marshal.bytestype  row cache size / save period in seconds: 0.0/0  key cache size / save period in seconds: 0.0/14400  memtable thresholds: 0.103125/22/1440 (millions of ops/mb/minutes)  gc grace seconds: 60  compaction min/max thresholds: 4/32  read repair chance: 1.0  replicate on write: false  built indexes: [inode.path, inode.sentinel]  column metadata:  column name: path (70617468)  validation class: org.apache.cassandra.db.marshal.bytestype  column name: sentinel (73656e74696e656c)  validation class: org.apache.cassandra.db.marshal.bytestype keyspace: cfs:  replication strategy: org.apache.cassandra.locator.networktopologystrategy  options: [brisk:1, cassandra:0]  column families:  columnfamily: inode  'stores file meta data'  key validation class: org.apache.cassandra.db.marshal.bytestype  default column value validator: org.apache.cassandra.db.marshal.bytestype  columns sorted by: org.apache.cassandra.db.marshal.bytestype  row cache size / save period in seconds: 0.0/0  key cache size / save period in seconds: 0.0/14400  memtable thresholds: 0.103125/22/1440 (millions of ops/mb/minutes)  gc grace seconds: 60  compaction min/max thresholds: 4/32  read repair chance: 1.0  replicate on write: false  built indexes: []  column metadata:  column name: path (70617468)  validation class: org.apache.cassandra.db.marshal.bytestype  column name: sentinel (73656e74696e656c)  validation class: org.apache.cassandra.db.marshal.bytestype <text> when dropping the secondary index (via cassandra-cli), the describe keyspace still shows the built index entry. only after a restart of the cassandradaemon then the built index entry is gone. this seems indicate a problem with the index not really been dropped completed. to test, use a single node, create an index, then drop it from the cli (issue an update column family ... with metadata fields but not the index info) below is the original: issue an update: notice the red line on built indexes restart cassandradaemon, describe again: on another note, upon re-create the index, it does not appear the index is actually rebuilt. there is no need to restart cassandradaemon for the built index to show up from the describe. but the update goes very fast. we could tell the index is not being rebuilt because we were getting npe from: and after re-create the index, the exception resurface (the exception does not surface upon drop). if we drop the index files and remove them, then re-create the index, the npe is resolved:",
        "label": 274
    },
    {
        "text": "replace doesn't clean up system peers if you have a new ip <description> when you use replace_token (or replace_node or replace_address) if the new node has a different ip, the old node will still be in system.peers<stacktrace> <code> <text> when you use replace_token (or replace_node or replace_address) if the new node has a different ip, the old node will still be in system.peers",
        "label": 85
    },
    {
        "text": "auto expand replication factor for networktopologystrategy <description> right now when creating a keyspace with networktopologystrategy the user has to manually specify the datacenters they want their data replicated to with parameters, e.g.:  create keyspace test with replication = {'class': 'networktopologystrategy', 'dc1': 3, 'dc2': 3} this is a poor user interface because it requires the creator of the keyspace (typically a developer) to know the layout of the cassandra cluster (which may or may not be controlled by them). also, at least in my experience, folks typo the datacenters all the time. to work around this i see a number of users creating automation around this where the automation describes the cassandra cluster and automatically expands out to all the dcs that cassandra knows about. why can't cassandra just do this for us, re-using the previously forbidden replication_factor option (for backwards compatibility):  create keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3} this would automatically replicate this keyspace to all datacenters that are present in the cluster. if you need to override the default you could supply a datacenter name, e.g.: > create keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3, 'dc1': 2} > describe keyspace test create keyspace test with replication = {'class': 'networktopologystrategy', 'dc1': '2', 'dc2': 3} and durable_writes = true; on the implementation side i think this may be reasonably straightforward to do an auto-expansion at the time of keyspace creation (or alter), where the above would automatically expand to list out the datacenters. we could allow this to be recomputed whenever an alterkeyspacestatement runs so that to add datacenters you would just run: alter keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3} and this would check that if the dc's in the current schema are different you add in the new ones (for safety reasons we'd never remove non explicitly supplied zero dcs when auto-generating dcs). removing a datacenter becomes an alter that includes an override for the dc you want to remove (or of course you can always not use the auto-expansion and just use the old way): // tell it explicitly not to replicate to dc2 > alter keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3, 'dc2': 0} > describe keyspace test create keyspace test with replication = {'class': 'networktopologystrategy', 'dc1': '3'} and durable_writes = true;<stacktrace> <code>  create keyspace test with replication = {'class': 'networktopologystrategy', 'dc1': 3, 'dc2': 3}  create keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3} > create keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3, 'dc1': 2} > describe keyspace test create keyspace test with replication = {'class': 'networktopologystrategy', 'dc1': '2', 'dc2': 3} and durable_writes = true; alter keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3} // tell it explicitly not to replicate to dc2 > alter keyspace test with replication = {'class': 'networktopologystrategy', 'replication_factor': 3, 'dc2': 0} > describe keyspace test create keyspace test with replication = {'class': 'networktopologystrategy', 'dc1': '3'} and durable_writes = true; <text> right now when creating a keyspace with networktopologystrategy the user has to manually specify the datacenters they want their data replicated to with parameters, e.g.: this is a poor user interface because it requires the creator of the keyspace (typically a developer) to know the layout of the cassandra cluster (which may or may not be controlled by them). also, at least in my experience, folks typo the datacenters all the time. to work around this i see a number of users creating automation around this where the automation describes the cassandra cluster and automatically expands out to all the dcs that cassandra knows about. why can't cassandra just do this for us, re-using the previously forbidden replication_factor option (for backwards compatibility): this would automatically replicate this keyspace to all datacenters that are present in the cluster. if you need to override the default you could supply a datacenter name, e.g.: on the implementation side i think this may be reasonably straightforward to do an auto-expansion at the time of keyspace creation (or alter), where the above would automatically expand to list out the datacenters. we could allow this to be recomputed whenever an alterkeyspacestatement runs so that to add datacenters you would just run: and this would check that if the dc's in the current schema are different you add in the new ones (for safety reasons we'd never remove non explicitly supplied zero dcs when auto-generating dcs). removing a datacenter becomes an alter that includes an override for the dc you want to remove (or of course you can always not use the auto-expansion and just use the old way):",
        "label": 262
    },
    {
        "text": "harmless exception logged as error <description> after cassandra-8474, when running the dtest counter_test.py:testcounters.upgrade_test, we now see the following in the log: error [compactionexecutor:2] 2015-01-05 13:59:51,003 cassandradaemon.java:170 - exception in thread thread[compactionexecutor:2,1,main] java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@5e8ea989 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]         at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacereaders(datatracker.java:409) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacewithnewinstances(datatracker.java:303) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.movestarts(sstablerewriter.java:254) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.abort(sstablerewriter.java:180) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:205) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:75) ~[main/:na]         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[main/:na]         at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:226) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_67]         at java.lang.thread.run(thread.java:745) [na:1.7.0_67]         suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@681c91de rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                 at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]                 at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]                 at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablescanner.close(sstablescanner.java:176) ~[main/:na]                 at org.apache.cassandra.db.compaction.abstractcompactionstrategy$scannerlist.close(abstractcompactionstrategy.java:330) ~[main/:na]                 at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:255) ~[main/:na]                 ... 9 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@232f05a4 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@641b6407 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@42443ae9 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted error [compactionexecutor:1] 2015-01-05 13:59:51,010 cassandradaemon.java:170 - exception in thread thread[compactionexecutor:1,1,main] java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@46f365c7 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]         at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacereaders(datatracker.java:409) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacewithnewinstances(datatracker.java:303) ~[main/:na]                         ... 20 common frames omitted error [compactionexecutor:1] 2015-01-05 13:59:51,010 cassandradaemon.java:170 - exception in thread thread [compactionexecutor:1,1,main] java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$sch eduledfuturetask@46f365c7 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecu tor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2 048) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.jav a:325) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530)  ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]         at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/ :na]         at org.apache.cassandra.db.datatracker.replacereaders(datatracker.java:409) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacewithnewinstances(datatracker.java:303) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.movestarts(sstablerewriter.java:254) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.abort(sstablerewriter.java:180) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:205) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:75) ~[main/:na]         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[main/:na]         at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:226) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_67]         at java.lang.thread.run(thread.java:745) [na:1.7.0_67]         suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@159f1035 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                 at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]                 at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]                 at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablescanner.close(sstablescanner.java:176) ~[main/:na]                 at org.apache.cassandra.db.compaction.abstractcompactionstrategy$scannerlist.close(abstractcompactionstrategy.java:330) ~[main/:na]                 at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:255) ~[main/:na]                 ... 9 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@7de112a9 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@439055cf rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@38f1abb0 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted according to benedict, this is harmless. thus, it should be logged at warn.<stacktrace> error [compactionexecutor:2] 2015-01-05 13:59:51,003 cassandradaemon.java:170 - exception in thread thread[compactionexecutor:2,1,main] java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@5e8ea989 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]         at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacereaders(datatracker.java:409) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacewithnewinstances(datatracker.java:303) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.movestarts(sstablerewriter.java:254) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.abort(sstablerewriter.java:180) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:205) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:75) ~[main/:na]         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[main/:na]         at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:226) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_67]         at java.lang.thread.run(thread.java:745) [na:1.7.0_67]         suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@681c91de rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                 at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]                 at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]                 at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablescanner.close(sstablescanner.java:176) ~[main/:na]                 at org.apache.cassandra.db.compaction.abstractcompactionstrategy$scannerlist.close(abstractcompactionstrategy.java:330) ~[main/:na]                 at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:255) ~[main/:na]                 ... 9 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@232f05a4 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@641b6407 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@42443ae9 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted error [compactionexecutor:1] 2015-01-05 13:59:51,010 cassandradaemon.java:170 - exception in thread thread[compactionexecutor:1,1,main] java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@46f365c7 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]         at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacereaders(datatracker.java:409) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacewithnewinstances(datatracker.java:303) ~[main/:na]                         ... 20 common frames omitted error [compactionexecutor:1] 2015-01-05 13:59:51,010 cassandradaemon.java:170 - exception in thread thread [compactionexecutor:1,1,main] java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$sch eduledfuturetask@46f365c7 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecu tor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]         at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2 048) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.jav a:325) ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530)  ~[na:1.7.0_67]         at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]         at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/ :na]         at org.apache.cassandra.db.datatracker.replacereaders(datatracker.java:409) ~[main/:na]         at org.apache.cassandra.db.datatracker.replacewithnewinstances(datatracker.java:303) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.movestarts(sstablerewriter.java:254) ~[main/:na]         at org.apache.cassandra.io.sstable.sstablerewriter.abort(sstablerewriter.java:180) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:205) ~[main/:na]         at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) ~[main/:na]         at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:75) ~[main/:na]         at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:59) ~[main/:na]         at org.apache.cassandra.db.compaction.compactionmanager$backgroundcompactiontask.run(compactionmanager.java:226) ~[main/:na]         at java.util.concurrent.executors$runnableadapter.call(executors.java:471) ~[na:1.7.0_67]         at java.util.concurrent.futuretask.run(futuretask.java:262) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145) ~[na:1.7.0_67]         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615) [na:1.7.0_67]         at java.lang.thread.run(thread.java:745) [na:1.7.0_67]         suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@159f1035 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                 at java.util.concurrent.threadpoolexecutor$abortpolicy.rejectedexecution(threadpoolexecutor.java:2048) ~[na:1.7.0_67]                 at java.util.concurrent.threadpoolexecutor.reject(threadpoolexecutor.java:821) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.delayedexecute(scheduledthreadpoolexecutor.java:325) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.schedule(scheduledthreadpoolexecutor.java:530) ~[na:1.7.0_67]                 at java.util.concurrent.scheduledthreadpoolexecutor.execute(scheduledthreadpoolexecutor.java:619) ~[na:1.7.0_67]                 at org.apache.cassandra.io.sstable.sstablereader.scheduletidy(sstablereader.java:638) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.tidy(sstablereader.java:619) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablereader.releasereference(sstablereader.java:1650) ~[main/:na]                 at org.apache.cassandra.io.sstable.sstablescanner.close(sstablescanner.java:176) ~[main/:na]                 at org.apache.cassandra.db.compaction.abstractcompactionstrategy$scannerlist.close(abstractcompactionstrategy.java:330) ~[main/:na]                 at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:255) ~[main/:na]                 ... 9 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@7de112a9 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@439055cf rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted                 suppressed: java.util.concurrent.rejectedexecutionexception: task java.util.concurrent.scheduledthreadpoolexecutor$scheduledfuturetask@38f1abb0 rejected from org.apache.cassandra.concurrent.debuggablescheduledthreadpoolexecutor@7fc92f94[terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5]                         ... 20 common frames omitted <code> <text> after cassandra-8474, when running the dtest counter_test.py:testcounters.upgrade_test, we now see the following in the log: according to benedict, this is harmless. thus, it should be logged at warn.",
        "label": 521
    },
    {
        "text": "dtest failure in topology test testtopology crash during decommission test <description> looks like some kind of streaming error. example failure: http://cassci.datastax.com/job/trunk_dtest_win32/382/testreport/topology_test/testtopology/crash_during_decommission_test failed on cassci build trunk_dtest_win32 #382 error message unexpected error in log, see stdout -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: d:\\temp\\dtest-ce_wos dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack un  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  162.38 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  222.26 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  336.69 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  360.82 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  240.54 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: decommission failed with exception: nodetool command 'd:\\jenkins\\workspace\\trunk_dtest_win32\\cassandra\\bin\\nodetool.bat -h localhost -p 7100 decommission' failed; exit status: 2; stderr: error: stream failed -- stacktrace -- org.apache.cassandra.streaming.streamexception: stream failed at org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85) at com.google.common.util.concurrent.futures$6.run(futures.java:1310) at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457) at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156) at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145) at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202) at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:215) at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:191) at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:429) at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:527) at org.apache.cassandra.streaming.streamsession.start(streamsession.java:246) at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:263) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at java.lang.thread.run(thread.java:745) dtest: debug: waiting for decommission to complete dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: sleeping for 30 seconds to allow gossip updates dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: removing ccm cluster test at: d:\\temp\\dtest-ce_wos dtest: debug: clearing ssl stores from [d:\\temp\\dtest-ce_wos] directory --------------------- >> end captured logging << --------------------- stacktrace   file \"c:\\tools\\python2\\lib\\unittest\\case.py\", line 358, in run     self.teardown()   file \"d:\\jenkins\\workspace\\trunk_dtest_win32\\cassandra-dtest\\dtest.py\", line 667, in teardown     raise assertionerror('unexpected error in log, see stdout') \"unexpected error in log, see stdout\\n-------------------- >> begin captured logging << --------------------\\ndtest: debug: cluster ccm directory: d:\\\\temp\\\\dtest-ce_wos\\ndtest: debug: custom init_config not found. setting defaults.\\ndtest: debug: done setting configuration options:\\n{   'initial_token': none,\\n    'num_tokens': '32',\\n    'phi_convict_threshold': 5,\\n    'range_request_timeout_in_ms': 10000,\\n    'read_request_timeout_in_ms': 10000,\\n    'request_timeout_in_ms': 10000,\\n    'truncate_request_timeout_in_ms': 10000,\\n    'write_request_timeout_in_ms': 10000}\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nun  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  162.38 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: restarting node2\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nul  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  222.26 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: restarting node2\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  336.69 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: restarting node2\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  360.82 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: restarting node2\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  240.54 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: restarting node2\\ndtest: debug: decommission failed with exception: nodetool command 'd:\\\\jenkins\\\\workspace\\\\trunk_dtest_win32\\\\cassandra\\\\bin\\\\nodetool.bat -h localhost -p 7100 decommission' failed; exit status: 2; stderr: error: stream failed\\n-- stacktrace --\\norg.apache.cassandra.streaming.streamexception: stream failed\\n\\tat org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85)\\n\\tat com.google.common.util.concurrent.futures$6.run(futures.java:1310)\\n\\tat com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457)\\n\\tat com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156)\\n\\tat com.google.common.util.concurrent.executionlist.execute(executionlist.java:145)\\n\\tat com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202)\\n\\tat org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:215)\\n\\tat org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:191)\\n\\tat org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:429)\\n\\tat org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:527)\\n\\tat org.apache.cassandra.streaming.streamsession.start(streamsession.java:246)\\n\\tat org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:263)\\n\\tat java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)\\n\\tat java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)\\n\\tat java.lang.thread.run(thread.java:745)\\n\\n\\ndtest: debug: waiting for decommission to complete\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: sleeping for 30 seconds to allow gossip updates\\ndtest: debug: status as reported by node 127.0.0.2\\ndtest: debug: datacenter: datacenter1\\n========================\\nstatus=up/down\\n|/ state=normal/leaving/joining/moving\\n--  address    load       tokens       owns (effective)  host id                               rack\\nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1\\nun  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1\\nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1\\n\\n\\ndtest: debug: removing ccm cluster test at: d:\\\\temp\\\\dtest-ce_wos\\ndtest: debug: clearing ssl stores from [d:\\\\temp\\\\dtest-ce_wos] directory\\n--------------------- >> end captured logging << ---------------------\" standard output unexpected error in node1 log, error:  error [streamconnectionestablisher:1] 2016-04-04 21:20:13,361 streamsession.java:519 - [stream #df460340-faaa-11e5-a489-9fa05b8758d9] streaming error occurred on session with peer 127.0.0.2 java.net.connectexception: connection refused: connect at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_51] at sun.nio.ch.net.connect(net.java:458) ~[na:1.8.0_51] at sun.nio.ch.net.connect(net.java:450) ~[na:1.8.0_51] at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_51] at org.apache.cassandra.net.outboundtcpconnectionpool.newsocket(outboundtcpconnectionpool.java:141) ~[main/:na] at org.apache.cassandra.streaming.defaultconnectionfactory.createconnection(defaultconnectionfactory.java:52) ~[main/:na] at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:253) ~[main/:na] at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:83) ~[main/:na] at org.apache.cassandra.streaming.streamsession.start(streamsession.java:240) ~[main/:na] at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:263) [main/:na] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_51] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_51] at java.lang.thread.run(thread.java:745) [na:1.8.0_51] standard error started: node1 with pid: 7740 started: node3 with pid: 7796 started: node2 with pid: 128 started: node2 with pid: 4088 started: node2 with pid: 6116 started: node2 with pid: 5168 started: node2 with pid: 1908 started: node2 with pid: 4436<stacktrace> error message unexpected error in log, see stdout -------------------- >> begin captured logging << -------------------- dtest: debug: cluster ccm directory: d:/temp/dtest-ce_wos dtest: debug: custom init_config not found. setting defaults. dtest: debug: done setting configuration options: {   'initial_token': none,     'num_tokens': '32',     'phi_convict_threshold': 5,     'range_request_timeout_in_ms': 10000,     'read_request_timeout_in_ms': 10000,     'request_timeout_in_ms': 10000,     'truncate_request_timeout_in_ms': 10000,     'write_request_timeout_in_ms': 10000} dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack un  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  162.38 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  222.26 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  336.69 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  360.82 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  240.54 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: restarting node2 dtest: debug: decommission failed with exception: nodetool command 'd:/jenkins/workspace/trunk_dtest_win32/cassandra/bin/nodetool.bat -h localhost -p 7100 decommission' failed; exit status: 2; stderr: error: stream failed -- stacktrace -- org.apache.cassandra.streaming.streamexception: stream failed at org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85) at com.google.common.util.concurrent.futures$6.run(futures.java:1310) at com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457) at com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156) at com.google.common.util.concurrent.executionlist.execute(executionlist.java:145) at com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202) at org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:215) at org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:191) at org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:429) at org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:527) at org.apache.cassandra.streaming.streamsession.start(streamsession.java:246) at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:263) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at java.lang.thread.run(thread.java:745) dtest: debug: waiting for decommission to complete dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: sleeping for 30 seconds to allow gossip updates dtest: debug: status as reported by node 127.0.0.2 dtest: debug: datacenter: datacenter1 ======================== status=up/down |/ state=normal/leaving/joining/moving --  address    load       tokens       owns (effective)  host id                               rack ul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1 un  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1 un  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1 dtest: debug: removing ccm cluster test at: d:/temp/dtest-ce_wos dtest: debug: clearing ssl stores from [d:/temp/dtest-ce_wos] directory --------------------- >> end captured logging << --------------------- stacktrace   file 'c:/tools/python2/lib/unittest/case.py', line 358, in run     self.teardown()   file 'd:/jenkins/workspace/trunk_dtest_win32/cassandra-dtest/dtest.py', line 667, in teardown     raise assertionerror('unexpected error in log, see stdout') 'unexpected error in log, see stdout/n-------------------- >> begin captured logging << --------------------/ndtest: debug: cluster ccm directory: d://temp//dtest-ce_wos/ndtest: debug: custom init_config not found. setting defaults./ndtest: debug: done setting configuration options:/n{   'initial_token': none,/n    'num_tokens': '32',/n    'phi_convict_threshold': 5,/n    'range_request_timeout_in_ms': 10000,/n    'read_request_timeout_in_ms': 10000,/n    'request_timeout_in_ms': 10000,/n    'truncate_request_timeout_in_ms': 10000,/n    'write_request_timeout_in_ms': 10000}/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nun  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  162.38 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: restarting node2/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nul  127.0.0.1  98.73 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  222.26 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  98.71 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: restarting node2/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  336.69 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: restarting node2/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  360.82 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: restarting node2/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  240.54 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: restarting node2/ndtest: debug: decommission failed with exception: nodetool command 'd://jenkins//workspace//trunk_dtest_win32//cassandra//bin//nodetool.bat -h localhost -p 7100 decommission' failed; exit status: 2; stderr: error: stream failed/n-- stacktrace --/norg.apache.cassandra.streaming.streamexception: stream failed/n/tat org.apache.cassandra.streaming.management.streameventjmxnotifier.onfailure(streameventjmxnotifier.java:85)/n/tat com.google.common.util.concurrent.futures$6.run(futures.java:1310)/n/tat com.google.common.util.concurrent.moreexecutors$directexecutor.execute(moreexecutors.java:457)/n/tat com.google.common.util.concurrent.executionlist.executelistener(executionlist.java:156)/n/tat com.google.common.util.concurrent.executionlist.execute(executionlist.java:145)/n/tat com.google.common.util.concurrent.abstractfuture.setexception(abstractfuture.java:202)/n/tat org.apache.cassandra.streaming.streamresultfuture.maybecomplete(streamresultfuture.java:215)/n/tat org.apache.cassandra.streaming.streamresultfuture.handlesessioncomplete(streamresultfuture.java:191)/n/tat org.apache.cassandra.streaming.streamsession.closesession(streamsession.java:429)/n/tat org.apache.cassandra.streaming.streamsession.onerror(streamsession.java:527)/n/tat org.apache.cassandra.streaming.streamsession.start(streamsession.java:246)/n/tat org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:263)/n/tat java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142)/n/tat java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617)/n/tat java.lang.thread.run(thread.java:745)/n/n/ndtest: debug: waiting for decommission to complete/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: sleeping for 30 seconds to allow gossip updates/ndtest: debug: status as reported by node 127.0.0.2/ndtest: debug: datacenter: datacenter1/n========================/nstatus=up/down/n|/ state=normal/leaving/joining/moving/n--  address    load       tokens       owns (effective)  host id                               rack/nul  127.0.0.1  174.2 kib  32           78.4%             b8c55c71-bf3d-462b-8c17-3c88d7ac2284  rack1/nun  127.0.0.2  370.04 kib  32           65.9%             71aacf1d-8e2f-44cf-b354-f10c71313ec6  rack1/nun  127.0.0.3  116.7 kib  32           55.7%             3a4529a3-dc7f-445c-aec3-94417c920fdf  rack1/n/n/ndtest: debug: removing ccm cluster test at: d://temp//dtest-ce_wos/ndtest: debug: clearing ssl stores from [d://temp//dtest-ce_wos] directory/n--------------------- >> end captured logging << ---------------------' standard output unexpected error in node1 log, error:  error [streamconnectionestablisher:1] 2016-04-04 21:20:13,361 streamsession.java:519 - [stream #df460340-faaa-11e5-a489-9fa05b8758d9] streaming error occurred on session with peer 127.0.0.2 java.net.connectexception: connection refused: connect at sun.nio.ch.net.connect0(native method) ~[na:1.8.0_51] at sun.nio.ch.net.connect(net.java:458) ~[na:1.8.0_51] at sun.nio.ch.net.connect(net.java:450) ~[na:1.8.0_51] at sun.nio.ch.socketchannelimpl.connect(socketchannelimpl.java:648) ~[na:1.8.0_51] at org.apache.cassandra.net.outboundtcpconnectionpool.newsocket(outboundtcpconnectionpool.java:141) ~[main/:na] at org.apache.cassandra.streaming.defaultconnectionfactory.createconnection(defaultconnectionfactory.java:52) ~[main/:na] at org.apache.cassandra.streaming.streamsession.createconnection(streamsession.java:253) ~[main/:na] at org.apache.cassandra.streaming.connectionhandler.initiate(connectionhandler.java:83) ~[main/:na] at org.apache.cassandra.streaming.streamsession.start(streamsession.java:240) ~[main/:na] at org.apache.cassandra.streaming.streamcoordinator$streamsessionconnector.run(streamcoordinator.java:263) [main/:na] at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) [na:1.8.0_51] at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) [na:1.8.0_51] at java.lang.thread.run(thread.java:745) [na:1.8.0_51] standard error started: node1 with pid: 7740 started: node3 with pid: 7796 started: node2 with pid: 128 started: node2 with pid: 4088 started: node2 with pid: 6116 started: node2 with pid: 5168 started: node2 with pid: 1908 started: node2 with pid: 4436 <code> http://cassci.datastax.com/job/trunk_dtest_win32/382/testreport/topology_test/testtopology/crash_during_decommission_test failed on cassci build trunk_dtest_win32 #382<text> looks like some kind of streaming error. example failure: ",
        "label": 409
    },
    {
        "text": "repair  pr throws eofexception <description> nodetool repair -pr threw an eof exception node1 error 12:50:18,723 exception in thread thread[streaming to /10.8.25.113:1,5,main] java.lang.runtimeexception: java.io.eofexception at com.google.common.base.throwables.propagate(throwables.java:160) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:32) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.io.eofexception at java.io.datainputstream.readint(datainputstream.java:375) at org.apache.cassandra.streaming.filestreamtask.receivereply(filestreamtask.java:193) at org.apache.cassandra.streaming.compress.compressedfilestreamtask.stream(compressedfilestreamtask.java:114) at org.apache.cassandra.streaming.filestreamtask.runmaythrow(filestreamtask.java:91) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) node2  info 12:49:45,139 finished streaming session to /10.8.30.13 error 12:50:18,799 exception in thread thread[thread-4031,5,main] java.lang.runtimeexception: last written key decoratedkey(167625858728826091814875924785363245309, 6634333531356661643161636636373738353431363162353031376164386339) >= current ke y decoratedkey(33957321636818582219838207277782228619, 696c2e636f6d200a3c42523e0a3c42523e0a5472656e7420202020202020202020202020202020422e204d697261636c652020202020202020202020202 020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e62737020746d697261636c654073696d6d6f6e736669726d2e636f6d2c206c776f6f74656e4073696d6d6f6e736669726 d2e636f6d203c42523e0a3c42523e0a56616e636520202020202020202020202020202020522e20416e64727573202020202020202020202020202020202020202020202020202020202020202020202020202020202020200 a2020266e62737020266e627370207672614061622d706c632e636f6d203c42523e0a3c42523e0a5665726e6f6e202020202020202020202020202020462e20476c656e6e20202020202020202020202020202020202020202 020202020202020202020202020202020202020202020200a2020266e62737020266e62737020676c656e6e6c6177406c6f77636f756e7472796c61777965722e636f6d203c42523e0a3c42523e0a56696e63656e742020202 0202020202020202020204a2e20446573616c766f2020202020202020202020202020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e6273702076646573616c766f4064657 3616c766f6c61776669726d2e636f6d203c42523e0a3c42523e0a56696e63656e7420202020202020202020202020204a616d65732043617274657220202020202020202020202020202020202020202020202020202020202 0202020202020202020200a2020202020266e62737020266e627370207663617274657240676972617264696b656573652e636f6d2c207479616d6173616b6940676972617264696b656573652e636f6d203c42523e0a0a3c4 2523e0a572e202020202020202020202020202020202020204a616d65732053696e676c65746f6e202020202020202020202020202020202020202020202020202020202020202020202020200a2020202020266e627370202...trunkated...324132393239413134333439413834453531394133373431) writing into /data/cassandra/evidence/fingerprints/evidence-fingerprints-tmp-ia-161-data.db         at org.apache.cassandra.io.sstable.sstablewriter.beforeappend(sstablewriter.java:133)         at org.apache.cassandra.io.sstable.sstablewriter.appendfromstream(sstablewriter.java:209)         at org.apache.cassandra.streaming.incomingstreamreader.streamin(incomingstreamreader.java:179)         at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:122)         at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:226)         at org.apache.cassandra.net.incomingtcpconnection.handlestream(incomingtcpconnection.java:166)         at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:66)<stacktrace> error 12:50:18,723 exception in thread thread[streaming to /10.8.25.113:1,5,main] java.lang.runtimeexception: java.io.eofexception at com.google.common.base.throwables.propagate(throwables.java:160) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:32) at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908) at java.lang.thread.run(thread.java:662) caused by: java.io.eofexception at java.io.datainputstream.readint(datainputstream.java:375) at org.apache.cassandra.streaming.filestreamtask.receivereply(filestreamtask.java:193) at org.apache.cassandra.streaming.compress.compressedfilestreamtask.stream(compressedfilestreamtask.java:114) at org.apache.cassandra.streaming.filestreamtask.runmaythrow(filestreamtask.java:91) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28)  info 12:49:45,139 finished streaming session to /10.8.30.13 error 12:50:18,799 exception in thread thread[thread-4031,5,main] java.lang.runtimeexception: last written key decoratedkey(167625858728826091814875924785363245309, 6634333531356661643161636636373738353431363162353031376164386339) >= current ke y decoratedkey(33957321636818582219838207277782228619, 696c2e636f6d200a3c42523e0a3c42523e0a5472656e7420202020202020202020202020202020422e204d697261636c652020202020202020202020202 020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e62737020746d697261636c654073696d6d6f6e736669726d2e636f6d2c206c776f6f74656e4073696d6d6f6e736669726 d2e636f6d203c42523e0a3c42523e0a56616e636520202020202020202020202020202020522e20416e64727573202020202020202020202020202020202020202020202020202020202020202020202020202020202020200 a2020266e62737020266e627370207672614061622d706c632e636f6d203c42523e0a3c42523e0a5665726e6f6e202020202020202020202020202020462e20476c656e6e20202020202020202020202020202020202020202 020202020202020202020202020202020202020202020200a2020266e62737020266e62737020676c656e6e6c6177406c6f77636f756e7472796c61777965722e636f6d203c42523e0a3c42523e0a56696e63656e742020202 0202020202020202020204a2e20446573616c766f2020202020202020202020202020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e6273702076646573616c766f4064657 3616c766f6c61776669726d2e636f6d203c42523e0a3c42523e0a56696e63656e7420202020202020202020202020204a616d65732043617274657220202020202020202020202020202020202020202020202020202020202 0202020202020202020200a2020202020266e62737020266e627370207663617274657240676972617264696b656573652e636f6d2c207479616d6173616b6940676972617264696b656573652e636f6d203c42523e0a0a3c4 2523e0a572e202020202020202020202020202020202020204a616d65732053696e676c65746f6e202020202020202020202020202020202020202020202020202020202020202020202020200a2020202020266e627370202...trunkated...324132393239413134333439413834453531394133373431) writing into /data/cassandra/evidence/fingerprints/evidence-fingerprints-tmp-ia-161-data.db         at org.apache.cassandra.io.sstable.sstablewriter.beforeappend(sstablewriter.java:133)         at org.apache.cassandra.io.sstable.sstablewriter.appendfromstream(sstablewriter.java:209)         at org.apache.cassandra.streaming.incomingstreamreader.streamin(incomingstreamreader.java:179)         at org.apache.cassandra.streaming.incomingstreamreader.read(incomingstreamreader.java:122)         at org.apache.cassandra.net.incomingtcpconnection.stream(incomingtcpconnection.java:226)         at org.apache.cassandra.net.incomingtcpconnection.handlestream(incomingtcpconnection.java:166)         at org.apache.cassandra.net.incomingtcpconnection.run(incomingtcpconnection.java:66) <code> <text> nodetool repair -pr threw an eof exception",
        "label": 577
    },
    {
        "text": "ability to distinguish between null and unset values in prepared statements <description> currently cassandra inserts tombstones when a value of a column is bound to null in a prepared statement. at higher insert rates managing all these tombstones becomes an unnecessary overhead. this limits the usefulness of the prepared statements since developers have to either create multiple prepared statements (each with a different combination of column names, which at times is just unfeasible because of the sheer number of possible combinations) or fall back to using regular (non-prepared) statements. this jira is here to explore the possibility of either:  a. have a flag on prepared statements that once set, tells cassandra to ignore null columns or b. have an \"unset\" value which makes cassandra skip the null columns and not tombstone them basically, in the context of a prepared statement, a null value means delete, but we don\u2019t have anything that means \"ignore\" (besides creating a new prepared statement without the ignored column). please refer to the original conversation on datastax java driver mailing list for more background:  https://groups.google.com/a/lists.datastax.com/d/topic/java-driver-user/che3oosixbu/discussion edit 18/12/14 - oded peer implementation notes: the motivation hasn't changed. protocol version 4 specifies that bind variables do not require having a value when executing a statement. bind variables without a value are called 'unset'. the 'unset' bind variable is serialized as the int value '-2' without following bytes.    an unset bind variable in an execute or batch request on a value does not modify the value and does not create a tombstone on the ttl clause is treated as 'unlimited' on the timestamp clause is treated as 'now' on a map key or a list index throws invalidrequestexception on a counter increment or decrement operation does not change the counter value, e.g. update my_tab set c = c - ? where k = 1 does change the value of counter c on a tuple field or udt field throws invalidrequestexception an unset bind variable in a query request on a partition column, clustering column or index column in the where clause throws invalidrequestexception on the limit clause is treated as 'unlimited'<stacktrace> <code> please refer to the original conversation on datastax java driver mailing list for more background:  https://groups.google.com/a/lists.datastax.com/d/topic/java-driver-user/che3oosixbu/discussion <text> currently cassandra inserts tombstones when a value of a column is bound to null in a prepared statement. at higher insert rates managing all these tombstones becomes an unnecessary overhead. this limits the usefulness of the prepared statements since developers have to either create multiple prepared statements (each with a different combination of column names, which at times is just unfeasible because of the sheer number of possible combinations) or fall back to using regular (non-prepared) statements. this jira is here to explore the possibility of either:  a. have a flag on prepared statements that once set, tells cassandra to ignore null columns or b. have an 'unset' value which makes cassandra skip the null columns and not tombstone them basically, in the context of a prepared statement, a null value means delete, but we don't have anything that means 'ignore' (besides creating a new prepared statement without the ignored column). edit 18/12/14 - oded peer implementation notes: the motivation hasn't changed. protocol version 4 specifies that bind variables do not require having a value when executing a statement. bind variables without a value are called 'unset'. the 'unset' bind variable is serialized as the int value '-2' without following bytes.  ",
        "label": 391
    },
    {
        "text": "sstable2json   sstablekeys should verify key order in  data and  index files <description> some cassandra users use sstable2json and sstablekeys to check -data and -index files for integrity. for example, if compaction fails, you can find out which files are causing the compaction to fail because they're corrupt. one type of corruption that can happen in both -data and -index files are keys getting out of order. (this shouldn't happen, but it can) cassandra catches this error during compaction, but both tools didn't catch it. this small patch simply causes an io exception during export if it finds out of order keys. some further work on it may make this optional with a command-line switch - it may be useful to export the data to json even though it's out of order to manually play it back, or have another script re-order it.<stacktrace> <code> <text> some cassandra users use sstable2json and sstablekeys to check -data and -index files for integrity. for example, if compaction fails, you can find out which files are causing the compaction to fail because they're corrupt. one type of corruption that can happen in both -data and -index files are keys getting out of order. (this shouldn't happen, but it can) cassandra catches this error during compaction, but both tools didn't catch it. this small patch simply causes an io exception during export if it finds out of order keys. some further work on it may make this optional with a command-line switch - it may be useful to export the data to json even though it's out of order to manually play it back, or have another script re-order it.",
        "label": 292
    },
    {
        "text": "range queries do not yet span multiple nodes <description> need ability to continue a query on the next node in the ring, if necessary<stacktrace> <code> <text> need ability to continue a query on the next node in the ring, if necessary",
        "label": 53
    },
    {
        "text": "nodetool repair hangs on non existant table <description> if nodetool repair is called with a table that does not exist, ist hangs infinitely without any error message or logs. e.g. nodetool repair foo bar keyspace foo exists but table bar does not<stacktrace> <code> e.g. <text> if nodetool repair is called with a table that does not exist, ist hangs infinitely without any error message or logs. nodetool repair foo bar keyspace foo exists but table bar does not",
        "label": 328
    },
    {
        "text": "consistent reads after move test is failing on trunk <description> the novnode dtest consistent_bootstrap_test.testbootstrapconsistency.consistent_reads_after_move_test is failing on trunk. see an example failure here. on trunk i am getting an oom of one of my c* nodes [node3], which is what causes the nodetool move to fail. logs are attached.<stacktrace> <code> <text> the novnode dtest consistent_bootstrap_test.testbootstrapconsistency.consistent_reads_after_move_test is failing on trunk. see an example failure here. on trunk i am getting an oom of one of my c* nodes [node3], which is what causes the nodetool move to fail. logs are attached.",
        "label": 321
    },
    {
        "text": "bootstrapper should consult the replicationstrategy for determining which replicas to request data from <description> storageservice.findsuitableendpoint is currently the only place we do this bootstrapper.getworkmap should also try to read from \"close\" nodes (in same rack, or same dc) also, this todo from getworkmap:  // todo look for contiguous ranges and map them to the same source (this requires cassandra-483 to be finished)<stacktrace> <code> also, this todo from getworkmap:  // todo look for contiguous ranges and map them to the same source (this requires cassandra-483 to be finished)<text> storageservice.findsuitableendpoint is currently the only place we do this bootstrapper.getworkmap should also try to read from 'close' nodes (in same rack, or same dc) ",
        "label": 555
    },
    {
        "text": "nodetool compact and flush fail with  error  null  <description> nodetool flush and nodetool compact return an error message that is not clear. this could probably be improved. both of my two nodes return this error. nodetool flush will return this error the first 2-3 times you invoke it, then the error temporarily disappears. nodetool compress always returns this error message no matter how many times you invoke it. i have tried deleting saved_caches, commit logs, doing nodetool compact/rebuild/scrub, and nothing seems to remove the error. cass@s5:~/apache-cassandra-3.11.0$ nodetool compact error: null -- stacktrace -- java.lang.assertionerror at org.apache.cassandra.cache.chunkcache$cachingrebufferer.<init>(chunkcache.java:222) at org.apache.cassandra.cache.chunkcache.wrap(chunkcache.java:175) at org.apache.cassandra.io.util.filehandle$builder.maybecached(filehandle.java:412) at org.apache.cassandra.io.util.filehandle$builder.complete(filehandle.java:381) at org.apache.cassandra.io.util.filehandle$builder.complete(filehandle.java:331) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.openfinal(bigtablewriter.java:333) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.openfinalearly(bigtablewriter.java:318) at org.apache.cassandra.io.sstable.sstablerewriter.switchwriter(sstablerewriter.java:322) at org.apache.cassandra.io.sstable.sstablerewriter.doprepare(sstablerewriter.java:370) at org.apache.cassandra.utils.concurrent.transactional$abstracttransactional.preparetocommit(transactional.java:173) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.doprepare(compactionawarewriter.java:111) at org.apache.cassandra.utils.concurrent.transactional$abstracttransactional.preparetocommit(transactional.java:173) at org.apache.cassandra.utils.concurrent.transactional$abstracttransactional.finish(transactional.java:184) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.finish(compactionawarewriter.java:121) at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:220) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:85) at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:61) at org.apache.cassandra.db.compaction.compactionmanager$10.runmaythrow(compactionmanager.java:733) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:81) at java.lang.thread.run(thread.java:748) <stacktrace> cass@s5:~/apache-cassandra-3.11.0$ nodetool compact error: null -- stacktrace -- java.lang.assertionerror at org.apache.cassandra.cache.chunkcache$cachingrebufferer.<init>(chunkcache.java:222) at org.apache.cassandra.cache.chunkcache.wrap(chunkcache.java:175) at org.apache.cassandra.io.util.filehandle$builder.maybecached(filehandle.java:412) at org.apache.cassandra.io.util.filehandle$builder.complete(filehandle.java:381) at org.apache.cassandra.io.util.filehandle$builder.complete(filehandle.java:331) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.openfinal(bigtablewriter.java:333) at org.apache.cassandra.io.sstable.format.big.bigtablewriter.openfinalearly(bigtablewriter.java:318) at org.apache.cassandra.io.sstable.sstablerewriter.switchwriter(sstablerewriter.java:322) at org.apache.cassandra.io.sstable.sstablerewriter.doprepare(sstablerewriter.java:370) at org.apache.cassandra.utils.concurrent.transactional$abstracttransactional.preparetocommit(transactional.java:173) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.doprepare(compactionawarewriter.java:111) at org.apache.cassandra.utils.concurrent.transactional$abstracttransactional.preparetocommit(transactional.java:173) at org.apache.cassandra.utils.concurrent.transactional$abstracttransactional.finish(transactional.java:184) at org.apache.cassandra.db.compaction.writers.compactionawarewriter.finish(compactionawarewriter.java:121) at org.apache.cassandra.db.compaction.compactiontask.runmaythrow(compactiontask.java:220) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at org.apache.cassandra.db.compaction.compactiontask.executeinternal(compactiontask.java:85) at org.apache.cassandra.db.compaction.abstractcompactiontask.execute(abstractcompactiontask.java:61) at org.apache.cassandra.db.compaction.compactionmanager$10.runmaythrow(compactionmanager.java:733) at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:28) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at org.apache.cassandra.concurrent.namedthreadfactory.lambda$threadlocaldeallocator$0(namedthreadfactory.java:81) at java.lang.thread.run(thread.java:748) <code> <text> nodetool flush and nodetool compact return an error message that is not clear. this could probably be improved. both of my two nodes return this error. nodetool flush will return this error the first 2-3 times you invoke it, then the error temporarily disappears. nodetool compress always returns this error message no matter how many times you invoke it. i have tried deleting saved_caches, commit logs, doing nodetool compact/rebuild/scrub, and nothing seems to remove the error.",
        "label": 508
    },
    {
        "text": "log compaction active tasks in statuslogger instead of n a <description> currently statuslogger log: logger.info(string.format(\"%-25s%10s%10s\",                                   \"compactionmanager\", \"n/a\", compactionmanager.instance.getpendingtasks())); it'd be great if it could actually log the number of active tasks being processed. without looking into the code, i thought there was no compaction running when reading the log. compactionmanager.java     public int getactivecompactions()     {         return compactionexecutor.compactions.size();     }<stacktrace> <code>     public int getactivecompactions()     {         return compactionexecutor.compactions.size();     } logger.info(string.format('%-25s%10s%10s',                                   'compactionmanager', 'n/a', compactionmanager.instance.getpendingtasks())); <text> currently statuslogger log: it'd be great if it could actually log the number of active tasks being processed. without looking into the code, i thought there was no compaction running when reading the log.",
        "label": 274
    },
    {
        "text": "commit log replay issues <description> i've been having a bunch of trouble replaying commit logs. i've seen various exceptions, including: java.lang.negativearraysizeexception  at org.apache.cassandra.db.commitlog.recover(commitlog.java:273)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156)  exception encountered during startup.  java.lang.negativearraysizeexception  at org.apache.cassandra.db.commitlog.recover(commitlog.java:273)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156) i also got this: java.lang.runtimeexception: unable to load comparator class 'org.apache.cassandra.db.marshal.utf8typ'. probably this means you have obsolete sstables lying around  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.readcomparator(columnfamily.java:525)  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.deserializeempty(columnfamily.java:535)  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.deserialize(columnfamily.java:500)  at org.apache.cassandra.db.rowserializer.deserialize(row.java:225)  at org.apache.cassandra.db.commitlog.recover(commitlog.java:284)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156)  caused by: java.lang.classnotfoundexception: org/apache/cassandra/db/marshal/utf8typ  at java.lang.class.forname0(native method)  at java.lang.class.forname(class.java:169)  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.readcomparator(columnfamily.java:521)  ... 7 more and this: java.io.eofexception  at java.io.datainputstream.readunsignedshort(datainputstream.java:323)  at java.io.datainputstream.readutf(datainputstream.java:572)  at java.io.datainputstream.readutf(datainputstream.java:547)  at org.apache.cassandra.db.rowserializer.deserialize(row.java:218)  at org.apache.cassandra.db.commitlog.recover(commitlog.java:284)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156)  exception encountered during startup. not sure if any of them are related.<stacktrace> java.lang.negativearraysizeexception  at org.apache.cassandra.db.commitlog.recover(commitlog.java:273)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156)  exception encountered during startup.  java.lang.negativearraysizeexception  at org.apache.cassandra.db.commitlog.recover(commitlog.java:273)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156) java.lang.runtimeexception: unable to load comparator class 'org.apache.cassandra.db.marshal.utf8typ'. probably this means you have obsolete sstables lying around  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.readcomparator(columnfamily.java:525)  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.deserializeempty(columnfamily.java:535)  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.deserialize(columnfamily.java:500)  at org.apache.cassandra.db.rowserializer.deserialize(row.java:225)  at org.apache.cassandra.db.commitlog.recover(commitlog.java:284)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156)  caused by: java.lang.classnotfoundexception: org/apache/cassandra/db/marshal/utf8typ  at java.lang.class.forname0(native method)  at java.lang.class.forname(class.java:169)  at org.apache.cassandra.db.columnfamily$columnfamilyserializer.readcomparator(columnfamily.java:521)  ... 7 more java.io.eofexception  at java.io.datainputstream.readunsignedshort(datainputstream.java:323)  at java.io.datainputstream.readutf(datainputstream.java:572)  at java.io.datainputstream.readutf(datainputstream.java:547)  at org.apache.cassandra.db.rowserializer.deserialize(row.java:218)  at org.apache.cassandra.db.commitlog.recover(commitlog.java:284)  at org.apache.cassandra.db.recoverymanager.dorecovery(recoverymanager.java:63)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:95)  at org.apache.cassandra.service.cassandradaemon.main(cassandradaemon.java:156)  exception encountered during startup. <code> <text> i've been having a bunch of trouble replaying commit logs. i've seen various exceptions, including: i also got this: and this: not sure if any of them are related.",
        "label": 274
    },
    {
        "text": "allow restricting get key range to a subset of columnfamilies <description> this should make it a lot faster, besides affording more control to the user<stacktrace> <code> <text> this should make it a lot faster, besides affording more control to the user",
        "label": 274
    },
    {
        "text": "groupcommitlogservice <description> i propose a new commitlogservice, groupcommitlogservice, to improve the throughput when lots of requests are received.  it improved the throughput by maximum 94%.  i'd like to discuss about this commitlogservice. currently, we can select either 2 commitlog services; periodic and batch.  in periodic, we might lose some commit log which hasn't written to the disk.  in batch, we can write commit log to the disk every time. the size of commit log to write is too small (< 4kb). when high concurrency, these writes are gathered and persisted to the disk at once. but, when insufficient concurrency, many small writes are issued and the performance decreases due to the latency of the disk. even if you use ssd, processes of many io commands decrease the performance. groupcommitlogservice writes some commitlog to the disk at once.  the patch adds groupcommitlogservice (it is enabled by setting `commitlog_sync` and `commitlog_sync_group_window_in_ms` in cassandra.yaml).  the difference from batch is just only waiting for the semaphore.  by waiting for the semaphore, some writes for commit logs are executed at the same time.  in groupcommitlogservice, the latency becomes worse if the there is no concurrency. i measured the performance with my microbench (microrequestthread.java) by increasing the number of threads.the cluster has 3 nodes (replication factor: 3). each nodes is aws ec2 m4.large instance + 200iops io1 volume.  the result is as below. the groupcommitlogservice with 10ms window improved update with paxos by 94% and improved select with paxos by 76%. select / sec # of threads batch 2ms group 10ms 1 192 103 2 163 212 4 264 416 8 454 800 16 744 1311 32 1151 1481 64 1767 1844 128 2949 3011 256 4723 5000 update / sec # of threads batch 2ms group 10ms 1 45 26 2 39 51 4 58 102 8 102 198 16 167 213 32 289 295 64 544 548 128 1046 1058 256 2020 2061<stacktrace> <code> <text> i propose a new commitlogservice, groupcommitlogservice, to improve the throughput when lots of requests are received.  it improved the throughput by maximum 94%.  i'd like to discuss about this commitlogservice. currently, we can select either 2 commitlog services; periodic and batch.  in periodic, we might lose some commit log which hasn't written to the disk.  in batch, we can write commit log to the disk every time. the size of commit log to write is too small (< 4kb). when high concurrency, these writes are gathered and persisted to the disk at once. but, when insufficient concurrency, many small writes are issued and the performance decreases due to the latency of the disk. even if you use ssd, processes of many io commands decrease the performance. groupcommitlogservice writes some commitlog to the disk at once.  the patch adds groupcommitlogservice (it is enabled by setting `commitlog_sync` and `commitlog_sync_group_window_in_ms` in cassandra.yaml).  the difference from batch is just only waiting for the semaphore.  by waiting for the semaphore, some writes for commit logs are executed at the same time.  in groupcommitlogservice, the latency becomes worse if the there is no concurrency. i measured the performance with my microbench (microrequestthread.java) by increasing the number of threads.the cluster has 3 nodes (replication factor: 3). each nodes is aws ec2 m4.large instance + 200iops io1 volume.  the result is as below. the groupcommitlogservice with 10ms window improved update with paxos by 94% and improved select with paxos by 76%.",
        "label": 576
    },
    {
        "text": "negative mean write latency <description> the mean write latency returned by jmx turns negative every 30 minutes. as the attached screenshots show, the value turns negative every 30 minutes after the startup of the node.  we did not experience this behavior in 2.1.16.<stacktrace> <code> <text> the mean write latency returned by jmx turns negative every 30 minutes. as the attached screenshots show, the value turns negative every 30 minutes after the startup of the node.  we did not experience this behavior in 2.1.16.",
        "label": 419
    },
    {
        "text": "stack overflow while compacting <description> this is a trunk build from may 3. after adding cassandra-2401, i have gotten the following on several nodes.  i am not 100% sure right now if it is related to 2401 but it may seem likely. unfortunately, as often is the case with stack overflows, i don't see the start of the stack error [compactionexecutor:17] 2011-05-09 07:56:32,479 abstractcassandradaemon.java (line 112) fatal exception in thread thread[compactionexecutor:17,1,main]  java.lang.stackoverflowerror  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)<stacktrace> error [compactionexecutor:17] 2011-05-09 07:56:32,479 abstractcassandradaemon.java (line 112) fatal exception in thread thread[compactionexecutor:17,1,main]  java.lang.stackoverflowerror  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)  at java.util.collections$unmodifiablecollection.size(collections.java:998)<code> <text> this is a trunk build from may 3. after adding cassandra-2401, i have gotten the following on several nodes.  i am not 100% sure right now if it is related to 2401 but it may seem likely. unfortunately, as often is the case with stack overflows, i don't see the start of the stack ",
        "label": 498
    },
    {
        "text": "add new show command in cql shell to list column families <description> add command show column families to list all column families in a current keyspace by name.<stacktrace> <code> <text> add command show column families to list all column families in a current keyspace by name.",
        "label": 593
    },
    {
        "text": "expose rpc address and broadcast address of each cassandra node <description> when running cassandra nodes with collocated spark nodes and accessing such cluster from remote, to get data-locality right, we need to tell spark the locations of the cassandra nodes and they should match the addresses that spark nodes bind to. therefore in cloud environments we need to use private ips for that. unfortunately, the client which connects from remote would know only the broadcast rpc_addresses which are different. can we have the ip/hostname that every c* node binds to exposed in a system table?   system.peers table contains that information, but it doesn't contain that information for the local node.  so can we have broadcast_address and rpc_address added to the system.local table?<stacktrace> <code> <text> when running cassandra nodes with collocated spark nodes and accessing such cluster from remote, to get data-locality right, we need to tell spark the locations of the cassandra nodes and they should match the addresses that spark nodes bind to. therefore in cloud environments we need to use private ips for that. unfortunately, the client which connects from remote would know only the broadcast rpc_addresses which are different. can we have the ip/hostname that every c* node binds to exposed in a system table?   system.peers table contains that information, but it doesn't contain that information for the local node.  so can we have broadcast_address and rpc_address added to the system.local table?",
        "label": 98
    },
    {
        "text": "dc local repair or  hosts should only be allowed with  full repair <description> we should not let users mix incremental repair with dc local repair or -host or any repair which does not include all replicas. this will currently cause stables on some replicas to be marked as repaired. the next incremental repair will not work on same set of data.<stacktrace> <code> <text> we should not let users mix incremental repair with dc local repair or -host or any repair which does not include all replicas. this will currently cause stables on some replicas to be marked as repaired. the next incremental repair will not work on same set of data.",
        "label": 321
    },
    {
        "text": "sstableloader fails when attempting to load data from a single node into a multi node cluster <description> i'm running into this exception when trying to use sstableloader to bring in data from another cluster: rhatch@whatup:~/.ccm/test_cluster_1391031988/node1$ bin/sstableloader -d 127.0.0.1 ~/tmp/keyspace1/standard1 established connection to initial hosts opening sstables and calculating sections to stream streaming relevant part of /home/rhatch/tmp/keyspace1/standard1/keyspace1-standard1-jb-5-data.db /home/rhatch/tmp/keyspace1/standard1/keyspace1-standard1-jb-6-data.db to [/127.0.0.1, /127.0.0.2, /127.0.0.3] exception in thread \"stream-out-/127.0.0.1\" java.lang.nullpointerexception at org.apache.cassandra.streaming.connectionhandler$messagehandler.signalclosedone(connectionhandler.java:249) at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:375) at java.lang.thread.run(thread.java:744) this is what i see in the node system.log: ==> ./test_cluster_1391031988/node1/logs/system.log <==  info [stream-init-/127.0.0.1:60971] 2014-01-29 14:57:25,375 streamresultfuture.java (line 116) [stream #564ded70-8930-11e3-84e9-2766c3cc4197] received streaming plan for bulk load  info [stream-in-/127.0.1.1] 2014-01-29 14:57:25,375 streamresultfuture.java (line 168) [stream #564ded70-8930-11e3-84e9-2766c3cc4197] prepare completed. receiving 2 files(91047224 bytes), sending 0 files(0 bytes) steps to reproduce: create a 3 node cluster with ccm run stress on one node with 'ccm node1 stress' copy the node's data from the data/keyspace1/standard1 directory to save it for re-loading (preserve the directory structure for sstableloader) remove the cluster, and create a new 3 node cluster pick a node and run bin/nodetool -d localhost ~/saved_data_location/keyspace1/standard1<stacktrace> rhatch@whatup:~/.ccm/test_cluster_1391031988/node1$ bin/sstableloader -d 127.0.0.1 ~/tmp/keyspace1/standard1 established connection to initial hosts opening sstables and calculating sections to stream streaming relevant part of /home/rhatch/tmp/keyspace1/standard1/keyspace1-standard1-jb-5-data.db /home/rhatch/tmp/keyspace1/standard1/keyspace1-standard1-jb-6-data.db to [/127.0.0.1, /127.0.0.2, /127.0.0.3] exception in thread 'stream-out-/127.0.0.1' java.lang.nullpointerexception at org.apache.cassandra.streaming.connectionhandler$messagehandler.signalclosedone(connectionhandler.java:249) at org.apache.cassandra.streaming.connectionhandler$outgoingmessagehandler.run(connectionhandler.java:375) at java.lang.thread.run(thread.java:744) <code> ==> ./test_cluster_1391031988/node1/logs/system.log <==  info [stream-init-/127.0.0.1:60971] 2014-01-29 14:57:25,375 streamresultfuture.java (line 116) [stream #564ded70-8930-11e3-84e9-2766c3cc4197] received streaming plan for bulk load  info [stream-in-/127.0.1.1] 2014-01-29 14:57:25,375 streamresultfuture.java (line 168) [stream #564ded70-8930-11e3-84e9-2766c3cc4197] prepare completed. receiving 2 files(91047224 bytes), sending 0 files(0 bytes) <text> i'm running into this exception when trying to use sstableloader to bring in data from another cluster: this is what i see in the node system.log: steps to reproduce: create a 3 node cluster with ccm run stress on one node with 'ccm node1 stress' copy the node's data from the data/keyspace1/standard1 directory to save it for re-loading (preserve the directory structure for sstableloader) remove the cluster, and create a new 3 node cluster pick a node and run bin/nodetool -d localhost ~/saved_data_location/keyspace1/standard1",
        "label": 577
    },
    {
        "text": "oom in cassandra <description> i have a program to stress test cassandra. what it does is remove/insert rows with a small set of row keys as fast as possible. two cfs are involved. when i test against c* 1.2.3 with default configurations, it ran for 24 hours and c* doesn't having any issue. however after i upgraded to c* 2.0.1, c* crashes on oom within 1-2 minutes. i can consistently reproduce this. i built c* from the source and found out the last good changeset is cfa097cdd5e28d7fe8204248e246a1fae226d2c0. as soon as i include the next changeset 1e0d9513b748fae4ec0737283da71c65e9272102, c* starts to crash. what's interesting is although it seems the change was reverted by fc1a7206fe15882fd64e7ba8eb68ba9dc320275f. c* built from fc1a7206fe15882fd64e7ba8eb68ba9dc320275f has the same problem - oom within minutes. i didn't test against the official 2.0.0. but the c* built from 03045ca22b11b0e5fc85c4fabd83ce6121b5709b seems ok. i assume that's what 2.0.0 is. i use default configurations in all cases. i didn't tune anything.<stacktrace> <code> <text> i have a program to stress test cassandra. what it does is remove/insert rows with a small set of row keys as fast as possible. two cfs are involved. when i test against c* 1.2.3 with default configurations, it ran for 24 hours and c* doesn't having any issue. however after i upgraded to c* 2.0.1, c* crashes on oom within 1-2 minutes. i can consistently reproduce this. i built c* from the source and found out the last good changeset is cfa097cdd5e28d7fe8204248e246a1fae226d2c0. as soon as i include the next changeset 1e0d9513b748fae4ec0737283da71c65e9272102, c* starts to crash. what's interesting is although it seems the change was reverted by fc1a7206fe15882fd64e7ba8eb68ba9dc320275f. c* built from fc1a7206fe15882fd64e7ba8eb68ba9dc320275f has the same problem - oom within minutes. i didn't test against the official 2.0.0. but the c* built from 03045ca22b11b0e5fc85c4fabd83ce6121b5709b seems ok. i assume that's what 2.0.0 is. i use default configurations in all cases. i didn't tune anything.",
        "label": 274
    },
    {
        "text": " patch  fix some obvious javadoc issues generated via ant javadoc <description> <stacktrace> <code> <text> ",
        "label": 139
    },
    {
        "text": "anitentropy merkletree error <description> we are seeing antientropy errors when performing repair jobs in one of our cassandra clusters. it seems to have started with 1.2. (maybe an issue with vnodes) the exceptions occur almost every time we try to do a repair on all column families in the cluster. doing the same task on 1.1 does not trigger this. 6 nodes cluster (vnodes, murmur3, rf:3)  very low activity  running a nodetool repair -pr loop on the cluster nodes  nodetool hangs, and same big stacktrace in logs. root 11025 0.0 0.0 106100 1436 pts/0 s+ feb11 0:00 _ /bin/sh /usr/bin/nodetool -h host -p 7199 -pr repair keyspace column_family error [antientropystage:3] 2013-02-11 17:08:12,630 cassandradaemon.java (line 133) exception in thread thread[antientropystage:3,5,main]  java.lang.assertionerror  at org.apache.cassandra.utils.merkletree.inc(merkletree.java:137)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:245)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.difference(merkletree.java:227)  at org.apache.cassandra.service.antientropyservice$repairsession$differencer.run(antientropyservice.java:982)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) this issue was partially solved earlier but seems to be back with vnodes: https://issues.apache.org/jira/browse/cassandra-3014<stacktrace> error [antientropystage:3] 2013-02-11 17:08:12,630 cassandradaemon.java (line 133) exception in thread thread[antientropystage:3,5,main]  java.lang.assertionerror  at org.apache.cassandra.utils.merkletree.inc(merkletree.java:137)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:245)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:256)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.differencehelper(merkletree.java:267)  at org.apache.cassandra.utils.merkletree.difference(merkletree.java:227)  at org.apache.cassandra.service.antientropyservice$repairsession$differencer.run(antientropyservice.java:982)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662) <code> root 11025 0.0 0.0 106100 1436 pts/0 s+ feb11 0:00 _ /bin/sh /usr/bin/nodetool -h host -p 7199 -pr repair keyspace column_family <text> we are seeing antientropy errors when performing repair jobs in one of our cassandra clusters. it seems to have started with 1.2. (maybe an issue with vnodes) the exceptions occur almost every time we try to do a repair on all column families in the cluster. doing the same task on 1.1 does not trigger this. 6 nodes cluster (vnodes, murmur3, rf:3)  very low activity  running a nodetool repair -pr loop on the cluster nodes  nodetool hangs, and same big stacktrace in logs. this issue was partially solved earlier but seems to be back with vnodes: https://issues.apache.org/jira/browse/cassandra-3014",
        "label": 520
    },
    {
        "text": "failure reading a erroneous spurious autosavingcache file can result in a failed application of a migration  which can prevent a node from reaching schema agreement  <description> failure reading a erroneous/spurious autosavingcache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement. this is distinctly possible when a machine loses it's data partition, and attempts to recover the schema upon restart, and so has to apply all the migrations. the initial stack traces look like this: add column family: org.apache.cassandra.config.cfmetadata@38bcfee6[cfid=1000,ksname=someks,cfname=somecf,cftype=standard,comparat  or=org.apache.cassandra.db.marshal.utf8type,subcolumncomparator=<null>, ... followed by: error 00:56:47,974 fatal exception in thread thread[migrationstage:1,5,main]  java.lang.runtimeexception: java.lang.negativearraysizeexception  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.negativearraysizeexception  at org.apache.cassandra.cache.autosavingcache.readsaved(autosavingcache.java:130)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:273)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:465)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:435)  at org.apache.cassandra.db.table.initcf(table.java:369)  at org.apache.cassandra.db.migration.addcolumnfamily.applymodels(addcolumnfamily.java:93)  at org.apache.cassandra.db.migration.migration.apply(migration.java:153)  at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:73)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more ultimately, attempted changes to this keyspace/cf will fail like this: error 13:07:51,006 fatal exception in thread thread[migrationstage:1,5,main]  java.lang.runtimeexception: java.lang.illegalargumentexception: unknown cf 1000  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.illegalargumentexception: unknown cf 1000  at org.apache.cassandra.db.table.getcolumnfamilystore(table.java:155)  at org.apache.cassandra.db.table.getcolumnfamilystore(table.java:148)  at org.apache.cassandra.db.migration.dropkeyspace.applymodels(dropkeyspace.java:63)  at org.apache.cassandra.db.migration.migration.apply(migration.java:153)  at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:73)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more<stacktrace> error 00:56:47,974 fatal exception in thread thread[migrationstage:1,5,main]  java.lang.runtimeexception: java.lang.negativearraysizeexception  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.negativearraysizeexception  at org.apache.cassandra.cache.autosavingcache.readsaved(autosavingcache.java:130)  at org.apache.cassandra.db.columnfamilystore.<init>(columnfamilystore.java:273)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:465)  at org.apache.cassandra.db.columnfamilystore.createcolumnfamilystore(columnfamilystore.java:435)  at org.apache.cassandra.db.table.initcf(table.java:369)  at org.apache.cassandra.db.migration.addcolumnfamily.applymodels(addcolumnfamily.java:93)  at org.apache.cassandra.db.migration.migration.apply(migration.java:153)  at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:73)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more error 13:07:51,006 fatal exception in thread thread[migrationstage:1,5,main]  java.lang.runtimeexception: java.lang.illegalargumentexception: unknown cf 1000  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:34)  at java.util.concurrent.executors$runnableadapter.call(executors.java:441)  at java.util.concurrent.futuretask$sync.innerrun(futuretask.java:303)  at java.util.concurrent.futuretask.run(futuretask.java:138)  at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)  at java.lang.thread.run(thread.java:662)  caused by: java.lang.illegalargumentexception: unknown cf 1000  at org.apache.cassandra.db.table.getcolumnfamilystore(table.java:155)  at org.apache.cassandra.db.table.getcolumnfamilystore(table.java:148)  at org.apache.cassandra.db.migration.dropkeyspace.applymodels(dropkeyspace.java:63)  at org.apache.cassandra.db.migration.migration.apply(migration.java:153)  at org.apache.cassandra.db.definitionsupdateverbhandler$1.runmaythrow(definitionsupdateverbhandler.java:73)  at org.apache.cassandra.utils.wrappedrunnable.run(wrappedrunnable.java:30)  ... 6 more<code> add column family: org.apache.cassandra.config.cfmetadata@38bcfee6[cfid=1000,ksname=someks,cfname=somecf,cftype=standard,comparat  or=org.apache.cassandra.db.marshal.utf8type,subcolumncomparator=<null>, ... <text> failure reading a erroneous/spurious autosavingcache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement. this is distinctly possible when a machine loses it's data partition, and attempts to recover the schema upon restart, and so has to apply all the migrations. the initial stack traces look like this: followed by: ultimately, attempted changes to this keyspace/cf will fail like this: ",
        "label": 166
    },
    {
        "text": "cqlparser throws stackoverflowerror on bigger batch operation <description> we are seeing a problem with cql3/cassandra 1.2.8 where a large batch operation causes the cqlparser to throw a stackoverflowerror (-xss180k initially, then -xss325k). shouldn't a batch be processed iteratively to avoid having to bump stack sizes to unreasonably large values? here is more info from the original problem description: <<<  it looks like the cqlparser in 1.2.8 (probably 1.2.x, but i didn't look) is implemented recursively in such a way that large batch statements blow up the stack. we, of course on a friday night, have a particular piece of code that's hitting a degenerate case that creates a batch of inserts with a very large number of collection items, and it manifests as a stackoverflow coming out the cass servers: java.lang.stackoverflowerror  at org.apache.cassandra.cql3.cqlparser.value(cqlparser.java:5266)  at org.apache.cassandra.cql3.cqlparser.term(cqlparser.java:5627)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4807)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  ... i think in the short term i can give up the atomicity of a batch in this code and kind of suck it up, but obviously i'd prefer not to. i'm also not sure if i kept a single batch, but split this into smaller pieces in each statement, whether that would still fail. i'm guessing i could also crank the hell out of the stack size on the servers, but that feels pretty dirty. it seems like the cqlparser should probably be implemented in a way that isn't quite so vulnerable to this, though i fully accept that this batch is koo-koo-bananas.  >>> thanks!<stacktrace> java.lang.stackoverflowerror  at org.apache.cassandra.cql3.cqlparser.value(cqlparser.java:5266)  at org.apache.cassandra.cql3.cqlparser.term(cqlparser.java:5627)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4807)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  at org.apache.cassandra.cql3.cqlparser.set_tail(cqlparser.java:4813)  ... <code> <text> we are seeing a problem with cql3/cassandra 1.2.8 where a large batch operation causes the cqlparser to throw a stackoverflowerror (-xss180k initially, then -xss325k). shouldn't a batch be processed iteratively to avoid having to bump stack sizes to unreasonably large values? here is more info from the original problem description: <<<  it looks like the cqlparser in 1.2.8 (probably 1.2.x, but i didn't look) is implemented recursively in such a way that large batch statements blow up the stack. we, of course on a friday night, have a particular piece of code that's hitting a degenerate case that creates a batch of inserts with a very large number of collection items, and it manifests as a stackoverflow coming out the cass servers: i think in the short term i can give up the atomicity of a batch in this code and kind of suck it up, but obviously i'd prefer not to. i'm also not sure if i kept a single batch, but split this into smaller pieces in each statement, whether that would still fail. i'm guessing i could also crank the hell out of the stack size on the servers, but that feels pretty dirty. it seems like the cqlparser should probably be implemented in a way that isn't quite so vulnerable to this, though i fully accept that this batch is koo-koo-bananas.  >>> thanks!",
        "label": 18
    },
    {
        "text": "move twcs message  no compaction necessary for bucket size  to trace level <description> when using twcs, this message sometimes spams the debug logs: debug compactionexecutor:4993 2018-04-20 00:41:13,795 timewindowcompactionstrategy.java:304 - no compaction necessary for bucket size 1 , key 1521763200000, now 1524182400000 the similar message is already at trace level for lcs, so this patch changes the message from twcs to trace as well.<stacktrace> <code> debug compactionexecutor:4993 2018-04-20 00:41:13,795 timewindowcompactionstrategy.java:304 - no compaction necessary for bucket size 1 , key 1521763200000, now 1524182400000 <text> when using twcs, this message sometimes spams the debug logs: the similar message is already at trace level for lcs, so this patch changes the message from twcs to trace as well.",
        "label": 217
    },
    {
        "text": "sstableexport can not accept  f    k options <description> the sstableexport command can not accept -f , -k options correct, always said as bellow: [root@hfdevcasda01 bin]# ./sstable2json -f out.json /opt/cassandra-wbx/data/content_hf/changehistory-2-data.db  you must supply exactly one sstable  usage: org.apache.cassandra.tools.sstableexport [-f outfile] <sstable> [-k key [-k key [...]]]<stacktrace> <code> <text> the sstableexport command can not accept -f , -k options correct, always said as bellow: [root@hfdevcasda01 bin]# ./sstable2json -f out.json /opt/cassandra-wbx/data/content_hf/changehistory-2-data.db  you must supply exactly one sstable  usage: org.apache.cassandra.tools.sstableexport [-f outfile] <sstable> [-k key [-k key [...]]]",
        "label": 274
    },
    {
        "text": "cql  support if exists extension for drop commands  table  keyspace  index  <description> <stacktrace> <code> <text> ",
        "label": 352
    },
    {
        "text": "exceptions after cleanup <description> it looks like cassandra-1916 may have introduced a regression. after running a cleanup, i get the following exception when trying to read: error 17:25:23,574 fatal exception in thread thread[readstage:99,5,main] java.lang.assertionerror: skipping negative bytes is illegal: -1393754107         at org.apache.cassandra.io.util.mappedfiledatainput.skipbytes(mappedfiledatainput.java:96)         at org.apache.cassandra.io.sstable.indexhelper.skipbloomfilter(indexhelper.java:50)         at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:56)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:91)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:67)         at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:68)         at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:80)         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1215)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1107)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1077)         at org.apache.cassandra.db.table.getrow(table.java:384)         at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)         at org.apache.cassandra.db.readverbhandler.doverb(readverbhandler.java:68)         at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:63)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662)<stacktrace> error 17:25:23,574 fatal exception in thread thread[readstage:99,5,main] java.lang.assertionerror: skipping negative bytes is illegal: -1393754107         at org.apache.cassandra.io.util.mappedfiledatainput.skipbytes(mappedfiledatainput.java:96)         at org.apache.cassandra.io.sstable.indexhelper.skipbloomfilter(indexhelper.java:50)         at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:56)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:91)         at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:67)         at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:68)         at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:80)         at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1215)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1107)         at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1077)         at org.apache.cassandra.db.table.getrow(table.java:384)         at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:63)         at org.apache.cassandra.db.readverbhandler.doverb(readverbhandler.java:68)         at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:63)         at java.util.concurrent.threadpoolexecutor$worker.runtask(threadpoolexecutor.java:886)         at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:908)         at java.lang.thread.run(thread.java:662) <code> <text> it looks like cassandra-1916 may have introduced a regression. after running a cleanup, i get the following exception when trying to read:",
        "label": 274
    },
    {
        "text": "dtest failure in repair tests incremental repair test testincrepair sstable marking test <description> example failure: http://cassci.datastax.com/job/trunk_dtest/1525/testreport/repair_tests.incremental_repair_test/testincrepair/sstable_marking_test error message 'repaired at: 0' unexpectedly found in 'sstable: /tmp/dtest-qoneec/test/node1/data0/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-4-big\\npartitioner: org.apache.cassandra.dht.murmur3partitioner\\nbloom filter fp chance: 0.010000\\nminimum timestamp: 1490129948985001\\nmaximum timestamp: 1490129952789002\\nsstable min local deletion time: 2147483647\\nsstable max local deletion time: 2147483647\\ncompressor: -\\nttl min: 0\\nttl max: 0\\nfirst token: -9222701292667950301 (key=5032394c323239385030)\\nlast token: -3062233317334255711 (key=3032503434364f4e4f30)\\nestimated droppable tombstones: 0.0\\nsstable level: 0\\nrepaired at: 0\\npending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\\nreplay positions covered: {commitlogposition(segmentid=1490129923946, position=42824)=commitlogposition(segmentid=1490129923946, position=2605214)}\\ntotalcolumnsset: 16550\\ntotalrows: 3310\\nestimated tombstone drop times:\\ncount               row size        cell count\\n1                          0                 0\\n2                          0                 0\\n3                          0                 0\\n4                          0                 0\\n5                          0              3310\\n6                          0                 0\\n7                          0                 0\\n8                          0                 0\\n10                         0                 0\\n12                         0                 0\\n14                         0                 0\\n17                         0                 0\\n20                         0                 0\\n24                         0                 0\\n29                         0                 0\\n35                         0                 0\\n42                         0                 0\\n50                         0                 0\\n60                         0                 0\\n72                         0                 0\\n86                         0                 0\\n103                        0                 0\\n124                        0                 0\\n149                        0                 0\\n179                        0                 0\\n215                        1                 0\\n258                     3309                 0\\n310                        0                 0\\n372                        0                 0\\n446                        0                 0\\n535                        0                 0\\n642                        0                 0\\n770                        0                 0\\n924                        0                 0\\n1109                       0                 0\\n1331                       0                 0\\n1597                       0                 0\\n1916                       0                 0\\n2299                       0                 0\\n2759                       0                 0\\n3311                       0                 0\\n3973                       0                 0\\n4768                       0                 0\\n5722                       0                 0\\n6866                       0                 0\\n8239                       0                 0\\n9887                       0                 0\\n11864                      0                 0\\n14237                      0                 0\\n17084                      0                 0\\n20501                      0                 0\\n24601                      0                 0\\n29521                      0                 0\\n35425                      0                 0\\n42510                      0                 0\\n51012                      0                 0\\n61214                      0                 0\\n73457                      0                 0\\n88148                      0                 0\\n105778                     0                 0\\n126934                     0                 0\\n152321                     0                 0\\n182785                     0                 0\\n219342                     0                 0\\n263210                     0                 0\\n315852                     0                 0\\n379022                     0                 0\\n454826                     0                 0\\n545791                     0                 0\\n654949                     0                 0\\n785939                     0                 0\\n943127                     0                 0\\n1131752                    0                 0\\n1358102                    0                 0\\n1629722                    0                 0\\n1955666                    0                 0\\n2346799                    0                 0\\n2816159                    0                 0\\n3379391                    0                 0\\n4055269                    0                 0\\n4866323                    0                 0\\n5839588                    0                 0\\n7007506                    0                 0\\n8409007                    0                 0\\n10090808                   0                 0\\n12108970                   0                 0\\n14530764                   0                 0\\n17436917                   0                 0\\n20924300                   0                 0\\n25109160                   0                 0\\n30130992                   0                 0\\n36157190                   0                 0\\n43388628                   0                 0\\n52066354                   0                 0\\n62479625                   0                 0\\n74975550                   0                 0\\n89970660                   0                 0\\n107964792                  0                 0\\n129557750                  0                 0\\n155469300                  0                 0\\n186563160                  0                 0\\n223875792                  0                 0\\n268650950                  0                 0\\n322381140                  0                 0\\n386857368                  0                 0\\n464228842                  0                 0\\n557074610                  0                 0\\n668489532                  0                 0\\n802187438                  0                 0\\n962624926                  0                 0\\n1155149911                 0                 0\\n1386179893                 0                 0\\n1663415872                 0                 0\\n1996099046                 0                 0\\n2395318855                 0                 0\\n2874382626                 0                  \\n3449259151                 0                  \\n4139110981                 0                  \\n4966933177                 0                  \\n5960319812                 0                  \\n7152383774                 0                  \\n8582860529                 0                  \\n10299432635                 0                  \\n12359319162                 0                  \\n14831182994                 0                  \\n17797419593                 0                  \\n21356903512                 0                  \\n25628284214                 0                  \\n30753941057                 0                  \\n36904729268                 0                  \\n44285675122                 0                  \\n53142810146                 0                  \\n63771372175                 0                  \\n76525646610                 0                  \\n91830775932                 0                  \\n110196931118                 0                  \\n132236317342                 0                  \\n158683580810                 0                  \\n190420296972                 0                  \\n228504356366                 0                  \\n274205227639                 0                  \\n329046273167                 0                  \\n394855527800                 0                  \\n473826633360                 0                  \\n568591960032                 0                  \\n682310352038                 0                  \\n818772422446                 0                  \\n982526906935                 0                  \\n1179032288322                 0                  \\n1414838745986                 0                  \\nestimated cardinality: 3310\\nencodingstats minttl: 0\\nencodingstats minlocaldeletiontime: 1442880000\\nencodingstats mintimestamp: 1490129948985001\\nkeytype: org.apache.cassandra.db.marshal.bytestype\\nclusteringtypes: [org.apache.cassandra.db.marshal.utf8type]\\nstaticcolumns: {c3:org.apache.cassandra.db.marshal.bytestype, c4:org.apache.cassandra.db.marshal.bytestype, c0:org.apache.cassandra.db.marshal.bytestype, c1:org.apache.cassandra.db.marshal.bytestype, c2:org.apache.cassandra.db.marshal.bytestype}\\nregularcolumns: {}\\nsstable: /tmp/dtest-qoneec/test/node1/data1/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-5-big\\npartitioner: org.apache.cassandra.dht.murmur3partitioner\\nbloom filter fp chance: 0.010000\\nminimum timestamp: 1490129948987000\\nmaximum timestamp: 1490129952789004\\nsstable min local deletion time: 2147483647\\nsstable max local deletion time: 2147483647\\ncompressor: -\\nttl min: 0\\nttl max: 0\\nfirst token: -3060251208033125494 (key=33364e4b313936504b30)\\nlast token: 2923054332122545251 (key=344b3634354b35353430)\\nestimated droppable tombstones: 0.0\\nsstable level: 0\\nrepaired at: 0\\npending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\\nreplay positions covered: {commitlogposition(segmentid=1490129923946, position=42824)=commitlogposition(segmentid=1490129923946, position=2605214)}\\ntotalcolumnsset: 16565\\ntotalrows: 3313\\nestimated tombstone drop times:\\ncount               row size        cell count\\n1                          0                 0\\n2                          0                 0\\n3                          0                 0\\n4                          0                 0\\n5                          0              3313\\n6                          0                 0\\n7                          0                 0\\n8                          0                 0\\n10                         0                 0\\n12                         0                 0\\n14                         0                 0\\n17                         0                 0\\n20                         0                 0\\n24                         0                 0\\n29                         0                 0\\n35                         0                 0\\n42                         0                 0\\n50                         0                 0\\n60                         0                 0\\n72                         0                 0\\n86                         0                 0\\n103                        0                 0\\n124                        0                 0\\n149                        0                 0\\n179                        0                 0\\n215                        1                 0\\n258                     3312                 0\\n310                        0                 0\\n372                        0                 0\\n446                        0                 0\\n535                        0                 0\\n642                        0                 0\\n770                        0                 0\\n924                        0                 0\\n1109                       0                 0\\n1331                       0                 0\\n1597                       0                 0\\n1916                       0                 0\\n2299                       0                 0\\n2759                       0                 0\\n3311                       0                 0\\n3973                       0                 0\\n4768                       0                 0\\n5722                       0                 0\\n6866                       0                 0\\n8239                       0                 0\\n9887                       0                 0\\n11864                      0                 0\\n14237                      0                 0\\n17084                      0                 0\\n20501                      0                 0\\n24601                      0                 0\\n29521                      0                 0\\n35425                      0                 0\\n42510                      0                 0\\n51012                      0                 0\\n61214                      0                 0\\n73457                      0                 0\\n88148                      0                 0\\n105778                     0                 0\\n126934                     0                 0\\n152321                     0                 0\\n182785                     0                 0\\n219342                     0                 0\\n263210                     0                 0\\n315852                     0                 0\\n379022                     0                 0\\n454826                     0                 0\\n545791                     0                 0\\n654949                     0                 0\\n785939                     0                 0\\n943127                     0                 0\\n1131752                    0                 0\\n1358102                    0                 0\\n1629722                    0                 0\\n1955666                    0                 0\\n2346799                    0                 0\\n2816159                    0                 0\\n3379391                    0                 0\\n4055269                    0                 0\\n4866323                    0                 0\\n5839588                    0                 0\\n7007506                    0                 0\\n8409007                    0                 0\\n10090808                   0                 0\\n12108970                   0                 0\\n14530764                   0                 0\\n17436917                   0                 0\\n20924300                   0                 0\\n25109160                   0                 0\\n30130992                   0                 0\\n36157190                   0                 0\\n43388628                   0                 0\\n52066354                   0                 0\\n62479625                   0                 0\\n74975550                   0                 0\\n89970660                   0                 0\\n107964792                  0                 0\\n129557750                  0                 0\\n155469300                  0                 0\\n186563160                  0                 0\\n223875792                  0                 0\\n268650950                  0                 0\\n322381140                  0                 0\\n386857368                  0                 0\\n464228842                  0                 0\\n557074610                  0                 0\\n668489532                  0                 0\\n802187438                  0                 0\\n962624926                  0                 0\\n1155149911                 0                 0\\n1386179893                 0                 0\\n1663415872                 0                 0\\n1996099046                 0                 0\\n2395318855                 0                 0\\n2874382626                 0                  \\n3449259151                 0                  \\n4139110981                 0                  \\n4966933177                 0                  \\n5960319812                 0                  \\n7152383774                 0                  \\n8582860529                 0                  \\n10299432635                 0                  \\n12359319162                 0                  \\n14831182994                 0                  \\n17797419593                 0                  \\n21356903512                 0                  \\n25628284214                 0                  \\n30753941057                 0                  \\n36904729268                 0                  \\n44285675122                 0                  \\n53142810146                 0                  \\n63771372175                 0                  \\n76525646610                 0                  \\n91830775932                 0                  \\n110196931118                 0                  \\n132236317342                 0                  \\n158683580810                 0                  \\n190420296972                 0                  \\n228504356366                 0                  \\n274205227639                 0                  \\n329046273167                 0                  \\n394855527800                 0                  \\n473826633360                 0                  \\n568591960032                 0                  \\n682310352038                 0                  \\n818772422446                 0                  \\n982526906935                 0                  \\n1179032288322                 0                  \\n1414838745986                 0                  \\nestimated cardinality: 3313\\nencodingstats minttl: 0\\nencodingstats minlocaldeletiontime: 1442880000\\nencodingstats mintimestamp: 1490129948987000\\nkeytype: org.apache.cassandra.db.marshal.bytestype\\nclusteringtypes: [org.apache.cassandra.db.marshal.utf8type]\\nstaticcolumns: {c3:org.apache.cassandra.db.marshal.bytestype, c4:org.apache.cassandra.db.marshal.bytestype, c0:org.apache.cassandra.db.marshal.bytestype, c1:org.apache.cassandra.db.marshal.bytestype, c2:org.apache.cassandra.db.marshal.bytestype}\\nregularcolumns: {}\\nsstable: /tmp/dtest-qoneec/test/node1/data2/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-6-big\\npartitioner: org.apache.cassandra.dht.murmur3partitioner\\nbloom filter fp chance: 0.010000\\nminimum timestamp: 1490129948984000\\nmaximum timestamp: 1490129952789003\\nsstable min local deletion time: 2147483647\\nsstable max local deletion time: 2147483647\\ncompressor: -\\nttl min: 0\\nttl max: 0\\nfirst token: 2925175199546211606 (key=3250365032354e4c3231)\\nlast token: 9222137691148971235 (key=4c30334f32394d4c3031)\\nestimated droppable tombstones: 0.0\\nsstable level: 0\\nrepaired at: 0\\npending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\\nreplay positions covered: {commitlogposition(segmentid=1490129923946, position=42824)=commitlogposition(segmentid=1490129923946, position=2605214)}\\ntotalcolumnsset: 16885\\ntotalrows: 3377\\nestimated tombstone drop times:\\ncount               row size        cell count\\n1                          0                 0\\n2                          0                 0\\n3                          0                 0\\n4                          0                 0\\n5                          0              3377\\n6                          0                 0\\n7                          0                 0\\n8                          0                 0\\n10                         0                 0\\n12                         0                 0\\n14                         0                 0\\n17                         0                 0\\n20                         0                 0\\n24                         0                 0\\n29                         0                 0\\n35                         0                 0\\n42                         0                 0\\n50                         0                 0\\n60                         0                 0\\n72                         0                 0\\n86                         0                 0\\n103                        0                 0\\n124                        0                 0\\n149                        0                 0\\n179                        0                 0\\n215                        1                 0\\n258                     3376                 0\\n310                        0                 0\\n372                        0                 0\\n446                        0                 0\\n535                        0                 0\\n642                        0                 0\\n770                        0                 0\\n924                        0                 0\\n1109                       0                 0\\n1331                       0                 0\\n1597                       0                 0\\n1916                       0                 0\\n2299                       0                 0\\n2759                       0                 0\\n3311                       0                 0\\n3973                       0                 0\\n4768                       0                 0\\n5722                       0                 0\\n6866                       0                 0\\n8239                       0                 0\\n9887                       0                 0\\n11864                      0                 0\\n14237                      0                 0\\n17084                      0                 0\\n20501                      0                 0\\n24601                      0                 0\\n29521                      0                 0\\n35425                      0                 0\\n42510                      0                 0\\n51012                      0                 0\\n61214                      0                 0\\n73457                      0                 0\\n88148                      0                 0\\n105778                     0                 0\\n126934                     0                 0\\n152321                     0                 0\\n182785                     0                 0\\n219342                     0                 0\\n263210                     0                 0\\n315852                     0                 0\\n379022                     0                 0\\n454826                     0                 0\\n545791                     0                 0\\n654949                     0                 0\\n785939                     0                 0\\n943127                     0                 0\\n1131752                    0                 0\\n1358102                    0                 0\\n1629722                    0                 0\\n1955666                    0                 0\\n2346799                    0                 0\\n2816159                    0                 0\\n3379391                    0                 0\\n4055269                    0                 0\\n4866323                    0                 0\\n5839588                    0                 0\\n7007506                    0                 0\\n8409007                    0                 0\\n10090808                   0                 0\\n12108970                   0                 0\\n14530764                   0                 0\\n17436917                   0                 0\\n20924300                   0                 0\\n25109160                   0                 0\\n30130992                   0                 0\\n36157190                   0                 0\\n43388628                   0                 0\\n52066354                   0                 0\\n62479625                   0                 0\\n74975550                   0                 0\\n89970660                   0                 0\\n107964792                  0                 0\\n129557750                  0                 0\\n155469300                  0                 0\\n186563160                  0                 0\\n223875792                  0                 0\\n268650950                  0                 0\\n322381140                  0                 0\\n386857368                  0                 0\\n464228842                  0                 0\\n557074610                  0                 0\\n668489532                  0                 0\\n802187438                  0                 0\\n962624926                  0                 0\\n1155149911                 0                 0\\n1386179893                 0                 0\\n1663415872                 0                 0\\n1996099046                 0                 0\\n2395318855                 0                 0\\n2874382626                 0                  \\n3449259151                 0                  \\n4139110981                 0                  \\n4966933177                 0                  \\n5960319812                 0                  \\n7152383774                 0                  \\n8582860529                 0                  \\n10299432635                 0                  \\n12359319162                 0                  \\n14831182994                 0                  \\n17797419593                 0                  \\n21356903512                 0                  \\n25628284214                 0                  \\n30753941057                 0                  \\n36904729268                 0                  \\n44285675122                 0                  \\n53142810146                 0                  \\n63771372175                 0                  \\n76525646610                 0                  \\n91830775932                 0                  \\n110196931118                 0                  \\n132236317342                 0                  \\n158683580810                 0                  \\n190420296972                 0                  \\n228504356366                 0                  \\n274205227639                 0                  \\n329046273167                 0                  \\n394855527800                 0                  \\n473826633360                 0                  \\n568591960032                 0                  \\n682310352038                 0                  \\n818772422446                 0                  \\n982526906935                 0                  \\n1179032288322                 0                  \\n1414838745986                 0                  \\nestimated cardinality: 3377\\nencodingstats minttl: 0\\nencodingstats minlocaldeletiontime: 1442880000\\nencodingstats mintimestamp: 1490129948984000\\nkeytype: org.apache.cassandra.db.marshal.bytestype\\nclusteringtypes: [org.apache.cassandra.db.marshal.utf8type]\\nstaticcolumns: {c3:org.apache.cassandra.db.marshal.bytestype, c4:org.apache.cassandra.db.marshal.bytestype, c0:org.apache.cassandra.db.marshal.bytestype, c1:org.apache.cassandra.db.marshal.bytestype, c2:org.apache.cassandra.db.marshal.bytestype}\\nregularcolumns: {}\\n' stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py\", line 332, in sstable_marking_test     self.assertnotin('repaired at: 0', out)   file \"/usr/lib/python2.7/unittest/case.py\", line 810, in assertnotin     self.fail(self._formatmessage(msg, standardmsg))   file \"/usr/lib/python2.7/unittest/case.py\", line 410, in fail     raise self.failureexception(msg)<stacktrace> <code> error message 'repaired at: 0' unexpectedly found in 'sstable: /tmp/dtest-qoneec/test/node1/data0/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-4-big/npartitioner: org.apache.cassandra.dht.murmur3partitioner/nbloom filter fp chance: 0.010000/nminimum timestamp: 1490129948985001/nmaximum timestamp: 1490129952789002/nsstable min local deletion time: 2147483647/nsstable max local deletion time: 2147483647/ncompressor: -/nttl min: 0/nttl max: 0/nfirst token: -9222701292667950301 (key=5032394c323239385030)/nlast token: -3062233317334255711 (key=3032503434364f4e4f30)/nestimated droppable tombstones: 0.0/nsstable level: 0/nrepaired at: 0/npending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf/nreplay positions covered: {commitlogposition(segmentid=1490129923946, position=42824)=commitlogposition(segmentid=1490129923946, position=2605214)}/ntotalcolumnsset: 16550/ntotalrows: 3310/nestimated tombstone drop times:/ncount               row size        cell count/n1                          0                 0/n2                          0                 0/n3                          0                 0/n4                          0                 0/n5                          0              3310/n6                          0                 0/n7                          0                 0/n8                          0                 0/n10                         0                 0/n12                         0                 0/n14                         0                 0/n17                         0                 0/n20                         0                 0/n24                         0                 0/n29                         0                 0/n35                         0                 0/n42                         0                 0/n50                         0                 0/n60                         0                 0/n72                         0                 0/n86                         0                 0/n103                        0                 0/n124                        0                 0/n149                        0                 0/n179                        0                 0/n215                        1                 0/n258                     3309                 0/n310                        0                 0/n372                        0                 0/n446                        0                 0/n535                        0                 0/n642                        0                 0/n770                        0                 0/n924                        0                 0/n1109                       0                 0/n1331                       0                 0/n1597                       0                 0/n1916                       0                 0/n2299                       0                 0/n2759                       0                 0/n3311                       0                 0/n3973                       0                 0/n4768                       0                 0/n5722                       0                 0/n6866                       0                 0/n8239                       0                 0/n9887                       0                 0/n11864                      0                 0/n14237                      0                 0/n17084                      0                 0/n20501                      0                 0/n24601                      0                 0/n29521                      0                 0/n35425                      0                 0/n42510                      0                 0/n51012                      0                 0/n61214                      0                 0/n73457                      0                 0/n88148                      0                 0/n105778                     0                 0/n126934                     0                 0/n152321                     0                 0/n182785                     0                 0/n219342                     0                 0/n263210                     0                 0/n315852                     0                 0/n379022                     0                 0/n454826                     0                 0/n545791                     0                 0/n654949                     0                 0/n785939                     0                 0/n943127                     0                 0/n1131752                    0                 0/n1358102                    0                 0/n1629722                    0                 0/n1955666                    0                 0/n2346799                    0                 0/n2816159                    0                 0/n3379391                    0                 0/n4055269                    0                 0/n4866323                    0                 0/n5839588                    0                 0/n7007506                    0                 0/n8409007                    0                 0/n10090808                   0                 0/n12108970                   0                 0/n14530764                   0                 0/n17436917                   0                 0/n20924300                   0                 0/n25109160                   0                 0/n30130992                   0                 0/n36157190                   0                 0/n43388628                   0                 0/n52066354                   0                 0/n62479625                   0                 0/n74975550                   0                 0/n89970660                   0                 0/n107964792                  0                 0/n129557750                  0                 0/n155469300                  0                 0/n186563160                  0                 0/n223875792                  0                 0/n268650950                  0                 0/n322381140                  0                 0/n386857368                  0                 0/n464228842                  0                 0/n557074610                  0                 0/n668489532                  0                 0/n802187438                  0                 0/n962624926                  0                 0/n1155149911                 0                 0/n1386179893                 0                 0/n1663415872                 0                 0/n1996099046                 0                 0/n2395318855                 0                 0/n2874382626                 0                  /n3449259151                 0                  /n4139110981                 0                  /n4966933177                 0                  /n5960319812                 0                  /n7152383774                 0                  /n8582860529                 0                  /n10299432635                 0                  /n12359319162                 0                  /n14831182994                 0                  /n17797419593                 0                  /n21356903512                 0                  /n25628284214                 0                  /n30753941057                 0                  /n36904729268                 0                  /n44285675122                 0                  /n53142810146                 0                  /n63771372175                 0                  /n76525646610                 0                  /n91830775932                 0                  /n110196931118                 0                  /n132236317342                 0                  /n158683580810                 0                  /n190420296972                 0                  /n228504356366                 0                  /n274205227639                 0                  /n329046273167                 0                  /n394855527800                 0                  /n473826633360                 0                  /n568591960032                 0                  /n682310352038                 0                  /n818772422446                 0                  /n982526906935                 0                  /n1179032288322                 0                  /n1414838745986                 0                  /nestimated cardinality: 3310/nencodingstats minttl: 0/nencodingstats minlocaldeletiontime: 1442880000/nencodingstats mintimestamp: 1490129948985001/nkeytype: org.apache.cassandra.db.marshal.bytestype/nclusteringtypes: [org.apache.cassandra.db.marshal.utf8type]/nstaticcolumns: {c3:org.apache.cassandra.db.marshal.bytestype, c4:org.apache.cassandra.db.marshal.bytestype, c0:org.apache.cassandra.db.marshal.bytestype, c1:org.apache.cassandra.db.marshal.bytestype, c2:org.apache.cassandra.db.marshal.bytestype}/nregularcolumns: {}/nsstable: /tmp/dtest-qoneec/test/node1/data1/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-5-big/npartitioner: org.apache.cassandra.dht.murmur3partitioner/nbloom filter fp chance: 0.010000/nminimum timestamp: 1490129948987000/nmaximum timestamp: 1490129952789004/nsstable min local deletion time: 2147483647/nsstable max local deletion time: 2147483647/ncompressor: -/nttl min: 0/nttl max: 0/nfirst token: -3060251208033125494 (key=33364e4b313936504b30)/nlast token: 2923054332122545251 (key=344b3634354b35353430)/nestimated droppable tombstones: 0.0/nsstable level: 0/nrepaired at: 0/npending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf/nreplay positions covered: {commitlogposition(segmentid=1490129923946, position=42824)=commitlogposition(segmentid=1490129923946, position=2605214)}/ntotalcolumnsset: 16565/ntotalrows: 3313/nestimated tombstone drop times:/ncount               row size        cell count/n1                          0                 0/n2                          0                 0/n3                          0                 0/n4                          0                 0/n5                          0              3313/n6                          0                 0/n7                          0                 0/n8                          0                 0/n10                         0                 0/n12                         0                 0/n14                         0                 0/n17                         0                 0/n20                         0                 0/n24                         0                 0/n29                         0                 0/n35                         0                 0/n42                         0                 0/n50                         0                 0/n60                         0                 0/n72                         0                 0/n86                         0                 0/n103                        0                 0/n124                        0                 0/n149                        0                 0/n179                        0                 0/n215                        1                 0/n258                     3312                 0/n310                        0                 0/n372                        0                 0/n446                        0                 0/n535                        0                 0/n642                        0                 0/n770                        0                 0/n924                        0                 0/n1109                       0                 0/n1331                       0                 0/n1597                       0                 0/n1916                       0                 0/n2299                       0                 0/n2759                       0                 0/n3311                       0                 0/n3973                       0                 0/n4768                       0                 0/n5722                       0                 0/n6866                       0                 0/n8239                       0                 0/n9887                       0                 0/n11864                      0                 0/n14237                      0                 0/n17084                      0                 0/n20501                      0                 0/n24601                      0                 0/n29521                      0                 0/n35425                      0                 0/n42510                      0                 0/n51012                      0                 0/n61214                      0                 0/n73457                      0                 0/n88148                      0                 0/n105778                     0                 0/n126934                     0                 0/n152321                     0                 0/n182785                     0                 0/n219342                     0                 0/n263210                     0                 0/n315852                     0                 0/n379022                     0                 0/n454826                     0                 0/n545791                     0                 0/n654949                     0                 0/n785939                     0                 0/n943127                     0                 0/n1131752                    0                 0/n1358102                    0                 0/n1629722                    0                 0/n1955666                    0                 0/n2346799                    0                 0/n2816159                    0                 0/n3379391                    0                 0/n4055269                    0                 0/n4866323                    0                 0/n5839588                    0                 0/n7007506                    0                 0/n8409007                    0                 0/n10090808                   0                 0/n12108970                   0                 0/n14530764                   0                 0/n17436917                   0                 0/n20924300                   0                 0/n25109160                   0                 0/n30130992                   0                 0/n36157190                   0                 0/n43388628                   0                 0/n52066354                   0                 0/n62479625                   0                 0/n74975550                   0                 0/n89970660                   0                 0/n107964792                  0                 0/n129557750                  0                 0/n155469300                  0                 0/n186563160                  0                 0/n223875792                  0                 0/n268650950                  0                 0/n322381140                  0                 0/n386857368                  0                 0/n464228842                  0                 0/n557074610                  0                 0/n668489532                  0                 0/n802187438                  0                 0/n962624926                  0                 0/n1155149911                 0                 0/n1386179893                 0                 0/n1663415872                 0                 0/n1996099046                 0                 0/n2395318855                 0                 0/n2874382626                 0                  /n3449259151                 0                  /n4139110981                 0                  /n4966933177                 0                  /n5960319812                 0                  /n7152383774                 0                  /n8582860529                 0                  /n10299432635                 0                  /n12359319162                 0                  /n14831182994                 0                  /n17797419593                 0                  /n21356903512                 0                  /n25628284214                 0                  /n30753941057                 0                  /n36904729268                 0                  /n44285675122                 0                  /n53142810146                 0                  /n63771372175                 0                  /n76525646610                 0                  /n91830775932                 0                  /n110196931118                 0                  /n132236317342                 0                  /n158683580810                 0                  /n190420296972                 0                  /n228504356366                 0                  /n274205227639                 0                  /n329046273167                 0                  /n394855527800                 0                  /n473826633360                 0                  /n568591960032                 0                  /n682310352038                 0                  /n818772422446                 0                  /n982526906935                 0                  /n1179032288322                 0                  /n1414838745986                 0                  /nestimated cardinality: 3313/nencodingstats minttl: 0/nencodingstats minlocaldeletiontime: 1442880000/nencodingstats mintimestamp: 1490129948987000/nkeytype: org.apache.cassandra.db.marshal.bytestype/nclusteringtypes: [org.apache.cassandra.db.marshal.utf8type]/nstaticcolumns: {c3:org.apache.cassandra.db.marshal.bytestype, c4:org.apache.cassandra.db.marshal.bytestype, c0:org.apache.cassandra.db.marshal.bytestype, c1:org.apache.cassandra.db.marshal.bytestype, c2:org.apache.cassandra.db.marshal.bytestype}/nregularcolumns: {}/nsstable: /tmp/dtest-qoneec/test/node1/data2/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-6-big/npartitioner: org.apache.cassandra.dht.murmur3partitioner/nbloom filter fp chance: 0.010000/nminimum timestamp: 1490129948984000/nmaximum timestamp: 1490129952789003/nsstable min local deletion time: 2147483647/nsstable max local deletion time: 2147483647/ncompressor: -/nttl min: 0/nttl max: 0/nfirst token: 2925175199546211606 (key=3250365032354e4c3231)/nlast token: 9222137691148971235 (key=4c30334f32394d4c3031)/nestimated droppable tombstones: 0.0/nsstable level: 0/nrepaired at: 0/npending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf/nreplay positions covered: {commitlogposition(segmentid=1490129923946, position=42824)=commitlogposition(segmentid=1490129923946, position=2605214)}/ntotalcolumnsset: 16885/ntotalrows: 3377/nestimated tombstone drop times:/ncount               row size        cell count/n1                          0                 0/n2                          0                 0/n3                          0                 0/n4                          0                 0/n5                          0              3377/n6                          0                 0/n7                          0                 0/n8                          0                 0/n10                         0                 0/n12                         0                 0/n14                         0                 0/n17                         0                 0/n20                         0                 0/n24                         0                 0/n29                         0                 0/n35                         0                 0/n42                         0                 0/n50                         0                 0/n60                         0                 0/n72                         0                 0/n86                         0                 0/n103                        0                 0/n124                        0                 0/n149                        0                 0/n179                        0                 0/n215                        1                 0/n258                     3376                 0/n310                        0                 0/n372                        0                 0/n446                        0                 0/n535                        0                 0/n642                        0                 0/n770                        0                 0/n924                        0                 0/n1109                       0                 0/n1331                       0                 0/n1597                       0                 0/n1916                       0                 0/n2299                       0                 0/n2759                       0                 0/n3311                       0                 0/n3973                       0                 0/n4768                       0                 0/n5722                       0                 0/n6866                       0                 0/n8239                       0                 0/n9887                       0                 0/n11864                      0                 0/n14237                      0                 0/n17084                      0                 0/n20501                      0                 0/n24601                      0                 0/n29521                      0                 0/n35425                      0                 0/n42510                      0                 0/n51012                      0                 0/n61214                      0                 0/n73457                      0                 0/n88148                      0                 0/n105778                     0                 0/n126934                     0                 0/n152321                     0                 0/n182785                     0                 0/n219342                     0                 0/n263210                     0                 0/n315852                     0                 0/n379022                     0                 0/n454826                     0                 0/n545791                     0                 0/n654949                     0                 0/n785939                     0                 0/n943127                     0                 0/n1131752                    0                 0/n1358102                    0                 0/n1629722                    0                 0/n1955666                    0                 0/n2346799                    0                 0/n2816159                    0                 0/n3379391                    0                 0/n4055269                    0                 0/n4866323                    0                 0/n5839588                    0                 0/n7007506                    0                 0/n8409007                    0                 0/n10090808                   0                 0/n12108970                   0                 0/n14530764                   0                 0/n17436917                   0                 0/n20924300                   0                 0/n25109160                   0                 0/n30130992                   0                 0/n36157190                   0                 0/n43388628                   0                 0/n52066354                   0                 0/n62479625                   0                 0/n74975550                   0                 0/n89970660                   0                 0/n107964792                  0                 0/n129557750                  0                 0/n155469300                  0                 0/n186563160                  0                 0/n223875792                  0                 0/n268650950                  0                 0/n322381140                  0                 0/n386857368                  0                 0/n464228842                  0                 0/n557074610                  0                 0/n668489532                  0                 0/n802187438                  0                 0/n962624926                  0                 0/n1155149911                 0                 0/n1386179893                 0                 0/n1663415872                 0                 0/n1996099046                 0                 0/n2395318855                 0                 0/n2874382626                 0                  /n3449259151                 0                  /n4139110981                 0                  /n4966933177                 0                  /n5960319812                 0                  /n7152383774                 0                  /n8582860529                 0                  /n10299432635                 0                  /n12359319162                 0                  /n14831182994                 0                  /n17797419593                 0                  /n21356903512                 0                  /n25628284214                 0                  /n30753941057                 0                  /n36904729268                 0                  /n44285675122                 0                  /n53142810146                 0                  /n63771372175                 0                  /n76525646610                 0                  /n91830775932                 0                  /n110196931118                 0                  /n132236317342                 0                  /n158683580810                 0                  /n190420296972                 0                  /n228504356366                 0                  /n274205227639                 0                  /n329046273167                 0                  /n394855527800                 0                  /n473826633360                 0                  /n568591960032                 0                  /n682310352038                 0                  /n818772422446                 0                  /n982526906935                 0                  /n1179032288322                 0                  /n1414838745986                 0                  /nestimated cardinality: 3377/nencodingstats minttl: 0/nencodingstats minlocaldeletiontime: 1442880000/nencodingstats mintimestamp: 1490129948984000/nkeytype: org.apache.cassandra.db.marshal.bytestype/nclusteringtypes: [org.apache.cassandra.db.marshal.utf8type]/nstaticcolumns: {c3:org.apache.cassandra.db.marshal.bytestype, c4:org.apache.cassandra.db.marshal.bytestype, c0:org.apache.cassandra.db.marshal.bytestype, c1:org.apache.cassandra.db.marshal.bytestype, c2:org.apache.cassandra.db.marshal.bytestype}/nregularcolumns: {}/n' stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py', line 332, in sstable_marking_test     self.assertnotin('repaired at: 0', out)   file '/usr/lib/python2.7/unittest/case.py', line 810, in assertnotin     self.fail(self._formatmessage(msg, standardmsg))   file '/usr/lib/python2.7/unittest/case.py', line 410, in fail     raise self.failureexception(msg) http://cassci.datastax.com/job/trunk_dtest/1525/testreport/repair_tests.incremental_repair_test/testincrepair/sstable_marking_test<text> example failure: ",
        "label": 79
    },
    {
        "text": " patch  presize arraylists where possible <description> many places the size of an arraylist is easily predetermined, in those cases pre size them as appropriate, to avoid reallocations.<stacktrace> <code> <text> many places the size of an arraylist is easily predetermined, in those cases pre size them as appropriate, to avoid reallocations.",
        "label": 139
    },
    {
        "text": "make the tokens app state the canonical place for tokens <description> currently we store legacy-style single token deployments in our old, hackish status and split on a comma delimeter, but store vnodes in the tokens app state properly. we should be able to put the single token in tokens as well in a 1.2 minor release, make that the minimum release needed to go to 2.0, and remove the status hacks for 2.0.<stacktrace> <code> <text> currently we store legacy-style single token deployments in our old, hackish status and split on a comma delimeter, but store vnodes in the tokens app state properly. we should be able to put the single token in tokens as well in a 1.2 minor release, make that the minimum release needed to go to 2.0, and remove the status hacks for 2.0.",
        "label": 85
    },
    {
        "text": "composite case sensitive primary key  first item is not quoted in describe table <description> a table is created with case-sensitive composite primary key: create table foo (  \"key1\" text,  \"key2\" text,  \"value\" text  primary key (\"key1\", \"key2\") ); in the result of describe table foo;: create table foo (  \"key1\" text,  \"key2\" text,  \"value\" text  primary key (key1, \"key2\") ); key1 is not quoted.  when trying to re-create the table with this description, there is an error: invalidrequest: code=2200 [invalid query] message=\"unknown definition key1 referenced in primary key\"<stacktrace> <code> create table foo (  'key1' text,  'key2' text,  'value' text  primary key ('key1', 'key2') ); create table foo (  'key1' text,  'key2' text,  'value' text  primary key (key1, 'key2') ); invalidrequest: code=2200 [invalid query] message='unknown definition key1 referenced in primary key' <text> a table is created with case-sensitive composite primary key: in the result of describe table foo;: key1 is not quoted.  when trying to re-create the table with this description, there is an error:",
        "label": 98
    },
    {
        "text": "cqlsh errors on comments that end with a semicolon <description> commented-out lines that end in a semicolon cause an error. examples: cqlsh> \u2013 create keyspace ele with replication_factor = 3 and strategy_class = simplestrategy and strategy_options:replication_factor=3;  bad request: line 0:-1 no viable alternative at input '<eof>'  cqlsh> \u2013 create keyspace ele with replication_factor = 3 and strategy_class = simplestrategy and strategy_options:replication_factor=3  ...   ...   ... ;  bad request: line 2:0 no viable alternative at input ';'  cqlsh> \u2013 ;  bad request: line 0:-1 no viable alternative at input '<eof>'  cqlsh> --;  bad request: line 0:-1 no viable alternative at input '<eof>' as long as there's a line with valid cql before the semicolon, things work fine though. i'm pretty sure the problem is on line 75 of cqlsh:  if not line.endswith(\";\"):  self.set_prompt(shell.continue_prompt)  return none a quick workaround would be to kill the pretty continue prompt. a more involved fix would detect whether or not the semicolon was in a comment. this is harder than it sounds, since /* and */ allow multi-line comments.<stacktrace> <code> cqlsh> - create keyspace ele with replication_factor = 3 and strategy_class = simplestrategy and strategy_options:replication_factor=3;  bad request: line 0:-1 no viable alternative at input '<eof>'  cqlsh> - create keyspace ele with replication_factor = 3 and strategy_class = simplestrategy and strategy_options:replication_factor=3  ...   ...   ... ;  bad request: line 2:0 no viable alternative at input ';'  cqlsh> - ;  bad request: line 0:-1 no viable alternative at input '<eof>'  cqlsh> --;  bad request: line 0:-1 no viable alternative at input '<eof>' <text> commented-out lines that end in a semicolon cause an error. examples: as long as there's a line with valid cql before the semicolon, things work fine though. i'm pretty sure the problem is on line 75 of cqlsh:  if not line.endswith(';'):  self.set_prompt(shell.continue_prompt)  return none a quick workaround would be to kill the pretty continue prompt. a more involved fix would detect whether or not the semicolon was in a comment. this is harder than it sounds, since /* and */ allow multi-line comments.",
        "label": 169
    },
    {
        "text": "create a dtest for cassandra <description> as the title suggests. a small complication is the the test will need to ensure it reduces the column_index_size_in_kb and then writes more columns than that,<stacktrace> <code> <text> as the title suggests. a small complication is the the test will need to ensure it reduces the column_index_size_in_kb and then writes more columns than that,",
        "label": 132
    },
    {
        "text": "typo in src java org apache cassandra cli cliclient <description> <stacktrace> <code> but then i was receiving error: 'no enum const class org.apache.cassandra.cli.cliclient$columnfamilyargument.keys_cache_save_period' <text> i have read your documentation about syntax for creating column family and parameters that i can pass.  according to documentation i can use parameter : ' - keys_cache_save_period: duration in seconds after which cassandra should  safe the keys cache. caches are saved to saved_caches_directory as  specified in conf/cassandra.yaml. default is 14400 or 4 hours. ' in class mentioned in title we have: protected enum columnfamilyargument  115",
        "label": 274
    },
    {
        "text": "after bootstrap or replace node startup  expiring map reaper is shutdown and cannot be restarted  causing callbacks to collect indefinitely <description> since expiringmap.shutdown() shuts down the static executor service, it cannot be restarted (and in fact reset() makes no attempt to do so). as such callbacks that receive no response are never removed from the map, and eventually either than server will run out of memory or will loop around the integer space and start reusing messageids that have not been expired, causing assertions to be thrown and messages to fail to be sent. it appears that this situation only arises on bootstrap or node replacement, as messagingservice is shutdown before being attached to the listen address. this can cause the following errors to begin occurring in the log: error [native-transport-requests:7636] 2014-03-28 13:32:10,638 errormessage.java (line 222) unexpected exception during request  java.lang.assertionerror: callback already exists for id -1665979622! (callbackinfo(target=/10.106.160.84, callback=org.apache.cassandra.service.writeresponsehandler@5d36d8ea, serializer=org.apache.cassandra.db.writeresponse$writeresponseserializer@6ed37f0b))  at org.apache.cassandra.net.messagingservice.addcallback(messagingservice.java:549)  at org.apache.cassandra.net.messagingservice.sendrr(messagingservice.java:601)  at org.apache.cassandra.service.storageproxy.mutatecounter(storageproxy.java:984)  at org.apache.cassandra.service.storageproxy.mutate(storageproxy.java:449)  at org.apache.cassandra.service.storageproxy.mutatewithtriggers(storageproxy.java:524)  at org.apache.cassandra.cql3.statements.modificationstatement.executewithoutcondition(modificationstatement.java:521)  at org.apache.cassandra.cql3.statements.modificationstatement.execute(modificationstatement.java:505)  at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:188)  at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:358)  at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:131)  at org.apache.cassandra.transport.message$dispatcher.messagereceived(message.java:304)  at org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun(channelupstreameventrunnable.java:43)  at org.jboss.netty.handler.execution.channeleventrunnable.run(channeleventrunnable.java:67)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)  error [replicateonwritestage:102766] 2014-03-28 13:32:10,638 cassandradaemon.java (line 196) exception in thread thread[replicateonwritestage:102766,5,main]  java.lang.assertionerror: callback already exists for id -1665979620! (callbackinfo(target=/10.106.160.84, callback=org.apache.cassandra.service.writeresponsehandler@3bdb1a75, serializer=org.apache.cassandra.db.writeresponse$writeresponseserializer@6ed37f0b))  at org.apache.cassandra.net.messagingservice.addcallback(messagingservice.java:549)  at org.apache.cassandra.net.messagingservice.sendrr(messagingservice.java:601)  at org.apache.cassandra.service.storageproxy.sendtohintedendpoints(storageproxy.java:806)  at org.apache.cassandra.service.storageproxy$8$1.runmaythrow(storageproxy.java:1074)  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1896)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)<stacktrace> error [native-transport-requests:7636] 2014-03-28 13:32:10,638 errormessage.java (line 222) unexpected exception during request  java.lang.assertionerror: callback already exists for id -1665979622! (callbackinfo(target=/10.106.160.84, callback=org.apache.cassandra.service.writeresponsehandler@5d36d8ea, serializer=org.apache.cassandra.db.writeresponse$writeresponseserializer@6ed37f0b))  at org.apache.cassandra.net.messagingservice.addcallback(messagingservice.java:549)  at org.apache.cassandra.net.messagingservice.sendrr(messagingservice.java:601)  at org.apache.cassandra.service.storageproxy.mutatecounter(storageproxy.java:984)  at org.apache.cassandra.service.storageproxy.mutate(storageproxy.java:449)  at org.apache.cassandra.service.storageproxy.mutatewithtriggers(storageproxy.java:524)  at org.apache.cassandra.cql3.statements.modificationstatement.executewithoutcondition(modificationstatement.java:521)  at org.apache.cassandra.cql3.statements.modificationstatement.execute(modificationstatement.java:505)  at org.apache.cassandra.cql3.queryprocessor.processstatement(queryprocessor.java:188)  at org.apache.cassandra.cql3.queryprocessor.processprepared(queryprocessor.java:358)  at org.apache.cassandra.transport.messages.executemessage.execute(executemessage.java:131)  at org.apache.cassandra.transport.message$dispatcher.messagereceived(message.java:304)  at org.jboss.netty.handler.execution.channelupstreameventrunnable.dorun(channelupstreameventrunnable.java:43)  at org.jboss.netty.handler.execution.channeleventrunnable.run(channeleventrunnable.java:67)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)  error [replicateonwritestage:102766] 2014-03-28 13:32:10,638 cassandradaemon.java (line 196) exception in thread thread[replicateonwritestage:102766,5,main]  java.lang.assertionerror: callback already exists for id -1665979620! (callbackinfo(target=/10.106.160.84, callback=org.apache.cassandra.service.writeresponsehandler@3bdb1a75, serializer=org.apache.cassandra.db.writeresponse$writeresponseserializer@6ed37f0b))  at org.apache.cassandra.net.messagingservice.addcallback(messagingservice.java:549)  at org.apache.cassandra.net.messagingservice.sendrr(messagingservice.java:601)  at org.apache.cassandra.service.storageproxy.sendtohintedendpoints(storageproxy.java:806)  at org.apache.cassandra.service.storageproxy$8$1.runmaythrow(storageproxy.java:1074)  at org.apache.cassandra.service.storageproxy$droppablerunnable.run(storageproxy.java:1896)  at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1145)  at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:615)  at java.lang.thread.run(thread.java:744)<code> <text> since expiringmap.shutdown() shuts down the static executor service, it cannot be restarted (and in fact reset() makes no attempt to do so). as such callbacks that receive no response are never removed from the map, and eventually either than server will run out of memory or will loop around the integer space and start reusing messageids that have not been expired, causing assertions to be thrown and messages to fail to be sent. it appears that this situation only arises on bootstrap or node replacement, as messagingservice is shutdown before being attached to the listen address. this can cause the following errors to begin occurring in the log: ",
        "label": 85
    },
    {
        "text": "more efficient executorservice for improved throughput <description> currently all our execution stages dequeue tasks one at a time. this can result in contention between producers and consumers (although we do our best to minimize this by using linkedblockingqueue). one approach to mitigating this would be to make consumer threads do more work in \"bulk\" instead of just one task per dequeue. (producer threads tend to be single-task oriented by nature, so i don't see an equivalent opportunity there.) blockingqueue has a drainto(collection, int) method that would be perfect for this. however, no executorservice in the jdk supports using drainto, nor could i google one. what i would like to do here is create just such a beast and wire it into (at least) the write and read stages. (other possible candidates for such an optimization, such as the commitlog and outboundtcpconnection, are not executorservice-based and will need to be one-offs.) abstractexecutorservice may be useful. the implementations of icommitlogexecutorservice may also be useful. (despite the name these are not actual executorservices, although they share the most important properties of one.)<stacktrace> <code> <text> currently all our execution stages dequeue tasks one at a time. this can result in contention between producers and consumers (although we do our best to minimize this by using linkedblockingqueue). one approach to mitigating this would be to make consumer threads do more work in 'bulk' instead of just one task per dequeue. (producer threads tend to be single-task oriented by nature, so i don't see an equivalent opportunity there.) blockingqueue has a drainto(collection, int) method that would be perfect for this. however, no executorservice in the jdk supports using drainto, nor could i google one. what i would like to do here is create just such a beast and wire it into (at least) the write and read stages. (other possible candidates for such an optimization, such as the commitlog and outboundtcpconnection, are not executorservice-based and will need to be one-offs.) abstractexecutorservice may be useful. the implementations of icommitlogexecutorservice may also be useful. (despite the name these are not actual executorservices, although they share the most important properties of one.)",
        "label": 67
    },
    {
        "text": "jmx stats for bloomfilters <description> handy for debugging bloomfilter blowouts etc.<stacktrace> <code> <text> handy for debugging bloomfilter blowouts etc.",
        "label": 240
    },
    {
        "text": "listing wide rows from cli crashes cassandra <description> if a user attempts to list a column family from the cli that contains a wide-row (e.g. 10 million columns). it crashes hangs the cli and then cassandra eventually crashes with an oom. we should introduce a default limit on columns when listing a column family. (patch on its way)<stacktrace> <code> <text> if a user attempts to list a column family from the cli that contains a wide-row (e.g. 10 million columns). it crashes hangs the cli and then cassandra eventually crashes with an oom. we should introduce a default limit on columns when listing a column family. (patch on its way)",
        "label": 90
    },
    {
        "text": "sign rpm artifacts <description> rpms should be gpg signed just as the deb packages. also add documentation how to verify to download page.<stacktrace> <code> <text> rpms should be gpg signed just as the deb packages. also add documentation how to verify to download page.",
        "label": 348
    },
    {
        "text": "extract sstable interfaces to allow for alternative implementations <description> in order to create alternate implementations of sstablereader and scanner, we need to extract abstract bases for those classes.<stacktrace> <code> <text> in order to create alternate implementations of sstablereader and scanner, we need to extract abstract bases for those classes.",
        "label": 515
    },
    {
        "text": "update cassandra yaml comments to reflect memory allocator deprecation  remove in <description> looks like in 2.2+ changing the memory_allocator field in cassandra.yaml has no effect: https://github.com/apache/cassandra/commit/0d2ec11c7e0abfb84d872289af6d3ac386cf381f#diff-b66584c9ce7b64019b5db5a531deeda1r207 the instructions in comments on how to use jemalloc haven't been updated to reflect this change. robert stupp, is that an accurate assessment? [~iamaleksey] how do we want to handle the deprecation more generally? warn on 2.2, remove in 3.0? edit: i described this in a way that isn't very clear to those unfamiliar with robert's change. to be clear: you can still use jemalloc with cassandra. the memory_allocator option was deprecated in favor of automatically detecting if jemalloc is installed, and, if so, using it.<stacktrace> <code> https://github.com/apache/cassandra/commit/0d2ec11c7e0abfb84d872289af6d3ac386cf381f#diff-b66584c9ce7b64019b5db5a531deeda1r207 <text> looks like in 2.2+ changing the memory_allocator field in cassandra.yaml has no effect: the instructions in comments on how to use jemalloc haven't been updated to reflect this change. robert stupp, is that an accurate assessment? [~iamaleksey] how do we want to handle the deprecation more generally? warn on 2.2, remove in 3.0? edit: i described this in a way that isn't very clear to those unfamiliar with robert's change. to be clear: you can still use jemalloc with cassandra. the memory_allocator option was deprecated in favor of automatically detecting if jemalloc is installed, and, if so, using it.",
        "label": 453
    },
    {
        "text": "new default jmx port <description> 8080 is too commonly used as an alternate port for web interfaces. a less commonly used port would make for a better default because fewer people would need to change it.<stacktrace> <code> <text> 8080 is too commonly used as an alternate port for web interfaces. a less commonly used port would make for a better default because fewer people would need to change it.",
        "label": 169
    },
    {
        "text": "explore not returning range ghosts <description> this ticket proposes to remove range ghosts in cql3.  the basic argument is that range ghosts confuses users a lot and don't add any value since range ghost don't allow to distinguish between the two following case: the row is deleted the row is not deleted but don't have data for the provided filter<stacktrace> <code> <text> this ticket proposes to remove range ghosts in cql3.  the basic argument is that range ghosts confuses users a lot and don't add any value since range ghost don't allow to distinguish between the two following case:",
        "label": 520
    },
    {
        "text": "unsafeassassinateendpoint throws nullpointerexception and fails to remove node from gossip <description> unsafeassassinateendpoint() throws nullpointerexception and the node still seems to be in gossip. gossip info for node in question: /10.8.30.15   host_id:d84a5632-d6d5-4b06-8e1b-ae39ab185ca1   rpc_address:0.0.0.0   rack:rac1   dc:dc1   removal_coordinator:remover,b63fe173-5d13-4905-a59f-a78790f4f980   release_version:1.2.0   net_version:6   load:2.64185473948e11   status:removed,d84a5632-d6d5-4b06-8e1b-ae39ab185ca1,1357874470406   schema:5cd8420d-ce3c-3625-8293-67558a24816b error 19:26:20,078 exception in thread thread[gossipstage:1,5,main] java.lang.nullpointerexception at org.apache.cassandra.service.storageservice.getapplicationstatevalue(storageservice.java:1192) at org.apache.cassandra.service.storageservice.gettokensfor(storageservice.java:1200) at org.apache.cassandra.service.storageservice.handlestateleft(storageservice.java:1452) at org.apache.cassandra.service.storageservice.onchange(storageservice.java:1163) at org.apache.cassandra.service.storageservice.onjoin(storageservice.java:1895) at org.apache.cassandra.gms.gossiper.handlemajorstatechange(gossiper.java:805) at org.apache.cassandra.gms.gossiper.applystatelocally(gossiper.java:856) at org.apache.cassandra.gms.gossipdigestackverbhandler.doverb(gossipdigestackverbhandler.java:57) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:56) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) <stacktrace> error 19:26:20,078 exception in thread thread[gossipstage:1,5,main] java.lang.nullpointerexception at org.apache.cassandra.service.storageservice.getapplicationstatevalue(storageservice.java:1192) at org.apache.cassandra.service.storageservice.gettokensfor(storageservice.java:1200) at org.apache.cassandra.service.storageservice.handlestateleft(storageservice.java:1452) at org.apache.cassandra.service.storageservice.onchange(storageservice.java:1163) at org.apache.cassandra.service.storageservice.onjoin(storageservice.java:1895) at org.apache.cassandra.gms.gossiper.handlemajorstatechange(gossiper.java:805) at org.apache.cassandra.gms.gossiper.applystatelocally(gossiper.java:856) at org.apache.cassandra.gms.gossipdigestackverbhandler.doverb(gossipdigestackverbhandler.java:57) at org.apache.cassandra.net.messagedeliverytask.run(messagedeliverytask.java:56) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1110) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:603) at java.lang.thread.run(thread.java:722) <code> /10.8.30.15   host_id:d84a5632-d6d5-4b06-8e1b-ae39ab185ca1   rpc_address:0.0.0.0   rack:rac1   dc:dc1   removal_coordinator:remover,b63fe173-5d13-4905-a59f-a78790f4f980   release_version:1.2.0   net_version:6   load:2.64185473948e11   status:removed,d84a5632-d6d5-4b06-8e1b-ae39ab185ca1,1357874470406   schema:5cd8420d-ce3c-3625-8293-67558a24816b <text> unsafeassassinateendpoint() throws nullpointerexception and the node still seems to be in gossip. gossip info for node in question:",
        "label": 85
    },
    {
        "text": "missing files in debian  etc cassandra conf folder <description> the standard debian installation puts; cassandra-env.sh cassandra.yaml log4j-server.properties into /etc/cassandra. however there seem to be additional files that might make sense there: commitlog_archiving.properties cassandra-rackdc.properties log4j-tools.properties cassandra-topology.properties (these are at least installed by the dsc rpm installer). so should those be added in debian or removed in rpms?<stacktrace> <code> the standard debian installation puts; cassandra-env.sh cassandra.yaml log4j-server.properties commitlog_archiving.properties cassandra-rackdc.properties log4j-tools.properties cassandra-topology.properties <text> into /etc/cassandra. however there seem to be additional files that might make sense there: (these are at least installed by the dsc rpm installer). so should those be added in debian or removed in rpms?",
        "label": 85
    },
    {
        "text": "replace sstable2json <description> both tools are pretty awful. they are primarily meant for debugging (there is much more efficient and convenient ways to do import/export data), but their output manage to be hard to handle both for humans and for tools (especially as soon as you have modern stuff like composites). there is value to having tools to export sstable contents into a format that is easy to manipulate by human and tools for debugging, small hacks and general tinkering, but sstable2json and json2sstable are not that. so i propose that we deprecate those tools and consider writing better replacements. it shouldn't be too hard to come up with an output format that is more aware of modern concepts like composites, udts, ....<stacktrace> <code> <text> both tools are pretty awful. they are primarily meant for debugging (there is much more efficient and convenient ways to do import/export data), but their output manage to be hard to handle both for humans and for tools (especially as soon as you have modern stuff like composites). there is value to having tools to export sstable contents into a format that is easy to manipulate by human and tools for debugging, small hacks and general tinkering, but sstable2json and json2sstable are not that. so i propose that we deprecate those tools and consider writing better replacements. it shouldn't be too hard to come up with an output format that is more aware of modern concepts like composites, udts, ....",
        "label": 106
    },
    {
        "text": "java lang incompatibleclasschangeerror when starting embeddedcassandraservice <description> when i try to start embeddedcassandraservice thus: embeddedcassandraservice cassandra = new embeddedcassandraservice();  cassandra.start(); i get the following exception java.lang.incompatibleclasschangeerror: found interface net.jpountz.lz4.lz4decompressor, but class was expected  at org.apache.cassandra.io.compress.lz4compressor.uncompress(lz4compressor.java:84)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.decompresschunk(compressedrandomaccessreader.java:116)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.rebuffer(compressedrandomaccessreader.java:85)  at org.apache.cassandra.io.util.randomaccessreader.seek(randomaccessreader.java:275)  at org.apache.cassandra.io.util.poolingsegmentedfile.getsegment(poolingsegmentedfile.java:42)  at org.apache.cassandra.io.sstable.sstablereader.getfiledatainput(sstablereader.java:1093)  at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:57)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:65)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:42)  at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:171)  at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:62)  at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:269)  at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1469)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1295)  at org.apache.cassandra.db.keyspace.getrow(keyspace.java:332)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)  at org.apache.cassandra.cql3.statements.selectstatement.readlocally(selectstatement.java:231)  at org.apache.cassandra.cql3.statements.selectstatement.executeinternal(selectstatement.java:249)  at org.apache.cassandra.cql3.statements.selectstatement.executeinternal(selectstatement.java:56)  at org.apache.cassandra.cql3.queryprocessor.processinternal(queryprocessor.java:151)  at org.apache.cassandra.db.systemkeyspace.checkhealth(systemkeyspace.java:459)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:227)  at org.apache.cassandra.service.cassandradaemon.init(cassandradaemon.java:377)  at org.apache.cassandra.service.embeddedcassandraservice.start(embeddedcassandraservice.java:52)<stacktrace> java.lang.incompatibleclasschangeerror: found interface net.jpountz.lz4.lz4decompressor, but class was expected  at org.apache.cassandra.io.compress.lz4compressor.uncompress(lz4compressor.java:84)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.decompresschunk(compressedrandomaccessreader.java:116)  at org.apache.cassandra.io.compress.compressedrandomaccessreader.rebuffer(compressedrandomaccessreader.java:85)  at org.apache.cassandra.io.util.randomaccessreader.seek(randomaccessreader.java:275)  at org.apache.cassandra.io.util.poolingsegmentedfile.getsegment(poolingsegmentedfile.java:42)  at org.apache.cassandra.io.sstable.sstablereader.getfiledatainput(sstablereader.java:1093)  at org.apache.cassandra.db.columniterator.simpleslicereader.<init>(simpleslicereader.java:57)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.createreader(sstablesliceiterator.java:65)  at org.apache.cassandra.db.columniterator.sstablesliceiterator.<init>(sstablesliceiterator.java:42)  at org.apache.cassandra.db.filter.slicequeryfilter.getsstablecolumniterator(slicequeryfilter.java:171)  at org.apache.cassandra.db.filter.queryfilter.getsstablecolumniterator(queryfilter.java:62)  at org.apache.cassandra.db.collationcontroller.collectalldata(collationcontroller.java:269)  at org.apache.cassandra.db.collationcontroller.gettoplevelcolumns(collationcontroller.java:53)  at org.apache.cassandra.db.columnfamilystore.gettoplevelcolumns(columnfamilystore.java:1469)  at org.apache.cassandra.db.columnfamilystore.getcolumnfamily(columnfamilystore.java:1295)  at org.apache.cassandra.db.keyspace.getrow(keyspace.java:332)  at org.apache.cassandra.db.slicefromreadcommand.getrow(slicefromreadcommand.java:65)  at org.apache.cassandra.cql3.statements.selectstatement.readlocally(selectstatement.java:231)  at org.apache.cassandra.cql3.statements.selectstatement.executeinternal(selectstatement.java:249)  at org.apache.cassandra.cql3.statements.selectstatement.executeinternal(selectstatement.java:56)  at org.apache.cassandra.cql3.queryprocessor.processinternal(queryprocessor.java:151)  at org.apache.cassandra.db.systemkeyspace.checkhealth(systemkeyspace.java:459)  at org.apache.cassandra.service.cassandradaemon.setup(cassandradaemon.java:227)  at org.apache.cassandra.service.cassandradaemon.init(cassandradaemon.java:377)  at org.apache.cassandra.service.embeddedcassandraservice.start(embeddedcassandraservice.java:52)<code> embeddedcassandraservice cassandra = new embeddedcassandraservice();  cassandra.start(); <text> when i try to start embeddedcassandraservice thus: i get the following exception ",
        "label": 520
    },
    {
        "text": "dtest failure in materialized views test testmaterializedviews add dc after mv simple replication test <description> example failure: http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testreport/materialized_views_test/testmaterializedviews/add_dc_after_mv_simple_replication_test/ stacktrace   file \"/usr/lib/python2.7/unittest/case.py\", line 329, in run     testmethod()   file \"/home/automaton/cassandra-dtest/materialized_views_test.py\", line 389, in add_dc_after_mv_simple_replication_test     self._add_dc_after_mv_test(1)   file \"/home/automaton/cassandra-dtest/materialized_views_test.py\", line 363, in _add_dc_after_mv_test     node5.start(jvm_args=[\"-dcassandra.migration_task_wait_in_seconds={}\".format(migration_wait)])   file \"/home/automaton/ccm/ccmlib/node.py\", line 636, in start     self._update_pid(process)   file \"/home/automaton/ccm/ccmlib/node.py\", line 1770, in _update_pid     raise nodeerror('problem starting node %s due to %s' % (self.name, e), process) \"problem starting node node5 due to [errno 2] no such file or directory: '/tmp/dtest-s4bmf0/test/node5/cassandra.pid' related failure:  http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testreport/materialized_views_test/testmaterializedviews/add_dc_after_mv_network_replication_test/<stacktrace> <code> stacktrace   file '/usr/lib/python2.7/unittest/case.py', line 329, in run     testmethod()   file '/home/automaton/cassandra-dtest/materialized_views_test.py', line 389, in add_dc_after_mv_simple_replication_test     self._add_dc_after_mv_test(1)   file '/home/automaton/cassandra-dtest/materialized_views_test.py', line 363, in _add_dc_after_mv_test     node5.start(jvm_args=['-dcassandra.migration_task_wait_in_seconds={}'.format(migration_wait)])   file '/home/automaton/ccm/ccmlib/node.py', line 636, in start     self._update_pid(process)   file '/home/automaton/ccm/ccmlib/node.py', line 1770, in _update_pid     raise nodeerror('problem starting node %s due to %s' % (self.name, e), process) 'problem starting node node5 due to [errno 2] no such file or directory: '/tmp/dtest-s4bmf0/test/node5/cassandra.pid' http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testreport/materialized_views_test/testmaterializedviews/add_dc_after_mv_simple_replication_test/ related failure:  http://cassci.datastax.com/job/cassandra-3.0_novnode_dtest/301/testreport/materialized_views_test/testmaterializedviews/add_dc_after_mv_network_replication_test/<text> example failure: ",
        "label": 428
    },
    {
        "text": "oracle java u4 does not allow xss128k <description> problem: this happens when you try to start it with default xss setting of 128k  =======  the stack size specified is too small, specify at least 160k  error: could not create the java virtual machine.  error: a fatal exception has occurred. program will exit. solution  =======  set -xss to 256k problem: this happens when you try to start it with xss = 160k  ========  error [thrift:14] 2012-05-22 14:42:40,479 abstractcassandradaemon.java (line 139) fatal exception in thread thread[thrift:14,5,main]  java.lang.stackoverflowerror solution  =======  set -xss to 256k<stacktrace> <code> solution  =======  set -xss to 256k problem: this happens when you try to start it with xss = 160k  ========  error [thrift:14] 2012-05-22 14:42:40,479 abstractcassandradaemon.java (line 139) fatal exception in thread thread[thrift:14,5,main]  java.lang.stackoverflowerror solution  =======  set -xss to 256k<text> problem: this happens when you try to start it with default xss setting of 128k  =======  the stack size specified is too small, specify at least 160k  error: could not create the java virtual machine.  error: a fatal exception has occurred. program will exit. ",
        "label": 520
    },
    {
        "text": "cql3 docs don't mention batch unlogged <description> here http://cassandra.apache.org/doc/cql3/cql.html#batchstmt there is no mentioned of batch unlogged or batch counter. the datastax documentation has it: http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/batch_r.html<stacktrace> <code> <text> here http://cassandra.apache.org/doc/cql3/cql.html#batchstmt there is no mentioned of batch unlogged or batch counter. the datastax documentation has it: http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/batch_r.html",
        "label": 538
    }
]